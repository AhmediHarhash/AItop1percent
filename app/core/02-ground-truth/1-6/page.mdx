# 1.6 — Ground Truth Is a Living Document

Let me tell you about a team that defined perfect ground truth and then watched it become completely wrong.

They built an AI customer service system for a travel company in early 2023. They spent weeks crafting ground truth definitions. Every response type had clear criteria. Correctness: matches booking policies. Safety: includes cancellation terms. Usefulness: solves customer problems efficiently.

They launched in March 2023. By June 2023, their ground truth was obsolete. Why? Because the company changed its cancellation policy in response to market conditions. Then they added new service tiers with different rules. Then they expanded to three new countries with different regulations. Then they integrated with new airline partners with different booking flows.

The ground truth document sat unchanged in their repository while the product reality shifted underneath it. By August, evaluators were labeling responses as "correct" that were actually giving customers outdated information. The system was optimizing for ground truth that no longer matched actual truth.

This is the reality nobody tells you: ground truth isn't a one-time definition. It's a living document that must evolve with your product, your business, and the world around you.

## The Decay Problem

Ground truth decays over time. The criteria that perfectly defined quality at launch become partially wrong months later.

Think about it like documentation. Code documentation that's never updated becomes dangerously misleading. It's worse than no documentation because it gives engineers false confidence. They trust it and make wrong assumptions.

Outdated ground truth is the same. It's worse than undefined ground truth because your team trusts it. You're measuring against yesterday's standard and thinking you're measuring quality today.

Let me show you what ground truth decay looks like across different time scales:

Daily decay: Real-time information like stock prices, sports scores, breaking news. Ground truth from yesterday is literally wrong today.

Weekly decay: Product inventory, event schedules, promotional offers. Your ground truth about what's in stock becomes wrong as inventory changes.

Monthly decay: Feature releases, policy updates, team changes. Your ground truth about how the product works becomes wrong as you ship updates.

Quarterly decay: Business strategy, market position, competitive landscape. Your ground truth about what matters to customers shifts as the market evolves.

Yearly decay: Regulations, cultural norms, user expectations. Your ground truth about what's acceptable or required changes as society and law evolve.

If you're not actively maintaining ground truth, it's decaying right now.

## The Six Ground Truth Update Triggers

Elite teams don't wait for ground truth to become obviously wrong. They have triggers that prompt systematic review.

Trigger 1: Product Changes

Every time you ship a meaningful product update, ask: does this change what "correct" means?

Added a new feature? Ground truth needs criteria for that feature.
Deprecated old functionality? Ground truth still references it and needs updating.
Changed pricing? Ground truth about cost-related queries is now wrong.
Updated UI? Ground truth about user guidance needs updating.

I watched a team ship a redesigned checkout flow and forget to update ground truth for their support bot. For two months, the bot gave correct instructions for the old flow. Customers got confused. Support tickets increased. Nobody connected it to ground truth because nobody thought of ground truth as a thing that needs updating.

Trigger 2: Policy Changes

Business policies change constantly. Every change invalidates some portion of ground truth.

Legal updates a contract template? Ground truth for contract review tools must update.
HR changes remote work policy? Ground truth for internal chatbots must update.
Support changes escalation criteria? Ground truth for support quality must update.
Finance changes approval thresholds? Ground truth for expense assistants must update.

Set up a system where policy owners notify the ground truth owners when policies change. Don't rely on catching it retroactively.

Trigger 3: Regulation Changes

Regulatory environments shift, especially in 2026 with the EU AI Act fully in effect.

A healthcare regulation changes disclosure requirements? Your ground truth about medical advice must update.
A privacy law tightens data handling rules? Your ground truth about what information to surface must update.
An advertising standard changes claim requirements? Your ground truth about marketing content must update.
A financial regulation changes advice licensing? Your ground truth about financial suggestions must update.

Track regulatory deadlines. The EU AI Act has specific compliance dates. Your ground truth must update before those dates, not after.

Trigger 4: User Feedback Patterns

When you see repeated user complaints about the same issue, that's ground truth feedback.

Users consistently say responses are "too formal"? Your ground truth tone criteria might be wrong for your actual audience.
Users frequently ask follow-up questions to "complete" answers? Your ground truth completeness standard might be too low.
Users escalate to humans after getting "correct" answers? Your ground truth is measuring accuracy but missing usefulness.

Set up a review process: every month, look at top user complaint themes and ask if they reveal ground truth mismatches.

Trigger 5: Model Updates

When you change the underlying model, ground truth might need adjustment.

New models have different capabilities. What was impossible to require in ground truth with your old model might be achievable with your new model.

Example: Your old model couldn't reliably generate tables, so your ground truth said "provide information in paragraph form." Your new model handles tables well. Your ground truth should update to "use tables when they improve clarity."

Or the opposite: your new model is worse at something. If ground truth requires capability the model no longer has, you need to adjust.

Trigger 6: Competitive Shifts

What users consider "good enough" changes as competitive products improve.

In 2022, users tolerated slower responses. By 2026, competitive products are faster, so user expectations increased.

In 2023, users accepted generic answers. By 2026, competitive products personalize, so generic feels low-quality.

Your ground truth must evolve with market standards. "Good enough" is a moving target.

## The Versioning Imperative

Ground truth should be versioned like code. Not because it's code—because it's a critical product artifact that changes over time.

Here's what proper ground truth versioning looks like:

Version 1.0.0: Initial ground truth at launch
Version 1.1.0: Minor update adding criteria for new feature
Version 1.2.0: Minor update adjusting tone requirements based on user feedback
Version 2.0.0: Major update reflecting policy change that invalidates previous criteria

Each version should have:
- A changelog describing what changed and why
- A date range when this version was active
- Tags for which product version it corresponds to
- Owner who approved the change

Why does this matter? Because when you're debugging a production issue from three months ago, you need to know what ground truth was active then, not what it is now.

## The Backward Compatibility Problem

When you update ground truth, you face a backward compatibility question: what happens to old eval data labeled under the old criteria?

Option 1: Relabel everything under new criteria. This is correct but expensive. If you have 50,000 labeled examples and ground truth changes, do you relabel all 50,000?

Option 2: Keep old data with old labels but don't mix it with new data. Track which ground truth version each label corresponds to. Only compare performance within the same ground truth version.

Option 3: Identify which subset of old data is affected by the ground truth change and only relabel that subset.

Elite teams use Option 3. When ground truth changes, they:
- Identify which task types or response categories are affected
- Relabel only the affected subset
- Keep unaffected data as-is but tagged with ground truth version
- Maintain separate metrics for different ground truth versions

This balances correctness with practicality.

## The Communication Problem

When ground truth changes, everybody needs to know:

Engineers need to know because they're optimizing prompts and systems to meet ground truth.
Labelers need to know because they're applying ground truth to new examples.
Product managers need to know because they're making decisions based on ground truth metrics.
Leadership needs to know because ground truth changes might affect reported quality.

But here's what usually happens: someone updates the ground truth document, commits it to a repo, and nobody notices. Engineers keep optimizing for old criteria. Labelers keep applying old standards. Metrics become meaningless because different people are using different versions.

Set up ground truth change notifications. When ground truth updates:
- Announce it in team channels
- Send email to stakeholders
- Update labeling instructions
- Re-calibrate labelers with new examples
- Add a note to your metrics dashboard indicating when the standard changed

Don't let ground truth changes be invisible.

## The Quarterly Review Cadence

Even if none of the six triggers fire, review ground truth quarterly. Set a calendar reminder. Treat it like infrastructure maintenance.

In the quarterly review:

Check relevance: Are we still measuring what matters? Have business priorities shifted?

Check completeness: Are there new task types or scenarios we're not covering?

Check clarity: Are the criteria still clear, or have they gotten fuzzy through partial updates?

Check consistency: Do different sections of ground truth contradict each other after various updates?

Check performance: Are there criteria we consistently fail on that might be unrealistic?

This quarterly review catches drift before it becomes a crisis.

## The Stakeholder Alignment Problem

Ground truth changes require stakeholder agreement, just like initial definition.

When you update ground truth:
- Legal needs to approve safety criteria changes
- Product needs to approve usefulness criteria changes
- Domain experts need to approve correctness criteria changes
- Leadership needs to understand how metrics will be affected

Don't update ground truth unilaterally. You'll end up with an internally consistent document that doesn't reflect what stakeholders actually want.

## Real Example: Privacy Policy Change

Let me walk through a real ground truth update to make this concrete.

A company has a customer service bot. Ground truth says: "Responses must provide complete information from our knowledge base to fully answer the customer's question."

Then GDPR enforcement tightens. Legal says: "We can't proactively share certain customer data even if the customer asks for it. We need them to verify identity first."

This directly contradicts existing ground truth. "Complete information" now means "all information except identity-sensitive data, which requires escalation."

The update process:

1. Legal identifies the policy change and notifies the AI team
2. AI team updates ground truth to add identity-sensitive data handling criteria
3. They relabel a sample of existing eval data to check how many examples are affected (turns out 8% of examples involve identity-sensitive queries)
4. They relabel that 8% under new criteria
5. They notify labelers of the change with updated guidelines and new examples
6. They add a note to their metrics dashboard: "Ground truth updated on [date] for privacy compliance. Metrics before and after this date not directly comparable."
7. They version the ground truth document from 2.1 to 3.0 (major change)

That's what proper ground truth evolution looks like.

## The Metric Continuity Challenge

When ground truth changes, your metrics change. This creates a challenge: how do you track progress over time if the standard keeps shifting?

Bad approach: Pretend ground truth never changed and report continuous metrics. This gives you false trend lines.

Better approach: Clearly mark when ground truth changed and note that metrics before and after aren't directly comparable.

Best approach: Maintain "stable subset" metrics—a core set of criteria that rarely change—alongside "full" metrics that include evolving criteria. You can track progress on the stable subset over time while still measuring against complete current standards.

## The Regulatory Compliance Angle

In 2026, the EU AI Act requires documented quality standards for high-risk AI systems. Those standards must be maintained and updated.

If you're subject to the Act:
- Ground truth changes must be documented with justification
- You must maintain records of what standard was active when
- You must be able to demonstrate that updates improved or maintained safety
- You need audit trails showing who approved changes and when

This isn't optional. Treat ground truth updates with the same rigor you'd treat production code changes in a regulated environment.

## The Anti-Pattern: Informal Drift

Here's what happens when ground truth isn't maintained:

Month 1: Ground truth is documented and everyone follows it.
Month 2: An edge case comes up. Team discusses and agrees on how to handle it. Nobody updates the document.
Month 3: A labeler makes a judgment call on a new scenario. Others follow their lead. Nobody updates the document.
Month 4: A stakeholder casually mentions a preference in a meeting. Team starts optimizing for it. Nobody updates the document.
Month 6: The documented ground truth no longer reflects how anyone is actually evaluating quality. It's fiction.

This informal drift is poison. You think you have ground truth. You actually have tribal knowledge that varies by person and by day.

The fix: every time there's a conversation that clarifies or extends ground truth, update the document immediately. Make it a cultural practice.

## What Elite Teams Do Differently

Let me show you the practices that separate the top 1% from everyone else:

They version ground truth in the same repo as their code, with the same review process.

They have an owner responsible for ground truth maintenance, not just definition.

They tag ground truth version in all eval data, so they always know which standard was applied.

They have automated alerts when dependencies change—if a policy doc updates, the ground truth owner gets notified.

They run monthly "ground truth drift" reviews where they sample recent production cases and ask: do these meet our documented ground truth, or has reality diverged?

They maintain a ground truth changelog that's visible to the whole team and referenced in planning meetings.

They treat ground truth updates as product releases—with testing, stakeholder review, and careful rollout.

## The Living Document Mindset

The fundamental shift is mindset: ground truth isn't a fixed artifact you create once. It's a living document that evolves with your product.

This means:
- Building update processes, not just creation processes
- Assigning ongoing ownership, not just initial ownership
- Budgeting time for maintenance, not just development
- Accepting that metrics will shift as standards evolve
- Communicating changes, not just changes to code

If you're treating ground truth as "done" after initial definition, you're setting yourself up for silent degradation where your evaluation becomes less useful over time until you're essentially measuring nothing meaningful.

## The Review Checklist

Set a quarterly calendar reminder to review ground truth. Use this checklist:

- Have we shipped product changes that affect what "correct" means?
- Have business policies changed that affect what we should say or do?
- Have regulations changed that affect what we're allowed to say or do?
- Are users complaining about things we're marking as "correct"?
- Have we switched models or updated prompts in ways that change capabilities?
- Have competitive products improved in ways that raise user expectations?
- Does our ground truth still reflect stakeholder priorities?
- Are there new task types or use cases we haven't defined ground truth for?
- Are our criteria still clear and testable, or have they gotten vague?
- Can we still achieve our ground truth with our current system?

If you answer "yes" to any of these, update ground truth before the next eval cycle.

## The Documentation Standard

Your ground truth document should include:

- Version number and date
- Changelog for this version
- Criteria for each task type
- Examples showing what meets and doesn't meet criteria
- Edge case handling decisions
- Trade-off decisions (when criteria conflict, what takes priority)
- Update trigger list (what should prompt review of this document)
- Owner and stakeholders
- Last review date
- Next scheduled review date

This makes ground truth maintainable by someone other than the original author and survivable across team changes.

## The Bottom Line

Ground truth that sits unchanged for months is probably wrong. Products change. Policies change. Regulations change. User expectations change. Competitive landscapes change.

If your ground truth isn't changing, it's not because you got it perfect at launch. It's because you're not maintaining it. And unmaintained ground truth becomes a liability—a false standard that gives you false confidence.

Treat ground truth as a living document. Version it. Update it. Review it. Communicate changes. Track which version applies to which data. Budget time for maintenance.

That's the difference between evaluation that stays useful and evaluation that silently becomes meaningless.

In the next section, we'll address one of the most contentious organizational questions: who owns ground truth, and why that ownership structure matters more than most teams realize.
