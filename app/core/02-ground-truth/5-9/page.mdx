# 5.9 — Adjudication Workflow: Who Decides When Experts Disagree

Two pediatricians looked at the same AI response about fever management in toddlers. Both had fifteen-plus years of clinical experience. Both were board-certified. Both were careful, thoughtful evaluators.

One marked it as "excellent" — accurate information, appropriate tone, helpful guidance for parents.

The other marked it as "failing" — missing a critical safety consideration about when to seek immediate medical attention.

The team building the AI stared at the conflicting ratings. If two experienced pediatricians can't agree on whether a response is good or bad, how are they supposed to know what ground truth is?

This is the expert disagreement problem, and it's more common than you'd think.

Here's what teams get wrong: they assume expert disagreement means something is broken. Either the experts don't know what they're talking about, or the ground truth standards are unclear, or the example is just too ambiguous.

Sometimes that's true. But often, expert disagreement is data. It tells you something important about the complexity of the domain, the subjectivity of quality, or gaps in your standards.

The question isn't "how do we eliminate disagreement?" The question is "how do we resolve disagreement systematically and learn from it?"

## Why Experts Disagree (And Why That's Okay)

Let me walk you through why disagreement happens, even among highly qualified experts.

Reason one: legitimate professional disagreement. In many domains, there isn't one right answer. Two experienced doctors can have different clinical philosophies. Two lawyers can interpret the same case law differently. Two designers can have different aesthetic judgments.

This isn't a bug, it's reality. Your ground truth process needs to handle legitimate professional disagreement.

Reason two: different interpretation of standards. Your quality framework says responses should be "concise but complete." Two experts interpret that differently. One values conciseness and errs toward brevity. The other values completeness and errs toward thoroughness.

This suggests your standards need more clarity or examples, but it's not a fundamental problem.

Reason three: different risk tolerance. One expert is comfortable with a certain level of ambiguity or uncertainty. Another expert wants more caution.

In the fever example above, both pediatricians agreed on the medical facts. They disagreed on whether the safety consideration was critical enough to make the response failing. That's a risk tolerance difference.

Reason four: different context assumptions. One expert assumes the user is a first-time parent who needs more guidance. Another assumes the user is experienced and just needs quick facts.

Different context assumptions lead to different quality judgments.

Reason five: genuine edge case. Sometimes an example is genuinely borderline. It has elements of both good and bad. Reasonable experts will land on different sides.

This is valuable data — it tells you where the boundary is.

Reason six: evaluator error. Sometimes someone just missed something or misread the example. This happens. Humans aren't perfect.

Your adjudication process needs to handle all of these causes.

## The Independent Review Pattern

Here's the foundational pattern: independent review before discussion.

When you need multiple experts to evaluate something, have them evaluate it independently first. No discussion, no collaboration. Each expert looks at the example and makes their judgment.

Only after independent ratings are collected do you bring experts together to discuss disagreements.

Why independent first? Because you want to see genuine differences in perspective, not groupthink.

If experts discuss before rating, you get anchoring bias. The first person to speak sets the frame, and others adjust toward that opinion. You lose the diversity of perspective that makes multiple experts valuable.

Independent review surfaces true disagreements. Then discussion helps resolve them.

## The Disagreement Resolution Workflow

Here's the step-by-step process for handling expert disagreements.

Step one: independent evaluation. Each expert evaluates the example independently. For ground truth creation, two to three experts per example is usually right. More than five is diminishing returns unless the domain is extremely high-stakes.

Step two: identify disagreements. Automatically flag examples where experts gave different ratings. Focus on meaningful disagreements (one person said excellent, another said failing) more than minor variations (one person said good, another said very good).

Step three: structured disagreement discussion. Bring the disagreeing experts together (or do this asynchronously if that works better). Have each expert explain their reasoning.

"I rated this as excellent because the medical information is accurate, the tone is reassuring but not dismissive, and it gives parents actionable guidance."

"I rated this as failing because it doesn't mention that prolonged high fever in toddlers can be a sign of serious infection and should prompt immediate medical evaluation. A parent reading this might wait too long to seek care."

Step four: look for the root cause. Is this legitimate professional disagreement? Interpretation difference? Missing context? Edge case? Evaluator error?

Step five: resolve based on the root cause.

If it's legitimate professional disagreement, escalate to a senior expert or use a tie-break rule (more on this below).

If it's interpretation difference, clarify the standards and re-evaluate.

If it's missing context, add context to the example and re-evaluate.

If it's an edge case, this becomes a documented borderline case in your example library.

If it's evaluator error, the evaluator corrects their rating.

Step six: document the resolution. Capture the disagreement, the reasoning from both sides, and the final decision. This becomes part of your knowledge base.

Step seven: learn from patterns. If experts always disagree on a certain type of example, your standards for that type need work.

## Majority Vote vs Expert Override

Here's a question teams wrestle with: when experts disagree, do you go with majority vote or do you give one expert override authority?

The answer: it depends on the situation.

Use majority vote when:

- The disagreement is about subjective judgment (tone, helpfulness, user experience)
- You have three or more independent evaluators
- No single evaluator has significantly more expertise than the others
- The stakes are medium (errors are fixable)

Use expert override when:

- The disagreement is about factual accuracy or safety
- One evaluator has significantly more domain expertise
- The stakes are high (errors could cause harm)
- You're in a regulated domain where credentials matter

Example: for a medical AI, use majority vote for tone and helpfulness. Use expert override (defer to the most senior clinician) for clinical accuracy and safety.

Example: for a legal AI, use majority vote for clarity and user experience. Use expert override (defer to the lawyer with expertise in this specific area of law) for legal accuracy.

Document which dimensions use which approach so it's not arbitrary.

## The Gold Annotator Concept

Some teams designate a "gold annotator" — a person whose judgment is trusted to be the tiebreaker or gold standard.

This person is typically:

- The most experienced in the domain
- The one who's been most involved in creating the ground truth standards
- The one with the best calibration (their judgments align most consistently with team consensus)

The gold annotator's role:

Break ties when experts are split.

Review and approve borderline cases.

Calibrate other evaluators (new evaluators compare their ratings to the gold annotator to learn).

Quality check: periodically review a sample of ground truth to ensure quality.

The gold annotator isn't infallible, and their judgments shouldn't be blindly accepted. But having someone in this role speeds up resolution and ensures consistency.

As you scale, you might have multiple gold annotators for different domains or product areas.

## Tracking Disagreement Patterns

Here's something crucial: track where disagreements happen, because patterns in disagreement tell you where your ground truth needs work.

Set up a simple tracking system:

For each disagreement, log:
- What was the example?
- What dimension did experts disagree on?
- What was the disagreement? (One said X, another said Y)
- What was the root cause?
- How was it resolved?

Monthly or quarterly, review the log and look for patterns:

"We have fifteen disagreements about response length in the past month. Our 'concise but complete' standard is too vague. We need better definition or examples."

"Experts always disagree on medical responses for pediatric cases but rarely for adult cases. We need pediatric-specific standards."

"We have consistent disagreement between our two legal reviewers about disclaimers. They have different interpretations of the legal requirements. We need to get them aligned."

Patterns in disagreement are signals. Listen to them.

## Calibration Sessions to Maintain Consistency

Even with clear standards, evaluators will drift over time. Someone who was well-calibrated six months ago might be applying standards differently now.

Run regular calibration sessions to maintain consistency.

Format: monthly or quarterly, bring all evaluators together. Everyone independently rates ten to twenty examples, then discusses the ratings.

Focus on disagreements. "Why did you rate this as good when others rated it as failing?"

Recalibrate understanding. "Remember, our standard for helpfulness is that the response should address the user's likely intent, not just their literal question."

These sessions serve multiple purposes:

- Keep evaluators consistent with each other
- Keep everyone updated on evolving standards
- Identify areas where standards need clarification
- Build shared judgment and trust

Calibration is ongoing, not one-time.

## When Disagreement Means Your Standards Need Work

Sometimes the disagreement isn't about the example, it's about the standards.

If experts consistently disagree on the same types of examples, your standards are probably too vague or incomplete.

"Responses should be appropriately detailed" is too vague. Appropriately detailed for whom? In what context?

When you see repeated disagreement, stop and fix the standards.

Run a focused alignment workshop on that specific area. Bring examples where disagreement happened. Discuss until you have clearer standards. Document them.

Then re-evaluate the disputed examples with the new standards and see if disagreement drops.

Sometimes you'll discover that the disagreement can't be resolved because it's legitimately subjective. That's fine — document it as a borderline area where judgment calls are expected.

## The Adjudication Rubric

Here's a rubric to help decide how to resolve disagreements:

Is this about facts or judgment?

If facts: the correct answer can be verified. Defer to the expert who's right, or do research to determine the correct answer.

If judgment: there's no single right answer. Use majority vote or escalation.

Is this a safety or compliance dimension?

If yes: err on the side of the more conservative judgment. It's better to be overly cautious than to miss a safety issue.

If no: use majority vote or expert override based on expertise.

Is this a pattern or a one-off?

If pattern: fix the underlying standard, don't just resolve this one disagreement.

If one-off: resolve and move on.

How high are the stakes?

If high-stakes (medical, legal, financial, safety): escalate to the most senior expert.

If medium-stakes: use majority vote.

If low-stakes: designate one expert as decision-maker and move on.

This rubric gives you a framework instead of ad-hoc decisions.

## Handling Persistent Disagreement Between Two Experts

What do you do when two experts consistently disagree and neither is clearly wrong?

This happens. Two people with different philosophies, different training, different risk tolerances.

Option one: accept that this dimension is subjective and use majority vote with a third expert as tiebreaker.

Option two: define different standards for different contexts. "For high-risk medical advice, use Expert A's conservative approach. For general health information, use Expert B's more permissive approach."

Option three: escalate to a senior expert who can synthesize both perspectives.

Option four: document it as an area of legitimate disagreement and be transparent that different experts would judge differently.

Don't force artificial consensus. Sometimes the right answer is "this is a judgment call and reasonable experts disagree."

## Asynchronous Adjudication

Not every disagreement requires a synchronous meeting. For many cases, asynchronous adjudication works fine.

Expert A and Expert B rate differently. The system flags it. Both experts are asked to write a brief explanation of their reasoning.

A third party (gold annotator, steward, or another expert) reviews both explanations and makes a decision.

The decision and reasoning are documented.

Everyone is notified of the outcome.

This is faster and less disruptive than scheduling meetings for every disagreement.

Reserve synchronous discussion for complex, high-stakes, or pattern disagreements.

## The Teach-Back Method

When you resolve a disagreement, use it as a teaching opportunity.

"We resolved this disagreement by clarifying that safety warnings should be included for any condition that could escalate quickly. Here's why we made that call and here's how to apply it to future examples."

Share the resolution with all evaluators, not just the ones who disagreed. This is how everyone learns.

Over time, disagreements become less frequent because everyone's working from the same learned context.

## What Good Adjudication Looks Like

Let me show you a team that handled this well.

They built a financial advice AI. Three financial advisors evaluated ground truth examples. Disagreements were common initially — about forty percent of examples had at least one disagreement.

They implemented:

Independent evaluation: each advisor rated examples without seeing others' ratings.

Structured discussion: weekly meetings to discuss disagreements.

Pattern tracking: logged every disagreement and reviewed monthly for patterns.

Standard refinement: when patterns emerged, they ran focused sessions to clarify standards.

Gold annotator: the most senior advisor served as tiebreaker for unresolvable disagreements.

Calibration: monthly sessions where all three advisors rated the same examples and discussed differences.

After six months, disagreement rate dropped to fifteen percent. The remaining disagreements were mostly legitimate edge cases.

After a year, they onboarded a fourth advisor. That person calibrated within three weeks using the documentation and calibration sessions.

The adjudication process wasn't just about resolving disagreements — it was about building shared understanding and institutional knowledge.

## What's Next

You now know how to handle expert disagreements systematically. But all the process in the world doesn't help if you don't have clear tie-break rules for when there's genuinely no consensus.

In the next and final subchapter of this chapter, we're covering tie-break rules: when safety wins, when policy wins, when legal wins. The hierarchy that ensures you can make decisions even when there's no perfect answer.

Good adjudication resolves most disagreements. Good tie-break rules resolve the ones that can't be adjudicated. Together, they prevent decision paralysis and keep ground truth work moving forward. Let's talk about how to define tie-break rules that reflect your values and priorities.
