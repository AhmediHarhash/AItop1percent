# 7.4 — When Models Get Better Than Your Ground Truth

Let me tell you about the most confusing week I ever had as an AI engineer.

We upgraded our customer support system to a newer model. We ran our standard evaluation suite — the same tests we always ran, against the same ground truth we'd carefully crafted over months. The results came back: 87 percent accuracy, down from 91 percent on the old model.

This made no sense. The new model was supposed to be better. It was more capable in every published benchmark. Users in our beta test loved it. But our evaluations said it was worse.

We spent days debugging. Were we using it wrong? Was there a prompt regression? A configuration error? We checked everything. The model was working fine. It just disagreed with our ground truth.

Finally, someone suggested: "What if we just read the responses?" We pulled up examples where the new model "failed" our ground truth. And we immediately saw the problem.

The new model was giving better answers than our ground truth said were correct.

Our ground truth said a good response to "How do I reset my password?" should provide step-by-step instructions. The old model did exactly that. The new model did something smarter: it noticed the user was on mobile based on context and provided mobile-specific instructions, then offered to send a reset link directly. This was objectively better, but our ground truth marked it wrong because it didn't match our reference answer.

We weren't measuring model quality. We were measuring model conformity to outdated standards. The model had surpassed our ground truth, and we'd almost rejected it because our evaluation system couldn't recognize excellence.

## The Ground Truth Ceiling

Think of your ground truth as a target. Your AI tries to hit that target. As you improve your AI, it gets closer and closer to the bullseye.

But what happens when your AI gets so good that it exceeds what your target defines as "perfect"? It can generate answers better than your reference examples. It can satisfy unstated user needs that your rubric doesn't measure. It can solve problems in ways your ground truth didn't anticipate.

At that point, your ground truth becomes a ceiling instead of a target. It limits what you recognize as good, which limits what you optimize for, which limits what you achieve.

This happens more often than you'd think, especially in 2026 when model capabilities are advancing rapidly. The ground truth you created six months ago might have been ambitious then but limiting now.

## How to Detect It

The tricky part is that your evaluation metrics won't tell you when this is happening. In fact, they'll tell you the opposite — they'll say your better model is worse.

Here are the signs to watch for:

### The Benchmark Mismatch

Your model performs better on external benchmarks but worse on your internal evaluations. External benchmarks are measuring general capability. Your internal evaluations are measuring conformity to your specific ground truth.

If those diverge, suspect that your ground truth might be the limiting factor.

### The User Preference Flip

You A/B test two models. Users prefer model B. Your evaluations say model A is better.

Trust the users. They're telling you that your ground truth doesn't capture what they actually value.

### The Expert Override Pattern

You have subject matter experts review outputs. They consistently disagree with your automated evaluations, preferring responses that your ground truth rates poorly.

The experts are recognizing quality dimensions that your ground truth doesn't capture.

### The Improvement Paradox

You make changes that should obviously improve your system — better models, refined prompts, enhanced context — and your evaluation scores go down or stay flat.

Either you made a mistake (possible) or your ground truth can't recognize the improvement (also possible).

## The Re-Calibration Process

When you suspect your model has exceeded your ground truth, here's how to investigate and respond.

### Step 1: Sample and Compare

Pull 50-100 examples where the new model differs from the old model. Don't pre-filter for cases where the new model "failed" — just cases where it gave different answers.

For each example, look at:

The old model's response.

The new model's response.

What your ground truth says is correct.

Which response is actually better for the user.

You're looking for patterns. Is the new model consistently doing something your ground truth doesn't account for? Is it finding better solutions that your ground truth doesn't recognize?

### Step 2: Blind Expert Review

Take the same examples and have domain experts review them blind. Don't tell them which response came from which model or what your ground truth says.

Just ask: "Which response is better, and why?"

If experts consistently prefer the new model's responses even when your ground truth doesn't, you've confirmed the problem. Your ground truth is the bottleneck.

### Step 3: Analyze the Gaps

Look at the cases where the new model is better but your ground truth doesn't recognize it. What's missing from your ground truth?

Common gaps:

Context awareness — the model adapts to user context in ways your ground truth doesn't measure.

Unstated needs — the model satisfies implicit user needs that your ground truth doesn't specify.

Creative solutions — the model finds novel approaches that your ground truth's example-based evaluation doesn't accommodate.

Proactive helpfulness — the model anticipates next questions or offers additional value that your ground truth doesn't require.

Quality dimensions — the model optimizes for aspects of quality (clarity, tone, completeness) that your ground truth under-weights.

### Step 4: Update Your Ground Truth

Once you've identified what's missing, update your ground truth to capture it.

This might mean:

Adding new criteria to your rubric (context adaptation, proactiveness).

Updating reference answers to reflect better solutions.

Changing from strict answer matching to criteria-based evaluation.

Raising the bar for what counts as "excellent."

Importantly, this is a major version change. You're fundamentally changing what you measure. Version it appropriately.

### Step 5: Re-evaluate Under New Standards

Take your evaluation data from both models and re-evaluate under the updated ground truth.

Now you should see the new model score higher, because you're measuring dimensions of quality it actually excels at.

This re-evaluation also validates that your ground truth update was correct. If the new model still scores lower under updated ground truth, you might have misdiagnosed the problem.

## The Progressive Ground Truth Pattern

Smart teams don't wait for models to exceed their ground truth. They build ground truth that grows with model capability.

Here's how this works:

### Tiered Standards

Instead of one definition of "good," have multiple tiers: acceptable, good, excellent, exceptional.

Early models aim for "acceptable." As capabilities improve, you raise expectations. You might accept "good" today but require "excellent" after the next model upgrade.

This prevents your ground truth from becoming a ceiling because the ceiling is already built in multiple levels.

### Aspirational Examples

Include examples in your ground truth that current models can't quite achieve yet. Mark them as aspirational or stretch goals.

This does two things. First, it gives you a target to optimize toward as models improve. Second, it helps you recognize when a model has reached that tier.

When your new model starts achieving aspirational examples, you know it's exceeded your baseline ground truth.

### Capability-Aware Rubrics

Design rubrics that check whether the model used capabilities it has available, not just whether it matched a specific answer.

Instead of: "Did the response include step-by-step instructions?"

Use: "Did the response use available context to personalize instructions, or default to generic instructions when personalization was possible?"

The first approach rewards any model that follows instructions. The second approach rewards models that use advanced capabilities.

## Backward-Compatible Metrics

Here's the challenge: you need to update ground truth to recognize new model capabilities, but you also need to compare new models to old models fairly.

If you change what you measure, historical comparisons become apples-to-oranges.

The solution is maintaining backward-compatible metrics alongside new metrics.

### The Core + Extensions Pattern

Define a core set of evaluation criteria that stays stable over time. These measure fundamental requirements that all model generations should meet.

Then define extension criteria that capture capabilities of newer models. These are additive — they don't replace core criteria, they supplement them.

When you evaluate a new model:

Measure it on core criteria and compare to historical baselines.

Measure it on extension criteria to understand new capabilities.

Report both sets of metrics.

This lets you say: "The new model matches the old model on core quality (91 percent vs 91 percent) and exceeds it on advanced capabilities (78 percent vs N/A because old model couldn't do this)."

### Normalized vs Absolute Scoring

Another approach: report both normalized scores (relative to what's possible for that model generation) and absolute scores (against an aspirational standard).

An older model might score 85 percent normalized (very good for its generation) but 60 percent absolute (there's still a lot of room to improve).

A newer model might score 88 percent normalized (very good for its generation) and 72 percent absolute (closer to the aspirational standard).

This shows progress on absolute quality while still recognizing that different model generations have different ceilings.

## When NOT to Update Ground Truth

Not every case of model-ground truth divergence means you should update your ground truth. Sometimes the model is actually wrong, even if it seems sophisticated.

### The Overfitting Red Flag

The model produces verbose, elaborate responses that look impressive but don't actually help users. Your ground truth correctly prefers concise, direct answers.

Don't update your ground truth to reward verbosity just because the new model is verbose.

### The Hallucination Trap

The model generates creative, confident answers that go beyond available information. Your ground truth correctly requires staying within known facts.

Don't update your ground truth to accept hallucinations just because they sound good.

### The Capability Mirage

The model seems to use advanced reasoning, but when you dig deeper, it's just pattern matching in sophisticated ways. Your ground truth correctly distinguishes real capability from appearance of capability.

Don't lower your standards for actual understanding just because surface behavior improved.

### The Premature Optimization

The model optimizes for a metric that's easy to measure but not what users actually want. Your ground truth captures the harder-to-measure thing that actually matters.

Don't update your ground truth to measure what's easy instead of what's right.

## The Update Decision Framework

When you're deciding whether to update ground truth based on new model capabilities, ask these questions:

"Is the new behavior better for users?" If yes, consider updating. If no, don't.

"Does the new behavior satisfy a need that our ground truth should have captured but didn't?" If yes, update. If the need was never part of the goal, don't add it retroactively.

"Would we want future models to continue doing what the new model does?" If yes, encode it in ground truth. If it's a quirk we'd rather eliminate, don't.

"Can we objectively evaluate the new behavior, or are we just impressed by novelty?" If you can evaluate it objectively, update ground truth to include criteria for it. If it's just shiny, wait and see if it has lasting value.

## Communicating Ground Truth Updates

When you update ground truth because models exceeded it, communicate this carefully to stakeholders.

Don't say: "Our evaluation was wrong." That undermines trust in your evaluation process.

Do say: "Our ground truth was designed for model capabilities as of last year. The new model has capabilities we didn't anticipate. We're updating our ground truth to measure these new capabilities while maintaining our core quality standards."

Frame it as evolution, not error.

And be specific about what changed. Show examples of the new behaviors you're now recognizing. Demonstrate that the update is grounded in user value, not just model capability.

## The Continuous Improvement Loop

The healthiest teams treat this as an ongoing loop:

Models improve → we discover gaps in ground truth → we update ground truth to capture what the better models can do → we use that ground truth to push models even further → models improve more.

This loop means your ground truth is never "done." It's always evolving alongside your systems.

But evolution is different from drift. Drift is passive, unintentional degradation. Evolution is active, intentional improvement.

When you update ground truth because models exceeded it, you're not fixing a mistake. You're raising the bar. That's the sign of a mature evaluation practice.

## The Meta-Question

Here's what this whole chapter is really about: how do you measure quality in a domain where the frontier of what's possible keeps moving?

If your ground truth is static, it becomes obsolete. If it's too reactive, it loses meaning.

The answer is ground truth that's rooted in user value — that's your stable foundation — but open to recognizing new ways to deliver that value as capabilities expand.

You're not changing what "good" means. You're recognizing more ways to achieve it.

In the next section, we'll talk about how production feedback — what real users do with your AI in the wild — should flow back into ground truth updates, creating a loop between lab standards and real-world performance.
