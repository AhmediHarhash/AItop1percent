# 1.3 — Why "It Looks Good" Isn't Good Enough

Three words have destroyed more AI products than any technical failure: "It looks good."

Let me tell you about a team that learned this lesson the expensive way.

They were building an AI writing assistant for a sales team at a B2B software company. The tool would draft personalized email outreach based on prospect information. They spent four months building it. Every demo went great. The VP of Sales loved it. The CEO showed it to investors. The sales team was excited.

They rolled it out to 50 salespeople on a Monday. By Wednesday, usage had dropped to nearly zero. The feedback was brutal: "It sounds like a robot." "This would never work for my accounts." "It's too generic." "It doesn't understand our value prop."

The product team was stunned. It had looked so good in demos. What changed?

In demos, they'd shown cherry-picked examples with prospects they'd carefully selected. They'd demonstrated the tool to executives who hadn't sent a cold email in a decade. They'd optimized for wows in conference rooms, not for utility in real sales workflows.

"It looks good" had been their evaluation standard. It wasn't good enough.

## The Demo-Production Gap

Let me walk you through why demos lie.

In a demo, you control everything. You pick the inputs. You know what the system can handle. You avoid edge cases. You show the happy path. If something breaks, you smoothly move to a backup example. The audience doesn't know what's happening behind the curtain.

In production, users control everything. They type weird inputs. They ask questions you didn't anticipate. They hit edge cases constantly. They expect it to work every time. When it breaks, they just stop using it.

The gap between these two environments is where "it looks good" falls apart.

Here's what happens in a typical demo of an AI product:

You show a customer support bot answering a question about return policies. The question is clear and well-formed. The answer is accurate and helpful. Everyone nods. "Wow, that looks great!"

Here's what happens in production with the same bot:

"hi can i return something i bought last month but i lost the receipt and also it was a gift so idk if thats different and also its been opened is that ok???"

The real question is poorly formatted, combines multiple scenarios, contains ambiguity, and requires the bot to handle uncertainty. The demo example prepared you for none of this.

"It looks good" in the demo tells you nothing about production performance.

## The Vibes-Based Evaluation Problem

When teams rely on "it looks good," they're doing what I call vibes-based evaluation. They're judging quality based on subjective feelings in the moment rather than systematic measurement.

Vibes-based evaluation happens when:

- You read through a few examples and decide if they "feel right"
- You demo to stakeholders and gauge their reactions
- You spot-check outputs and use your gut to judge quality
- You ask team members "what do you think?" and average their opinions

None of this is systematic. None of it scales. None of it catches the problems that will destroy you in production.

Let me show you why vibes-based evaluation fails:

First, it's inconsistent. Ask the same person to evaluate the same output on Monday and Friday. You'll get different answers based on their mood, how much coffee they've had, and what else is on their mind. Ask five people to evaluate the same output. You'll get five different opinions based on their backgrounds, preferences, and interpretations.

Second, it's biased toward novelty. When you first see an AI-generated response, it feels impressive. The tenth time you see a similar response, you start noticing flaws. The hundredth time, it feels generic. Your evaluation changes based on exposure, not based on actual quality.

Third, it's biased toward the happy path. In demos and spot checks, you naturally gravitate toward cases where the system works well. You don't systematically seek out failures. You don't test edge cases unless something specifically prompts you to. You end up with a skewed view of overall quality.

Fourth, it doesn't measure what matters. "It looks good" measures aesthetic appeal and surface plausibility. It doesn't measure accuracy, safety, policy compliance, user satisfaction, or business impact. You're optimizing for the wrong thing.

## The Scale Problem

Even if vibes-based evaluation worked for individual examples, it completely breaks at scale.

You can read through 50 examples and form an opinion. Maybe even 200 if you're dedicated. You cannot read through 10,000 examples and maintain consistent judgment. Your standards will drift. You'll get fatigued. You'll start skimming. Your evaluation will become meaningless.

I watched a team try to launch a content moderation system using vibes-based evaluation. They had three people manually reviewing flagged content to decide if the AI's moderation decisions were correct. They could review about 500 cases per day.

Their system was seeing 50,000 cases per day. They were evaluating 1% of actual traffic. They had no idea how the system performed on the other 99%. They were flying blind.

When they finally built a systematic evaluation framework based on explicit ground truth criteria, they discovered their precision was 40% lower than they'd estimated. The 1% they were manually reviewing wasn't representative. It was accidentally biased toward cases where the AI was more confident, which happened to be easier cases.

Vibes don't scale. Measurements scale.

## The Demographic Blindness Problem

Here's a failure mode that "it looks good" evaluation misses completely: demographic performance gaps.

Your product might work great for users like you and terribly for users unlike you. If you're evaluating based on vibes, you'll never notice because you're judging from your own perspective.

A team built a voice transcription system for medical consultations. It looked great in demos with doctors who spoke standard American English. It was 90% accurate.

They shipped it. Complaints poured in. It was terrible at understanding doctors with accents, doctors who spoke quickly, doctors who used certain regional medical terms. For some users, accuracy dropped to 60%.

Why didn't they catch this? Because the team doing vibes-based evaluation all spoke standard American English. "It looks good" to them meant something completely different than "it looks good" to doctors with Indian, Nigerian, or Scottish accents.

Systematic evaluation based on ground truth would have required testing across demographic groups. Vibes-based evaluation never even surfaced the question.

## The Edge Case Invisibility Problem

Edge cases destroy products. "It looks good" evaluation focuses on common cases and misses edges completely.

Let me give you a concrete example. A team built an AI calendar assistant that schedules meetings based on natural language requests. In demos, it worked perfectly:

"Schedule a meeting with Sarah next Tuesday at 2pm." The system schedules it. Looks good.

In production, users said things like:

- "Find time for the three of us sometime in the next two weeks, preferably mornings, but Jane can't do Mondays."
- "Move my 3pm to tomorrow unless I have a conflict, in which case just cancel it."
- "Schedule a recurring biweekly meeting with the product team, alternating between our office and theirs."

These edge cases broke the system constantly. But they never appeared in demos because demos focused on simple, clean examples that looked good.

If you skip systematic edge case testing, here's what happens: you ship. Users hit edge cases. The system fails. Users lose trust. Usage drops. You're now in reactive damage control mode instead of proactive quality improvement mode.

## The Measurement Substitute

When I say "it looks good isn't good enough," teams often push back: "But we do need human judgment at some point. Not everything is measurable."

That's true. But there's a huge difference between "using human judgment within a systematic framework" and "relying on vibes."

Systematic evaluation with human judgment looks like:

- Defining explicit ground truth criteria
- Sampling systematically (not cherry-picking examples)
- Having multiple evaluators judge the same examples to measure agreement
- Tracking evaluator consistency over time
- Aggregating judgments into metrics
- Stratifying analysis across user groups, edge cases, and time periods

Vibes-based evaluation looks like:

- Reading some examples
- Asking "does this feel right?"
- Moving on when it feels good enough

The first approach uses human judgment as input to a rigorous measurement system. The second approach uses human judgment as a substitute for measurement.

## What Actually Replaces Opinions

Ground truth replaces opinions with measurements. Instead of asking "does this look good?", you ask "does this meet our defined criteria?"

For the sales email assistant, defined ground truth might be:

"An email is good if it: (1) personalizes based on specific information about the prospect's company, (2) references a real pain point relevant to their industry, (3) proposes a specific next step, (4) matches our brand voice guidelines, and (5) avoids generic marketing language from our templates."

Now when you evaluate an email, you're not asking for vibes. You're checking:

- Did it personalize? Yes or no.
- Did it reference a real pain point? Yes or no.
- Did it propose a next step? Yes or no.
- Does it match brand voice? Yes or no.
- Does it avoid generic language? Yes or no.

Five objective questions. You can measure agreement between evaluators. You can track performance over time. You can identify which criteria the system struggles with. You can improve systematically.

That's what replacing opinions with measurements looks like.

## The Speed Trap

Teams sometimes defend vibes-based evaluation by saying: "We need to move fast. We don't have time for rigorous evaluation."

This is backwards. Vibes-based evaluation is slower in total time, even though it feels faster in the moment.

When you rely on vibes:

- You ship based on a hunch
- Users report problems you didn't anticipate
- You investigate reactively
- You make changes based on new hunches
- You ship again
- Users report different problems
- The cycle repeats

You're fast at each individual step but slow overall because you're going in circles.

When you use systematic evaluation:

- You define ground truth (this takes time upfront)
- You evaluate rigorously before shipping
- You catch problems before users see them
- You fix systematically based on measurements
- You ship with confidence
- Users encounter far fewer problems
- You iterate based on data, not guesses

You're slower at the start but dramatically faster overall because you're moving in a straight line.

The teams that move fastest long-term are the ones that define ground truth and measure systematically from day one.

## The False Confidence Problem

Here's one of the most dangerous aspects of "it looks good" evaluation: it creates false confidence.

When you demo your product and everyone says "wow, this is great," you feel confident shipping it. That confidence is based on a tiny, unrepresentative sample. It's false.

I've seen teams ship products with 90% subjective confidence that turned out to have 40% actual quality. The gap between perceived quality and real quality is where disasters happen.

Systematic evaluation based on ground truth gives you calibrated confidence. You know what you measured. You know how you measured it. You know what you didn't measure. You can make informed decisions about what's safe to ship.

Vibes give you uncalibrated confidence. You feel good, but you don't actually know what quality level you've achieved.

## What Elite Teams Do Instead

Let me show you what good looks like.

An elite team building an AI product defines ground truth first. They write down explicit criteria. They create a structured evaluation framework.

Then they evaluate systematically:

- They create a test set that covers common cases, edge cases, and adversarial cases
- They have multiple people evaluate the same examples to measure inter-rater reliability
- They track which ground truth criteria are met and which are violated
- They stratify results by user demographic, use case, and input characteristics
- They identify specific failure modes rather than just overall scores
- They use these measurements to guide improvement

When they demo, they show real metrics based on systematic evaluation, not cherry-picked examples. "Our system meets all five ground truth criteria on 87% of common cases and 72% of edge cases. Here's the breakdown by criteria."

That's a demo backed by measurement, not vibes.

## The Transition

If you're currently relying on "it looks good" evaluation, here's how to transition:

First, stop demoing cherry-picked examples. Start demoing your systematic evaluation results, even if they're not impressive yet. Honesty builds better products than optimism.

Second, define ground truth explicitly. Write down what "good" actually means. Get agreement from stakeholders. Make it testable.

Third, build an evaluation set that represents real production diversity. Include edge cases. Include different user types. Include ambiguous scenarios.

Fourth, evaluate systematically against your ground truth. Track metrics. Identify failure modes. Stop relying on gut feelings.

Fifth, use vibes only as a final sanity check, not as your primary evaluation. After systematic evaluation shows quality is high, then ask "does this feel right?" as a safety check. Not the other way around.

## When Vibes Actually Matter

I don't want to suggest that subjective judgment never matters. It does. But it matters in specific, limited ways:

Vibes are useful for identifying gaps in your ground truth definition. If systematic evaluation says quality is high but it still feels wrong, maybe your ground truth is missing something important. Investigate that feeling and formalize it.

Vibes are useful for aesthetic judgment on dimensions that are hard to formalize. "Does this sound natural?" is partially subjective. But even there, you can systematize it by getting multiple judges and measuring agreement.

Vibes are useful for sanity-checking that your metrics aren't gameable. If you hit your target numbers but the output feels off, maybe you're measuring the wrong thing.

But vibes should never be your primary evaluation method. They should be a supplement to systematic measurement, not a replacement for it.

## The Production Mirror

Here's the test for whether your evaluation is good enough: does it predict production performance?

If you evaluate in demos and say "it looks good," then ship and discover it's actually bad, your evaluation failed. It wasn't predictive.

If you evaluate systematically against ground truth and measure 85% quality, then ship and see 83% quality in production, your evaluation succeeded. It was predictive.

The gap between evaluation quality and production quality tells you if your evaluation method works. "It looks good" usually has a massive gap. Systematic ground truth evaluation has a small gap.

Your goal is to make evaluation a mirror of production, not a window into your hopes.

## The Warning Sign

If you're on a team that relies heavily on "it looks good" evaluation, here are the warning signs that you're headed for trouble:

- Demos always go great, but production performance is inconsistent
- Different team members have very different opinions on quality
- You can't articulate what makes a good output good
- You're surprised by user complaints
- You ship frequently but quality doesn't improve systematically
- You have metrics, but nobody trusts them
- Debates about quality are resolved by seniority, not data

If you're seeing these signs, you need to move from vibes to measurements. Define ground truth. Build systematic evaluation. Make decisions based on data instead of opinions.

## The Bottom Line

"It looks good" is the most dangerous phrase in AI product development because it feels sufficient when it's actually empty.

It feels like you've evaluated quality. You haven't. You've expressed a subjective opinion about a non-representative sample.

It feels like you're ready to ship. You're not. You have no idea how the system performs at scale, across demographics, or on edge cases.

It feels like progress. It's not. You're building on sand.

Replace "it looks good" with "it meets our ground truth definition on X% of cases, here's the breakdown." Replace vibes with measurements. Replace opinions with criteria.

That's the difference between products that survive production and products that collapse under real-world load.

In the next section, we'll break down what those ground truth criteria should actually include. Most teams only define one dimension of quality—correctness—and miss two others that are just as important: safety and usefulness. Understanding all three is essential to building evaluation that works.
