# 3.6 — Confidence Thresholds & Uncertainty Disclosure

There's a doctor I know who has a remarkable habit. When I ask her a question about health, she sometimes says "I'm quite confident this is X," and other times she says "This could be X or Y or Z — here's why I'm not sure." She never says she's certain when she's not. She never hides uncertainty behind medical jargon. She calibrates her language to match her actual confidence.

Most doctors don't do this. They present every opinion with equal certainty because they worry that expressing doubt will undermine trust. But research shows the opposite: patients trust physicians more when they're honest about uncertainty, not less.

Your AI system faces the same choice. It can present every response with the same confident tone, or it can calibrate its language to match its actual confidence. The first approach feels safer but creates problems. The second approach feels risky but builds trust.

Let me walk you through how to set confidence thresholds, express uncertainty naturally, and build a system that users trust precisely because it's honest about what it knows and doesn't know.

## The Confident Wrong Answer Problem

Here's the pattern that destroys user trust:

User: "When did the company announce the Q4 results?"
System: "The Q4 results were announced on March 15th."
User: (Checks) "That's wrong. It was March 22nd."

The problem isn't just that the system got it wrong. It's that the system said it *confidently*. There was no "I believe" or "Based on my information" or "I'm not certain but." Just a flat, confident statement that turned out to be false.

Now the user has a problem: How do they know when to trust the system? If it's equally confident when it's right and when it's wrong, confidence means nothing.

Compare this:

User: "When did the company announce the Q4 results?"
System: "Based on my records, I see March 15th, but I'm not fully certain of that date. Would you like me to check the official announcement page to verify?"

If this is wrong, the user isn't shocked. The system signaled uncertainty. The user knows to verify. Trust is preserved.

## The Three Confidence Tiers

Every response your system gives falls into one of three confidence tiers. Your job is to define the boundaries between them and how to express each.

**Tier 1: High Confidence (>90% certain)**

You're quite sure this is correct. The information is clear, sources are authoritative, context supports it, no contradictory signals.

Expression: Direct, confident language.
- "The Q4 results were announced on March 22nd."
- "Your appointment is scheduled for Tuesday at 3pm."
- "That order shipped yesterday via FedEx."

No hedging needed. State it clearly.

**Tier 2: Medium Confidence (70-90% certain)**

You believe this is correct, but there's meaningful uncertainty. Information is incomplete, sources are mixed, context is partially ambiguous.

Expression: Caveat language that signals uncertainty without undermining usefulness.
- "Based on available records, the Q4 results were announced on March 22nd, though I'd recommend checking the investor relations page to confirm."
- "It looks like your appointment is Tuesday at 3pm, but let me verify that with you since I'm seeing a recent schedule change."
- "According to the tracking info I have, that order shipped yesterday, but delivery estimates can vary."

The information is still useful, but the user knows to apply some verification.

**Tier 3: Low Confidence (<70% certain)**

You're not confident enough to give a direct answer. Information is contradictory, sources are weak, context is unclear, or this is outside your knowledge area.

Expression: Clear disclosure of uncertainty, with path to better information.
- "I don't have reliable information about when Q4 results were announced. You can find official announcements on the investor relations page at [link]."
- "I see conflicting information about your appointment time. Let me connect you with scheduling to confirm."
- "I don't have tracking information for that order. Let me pull up your order history to check status."

Don't guess. Don't hedge a guess. Admit you don't know and point toward a solution.

## Setting Confidence Thresholds by Risk

Not every question needs the same confidence level to answer. The threshold should scale with risk.

**Low-Risk Questions: Answer at 70%+ Confidence**

Questions where being wrong is low-consequence:
- "What's a good restaurant in this neighborhood?"
- "What are common ways to format a business email?"
- "What's the typical weather in June?"

Being somewhat wrong here is annoying but not costly. You can afford a lower threshold.

Response at 75% confidence: "The Blue Door is popular in that neighborhood for Italian food. Would you like other suggestions too?"

**Medium-Risk Questions: Answer at 85%+ Confidence**

Questions where being wrong has meaningful consequences:
- "What's the return policy?"
- "What does this contract clause mean?"
- "How do I configure this security setting?"

Being wrong here creates friction, wasted time, possibly financial cost. Raise the threshold.

Response at 80% confidence: "The return policy appears to be 30 days for most items, but let me verify that for your specific purchase since some categories have different terms."

**High-Risk Questions: Answer at 95%+ Confidence**

Questions where being wrong could cause harm, legal issues, or safety problems:
- "What's the dosage for this medication?"
- "Is this investment FDIC insured?"
- "What should I do in a fire emergency?"

Being wrong here is dangerous. Either be very confident or don't answer.

Response at 85% confidence (not high enough): "I'm not confident enough to advise on medication dosage — that's something you should verify with your pharmacist or the medication guide. I can help you contact them if you'd like."

## The Calibration Problem

Here's the tricky part: your AI model outputs a confidence score, but that score is often poorly calibrated.

The model says it's 85% confident, but in reality, it's only correct 60% of the time at that confidence level. Or it says 70% confident, but it's actually correct 90% of the time.

**Calibration means:** If your system says it's 80% confident 100 times, it should be correct about 80 of those times.

Most models are overconfident (especially on edge cases) or underconfident (especially on familiar patterns).

**How to calibrate:**

1. Collect predictions with confidence scores
2. Evaluate correctness for each prediction
3. Group by confidence bands (70-75%, 75-80%, etc.)
4. Calculate actual accuracy in each band
5. Build a calibration curve
6. Adjust thresholds or rescale confidence scores

Example:
- Model outputs 80-85% confidence on 100 predictions
- Actual correctness: 65%
- Conclusion: Model is overconfident in this range
- Action: Either rescale 80-85% model confidence to 60-70% system confidence, or raise the threshold for confident responses

This is an empirical process. You can't assume model confidence matches reality.

## Expressing Uncertainty Naturally

Once you know you're in the medium or low confidence tier, how do you express that without sounding robotic or undermining yourself?

**Bad uncertainty expressions:**
- "I am 73.2% confident that..."
- "There is a probability of 0.68 that..."
- "My uncertainty metric indicates..."

These are technically accurate but terrible UX.

**Good uncertainty expressions for medium confidence:**

Instead of: "I'm 75% sure this is correct"
Say: "Based on what I can see, this appears to be correct, but I'd recommend verifying"

Instead of: "Confidence level: 0.82"
Say: "I believe this is right, though there might be exceptions I'm not aware of"

Instead of: "Probability: 80%"
Say: "This is usually the case, but let me check if your situation is different"

The pattern: Natural language that conveys "I think this is right, but I'm not certain" without quantifying the exact confidence level.

**Good uncertainty expressions for low confidence:**

Instead of: "I'm only 55% confident"
Say: "I'm not sure about this"

Instead of: "Confidence below threshold"
Say: "I don't have reliable information on this"

Instead of: "Insufficient data"
Say: "I can't find a clear answer to that"

The pattern: Honest admission without over-explaining why you're uncertain.

## The Source of Confidence

Your confidence should come from multiple signals, not just the AI model's internal score.

**Signal 1: Model Confidence**
The base score the model assigns to its response.

**Signal 2: Source Quality**
Are you citing authoritative, recent, verified sources, or vague, old, unverified ones?

High confidence: Official documentation, verified database, primary sources
Medium confidence: Secondary sources, general knowledge, inferred from context
Low confidence: No sources, conflicting sources, outdated information

**Signal 3: Context Clarity**
Is the user's question clear and your interpretation certain, or are there ambiguities?

High confidence: Specific, detailed question with clear intent
Medium confidence: Somewhat vague but interpretable
Low confidence: Multiple possible interpretations

**Signal 4: Historical Accuracy**
Has your system been accurate on similar questions in the past?

High confidence: Similar questions have 95%+ accuracy
Medium confidence: Similar questions have 80%+ accuracy
Low confidence: Similar questions have <80% accuracy or no history

**Signal 5: Consistency Check**
Do multiple approaches to answering this question yield the same answer?

High confidence: All signals point to same answer
Medium confidence: Most signals agree, some uncertainty
Low confidence: Conflicting signals

Aggregate these signals into a composite confidence score.

## Confidence Thresholds by Question Type

Different question types need different thresholds.

**Factual Retrieval: High Threshold (90%+)**
"What's the return policy?"
"When is my appointment?"

These have objective right answers. Don't guess.

**Recommendations: Medium Threshold (75%+)**
"What's a good restaurant nearby?"
"Which option should I choose?"

These are subjective. Being "wrong" is less clear-cut.

**Creative/Open-Ended: Low Threshold (60%+)**
"Write me a poem about spring"
"Brainstorm marketing ideas"

These have no right answer. Confidence is about relevance, not correctness.

**Safety-Critical: Very High Threshold (95%+)**
"What should I do in this emergency?"
"Is this substance safe?"

These require near-certainty or refusal.

**Procedural: Medium-High Threshold (85%+)**
"How do I reset my password?"
"What's the process to file a claim?"

These need to be accurate, but users can recover if instructions are slightly off.

## The Confidence-Verbosity Tradeoff

There's a tension between expressing uncertainty and being concise.

Too brief: "March 15th" (confident-sounding even if you're not)
Too verbose: "Based on my analysis of available data sources, cross-referenced with historical patterns and adjusted for temporal context, I believe with moderate confidence that the date in question is approximately March 15th, though I acknowledge significant epistemic uncertainty."

The sweet spot:
- High confidence: No caveats needed → concise
- Medium confidence: Brief caveat → slightly longer
- Low confidence: Clear "I don't know" → still concise

**Efficiency pattern:**

High confidence (no caveat needed):
"Your appointment is Tuesday at 3pm."

Medium confidence (one-phrase caveat):
"Your appointment appears to be Tuesday at 3pm, though let me verify that."

Low confidence (clear uncertainty, point to solution):
"I don't see a confirmed appointment. Let me check your scheduling history."

Don't sacrifice clarity for brevity, but don't ramble about uncertainty either.

## Confidence Calibration for RAG Systems

For retrieval-augmented generation systems, confidence has an extra dimension: how much do you trust the retrieved information?

**Confidence factors for RAG:**

**Retrieval Confidence:**
- How well does the retrieved content match the query?
- Are you retrieving from authoritative sources?
- Is the retrieved content complete or fragmentary?

**Generation Confidence:**
- How well can you answer based on retrieved content?
- Are you making leaps beyond what's stated?
- Is the content sufficient or are you filling gaps?

**Staleness Confidence:**
- How recent is the retrieved information?
- Is this the kind of information that changes frequently?
- Do you have reason to believe it might be outdated?

Example:

Query: "What's the current CEO of the company?"
Retrieved: Document from 2023 saying "John Smith is CEO"
Current year: 2026

Even if retrieval quality is high, staleness risk is high for this type of query.

Response: "As of the information I have from 2023, John Smith was CEO, but leadership can change. I'd recommend checking the company's current leadership page for the most up-to-date information."

The confidence is medium because of staleness risk, even though retrieval quality was high.

## Testing Confidence Calibration

Build test sets that verify your confidence thresholds work as intended.

**Test Set 1: High Confidence Accuracy**
Take all responses where your system expressed high confidence (no caveats, direct statements).
Verify: Should be 90%+ accurate.

If accuracy is lower, your high-confidence threshold is too low (you're expressing confidence when you shouldn't).

**Test Set 2: Medium Confidence Accuracy**
Take all responses with medium confidence (caveats like "I believe," "appears to be").
Verify: Should be 70-90% accurate.

If accuracy is <70%, you need to lower confidence thresholds (express more uncertainty).
If accuracy is >90%, you're being too cautious (could express more confidence).

**Test Set 3: Low Confidence Refusal Rate**
Take all queries where your system expressed low confidence or refused to answer.
Verify: What percentage could have been answered correctly if you had tried?

If >50% could have been answered, your refusal threshold is too conservative.
If <20% could have been answered, your threshold is well-calibrated.

**Test Set 4: User Trust**
After responses at different confidence levels, measure user behavior:
- Do they verify high-confidence responses? (They shouldn't need to)
- Do they verify medium-confidence responses? (They should, and do)
- Do they seek alternative information after low-confidence responses? (They should)

User behavior tells you if your confidence signaling is working.

## The Overconfident Failure Mode

The most dangerous pattern: a system that doesn't know what it doesn't know.

It confidently answers questions outside its knowledge domain.
It doesn't recognize when it's extrapolating beyond its data.
It presents guesses as facts.

This is worse than a system that refuses too much, because users can't tell when to trust it.

**Warning signs of overconfidence:**
- High confidence scores on questions outside training domain
- Confident responses when sources are weak or absent
- No increase in uncertainty as questions become more specific
- Generating specific details (numbers, names, dates) without source verification

**Fixes for overconfidence:**
- Implement source-checking: Don't cite specifics unless you can point to them
- Implement domain-checking: Lower confidence for queries outside known areas
- Implement specificity-checking: Higher specificity requires higher confidence threshold
- Implement consistency-checking: If you get different answers from different approaches, flag uncertainty

## The Underconfident Failure Mode

The opposite problem: a system that hedges everything, even when it's clearly correct.

"It appears the sky is blue, though I'm not entirely certain."
"According to my information, which may not be complete, 2+2 seems to equal 4."

This is annoying and undermines itself unnecessarily.

**Warning signs of underconfidence:**
- Low confidence scores on questions well within training domain
- Caveats on objective, verifiable, simple facts
- Hedging on questions that were answered correctly 99% of the time historically

**Fixes for underconfidence:**
- Raise confidence thresholds for well-established facts
- Remove caveats for information from authoritative, current sources
- Build confidence from historical accuracy on similar questions

The goal: calibrated confidence, not maximum caution.

## Confidence and Personalization

Your confidence thresholds can adapt based on user context.

**Expert Users:**
- Can handle more ambiguity
- Appreciate when you skip caveats on basics
- Want you to be direct even at medium confidence

Example for expert:
"The API returns JSON. Here's the structure: [details]"

**Novice Users:**
- Need more guidance
- Appreciate when you explain uncertainty
- Want you to be cautious

Example for novice:
"The API returns data in JSON format (a common data structure). I'll show you what that looks like and explain each part."

**Risk-Averse Users:**
- Prefer more caveats
- Want to verify before acting
- Appreciate transparency about confidence

Example for risk-averse:
"Based on the documentation, this is the correct approach, but I'd recommend testing in a development environment first to be certain it works for your specific setup."

**High-Trust Users:**
- Have used the system successfully many times
- Okay with fewer caveats
- Want efficiency over caution

Example for high-trust:
"Done. The setting is updated."

Personalize confidence expression, not the underlying confidence threshold.

## When Confidence Changes Mid-Response

Sometimes you start answering with high confidence, then realize mid-response that you're less certain than you thought.

**Don't do this:**
"The answer is definitely X. Well, actually, I'm not sure. It might be Y. Or possibly Z."

This destroys trust. You sounded certain, then weren't.

**Do this:**
Detect confidence issues before you commit to an answer. If confidence drops as you generate the response, reformulate.

"Let me check that to be sure... I see some conflicting information here. It appears to be X, but I'd recommend verifying that before relying on it."

Or if you've already committed:
"Actually, let me revise that — I'm seeing some information that contradicts what I just said. Let me connect you with someone who can give you a definitive answer."

Better to catch yourself than to bulldoze forward with decreasing confidence.

## Documenting Your Confidence Thresholds

Your confidence policy should be documented clearly:

**For each response type:**
- High confidence threshold: X%
- Medium confidence threshold: Y%
- Signals that increase confidence: [list]
- Signals that decrease confidence: [list]
- Expression pattern for high confidence: [example]
- Expression pattern for medium confidence: [example]
- Expression pattern for low confidence: [example]

**For each domain:**
- Required confidence by risk level
- Special calibration factors
- Historical accuracy benchmarks

This documentation ensures consistency across your system and helps you onboard new team members.

## Building Trust Through Calibrated Confidence

Here's the paradox: expressing uncertainty makes you more trustworthy, not less.

Users don't expect you to know everything. They expect you to know what you know and what you don't.

When you're honest about uncertainty, users learn when to trust you fully and when to verify. That's not a weakness — that's a working relationship.

The systems that fail aren't the ones that refuse when uncertain. They're the ones that confidently state incorrect information, training users not to trust them at all.

Calibrated confidence is the foundation of trust.

In the next subchapter, we'll explore edge cases that break simple rules — the weird, unexpected, boundary-probing situations that every behavior spec looks perfect until they happen. These are the tests that reveal whether your confidence thresholds, refusal rules, and behavior boundaries actually hold up in the wild.
