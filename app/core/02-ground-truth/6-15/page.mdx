# 6.15 — Multi-Agent Coordination Truth

Let me tell you about the customer service disaster caused by perfect individual agents. A company built a multi-agent support system: Agent A handled initial triage, Agent B handled technical issues, Agent C handled billing, and Agent D handled escalations. Each agent was tested individually and worked beautifully. 95% accuracy, great response quality, users loved them in isolation.

Then they connected the agents to work as a team. Chaos ensued.

Agent A would categorize an issue as "technical" and hand it to Agent B. Agent B would realize it was actually billing-related and try to hand it to Agent C. But Agent C's context window didn't include the customer's earlier messages to Agent A, so the customer had to repeat themselves. Meanwhile, if the customer expressed frustration, Agent D (escalation) would sometimes interrupt Agent B mid-response, creating a confusing experience where two agents responded to the same message.

Each agent was individually excellent. The system was a disaster.

This is the multi-agent ground truth challenge: system-level correctness is not the sum of individual agent correctness. Coordination, communication, and handoff quality matter just as much as individual agent capability.

## Why Multi-Agent Systems Are Different

When you have a single AI agent, ground truth is about input-output pairs. Give it a prompt, evaluate the response. Simple.

When you have multiple agents working together, ground truth needs to cover:

**Individual Agent Quality**: Is each agent good at its specific task?

**Coordination Correctness**: Do agents work together effectively?

**Communication Quality**: Is information shared properly between agents?

**Handoff Completeness**: When one agent passes work to another, is context preserved?

**Conflict Resolution**: When agents disagree, is the conflict resolved correctly?

**Overall Task Completion**: Did the team achieve the user's goal?

You can't evaluate just individual agents. You have to evaluate the system as a whole.

## The Coordination Correctness Challenge

Coordination is about agents working together without duplicating effort, missing work, or interfering with each other.

Your ground truth should verify:

**No Duplication**: Agents don't perform the same task multiple times.

Example bad coordination:
User asks for flight options. Agent A searches flights. Agent B also searches flights (duplication). User gets two sets of results, causing confusion.

**No Gaps**: All necessary work is assigned to some agent.

Example bad coordination:
User asks to "book a flight and reserve a hotel." Agent A books the flight. No agent takes the hotel reservation. Task incomplete.

**Clear Ownership**: At any point, it's clear which agent is responsible for what.

Example bad coordination:
Two agents both try to respond to the same user message simultaneously, creating competing responses.

Your ground truth should include multi-step tasks requiring coordination and verify clean division of labor.

## Handoff Quality Ground Truth

When Agent A finishes its work and passes to Agent B, the handoff must be complete. Your ground truth should verify:

**Context Preservation**

Does Agent B have access to everything Agent A learned?

Test: User tells Agent A: "I'm allergic to peanuts." Agent A hands off to Agent B (restaurant booking). Does Agent B know about the allergy when suggesting restaurants?

If Agent B suggests a Thai restaurant famous for peanut sauce, the handoff failed.

**Intent Preservation**

Does Agent B understand what the user is trying to accomplish overall, not just its narrow task?

Test: User's goal is "plan a romantic anniversary dinner." Agent A handles date selection, Agent B handles restaurant booking. Does Agent B know it should suggest romantic restaurants, or does it just pick any restaurant?

**State Consistency**

If Agent A made decisions or promises, does Agent B honor them?

Test: Agent A tells user: "I'll have this resolved by end of day." Agent B takes over. Does Agent B know about this deadline?

Your ground truth should include handoff scenarios and verify information doesn't get lost in transition.

## Communication Protocol Ground Truth

Multi-agent systems need protocols for how agents communicate. Your ground truth should verify these protocols are followed.

**Message Format Consistency**

Do agents use a consistent format for sharing information?

If Agent A sends data as JSON and Agent B expects XML, communication breaks.

**Required Fields**

Are all necessary pieces of information communicated?

Example: Agent A passing a customer issue to Agent B should include: customer_id, issue_description, priority, history, attempted_solutions. If Agent A forgets "attempted_solutions," Agent B might try things that already failed.

**Acknowledgment**

When Agent A hands off to Agent B, does Agent B acknowledge receipt?

Without acknowledgment, Agent A doesn't know if Agent B got the message or if it needs to retry.

**Timeout Handling**

If Agent B doesn't respond within expected time, does the system handle it gracefully?

Your ground truth should include scenarios where communication fails or delays, verifying graceful handling.

## Conflict Resolution Ground Truth

When agents disagree, how is the conflict resolved? Your ground truth should test this.

Example conflict scenarios:

**Contradictory Information**

Agent A (data retrieval): "The user's account balance is $100."
Agent B (independent check): "The user's account balance is $150."

Who's right? How is this resolved?

Your ground truth should specify:
- Does the system detect the contradiction?
- Does it attempt to resolve it (check again, use most recent data, escalate to human)?
- Does it avoid confidently stating contradictory information to the user?

**Competing Priorities**

Agent A (efficiency): "Use the fastest option."
Agent B (cost): "Use the cheapest option."

These might conflict. Your ground truth should verify the system resolves priority conflicts according to specified rules (e.g., user preference if stated, otherwise default to efficiency).

**Task Ownership Disputes**

Agent A thinks this is a technical issue (its domain).
Agent B thinks this is a billing issue (its domain).

Your ground truth should verify:
- Is there a clear decision mechanism?
- Does one agent defer to the other appropriately?
- Is there an arbiter agent or rule that resolves the dispute?

## Overall Task Completion Metrics

Individual agents might succeed at their sub-tasks, but the overall task might still fail. Your ground truth should verify end-to-end success.

**Task Goal Achievement**

User goal: "Plan a vacation to Paris including flight, hotel, and restaurant reservations."

Individual agent evaluation:
- Flight agent: Successfully found and presented flights ✓
- Hotel agent: Successfully found and presented hotels ✓
- Restaurant agent: Successfully found restaurants ✓

System evaluation:
- Did the user actually end up with a complete vacation plan? ✗ (User got options but nothing was booked because no agent took responsibility for final booking)

Your ground truth should evaluate whether the user's overall goal was achieved, not just whether each agent completed its narrow task.

**User Satisfaction**

Even if the task was technically completed, was the user satisfied with the process?

Measure:
- Conversation smoothness (did it feel like talking to one intelligent assistant, or multiple disconnected bots?)
- Repetition (did the user have to repeat information?)
- Confusion (was the user confused about which agent they were talking to or what was happening?)

Your ground truth should include user experience metrics, not just task completion.

## Emergent Behavior Evaluation

Here's where multi-agent systems get really tricky: emergent behavior. The agents interact in ways that weren't explicitly programmed.

**Positive Emergent Behavior**

Agent A gathers information, Agent B analyzes it, Agent C makes a recommendation. Together, they solve problems none could solve alone. This is good emergence.

Your ground truth should reward beneficial emergent behaviors.

**Negative Emergent Behavior**

Agent A's outputs confuse Agent B, which produces poor outputs that mislead Agent C, creating a cascade of errors. Or agents get stuck in a loop, each waiting for the other.

Example loop:
- Agent A: "I need input from Agent B."
- Agent B: "I need input from Agent A."
- System: Deadlock.

Your ground truth should detect harmful emergent patterns:
- Infinite loops
- Error cascades (one agent's error propagates through the system)
- Resource exhaustion (agents keep spawning sub-agents until the system crashes)

Include stress tests and adversarial scenarios to surface emergent failures.

## Testing Agent Communication Channels

Multi-agent systems communicate through various channels. Your ground truth should verify each channel works correctly.

**Direct Agent-to-Agent Communication**

Agent A sends a message directly to Agent B.

Test: Is the message received correctly? Is it interpreted correctly?

**Shared State/Memory**

Multiple agents read and write to a shared data store.

Test: Do agents see consistent state? Are there race conditions where simultaneous writes cause corruption?

**Broadcast Communication**

One agent sends a message to all agents.

Test: Do all agents receive it? Do they process it appropriately?

**Hierarchical Communication**

Worker agents report to a coordinator agent.

Test: Does the coordinator receive all necessary reports? Does it synthesize them correctly?

Your ground truth should include scenarios exercising each communication pattern.

## The Individual vs System Quality Gap

Here's a critical measurement: how much better or worse is the system than the sum of its parts?

**Positive Synergy**: The system performs better than the best individual agent could alone.

Example: Agent A is great at research, Agent B is great at analysis. Together, they produce insights neither could alone.

**Negative Synergy**: The system performs worse than individual agents due to coordination overhead.

Example: Each agent is 90% accurate, but handoffs introduce 10% error rate, so system is only 80% accurate.

Your ground truth should measure:
- Individual agent performance (in isolation)
- System performance (agents working together)
- Gap between them (positive or negative synergy)

If the gap is negative (system worse than individuals), you have a coordination problem.

## Multi-Agent Evaluation Scenarios

Your ground truth should include scenarios specifically designed to test multi-agent capabilities:

**Sequential Task Scenario**

Task requires steps that must happen in order, with each agent handling one step.

Example: "Research the topic, write a summary, format it as a presentation."
- Agent A: Research (gathers information)
- Agent B: Summarization (writes summary based on A's research)
- Agent C: Formatting (creates presentation from B's summary)

Verify:
- Does information flow correctly through the chain?
- Does final output reflect all steps?

**Parallel Task Scenario**

Task can be divided into parallel sub-tasks.

Example: "Get weather, traffic, and news for my commute."
- Agent A: Weather (independently fetches weather)
- Agent B: Traffic (independently fetches traffic)
- Agent C: News (independently fetches news)
- Agent D: Synthesis (combines into cohesive briefing)

Verify:
- Do parallel agents work without interfering with each other?
- Does synthesis agent correctly combine results?

**Recursive Task Scenario**

Task requires spawning sub-agents dynamically.

Example: "Analyze feedback from all our customer surveys."
- Coordinator agent: Identifies 50 surveys to analyze
- Spawns 50 analysis agents, one per survey
- Collects and synthesizes results

Verify:
- Are sub-agents spawned correctly?
- Are results collected completely?
- Is synthesis accurate?

**Conflict Scenario**

Task where agents must resolve disagreements.

Example: "Find the best restaurant for our group."
- Agent A prioritizes user Alice's preference (vegetarian)
- Agent B prioritizes user Bob's preference (seafood)
- Agents must negotiate

Verify:
- Is conflict detected?
- Is resolution approach reasonable (find place with both options, prioritize based on defined rules)?

## The 2026 Multi-Agent Framework Landscape

As of 2026, several multi-agent frameworks have emerged. Your ground truth should work across frameworks:

**LangGraph and Similar**

These frameworks use directed graphs where agents are nodes and communication is edges.

Your ground truth should verify:
- Correct graph execution (nodes execute in right order)
- Edge conditions (messages flow correctly between nodes)

**Swarm Frameworks**

These allow dynamic agent spawning and self-organization.

Your ground truth should verify:
- Agents are spawned when needed
- Swarm converges to solution (doesn't grow unbounded)
- Redundant agents are pruned

**Hierarchical Frameworks**

These use supervisor/worker patterns.

Your ground truth should verify:
- Supervisors correctly delegate to workers
- Workers report back to supervisors
- Hierarchy doesn't create bottlenecks

Framework-specific ground truth ensures you're testing the patterns your system actually uses.

## Debugging Multi-Agent Failures

When a multi-agent system fails, root cause can be hard to find. Your ground truth should help identify where the failure occurred:

**Agent-Level Logging**

Track each agent's inputs, outputs, and state transitions.

When a test fails, you can trace exactly where things went wrong.

**Communication Logging**

Track all inter-agent messages.

When information is lost or corrupted, you can see which message transmission failed.

**Decision Logging**

When agents make decisions (which agent to hand off to, how to resolve conflict), log the decision and reasoning.

This helps identify whether coordination logic is faulty.

Your ground truth framework should include diagnostic logging to support debugging.

## The Warning: What Happens If You Skip This

If you evaluate multi-agent systems only by testing individual agents in isolation, here's what happens:

Your agents will work beautifully alone and terribly together. Coordination will break down. Information will be lost in handoffs. Agents will duplicate work or miss work entirely. Conflicts won't be resolved. Users will have frustrating experiences with capable but uncoordinated agents.

You'll deploy a system that passes all your tests but fails in production because you never tested the interactions.

I've seen teams spend months optimizing individual agents, only to discover their multi-agent system was worse than a single agent because coordination overhead destroyed the benefits.

Don't test the parts. Test the whole. Multi-agent ground truth is about the system, not the components.

## Bridge to Dataset Design

We've completed our tour of ground truth across AI modalities—from chat and RAG to agents and multi-agent systems, from classification and code generation to creative work and reasoning, from voice and multilingual systems to ranking and personalized truth. We've covered what makes a response "correct" in each context.

But here's the next challenge: how do you assemble these ground truth examples into a dataset that actually tells you whether your AI is ready for production? It's not enough to have a hundred perfect examples. You need a dataset that's representative of real usage, covers edge cases, avoids bias, and evolves as your system changes. That's dataset design, and it's where ground truth theory meets practical evaluation. Let's walk through how to build evaluation datasets that actually work in Chapter 3.
