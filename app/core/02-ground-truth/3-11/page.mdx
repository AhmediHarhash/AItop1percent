# 3.11 â€” Constraint-Aware Truth: Good Under Latency, Cost & Tool Limits

There's a famous surgery story from a field hospital. A doctor has three patients: one needs immediate surgery or they'll die in an hour, one needs surgery within 24 hours, and one needs surgery eventually but it's not urgent. The doctor only has time to operate on two patients today.

In a fully-equipped hospital with unlimited staff, the right answer is "treat all three optimally." But in a field hospital with constraints, the right answer is "triage based on constraints." Treat the urgent case first, do what you can for the 24-hour case, stabilize the third for transport.

Same medical principles. Different constraints. Different definition of "correct."

Your AI system faces this constantly. The theoretically best response isn't always achievable within your constraints. A voice assistant with a 300ms latency budget can't give the same detailed response as an email assistant with no time limit. A system on a tight cost budget can't make the same number of API calls as one with unlimited budget.

Let me walk you through defining "good enough" responses that respect real-world constraints while still meeting user needs. This is where ground truth gets pragmatic.

## Why "Best Possible" Isn't Always the Target

Most teams define ground truth like this:

"What's the ideal response to this query?"

Then they build a system that tries to achieve that ideal. And it fails because the ideal requires:
- More time than users will wait
- More compute than the budget allows
- More context than fits in the window
- More tool calls than the architecture supports
- More tokens than the model can generate

The result: a system that's slow, expensive, or frequently times out trying to achieve the unachievable.

Better approach:

"What's the best response achievable within our constraints?"

This isn't settling for worse quality. It's defining quality realistically.

## The Four Constraint Types

Every AI system operates under at least one of these constraints:

**Constraint 1: Latency**

How long can the user wait for a response?

Voice assistant: 300ms before it feels broken
Chatbot: 5 seconds before user assumes it's stuck
Email assistant: Hours or days is fine
Batch processing: Days or weeks is acceptable

Your latency constraint changes what "good" means.

**Constraint 2: Cost**

How much can you spend per interaction?

Free tier: Pennies per query, use small models, minimal tools
Paid tier: Dollars per query, use larger models, more tools
Enterprise: Cost less important, prioritize quality and capability

Your cost constraint changes what's acceptable.

**Constraint 3: Context Window**

How much information can you work with?

Small context (4K tokens): Can't fit long documents
Medium context (32K tokens): Can fit most conversations
Large context (200K tokens): Can fit entire codebases

Your context constraint changes what you can reference.

**Constraint 4: Tool Availability**

What external resources can you access?

Full access: Can call any API, query any database
Limited access: Can only read, not write
Offline mode: No external tools at all

Your tool constraint changes what you can do.

## Constraint-Aware Ground Truth Examples

Let's see how constraints change the "right answer" for the same query.

**Query: "Summarize this 50-page document"**

Unconstrained ideal:
- Read entire document
- Identify key themes
- Extract important details
- Provide comprehensive summary with citations
- Takes 60 seconds, costs $0.50

Latency-constrained (must respond in 5 seconds):
- Read first 5 pages fully
- Skim remaining pages for headers and key terms
- Provide summary of what was actually processed
- Acknowledge document is longer, offer to read more
- Takes 5 seconds, costs $0.05

Cost-constrained (must stay under $0.05):
- Extract table of contents and section headers
- Provide structural summary
- Offer to deep-dive into specific sections user cares about
- Takes 10 seconds, costs $0.04

Context-constrained (only 8K token window):
- Fit as much of document as possible
- Summarize what fit
- Clearly indicate percentage of document covered
- Offer to process remainder in follow-up
- Takes 30 seconds, costs $0.20

Each response is "correct" for its constraint. The unconstrained version isn't better if it violates the latency requirement.

## The Latency-Quality Tradeoff

When latency is constrained, you have to decide: fast or thorough?

**Latency tiers and quality levels:**

Under 300ms (real-time interaction):
- Single model call, no tools
- Use cached information where possible
- Provide quick answer, acknowledge if incomplete
- Offer to elaborate if user asks

Example:
User: "What's the weather in Tokyo?"
Fast response: "Checking..." then show cached general climate info
Can't: Make API call to weather service (takes too long)
Alternative: "For current conditions, I'd need to check a weather service. I can tell you Tokyo's typical climate, or you can check [weather.com]."

300ms - 2 seconds (conversational):
- Single model call, maybe one fast tool call
- Retrieve from database if indexed well
- Provide direct answer with brief context
- Skip elaboration unless needed

Example:
User: "What's my account balance?"
Response: Query database (200ms), format response (50ms), return within 500ms total
Can't: Also fetch transaction history, calculate trends, provide recommendations (all take additional time)

2-5 seconds (standard):
- Multiple model calls or tool calls
- More comprehensive response
- Include relevant context and citations

Example:
User: "Explain the return policy"
Response: Retrieve policy doc (500ms), generate clear explanation (2s), include citations, format nicely

5+ seconds (complex):
- Many tool calls, complex reasoning
- Deep analysis or synthesis
- Progress indication required
- User expectations set appropriately

Example:
User: "Analyze this quarter's sales trends"
Response: Query multiple databases (2s), run analysis (3s), generate visualizations (2s), provide insights (2s) with progress updates throughout

**The key:** Define acceptable quality tiers for each latency budget. Don't try to fit Tier 4 quality into Tier 1 latency.

## The Cost-Quality Tradeoff

When cost is constrained, you have to decide: cheap or capable?

**Cost tiers:**

Minimal cost (<$0.01 per query):
- Smallest model that's functional
- No or minimal tool calls
- Cache aggressively
- Acceptable for high-volume, low-stakes queries

Example product: Free tier of a homework help app
- Use 8B parameter model
- No web search (costs too much)
- Rely on model knowledge
- Quality: Helpful but limited

Low cost ($0.01-$0.05 per query):
- Medium model or small model with some tools
- Selective tool use
- Balance quality and cost
- Acceptable for most consumer use cases

Example product: Standard chatbot
- Use 70B parameter model
- Allow 1-2 tool calls per query
- Quality: Good for common queries

Medium cost ($0.05-$0.25 per query):
- Large model or multiple model calls
- Liberal tool use
- Prioritize quality over cost
- Acceptable for paid tiers or business use

Example product: Enterprise assistant
- Use top-tier model
- Multiple tool calls as needed
- Quality: Comprehensive and accurate

High cost ($0.25+ per query):
- Best available models
- Unlimited tools
- Multi-step reasoning
- Acceptable for high-value use cases

Example product: Expert system for medical diagnosis support
- Use best models, multiple calls
- Extensive knowledge base queries
- Literature searches
- Quality: Maximum accuracy and thoroughness

**Degradation strategies when over budget:**

1. Model downgrade: Use smaller model for this query
2. Tool budget: Limit number of tool calls
3. Cache reliance: Use cached responses if available
4. Partial response: Answer what you can within budget
5. Defer: "This query would exceed cost limits. Would you like to proceed anyway?"

## The Context-Quality Tradeoff

When context window is constrained, you have to decide: what to include?

**Context prioritization:**

Tier 1: Critical context (always include):
- Current user query
- System instructions
- Required behavior rules
- Critical user preferences

Tier 2: High-value context (include if space):
- Recent conversation history (last 5-10 turns)
- Relevant user data
- Key reference documents

Tier 3: Nice-to-have context (include if room):
- Extended conversation history
- Background information
- Related documents

Tier 4: Optional context (usually omitted):
- Very old conversation history
- Tangentially related information
- General background

**When context doesn't fit:**

Strategy 1: Summarize
Replace lengthy context with summaries
Example: Replace 50-turn conversation with "User is trying to troubleshoot login issues. So far, we've tried: password reset, clearing cache, checking username."

Strategy 2: Prioritize recent
Keep recent context, drop old context
Example: Keep last 10 turns, drop everything before that

Strategy 3: Chunk and process
Break large documents into chunks, process separately
Example: "This document is too large for single analysis. I can analyze by chapter. Which section interests you most?"

Strategy 4: Extract and include
Pull key information from large context, include only that
Example: Extract key facts from 100-page document, work with extracted facts rather than full text

**The ground truth changes:**

Unconstrained: "Analyze this full conversation including all 100 turns"
Context-constrained: "Based on the recent conversation [last 10 turns], here's my understanding. For earlier context, I can search the history if you need something specific."

The second isn't worse. It's honest about constraints and still helpful.

## The Tool-Availability Tradeoff

When tools are unavailable, you have to decide: what can you still do?

**Degraded capability modes:**

Full capability (all tools available):
- Real-time data access
- Write operations
- External API calls
- Complete functionality

Example:
User: "What's the current stock price of AAPL?"
Response: Call stock API (real-time), return current price

Degraded capability (some tools unavailable):
- Cached data only
- Read operations only
- Limited functionality
- Clear disclosure to user

Example:
User: "What's the current stock price of AAPL?"
Response: "I don't have access to real-time stock data right now, but as of this morning's market open, AAPL was at $150. For current prices, check [finance.yahoo.com]."

Offline mode (no tools):
- Model knowledge only
- No external data
- Severely limited functionality
- Very clear disclosure

Example:
User: "What's the current stock price of AAPL?"
Response: "I don't have access to stock market data. For current prices, you'll need to check a financial website like Yahoo Finance or Bloomberg."

**The key:** Be honest about degraded capability. Don't fabricate data when tools are unavailable.

## Graceful Degradation Patterns

When you hit constraints, degrade gracefully:

**Pattern 1: Partial Response + Offer**

"Here's what I can tell you within [constraint]. If you need more detail, I can [alternative]."

Example:
"I've analyzed the first 10 pages of your document within the time limit. I can continue with the rest if you'd like, or focus on a specific section."

**Pattern 2: Best Effort + Disclosure**

"Based on [what I could access given constraints], here's my response. Note that [limitation]."

Example:
"Based on the information I have access to, your account balance is $450. This is as of this morning; for real-time balance, check the app."

**Pattern 3: Alternative Path**

"I can't do [ideal] due to [constraint], but I can do [alternative] instead."

Example:
"I can't search the entire company knowledge base within the response time limit, but I can search the most relevant category. Which topic area is this related to?"

**Pattern 4: User Choice**

"This query would exceed [constraint]. Do you want me to [fast but limited] or [slow but thorough]?"

Example:
"A full analysis would take about 30 seconds and use your premium query quota. Would you like the full analysis, or a quick summary that's free?"

## Testing Constraint-Aware Responses

Your test sets should include constraint scenarios:

**Test Set 1: Latency Constraints**

For each latency tier, test queries that would ideally take longer:
- Can the system respond within the time limit?
- Is the response helpful despite being constrained?
- Does it acknowledge limitations appropriately?
- Does it offer alternatives for getting complete answer?

**Test Set 2: Cost Constraints**

For each cost tier, test queries that would ideally cost more:
- Does the system stay within cost budget?
- Is quality acceptable given the constraint?
- Does it degrade predictably when hitting limits?

**Test Set 3: Context Constraints**

Test with queries requiring more context than available:
- Does it handle context overflow gracefully?
- Does it prioritize correctly?
- Does it acknowledge what it couldn't include?

**Test Set 4: Tool Availability**

Test each degraded mode:
- Full tools available
- Some tools unavailable
- Offline mode
Verify responses are appropriate for each mode.

## Defining Acceptable Quality Under Constraints

For each constraint level, define minimum acceptable quality:

**Latency-constrained quality standards:**

Under 300ms:
- Minimum: Acknowledge query, indicate processing
- Acceptable: Basic response from cache/index
- Good: Relevant quick answer with offer to elaborate

Under 2s:
- Minimum: Direct answer to query
- Acceptable: Answer with brief context
- Good: Comprehensive answer with citations

**Cost-constrained quality standards:**

Under $0.01:
- Minimum: Relevant response using small model
- Acceptable: Helpful response with caveats about limited capability
- Good: Accurate response for common queries

Under $0.05:
- Minimum: Accurate response using standard tools
- Acceptable: Comprehensive response
- Good: Response with verification and alternatives

**Context-constrained quality standards:**

4K context:
- Minimum: Respond based on current query only
- Acceptable: Include last 3-5 turns of conversation
- Good: Include prioritized context from full conversation

32K context:
- Minimum: Include full recent conversation
- Acceptable: Include relevant documents/data
- Good: Include comprehensive context

## Constraint-Aware Prompt Engineering

Your prompts should acknowledge constraints:

**Latency-aware prompt:**

```
You have a strict 2-second response deadline. For complex queries:
1. Provide immediate direct answer
2. Skip extended explanations unless essential
3. Offer to elaborate if user asks
4. Don't try to be comprehensive if it would exceed time limit

If you can't answer well in 2 seconds, say so and explain why, don't force a rushed response.
```

**Cost-aware prompt:**

```
You have a budget of $0.02 per query. This means:
1. Use tools sparingly (each call costs money)
2. Rely on provided context before calling external APIs
3. If a comprehensive answer would exceed budget, provide best answer within budget and note limitations
4. Don't make multiple redundant API calls
```

**Context-aware prompt:**

```
You have an 8K token context window. Current usage: 6K tokens. This means:
1. You have ~2K tokens for your response
2. Be concise but helpful
3. If user asks for comprehensive response, note that you'll need multiple exchanges to fully answer
4. Prioritize most important information first
```

## Dynamic Constraint Adjustment

Constraints aren't always fixed. Sometimes you can adjust them based on query importance.

**Priority tiers:**

Low priority (casual queries):
- Strict constraints apply
- Optimize for cost and speed
- Quality is "good enough"

Medium priority (typical use):
- Standard constraints
- Balance cost, speed, quality
- Quality is "good"

High priority (important queries):
- Relaxed constraints
- Prioritize quality over cost/speed
- Quality is "best possible"

**Auto-detection of priority:**

Signals of high priority:
- User indicates urgency
- Query involves critical operations (financial, security, health)
- Previous attempts failed
- User is frustrated

Signals of low priority:
- Exploratory query
- User is browsing/learning
- Casual conversation
- No time pressure

Adjust constraints accordingly.

## Communicating Constraints to Users

When constraints limit your response, tell users:

**Bad (hidden constraint):**
User: "Analyze this 200-page report."
System: Provides analysis of only first 20 pages without acknowledging limitation.
User: Doesn't realize analysis is incomplete, makes decisions on partial information.

**Good (disclosed constraint):**
User: "Analyze this 200-page report."
System: "I've analyzed the first 20 pages in detail (10% of the full report). Would you like me to continue with the rest, or focus on specific sections you're most interested in?"
User: Understands limitation, can make informed choice.

Transparency about constraints builds trust.

## The "Good Enough" Threshold

How do you know if a constrained response is good enough?

**Minimum quality criteria:**

1. **Helpful:** Does it address user's actual need, even if not perfectly?
2. **Honest:** Does it accurately represent what it can and can't do?
3. **Safe:** Does it avoid harm despite constraints?
4. **Actionable:** Does user have a path forward, even if it's not ideal?

If all four are met, it's good enough given constraints.

**Not good enough:**

Response that is:
- Unhelpful (doesn't address need at all)
- Misleading (pretends to be comprehensive when it's not)
- Unsafe (violates safety rules to save cost/time)
- Dead-end (user is stuck with no path forward)

These responses fail quality standards even when constrained.

## Constraint Trade-Off Decisions

Sometimes you can trade one constraint for another:

**Speed vs Cost:**
- Fast response with small model (cheap but lower quality)
- Slower response with large model (expensive but higher quality)

**Context vs Latency:**
- Quick response with limited context
- Slower response processing full context

**Accuracy vs Cost:**
- Cheap response with no verification
- Expensive response with fact-checking

Document these trade-offs and make them explicitly:

"For [query type], we prioritize [constraint 1] over [constraint 2] because [reason]."

Example:
"For customer support queries, we prioritize response speed over comprehensive detail because users value quick resolution. Detailed answers are available on-demand."

## Your Constraints Are Part of Ground Truth

Here's the key insight: your constraints aren't excuses for poor quality. They're part of the definition of quality.

A voice assistant that responds in 300ms with a helpful but brief answer is higher quality than one that responds in 5 seconds with a comprehensive answer. Because in that context, speed is part of quality.

A free tier that provides good responses within cost limits is higher quality than one that tries to provide premium responses and constantly fails due to budget overruns.

Constraint-aware ground truth means defining success in the real world you operate in, not an idealized world with unlimited resources.

In the next and final subchapter of this chapter, we'll explore no-inference zones: the things your system must never guess or assume, even when inference seems reasonable. This is about knowing when to stick strictly to what the user said and when contextual inference is safe. It's a subtle but critical boundary for trustworthy systems.
