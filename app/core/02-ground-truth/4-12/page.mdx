# 4.12 — Inter-Annotator Agreement: Cohen's Kappa, Krippendorff's Alpha & When Numbers Lie

Let me tell you about a number that will save you from disaster or give you false confidence, depending on whether you understand it.

You've built your ground truth. You have three annotators independently label one hundred examples. You calculate that they agreed on ninety examples. Ninety percent agreement! That's excellent, right?

Maybe. Or maybe they agreed by accident.

Imagine I ask you and a friend to flip coins and record heads or tails for one hundred flips. You do it independently, not showing each other your results. When you compare notes, you'll agree on about fifty flips just by random chance. Fifty percent agreement despite having zero actual coordination.

Now imagine I tell you this means you and your friend have "fifty percent inter-rater reliability in coin classification." You'd laugh. Of course you agree half the time — there are only two options and you're both guessing randomly. The agreement is meaningless.

The same trap exists in ground truth annotation. If you have two possible labels (good/bad) and your annotators are confused, they might agree fifty percent of the time just by chance. If you have five possible labels and they're rating randomly, they'll still agree twenty percent of the time by luck.

This is why simple agreement percentage is misleading. You need to know: how much better than chance are your annotators doing?

That's what Cohen's Kappa and Krippendorff's Alpha measure. They're statistical corrections that tell you whether your annotators are actually consistent or just randomly overlapping.

Let me walk you through what these numbers mean and how to use them, without making you do any math.

## The Agreement-by-Chance Problem

Start with intuition. You have two annotators scoring responses as good, neutral, or bad.

If they're both just guessing randomly, they'll sometimes guess the same thing by accident. With three categories, random guessing produces about thirty-three percent agreement just by chance.

If they're somewhat aligned but still pretty confused, they might agree sixty percent of the time. That's better than chance, but it's not strong reliability.

If they're well-calibrated and using clear criteria, they might agree ninety percent of the time. That's way better than chance.

The question is: given that some agreement happens by luck, how much of your observed agreement is real?

Simple agreement percentage can't answer this. You need a metric that:

1. Measures the observed agreement (how often they actually agreed)
2. Estimates the expected agreement by chance (how often they'd agree if guessing)
3. Calculates how much better than chance they're doing

That's what Cohen's Kappa does.

## Cohen's Kappa: The Concept

Think of Cohen's Kappa as a scale:

Kappa = 0 means your annotators agree exactly as much as random chance would predict. They're no better than flipping coins.

Kappa = 1 means perfect agreement beyond chance. Every time they agree, it's because they're using the same criteria, not because they got lucky.

Kappa < 0 means they agree LESS than random chance, which happens if they're systematically disagreeing (one annotator is contrarian or misunderstood the task).

Kappa between 0 and 1 means partial agreement beyond chance. The higher the number, the more consistent they are.

Here's the intuition behind the formula in plain English:

Start with the percentage of times they actually agreed.
Subtract the percentage of times they would have agreed by random chance.
Divide by the maximum possible improvement over chance.

This gives you a number that's corrected for chance agreement.

Example with made-up numbers:

Your annotators agreed on seventy percent of examples (observed agreement: 0.7).

If they were labeling randomly given the distribution of labels, they'd agree forty percent of the time by chance (expected agreement: 0.4).

The maximum possible agreement beyond chance is sixty percent (100% - 40% = 60%).

Your actual agreement beyond chance is thirty percent (70% - 40% = 30%).

Kappa = 30% / 60% = 0.5

So they're halfway between random and perfect agreement.

## Interpreting Kappa Values

The standard interpretation guidelines:

Kappa < 0.0: Worse than chance. Something is seriously wrong.

Kappa 0.0 - 0.20: Slight agreement. Barely better than random. Not usable.

Kappa 0.21 - 0.40: Fair agreement. Better than random but still quite inconsistent. Needs work.

Kappa 0.41 - 0.60: Moderate agreement. Okay for exploratory work, not for production ground truth.

Kappa 0.61 - 0.80: Substantial agreement. Good enough for most uses. This is your target.

Kappa 0.81 - 1.0: Almost perfect agreement. Excellent. Hard to achieve for subjective judgments.

For production ground truth, aim for Kappa above 0.6. Above 0.8 is ideal but might be unrealistic for subjective dimensions like tone or empathy.

If you're below 0.6, your ground truth is not reliable enough. Your rubrics are too vague, your annotators need more training, or the task is genuinely too subjective.

## When Kappa Misleads

Cohen's Kappa is useful but not perfect. It can mislead in a few scenarios:

Scenario one: Imbalanced labels. If ninety-five percent of your examples are labeled "good" and only five percent are "bad," annotators will agree ninety-five percent of the time just by always choosing "good." Kappa corrects for this, but it can be overly harsh. You might have strong agreement on the rare "bad" cases that actually matter, but Kappa averages over everything.

Scenario two: More than two annotators. Cohen's Kappa is designed for two annotators. If you have three or more, you need a different metric (that's where Krippendorff's Alpha comes in).

Scenario three: Ordered categories. If your labels have a natural order (1-2-3-4-5 ratings), Kappa treats disagreeing by one point the same as disagreeing by four points. A weighted version of Kappa can handle this, but standard Kappa can't.

Scenario four: Partial agreement. Sometimes annotators give overlapping but not identical labels. "This response is good at accuracy but bad at tone" vs "This response is acceptable overall." Standard Kappa treats this as full disagreement, even though they're partially aligned.

Be aware of these limitations. Kappa is a tool, not gospel.

## Krippendorff's Alpha: The Generalization

Krippendorff's Alpha is a more flexible metric that handles cases Cohen's Kappa struggles with:

It works with any number of annotators (two, three, ten, doesn't matter).

It works with different types of data (binary, categorical, ordinal, interval).

It handles missing data (not every annotator has to label every example).

It can account for different disagreement severities (disagreeing by 1 on a 5-point scale is less bad than disagreeing by 4).

The interpretation is similar to Kappa:

Alpha = 1 means perfect agreement.
Alpha = 0 means agreement at the level of chance.
Alpha < 0 means worse than chance.

The thresholds are slightly different:

Alpha < 0.67: Data are unreliable, don't use for conclusions.
Alpha 0.67 - 0.80: Tentative conclusions only.
Alpha > 0.80: Reliable for most purposes.

Krippendorff is stricter than Kappa. An Alpha of 0.67 is roughly equivalent to a Kappa of 0.6-0.7.

Use Alpha when:

You have more than two annotators.
Your labels are ordinal (ordered categories) and you want to account for closeness of disagreements.
You have missing labels (some annotators didn't label all examples).
You want a more conservative, robust estimate.

## Calculating These Metrics

You don't need to do the math by hand. Libraries and tools do it for you.

In Python:

For Cohen's Kappa: scikit-learn has sklearn.metrics.cohen_kappa_score

For Krippendorff's Alpha: krippendorff package

Most annotation platforms (Labelbox, Scale AI, Argilla) calculate these automatically.

But you should understand conceptually what the numbers mean so you can interpret them.

## Using Agreement Metrics in Practice

Here's how to use these metrics in your ground truth workflow:

Step one: Have multiple annotators (two or three) independently label a sample of examples (fifty to one hundred).

Step two: Calculate Kappa (if two annotators) or Alpha (if more than two).

Step three: Interpret:

If Kappa/Alpha > 0.8: Your rubrics and training are excellent. Proceed with confidence.

If 0.6 - 0.8: Acceptable. Identify the specific examples where annotators disagreed and refine rubrics around those.

If 0.4 - 0.6: Not good enough for production. Investigate why agreement is low. Common causes:
  - Rubrics are too vague
  - Annotators weren't trained well
  - Examples are genuinely ambiguous

If < 0.4: Serious problems. Don't use this ground truth until you fix the consistency issues.

Step four: For examples where annotators disagreed, bring them together to discuss. Often they're interpreting rubrics differently or focusing on different aspects. Resolve the disagreements and clarify rubrics.

Step five: Retrain annotators and measure agreement again. Repeat until you hit acceptable levels.

## Disagreement as Signal

Low agreement is not just a problem to fix. It's information.

When annotators disagree, ask why:

Is the rubric ambiguous for this type of example? Fix the rubric.

Is the example itself edge-casey and genuinely hard to judge? Mark it as such. Not every example needs confident labels.

Are annotators applying different value systems? Maybe one cares more about brevity, another about completeness. Make the trade-offs explicit in your rubrics.

Is there a systematic difference between annotators? Maybe expert annotators are stricter than novices. This tells you who to trust more.

Track which examples have low agreement. These are your "hard cases." You might:

Exclude them from your test set (if they're too ambiguous to be useful).
Keep them but flag them (they test edge cases where even experts disagree).
Use them for training (they teach annotators to recognize ambiguity).

Disagreement is not failure. It's your ground truth telling you where judgment is genuinely difficult.

## Agreement on Different Dimensions

If you're using rubrics with multiple dimensions (accuracy, tone, completeness), calculate agreement separately for each dimension.

You might find:

High agreement on accuracy (Kappa = 0.85) — factual correctness is objective.
Moderate agreement on completeness (Kappa = 0.65) — some judgment required.
Low agreement on tone (Kappa = 0.45) — very subjective.

This tells you:

Trust accuracy labels. They're reliable.
Use completeness labels but be aware there's some noise.
Don't trust tone labels yet. Refine the rubric or accept that tone is too subjective to label consistently.

You can prioritize fixing the dimensions with low agreement or accept that some dimensions are inherently harder to label.

## Sample Size for Agreement Testing

How many examples do you need to label to get a reliable agreement estimate?

Too few examples (under twenty) and your Kappa/Alpha will be noisy — sensitive to a few examples.

Too many examples (over two hundred) is overkill for agreement testing. You're wasting annotator time.

The sweet spot is fifty to one hundred examples. Enough to get a stable estimate, not so many that it's expensive.

Stratify your sample: include easy examples, hard examples, and edge cases in proportion to your full dataset. Don't only test agreement on easy examples — you need to know if annotators agree on the hard stuff.

## When Numbers Lie: Agreement Without Quality

Here's the dark side of agreement metrics. You can have high inter-annotator agreement on wrong labels.

Imagine two annotators who both think responses should be extremely formal. They'll agree with each other consistently (high Kappa) but they're both systematically wrong if your users actually prefer casual tone.

Or imagine annotators who weren't trained properly and are both applying the rubric incorrectly in the same way. High agreement, bad labels.

Agreement metrics tell you about consistency, not correctness. You need both.

To check correctness:

Validate annotator labels against expert gold standard examples.
Validate labeled data against real-world outcomes (do high-scored responses actually perform better?).
Have domain experts spot-check a sample of annotated data.

Think of agreement as a necessary but not sufficient condition. High agreement without validation is fool's gold.

## Agreement Over Time

Annotator consistency can drift over time. They get tired, they forget training, they develop personal shortcuts.

Track agreement over time:

Month 1: Kappa = 0.78
Month 2: Kappa = 0.72
Month 3: Kappa = 0.65

This drift signals that annotators are losing calibration. They need refresher training or clearer rubrics.

Best practice: Periodically slip "gold standard" examples into the annotation queue — examples with known correct labels. See if annotators still label them correctly. If accuracy on gold standard examples drops, retrain.

## Agreement Across Annotator Types

You might have different types of annotators:

Domain experts (expensive, slow, high-quality)
Trained annotators (moderate cost, faster, good quality)
Crowd workers (cheap, fast, variable quality)

Measure agreement within each group and between groups.

If experts agree with each other (high Kappa) but trained annotators also agree with each other (high Kappa) but the two groups disagree with each other (low cross-group Kappa), you have a systematic difference in standards.

Maybe experts are stricter. Maybe trained annotators are applying the rubric literally while experts are using judgment. You need to decide which standard you want and align everyone to it.

## Using Agreement to Allocate Annotation Effort

Here's a practical use of agreement metrics: deciding which examples need multiple annotators.

For easy examples where annotators always agree, one annotator is enough.
For hard examples where annotators often disagree, use three annotators and use majority vote or discussion to resolve.

Workflow:

Step one: Have two annotators label a sample. Calculate Kappa per example type.
Step two: For types with Kappa > 0.85, use single annotation going forward.
Step three: For types with Kappa 0.6-0.85, use double annotation.
Step four: For types with Kappa < 0.6, use triple annotation plus expert review.

This optimizes cost while maintaining quality.

## When Perfect Agreement is Suspicious

If your Kappa is 1.0 (perfect agreement), be suspicious unless the task is completely objective.

Perfect agreement might mean:

The task is truly objective and unambiguous (great).
Annotators are colluding or copying each other (bad).
The examples are all extremely easy (you're not testing the hard stuff).
Your sample is too small and you got lucky.

Investigate perfect agreement. If it's legitimate, excellent. If annotators are just clicking the same buttons without thinking, you have a problem.

For subjective judgments (tone, quality, appropriateness), perfect agreement is unrealistic. Even experts disagree sometimes. Kappa in the 0.7-0.9 range is more realistic for quality work.

## Multi-Class vs Binary Agreement

Agreement is harder to achieve with more label categories.

Binary labels (good/bad): Easier to get high agreement. Clear distinction.

Three categories (good/okay/bad): Moderate agreement expected. The middle category is fuzzy.

Five categories (1-5 scale): Harder agreement. Distinguishing 3 from 4 is subjective.

Ten categories: Very hard. Expect lower Kappa.

If your agreement is low, consider whether you have too many label categories. Collapsing categories can help. Maybe combine 1-2 into "poor," 3 into "acceptable," 4-5 into "good."

You lose granularity but gain consistency. Often worth it.

## Reporting Agreement in Documentation

When you document your ground truth, report agreement metrics:

"Ground truth was created by three trained annotators. Inter-annotator agreement measured on a sample of 100 examples yielded Krippendorff's Alpha of 0.76 overall, with per-dimension values of: accuracy 0.88, completeness 0.71, tone 0.62. Examples with disagreement were resolved through annotator discussion and expert review."

This tells users of your ground truth:

How reliable it is.
Which dimensions are more vs less reliable.
That you've validated consistency.

It builds trust in your ground truth and shows you've done due diligence.

## The Bottom Line

Cohen's Kappa and Krippendorff's Alpha answer one critical question: are your annotators actually consistent, or are they just randomly agreeing sometimes?

Use these metrics to:

Validate that your ground truth is reliable before you build on it.
Identify which dimensions or example types need clearer rubrics.
Track annotator calibration over time.
Decide how many annotators you need per example.
Demonstrate rigor to stakeholders and regulators.

Aim for Kappa/Alpha above 0.6, ideally above 0.8. If you're below 0.6, don't trust your ground truth until you fix the consistency.

But remember: high agreement is necessary, not sufficient. You also need to validate that your annotators are consistently labeling things correctly, not consistently labeling things wrong.

In the final section of this chapter, we'll talk about protecting your ground truth from contamination — the ways that test data can leak, get gamed, or become stale, destroying the integrity of your evaluation system. Let me show you how to keep your ground truth clean and trustworthy over time.
