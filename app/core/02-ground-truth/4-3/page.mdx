# 4.3 — Existing Documentation as Ground Truth Source

Let me tell you about a discovery that happens in almost every ground truth project.

You're three weeks into building your evaluation dataset. You've done expert elicitation sessions. You've hand-crafted examples. You're making progress, but it's slow and expensive. Each new example requires careful thought and expert review.

Then someone on your team mentions: "Hey, didn't the compliance team write a hundred-page policy manual about exactly this stuff? And doesn't customer support have a knowledge base with like two thousand articles? And didn't we spend six months last year writing SOPs for the new product launch?"

You pull up the policy manual. Page after page of explicit rules about what agents should and shouldn't say. The knowledge base is full of approved answers to common questions. The SOPs document step-by-step procedures with examples.

You've been struggling to build ground truth from scratch while sitting on a mountain of existing ground truth. You just didn't recognize it as such.

This is not a stupid mistake. Documentation doesn't look like ground truth. Ground truth, in your mind, is labeled examples in a spreadsheet. Documentation is paragraphs of text in PDFs and Confluence pages. But here's the insight: documentation IS ground truth, just in a different format. It's explicit knowledge about how things should work. Your job is to mine it and convert it into testable form.

Let me walk you through how to do this systematically.

## Why Documentation Gets Ignored

Before we talk about mining documentation, let's understand why teams overlook it.

First, documentation is often scattered. You have policy manuals in one system, help articles in another, training materials in a third, SOPs in Google Docs, email threads with guidance, Slack conversations where decisions were made. No single place to look. So people don't look anywhere.

Second, documentation is written for humans reading sequentially, not for extraction. A policy might say "agents should use empathetic language when customers express frustration, but should remain professional and avoid overpromising." That's useful guidance for a human. But it's not in a form you can directly evaluate against. There's no clear pass/fail threshold. No examples of what "empathetic but professional" looks like.

Third, documentation has a reputation problem. Everyone knows it's outdated. The help articles reference features that were deprecated two years ago. The policy manual was written for a product that's been redesigned three times. The SOPs describe workflows that nobody follows anymore. So why would you trust it as ground truth?

These are real problems, but they're solvable. And the payoff for solving them is enormous. Let me show you how.

## The Documentation Audit

Start with an audit. Spend two days mapping every source of documented knowledge in your organization that's relevant to your AI system.

Make a list:

Policy manuals - formal rules about what's allowed and required
Help documentation - articles explaining features and answering questions
Training materials - onboarding guides for new employees
Standard operating procedures - step-by-step workflows
FAQ pages - common questions and approved answers
Compliance guides - legal and regulatory requirements
Style guides - tone, voice, formatting standards
Email templates - pre-written responses to common scenarios
Chat scripts - conversation flows for specific situations
Decision trees - logic for how to handle different cases
Past incident reports - documented failures and lessons learned
Regulatory filings - commitments made to regulators
Customer contracts - promises made to specific customers
Product requirements documents - specifications for how things should work

For each source, note:

- Where it lives
- When it was last updated
- Who owns it
- Whether it's actually followed in practice
- How much effort it would take to extract ground truth from it

You'll quickly see patterns. Some documentation is pristine and current. Some is abandoned. Some is critical for compliance. Some is aspirational nonsense nobody reads.

Prioritize sources that are: recently updated, widely referenced, tied to real consequences (legal, financial, regulatory), and aligned with how things actually work. Those are your gold mines.

## Converting Policies to Test Cases

Let's start with the easiest conversion: turning policy statements into test cases.

Here's a policy statement from a customer support manual: "Agents must not disclose other customers' information, even if the requesting customer claims to be associated with that account."

That's a clear rule. It's testable. You can convert it directly:

Test case input: Customer emails saying "I'm calling on behalf of my wife's account, can you tell me her current balance?"

Expected output: Agent politely explains they cannot disclose account information without verification, and provides instructions for the account holder to authorize information sharing.

Failure mode: Agent provides the balance without verification.

One policy statement becomes one or more test cases. If the policy is critical (legal, safety, financial), you create multiple test cases covering edge cases. "What if the customer says they're calling for their spouse? Their business partner? Their elderly parent? Their attorney?"

Go through your policy manuals page by page. Every "must," "must not," "should," and "should not" is a potential test case. Extract them systematically.

This is tedious work, but it's valuable. You're converting implicit organizational knowledge into explicit ground truth. And you're doing it in a way that's auditable — when someone asks "where did this test case come from?" you can point to the exact policy section.

## Mining Help Documentation

Help documentation is trickier because it's descriptive, not prescriptive. It tells you how something works, not how your AI should respond. But you can still extract ground truth.

Look for articles that explain complex topics clearly. Those become positive examples of how to explain those topics. If your help article about password reset is clear and well-structured, it shows you what a good password reset explanation looks like. Use it as a template.

Look for FAQs. Each FAQ is essentially a test case: given this question, here's the approved answer. You might need to adapt the answer for different contexts (chatbot vs email vs phone), but the content is ground truth.

Look for troubleshooting guides. These show you the diagnostic logic for common problems. "If the user sees error A, check B. If B is fine, check C." That diagnostic logic can become evaluation criteria for your AI. Is it asking the right diagnostic questions? Is it following the documented logic?

Look for examples and screenshots. Documentation often includes sample outputs, example emails, or annotated screenshots showing the correct way to do something. These are gold. They're concrete examples of correct behavior.

Here's a technique that works well: take ten high-quality help articles on different topics. Have a human or AI generate responses to those same topics without seeing the articles. Then compare. Where do the responses diverge from the documented approach? Those divergences might be improvements, or they might be errors. Either way, you've found something worth codifying in ground truth.

## Extracting Ground Truth from Training Materials

Training materials are often better than official documentation because they're written for teaching, not reference. They explain not just what to do but why. They include examples. They cover common mistakes.

If your organization onboards new support agents, sales reps, or product specialists, you have training materials. Find them.

Look for the examples used in training. "Here's a good response" vs "here's a bad response." Those are literally labeled examples — exactly what you need for ground truth.

Look for the rubrics used in training exercises. If trainers evaluate new employees using a rubric, that rubric is ground truth. It codifies what experienced people think quality looks like.

Look for the common mistakes section. Every good training material has a "here's what people get wrong" section. Those mistakes become negative examples in your test set.

Look for the role-playing scenarios. Training often includes practice exercises where new employees respond to simulated situations. The prompts are test inputs. The feedback given to trainees reveals the quality criteria.

If training materials include recorded sessions with expert feedback, those are treasure. You're seeing expert judgment in context, which is exactly what expert elicitation tries to extract. But it's already extracted and documented.

## Standard Operating Procedures as Ground Truth

SOPs are procedural documentation. "When X happens, do Y." They're often written for compliance or quality control. They're also often excellent sources of ground truth.

An SOP might say: "When a customer reports a billing error, first verify their identity, then pull up their transaction history, then review the charge with them line by line, then process a refund if the error is confirmed, then send a confirmation email with the reference number."

That's a complete specification of correct behavior. You can turn it into test cases:

- Does the AI verify identity before accessing account information?
- Does it review the specific charge with the customer?
- Does it process refunds only after confirmation?
- Does it provide a reference number?

Each step in the SOP is a criterion in your rubric.

The challenge with SOPs is they're often aspirational. The SOP says one thing, but people actually do it differently because the SOP is outdated or impractical. This is why you need to validate SOP-derived ground truth against actual practice.

Here's how: take ten real examples of people following the SOP process. Take ten examples where the process went well but didn't follow the SOP exactly. Compare them. If the non-SOP approaches consistently work better, maybe the SOP is wrong. If the SOP approaches work better, maybe people need to follow it more closely. Either way, you're identifying the true standard of quality.

## Style Guides and Brand Voice

If your organization has a style guide or brand voice document, it's a gold mine for tone-related ground truth.

A style guide might say: "We use conversational language, contractions, and active voice. We avoid jargon unless it's industry-standard. We're warm but not cutesy. Professional but not stiff. We use humor sparingly and never at the customer's expense."

That's a description of good tone. Now you need to operationalize it. Pull examples from your style guide that demonstrate these principles. Those become your positive examples. Create contrastive examples that violate each principle. Those become your negative examples.

For each style principle, define what violating it would look like:

- "Use conversational language" violated: overly formal, legal-sounding, corporate jargon
- "Warm but not cutesy" violated: infantilizing language, excessive emoji, forced enthusiasm
- "Humor sparingly" violated: jokes that distract from helping, sarcasm, cultural references that won't age well

These violations become test cases. You're not just testing whether your AI follows the style guide — you're testing whether it understands the boundaries.

## Compliance and Legal Documents

Compliance documentation is special because violations have real consequences. If your AI makes a promise that's illegal in California, you have legal liability. If it discloses information that violates GDPR, you get fined. If it gives medical advice without proper disclaimers, people get hurt.

Compliance documents tell you what you absolutely cannot do. These become hard constraints in your ground truth.

Pull every compliance requirement relevant to your domain:

- GDPR requirements for handling personal data
- TCPA rules for automated calling and texting
- HIPAA rules for healthcare information
- Financial regulations for advice or transactions
- Age restrictions for certain content
- Accessibility requirements for interfaces
- Industry-specific regulations

For each requirement, create test cases that would violate it. These are your "must never pass" examples. If your AI ever produces something that would trigger one of these test cases, it's an automatic failure regardless of other quality dimensions.

This is where ground truth becomes a safety mechanism, not just a quality mechanism. You're encoding legal and ethical boundaries that cannot be crossed.

## The Staleness Problem

Here's the hard truth about documentation-derived ground truth: it gets stale.

Your help articles reference features that don't exist anymore. Your policies were written for a different business model. Your training materials assume workflows that have changed. Your style guide predates a rebrand.

Stale documentation is worse than no documentation because it gives you false confidence. You think you're building ground truth grounded in organizational knowledge, but you're actually building ground truth grounded in how things used to work.

You must validate currency. For every piece of documentation you use as ground truth:

- Check the last-updated date
- Ask the owner if it's still accurate
- Test it against current practice
- Look for contradictions with other sources

If you find stale documentation, you have three options:

Option one: Fix the documentation. This is the right move if the documentation is important and widely used. Update it, get it reviewed, then use it as ground truth.

Option two: Use it anyway but flag it. Sometimes documentation is slightly outdated but still mostly correct. Use it as ground truth but add metadata: "derived from 2024 policy manual, may need review." This lets you benefit from it while staying honest about limitations.

Option three: Discard it. If the documentation is seriously outdated and nobody follows it, don't use it. You're better off building ground truth from scratch than building it on false foundations.

## Converting Unstructured Text to Structured Ground Truth

Most documentation is unstructured prose. Ground truth needs structure. Here's the conversion process.

Step one: Extract principles. Read through a document and pull out every sentence that states a rule, guideline, standard, or example. These are your raw materials.

Step two: Classify each principle. Is it a hard constraint (must always be followed), a strong guideline (should be followed unless there's a good reason), or a suggestion (good to follow but not required)? This classification determines how you use it in evaluation.

Step three: Operationalize it. Turn the principle into something testable. "Agents should be empathetic" becomes "Does the response acknowledge the customer's emotion?" with clear examples of what that looks like.

Step four: Create examples. For each operationalized principle, create at least two examples: one that clearly follows it and one that clearly violates it. Ideally create edge cases too.

Step five: Link back to source. Every ground truth record should include a citation to the original documentation. This creates an audit trail and lets you update ground truth when documentation changes.

This conversion is labor-intensive. You cannot fully automate it. But you can use AI to help. Give a language model a policy document and ask it to extract testable rules. The output will be imperfect, but it's a useful first pass that humans can refine.

## Building a Documentation-to-Ground-Truth Pipeline

If you have hundreds of pages of documentation, you need a systematic pipeline, not ad-hoc extraction.

Here's a pipeline that works:

Stage one: Document inventory. List all documentation sources, prioritize them, assign owners.

Stage two: Automated extraction. Use a language model to scan documents and extract potential ground truth statements. This gives you a rough draft.

Stage three: Human review. Subject matter experts review the extracted statements, fix errors, add context, mark staleness.

Stage four: Test case generation. For each validated statement, create one or more test cases with inputs and expected outputs.

Stage five: Cross-reference. Check for conflicts between different sources. Resolve them or flag them as ambiguous.

Stage six: Validation. Test the ground truth against real examples. Do your derived test cases actually differentiate good from bad in practice?

Stage seven: Documentation sync. Set up a process so that when documentation changes, the derived ground truth gets updated. This prevents drift.

This pipeline should be run initially and then periodically. Documentation changes. Ground truth needs to track those changes.

## When Documentation Contradicts Reality

Here's a scenario that happens constantly. The documentation says agents should always close tickets within twenty-four hours. But when you analyze actual tickets, your best agents often leave tickets open for days while working through complex issues. The ones who rush to close tickets in twenty-four hours have lower customer satisfaction.

The documentation is wrong. Or at least, it's optimizing for the wrong metric. What do you use as ground truth?

The answer is: reality wins, but document the contradiction.

Your ground truth should reflect what actually works, not what the handbook says should work. If keeping tickets open longer leads to better outcomes, that's your ground truth. But you document: "Ground truth contradicts SOP section 4.2. SOP prioritizes speed, actual performance data prioritizes resolution quality."

This documentation serves two purposes. First, it flags that the SOP needs updating. Second, it explains to future reviewers why your ground truth diverges from official policy.

Sometimes the contradiction goes the other way. The documentation says one thing, people do another thing, and the documentation is actually right. People have drifted into bad habits or shortcuts that compromise quality. In that case, use the documentation as ground truth and use evaluation to pull practice back toward the standard.

The key is: investigate contradictions, don't ignore them. When documentation and reality diverge, there's always a reason. Understanding that reason makes your ground truth stronger.

## Documentation as Living Ground Truth

The best organizations treat documentation and ground truth as connected systems. When documentation updates, ground truth updates. When ground truth reveals a gap, documentation gets written.

Set up bidirectional linking:

- Every ground truth record cites source documentation
- Every piece of documentation notes which test cases derive from it
- When documentation changes, affected test cases get reviewed
- When test cases reveal ambiguity, documentation gets clarified

This creates a virtuous cycle. Ground truth makes documentation testable. Documentation makes ground truth auditable. Both improve together.

In 2026, tools like Notion, Confluence, and Glean have features for this. You can tag documentation sections and auto-track which ground truth records derive from them. When a tagged section updates, you get notifications to review derived test cases. This infrastructure makes documentation-grounded ground truth sustainable at scale.

## What Documentation Can't Give You

Let me be clear about the limits. Documentation-derived ground truth is excellent for explicit rules, policies, and procedures. It's much weaker for subjective quality, edge cases, and implicit knowledge.

Documentation will tell you that agents must verify identity. It won't tell you exactly how warm vs professional the tone should be in different situations. That requires expert elicitation or customer feedback analysis.

Documentation will cover the common cases. It won't cover the weird edge cases that happen once a month. Those require building ground truth from real examples.

Documentation reflects past decisions. It won't help you with new features, new risks, or new use cases that didn't exist when it was written. Those require creating ground truth from scratch.

Think of documentation as one source among several. It's often your richest single source, especially in established organizations. But it's not sufficient alone.

## Your Documentation Mining Checklist

If you're about to mine your organization's documentation for ground truth, here's your checklist:

Week one: Audit. List every documentation source, note last update, assess relevance.

Week two: Prioritize. Pick the top five sources that are current, important, and rich in testable statements.

Week three: Extract. Go through those sources and pull out every rule, principle, and example that could become ground truth.

Week four: Structure. Convert the extracted items into test cases with inputs, expected outputs, and pass/fail criteria.

Week five: Validate. Check a sample of documentation-derived ground truth against real examples. Fix contradictions.

Week six: Integrate. Merge documentation-derived ground truth with your existing examples from expert elicitation and hand-crafted creation.

By the end of six weeks, you should have hundreds of ground truth test cases extracted from documentation, validated, and ready to use.

This is not glamorous work. It's archaeology. You're digging through layers of organizational knowledge to find the valuable pieces and bring them into the light. But the ROI is massive. You're getting ground truth for the cost of reading and structuring existing knowledge instead of building it from scratch.

In the next section, we'll look at another underutilized source: your customers themselves. Every support ticket, every feature request, every complaint and compliment is implicit ground truth about what's working and what's not. Let me show you how to systematically extract quality signal from customer feedback.
