# 5.8 — Documentation That Survives Team Turnover

The AI team's top performer gave her two weeks' notice. She'd been the ground truth steward for eighteen months, had run all the alignment workshops, created most of the evaluation datasets, and was the go-to person for any quality question.

The team panicked. Her knowledge about why certain decisions were made, how to handle edge cases, what the tradeoffs were — all of that was in her head.

They asked her to document everything before she left. She spent her last week writing up what she could, but it was impossible. Eighteen months of context, hundreds of nuanced decisions, countless edge cases — you can't capture all of that in a week.

Three months after she left, the team was struggling. They had her documentation, but it didn't answer the questions they actually faced. "Why did we decide that disclaimers should appear in this scenario but not that scenario?" The documentation said what the standard was, but not why.

They found themselves reverting to the old dysfunction: relitigating decisions that had already been made, inconsistent quality standards, stakeholders talking past each other.

This is the bus factor problem. If one person gets hit by a bus (or just takes a new job), how much of your ground truth knowledge disappears?

Let me show you how to build documentation that actually survives turnover.

## Why Traditional Documentation Fails

Most documentation captures what, not why. "Responses about medical conditions should include a disclaimer." Okay, but why? Under what circumstances? What edge cases did we debate? What was the trade-off we accepted?

Without the why, the what is fragile. Someone new comes in, doesn't understand the rationale, and either applies the standard incorrectly or questions it and opens up debates that were already settled.

Traditional documentation also fails because it's:

Too dense. Hundred-page quality manuals that nobody reads.

Too abstract. General principles without concrete examples.

Too static. Written once, never updated, gradually becoming less relevant.

Too separated from workflow. The documentation lives in a wiki nobody checks, not embedded in the actual work.

Too focused on happy path. Edge cases and judgment calls are where the real knowledge lives, but they're often not documented.

You need documentation that's concise, concrete, living, embedded, and edge-case-focused.

## The Decision Rationale Pattern

Here's the single most important documentation pattern: always capture why, not just what.

For every significant ground truth decision, document:

The decision: "Medical responses about prescription medications must include information about common side effects."

The context: "We were seeing users take medications without understanding side effects, leading to adverse reactions and support tickets."

The alternatives considered: "We considered: one) no side effect information (too risky), two) full clinical side effect list (too overwhelming), three) common side effects only (chosen approach), four) side effects only if user asks (too late)."

The trade-offs: "We're accepting that responses are longer and users might not always read the side effect info. We decided this was better than the risk of users not knowing about side effects."

The edge cases: "For over-the-counter medications with very rare side effects, we don't include them. For prescription medications with serious common side effects (like blood thinners), we prominently highlight them."

Now when someone new looks at this, they understand not just the rule, but the reasoning. They can apply the principle to new situations. They know what trade-offs were accepted and can revisit them if circumstances change.

## Example Libraries with Annotations

Ground truth is concrete. The best documentation shows examples.

Build example libraries that include:

Good examples: "This is what a high-quality response looks like in this category."

Bad examples: "This is what a failing response looks like and why it fails."

Borderline examples: "This is a tricky case. Here's how we decided to handle it and why."

For each example, include annotations:

"This example is good because it balances directness with necessary caveats, includes a relevant disclaimer without being overwhelming, and uses plain language."

"This example fails because it's technically accurate but misses the user's actual question."

"This borderline example was debated extensively. Product wanted shorter, medical experts wanted more detail. We landed on this version because it includes the critical safety information while remaining scannable."

The annotations are where the knowledge lives. They capture the reasoning, the debates, the trade-offs.

One team I worked with created a "quality gallery" — a visual library of ground truth examples with rich annotations. New team members spent their first week studying the gallery, and it gave them context that would have taken months to absorb otherwise.

## Video Walkthroughs for Complex Judgments

Some ground truth decisions are too nuanced to capture in writing. For these, record video walkthroughs.

Have the person who understands the domain walk through examples, talking through their reasoning: "Looking at this response, I'm checking for accuracy, helpfulness, and safety. The accuracy is good — this matches clinical guidelines. The helpfulness is borderline — it answers the question but doesn't anticipate the obvious follow-up. The safety is good — appropriate disclaimers. Overall, I'd rate this as passing but not great."

These videos become onboarding gold. New team members can watch the expert's thought process, hear the reasoning, see how they handle ambiguity.

You don't need fancy production. A screen recording with voiceover works fine. Fifteen to twenty minutes per major quality dimension is enough.

Update the videos when ground truth changes. Archive old videos so you can see how thinking evolved.

## Onboarding Materials for New Team Members

When someone new joins, what do they need to understand ground truth?

Create a structured onboarding path:

Day one: Overview of the quality framework. What dimensions do we evaluate? What's the hierarchy when dimensions conflict?

Day two: Example gallery walkthrough. Look at twenty to thirty examples across different categories, read the annotations, start building intuition.

Day three: Watch video walkthroughs. See experts evaluating real examples.

Day four: Practice evaluation. Rate twenty examples independently, then compare ratings with an experienced team member and discuss differences.

Day five: Attend a calibration session. Join the team for a regular calibration meeting where you evaluate new examples together.

Week two: Shadow the ground truth steward. See how ground truth is created, maintained, and used in practice.

Week three: Start contributing. Create ground truth examples for a small, well-defined area with mentorship.

This structured path gets new people up to speed in weeks instead of months.

## The Bus Factor Test

Here's how to assess whether your documentation is good enough: the bus factor test.

Imagine your most knowledgeable person about ground truth leaves tomorrow. Can the team continue operating effectively?

Can someone new understand the ground truth framework? (If yes, your overview documentation is good.)

Can they evaluate examples consistently with how the team would evaluate them? (If yes, your example library and annotations are good.)

Can they understand why decisions were made? (If yes, your decision rationale documentation is good.)

Can they handle edge cases? (If yes, your edge case documentation is good.)

Can they onboard the next new person? (If yes, your onboarding materials are good.)

If the answer to any of these is no, you have documentation gaps.

Run this test periodically. Have someone who wasn't involved in ground truth creation try to use the documentation and give feedback on what's missing or confusing.

## Living Documentation vs Dead Wikis

I've seen countless teams create comprehensive ground truth documentation, then never touch it again. Six months later, it's outdated and nobody trusts it.

The documentation is dead, and dead documentation is worse than no documentation because people waste time reading it and getting wrong information.

Living documentation is:

Actively maintained. When ground truth changes, documentation is updated simultaneously, not "we'll update the docs later."

Regularly reviewed. Quarterly, someone reads through the documentation and fixes inaccuracies, adds new examples, removes outdated information.

Clearly versioned and dated. You can see when it was last updated and know if it's current.

Easy to update. If updating documentation is painful, it won't get done. Use tools that make updates easy.

Integrated into workflow. Documentation isn't a separate artifact — it's part of the systems people use daily.

Here's a pattern that works: embed documentation into your evaluation tooling. When someone's rating examples, relevant documentation appears inline. When someone's creating new ground truth, templates guide them through documenting the rationale.

This makes documentation part of the work, not extra work.

## The Knowledge Transfer Session

When someone who holds critical ground truth knowledge is leaving, do a formal knowledge transfer session. Don't just ask them to write documentation — have structured conversations.

Bring together the person leaving and the people who'll carry the work forward. Spend half a day walking through:

The most common questions you get about ground truth and how you answer them.

The edge cases that caused the most debate and how they were resolved.

The stakeholder dynamics and how to navigate them.

The things you wish you'd known when you started.

The things that are documented but people miss or misinterpret.

The things you haven't documented but should.

Record the session. Take notes. Turn the conversation into documentation.

This doesn't replace written documentation, but it captures context and nuance that's hard to write down.

## The FAQ Pattern

As you build ground truth, you'll get the same questions repeatedly. Document them as FAQ.

Q: When do we include disclaimers on medical advice?
A: Always for prescription medications and serious conditions. Rarely for general wellness information. See examples: [links]

Q: How do we handle responses that are technically accurate but not helpful?
A: We mark them as failing on the helpfulness dimension. Accuracy is necessary but not sufficient. See examples: [links]

Q: Who makes the final call when stakeholders disagree on a quality judgment?
A: Depends on the dimension. See our RACI matrix: [link]

The FAQ becomes a living document. Every time someone asks a new question, add it to the FAQ with the answer.

This is particularly valuable for onboarding. New team members can read through the FAQ and get answers to questions they didn't know they had.

## Version Control and Change Logs

I mentioned this in the previous chapter, but it's worth emphasizing: version control is documentation.

When you update ground truth, the change log tells the story:

Version 2.1 to 2.2 (March 2026):
- Added examples for handling pediatric medical questions (why: we're expanding to serve parents)
- Updated disclaimer requirements to comply with new EU regulations (why: regulatory change)
- Clarified standards for urgency escalation (why: we had three cases of missed escalations)

The change log documents what changed, but also why and when. This is historical context that's invaluable for understanding the current state.

## Calibration Sessions as Documentation

Regular calibration sessions (where the team evaluates examples together) serve a dual purpose: they keep people aligned and they create documentation.

Record calibration sessions. The discussions are rich with reasoning and context.

Document interesting cases from calibration. "We spent twenty minutes debating this example. Here's why it was tricky and how we resolved it."

New team members can watch recorded calibration sessions to see how the team thinks about quality.

Calibration isn't just about staying aligned — it's about creating living documentation of how quality judgments are made.

## What to Document at Different Scales

The documentation burden should scale with your team and product.

Early stage (one product, small team):

- Core quality framework (one-pager)
- Annotated example library (twenty to thirty examples)
- Decision log (why we made key decisions)
- Simple onboarding guide

Growth stage (multiple products, medium team):

- Comprehensive quality framework
- Large example library (hundreds of examples, well-organized)
- Detailed decision rationale for major choices
- Video walkthroughs for complex areas
- Structured onboarding program
- FAQ
- Regular calibration sessions

Mature stage (many products, large team):

- All of the above, plus:
- Product-specific documentation with central framework
- Specialized documentation for different domains
- Formal training programs
- Dedicated documentation steward role
- Automated documentation tools

Don't over-document early or under-document late. Scale the documentation with your needs.

## The Quarterly Documentation Review

Set a recurring calendar event: quarterly documentation review.

Bring together the ground truth steward and a few team members. Spend two hours reviewing all documentation:

Is it accurate? Has anything changed that makes this outdated?

Is it complete? Are there gaps we need to fill?

Is it clear? Are there parts that people misinterpret?

Is it used? Are there documents nobody looks at? (If so, either improve them or archive them.)

What questions came up this quarter that aren't answered in documentation?

Fix issues immediately or create tasks to fix them. Don't let documentation rot.

## Tooling for Documentation

The right tools make documentation easier.

Knowledge base tools: Notion, Confluence, GitBook — choose something your team actually uses.

Example management tools: Some teams build custom tools for managing and annotating examples. Airtable or spreadsheets work for small scale.

Video tools: Loom for quick screen recordings, longer videos for complex walkthroughs.

Version control: Git for datasets and code, whatever your knowledge base tool supports for documents.

Collaborative docs: Google Docs, Microsoft Office, whatever enables easy collaboration.

The best tool is the one your team will actually use consistently. Don't over-optimize for the perfect tool.

## What Good Documentation Looks Like

Let me show you a team that got this right.

They built a content moderation AI. Complex domain, high stakes, lots of edge cases. Strong documentation was essential.

They had:

A twenty-page quality framework document: clear, well-organized, updated quarterly.

An example library with over three hundred annotated examples, organized by category and searchable.

Video walkthroughs: fifteen videos, fifteen to twenty minutes each, covering different moderation categories.

A detailed FAQ: seventy-five questions and answers, continuously updated.

Decision log: every significant ground truth decision documented with full context.

Onboarding program: two-week structured path with clear milestones.

Regular calibration: weekly sessions, recorded, key discussions documented.

Quarterly reviews: documentation reviewed and updated every quarter.

When their lead moderator left after three years, the new lead was fully productive within a month. The documentation worked.

When they expanded to a new country with different cultural context, they were able to adapt the framework because they understood the reasoning behind every decision.

When they had a regulatory audit, they could produce comprehensive documentation of their quality standards and how they evolved.

Documentation isn't glamorous, but it's foundational.

## What's Next

You've now got documentation that survives turnover. But documentation doesn't prevent disagreements — it just gives you shared context for resolving them.

In the next subchapter, we're diving into the adjudication workflow: what happens when two domain experts look at the same output and disagree? How do you resolve those disagreements systematically? When do you use majority vote versus expert override? How do you maintain consistency across evaluators?

Good documentation gives you the foundation. Good adjudication processes ensure you build consistently on that foundation. Let's talk about how to handle expert disagreements without creating chaos.
