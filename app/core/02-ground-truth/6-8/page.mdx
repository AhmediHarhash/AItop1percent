# 6.8 — Creative & Generative Tasks

Let me tell you about the marketing campaign that tested perfectly and bombed completely. A company used AI to generate email subject lines for a product launch. They evaluated the AI's outputs using their standard ground truth: click-through rate predictions based on historical data. The AI generated subject lines that the model predicted would perform excellently. They sent them to 100,000 customers.

The click rate was 0.3%, far below the predicted 12%. What went wrong?

The AI had optimized for patterns from past campaigns: urgency, scarcity, personalization. But it had no concept of whether the subject lines were actually interesting, surprising, or appropriate for this specific product. "URGENT: Your last chance expires in 2 hours!" works for flash sales, not for a thoughtful product announcement. The subject lines technically matched successful patterns but felt completely wrong in context.

This is the creative ground truth challenge: there's no formula for "good." You can't measure creativity with a test suite. You can't define quality with a rubric alone. Human judgment is the only real measure, and humans disagree.

## Why Creative Evaluation Is Different

For most AI tasks, there's an objectively correct answer. A question has a factual answer. Code either works or it doesn't. Classification categories are defined.

But creative tasks are fundamentally different. Ask three people if a poem is good and you might get three different answers. All valid. This is not a bug, it's the nature of creativity.

Your ground truth framework needs to embrace subjectivity while still providing useful signal for improving your AI.

## The Dimensions of Creative Quality

Creative work can't be reduced to a single score, but it can be evaluated across multiple dimensions. Here's a framework that works:

**Originality**

Is it novel and fresh, or generic and derivative?

This is tricky because you want originality but not randomness. A completely nonsensical poem is original but not good. A cliché-filled poem is coherent but not original.

Ground truth evaluation: Rate on a scale from "generic/clichéd" to "fresh/novel." Include examples of each end of the spectrum in your labeling guidelines.

**Relevance**

Does it match the brief and context?

If someone asked for "a professional email to a client explaining a delay," a beautifully written poem about the passage of time is original but completely irrelevant.

Ground truth evaluation: Does the output address the request? Is it appropriate for the stated audience and purpose?

**Coherence**

Does it make sense? Is it internally consistent?

Creative doesn't mean random. A marketing slogan should be coherent even if it's creative. A story should have logical flow even if it's fantastical.

Ground truth evaluation: Can you follow the logic/narrative? Are there contradictions or confusing elements?

**Engagement**

Would someone actually want to read/use this?

This is the "so what?" test. Lots of AI-generated content is technically fine but utterly boring. No grammar errors, no factual mistakes, just completely uninteresting.

Ground truth evaluation: Rate how compelling/engaging/interesting this is. Would you keep reading? Would you click this headline? Would you use this tagline?

**Style Match**

Does it match the requested tone, voice, and style?

Asking for "casual and friendly" should not yield formal corporate speak. Asking for "professional and authoritative" should not yield slang.

Ground truth evaluation: Does the style match the request? Include specific style dimensions: formality, emotion, voice (first person / third person), length, complexity.

**Technical Quality**

Even creative content has technical requirements: grammar, spelling, readability.

Ground truth evaluation: Is it technically correct? Are there errors that would undermine credibility?

Your ground truth should evaluate creative work across all these dimensions, not reduce it to a single "good/bad" judgment.

## Rubric-Based Evaluation

A rubric is a structured scoring guide. It's the primary tool for creative ground truth because it turns subjective judgment into somewhat-consistent scoring.

Here's an example rubric for evaluating AI-generated marketing copy:

**Relevance (0-3 points)**
- 0: Does not address the prompt or is completely off-topic
- 1: Addresses the prompt but misses key requirements
- 2: Addresses the prompt and most requirements
- 3: Fully addresses the prompt and all requirements

**Engagement (0-3 points)**
- 0: Boring, generic, or confusing
- 1: Acceptable but unmemorable
- 2: Interesting and engaging
- 3: Compelling and memorable

**Originality (0-2 points)**
- 0: Entirely generic or clichéd
- 1: Some fresh elements mixed with common patterns
- 2: Notably original and fresh

**Style Appropriateness (0-2 points)**
- 0: Wrong tone/voice for the context
- 1: Acceptable tone/voice with some mismatches
- 2: Perfect tone/voice for the context

**Technical Quality (0-2 points)**
- 0: Multiple errors that undermine quality
- 1: Minor errors or awkward phrasing
- 2: Polished and error-free

Total: 12 points possible

This rubric turns "is this good?" into a structured evaluation. Labelers can disagree on whether something deserves a 2 or 3 on engagement, but they're thinking about the same question.

Your rubric should:
- Be specific to your creative task
- Have clear criteria for each score
- Include examples of each score level
- Be tested with real labelers and refined based on their feedback

## The "Would a Human Choose This?" Test

Here's a powerful ground truth technique: preference ranking.

Instead of asking "is this good?" (hard to answer consistently), ask "which of these would you choose?" (easier to answer consistently).

Show labelers:
- The AI-generated output
- One or more human-written alternatives (or outputs from different models)

Ask: "Which would you choose for this use case?"

This gives you:
- Comparative quality (is AI better than human, or worse?)
- Clearer signal (preference is easier to judge than absolute quality)
- Realistic benchmarking (you care whether AI is good enough to use, not whether it's perfect)

Ground truth becomes: "For what percentage of tasks would humans prefer AI output over human output?"

This is more actionable than "average quality score: 7.3 out of 10."

## The Creativity-Safety Tension

Here's a fundamental tension in creative AI: creativity requires risk-taking, but risk-taking increases the chance of inappropriate outputs.

Safe, conservative outputs are boring. Creative, surprising outputs occasionally cross lines.

Your ground truth needs to navigate this tension.

**Define Hard Boundaries**

Some things are never acceptable:
- Offensive, hateful, or discriminatory content
- Plagiarism or copyright violation
- Factual misinformation (in contexts where facts matter)
- Violations of brand guidelines or legal requirements

Ground truth: Automatic failure if any hard boundary is crossed, regardless of creativity.

**Define Soft Preferences**

Within boundaries, prefer:
- Fresh over generic
- Surprising over predictable
- Distinctive over bland

Ground truth: Higher scores for creative risk-taking that stays within boundaries.

The key: be very clear about boundaries so the AI can be creative within safe space.

## When "Surprising" Is Good vs Bad

Surprise is a core element of creativity. But not all surprises are good.

**Good Surprise**

An unexpected metaphor that illuminates the topic. A clever wordplay that makes you smile. A perspective you hadn't considered.

**Bad Surprise**

A non-sequitur that confuses the message. A tone shift that feels jarring. An inappropriate reference that undermines the content.

Your ground truth needs to distinguish:

**Surprising and Effective**: Creative in a way that enhances the content

**Surprising and Ineffective**: Creative in a way that distracts or confuses

**Unsurprising and Effective**: Safe but works well

**Unsurprising and Ineffective**: Generic and boring

The best creative outputs are in the first category. The worst are in the fourth. The middle two are judgment calls based on context.

## Context-Dependent Quality

Creative quality is deeply context-dependent. A joke is great in casual content, terrible in a legal document. Emotional language is good for storytelling, bad for technical documentation.

Your ground truth must include context:

**Audience**: Who will read/use this? (executives, consumers, children, experts)

**Purpose**: What should this achieve? (entertain, persuade, inform, inspire)

**Medium**: Where will this appear? (social media, email, website, print ad)

**Constraints**: What are the requirements? (length, tone, keywords, format)

The same output can be excellent in one context and terrible in another. Ground truth should always evaluate appropriateness for the specific context.

## Handling Subjectivity and Disagreement

Humans will disagree on creative quality. This is expected. How do you handle it?

**Multiple Labelers**

Have each output rated by at least 3 labelers (5 is better for high-subjectivity tasks).

**Disagreement Metrics**

Measure how much labelers disagree. High disagreement means:
- The output is genuinely ambiguous (some will like it, some won't)
- Your rubric needs clearer criteria
- You need labelers with more aligned taste

**Consensus Thresholds**

Decide how to handle disagreement:
- Majority vote: Use the most common rating
- Average: Use the mean score across labelers
- High agreement only: Only include items where labelers mostly agree
- Modeling disagreement: Keep the full distribution (3 labelers rated 8/10, 2 rated 6/10)

**Expert Adjudication**

For important or disputed cases, have a senior reviewer (domain expert, creative director) make the final call.

Don't try to eliminate disagreement. It's inherent to creative work. Just measure it and account for it.

## Domain Experts vs General Labelers

Who should evaluate creative outputs? This matters enormously.

**General Labelers**

Pros: Cheaper, easier to scale, represent general audience perspective
Cons: Miss domain-specific quality signals, inconsistent expertise

Use when: The audience is general (consumer content) and basic quality checks suffice

**Domain Experts**

Pros: Understand subtle quality signals, provide reliable judgments, catch domain-specific issues
Cons: Expensive, harder to find, may not represent broader audience

Use when: The task requires expertise (legal writing, medical content, technical documentation, brand-specific content)

**Hybrid Approach**

Use general labelers for initial screening (filter out obviously bad outputs) and experts for final evaluation.

Or use general labelers to represent audience reaction and experts to verify correctness and appropriateness.

## The Benchmark Content Approach

One powerful ground truth technique: include known high-quality examples in your eval set.

Collect:
- 50 AI-generated outputs
- 25 human-written outputs you know are excellent
- 25 human-written outputs you know are mediocre

Mix them together and have labelers rate them without knowing which is which.

This gives you:
- Calibration: Can labelers distinguish good from mediocre human content?
- Benchmarking: How does AI compare to good and mediocre human content?
- Validation: If labelers rate known-excellent content poorly, your rubric needs work

If your labelers rate excellent human content at 6/10 and mediocre human content at 7/10, your rubric isn't working.

## Evaluating Variations and Iterations

Creative work often requires generating multiple options and selecting the best. Your ground truth should test this.

Ask the AI to generate 5 variations of a headline. Your ground truth evaluates:

**Range**: Do the variations actually differ, or are they all slight rewordings?

**Quality Distribution**: Is at least one variation good, or are all mediocre?

**Best-of-N Selection**: If a human picks the best from the set, is it better than a single random generation?

This matters because in practice, creative AI is often used to generate options for humans to choose from, not to generate a single final output.

## The Style Transfer Challenge

Sometimes creative tasks involve matching an existing style. "Write in the style of Hemingway." "Generate a product description matching our brand voice."

Ground truth for style transfer is tricky because you need to evaluate both:

**Style Match**: Does it sound like the target style?

**Content Quality**: Is it good, or just a hollow imitation?

Bad outputs: Generic content with superficial style markers (using short sentences doesn't make it Hemingway)

Good outputs: Captures the essence of the style while delivering quality content

Your ground truth should include examples of the target style and evaluate how well the AI captures both surface features (sentence length, vocabulary) and deeper patterns (themes, tone, perspective).

## Temporal and Cultural Context

Creative quality isn't static. What was creative in 2020 might be cliché in 2026. What's funny in one culture might be offensive in another.

Your ground truth needs to be:

**Temporally Current**: Use recent examples, update your rubric as trends change, retire stale examples

**Culturally Appropriate**: Include diverse cultural perspectives in your labeling team, test for cultural sensitivity

This is especially important for marketing, humor, and pop culture references.

## The "Too Creative" Problem

Yes, this is a thing. Sometimes AI outputs are so creative they're unusable.

A product description that's a surrealist poem might be highly creative but completely inappropriate for an e-commerce site.

Your ground truth should penalize excessive creativity that undermines utility:

**Creativity Within Bounds**: Fresh and interesting while still serving the purpose

**Creativity Beyond Bounds**: So creative it no longer functions for the intended use

The rubric should reward the first and penalize the second.

## A/B Testing as Ground Truth

In production, creative outputs can be evaluated by real user behavior:
- Email subject lines by open rates
- Headlines by click rates
- Product descriptions by conversion rates
- Ad copy by engagement metrics

This is ultimate ground truth: what actually works with real users.

Use A/B testing to validate your labeler-based ground truth. If your labelers rate something highly but users don't engage with it, your rubric is misaligned with real-world success.

Conversely, if something tests well with users but labelers rate it poorly, your labelers might be applying the wrong criteria.

## Evaluating Long-Form Creative Content

Short content (headlines, taglines) is relatively easy to evaluate. Long content (articles, stories, reports) is much harder.

For long content, your ground truth needs to check:

**Structure**: Is it well-organized with clear sections and flow?

**Coherence**: Does it maintain consistency throughout?

**Engagement**: Does it maintain interest across the full length?

**Completeness**: Does it cover the topic thoroughly?

**Pacing**: Does it move at an appropriate speed (not rushed, not dragging)?

This requires significant labeler time. A 2000-word article takes 10-15 minutes to read and evaluate properly.

Consider hierarchical evaluation:
- Quick scan for fatal flaws (immediate rejection if found)
- Detailed read for quality assessment (if it passes initial scan)

## The Warning: What Happens If You Skip This

If you evaluate creative AI outputs only on technical correctness (grammar, factual accuracy) without considering engagement, originality, and appropriateness, here's what happens:

Your AI will generate technically perfect, completely boring content. It will be correct but generic. Safe but forgettable. It will sound like AI.

Users will find it bland and corporate. Your content will perform poorly compared to human-written alternatives. Your brand voice will become homogenized and lifeless.

Worse, you won't be able to improve because you're not measuring the dimensions that actually matter for creative work.

I've seen companies deploy AI-generated content that passed all their quality checks but got zero engagement. It was grammatically perfect, factually accurate, and completely uninteresting. Conversion rates dropped. They had to revert to human writers.

Creative AI needs creative evaluation. Rubrics, human judgment, preference ranking, and real-world testing. There's no shortcut.

If your ground truth doesn't include "would a human actually want to read this?" then you're missing the point of creative content.

## Bridge to Multi-Modal Systems

We've been talking about evaluating text—creative text, but still just text. But increasingly, AI systems work across multiple modalities: text and images, text and audio, text and video. A system might generate an image from a text description, or caption a photo, or create a video with narration. Ground truth gets exponentially more complex when you're not evaluating one type of output, but the interaction between multiple types. How do you verify that a generated image actually matches the text prompt? How do you check if a caption accurately describes a photo? Let's walk through the unique challenges of multi-modal ground truth.
