# 6.2 — RAG & Knowledge Systems

Let me tell you about the day a legal research AI almost destroyed a law firm's credibility. They had built a RAG system that could answer questions about case law by searching through thousands of legal documents. One day, it confidently cited a case that didn't exist. The lawyer used it in a court filing. The judge was not amused.

The problem? The ground truth only checked if the answer sounded correct. Nobody verified that the sources were real, that the information was actually in those sources, or that the citations were accurate. The model had hallucinated a case name that sounded plausible, and it sailed right through their evaluation.

This is the RAG ground truth nightmare: you're not evaluating one thing, you're evaluating a pipeline of four distinct things, and any one of them can fail independently. A RAG system can retrieve the wrong documents but generate a great answer (by hallucinating). It can retrieve the right documents but extract the wrong information. It can get everything right but cite it incorrectly. Your ground truth needs to catch all of these failure modes.

## What Makes RAG Different

Traditional chatbots generate responses from their training data and parameters. RAG systems are different: they're supposed to look up information and base their answers on what they find. This "grounding" is the entire point, and it's what makes ground truth so much more complex.

Think of it like the difference between a student taking a test from memory versus taking an open-book test. With memory, you just check if the answer is right. With open-book, you need to check: did they look up the right information? Did they interpret it correctly? Did they cite their sources? Did they combine information from multiple sources appropriately?

Every one of those steps can fail independently, and your ground truth needs to verify each one.

## The Four Stages of RAG Ground Truth

Let's break down a RAG system into its components, because your ground truth needs to cover each one.

**Stage 1: Retrieval**

The system searches for relevant documents or passages. Ground truth question: did it retrieve the RIGHT stuff?

**Stage 2: Extraction**

The system reads the retrieved documents and extracts relevant information. Ground truth question: did it understand the documents correctly?

**Stage 3: Generation**

The system generates a response based on the extracted information. Ground truth question: is the response faithful to the source material?

**Stage 4: Citation**

The system attributes information to sources. Ground truth question: are the citations accurate and complete?

Most teams only evaluate Stage 3 (is the final answer good?). Elite teams evaluate all four stages independently. Here's why that matters.

## Stage 1 Ground Truth: Retrieval Relevance

Before the model can answer anything, it needs to find the right documents. This is the retrieval step, and it's more nuanced than you might think.

"Relevant" doesn't just mean "contains the keywords." It means "actually helps answer the user's question."

Imagine a user asks: "What's our company's parental leave policy for adoptive parents?"

The system might retrieve:
- Document A: General parental leave policy (relevant)
- Document B: Adoption assistance program details (relevant)
- Document C: History of how our parental leave policy changed over time (somewhat relevant)
- Document D: Someone's blog post mentioning parental leave (not relevant)

Your ground truth needs to label which documents are actually useful for answering this specific question. This is harder than it sounds because relevance is contextual.

Here's a real example that tripped up a team I worked with. User question: "How do I reset my password?" The system retrieved a document titled "Password Reset Procedure" (obviously relevant) but also retrieved "Password Policy" (which explains requirements for new passwords). Is the policy document relevant?

Depends. If the user is locked out right now, they need the procedure, not the policy. But if the password reset is failing because their new password doesn't meet requirements, the policy is crucial. Context matters.

Your ground truth for retrieval should include:

**Binary Relevance**: Is this document relevant at all to the query? (yes/no)

**Graded Relevance**: How relevant is it? (not relevant / somewhat relevant / highly relevant)

**Completeness**: Did the system retrieve all the important documents, or did it miss key ones?

**Precision**: Did it avoid retrieving lots of irrelevant junk?

Here's the hard part: you need to know what the "perfect retrieval set" would be to judge completeness. That means your ground truth needs to include not just what was retrieved, but what should have been retrieved.

A practical approach: for each question in your eval set, have an expert identify all the documents that would be useful for answering it. Then you can measure:
- How many of those "gold standard" documents did the system retrieve? (recall)
- How many of the retrieved documents are actually useful? (precision)

## Stage 2 Ground Truth: Extraction Accuracy

Okay, the system retrieved the right documents. Now it needs to read them and pull out the relevant information. This is where things get really interesting.

Extraction accuracy is about understanding: did the system comprehend the source material correctly?

Let's say your retrieved document says: "Employees are eligible for parental leave after completing 6 months of service, with the exception of senior executives who are eligible immediately."

The user asks: "When am I eligible for parental leave?"

The system needs to extract: there's a general rule (6 months) and an exception (senior executives immediately). If it only extracts the general rule, it's incomplete. If it mis-understands "with the exception of" and thinks executives need to wait 6 months, it's wrong.

Your ground truth for extraction should verify:

**Fact Accuracy**: Are the individual facts extracted correctly?

**Completeness**: Did it extract all the relevant information, including exceptions, conditions, and caveats?

**Interpretation**: Did it understand complex language like negations, conditionals, and exceptions?

Here's a pattern I see all the time: the model extracts information that's technically in the document but misses crucial context. A document says "This policy was in effect until December 2025" and the model extracts "Policy in effect December 2025" without the "until." Huge difference.

Your ground truth needs to catch these subtle misinterpretations. One way: for each key fact in your answer, verify it against the source with the full context, not just the keywords.

## Stage 3 Ground Truth: Faithfulness (No Hallucination)

This is the big one: is the generated response grounded in the source documents, or did the model make stuff up?

Faithfulness means every claim in the response should be supported by the retrieved documents. If the response says something that's not in the sources, that's a hallucination, even if it happens to be true.

Why does this matter? Because the whole point of RAG is to ground responses in verifiable sources. If the model can just make things up, you might as well use a regular chatbot.

Let me show you how subtle this can be. Retrieved document says: "Our standard warranty is 1 year. Extended warranties are available for purchase."

The model generates: "Your device comes with a 1-year warranty, and you can extend it up to 3 years."

See the problem? "Up to 3 years" is not in the source. Maybe it's true (and probably is, based on the model's training data), but it's not supported by the retrieved document. That's a hallucination in the RAG context.

Your ground truth for faithfulness should include:

**Supported Claims**: Every factual claim in the response should map to a statement in the source documents.

**Unsupported Claims**: Flag any claim that isn't directly supported, even if it's plausible.

**Contradictions**: If the response contradicts the sources, that's catastrophic.

Here's a practical labeling approach: go through the generated response sentence by sentence. For each factual claim, mark it as:
- Directly supported (the source says this explicitly)
- Implied (the source doesn't say it explicitly but it's a reasonable inference)
- Unsupported (the source doesn't contain this information)
- Contradicted (the source says something different)

The "implied" category is where judgment calls happen. If the source says "All employees receive parental leave" and the response says "Both full-time and part-time employees receive parental leave," is that supported?

It depends on whether "all employees" includes part-timers in your organization's language. This is where domain expertise in ground truth labeling becomes critical.

## The Supported vs Unsupported Distinction

Let me hammer this home because it's the most common RAG evaluation mistake: just because an answer is correct doesn't mean it's grounded.

I evaluated a RAG system for a financial services company. Question: "What's the current federal interest rate?" The model retrieved a document from 3 months ago and then answered with today's rate (which had changed since the document was written). The answer was factually correct, but it was not supported by the retrieved documents. The model had injected knowledge from its training data.

For some use cases, that's fine! Maybe you want the model to supplement retrieved information with its general knowledge. But if that's the case, you need to be explicit about it and have ground truth that separately evaluates "factual correctness" and "grounded in sources."

For most enterprise RAG systems, grounding is paramount. You want to know that every answer can be traced back to a specific source document, because that's how you ensure accountability and allow users to verify information.

Your ground truth should treat these as separate dimensions:
- **Factual correctness**: Is this true in the world?
- **Source grounding**: Is this stated in the retrieved documents?

A response can be 4 combinations:
1. Factually correct and grounded (ideal)
2. Factually correct but not grounded (hallucination from training data)
3. Factually incorrect but grounded (the source is wrong or outdated)
4. Factually incorrect and not grounded (total hallucination)

Each of these tells you something different about your system's failure modes.

## Stage 4 Ground Truth: Citation Correctness

Okay, the model gave a good answer that's grounded in the sources. Last step: did it cite those sources correctly?

Citation correctness is about attribution. If the response says "According to the Employee Handbook, parental leave is 12 weeks," your ground truth needs to verify:

1. Is there actually a document called "Employee Handbook" in the retrieved set?
2. Does that handbook actually say 12 weeks?
3. If there are multiple sources with conflicting information, did the model cite the right one (most recent, most authoritative, etc.)?

I've seen systems that cite documents that weren't even in the retrieval set. The model saw "Employee Handbook" in the query, assumed there must be such a document, and invented a citation. Your ground truth needs to catch this.

Your citation ground truth should verify:

**Citation Existence**: Does the cited source actually exist?

**Citation Accuracy**: Does the cited source actually contain the attributed information?

**Citation Completeness**: If information comes from multiple sources, are all sources cited?

**Citation Specificity**: For long documents, does the citation point to the specific section or page?

The last one is increasingly important for usability. If your response cites a 200-page manual, that's not very helpful. Better systems cite specific sections, pages, or even exact passages.

Some teams include the exact quote from the source alongside the citation. This is gold for ground truth evaluation because you can verify the quote against the source directly.

## Handling Partial Evidence: The Synthesis Challenge

Real questions often require synthesizing information from multiple sources. This is where RAG ground truth gets really complex.

User asks: "What's the total cost to attend your conference, including hotel and registration?"

The system might need to retrieve:
- Conference registration pricing page
- Recommended hotel information
- Any discount codes or group rates

Then it needs to synthesize this into a coherent answer. Your ground truth needs to verify that:

1. All relevant sources were retrieved
2. Information from each source was extracted correctly
3. The synthesis is mathematically and logically correct
4. All sources are cited appropriately

The synthesis step is where models often go wrong. They might add the hotel cost and registration cost correctly but forget to mention that hotel is per night, not total. Or they might include costs that only apply to certain attendee categories.

Your ground truth for synthesis should check:

**Completeness**: Are all pieces of information included?

**Accuracy**: Is the combination/calculation correct?

**Clarity**: Are conditions and caveats maintained (per night, early bird pricing, etc.)?

**Attribution**: Is it clear which piece of information comes from which source?

## Contradictory Sources: The Nightmare Scenario

Here's a scenario that will keep you up at night: the retrieved documents contradict each other.

User asks: "How many vacation days do I get?" The system retrieves:
- Old employee handbook: "Employees receive 15 vacation days"
- New HR policy update: "Vacation policy changed to 20 days as of January 2026"
- Manager's email to team: "Your team gets 15 days as discussed"

What's the right answer? It depends on:
- When the user is asking (before or after January 2026?)
- Which document is authoritative (official policy trumps manager email?)
- Whether there are special team-level exceptions

Your RAG system needs to navigate this, and your ground truth needs to verify it does so correctly.

Ground truth for contradictory sources should define:

**Conflict Detection**: Did the system recognize there's conflicting information?

**Resolution Strategy**: Did it apply the right rule to resolve the conflict (most recent, most authoritative, most specific)?

**Transparency**: Did it explain to the user that sources disagree?

**Appropriate Hedging**: Did it avoid overconfident claims when sources contradict?

The sophisticated approach: your ground truth should include examples with deliberate contradictions and specify the correct way to resolve them based on your domain's rules (recency wins, specificity wins, authority wins, etc.).

## Freshness: The Time Dimension

RAG systems are often used because information changes and you want answers based on current data, not training data from months ago. This adds a time dimension to ground truth.

Your ground truth should verify:

**Recency Awareness**: If multiple documents have the same information at different dates, did the system use the most recent?

**Staleness Detection**: If all retrieved documents are outdated for a time-sensitive query, does the system indicate this?

**Date Attribution**: Are dates mentioned when relevant (this policy as of June 2025)?

I saw a RAG system confidently answer COVID-19 questions using documents from early 2020. Technically the answers were supported by the sources, but they were dangerously outdated. Ground truth needs to catch this.

A practical approach: include timestamps in your ground truth evaluation. For each answer, note the date of the sources it's based on. Then flag answers where time-sensitive information comes from stale sources.

## The Retrieval-Generation Tradeoff

Here's a subtle point that affects ground truth: there's often a tradeoff between retrieval quality and generation quality.

Retrieving more documents (high recall) gives the model more information to work with, but it also gives it more opportunity to get confused, extract the wrong thing, or cherry-pick information incorrectly.

Retrieving fewer documents (high precision) reduces noise, but you might miss important context or nuance.

Your ground truth needs to be sensitive to this tradeoff. Sometimes a "worse" answer comes from better retrieval (more complete set of documents, but harder to synthesize). Sometimes a "better" answer comes from worse retrieval (missed some documents, but clearly processed the ones it found).

This is why evaluating the stages independently is so important. You need to know whether the failure is in retrieval, extraction, generation, or citation.

## Practical Labeling for RAG Ground Truth

Let's get practical. How do you actually create ground truth for RAG systems?

Here's a workflow that works:

**Step 1: Create question-answer pairs**

Start with questions that are representative of real user queries. For each question, have a domain expert write the ideal answer based on your knowledge base.

**Step 2: Identify gold standard sources**

For each question, have the expert identify which documents or passages are needed to answer it. This is your retrieval ground truth.

**Step 3: Label retrieved documents**

When your system retrieves documents for a question, label each one as relevant or not. This lets you measure retrieval precision and recall.

**Step 4: Verify extraction**

Check that the key facts in your system's answer are actually in the retrieved documents. Flag any unsupported claims.

**Step 5: Check citations**

Verify that citations point to real documents and accurately attribute information.

**Step 6: Overall quality**

Rate the complete answer on accuracy, completeness, clarity, and any domain-specific criteria.

This is labor-intensive, but there's no shortcut for high-quality RAG ground truth. The good news: once you have a solid eval set (even just 100-200 examples), you can use it to catch regressions and measure improvements.

## Automated Faithfulness Checking: A Powerful Tool

Here's a secret weapon for RAG ground truth: you can use LLMs to help create ground truth about faithfulness.

The technique: for each claim in the generated response, ask another LLM "Is this claim supported by this source document?" The LLM acts as a fact-checker.

This isn't perfect (LLMs can make mistakes), but research shows it correlates well with human judgments of faithfulness. You can use it to:

1. Pre-screen responses and flag potential hallucinations for human review
2. Augment human labeling by having the LLM label most examples and humans verify a sample
3. Create large-scale "silver standard" ground truth (not as good as human labels, but much cheaper to scale)

The key is to validate this approach on your specific domain. Label a few hundred examples by hand, then check how well LLM-based faithfulness checking agrees with human judgment. If correlation is high (above 0.8), you can use it to scale up.

## The Quote-Based Ground Truth Pattern

One highly effective pattern for RAG ground truth: require the system to quote the exact text from sources, then verify the quotes.

Instead of the system saying "Parental leave is 12 weeks," it says "Parental leave is 12 weeks (Employee Handbook, Section 5.2: 'Full-time employees are entitled to 12 weeks of paid parental leave')."

Now your ground truth verification is straightforward:
1. Find Section 5.2 of the Employee Handbook
2. Check if that exact quote appears
3. Check if the quote supports the claim

This is more restrictive than allowing paraphrase, but it's much easier to verify and gives users confidence that information is accurately sourced.

Some systems use hybrid: generate a natural language answer, but also return the supporting quotes separately for verification.

## Domain-Specific RAG Challenges

Different domains have different RAG ground truth challenges.

**Legal/Regulatory RAG**: Citation is critical, recency matters enormously, and errors can have legal consequences. Ground truth needs to verify jurisdictional accuracy (right state's law, right time period).

**Medical/Healthcare RAG**: Safety is paramount, sources must be authoritative, and the system should caveat limitations. Ground truth needs to verify appropriate hedging and disclaimer language.

**Technical Documentation RAG**: Versioning matters (answer for product version 2.0 vs 3.0), precision is critical, and incomplete answers are dangerous. Ground truth needs to verify version-appropriate responses.

**Customer Support RAG**: Speed matters, answers should be actionable, and tone needs to be empathetic. Ground truth needs to balance accuracy with usability.

Tailor your ground truth framework to your domain's specific requirements.

## The Warning: What Happens If You Skip This

If you evaluate your RAG system only on "is the answer good?" without checking retrieval, extraction, and citation independently, here's what happens:

Your system will give confident answers that sound right but aren't grounded in your actual sources. It will hallucinate information from its training data and mix it with retrieved information seamlessly. Users will trust it because it cites sources, but the citations will be wrong or incomplete. When challenged, you won't be able to trace answers back to authoritative sources.

In regulated industries, this can be catastrophic. In any industry, it erodes trust once users realize the sources don't actually say what the system claims.

I've seen a company launch a RAG system for internal policy questions. Employees loved it initially because it was fast and confident. Three months in, someone fact-checked an answer and found it was wrong. Trust evaporated overnight. The system went from highly-used to abandoned because they'd never verified faithfulness in their ground truth.

Don't let that be you.

## Bridge to Agents

We've covered ground truth for systems that retrieve and synthesize information, but what about systems that don't just talk—they take action? Agent systems move from "tell me the answer" to "do the thing." This is a fundamentally different ground truth challenge because now you're not just evaluating text generation, you're evaluating whether the right actions happened in the right order with the right parameters. Let's walk through what ground truth looks like when your AI has hands, not just a voice.
