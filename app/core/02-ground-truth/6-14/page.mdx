# 6.14 — Ground Truth for Chain-of-Thought & Reasoning

Let me tell you about the math tutoring AI that got every answer right and taught every concept wrong. It was helping high school students solve calculus problems, showing step-by-step reasoning. The final answers were always correct. Teachers loved it initially.

Then a math teacher actually read the reasoning steps. The AI was using circular logic, making unjustified leaps, and occasionally getting the right answer despite incorrect reasoning. One example: it "proved" a limit by assuming the answer and working backwards, which is mathematically invalid even though the answer was correct.

Students who learned from this AI could solve practice problems but couldn't explain their reasoning. They failed tests that required them to show their work. The AI had taught them to get answers, not to think mathematically.

This is the chain-of-thought ground truth challenge: right answers aren't enough. The reasoning has to be sound, the logic has to be valid, and the steps have to be correct—because that's what users are learning from or relying on.

## Why Reasoning Process Matters

In many AI applications, the output is all that matters. If a spam filter correctly identifies spam, nobody cares how it made the decision.

But chain-of-thought systems are different. They're used precisely because we care about the reasoning:

**Educational Use Cases**: Students need to learn HOW to solve problems, not just see answers.

**Explainability Requirements**: Regulated decisions (credit, hiring, medical) need justifiable reasoning.

**Human-AI Collaboration**: Humans need to understand AI reasoning to know when to trust it and when to override it.

**Complex Problem-Solving**: Multi-step problems require correct reasoning at each step; a right answer with wrong reasoning will fail on similar problems.

Your ground truth needs to evaluate both the destination (final answer) and the journey (reasoning process).

## The Four Dimensions of Reasoning Ground Truth

Let me break down what you need to evaluate in chain-of-thought outputs:

**Dimension 1: Logical Validity**

Is each reasoning step logically sound? Do the conclusions follow from the premises?

Good reasoning: "All humans are mortal. Socrates is human. Therefore, Socrates is mortal."

Bad reasoning: "Most humans like coffee. Socrates is human. Therefore, Socrates likes coffee."

The second example uses invalid logic (goes from "most" to "therefore" without justification).

Your ground truth should flag logically invalid steps, even if the conclusion happens to be correct.

**Dimension 2: Factual Correctness**

Are the facts and premises used in reasoning actually true?

Good reasoning: "Water freezes at 0°C. The temperature is -5°C. Therefore, water will freeze."

Bad reasoning: "Water freezes at 10°C. The temperature is 15°C. Therefore, water won't freeze."

The conclusion is correct (water won't freeze at 15°C) but the reasoning uses a false premise.

Your ground truth should verify factual accuracy of each claim.

**Dimension 3: Step Completeness**

Are all necessary steps included, or are there unjustified leaps?

Good reasoning:
"We need to solve 2x + 4 = 10.
Subtract 4 from both sides: 2x = 6.
Divide both sides by 2: x = 3."

Bad reasoning:
"We need to solve 2x + 4 = 10.
Therefore, x = 3."

The conclusion is correct, but the reasoning skips steps. A student couldn't reproduce this.

Your ground truth should verify all necessary intermediate steps are shown.

**Dimension 4: Relevance and Efficiency**

Are the reasoning steps relevant to the problem, or is there unnecessary digression?

Good reasoning: Direct path from problem to solution.

Bad reasoning: Includes correct but irrelevant steps, or takes an unnecessarily long path.

Your ground truth should penalize reasoning that's correct but inefficient or unfocused.

## The Right Answer, Wrong Reasoning Problem

This is the most insidious failure mode: the AI reaches the correct conclusion despite flawed reasoning.

Example:

Problem: "Is 17 a prime number?"

Wrong reasoning that gets right answer:
"17 is an odd number. Odd numbers are usually prime. Therefore, 17 is prime."

This reasoning is terrible (odd numbers are NOT usually prime: 9, 15, 21, 25, 27... are all odd and non-prime), but the conclusion happens to be correct.

Right reasoning:
"To check if 17 is prime, we test divisibility by all primes up to √17 ≈ 4.1. The primes to test are 2 and 3. 17 ÷ 2 = 8.5 (not a whole number). 17 ÷ 3 = 5.67 (not a whole number). Since 17 is not divisible by any prime up to its square root, it is prime."

Your ground truth must distinguish these cases:

**Right answer, right reasoning**: Full credit

**Right answer, wrong reasoning**: Partial credit for answer, but fail on reasoning quality

**Wrong answer, right reasoning**: Partial credit for process, but fail on final answer

**Wrong answer, wrong reasoning**: No credit

Don't give full credit just because the final answer is correct. The reasoning must be sound.

## Evaluating Multi-Step Reasoning Chains

Complex reasoning involves many steps. Your ground truth needs to evaluate the chain as a whole and identify where it breaks.

Consider a multi-step math problem:

Step 1: Calculate total revenue (correct)
Step 2: Calculate total costs (ERROR: added instead of subtracting a refund)
Step 3: Calculate profit using incorrect costs from Step 2 (logically correct given Step 2, but wrong because Step 2 was wrong)
Step 4: Calculate profit margin (logically correct, but wrong because Step 3 was wrong)

Your ground truth should identify:

**Error Location**: The error occurred in Step 2

**Error Propagation**: Steps 3 and 4 followed logically from previous steps but inherited the error

**Partial Credit**: Steps 1, 3, and 4 showed correct reasoning; only Step 2 had the actual error

This granular evaluation tells you what the model does well and what it struggles with.

## Ground Truth for Mathematical Reasoning

Mathematical reasoning has specific correctness criteria. Your ground truth should verify:

**Correct Operations**: Are arithmetic operations performed correctly?

**Equation Manipulation**: When solving equations, are transformations valid? (e.g., when dividing both sides, did it divide correctly?)

**Order of Operations**: Are operations performed in the correct order?

**Unit Consistency**: If working with units, are they handled correctly?

**Edge Cases**: Are division by zero, negative numbers under square roots, and other edge cases handled appropriately?

For proof-based reasoning:

**Valid Proof Techniques**: Are proof methods (direct proof, contradiction, induction) applied correctly?

**Logical Flow**: Does each statement follow from previous statements?

**Completeness**: Are all cases covered?

## Ground Truth for Logical and Deductive Reasoning

For logic problems and deductive reasoning, your ground truth should verify:

**Premise Identification**: Are the given premises correctly identified?

**Inference Rules**: Are logical inference rules (modus ponens, modus tollens, etc.) applied correctly?

**Assumption Tracking**: Are assumptions clearly stated and tracked throughout the reasoning?

**Conclusion Validity**: Does the conclusion necessarily follow from the premises?

Example:

Premises:
- If it's raining, the ground is wet.
- The ground is wet.

Invalid conclusion: "Therefore, it's raining." (This is the fallacy of affirming the consequent—the ground could be wet for other reasons)

Valid conclusion: "We cannot determine if it's raining based solely on this information."

Your ground truth should catch logical fallacies even when they lead to plausible conclusions.

## The "Show Your Work" Evaluation Pattern

In educational contexts, showing work is often as important as getting the answer. Your ground truth should use the "show your work" pattern:

**Requirement**: All significant reasoning steps must be explicitly shown.

**Evaluation**: Check that a student could follow the reasoning and reproduce the solution.

Bad example (skips steps):
"Solve for x: 3x² + 12x + 9 = 0
Answer: x = -1 or x = -3"

Good example (shows work):
"Solve for x: 3x² + 12x + 9 = 0
Factor out 3: 3(x² + 4x + 3) = 0
Factor the quadratic: 3(x + 1)(x + 3) = 0
Set each factor to zero: x + 1 = 0 or x + 3 = 0
Solve: x = -1 or x = -3"

Your ground truth should verify that intermediate steps are shown, not just the final answer.

## Evaluating Reasoning About Uncertainty

Many real-world reasoning tasks involve uncertainty. Your ground truth should verify appropriate handling:

**Probabilistic Reasoning**: When dealing with probabilities, are calculations correct?

**Confidence Calibration**: Does the system express appropriate confidence? (High confidence for certain conclusions, lower for uncertain ones)

**Handling Ambiguity**: When information is incomplete, does the reasoning acknowledge this?

Example:

Bad reasoning: "The patient has symptom X. Symptom X is associated with disease Y. Therefore, the patient has disease Y."

Better reasoning: "The patient has symptom X. Symptom X is associated with diseases Y and Z. We need additional tests to determine which is more likely."

Your ground truth should reward appropriate hedging and acknowledgment of uncertainty.

## Ground Truth for Analogical Reasoning

Analogical reasoning involves drawing parallels between different domains. Your ground truth should verify:

**Appropriate Analogy**: Is the analogy relevant and helpful?

**Correct Mapping**: Are the elements of the analogy correctly mapped?

**Limitation Awareness**: Does the reasoning acknowledge where the analogy breaks down?

Example:

Good analogy: "Learning a language is like learning a musical instrument—both require regular practice, pattern recognition, and gradual skill building."

Bad analogy: "Learning a language is like riding a bike—once you learn, you never forget." (This is false for languages; language skills atrophy without practice)

Your ground truth should evaluate whether analogies are helpful and accurate.

## Evaluating Reasoning Corrections and Self-Critique

Advanced chain-of-thought systems can critique and correct their own reasoning. Your ground truth should evaluate this meta-reasoning:

**Error Detection**: When the system makes an error, does it recognize it?

**Correction Validity**: When it corrects itself, is the correction actually better?

**Explanation Quality**: Does it explain what was wrong and why the correction is better?

Example:

System: "The answer is 42."
[Self-critique]: "Wait, I made an error in step 3. I added when I should have multiplied."
[Correction]: "The correct answer is 56."

Your ground truth should verify:
- Was there actually an error in step 3? (Yes)
- Is the correction correct? (Verify 56 is right)
- Is the explanation accurate? (Check if the error was indeed adding instead of multiplying)

## Handling Multiple Valid Reasoning Paths

For many problems, there are multiple correct ways to reason to the answer. Your ground truth should accept all valid paths.

Example: Proving a number is even

Approach 1 (Division): "We can write the number as 2k for some integer k, therefore it's even."

Approach 2 (Modulo): "The number modulo 2 equals 0, therefore it's even."

Approach 3 (Last Digit): "The number ends in 0, 2, 4, 6, or 8, therefore it's even."

All three are valid. Your ground truth should accept any of them.

Don't rigidly require one specific reasoning path. Verify logical validity, not conformity to a template.

## Ground Truth for Causal Reasoning

Causal reasoning is particularly tricky. Your ground truth should distinguish:

**Correlation vs Causation**: Does the reasoning conflate these?

Bad: "Ice cream sales and drowning deaths both increase in summer. Therefore, ice cream causes drowning."

Good: "Ice cream sales and drowning deaths both increase in summer due to a common cause: warm weather leads to more swimming (increasing drowning risk) and more ice cream consumption."

**Causal Direction**: Is the direction of causation correct?

Bad: "Countries with more hospitals have higher death rates. Therefore, hospitals cause death."

Good: "Countries with more hospitals may have higher death rates because hospitals are built in response to health needs, and sick people go to hospitals. The causation runs from illness to hospitals, not hospitals to illness."

Your ground truth should verify correct causal reasoning.

## The Explanation Depth Spectrum

Different audiences need different levels of reasoning detail. Your ground truth should specify appropriate depth:

**Expert Audience**: Can skip obvious steps, use domain-specific notation and terminology

**General Audience**: Should explain most steps, avoid jargon, provide context

**Educational Audience**: Should show all steps, explain why each step is taken, build understanding progressively

Your ground truth should evaluate whether explanation depth matches the target audience.

## Verifying Reasoning Against External Tools

For some reasoning tasks, you can verify steps using external tools:

**Mathematical Steps**: Use a computer algebra system to verify each step

**Logical Steps**: Use a theorem prover to verify logical validity

**Code Reasoning**: Execute code steps to verify they produce claimed outputs

When possible, use automated verification to check reasoning steps, not just final answers.

## The Warning: What Happens If You Skip This

If you evaluate chain-of-thought systems only on final answer correctness without checking reasoning quality, here's what happens:

Your AI will develop shortcuts that work in training but fail on novel problems. It will teach users wrong methods that happen to work on common problems. It will produce explanations that sound good but are logically flawed.

Users relying on the reasoning (students, professionals needing explainability) will learn bad habits or make decisions based on flawed logic. When corner cases arise, the AI will fail catastrophically because its reasoning was never sound.

I've seen educational AI systems that students loved because they got answers right, but teachers banned because the reasoning was teaching bad problem-solving habits.

Don't evaluate just the destination. Evaluate the journey. The reasoning process is the product for chain-of-thought systems.

## Bridge to Multi-Agent Systems

We've been discussing ground truth for single AI systems—one model generating one response, one chain of reasoning, one ranking. But increasingly, AI systems involve multiple agents working together: one agent gathers information, another analyzes it, a third makes decisions, a fourth executes actions. Ground truth for multi-agent systems is fundamentally different because you're not just evaluating individual agent quality—you're evaluating coordination, communication, handoffs, and emergent behavior. The whole system can fail even when each individual agent performs well. Let's walk through how ground truth changes when your AI is a team, not an individual.
