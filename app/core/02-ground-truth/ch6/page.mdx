# Chapter 6 — Ground Truth for Specific AI Modalities

Ground truth isn't one-size-fits-all — and nowhere is that more obvious than when you compare what "correct" means across different AI modalities. A chatbot, a RAG system, a code generator, and a voice assistant each need fundamentally different definitions of quality, different evaluation criteria, and different ground truth structures.

Most teams try to force a single evaluation framework across their entire product. It doesn't work. What makes a great chat response has almost nothing in common with what makes a great code completion. Multi-modal systems add another layer of complexity. Agent systems introduce state, tool use, and coordination challenges that don't exist in simpler products.

This chapter walks you through ground truth design for every major AI modality you'll encounter in 2026 — from conversational AI to multi-agent coordination, from classification to creative generation.

---

## What This Chapter Covers

- **6.1** — Chat & Conversational AI
- **6.2** — RAG & Knowledge Systems
- **6.3** — Agents & Tool-Using Systems
- **6.4** — Agent State Truth & Idempotency
- **6.5** — Voice & Real-Time Systems
- **6.6** — Classification & Extraction
- **6.7** — Code Generation
- **6.8** — Creative & Generative Tasks
- **6.9** — Multi-Modal Systems
- **6.10** — Internal Automation & Workflows
- **6.11** — Multilingual & Cultural Truth
- **6.12** — Personalized Truth & Consent Boundaries
- **6.13** — Truth for Ranking & Selection Systems
- **6.14** — Ground Truth for Chain-of-Thought & Reasoning
- **6.15** — Multi-Agent Coordination Truth

---

*Let's start with the modality most teams encounter first — chat.*
