# 4.11 — The Ground Truth Record: Fields, Metadata, Version & Source Links

Let me show you what separates amateur ground truth from production-grade ground truth.

You're sitting in a meeting six months after launch. A regulator asks: "Your AI told a customer they had 60 days to file a complaint when the actual deadline is 30 days. Why did your evaluation not catch this?"

You need to answer:

Which test case should have caught this?
When was that test case created and by whom?
What was the expected output at the time?
Has the policy changed since the test was created?
Which version of the ground truth was used in the evaluation that cleared this for launch?
What source document defines the correct answer?

If your ground truth record is just "input: question about complaint deadlines, output: 30 days," you can't answer these questions. You have no provenance, no versioning, no source links, no metadata. You're in serious trouble.

Production-grade ground truth is not just examples and labels. It's examples and labels plus the context, lineage, and documentation that make them auditable, maintainable, and trustworthy.

Let me walk you through what a real ground truth record looks like.

## The Minimum Viable Record

Start with the absolute minimum. A ground truth record needs at least:

Unique ID: Every record must have a stable identifier you can reference. "GT-2026-04-0157" or a UUID. This lets you track the record across systems and versions.

Input: The prompt, question, or scenario being tested. This needs to be complete and unambiguous. Not "user asks about returns" but the actual question: "I ordered a sweater last week but it doesn't fit. Can I return it?"

Expected output: What a good response looks like. This might be an exact expected response, or a rubric score, or a set of required elements, depending on your evaluation approach.

Timestamp: When was this record created? Dates matter for versioning and staleness tracking.

That's the minimum. Four fields. But this minimum is not enough for production. You need more.

## Essential Metadata Fields

Add these fields to make your ground truth usable at scale:

Truth type: What kind of ground truth is this? Options:
- Factual (objectively verifiable against source material)
- Policy (defined by company rules)
- Preference (subjective quality judgment)
- Business rule (defined by business logic)
- Brand voice (defined by style guide)

Why this matters: Factual truth changes when facts change. Policy truth changes when policies change. Preference truth might evolve as you learn what users like. Each type has different refresh requirements and different validation methods.

Risk tier: How critical is this test case? Tiers might be:
- Critical: Failure causes serious harm (legal, financial, safety, compliance)
- Important: Failure creates bad user experience or minor harm
- Standard: Failure is undesirable but low-stakes

Why this matters: You might require manual review for critical test cases but accept automated evaluation for standard ones. You might block deployment on critical failures but accept some important failures if overall quality is high.

Category or domain: What area does this test? "Billing," "Returns," "Technical Support," "Account Management." This lets you analyze performance by category and ensure coverage.

Complexity level: Is this an easy, medium, hard, or edge case example? This helps you understand whether you're failing on basics or only on hard cases.

Language: If you support multiple languages, tag which language this example is in.

Status: Is this record active, deprecated, under review, or retired? You need to mark obsolete records without deleting them.

## Source and Provenance Fields

These fields answer "where did this ground truth come from and why should we trust it?"

Source type: How was this record created?
- Expert elicitation
- Documentation extraction
- Real user interaction
- Synthetic generation
- Customer feedback
- Competitive analysis
- Regulatory requirement

Source reference: Link to the specific source. If from documentation, which document and section? If from expert elicitation, which session? If from a real user interaction, which ticket or session ID?

Author: Who created this record? Name or ID of the person or system.

Reviewer: Who validated it? Ideally multiple reviewers for high-stakes examples.

Review notes: Any discussion or context from the review process. "Three reviewers initially disagreed on tone scoring but converged after discussing brand guidelines."

Confidence level: How confident are you in this label? High confidence for clear-cut cases, medium for judgment calls, low for examples you're still debating.

Why this matters: When you find an error or need to update ground truth, you need to understand where it came from. If all records from source X turn out to be wrong, you can query for them and review them. If you need to explain to a regulator why this is the expected output, you point to the source document.

## Version and Change Tracking

Ground truth evolves. You need version control.

Version number: What version of ground truth is this? Use semantic versioning (1.0, 1.1, 2.0) or date-based versioning (2026-Q2).

Effective date: When did this ground truth become the standard? Important for time-based analysis. "This policy changed on March 15, so ground truth created before then is outdated."

Expiration date: When will this ground truth stop being valid? Some things have known expiration. "This promotion ends June 30, so any ground truth referencing it expires then."

Change history: What changed and when? "v1.0 (2026-01-15): Initial version. v1.1 (2026-03-20): Updated expected output to reflect policy change. v2.0 (2026-06-10): Reclassified from 'important' to 'critical' risk tier."

Deprecated reason: If this record is no longer active, why? "Policy changed," "Product feature removed," "Example was found to be mislabeled," "Replaced by GT-2026-06-0234."

Why this matters: When you evaluate your AI, you need to use the correct version of ground truth. If you use outdated ground truth, you'll penalize correct responses. Change tracking lets you understand how your standards have evolved and prevents using stale examples.

## Evaluation Metadata

These fields support using ground truth in evaluation pipelines:

Expected output format: Is the expected output an exact string match, a rubric score, a pass/fail judgment, a set of required elements, a claim-level breakdown?

Evaluation method: How should this be evaluated? Manual review, automated LLM scoring, exact match, similarity threshold, custom function?

Pass threshold: What score or criteria must be met to pass? For rubric-based, maybe "average score ≥ 3.5 and all critical dimensions ≥ 3." For claim-level, maybe "≥ 95% claims correct and zero critical claims false."

Associated test cases: Link to any automated test cases that use this ground truth. This helps you understand the impact of changing a record.

Failure impact: If the AI fails this test, what's the consequence? Helps prioritize fixes.

## Context Fields

Sometimes the input alone isn't enough context to evaluate a response. Add:

User context: What user attributes matter? "Free tier user," "Enterprise customer," "First-time user," "Power user." Responses might be appropriately different for different user types.

Session context: Is this part of a multi-turn conversation? If so, what's the conversation history?

System context: What system state matters? "User has an active order," "User's payment method expired," "Feature X is in beta for this user."

Temporal context: Does the time of day, day of week, or season matter? "Asked during holiday return period when extended windows apply."

Why this matters: The same question might have different correct answers depending on context. "When does my free trial end?" has a user-specific answer. "What are your hours?" has a time-zone-specific answer. Context fields make evaluation accurate.

## Compliance and Audit Fields

In 2026, especially with EU AI Act in effect, you need audit trails.

Compliance tags: Which regulations or policies does this test? "GDPR-personal-data," "TCPA-consent," "Financial-advice-disclaimer," "Age-restriction."

High-risk indicator: Is this a high-risk AI system under EU AI Act? Flag records that test high-risk scenarios.

Bias review status: Has this record been reviewed for potential bias? When and by whom?

Accessibility check: Has this been reviewed for accessibility considerations (screen readers, cognitive load, language complexity)?

Audit trail: Complete log of who accessed this record, when, and why. "Used in evaluation run ER-2026-04-15," "Reviewed in audit AU-2026-06-01."

Why this matters: Regulatory compliance requires demonstrating that you've tested for specific risks and harms. Audit fields let you prove you've done due diligence.

## Linking and Relationship Fields

Ground truth records don't exist in isolation.

Related records: Link to similar or related test cases. "See also GT-2026-04-0158 for the same scenario with enterprise customers."

Parent/child relationships: Some records are variations of others. Track these relationships.

Supersedes: This record replaces an older one. Link to the old record.

Conflicts with: Sometimes you have test cases that seem to contradict each other, usually due to context differences. Flag and explain these.

Part of test suite: Which test suite(s) include this record? "Core-regression-suite," "Critical-path-tests," "Compliance-tests."

Why this matters: When you update one record, you might need to update related ones. When you find a conflict, you can trace it. When you build test suites, you can include or exclude records by relationship.

## Example Production Record

Here's what a complete ground truth record looks like:

```
ID: GT-2026-04-0157
Version: 2.1
Status: Active

=== Core Content ===
Input: "I ordered a sweater last week but it doesn't fit. Can I return it?"
User Context: Standard tier customer, purchased 7 days ago, first return request
Expected Output: Response must (1) confirm returns are allowed, (2) state the 30-day window, (3) explain return process, (4) maintain empathetic tone
Output Format: Rubric scoring (accuracy, completeness, empathy, actionability)
Pass Threshold: All dimensions ≥ 3/5, accuracy must be 5/5

=== Classification ===
Truth Type: Policy
Category: Returns
Complexity: Standard
Risk Tier: Important
Language: en-US

=== Provenance ===
Source Type: Documentation extraction
Source Reference: Return Policy Manual v3.2, Section 4.1, pages 12-13
Author: Sarah Chen (sarah@company.com)
Created: 2026-04-15
Reviewers: Mike Torres, Lisa Patel
Confidence: High

=== Versioning ===
Effective Date: 2026-04-15
Expiration: None
Change History:
  - v1.0 (2026-01-10): Initial version with 60-day window
  - v2.0 (2026-04-15): Updated to 30-day window per policy change
  - v2.1 (2026-05-20): Added empathy requirement based on user feedback analysis

=== Evaluation ===
Evaluation Method: Automated LLM scoring with human spot-check
Associated Test Cases: TC-Returns-Basic-001, TC-Returns-Recent-Order
Failure Impact: User misinformed about rights, potential compliance issue

=== Compliance ===
Compliance Tags: Consumer-rights, Return-policy
High-Risk: No
Bias Review: Completed 2026-04-16 by D&I team
Audit Trail: Used in eval runs ER-2026-04-20, ER-2026-05-15

=== Relationships ===
Related: GT-2026-04-0158 (damaged item returns), GT-2026-04-0159 (wrong item returns)
Supersedes: GT-2026-01-0034 (outdated 60-day policy)
Part of Suites: Core-regression, Returns-flow-tests
```

This is comprehensive. Every question you might ask later has an answer.

## Storage and Queryability

How you store ground truth determines how usable it is.

Bad approach: Spreadsheets. They work for fifty examples but break at five hundred. No versioning, no access control, no automated queries.

Better approach: Structured databases with proper schemas. You can query "show me all critical-tier test cases about billing created after April 1," or "show me all deprecated records from documentation source X."

Best approach: Specialized ground truth management systems. Tools like Patronus, HoneyHive, or custom-built systems that provide:

- Version control with full change history
- Access control and audit trails
- Query and filtering capabilities
- Integration with evaluation pipelines
- Automated staleness detection
- Relationship mapping

In 2026, treating ground truth as code is standard practice. Store it in git with YAML or JSON records, use pull requests for changes, require reviews, maintain changelog.

## Querying Your Ground Truth

With proper structure, you can ask questions like:

"Show me all critical test cases that haven't been validated in the last six months" → Identifies staleness risk

"Show me all test cases that came from the returns policy document" → Lets you update them when the policy changes

"Show me all test cases where agreement between reviewers was low" → Identifies ambiguous cases that need clarification

"Show me all deprecated test cases and why they were deprecated" → Helps you learn from past mistakes

"Show me all test cases tagged with GDPR compliance" → Helps you prove compliance coverage

Queryability transforms ground truth from a static dataset to a living knowledge base.

## Maintenance Workflows

Good metadata enables maintenance workflows:

Staleness detection: Query for records where effective date is more than one year ago and status is active. Flag for review.

Source change propagation: When a source document updates, query for all records derived from that document. Review and update them.

Compliance audits: When preparing for an audit, query for all records tagged with relevant compliance tags. Generate a report showing coverage.

Quality improvement: Query for records where inter-rater agreement was low or failure rate is high. Prioritize refining these.

Deprecation cleanup: Query for deprecated records older than two years. Archive them to keep the active set manageable.

## The Minimal-Metadata Trap

Teams often start with minimal metadata ("we'll add more later") and then never do because it's too much work to backfill.

Avoid this. Define your full metadata schema up front, even if you leave some fields empty initially. It's easier to fill in fields as you go than to retrofit structure onto hundreds of records.

Require critical fields (ID, input, expected output, truth type, source, author, created date) from day one. Make other fields optional but encouraged.

## Ground Truth as Documentation

Here's a mindset shift: your ground truth records are not just test data. They're documentation of your quality standards.

When a new PM asks "what's our policy on refunds for damaged items?" you can point to ground truth records that codify the policy with examples.

When a new engineer asks "how should the AI handle ambiguous questions?" you point to ground truth records showing different approaches and their scoring.

When an executive asks "how do we know our AI meets safety standards?" you show the compliance-tagged ground truth records and evaluation results.

Ground truth with rich metadata serves double duty: test cases AND knowledge base.

## Version Control Strategies

A few strategies for managing versions:

Immutable records: Never change a record. Instead, create a new version and mark the old one deprecated. This preserves full history but creates clutter.

Mutable with changelog: Edit records in place but maintain detailed changelog. Cleaner but history is in a separate field.

Branch and merge: Treat ground truth like code. Create branches for major updates, review changes, merge when ready. Requires git-like tooling.

Time-based snapshots: Periodically snapshot your entire ground truth dataset. "This is ground truth as of 2026-Q2." Use snapshots for historical evaluations.

Most teams use mutable-with-changelog for regular updates and time-based snapshots for major milestones.

## The Audit Story

Let's return to that regulator meeting. With proper ground truth records, you can answer every question:

"Which test case should have caught this?"
→ GT-2026-03-0089, category: complaint-deadlines

"When was it created?"
→ 2026-03-15 by compliance team member James Liu

"What was the expected output?"
→ 30-day deadline per Consumer Protection Regulation 2025, section 14.2 (source linked)

"Has the policy changed?"
→ No, effective date is 2025-12-01, no expiration, no subsequent versions

"Which version of ground truth was used?"
→ Evaluation run ER-2026-05-10 used ground truth version 3.0 (audit trail shows this)

"Why did it fail?"
→ Test result shows response stated 60 days. Root cause analysis: knowledge base was not updated when regulation changed. (Linked to incident report IR-2026-05-12)

You have a complete, auditable story. That's the difference proper metadata makes.

In the next section, we'll dive into the statistics of ground truth: how to measure inter-annotator agreement using Cohen's Kappa and Krippendorff's Alpha. These metrics tell you whether your ground truth is consistent enough to trust. Let me explain them in plain English without the math jargon.
