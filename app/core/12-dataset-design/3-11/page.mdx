# 3.11 â€” Synthetic Data for Evaluation Sets vs Training Sets

In early 2025, a legal technology company built a contract clause extraction model and evaluated it using a synthetic test set of 2,000 contract excerpts generated by GPT-4. The model achieved 96 percent F1 score on the synthetic test set. The team celebrated, shipped the model to production, and began processing real customer contracts.

Within three days, customer complaints began arriving. The model was missing obvious clauses, extracting incorrect entities, and flagging boilerplate as high-risk. When the team evaluated the same model on a real-world test set of 500 actual customer contracts, F1 score dropped to 78 percent.

The synthetic test set had been optimized for clarity, grammatical correctness, and standard legal phrasing. Real contracts included OCR errors, non-standard formatting, merged clauses, and domain-specific shorthand. The model had never seen these patterns in training or evaluation, and it failed when it encountered them in production.

The mistake was treating synthetic evaluation data the same way the team treated synthetic training data. Synthetic training data and synthetic evaluation data have fundamentally different requirements and fundamentally different risks.

Training data teaches the model what patterns exist. Evaluation data measures whether the model has learned the right patterns. If your evaluation data is unrepresentative, your performance metrics are meaningless.

A model that scores 96 percent on synthetic data but 78 percent on real data is not a 96 percent model. It is a 78 percent model with misleading metrics. Synthetic evaluation data is acceptable only when it faithfully represents the distribution, difficulty, and edge case diversity of real-world data, and validating that fidelity requires comparison against human judgment on real examples.

## Why Synthetic Eval is Higher-Risk Than Synthetic Training

Training data errors are forgiving. If 5 percent of your synthetic training examples are low-quality, the model learns from the other 95 percent and the low-quality examples contribute little to the learned distribution. Training is robust to noise.

Evaluation data errors are unforgiving. If 5 percent of your synthetic evaluation examples are unrepresentative, your performance metrics are biased. You might overestimate model quality, underestimate failure rates, or miss critical weaknesses.

Evaluation is sensitive to noise.

The risk asymmetry exists because evaluation is a measurement tool and training is a learning process. Training aggregates over thousands of examples and converges to a representation of the average case. Evaluation samples a few hundred or a few thousand examples and uses them to estimate performance on millions of unseen examples.

If your sample is biased, your estimate is wrong. Measurement error compounds when you make decisions based on flawed metrics.

You ship a model you think is 94 percent accurate when it is actually 81 percent accurate. The downstream consequences include user complaints, damaged trust, and costly rollbacks.

Synthetic training data that is slightly cleaner or slightly more formal than real data is usually benign. The model might learn a slightly idealized version of the task, but it will generalize reasonably well to real data.

Synthetic evaluation data that is slightly cleaner or slightly more formal than real data is dangerous. Your metrics will overestimate performance, and you will not discover the gap until production.

Contamination risk is higher for synthetic eval data. If your synthetic test set was generated using the same prompts, the same model, or the same templates as your synthetic training set, your test set is not independent. The model has seen similar patterns during training.

Your evaluation is measuring memorization, not generalization. Contamination invalidates your metrics entirely. You must ensure that synthetic eval data is generated using different methods, different models, or different seeds than your synthetic training data.

## Contamination Concerns

Contamination in evaluation sets occurs when the test set leaks information about the training set or when the test set and training set share underlying generation logic. The most obvious form of contamination is direct overlap: the same example appears in both training and test sets. This is easy to detect and prevent with deduplication.

The more insidious form of contamination is distribution overlap: the test set and training set are generated from the same templates, the same prompt patterns, or the same LLM with similar seeds.

Distribution overlap is harder to detect and harder to prevent.

Distribution overlap means your test set measures whether the model has learned the synthetic generation process, not whether it has learned the real-world task. A model trained on synthetic customer support tickets generated by Claude and tested on synthetic customer support tickets generated by Claude will perform better than the same model tested on real customer support tickets.

The eval metrics are inflated because both the training set and test set share the same distributional artifacts: similar sentence structures, similar vocabulary, similar edge case coverage. The model is not learning customer support. It is learning Claude's customer support generation style.

You prevent distribution overlap by ensuring that your synthetic test set is generated differently than your synthetic training set. If training data was generated by GPT-4, generate test data with Claude or Llama. If training data used few-shot prompting, generate test data with zero-shot prompting.

If training data used templates with variable substitution, generate test data with freeform LLM generation. Methodological diversity reduces the risk that your test set rewards overfitting to synthetic generation artifacts.

Temporal separation also reduces contamination risk. If you generate training data in March and test data in June, using different model versions, different prompts, and updated edge case lists, the likelihood of distribution overlap is lower. Temporal separation is not a perfect defense, but it adds a layer of independence.

The gold standard for contamination prevention is to never use synthetic data for evaluation. You use synthetic data only for training and augmentation, and you evaluate exclusively on real-world data. This eliminates contamination risk entirely.

When synthetic eval data is unavoidable due to scarcity of real data, you validate synthetic eval sets against small real-world holdout sets to ensure that performance metrics on synthetic data correlate with performance metrics on real data.

## Difficulty Calibration

Difficulty calibration is the process of ensuring that your synthetic evaluation set has the same distribution of easy, medium, and hard examples as real-world data. Synthetic generation tends to produce examples that cluster around the average difficulty because LLMs generate prototypical examples unless explicitly instructed otherwise.

Real-world data includes a long tail of unusually easy examples and unusually hard examples. If your synthetic eval set lacks the tail, your metrics do not reflect real-world performance.

You measure difficulty by evaluating example-level model performance. Examples where the model achieves high confidence and correct predictions are easy. Examples where the model achieves low confidence or incorrect predictions are hard.

You calculate the distribution of example difficulties in a real-world validation set, and you compare it to the distribution in your synthetic test set. If the distributions match, your synthetic test set is well-calibrated.

If the synthetic test set has fewer hard examples, it will overestimate model performance.

Calibration requires intentional generation. You do not just generate 1,000 synthetic evaluation examples and assume they represent reality. You generate synthetic examples in multiple difficulty tiers. You generate 300 easy examples by using simple, unambiguous prompts.

You generate 400 medium examples by using realistic prompts with moderate complexity. You generate 300 hard examples by using adversarial prompts, edge case constraints, and ambiguity injection. You blend the tiers to match the real-world difficulty distribution you measured.

Difficulty calibration is domain-specific. In customer support, hard examples might include misspellings, slang, or multi-intent queries. In legal contracts, hard examples might include non-standard clause structures, ambiguous phrasing, or jurisdictional edge cases.

In medical records, hard examples might include incomplete documentation, conflicting information, or rare diagnoses. You define what hard means for your task before you generate synthetic eval data.

## When Synthetic Evals Are Acceptable vs When They Are Not

Synthetic evaluation sets are acceptable when real data is unavailable, when the task is well-defined and stable, and when synthetic data has been validated to correlate strongly with real-world performance. They are not acceptable in high-stakes domains, in tasks with rapidly shifting distributions, or when no real-world validation data exists to anchor the synthetic eval set.

Synthetic evals are acceptable for low-stakes classification tasks with narrow output spaces and stable vocabularies. A spam classifier eval set can be synthetic if you validate that synthetic spam examples trigger the same model behaviors as real spam examples.

A sentiment classifier eval set can be synthetic if you validate that synthetic product reviews produce the same sentiment distributions as real reviews. The validation step is mandatory. You cannot assume synthetic data is representative. You must measure it.

Synthetic evals are not acceptable for generative tasks where output quality is subjective and context-dependent. A summarization eval set should not be synthetic because synthetic summaries are generated by the same class of models you are evaluating, and the eval set will reward models that produce outputs similar to the synthetic generation style rather than outputs that are genuinely useful to humans.

A dialogue eval set should not be synthetic because real conversations have context, continuity, and user intent that synthetic conversations struggle to replicate.

Synthetic evals are not acceptable for high-stakes applications where performance miscalibration has serious consequences. A medical diagnosis eval set should not be synthetic because overestimating diagnostic accuracy could lead to patient harm.

A fraud detection eval set should not be synthetic because underestimating false negatives could lead to financial losses. A content moderation eval set should not be synthetic because missing harmful content in production damages user safety.

High-stakes applications require real-world eval sets with rigorous human review.

Synthetic evals are not acceptable when your real-world distribution is unknown or poorly characterized. If you do not know what real data looks like, you cannot generate synthetic data that matches it. You must start by collecting a small real-world sample, analyzing its properties, and using that analysis to inform synthetic generation.

Synthetic eval sets are derivative. They depend on a ground truth understanding of real data. Without that understanding, synthetic eval sets are arbitrary.

## How to Validate Synthetic Eval Sets Against Human Judgment

Validation starts with a real-world reference set. You collect 200 to 500 real-world examples, you label them using your standard annotation process, and you treat this set as ground truth. You evaluate your model on both the real-world reference set and your synthetic eval set.

You compare performance metrics across both sets. If metrics are within two percentage points, your synthetic eval set is well-calibrated.

If metrics diverge by more than five percentage points, your synthetic eval set is not representative and should not be used.

You also compare metric distributions, not just aggregate metrics. Aggregate accuracy might be similar, but per-category accuracy might differ. Your model might achieve 90 percent accuracy on both real and synthetic data overall, but 95 percent on synthetic data for category A and 85 percent on real data for category A.

Category-level divergence indicates that your synthetic data does not represent real data faithfully within that category. You regenerate synthetic examples for the divergent categories and revalidate.

Human review is the second validation layer. You sample 100 synthetic eval examples and you ask domain experts whether each example is realistic, representative, and appropriately difficult. You do not ask whether the example is correct or well-formed.

You ask whether it resembles something they would encounter in production. If experts flag more than 10 percent of examples as unrealistic, your synthetic eval set is not fit for purpose.

Human review also catches subtle issues that metrics miss. An example might be technically correct but stylistically off. A synthetic customer complaint might have perfect grammar and logical structure, but real customer complaints are often fragmented, emotional, and informal.

A synthetic contract clause might be legally sound but use phrasing that no practitioner would write. These subtleties do not appear in automated metrics, but they bias evaluation results. Human reviewers catch them.

## Synthetic Data for Slice-Specific Evaluation

Slice-specific evaluation measures model performance on subsets of your data defined by specific attributes: input length, domain, language, or user demographic. Synthetic data is often used to create eval slices that are underrepresented in real data.

If your real eval set has 50 examples of code-switching queries but you need 500 to measure performance reliably, you generate 450 synthetic code-switching queries to fill the gap.

Slice-specific synthetic eval data is higher-risk than general synthetic eval data because slices are often defined by edge cases, and synthetic generation of edge cases is harder than synthetic generation of typical cases. A synthetic code-switching query generated by an LLM might not exhibit the same morphological or syntactic patterns as a real code-switching query produced by a bilingual speaker.

You validate slice-specific synthetic data even more carefully than general synthetic data.

The validation protocol for slice-specific eval data involves comparing real examples from the slice to synthetic examples from the slice. You analyze vocabulary overlap, syntactic structure, and domain-specific features. If the synthetic examples match the real examples on these dimensions, the synthetic slice is usable.

If the synthetic examples are systematically different, you refine your generation prompts or you abandon synthetic generation for that slice and invest in collecting more real data.

## When to Avoid Synthetic Eval Data Entirely

Some tasks should never use synthetic eval data. Any task where the ground truth is subjective, context-dependent, or requires human judgment should evaluate on real data with human labels. Summarization, translation quality, dialogue coherence, and creative writing all fall into this category.

Synthetic eval data for these tasks measures alignment with synthetic generation patterns, not alignment with human preferences.

Any task where distributional shift is frequent and unpredictable should avoid synthetic eval data. If your task involves monitoring social media for emerging trends, financial markets for anomalies, or cybersecurity for novel attack vectors, your eval set must reflect current reality.

Synthetic eval data generated in March does not reflect trends that emerged in June. You need continuous real-world eval data collection to keep your metrics valid.

Any task where regulatory or compliance requirements mandate real-world testing should avoid synthetic eval data. Medical device software, autonomous vehicle perception systems, and financial credit models are all subject to regulatory standards that require validation on real-world test sets.

Synthetic eval data does not satisfy these requirements. You collect real data, you label it rigorously, and you use it for compliance testing.

## Best Practices for Synthetic Eval Data

When synthetic eval data is justified, you follow strict best practices. You generate synthetic eval data using different methods than you used for training data. You validate synthetic eval data against a real-world reference set. You calibrate difficulty to match real-world distributions.

You involve domain experts in human review. You monitor for contamination by checking that synthetic test examples do not overlap with or closely resemble training examples.

You document your generation process, your validation results, and your calibration analysis so stakeholders understand the provenance and limitations of your eval set.

You also version synthetic eval sets and you regenerate them periodically. A synthetic eval set generated in Q1 might not represent reality in Q4. You regenerate eval sets every six months, you revalidate against fresh real-world data, and you track how eval set changes affect reported model performance.

If model performance drops when you switch to a new eval set, either your model is degrading or your old eval set was miscalibrated. You investigate and you correct.

You never use synthetic eval data as your only eval set. You always maintain a real-world eval set, even if it is small. The real-world set is your ground truth. The synthetic set is a supplement.

You report metrics on both sets, and you flag when metrics diverge. Transparency about eval set composition builds trust and prevents overconfidence in synthetic data.

## The Feedback Loop Between Eval and Training Data

Synthetic eval data quality directly affects your ability to improve training data. If your eval set is unrepresentative, you cannot trust the metrics it produces. If you cannot trust your metrics, you cannot make informed decisions about which training data improvements will help.

A team that discovers their model scores 90 percent on their synthetic eval set but only 75 percent on real data has lost the feedback loop between training and evaluation.

They do not know whether the gap is due to insufficient training data coverage, poor training data quality, or eval set miscalibration. Without a trustworthy eval set, iterative improvement becomes guesswork.

The solution is to invest in a high-quality real-world eval set early. Even a small real-world eval set of 500 examples provides a trust anchor. You use the real-world set to validate your synthetic eval set. You use the synthetic eval set to scale evaluation when you need more examples for slice-specific analysis or statistical significance.

But you always report metrics on the real-world set as the primary metric.

## The Cost-Quality Trade-off for Eval Sets

Synthetic eval sets are cheaper and faster to generate than real eval sets, but they carry higher risk of miscalibration. Real eval sets are expensive and slow to collect, but they provide trustworthy metrics. The cost-quality trade-off is different for eval sets than for training sets because the consequences of low-quality eval data are different.

Low-quality training data degrades model performance gradually and incrementally. Low-quality eval data hides performance degradation until it manifests in production.

For high-stakes applications, the cost of a miscalibrated eval set far exceeds the cost of collecting a real eval set. A medical diagnosis model that scores 94 percent on a synthetic eval set but 80 percent in the real world could harm patients. The cost of patient harm is orders of magnitude higher than the cost of collecting 1,000 real annotated medical cases.

For low-stakes applications, the cost-quality trade-off tilts toward synthetic eval sets. A product recommendation model that scores 85 percent on a synthetic eval set and 82 percent in the real world has a small performance gap, and the business impact of that gap is limited. The cost of collecting real data might exceed the value gained from more accurate metrics.

You make the cost-quality trade-off explicitly based on risk tolerance, not implicitly based on convenience.

## Eval Set Composition Across the Development Lifecycle

Eval set composition should shift across the model development lifecycle, just as training set composition does. Early in development, when you are experimenting with architectures and features, you use small real-world eval sets supplemented with synthetic data. The goal is fast iteration, not precise measurement.

A prototype model might evaluate on 300 real examples and 700 synthetic examples because the 300 real examples provide directional feedback and the synthetic examples provide statistical power.

As you move toward production, you shift to larger real-world eval sets. A production candidate model might evaluate on 2,000 real examples and no synthetic examples because the goal is to measure real-world performance accurately. Synthetic eval data introduces risk without corresponding benefit at this stage.

After deployment, you shift to continuous evaluation on production data. A model in production evaluates on fresh real-world data collected weekly or monthly. Synthetic eval data is no longer relevant because real data is abundant and reflects current reality.

This lifecycle approach ensures that you use synthetic eval data where it accelerates development and avoid it where it introduces measurement risk.

## Reporting Metrics on Synthetic vs Real Eval Sets

When you report model performance to stakeholders, you distinguish clearly between metrics measured on synthetic eval sets and metrics measured on real eval sets. You do not present synthetic eval metrics as if they were real eval metrics. You report both and you explain the difference.

A model performance report might state: "On our real-world evaluation set of 500 customer support tickets, the model achieved 88 percent accuracy. On our synthetic evaluation set of 2,000 tickets, the model achieved 91 percent accuracy. The 3-point gap indicates that our synthetic eval set is slightly easier than real data, and we consider 88 percent to be the true performance estimate."

This transparency allows stakeholders to understand the limitations of synthetic eval data and make informed decisions about model deployment.

You also report the validation results that justify using synthetic eval data. You explain how you validated that synthetic data correlates with real data, what the correlation strength is, and what the residual risk is. Stakeholders need context to interpret synthetic eval metrics correctly.

Synthetic eval data is a tool, not a shortcut. It allows you to scale evaluation when real data is scarce, but it does not replace the need for real-world validation. The risks are higher, the requirements are stricter, and the validation burden is heavier.

You use synthetic eval data when you have no alternative, and you validate it exhaustively before trusting the metrics it produces. The next step in building robust synthetic data pipelines is ensuring that the data remains high-quality and representative as your task evolves, your domain shifts, and your model requirements change.
