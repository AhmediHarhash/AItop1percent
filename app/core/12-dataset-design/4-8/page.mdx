# 4.8 â€” Outlier Detection and Boundary Cases

In June 2025, a fraud detection startup spent $280,000 building a transaction monitoring model trained on two years of payment data from a regional payment processor. The model achieved 94% precision and 91% recall on their evaluation set. When deployed to production, it immediately flagged thousands of legitimate transactions as fraudulent, overwhelming the fraud investigation team and triggering customer complaints. The engineering team analyzed the false positives and discovered that the model was flagging any transaction over $15,000 as fraud. During dataset preparation, they had identified high-value transactions as statistical outliers and removed them to improve model stability. They removed 1,200 transactions over $15,000 from a dataset of 340,000 transactions, less than 0.4% of the data. The model never learned that high-value transactions could be legitimate because it never saw examples in training. In production, the payment processor served corporate clients who regularly made five-figure payments for equipment, payroll, and vendor settlements. The model treated every one of these as an anomaly.

The root cause was treating all outliers as errors to remove rather than distinguishing between outliers that represent data quality issues and outliers that represent legitimate edge cases. The team applied statistical outlier detection and removed everything beyond three standard deviations from the mean without examining what those outliers represented. They optimized for a clean, well-behaved training distribution at the cost of model robustness in production. By the time they discovered the issue, they had to retrain the model from scratch with outliers preserved, delay their production rollout by two months, and rebuild trust with customers who had been incorrectly flagged.

## Outliers Are Not All Errors

An outlier is a value that deviates significantly from the central tendency of a distribution. A transaction of $50,000 in a dataset where the median transaction is $47 is an outlier. A user session lasting six hours in a dataset where the median session is twelve minutes is an outlier. A product review of 3,000 words in a dataset where the median review is 40 words is an outlier. Outliers are unusual, but unusual is not the same as wrong.

Some outliers are errors: data entry mistakes, sensor malfunctions, parsing failures, or corruption. A transaction of negative $50,000 is an error. A user session lasting negative six hours is an error. A product review containing binary data instead of text is an error. These outliers should be removed because they represent data quality issues, not real-world variation.

Other outliers are edge cases: rare but legitimate events that occur in production and must be handled correctly. A corporate client making a $50,000 equipment purchase is an edge case. A user binge-watching six hours of video content is an edge case. A professional reviewer writing a 3,000-word analysis is an edge case. These outliers should be preserved because they represent real-world variation that your model must learn to handle.

The distinction between errors and edge cases cannot be made purely statistically. Statistical outlier detection identifies values that are unusual relative to the distribution, but it does not distinguish between unusual-because-wrong and unusual-because-rare. You must combine statistical detection with domain knowledge, validation rules, and inspection to classify outliers correctly.

Teams that remove all outliers optimize for training set cleanliness at the cost of production robustness. A model trained on a narrow, well-behaved distribution will perform well on evaluation data drawn from the same distribution but fail catastrophically when it encounters edge cases in production. A fraud detection model that never sees high-value transactions will treat all high-value transactions as fraud. A recommendation model that never sees long sessions will treat all long sessions as anomalies. A content moderation model that never sees lengthy, detailed reviews will treat all such reviews as spam.

Teams that preserve all outliers risk training on corrupted data and learning spurious patterns. A model trained on transactions with data entry errors will learn that negative amounts are possible. A model trained on sessions with timestamp corruption will learn that time can run backward. A model trained on reviews containing binary data will learn meaningless patterns from garbage. You must remove errors, but you must preserve edge cases. The challenge is distinguishing between them.

## Statistical Outlier Detection Methods

Statistical outlier detection identifies values that deviate from the expected distribution based on summary statistics or probabilistic models. The simplest method is the three-sigma rule: any value more than three standard deviations from the mean is flagged as an outlier. For normally distributed data, this captures approximately 99.7% of values, flagging the remaining 0.3% as outliers.

The three-sigma rule fails when data is not normally distributed. If your transaction amounts follow a long-tailed distribution where most transactions are small but a significant fraction are large, the mean and standard deviation do not describe the distribution well. Three standard deviations from the mean might flag all large transactions as outliers even though large transactions are a regular part of your data. You need distribution-aware outlier detection.

Interquartile range methods are more robust to non-normal distributions. You compute the first quartile Q1, the third quartile Q3, and the interquartile range IQR equals Q3 minus Q1. Any value below Q1 minus 1.5 times IQR or above Q3 plus 1.5 times IQR is flagged as an outlier. This method is less sensitive to extreme values and works well for skewed distributions. It flags approximately the most extreme 0.7% of values in a normal distribution, similar to the three-sigma rule, but adapts to the actual data distribution.

Percentile-based methods flag values below the Pth percentile or above the 100 minus P percentile. If you flag values below the 1st percentile or above the 99th percentile, you flag 2% of your data as outliers. Percentile-based methods are simple and distribution-agnostic, but they flag a fixed proportion of data regardless of whether that data is actually unusual. In a perfectly uniform distribution, the 1st and 99th percentiles are not outliers, they are just the edges of the range.

Isolation forests and other ensemble methods detect outliers by measuring how easily a value can be isolated from the rest of the data. An outlier is a value that can be separated from other values with fewer splits in a decision tree. Isolation forests build multiple random trees and measure the average path length to isolate each value. Short path lengths indicate outliers. This method works well for high-dimensional data and complex distributions, but it is slower and less interpretable than statistical rules.

You apply statistical outlier detection as a first pass to identify candidates, not as a final decision. Statistical methods flag values that are unusual, but you must inspect those values to determine whether they are errors or edge cases. You log all flagged values, analyze their distribution, and apply validation rules and domain knowledge to classify them. Statistical detection narrows the search space, but human judgment or rule-based validation makes the final call.

## Embedding-Space Outlier Detection

Embedding-space outlier detection identifies records that are semantically unusual, not just statistically unusual. You embed each record into a vector space using a model or embedding function, then measure distance to neighbors. Records that are far from all other records in embedding space are semantic outliers.

This method is particularly useful for text, where statistical outlier detection on length or word count misses semantic anomalies. A product review that is 500 words long is not statistically unusual, but if it contains unrelated content or nonsensical text, it is semantically unusual. You embed the review using a sentence transformer or language model, compute its distance to the nearest neighbors in embedding space, and flag reviews with large distances as outliers.

Embedding-space outlier detection requires choosing an embedding model and a distance metric. For text, you use sentence transformers, document embeddings, or fine-tuned language models. For images, you use vision transformers or convolutional embeddings. For structured data, you use learned embeddings or autoencoders. The choice of embedding model determines what "unusual" means: a model trained on product reviews will embed spam or off-topic reviews far from legitimate reviews, while a model trained on general text might not distinguish them.

Distance metrics include cosine distance, Euclidean distance, and Mahalanobis distance. Cosine distance measures angular similarity and is common for high-dimensional embeddings. Euclidean distance measures absolute position and is sensitive to embedding scale. Mahalanobis distance accounts for covariance and is robust to correlated dimensions. You choose a distance metric based on your embedding space properties and outlier definition.

You compute outlier scores based on distance to nearest neighbors. The simplest score is the distance to the kth nearest neighbor, where k is chosen based on dataset size and desired sensitivity. A record whose 10th nearest neighbor is far away is in a sparse region of embedding space and is likely an outlier. You can also use the average distance to the k nearest neighbors, or the local outlier factor, which compares a record's neighbor distances to its neighbors' neighbor distances. Local outlier factor detects records in sparse regions even if the overall dataset is dense.

Embedding-space outlier detection complements statistical outlier detection. Statistical methods catch records with unusual numeric values. Embedding methods catch records with unusual semantic content. You apply both methods and flag records identified by either. A product review that is both unusually long and semantically off-topic is flagged by both methods and is a strong outlier candidate. A review that is unusually long but semantically normal is flagged only by statistical methods and might be a legitimate edge case.

## Distinguishing Errors from Edge Cases

Once you have identified outlier candidates, you must classify them as errors to remove or edge cases to preserve. This classification requires validation rules, domain knowledge, and often manual inspection. You cannot make this decision purely algorithmically because the distinction depends on real-world semantics, not just data patterns.

Validation rules identify impossible or implausible values. A transaction amount less than zero is impossible in most payment systems and is an error. A user age greater than 120 is implausible and is likely an error. A timestamp in the future relative to the data collection date is impossible and is an error. You encode validation rules as explicit checks and automatically flag violations as errors. These outliers are removed without inspection.

Plausibility checks identify values that are possible but unlikely. A transaction of exactly $0.00 is possible but unusual and might indicate a refund, a test transaction, or a data quality issue. A user session of exactly zero seconds is possible but unusual and might indicate a session initialization failure. You flag plausible-but-unlikely values for inspection rather than automatic removal. You examine examples and determine whether they represent legitimate cases or errors.

Domain knowledge determines whether unusual values represent edge cases. In fraud detection, high-value transactions are edge cases because corporate clients make large payments. In content moderation, lengthy reviews are edge cases because professional reviewers write detailed critiques. In recommendation systems, long sessions are edge cases because some users binge-watch content. You consult domain experts, analyze production usage patterns, and examine historical data to identify edge cases. You preserve these outliers because they represent real user behavior.

Manual inspection is required when validation rules and domain knowledge are insufficient. You sample flagged outliers, inspect them individually, and classify them as errors or edge cases. You look for patterns: if all high-value transactions come from a single corrupted source file, they are likely errors; if they are distributed across time and sources, they are likely edge cases. If all unusually long reviews contain garbled text, they are errors; if they contain coherent, detailed content, they are edge cases. You document patterns and convert them into automated rules for future datasets.

You build a decision tree for outlier classification. First, apply validation rules to identify impossible values and remove them as errors. Second, apply plausibility checks to identify unlikely values and flag them for inspection. Third, apply domain knowledge to classify flagged values as edge cases or errors. Fourth, manually inspect ambiguous cases and document classification decisions. This process is iterative: as you inspect more outliers, you refine validation rules and plausibility checks to reduce manual inspection requirements.

## Boundary Case Preservation Strategies

Boundary cases are records at the edges of your feature distributions that are critical for model robustness. A fraud detection model must handle the boundary between legitimate high-value transactions and fraudulent high-value transactions. A recommendation model must handle the boundary between short sessions and long sessions. A content moderation model must handle the boundary between harsh-but-legitimate criticism and actual abuse. If you remove boundary cases during cleaning, your model will fail at these critical decision boundaries.

You identify boundary cases by analyzing feature distributions and flagging records near distribution edges. For continuous features, boundary cases are values in the top and bottom percentiles. For categorical features, boundary cases are rare categories. For multi-dimensional features, boundary cases are records in sparse regions of the feature space. You flag these records for preservation rather than removal.

Boundary case preservation is distinct from outlier preservation. Outliers are extreme values beyond the typical range. Boundary cases are values at the edge of the typical range. A transaction of $50,000 might be an outlier if the 99th percentile is $10,000, or a boundary case if the 99th percentile is $45,000. Both should be preserved, but for different reasons: outliers teach the model that extreme values exist, boundary cases teach the model where to draw decision boundaries.

You preserve boundary cases even if they are noisy or ambiguous. A high-value transaction that is ambiguously legitimate or fraudulent is a valuable training example because it teaches the model the boundary between classes. A product review that is ambiguously helpful or spam is a valuable training example because it teaches the model the boundary between classes. You do not want clean, unambiguous boundary cases because real-world boundaries are not clean and unambiguous.

You can oversample boundary cases to ensure the model learns them. If boundary cases represent 1% of your dataset, the model may underweight them during training. You duplicate boundary cases or assign them higher sample weights so the model prioritizes learning decision boundaries. This is particularly important for imbalanced tasks where the minority class is concentrated at distribution boundaries.

You evaluate models specifically on boundary cases. You construct a boundary case evaluation set by selecting records near decision boundaries and measure model performance on those records. A fraud detection model might achieve 95% accuracy overall but only 70% accuracy on high-value transactions. This signals that the model has not learned the boundary correctly and needs more boundary case examples or better features. Boundary case evaluation surfaces robustness issues that overall metrics hide.

## Outlier Handling and Model Robustness

Outlier handling directly impacts model robustness. A model trained on a dataset with outliers removed will fail when it encounters similar outliers in production. A model trained on a dataset with outliers preserved will learn that such values exist and handle them gracefully. The goal is not to remove all outliers, but to remove errors and preserve edge cases.

Robust models must generalize beyond the training distribution. If your training data covers transaction amounts from $1 to $10,000, and production sees transactions up to $50,000, your model must extrapolate. Extrapolation is unreliable unless the model has seen examples near the edge of the training distribution. If you remove all transactions over $10,000 as outliers, you eliminate the model's ability to extrapolate. If you preserve transactions from $10,000 to $50,000, you extend the training distribution and enable safer extrapolation.

Some model architectures are more sensitive to outliers than others. Linear models are sensitive to outliers because outliers have large residuals that dominate the loss function. Tree-based models are less sensitive because they partition the feature space and isolate outliers in separate leaves. Neural networks are moderately sensitive depending on the loss function and regularization. You must consider architecture when deciding how aggressively to remove outliers.

You can transform features to reduce outlier impact without removing outliers. Log transformations compress the range of long-tailed distributions and reduce the influence of extreme values. Clipping caps values at a maximum threshold, replacing all values above the threshold with the threshold value. Quantile transformation maps values to their percentile ranks, making the distribution uniform. These transformations preserve the presence of outliers while reducing their numerical influence on training.

Transformation-based outlier handling must be applied consistently in training and production. If you log-transform transaction amounts during training, you must log-transform them during inference. If you clip values at the 99th percentile during training, you must clip them at the same threshold during inference. Inconsistent transformations cause train-serve skew and model failures.

You document outlier handling decisions and make them explicit in your data pipeline. You record which outliers were removed as errors, which were preserved as edge cases, and which were transformed to reduce numerical influence. You version these decisions so you can reproduce training data and update handling logic as you learn more about your data. Transparency about outlier handling enables debugging and continuous improvement.

## When to Remove, When to Preserve, When to Flag

The decision to remove, preserve, or flag an outlier depends on confidence in classification, impact on model training, and impact on downstream usage. You remove outliers you are confident are errors. You preserve outliers you are confident are edge cases. You flag outliers you are uncertain about and handle them separately.

You remove outliers that violate validation rules or are impossible by definition. Negative transaction amounts, future timestamps, and garbled text are errors. Removing them improves data quality and does not harm model robustness. You remove these outliers automatically and do not preserve them.

You preserve outliers that represent legitimate edge cases identified through domain knowledge or manual inspection. High-value transactions, long sessions, and detailed reviews are edge cases. Preserving them ensures the model learns to handle real-world variation. You preserve these outliers and treat them as first-class training examples.

You flag outliers you are uncertain about and isolate them for separate analysis. If you cannot determine whether an outlier is an error or an edge case, you flag it and exclude it from training. You analyze flagged outliers over time, gather more context, and reclassify them as errors or edge cases. Flagging prevents you from making irreversible decisions based on incomplete information.

Flagged outliers can be used in evaluation even if they are not used in training. If you are uncertain whether high-value transactions are legitimate, you can exclude them from training but include them in your evaluation set to measure how the model handles them. If the model performs poorly on flagged outliers, you have evidence that they represent important cases and should be preserved in training. If the model performs well, you have evidence that it generalizes beyond the training distribution.

You build outlier handling as a multi-stage process. First, apply statistical and embedding-space outlier detection to identify candidates. Second, apply validation rules to remove obvious errors. Third, apply plausibility checks and domain knowledge to classify remaining outliers. Fourth, manually inspect ambiguous cases and flag uncertain outliers. Fifth, document decisions and update handling rules. This process is reproducible, auditable, and improvable over time.

## Outlier Detection as Continuous Quality Monitoring

Outlier detection is not a one-time cleaning step. It is a continuous monitoring process that tracks data distribution changes and detects new errors and edge cases as they appear. You build outlier detection into your data pipeline and run it on every batch of incoming data.

Continuous outlier detection tracks outlier rates over time. If the outlier rate spikes, you have a data quality issue or a distribution shift. If the outlier rate drops, you have a change in data collection or source systems. You set thresholds on outlier rates and alert when thresholds are exceeded. You investigate alerts to determine whether outliers represent errors, edge cases, or distribution changes.

You track outlier patterns over time. If outliers cluster around specific values, sources, or time periods, you have a systematic issue. If all outliers above $20,000 come from a single source system, you have a data quality issue in that system. If all outliers appear in the last week of data, you have a recent change in data collection or user behavior. Pattern analysis surfaces root causes that aggregate metrics hide.

You update outlier handling rules as you learn more about your data. If you discover that high-value transactions are legitimate edge cases, you update your plausibility checks to flag them less aggressively. If you discover that certain text patterns indicate spam, you update your validation rules to remove them automatically. Rule updates are versioned and tested before deployment.

Continuous outlier detection enables rapid response to data quality issues. If a source system starts emitting corrupted data, outlier detection catches it before it contaminates your training set. If a new user behavior emerges, outlier detection catches it before it surprises your production model. Monitoring transforms outlier detection from a reactive cleanup task into a proactive quality assurance process.

You treat outlier detection as part of data quality infrastructure, not as a modeling task. Outlier detection runs before modeling, informs data cleaning decisions, and ensures that training data is fit for purpose. It is the boundary between raw data and curated datasets, and it determines whether your models learn real-world patterns or data quality artifacts. You invest in robust outlier detection, document handling decisions, and monitor continuously to maintain data quality at scale.

From outlier detection and boundary case handling, we now move to quality scoring and tiering, where instead of binary keep-or-drop decisions, we assign quality scores and use data selectively based on fitness for purpose.
