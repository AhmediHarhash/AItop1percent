# 2.13 â€” Data Collection for Low-Resource Domains

Not every problem has enough data to support a data-driven solution. This is not a resourcing issue. It is a reality constraint. When you operate in domains where examples are genuinely scarce, where patients with rare genetic conditions number in the hundreds worldwide, where endangered languages have fewer than fifty fluent speakers remaining, or where novel industrial defect types have been observed twelve times in history, standard data collection practices fail. You cannot brute-force your way to sufficient training data. Hiring more annotators does not create examples that do not exist. The $420,000 spent collecting 127 labeled examples across eleven ultra-rare disorders produced a model barely better than random guessing because 127 examples cannot teach a model to distinguish eleven complex categories.

The root cause was not inadequate effort or poor execution. The team had followed standard data collection practices perfectly. The problem was that standard practices assume data availability. When you operate in a genuinely low-resource domain, where examples are scarce by nature and cannot be manufactured through simple labeling effort, you need fundamentally different strategies. You cannot brute-force your way to a large dataset. You must bootstrap intelligently, transfer knowledge from adjacent domains, synthesize thoughtfully, and extract maximum value from every real example you obtain. The company had treated rare disease diagnosis like a standard computer vision problem. It is not. Low-resource domains demand low-resource methods.

## Recognizing True Low-Resource Domains

Not every small dataset indicates a low-resource domain. If you have 200 labeled examples but could easily obtain 2,000 more by hiring annotators for a week, you have a labeling budget problem, not a low-resource problem. True low-resource domains exhibit intrinsic scarcity. The data does not exist in sufficient volume, or it exists but is inaccessible due to privacy, cost, expertise requirements, or physical rarity.

Rare languages represent classic low-resource domains. Fewer than 100 native speakers remain for some endangered languages. You cannot commission 10,000 translation pairs when only a handful of fluent speakers exist worldwide. Specialized medical conditions affecting tiny patient populations create low-resource domains. You cannot collect 50,000 labeled MRI scans for a disorder that affects 3,000 people globally, especially when privacy regulations and patient consent limit data sharing.

Emerging industries or novel phenomena create low-resource domains. When a new type of financial fraud appears, historical examples are scarce by definition. Industrial processes in highly specialized manufacturing sectors often have proprietary, limited, or zero publicly available training data. A semiconductor manufacturer developing defect detection for a novel chip architecture has no external dataset to reference. The defect patterns are unique to their process and their equipment.

The defining characteristic is that scaling your labeling effort does not proportionally increase available data. You hit a hard ceiling imposed by reality, not budget. When you recognize this ceiling early, you can deploy appropriate strategies. When you fail to recognize it, you waste months pursuing data that does not exist.

Recognition requires honest assessment. You ask domain experts how many examples exist worldwide, not just in your organization. You research academic literature to see if others have published datasets in this domain. You check whether commercial data providers offer relevant data. If the answer to all these questions is "very few" or "none," you are in a true low-resource domain. You adjust your strategy before you waste budget on unachievable collection goals.

## Transfer Learning from Adjacent Domains

The most powerful tool for low-resource domains is knowledge transfer from high-resource adjacent domains. Instead of learning your specific task from scratch using scarce examples, you leverage models pretrained on abundant related data, then fine-tune on your limited target examples. The challenge is defining "adjacent" correctly. Naive transfer uses superficially similar domains. Effective transfer identifies deep structural or semantic similarity.

Consider the rare disease imaging case. The failed approach treated each rare disorder as an isolated classification problem. A transfer-based approach would recognize that all genetic disorders share underlying biological mechanisms, imaging modality characteristics, and visual feature patterns. You start with a model pretrained on general medical imaging, using datasets like ChestX-ray14 or ImageNet medical subsets. This model already understands anatomical structures, tissue contrast, imaging artifacts, and pathology indicators.

You then fine-tune on related but more common conditions that share visual characteristics with your rare target disorders. If your rare disorder affects connective tissue, you fine-tune on common connective tissue disorders first. If it manifests as unusual bone growth patterns, you fine-tune on common bone pathologies. This staged transfer allows you to extract far more value from your 127 rare examples. The model is not learning to interpret medical images from scratch. It is learning to distinguish subtle variations within a framework it already understands.

A team at a medical AI company used this approach for a rare pediatric lung condition affecting roughly 2,000 patients worldwide. They had 89 labeled chest X-rays. They fine-tuned a model pretrained on 100,000 general pediatric chest X-rays, then on 5,000 common pediatric lung disease cases, then finally on their 89 rare condition examples. Their final model achieved 81% accuracy, compared to 42% when training only on the rare examples directly.

Transfer works beyond imaging. For rare languages, you transfer from linguistically related high-resource languages. If you are building a model for a rare indigenous language with only 300 translated sentence pairs, you transfer from a closely related language with millions of examples, leveraging shared grammatical structures and phonetic patterns. For niche industrial defect detection, you transfer from general defect detection models trained on common manufacturing processes, then fine-tune on your specific rare defect types.

The key is investing time in identifying the right source domain. This requires domain expertise. You must understand the structural similarities, not just surface-level category overlap. A rare skin condition might transfer better from dermatology datasets than from general medical imaging. A rare legal document type might transfer better from contract analysis models than from general text classification. Choosing the wrong source domain wastes your scarce labeled examples on a model that learned irrelevant patterns.

## Evaluating Transfer Domain Suitability

Before committing to a transfer domain, you evaluate its suitability through empirical testing. You take a small validation set from your target domain and measure how well the pretrained model performs without any fine-tuning. If the pretrained model achieves better-than-random performance, the domains share meaningful structure. If it performs no better than random, the domains are too distant.

You also measure feature similarity. You extract feature representations from the pretrained model for both source and target domain examples. You compute similarity metrics like cosine distance or clustering overlap. If target examples cluster near source examples in feature space, the domains are related. If they occupy entirely separate regions, transfer will be limited.

A financial services company evaluating transfer domains for fraud detection on a new cryptocurrency product tested three candidate source domains: traditional credit card fraud, stock market manipulation, and online auction fraud. They measured how well models pretrained on each domain performed on a small validation set of cryptocurrency fraud. The stock market manipulation model performed best, achieving 62% precision without fine-tuning, compared to 38% for credit card fraud and 41% for auction fraud. The domains shared temporal transaction patterns and price manipulation signals, making stock market fraud the best transfer source.

## Synthetic Data Augmentation for Scarce Examples

When real examples are scarce, synthetic augmentation can multiply your effective dataset size, but only if done with domain-appropriate transformations. Naive augmentation applies generic transformations that may destroy the very features your model needs to learn. Effective augmentation preserves critical characteristics while introducing realistic variation.

For image data in medical domains, simple geometric transformations like random crops, flips, and rotations often work well because anatomical structures remain recognizable under these changes. A lung nodule rotated 30 degrees is still a lung nodule. But aggressive color shifts or contrast changes can introduce unrealistic artifacts that confuse models, especially when working with specific imaging modalities where color and contrast carry diagnostic meaning. You augment within the bounds of real-world variation. If your imaging device produces a known range of brightness levels, you augment within that range, not beyond it.

For text in rare languages, back-translation provides effective augmentation. You translate your rare-language sentences to a high-resource language, then translate back. The round-trip introduces lexical and syntactic variation while preserving semantic meaning. This works when translation models exist for your rare language, even if imperfect. If no translation models exist, you can use synonym replacement based on available dictionaries or morphological transformations that apply your language's grammatical rules to generate valid alternative phrasings.

For audio in low-resource domains like rare dialects or endangered languages, you apply acoustic augmentation: adding background noise, adjusting pitch and speed within natural ranges, simulating different recording conditions. A linguist working on a language with 47 remaining speakers recorded 18 hours of conversational audio. Through careful augmentation adding realistic background environments, microphone quality variation, and speaker pitch adjustments, they expanded this to an effective 180 hours of training data. Their speech recognition model, trained on the augmented set, achieved word error rates comparable to models trained on 400 hours of unaugmented data in similar low-resource languages.

The principle is that augmentation must be domain-informed. You need to understand what variations are realistic and what features are invariant. Generic augmentation libraries apply transformations that work for high-resource domains like ImageNet classification. Those same transformations may destroy diagnostic features in medical imaging or introduce phonetically impossible sounds in speech data. You create custom augmentation pipelines informed by domain expertise, validated by domain experts. You show augmented examples to your medical reviewers or native speakers and ask: does this look or sound real? If not, you are training your model on unrealistic data, which degrades performance.

## Augmentation Quality Control

Quality control for augmented data requires systematic review by domain experts. You sample augmented examples and present them alongside real examples to experts who do not know which is which. You ask them to identify augmented examples. If experts cannot reliably distinguish augmented from real, your augmentation is realistic. If they can easily spot augmented examples, your transformations are introducing detectable artifacts.

You also measure model performance on augmented versus real data. You train two models: one on real examples only, one on real plus augmented examples. You evaluate both on a held-out set of real examples. If the augmented model outperforms the real-only model, augmentation is helping. If performance is equivalent or worse, augmentation is not adding value or is introducing noise.

A manufacturing company building defect detection for a rare component type had 45 labeled defect images. They created augmented versions using rotation, scaling, and lighting adjustments. They showed 100 mixed real and augmented images to three quality engineers. The engineers correctly identified augmented images 78% of the time, indicating augmentation was producing unrealistic examples. The team refined their augmentation to use narrower parameter ranges based on engineer feedback. In the second round, engineers identified augmented images only 52% of the time, near chance. Models trained on this refined augmented set achieved 12 percentage points higher precision than models trained on real data alone.

## Active Learning for Efficient Example Selection

When you can obtain new labeled examples but only at high cost or effort, active learning helps you choose which examples to label next. Instead of randomly sampling unlabeled data, you query your model to identify the examples it is most uncertain about or that would most improve its decision boundary. This maximizes the information gained per labeled example, critical when each label requires expensive expert time.

The process starts with a small initial labeled set. You train a model on this set, then apply it to a large pool of unlabeled examples. The model outputs predictions with confidence scores. You select examples where the model is least confident, uncertain between multiple classes, or near decision boundaries. You send these examples to human experts for labeling. Once labeled, you add them to your training set, retrain, and repeat. This iterative loop focuses labeling effort on the examples that teach the model the most.

A team building a fraud detection system for a new type of cryptocurrency scam had access to 50 confirmed fraud cases and millions of unlabeled transactions. Labeling required forensic blockchain analysis by specialists charging $200 per transaction review. Random sampling would have required labeling thousands of transactions to build a useful model. Instead, they used uncertainty sampling. They trained an initial model on the 50 confirmed cases, applied it to 100,000 unlabeled transactions, and selected the 50 transactions where the model's fraud probability score was closest to 0.5, maximally uncertain. Expert review of these 50 revealed 23 additional fraud cases. They retrained, repeated, and after four cycles with 200 total labeled examples, their model achieved 87% precision and 79% recall. Random sampling would have required approximately 800 labeled examples to reach similar performance, a $160,000 cost difference.

Active learning works best when your unlabeled pool is large and your labeling cost is high. If you already have only 100 unlabeled examples total, active learning provides minimal benefit. If labeling is cheap and fast, random sampling may be simpler. But in true low-resource domains, where expert time is the bottleneck, active learning is often the difference between a viable project and an economically impossible one.

## Active Learning Strategy Selection

Different active learning strategies work better for different domains. Uncertainty sampling selects examples where the model is least confident. This works well when your model's confidence scores are calibrated and when decision boundaries are the primary learning challenge. Query-by-committee uses an ensemble of models and selects examples where the models disagree most. This reduces the risk of selecting outliers that a single model finds confusing for idiosyncratic reasons.

Diversity-based sampling ensures you select examples covering different regions of feature space, not just clustering around a single ambiguous boundary. This prevents oversampling near one boundary while ignoring other important regions. Expected model change selects examples that would most change the model's parameters if added to the training set. This requires more computation but can be more efficient than uncertainty sampling.

A legal technology company building a contract clause classifier for a rare clause type tested all four strategies. They had 30 labeled examples and 50,000 unlabeled contracts. Uncertainty sampling selected 20 new examples for labeling, of which 8 were the target clause type. Query-by-committee selected 20 examples, of which 11 were the target clause. Diversity sampling selected 20 examples, of which 6 were the target clause but they covered more contract types. Expected model change selected 20 examples, of which 10 were the target clause. The team chose query-by-committee for its balance of efficiency and robustness.

## Expert Elicitation and Rule-Based Bootstrapping

When labeled examples are impossibly scarce, you can bootstrap models using expert knowledge directly encoded as rules or heuristics. This is not a replacement for data-driven learning, but a starting point that provides initial signal, which you then refine with whatever limited data you can obtain.

Domain experts often have explicit knowledge about patterns, indicators, and decision logic that can be formalized as rules. A medical expert can describe the visual features that distinguish a rare disorder: specific bone density patterns, tissue texture characteristics, size ratios between structures. You encode these as heuristic classifiers. These rule-based systems are brittle and incomplete, but they provide better-than-random predictions. You use their predictions as weak labels for large amounts of unlabeled data, creating a noisy but large synthetic training set. You then train a model on this weakly labeled data, and fine-tune on your small set of gold-standard expert-labeled examples.

A legal technology company building a classifier for a rare type of regulatory filing had access to only 34 confirmed examples. Hiring lawyers to label thousands of filings was cost-prohibitive. Instead, they interviewed regulatory specialists and extracted 18 heuristic rules: documents must contain specific regulatory language, reference certain statutes, follow particular formatting conventions, and appear in specific date ranges. They applied these rules to 50,000 unlabeled legal documents, generating weak labels. Approximately 15% of the weak labels were likely incorrect, but 85% were likely correct. They trained a model on the weakly labeled set, then fine-tuned on the 34 gold examples. The final model achieved 76% precision and 68% recall, far better than random and sufficient for a prioritization tool that routed documents to human reviewers.

Expert elicitation works when experts can articulate decision logic. Some domains resist this. Experts may have tacit knowledge they cannot easily verbalize. Radiologists can recognize a rare condition on sight but struggle to articulate exactly what visual features they use. In these cases, you can use interactive machine teaching approaches. You show the expert a series of unlabeled examples and ask them to label a few, then the model makes predictions on others, and the expert corrects mistakes. The model learns from corrections iteratively. This is more efficient than asking the expert to label thousands of examples blindly, because the expert focuses on correcting the model's specific errors, teaching by counterexample.

Rule-based bootstrapping also includes data programming frameworks like Snorkel, where you write labeling functions that encode heuristics, patterns, or distant supervision signals. Each function provides a noisy label. The framework learns the accuracy of each function and combines their outputs into probabilistic training labels. You write many weak labeling functions, each capturing different aspects of expert knowledge, and the combination produces a larger, noisier training set that still improves model performance when refined with limited gold labels.

## Community Partnerships and Crowdsourcing in Specialized Domains

Some low-resource domains have small but highly engaged communities. Rare disease patient advocacy groups, indigenous language preservation societies, niche hobbyist communities, and specialized professional networks often have members willing to contribute data or labeling effort if approached respectfully and given appropriate credit and control.

These partnerships differ from commercial crowdsourcing. You are not hiring anonymous workers on a labeling platform. You are collaborating with stakeholders who have deep personal or professional investment in the domain. This requires different engagement models. You offer co-ownership of data, transparency about how models will be used, and often shared access to resulting tools. A team building a speech recognition system for an endangered language with fewer than 200 fluent speakers partnered with a language preservation nonprofit. Community elders recorded conversational audio, and younger community members transcribed it, contributing thousands of hours over two years. The resulting model was released open-source, and the community retained full rights to the data. This model is now used in language education apps for the community.

Patient communities for rare diseases often collect their own health data through registries, self-reported outcomes, and shared medical records. Researchers building models for rare diseases can partner with these registries, accessing real-world data that would be impossible to obtain through traditional clinical trials or hospital data-sharing agreements. A rare cancer research group partnered with a patient advocacy organization that maintained a registry of 1,200 patients worldwide. The registry included treatment histories, outcomes, genetic profiles, and imaging data voluntarily contributed by patients. This data enabled training predictive models for treatment response that would have been impossible using any single institution's data.

Community crowdsourcing works when you align incentives. The community benefits from better tools, better understanding, or preservation of their language or culture. You benefit from access to scarce data and domain expertise. The relationship must be reciprocal and transparent. Extractive approaches, where you take data and provide nothing in return, damage trust and often violate ethical principles. Collaborative approaches, where you build tools that serve the community's goals and share results openly, build sustainable long-term partnerships.

You also need to recognize that community members are not professional annotators. Annotation quality may be variable. You need quality control mechanisms that are respectful, not punitive. You provide training, feedback, and tools that make annotation easier. You build consensus labeling, where multiple community members label the same examples and you use agreement as a quality signal. You involve community experts in defining labeling guidelines and resolving ambiguous cases. This process is slower than hiring a commercial labeling service, but it produces data that reflects genuine domain understanding and community values.

## Combining Strategies for Maximum Leverage

Low-resource domains require combining multiple strategies. You do not choose transfer learning or synthetic augmentation or active learning. You use all of them in sequence or in parallel, extracting every possible bit of signal from every available source.

A standard pipeline starts with transfer learning to initialize a model with relevant knowledge from a high-resource domain. You then apply synthetic augmentation to your small labeled set, expanding it to a larger effective training set. You train on the augmented data. Next, you use active learning to select the most informative unlabeled examples to send for expert labeling, maximizing the value of each new label. You incorporate expert elicitation by encoding known heuristics as additional features or as weak supervision signals. You engage community partners to obtain additional real-world data and validation. Each strategy compensates for the weaknesses of the others.

Transfer learning gives you a strong starting point but may not capture domain-specific nuances. Synthetic augmentation provides volume but introduces artificial patterns. Active learning focuses labeling effort but requires an initial model to bootstrap. Expert elicitation provides high-quality signal but does not scale. Community partnerships provide authentic data but require long-term relationship building. Together, they form a robust pipeline that makes low-resource projects viable.

A wildlife conservation organization building a species identification model for a critically endangered bird with fewer than 50 individuals remaining worldwide combined all these strategies. They transferred from a model pretrained on 10,000 common bird species. They augmented their 200 photographs of the rare species with realistic lighting, background, and pose variations validated by ornithologists. They used active learning to prioritize which new field camera trap images to send for expert review. They elicited expert rules about distinctive plumage patterns and encoded them as features. They partnered with a global network of birdwatchers who contributed sightings and photos through a citizen science app. The resulting model achieved 89% accuracy, enabling automated monitoring of the species across its remaining habitat. No single strategy would have succeeded alone.

## Ethical and Practical Considerations in Scarce Data Contexts

Low-resource domains often involve vulnerable populations, sensitive information, or high-stakes decisions. When data is scarce, each example carries outsize influence on model behavior. A single mislabeled example in a dataset of 50 can corrupt decision boundaries. A single biased example can encode harmful stereotypes that propagate through all predictions. You must handle scarce data with exceptional care.

Privacy and consent are heightened concerns. When you have only 100 patients with a rare disease, anonymization is harder. Individuals may be identifiable through combinations of rare attributes. You need stricter de-identification, differential privacy techniques, or federated learning approaches that train models without centralizing data. You also need informed consent that accurately describes risks. Patients contributing data to a rare disease registry need to understand that their data, while anonymized, may be uniquely identifiable due to rarity.

Bias is amplified in low-resource settings. If your 80 labeled examples come from a single hospital, a single geographic region, or a single demographic group, your model will reflect that narrow distribution. In high-resource domains, you can sample broadly to ensure diversity. In low-resource domains, you may not have that luxury. You must document limitations explicitly and constrain deployment to contexts that match your data distribution. A model trained on rare disease data from European hospitals should not be deployed in African or Asian contexts without validation, because disease presentation, comorbidities, and treatment patterns may differ.

You also face the risk of overfitting on small datasets. Models with millions of parameters can memorize 100 training examples perfectly, learning no generalizable patterns. You need aggressive regularization, simpler model architectures, or ensemble methods that average over many small models to reduce overfitting. Cross-validation becomes critical but also challenging when you have few examples. Leave-one-out cross-validation is common in low-resource settings, but it provides optimistic performance estimates. You need held-out test sets that you never touch during development, even if that means reserving 20% of your already tiny dataset.

Finally, you must recognize when a domain is too low-resource for a data-driven approach to succeed responsibly. If you have 20 labeled examples and no viable transfer domain, no augmentation strategy, and no access to experts or community data, a machine learning model may not be the right tool. Rule-based systems, expert consultation workflows, or acknowledging that the problem cannot yet be solved may be the responsible choice. Not every problem has a data-driven solution, especially when the data does not exist.

## Measuring Success in Low-Resource Contexts

Standard performance metrics still apply in low-resource domains, but your expectations must be calibrated to data availability. A model trained on 100,000 examples might achieve 95% accuracy. A model trained on 100 examples might achieve 75% accuracy, and that may still be valuable if it reduces manual review burden or provides decision support to experts.

You also measure data efficiency: how much performance you extract per labeled example. If your model achieves 70% accuracy with 50 examples, and a baseline model achieves 65% accuracy with 500 examples, you have achieved 10x data efficiency. This metric matters in low-resource domains because obtaining the next 50 examples may be impossible or prohibitively expensive.

You measure generalization to out-of-distribution data more rigorously. In high-resource domains, train and test data come from similar distributions. In low-resource domains, you often deploy to contexts slightly different from your training data because you cannot afford to collect training data from every deployment context. You need to test how well your model generalizes to related but distinct scenarios. Does your rare disease model trained on MRI data from one scanner generalize to a different scanner model? Does your rare language model trained on formal text generalize to conversational speech?

You also measure human-AI collaboration effectiveness. In low-resource high-stakes domains, models often serve as decision support tools, not autonomous decision-makers. You measure whether the model helps human experts make better decisions faster, not whether it replaces them. A model with 75% standalone accuracy might improve expert diagnostic accuracy from 82% to 91% when used as a second opinion, a meaningful real-world impact.

Success in low-resource domains is making the impossible possible. You take a problem where data-driven solutions seemed out of reach and create a working system through creative combination of transfer learning, augmentation, active learning, expert knowledge, and community partnership. You do not achieve state-of-the-art performance. You achieve sufficient performance to provide value, which is often transformative for domains that previously had no automated tools at all.

## Documentation and Knowledge Transfer

Low-resource domain work requires exceptional documentation because the strategies you develop are often unique to your problem. You document every decision: which transfer domain you chose and why, which augmentation transformations you validated, which active learning strategy worked best, which expert rules you encoded. This documentation serves two purposes. First, it enables reproducibility when you need to retrain or extend the model. Second, it provides a template for others working in similar low-resource domains.

You document data sources exhaustively. When you have only 100 examples, you need to know exactly where each one came from, when it was collected, who labeled it, and what quality checks it passed. Provenance is critical in low-resource settings because you cannot afford to lose data or discover months later that examples were mislabeled or duplicated.

Source documentation includes contact information for data providers, institutional review board approvals if applicable, data sharing agreements, and licensing terms. If you obtained examples from a rare disease patient registry, you document the registry contact, the data use agreement terms, and any restrictions on model deployment or publication. If a question arises three years later about data rights, you have the documentation to answer it.

You document model development choices. You record which architectures you tested, which hyperparameters you tuned, and which regularization techniques prevented overfitting. In high-resource settings, you can retrain quickly if you lose this information. In low-resource settings, retraining experiments are expensive because each requires scarce expert time for labeling or validation.

You maintain experiment logs that track every training run, including data version, model architecture, hyperparameters, and performance metrics. These logs enable you to identify which configuration produced the best results and why. They also prevent wasted effort repeating experiments that already failed.

You also invest in knowledge transfer to your team and stakeholders. Low-resource domain work often relies on specialized expertise. If the one data scientist who understands your transfer learning pipeline leaves, you need documented processes and trained backups. You pair team members, conduct knowledge-sharing sessions, and maintain runbooks that explain how to execute each pipeline step.

Knowledge transfer extends to domain experts who contributed to the project. You share results, explain how their input improved the model, and solicit feedback on model outputs. This builds trust and ensures experts remain engaged for future iterations. A rare language preservation project shared model performance reports with community elders quarterly, explaining improvements and requesting guidance on remaining errors. This ongoing dialogue kept the community invested and led to continuous model refinement.

## Long-Term Viability and Maintenance

Low-resource domain models require different maintenance strategies than high-resource models. You cannot simply retrain on fresh data quarterly because fresh data is scarce. Instead, you monitor model performance continuously and retrain only when performance degrades or when new examples become available.

You establish performance monitoring that tracks metrics on incoming data. When performance drops below thresholds, you investigate whether the data distribution has shifted, whether new edge cases have appeared, or whether the model needs retraining. You maintain a budget for emergency labeling when critical new examples must be added to the training set.

You also build relationships with data sources for long-term access. If your model relies on examples from a rare disease registry, you maintain contact with registry administrators and negotiate ongoing data access. If your model uses examples from an endangered language community, you maintain community relationships and contribute back to community goals. Long-term viability depends on sustained partnerships, not one-time data transactions.

The next subchapter addresses multimodal data collection, where you must coordinate pipelines across audio, image, and video modalities, each with unique technical and annotation challenges.
