# 5.5 â€” Reproducibility: Recreating Any Dataset State on Demand

In July 2025, a fintech company serving over 800,000 customers faced a regulatory inquiry from the SEC about a credit risk model used in loan approval decisions. The regulator requested that the company reproduce the exact training dataset and model from a specific date nine months prior to verify that the model complied with fair lending laws at the time of deployment. The engineering team had the model weights stored in S3. They had the training code in Git. They had logs showing which dataset version was used. What they could not do was recreate the dataset. The dataset had been generated by a series of SQL queries against a production database, and the database had been updated thousands of times since then. The queries were not deterministic: they used timestamps and random sampling without fixed seeds. The feature engineering pipeline pulled data from an external API that returned different results depending on when it was called. After two weeks of effort, the team produced a dataset that was approximately similar but not identical. The regulator was not satisfied with approximate. The company eventually settled the inquiry with a fine and a commitment to implement reproducible data pipelines. The root cause was not malice or negligence. It was the absence of reproducibility as a design requirement.

Reproducibility means that you can recreate the exact dataset used for any model training run or evaluation, on demand, even months or years later. This is not the same as versioning. Versioning stores snapshots. Reproducibility means you can regenerate the snapshot from source data and transformation logic. Reproducibility is required for debugging, auditing, compliance, and scientific rigor. If you cannot reproduce a dataset, you cannot validate your results, you cannot trust your experiments, and you cannot satisfy regulatory requirements.

## Deterministic Pipelines

The foundation of reproducibility is deterministic data pipelines. A pipeline is deterministic if it produces the same output every time it is run with the same inputs and configuration. Non-deterministic pipelines produce different outputs due to random sampling, timestamp dependencies, external API calls, or race conditions. You eliminate non-determinism by controlling randomness, freezing time, mocking external dependencies, and ensuring consistent execution order.

Random sampling is the most common source of non-determinism. If you sample 10,000 examples from a dataset of one million using a random function without a fixed seed, you will get a different sample every time you run the pipeline. The fix is to set a random seed at the beginning of the pipeline and use a deterministic random number generator. In Python, you call random.seed, numpy.random.seed, and torch.manual_seed with a fixed seed value. The seed value should be stored as part of the pipeline configuration and logged in the lineage metadata. If you need to generate multiple random samples in the same pipeline, you use a seeded random number generator and advance the state deterministically, rather than calling the system random function multiple times.

Timestamp dependencies are another common source of non-determinism. If your pipeline queries a database with a filter like "WHERE created_at less than NOW", the result will be different every time you run the query. The fix is to replace dynamic timestamps with fixed timestamps that are specified in the pipeline configuration. Instead of NOW, you use a configuration parameter like cutoff_timestamp. When you run the pipeline, you set cutoff_timestamp to the desired date, and the pipeline uses that value consistently. The cutoff_timestamp is logged in the lineage metadata so you can reproduce the exact query later.

External API calls introduce non-determinism because the API response can change over time. If your feature engineering pipeline calls a third-party API to fetch exchange rates, geocoding data, or sentiment scores, the API may return different results when called on different dates. The fix is to snapshot the API responses at the time of pipeline execution and store them as part of the dataset version. This means you are not querying the live API when you reproduce the dataset; you are replaying the cached API responses. An alternative is to use a versioned external dataset if the API provider offers historical snapshots.

Race conditions occur when your pipeline processes data in parallel and the execution order affects the result. For example, if you are deduplicating records based on arrival order, and the arrival order depends on which parallel worker finishes first, the result is non-deterministic. The fix is to impose a deterministic sort order before deduplication or aggregation. You sort by a stable field like a unique identifier or timestamp, so the order is the same every time.

Making pipelines deterministic does not mean making them slower. You can still use parallel processing, random sampling, and external data sources. You just need to control the sources of randomness and freeze the inputs so that the pipeline is reproducible.

## Seed Management

Random seeds are a critical part of reproducibility, and they need to be managed carefully. It is not enough to set a seed once at the beginning of your codebase and hope it propagates through all your libraries and functions. Different libraries maintain separate random state, and some libraries do not respect global seeds.

A robust seed management strategy has three parts: set seeds for all random number generators at the start of the pipeline, log the seed values in the pipeline configuration and lineage metadata, and verify that the pipeline is actually deterministic by running it twice and comparing outputs.

You set seeds for all major libraries. In Python, this means calling random.seed for the standard library random module, numpy.random.seed for NumPy, and torch.manual_seed and torch.cuda.manual_seed_all for PyTorch. If you are using TensorFlow, you call tf.random.set_seed. If you are using scikit-learn, many functions accept a random_state parameter, and you pass the same seed value to all of them. If you are using pandas for sampling, you pass the seed to the sample function. You do this at the very beginning of your pipeline script, before any data is loaded or processed.

You log the seed values in your pipeline configuration. The configuration file or parameter dictionary includes a field like random_seed, and the value is set when you run the pipeline. The seed is logged in the lineage metadata alongside the dataset version, the input dataset identifiers, and the code version. This means that when you reproduce the dataset, you use the same seed value, and you get the same random samples.

You verify determinism by running the pipeline twice with the same configuration and comparing the outputs. This is a test, not a hope. You write a test script that runs the pipeline, stores the output dataset, runs the pipeline again with the same configuration, and asserts that the two datasets are identical. If the datasets are not identical, you have a non-deterministic step somewhere in the pipeline, and you need to identify and fix it. This test should be part of your continuous integration for data pipelines, so you catch non-determinism as soon as it is introduced.

## Environment Pinning

Reproducibility requires that you pin the software environment: the language version, library versions, and system dependencies. If you run a pipeline with pandas version 1.5 and then try to reproduce it six months later with pandas version 2.0, you may get different results due to changes in sorting behavior, floating point precision, or API semantics.

Environment pinning is typically done with package managers and environment files. In Python, you use pip freeze or poetry lock to export a list of installed packages and their exact versions. You store this file in version control alongside your pipeline code. When you run the pipeline, you create a virtual environment and install the pinned versions. When you reproduce the pipeline, you use the same pinned environment.

For more complex environments, you use containerization. You define a Docker image that includes the language version, library versions, and system dependencies. You run the pipeline inside the container, and you tag the container image with the pipeline version. When you reproduce the pipeline, you use the same container image. This ensures that not only the Python packages but also the operating system, compilers, and system libraries are the same.

Environment pinning is not just for production pipelines. You should pin environments for exploratory data analysis and research experiments as well. If you run an experiment in a notebook and then try to reproduce it three months later, you want the same results. You do not want to spend hours debugging why your results changed, only to discover that a library update changed a default parameter.

The overhead of environment pinning is minimal. Package managers and container tools make it easy to export and restore environments. The cost of not pinning is high: irreproducible results, wasted debugging time, and failed audits.

## Snapshot Retrieval

Reproducibility is easier when you store snapshots of intermediate datasets and source data. If you need to reproduce a training dataset that was generated from a production database, you do not want to query the live database, because the data has changed since the original pipeline run. Instead, you query a snapshot of the database from the time of the original pipeline run.

Snapshot retrieval requires that you version your source data, not just your derived datasets. If your source data is in a database, you use database snapshots or point-in-time recovery. If your source data is in a data lake, you use versioned storage or partitioned by timestamp. If your source data comes from an external API, you cache the API responses and version the cache.

A common pattern is to partition source data by date. You store each day's data in a separate partition, and you never modify historical partitions. When you run a pipeline, you specify the date range, and the pipeline reads from the corresponding partitions. When you reproduce the pipeline, you specify the same date range, and you get the same source data. This works well for event data, logs, and transactional data that arrive continuously.

For data that is updated in place, such as database tables with mutable records, you need to take periodic snapshots. You export a full snapshot of the table at regular intervals, such as daily or weekly, and you store the snapshots in versioned storage. When you run a pipeline, you specify which snapshot to use, and the pipeline reads from that snapshot. When you reproduce the pipeline, you use the same snapshot. The downside of snapshots is storage cost, because you are duplicating data. The upside is perfect reproducibility and fast access without querying a production database.

Snapshot retrieval also applies to external data sources. If your pipeline uses a third-party dataset, you should download and version the dataset at the time you use it. Do not assume that the third-party provider will keep historical versions available. Providers often update datasets in place or remove old versions. If you need to reproduce your pipeline two years later, you cannot rely on the third-party provider to have the same data. You need to have stored your own copy.

## Testing Reproducibility Before You Need It

Reproducibility is not something you test during a regulatory audit. You test it continuously as part of your development process. Every time you update a data pipeline, you verify that the pipeline is still reproducible. Every time you version a dataset, you verify that you can regenerate the dataset from source data and transformation logic.

A reproducibility test runs the pipeline twice with the same configuration and compares the outputs. The test passes if the outputs are identical, and fails if they differ. Identical means byte-for-byte identical, not approximately similar. You compare dataset checksums, row counts, column schemas, and value distributions. If any of these differ, the pipeline is not reproducible.

You run reproducibility tests in continuous integration. When a pull request updates a data pipeline, the CI system runs the pipeline twice and verifies that the outputs match. If the outputs do not match, the pull request is rejected. This prevents non-deterministic changes from being merged into the main branch.

You also run reproducibility tests on a schedule for production pipelines. Once a week or once a month, you select a random historical dataset and attempt to reproduce it from the versioned source data and transformation logic. You compare the regenerated dataset to the stored snapshot. If they match, reproducibility is confirmed. If they do not match, you investigate and fix the issue before it becomes a problem during an audit or debugging session.

Testing reproducibility is not expensive. The computational cost is the same as running the pipeline once, because you are running it twice. The storage cost is minimal, because you are comparing checksums and metadata, not storing full duplicate datasets. The benefit is confidence that your pipelines are reproducible when you need them to be.

## Reproducibility for Model Training

Reproducibility for model training is harder than reproducibility for data pipelines, because model training involves additional sources of randomness: weight initialization, batch shuffling, dropout, data augmentation, and hardware-specific numerical precision. You cannot make model training perfectly reproducible across different hardware or different library versions, but you can make it reproducible on the same hardware with the same environment.

You start by pinning the environment: the deep learning framework version, the GPU driver version, and the CUDA or ROCm version. You set all random seeds: the language random seed, the framework random seed, and the GPU random seed. You disable non-deterministic algorithms in the framework. In PyTorch, you set torch.backends.cudnn.deterministic to True and torch.backends.cudnn.benchmark to False. In TensorFlow, you set environment variables to disable non-deterministic GPU operations.

You log all hyperparameters: learning rate, batch size, number of epochs, optimizer settings, data augmentation parameters. You log the dataset version used for training and validation. You log the code version, typically as a Git commit hash. You store all of this metadata with the model artifact.

You test reproducibility by retraining the model with the same dataset, hyperparameters, and code, and verifying that the final weights and evaluation metrics are identical or nearly identical. Numerical precision issues can cause small differences in floating point operations, so you allow for a small tolerance when comparing weights and metrics. If the difference is within the tolerance, the training is reproducible.

Reproducibility for model training is important for debugging and auditing. If a model behaves unexpectedly, you can retrain the model with the same configuration and see if the issue reproduces. If a regulator asks how a model was trained, you can retrain the model and show that you get the same results. If you cannot reproduce your model training, you cannot trust your models.

## Reproducibility and Experimentation

Reproducibility does not conflict with experimentation. You still run dozens or hundreds of experiments with different hyperparameters, different datasets, and different architectures. The difference is that each experiment is reproducible. You can rerun any experiment and get the same results.

This is especially important when you are comparing models or datasets. If you train two models with different configurations and compare their performance, you need to be sure that the performance difference is due to the configuration, not due to random variation. If your training is not reproducible, you cannot distinguish signal from noise.

Reproducibility also makes collaboration easier. If a colleague reports a result, you can reproduce their experiment by using the same dataset version, code version, and hyperparameters. You do not need to guess what they did or ask them to send you their data. The metadata is already recorded, and the pipeline is already reproducible.

In research contexts, reproducibility is a scientific requirement. If you publish a paper with experimental results, other researchers should be able to reproduce your results using the information in the paper. In industry contexts, reproducibility is an operational requirement. If you deploy a model to production, you should be able to reproduce the model training to verify that the model was built correctly.

## Reproducibility Checklist

You achieve reproducibility by building it into your pipelines from the start. Here is a checklist. Set random seeds for all libraries and log the seed values. Replace dynamic timestamps with fixed configuration parameters. Snapshot or cache external API responses. Pin the software environment and store the environment file with the code. Version source data or take periodic snapshots. Test reproducibility by running pipelines twice and comparing outputs. Log all configuration, hyperparameters, and metadata. Store snapshots of datasets used for training and evaluation. Run reproducibility tests in continuous integration and on a schedule.

You do not need to reproduce every intermediate dataset or every exploratory experiment. You need to reproduce the datasets used for production model training, the datasets used for evaluation and reporting, and the datasets used for compliance and auditing. If a dataset has a version identifier and is stored in a versioned location, it should be reproducible.

Reproducibility is not a one-time effort. It is a property of your data pipelines that you maintain continuously. Every time you update a pipeline, you verify that it is still reproducible. Every time you add a new data source, you ensure that the source is versioned or snapshotted. Every time you train a model, you log the configuration and metadata needed to reproduce the training. Reproducibility is not optional. It is the standard for professional data engineering in 2026.

The next subchapter covers storage architecture: where and how to store datasets to balance cost, access speed, and pipeline complexity.
