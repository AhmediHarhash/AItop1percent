# 10.3 â€” Intended Use and Known Limitations

Every dataset has boundaries. It was collected for a purpose, from a population, during a time period, under certain constraints. Those boundaries define where the dataset is valid and where it is not. A dataset built to train a fraud detection model for credit card transactions is not valid for detecting wire transfer fraud. A dataset collected from English-speaking users in North America is not valid for deployment in Southeast Asia. A dataset collected in 2024 may not reflect the patterns present in 2026. Documenting intended use and known limitations is not an exercise in legal defensiveness. It is an exercise in honesty and operational safety. It prevents your organization from deploying models in contexts where they will fail, and it prevents downstream teams from making incorrect assumptions about what the data represents.

Intended use defines what the dataset was designed for. Known limitations define what the dataset does not cover, where it is skewed, and what risks it introduces. Both sections must be specific, actionable, and honest. Generic statements like "this dataset may contain biases" or "this dataset is intended for research purposes" are worse than useless because they create the illusion of disclosure without providing information that can guide decisions. Real intended use and limitations sections name specific populations, time periods, edge cases, and gaps. They are written by people who understand both the data and the system it supports, and they are reviewed by people who will be held accountable if the dataset is misused.

## Why Documenting Intended Use Prevents Misuse Across Teams

Organizations are not monolithic. A dataset built by one team will be discovered, copied, and reused by other teams, often without consultation with the original creators. This is not malicious. It is efficient. If a dataset exists and appears relevant, teams will use it rather than spending time and money building a new one. The problem is that relevance is not always obvious from the dataset name or file path. A dataset called customer-support-emails might seem appropriate for training a sentiment analysis model, but if it was built for routing emails to support queues, it may not have been labeled for sentiment, and using it for sentiment analysis will produce a model that fails in unpredictable ways.

Documenting intended use creates a clear contract. It states what the dataset is for, what tasks it supports, what populations it covers, and what deployment contexts are valid. For example: "This dataset is intended for training and evaluating email routing models that assign customer support emails to the appropriate queue based on issue type and urgency. It is intended for use in English-language support channels serving customers in the United States, Canada, and the United Kingdom. It is intended for internal use only and must not be shared outside the organization or used in external research without legal review." This statement makes it clear that the dataset is not for sentiment analysis, not for non-English languages, and not for markets outside the specified regions. A team considering reusing the dataset can read this section and immediately know whether their use case is in scope.

Intended use also defines the lifecycle stage. Some datasets are intended only for prototyping and experimentation. Others are intended for production use. The distinction matters because production datasets are held to higher quality, compliance, and documentation standards. For example: "This dataset is intended for production training and evaluation. It has been reviewed for privacy compliance, bias, and data quality. It is versioned, maintained, and subject to quarterly audits." Contrast this with: "This dataset is intended for exploratory analysis and prototyping only. It has not been reviewed for production use and must not be used to train models deployed to customers." The lifecycle stage determines what due diligence is required before using the dataset.

Intended use statements also specify what model types and architectures the dataset is appropriate for. For example: "This dataset is intended for supervised classification models that predict issue type and urgency from email text. It is not suitable for generative models, retrieval systems, or unsupervised clustering without additional validation." This prevents a team from using a classification dataset to train a generative model and then being surprised when the model hallucinates issue types that do not exist in the label space.

Without documented intended use, every team using the dataset must reverse-engineer its purpose from the data itself, the schema, and any code that references it. This process is error-prone and time-consuming. It also fails when the original context has been lost because the team that built the dataset has disbanded or because the dataset is old enough that no one remembers why it was built. Intended use documentation captures that context while it is still available and makes it accessible to future users.

## The Dataset Drift Problem

Dataset drift is what happens when a dataset is repurposed for a task it was never designed to support. It is one of the most common and most dangerous failure modes in AI systems. The original dataset was built with care, reviewed for quality, and validated against its intended use. Then a new team discovers the dataset, assumes it is appropriate for their use case, and trains a model without revalidating the data. The model performs poorly or introduces unexpected biases, and the team spends weeks debugging before realizing that the problem is not the model architecture or the hyperparameters, but the dataset itself.

A financial services company built a dataset of transaction records labeled for fraud detection. The dataset was carefully balanced to include examples of card-present fraud, card-not-present fraud, and legitimate transactions. It was used to train a fraud detection model that performed well in production. Two years later, a different team discovered the dataset and decided to use it to train a model that predicted transaction risk for credit limit adjustments. The team assumed that fraud and risk were related concepts and that a dataset labeled for fraud would be useful for risk prediction. They trained a model, deployed it to production, and discovered six months later that the model was systematically denying credit limit increases to customers in certain geographic regions. The root cause: the fraud dataset was skewed toward regions with high fraud rates, and the risk model learned to associate those regions with high risk, even for customers with excellent payment history. The dataset was fit for fraud detection but not for risk prediction, and the team had not validated that assumption.

This is dataset drift. The dataset drifted from its intended use to a new use without the validation and adjustment required to ensure it was still appropriate. The failure was not technical. It was organizational. The dataset did not document its intended use or its known limitations, and the new team did not know to ask whether the dataset was appropriate for their task.

Preventing dataset drift requires two things: clear documentation of intended use, and a culture that requires teams to validate datasets before using them for new purposes. The documentation makes it possible to identify drift. The culture makes it mandatory to act on that information. A dataset's intended use section should include an explicit statement about repurposing: "If you intend to use this dataset for a purpose other than the one described in this document, you must validate that the dataset is appropriate for the new use case. Validation includes reviewing the target distribution, label definitions, known limitations, and bias analysis, and confirming that they align with the requirements of the new task. If alignment cannot be confirmed, do not use this dataset. Build a new one."

## Known Limitations: Coverage Gaps, Temporal Staleness, Demographic Skew, Domain Boundaries

Known limitations are the documented gaps, skews, and constraints that affect what the dataset represents and what conclusions can be drawn from it. They include geographic coverage gaps, temporal staleness, demographic skew, domain boundaries, labeling noise, and any other factors that limit the dataset's validity or generalizability. The limitations section is where you acknowledge what the dataset does not cover and what risks it introduces.

Geographic coverage gaps occur when the dataset represents only certain regions, markets, or locations. For example: "This dataset contains support emails only from customers in the United States, Canada, and the United Kingdom. It does not include examples from customers in Europe, Asia-Pacific, Latin America, or other regions. Models trained on this dataset may not generalize to customer populations in other markets due to differences in language, cultural communication norms, issue types, and product usage patterns." This limitation is specific. It names the regions that are included and the regions that are excluded, and it explains why the exclusion matters.

Temporal staleness occurs when the dataset represents a specific time period and may not reflect current patterns. For example: "This dataset was collected between March 2025 and September 2025. It does not include examples from October 2025 onward, when the new mobile app was launched and the issue distribution shifted significantly. It does not include examples from the period after the pricing change in November 2025, which altered the customer demographic mix. Models trained on this dataset may not perform well on current data without retraining or recalibration." This limitation acknowledges that the world changes and that datasets become stale. It sets expectations about when the dataset will need to be refreshed.

Demographic skew occurs when the dataset overrepresents or underrepresents certain populations. For example: "This dataset overrepresents enterprise customers, who make up sixty-two percent of examples but only thirty percent of the total customer base. It underrepresents individual subscribers, who make up thirty-eight percent of examples but seventy percent of the customer base. Models trained on this dataset may perform better on enterprise use cases than on individual subscriber use cases. Performance should be monitored separately for each customer segment." This limitation quantifies the skew and explains its implications. It gives the model owner actionable information about where to expect performance differences.

Domain boundaries define the scope of the dataset's validity. For example: "This dataset contains invoices from small business customers in retail, hospitality, and professional services industries. It does not include invoices from manufacturing, healthcare, or construction industries. It does not include government or nonprofit invoices. It does not include invoices in formats other than PDF. Models trained on this dataset may not generalize to industries, document types, or formats outside this scope." This limitation defines what is in scope and what is out of scope. It prevents a team from deploying a model trained on retail invoices to process healthcare invoices and expecting it to work.

Labeling noise is a known limitation when label quality is less than perfect. For example: "Labeler agreement on urgency labels was seventy-one percent, indicating significant subjectivity and noise. Urgency labels should be treated as approximate and noisy. Models trained on urgency labels may have lower precision and recall than models trained on more objective labels like issue type. Consider using confidence thresholds or human-in-the-loop workflows for urgency predictions." This limitation acknowledges that labels are not ground truth and that downstream systems must account for label noise.

Known limitations should be exhaustive. If you know about a gap, a skew, or a constraint, it must be documented. Omitting known limitations is negligence. It exposes your organization to risk and creates a false sense of confidence in downstream models. If you do not know whether a limitation exists, that uncertainty should be documented as well. For example: "We have not validated whether this dataset represents the full range of document quality encountered in production. Low-quality scans may be underrepresented. Performance on degraded documents should be monitored closely after deployment."

## How to Write Honest Limitation Sections That Are Specific Enough to Be Useful

Honest limitation sections are uncomfortable to write because they require admitting what you do not know, what you could not collect, and what compromises you made. There is organizational pressure to present datasets as comprehensive, representative, and high-quality. Admitting limitations feels like admitting failure. This is a mistake. Limitations are not failures. They are reality. Every dataset has them, and pretending they do not exist does not make them go away. It just means they will cause failures later, when they are more expensive to fix.

Writing honest limitation sections requires specificity. Do not write: "This dataset may not be representative of all users." Write: "This dataset overrepresents users in urban areas, who make up seventy-eight percent of examples but only fifty-two percent of the user base. It underrepresents users in rural areas, who make up twelve percent of examples but twenty-three percent of the user base. Models trained on this dataset may perform worse for rural users." Do not write: "This dataset contains some outdated examples." Write: "This dataset was collected between January 2025 and June 2025. It does not reflect product changes introduced in the July 2025 redesign, which altered the user interface and the distribution of user actions. Models trained on this dataset may not generalize to post-redesign usage patterns."

Quantification makes limitations actionable. Do not write: "Enterprise customers are overrepresented." Write: "Enterprise customers make up sixty-two percent of examples but only thirty percent of the total customer base, a two-to-one overrepresentation." Do not write: "Some labels may be noisy." Write: "Labeler agreement on sentiment labels was sixty-eight percent, indicating high noise. Expect sentiment model performance to be ten to fifteen percentage points lower than performance on higher-agreement labels like topic classification."

Limitations should also explain why the limitation exists and what it implies for downstream use. For example: "This dataset contains no examples from the Asia-Pacific region because the product had not launched in that region at the time of data collection. Models trained on this dataset must not be deployed in Asia-Pacific markets without collecting region-specific data and revalidating model performance. Cultural, linguistic, and behavioral differences may cause model performance to degrade significantly in that market." This explanation gives the reader both the fact and the consequence.

Limitations sections should avoid hedging language. Do not write: "There may be some potential bias in certain demographic groups." Write: "This dataset underrepresents users over sixty-five, who make up four percent of examples but eighteen percent of the user base. Models may perform worse for older users." Hedging dilutes the message and makes it easy for readers to dismiss the limitation as speculative. Concrete statements force readers to take the limitation seriously.

Honest limitations also acknowledge what you do not know. For example: "We have not analyzed this dataset for socioeconomic bias because we do not collect income data. It is possible that the dataset overrepresents higher-income users, who may have different usage patterns than lower-income users. Performance should be monitored across customer segments after deployment to detect any disparities." Admitting that you do not know something is better than pretending you do.

## The Legal Importance of Documented Limitations

Documented limitations are not just engineering best practices. They are legal protection. When a model fails and causes harm, one of the first questions in any investigation or lawsuit is: did the organization know about the limitations of the data the model was trained on, and did they disclose those limitations? If the limitations were known and documented, and the organization deployed the model outside the documented scope, that is negligence. If the limitations were known and not documented, that is also negligence. If the limitations were unknown because no one bothered to analyze the data, that is negligence. The only defensible position is: the limitations were documented, they were communicated to decision-makers, and the deployment decision was made with full knowledge of those limitations.

The EU AI Act, enforced as of 2026, requires that high-risk AI systems be trained on datasets that are relevant, representative, and free of errors to the extent possible. It also requires that data governance practices ensure the detection and correction of bias. Documented limitations are evidence that you have analyzed your dataset for relevance, representativeness, and bias, and that you understand where it falls short. If you cannot produce that documentation during an audit, you are in violation.

GDPR requires that automated decisions that significantly affect individuals be explainable and contestable. If your model makes a decision that harms a user, and that user challenges the decision, you need to be able to explain how the model was trained and what limitations the training data had. If the training data was biased or unrepresentative in ways that affected the decision, and you did not document that, you cannot defend the decision. Documented limitations make it possible to trace a failure back to its root cause and to demonstrate that the organization was aware of the risk.

Product liability law in many jurisdictions holds organizations responsible for harms caused by defective products. If an AI system is considered a product, and it fails due to inadequate training data, the organization can be held liable. One of the defenses is that the organization disclosed the product's limitations and that the failure occurred in a context outside the intended use. Documented intended use and limitations provide that disclosure. They demonstrate that the organization understood the boundaries of the system and communicated those boundaries to users or customers.

Documented limitations also shift liability when external parties are involved. If you license a dataset from a vendor, and that dataset has undisclosed limitations that cause a model failure, the vendor may be liable. If you build a dataset and license it to another organization, and you fail to document known limitations, you may be liable when their model fails. Clear documentation protects both parties by making the scope and constraints explicit.

Legal teams should review the intended use and limitations sections of dataset cards as part of the dataset approval process. If the legal team cannot understand the limitations or believes they are inadequately documented, the dataset should not be approved for production use until the documentation is improved. Legal review is not a formality. It is a safeguard.

## Examples of Limitation Documentation That Prevented Downstream Failures

A healthcare AI company built a dataset of electronic health records labeled for disease diagnosis. The dataset was collected from patients at five urban academic medical centers. The limitations section documented: "This dataset contains records only from patients treated at large urban academic medical centers. It does not include records from rural clinics, community hospitals, or private practices. Patient demographics skew toward insured, higher-income, and more educated populations who have access to academic medical centers. Disease presentations may differ in populations with limited access to preventive care or who delay seeking treatment due to cost. Models trained on this dataset may not generalize to underserved or rural populations." A product team planning to deploy the diagnostic model in a telehealth service targeting rural areas read the limitations section and decided to collect additional data from rural clinics before deployment. The model was retrained on the combined dataset and validated separately for urban and rural populations. The limitations documentation prevented a deployment that would have failed.

A legal tech company built a dataset of contracts labeled for clause extraction. The dataset was collected from contracts drafted by large law firms representing Fortune 500 companies. The limitations section documented: "This dataset contains contracts primarily from large corporations represented by Am Law 100 firms. It does not include contracts from small businesses, nonprofits, or individuals. Contract language is formal, heavily negotiated, and often includes jurisdiction-specific clauses common in Delaware or New York law. Models trained on this dataset may not perform well on contracts drafted by non-specialists, solo practitioners, or in-house counsel using simpler or less standardized language. The dataset does not include contracts governed by non-US law." A product manager planning to expand the product to small business customers read the limitations and commissioned a validation study on small business contracts. The study revealed significant performance degradation, and the product team collected a new dataset before launching in that market. The limitations documentation prevented a product failure.

An e-commerce company built a dataset of product reviews labeled for sentiment. The dataset was collected from reviews written in English on the US site. The limitations section documented: "This dataset contains only English-language reviews from US customers. It does not include reviews in other languages or from other markets. Sentiment expressions may be culture-specific. For example, understatement and indirect criticism are more common in British English than American English, and sarcasm is difficult to detect without cultural context. Models trained on this dataset may misclassify sentiment in reviews from non-US markets." A team planning to deploy the sentiment model in the UK read the limitations and ran a performance evaluation on a sample of UK reviews. The model's precision on negative sentiment dropped from ninety-two percent to seventy-eight percent due to misclassification of understated complaints. The team retrained the model on a combined US-UK dataset before deployment. The limitations documentation prevented a quality degradation that would have damaged the UK product launch.

These examples share a pattern. The limitations were documented honestly and specifically. The documentation was read by decision-makers planning new deployments. The decision-makers recognized that the deployment context was outside the dataset's validated scope. They took action to address the gap before deploying, rather than after. The cost of additional data collection or validation was far lower than the cost of a failed deployment. Documented limitations made this process possible.

## The Everything Is Fine Anti-Pattern

The everything is fine anti-pattern is what happens when the limitations section is empty, generic, or so full of hedging that it communicates nothing. For example: "This dataset may contain some biases or limitations common to all real-world datasets. Users should exercise caution and validate performance in their specific context." This statement is true but useless. It applies to every dataset ever created. It does not tell you what the specific limitations of this dataset are, and it does not guide decisions about where or how to use the data.

The everything is fine anti-pattern exists because writing honest limitations is uncomfortable and because there is organizational pressure to present datasets as ready for production. Teams want to ship, and admitting that the dataset has significant gaps or skews feels like an obstacle to shipping. This is short-term thinking. Shipping a model trained on inadequate data is not shipping. It is deferring failure.

The everything is fine anti-pattern also exists because teams do not know how to analyze datasets for limitations. They collect data, label it, and train a model, but they do not conduct coverage analysis, demographic breakdowns, or temporal stability checks. Without analysis, they cannot document limitations because they do not know what the limitations are. The solution is not to leave the limitations section empty. The solution is to do the analysis. If you do not know what your dataset's limitations are, you are not ready to use it in production.

A limitations section that says nothing is worse than no limitations section at all because it creates false confidence. It signals that someone reviewed the dataset for limitations and found none, when in reality no one looked. An empty limitations section signals that the documentation is incomplete and that due diligence is still required. A falsely reassuring limitations section signals that due diligence is done, which prevents downstream teams from doing their own validation.

If you do not have time or resources to analyze a dataset for limitations before using it, you should not be using it in production. If you do not have time to document the limitations you are aware of, you should not be releasing the dataset to other teams. Limitations documentation is not optional. It is the difference between a dataset that can be used safely and a dataset that is a liability waiting to happen.

Intended use and known limitations are the boundaries that define where a dataset is valid and where it is not. They prevent misuse, prevent drift, and provide legal protection. They are written with honesty, specificity, and quantification. They acknowledge what the dataset does not cover and what risks it introduces. They are reviewed by stakeholders and maintained as the dataset evolves. Without them, your datasets are uncontrolled experiments that will fail in ways you cannot predict or defend. With them, your datasets become tools that can be used safely, audited credibly, and trusted across teams and over time. The next step is to document not just the limitations, but the distribution itself: what the dataset contains and how those contents are spread across the dimensions that matter.
