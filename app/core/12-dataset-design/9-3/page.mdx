# 9.3 â€” De-Identification Techniques: Redaction, Masking, and Generalization

What do you do with a training example that contains a customer's name, email address, and transaction history? Delete the entire example and lose all the useful context? Replace the identifiers with placeholder tokens and hope that preserves enough signal for model training? Mask parts of the identifiers and accept some re-identification risk? Generalize the precise values to broader categories? Each de-identification technique makes a different trade-off between privacy protection and data utility. Aggressive redaction maximizes privacy but destroys signal. Partial masking preserves more utility but leaves re-identification vectors. Generalization broadens values to categories but loses precision. The correct choice is not universal. It depends on your threat model, your regulatory obligations, and how much identifying detail your model actually needs to learn useful patterns. There is no free lunch. Every de-identification technique either reduces utility or accepts residual risk.

The stakes are not theoretical. In July 2025, a financial services company released a customer support chatbot trained on de-identified support transcripts. The de-identification used simple masking: names were replaced with "Customer A," "Customer B," and account numbers were truncated to the last four digits. Within three weeks, a security researcher demonstrated that 14 percent of customers could be re-identified by correlating transaction timestamps, product types, and masked account suffixes with publicly available data breach records. The company faced regulatory inquiry in five jurisdictions, a class action lawsuit, and a consent decree requiring them to retrain the model using stronger de-identification. The cost was nine million dollars and eight months of engineering time. The root cause was not that de-identification failed: it was that the team chose a de-identification technique appropriate for low-risk internal analytics and applied it to a high-risk public-facing model without assessing re-identification risk.

## Redaction: Replacing PII with Tokens

Redaction replaces identified PII with placeholder tokens that preserve the structure of the text but remove the identifying content. A sentence like "John Doe called on March 15 to update his shipping address to 742 Evergreen Terrace" becomes "PERSON_1 called on DATE_1 to update his shipping address to ADDRESS_1." The redacted version retains the semantic structure: someone called on a date to update an address. The model can learn that people call to update addresses. It cannot learn John Doe's address.

Token-based redaction is the most common de-identification technique in production AI systems because it is simple to implement, easy to audit, and minimally lossy for many use cases. Named entity recognition flags person names, locations, organizations, dates, and numeric identifiers. The redaction pipeline replaces each entity with a typed token: PERSON, LOCATION, ORGANIZATION, DATE, PHONE, EMAIL, SSN. Tokens can be numbered within a document to preserve co-reference: if "John Doe" appears three times in a support ticket, all three mentions become PERSON_1, which tells the model that the same person is referenced multiple times.

The utility cost is low for tasks where the model needs to understand roles and relationships but does not need specific identities. A customer support model trained on redacted tickets can learn that customers call about billing issues, shipping delays, and product defects without learning any customer's name. A medical NLP model trained on redacted clinical notes can learn that patients with diabetes often take metformin without learning any patient's identity. Redaction works when the task is generalizable: when the correct response to a support query does not depend on who asked, and the correct diagnosis does not depend on which patient is being treated.

The utility cost is high for tasks where identity matters. A model trained to personalize responses based on customer history cannot learn from redacted data because customer history is tied to customer identity. A fraud detection model needs to learn that certain account numbers are associated with suspicious patterns, which requires preserving account identifiers or using pseudonymization instead of redaction. Redaction destroys continuity: if every instance of a customer's name is replaced with a different token, the model cannot learn that the same person had multiple interactions.

Re-identification risk from redaction is low but not zero. If redacted data is released publicly or shared with third parties, adversaries can attempt to re-identify individuals by correlating redacted records with external datasets. A redacted medical record that says "PERSON_1 is a 67-year-old male from ZIP_1 diagnosed with CONDITION_1 on DATE_1" can be re-identified if the adversary has access to voter registration records, hospital admission logs, or insurance claims that match the age, zip code, condition, and date. The risk increases with the number of quasi-identifiers left unredacted and the availability of external data for correlation.

## Masking: Partial Obscuring

Masking reveals part of an identifier while hiding the rest. Credit card numbers are masked by showing the last four digits: "XXXX-XXXX-XXXX-1234." Phone numbers are masked by showing area code only: "555-XXX-XXXX." Email addresses are masked by showing domain only: "XXXXX@example.com." The masked version preserves some utility while reducing identifiability. A customer service agent can verify that a payment was made with a card ending in 1234 without seeing the full card number. A fraud model can learn that certain area codes are associated with higher fraud rates without learning individual phone numbers.

Partial masking is common in production systems where humans interact with the data and need limited identifying information to perform their jobs. Call center agents see masked customer records. Fraud analysts see masked transaction details. The masking reduces insider threat risk: a rogue employee cannot steal full credit card numbers if the system only displays the last four digits. It also reduces accidental disclosure risk: if a masked record is leaked or mishandled, the privacy harm is smaller than leaking the full identifier.

The utility cost is variable. Masking credit card numbers to the last four digits retains enough information for payment verification but loses the issuer information encoded in the first six digits, which might be useful for fraud detection. Masking email addresses to domain only retains information about email provider but loses the username, which might encode demographic information or account age. The correct masking strategy depends on which parts of the identifier carry signal for your model and which parts are purely identifying.

Re-identification risk from masking depends on how much information is revealed and how much can be inferred from context. The last four digits of a credit card combined with transaction timestamp, merchant, and amount might uniquely identify a cardholder in a small dataset. The area code of a phone number combined with age, gender, and zip code might uniquely identify an individual in a sparse region. Masking reduces the bits of identifying information but does not eliminate them. Adversaries can use partial identifiers as filters to narrow the search space when correlating with external data.

The illusion of safety is the biggest risk with masking. Teams see "XXXX-XXXX-XXXX-1234" and assume the data is anonymized. It is not anonymized. It is partially obscured. If the adversary has access to a dataset of full credit card numbers and transaction logs, the last four digits plus transaction metadata might be sufficient to re-identify the cardholder. Masking is appropriate for reducing casual disclosure risk and limiting insider access, not for achieving anonymization in high-risk contexts.

## Generalization: Broadening Precise Values

Generalization replaces precise values with broader categories or ranges. Exact age becomes age range: 34 years old becomes 30 to 40. Exact location becomes region: Brooklyn, New York becomes New York City metro area. Exact timestamp becomes date or month: March 15, 2025 at 3:47 PM becomes March 2025. Exact salary becomes income bracket: 87,000 dollars becomes 75,000 to 100,000 dollars. Generalization reduces the precision of the data while preserving distributional information.

The utility of generalization depends on how much precision your model needs. A healthcare model predicting disease risk might perform nearly as well using age ranges instead of exact ages because risk correlates with decade of life, not individual years. A recommendation model might perform nearly as well using metro area instead of zip code because preferences correlate with urban versus suburban versus rural, not with specific neighborhoods. A fraud model might perform worse using generalized timestamps because fraud patterns depend on exact time-of-day and day-of-week signals that are lost when dates are truncated to months.

Generalization is tuned by choosing the granularity of the range or category. Age can be generalized to five-year ranges, ten-year ranges, or coarse categories like under 18, 18 to 65, over 65. Location can be generalized to city, metro area, state, or region. Timestamp can be generalized to hour, day, week, month, quarter, or year. The coarser the generalization, the lower the re-identification risk and the lower the model utility. The optimal granularity is the coarsest level that preserves the signal your model needs.

Re-identification risk from generalization is determined by how many quasi-identifiers remain and how finely they are generalized. The classic result from Latanya Sweeney's research is that 87 percent of the US population can be uniquely identified by the combination of five-digit zip code, birthdate, and gender. Generalizing zip code to three digits and birthdate to year reduces that to under 10 percent. Generalizing zip code to county and birthdate to five-year range reduces it to under 1 percent. The relationship is not linear: small increases in generalization can produce large decreases in re-identification risk when you cross thresholds that break uniqueness.

The challenge is that generalization must be applied consistently across all quasi-identifiers in a record. Generalizing age to ten-year ranges but leaving zip code at five digits still creates high re-identification risk because the zip code carries most of the identifying power. Generalizing both age and zip code but leaving exact diagnosis codes creates risk because rare diagnoses are highly identifying even without demographic information. Effective generalization requires analyzing which combinations of attributes are identifying and generalizing enough attributes to break those combinations.

## Context-Preserving De-Identification

De-identification for AI training data is harder than de-identification for statistical release because models learn from context, not just from isolated values. A model trained on de-identified text must still understand the relationships between entities, the temporal order of events, and the co-occurrence patterns that carry semantic meaning. Naive de-identification breaks these relationships and produces training data that is less useful or actively misleading.

Consistent tokenization preserves co-reference. If a person's name appears ten times in a document, all ten mentions must be replaced with the same token: PERSON_1. If each mention is replaced with a different token, the model learns that ten different people were involved, which is false. If mentions across documents are inconsistent, the model cannot learn that the same person appears in multiple interactions, which loses signal for tasks that require tracking entities over time.

Relationship preservation requires that tokens carry enough information to reconstruct relationships. If a support ticket says "John Doe called about an order placed by Jane Doe," the redacted version should be "PERSON_1 called about an order placed by PERSON_2," not "PERSON called about an order placed by PERSON." The latter loses the information that two different people were involved. Similarly, if a clinical note says "Patient presented with symptoms in March and was diagnosed in April," the redacted version should preserve the temporal order: "Patient presented with symptoms in MONTH_1 and was diagnosed in MONTH_2," not "Patient presented with symptoms in MONTH and was diagnosed in MONTH."

Format preservation matters for structured identifiers. If your training data includes examples of email addresses, phone numbers, and URLs, and you redact them all to generic tokens like IDENTIFIER, the model loses the ability to distinguish between different identifier types. Better practice is to use typed tokens: EMAIL, PHONE, URL. Even better is to use synthetic identifiers that preserve format: replace "jdoe@example.com" with "user1234@example.com," replace "555-867-5309" with "555-000-0001," replace real URLs with synthetic URLs that follow the same structure.

Synthetic replacement is the most sophisticated form of context-preserving de-identification. Instead of replacing "John Doe, age 34, from Brooklyn" with "PERSON_1, age AGE_1, from LOCATION_1," you replace it with "Michael Smith, age 30 to 40, from New York City metro area." The synthetic version preserves the narrative flow and readability while removing the real identifiers. The age is generalized to a range. The location is generalized to a metro area. The name is replaced with a common name that does not correspond to any real person in your dataset. Models trained on synthetic data perform closer to models trained on real data because the text remains natural.

The risk with synthetic replacement is collision: if you replace multiple real people named John with synthetic people named Michael, you create false correlations. If you replace multiple addresses in Brooklyn with synthetic addresses in Manhattan, you distort the geographic distribution. Synthetic data generation must ensure that replacements are diverse enough to avoid collisions and representative enough to preserve distributional properties. This requires tracking which real values map to which synthetic values and ensuring one-to-one or one-to-few mappings.

## Re-Identification Risk: How Quasi-Identifiers Combine

The hardest concept for engineering teams to internalize is that attributes that are not identifying in isolation become highly identifying in combination. Knowing someone's age does not identify them. Knowing their zip code does not identify them. Knowing their employer does not identify them. Knowing all three often does identify them uniquely, especially if the employer is small or the zip code is in a low-population area.

The mathematics of re-identification risk come from the k-anonymity framework introduced by Latanya Sweeney and Pierangela Samarati in 1998 and refined over the following decades. A dataset satisfies k-anonymity if every combination of quasi-identifiers appears for at least k individuals. If k equals 5, then every combination of age range, zip code, and gender must correspond to at least five people in the dataset. If a combination corresponds to fewer than five people, those individuals are at risk of re-identification because the combination narrows the search space enough that correlation with external data can identify them.

Achieving k-anonymity requires either generalization or suppression. Generalization broadens quasi-identifiers until each combination covers at least k people. Suppression removes records that cannot be generalized enough to meet the threshold. The choice of k is a policy decision: k equals 2 is minimal privacy protection, k equals 5 is moderate, k equals 10 or higher is strong. The cost of higher k is more aggressive generalization and more suppression, which reduces data utility.

L-diversity extends k-anonymity by requiring that within each group of k individuals sharing the same quasi-identifiers, there must be at least l distinct values for sensitive attributes. K-anonymity alone does not protect against attribute disclosure: if five people in a dataset share the same age, zip code, and gender, and all five have the same medical diagnosis, then an adversary who identifies the group knows the diagnosis even without knowing which of the five individuals is the target. L-diversity prevents this by requiring diversity in sensitive attributes within each equivalence class.

T-closeness further extends the framework by requiring that the distribution of sensitive attributes within each group is close to the distribution in the overall dataset. This prevents skewness attacks where an adversary infers sensitive information from the fact that a group has an unusual distribution even if the group satisfies l-diversity. These frameworks are not prescriptive: they do not tell you exactly how to de-identify data. They provide mathematical definitions of privacy properties and constraints that your de-identification must satisfy.

## When Each Technique Is Appropriate

Redaction is appropriate when identity is irrelevant to the task, when the dataset will not be joined with other datasets, and when regulatory requirements or risk tolerance demand strong de-identification. Use redaction for training classifiers, extractive models, and generation models where outputs do not depend on individual identity. Do not use redaction for personalization, longitudinal analysis, or any task that requires tracking entities across time or linking records across datasets.

Masking is appropriate when partial identity information is needed for operational purposes, when the data will be accessed by humans who need limited context, and when you are protecting against insider threats or casual disclosure. Use masking for customer service interfaces, fraud investigation tools, and internal dashboards. Do not use masking for public data release or for datasets that will be shared with third parties who might correlate masked identifiers with external data.

Generalization is appropriate when distributional properties matter but individual precision does not, when quasi-identifiers create re-identification risk even after direct identifiers are removed, and when you can tolerate some utility loss in exchange for provable privacy properties like k-anonymity. Use generalization for statistical analysis, aggregate reporting, and training models on demographic or geographic trends. Do not use generalization for tasks that require fine-grained distinctions: fraud models that need exact timestamps, recommendation models that need exact locations, or risk models that need exact ages.

Synthetic replacement is appropriate when you need natural language fluency, when you are training generative models that will produce text in similar style, and when you have the engineering capacity to build and validate synthetic data pipelines. Use synthetic replacement for training conversational agents, document generators, and summarization models. Do not use synthetic replacement unless you can verify that the synthetic data preserves the statistical properties of the real data and does not introduce biases or artifacts.

## The Utility-Privacy Tradeoff

Every de-identification technique reduces both privacy risk and data utility. The tradeoff is not one-to-one: some techniques reduce risk significantly while reducing utility only slightly, and some techniques reduce utility significantly while reducing risk only slightly. The efficient frontier is the set of techniques that maximize utility for a given level of privacy protection or minimize privacy risk for a given level of utility.

Aggressive de-identification produces training data that is safe to release publicly but might not be useful for training high-quality models. A dataset where all names are redacted, all ages are generalized to 20-year ranges, all locations are generalized to regions, and all timestamps are generalized to years might satisfy k-anonymity with k equal to 100, but a model trained on that data will not capture the fine-grained patterns that drive performance on tasks requiring demographic precision, temporal precision, or geographic precision.

Weak de-identification preserves utility but leaves re-identification risk that might be unacceptable under your threat model or regulatory obligations. A dataset where names are redacted but ages, zip codes, and timestamps are exact might produce a high-quality model but might violate GDPR's requirement to minimize personal data processing or the EU AI Act's requirement to implement technical safeguards against re-identification for high-risk systems.

The correct point on the tradeoff curve is not a technical question. It is a risk question that requires input from legal, compliance, product, and leadership. Engineering can quantify the utility loss from different de-identification techniques by training models on multiple versions of the dataset and measuring task performance. Engineering can estimate re-identification risk by applying k-anonymity analysis or by running simulated re-identification attacks using publicly available external datasets. But engineering cannot decide how much utility loss is acceptable or how much residual risk is tolerable. That is a business decision informed by regulatory requirements, competitive dynamics, and risk appetite.

## Validation and Auditing

De-identification is not a one-time operation. It is a pipeline that must be validated before deployment and audited continuously after deployment. Validation answers the question: did the de-identification work as intended? Auditing answers the question: is the de-identification still working as the data and the threat landscape evolve?

Validation starts with coverage measurement: what percentage of PII was detected and de-identified? You cannot measure this directly without a ground truth dataset where all PII is labeled, which is expensive to create. The practical approach is to create a small labeled validation set by having human reviewers annotate a sample of the data, apply your de-identification pipeline to the sample, and measure precision and recall. Precision is the percentage of flagged items that are actually PII. Recall is the percentage of actual PII that was flagged. Both must be high. Precision below 90 percent means you are wasting review effort on false positives. Recall below 95 percent means you are leaking PII at a rate that will cause incidents.

Re-identification testing validates that the de-identified data cannot be easily re-identified by simulating adversarial attacks. You attempt to re-identify records in the de-identified dataset by linking them to external datasets: voter registration, property records, social media profiles, data breach dumps, or synthetic datasets generated to match the demographic distribution of your population. The success rate of re-identification is a direct measure of residual risk. If you can re-identify 10 percent of records using publicly available data, your de-identification is inadequate for public release. If you can re-identify 1 percent, it might be adequate depending on risk tolerance. If you cannot re-identify any records after reasonable effort, you have evidence that the de-identification is robust.

Auditing checks for drift and degradation. As new data sources are added, new PII types might appear that were not covered by the original detection logic. As models improve, extraction attacks that failed in 2024 might succeed in 2026. As external datasets proliferate, re-identification attacks that were impractical become feasible. Auditing runs the validation tests periodically: monthly or quarterly depending on data velocity and risk profile. Auditing also monitors for leakage in production by scanning model outputs for PII, logging extraction attempts, and reviewing user-reported incidents.

## What Comes Next

De-identification prepares data for training by reducing the risk that individuals can be re-identified from the dataset. But de-identification is not the same as anonymization, and it is not the same as pseudonymization. The distinction matters because regulations treat anonymized data, pseudonymized data, and identifiable data differently. GDPR does not apply to truly anonymized data, applies with reduced obligations to pseudonymized data, and applies fully to identifiable data. Misclassifying your de-identified data as anonymized when it is actually pseudonymized or still identifiable creates compliance risk. The next subchapter covers the technical and legal distinctions between anonymization and pseudonymization, the tests that determine which category your data falls into, and the implications for how you handle, store, and govern AI training datasets under 2026 regulatory frameworks.
