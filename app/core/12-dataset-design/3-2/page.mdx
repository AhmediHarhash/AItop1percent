# 3.2 â€” LLM-Generated Synthetic Data: Techniques and Pitfalls

In November 2025, a financial services company built a synthetic dataset for training a fraud detection assistant. They used GPT-4o to generate 50,000 examples of fraudulent transaction descriptions. The generation prompt was straightforward: "Generate a description of a fraudulent credit card transaction." They ran the generation at temperature 0.9 to encourage diversity, filtered for formatting issues, and added the synthetic examples to their training set.

Four weeks into production, their fraud detection system began flagging legitimate transactions at triple the expected rate. Investigation revealed a pattern. The synthetic training data contained phrases that real fraudsters rarely used. GPT-4o had generated descriptions like "unauthorized purchase made without consent" and "fraudulent transaction using stolen credentials." Real fraud descriptions from actual transaction logs were terser: "didn't buy this," "not me," "stolen card." The model learned to detect the verbose, explanatory style of synthetic fraud rather than the terse, informal style of real fraud reports.

The team retrained using synthetic data generated with stronger style constraints and lower temperature, but the damage to user trust took months to repair. The incident cost the company $1.8 million in manual review overhead and delayed three planned product launches. The root cause was not that LLM-generated synthetic data failed, but that the team did not understand how generation parameters, prompt design, and validation interact to determine synthetic data quality.

## The Generative Process for Training Data

Generating synthetic data with LLMs follows a different logic than using LLMs for completion or chat. In completion or chat, you optimize for helpfulness, accuracy, and coherence in a single output. In data generation, you optimize for diversity, distribution match, and scalability across thousands of outputs.

The process starts with a generation prompt that specifies the output format, content constraints, and any domain-specific requirements. The prompt must be precise enough to ensure outputs match your target distribution but loose enough to allow variation. Overly constrained prompts produce repetitive outputs. Underconstrained prompts produce off-distribution outputs that fail filtering.

You run generation at scale by calling the model API thousands of times with the same prompt or variations of the prompt. Temperature controls randomness. Higher temperature increases diversity but also increases incoherence and format errors. Lower temperature produces more consistent outputs but risks mode collapse where the model generates near-identical examples.

Batch generation produces raw outputs that enter a filtering pipeline. Filtering removes malformed examples, duplicates, and outputs that violate hard constraints like length limits or content policies. Typical filtering rejects 25 to 45 percent of raw outputs from a well-tuned generation prompt.

Filtered outputs enter validation, where you check distribution match, factual correctness, and task-specific quality criteria. Validation can be automated for simple quality dimensions or manual for complex tasks requiring expert judgment. Validated examples join your training dataset or evaluation set depending on their intended use.

This pipeline runs continuously as your data needs evolve. You generate in batches of 1,000 to 10,000 examples, validate a sample, adjust generation parameters based on validation results, and generate the next batch with improved settings.

## Prompt Design for Diversity and Quality

The generation prompt is the primary lever controlling synthetic data quality. A good generation prompt balances specificity and openness, provides enough context to ground outputs in the target domain, and includes constraints that prevent common failure modes.

Start with a clear instruction that specifies the output type. "Generate a customer support message complaining about a delivery delay" is better than "Generate a complaint." The first version constrains the domain and scenario. The second version allows the model to generate complaints about anything, producing off-distribution outputs.

Add constraints that define quality boundaries. "Generate a customer support message complaining about a delivery delay. The message should be 50 to 200 words, written in informal English, and should not contain profanity or abusive language." These constraints prevent outputs that are too short, too long, overly formal, or inappropriate.

Include few-shot examples when you need outputs to match a specific style or format. Provide two to five real examples that demonstrate the target distribution. The model learns stylistic patterns, vocabulary level, and structural conventions from examples more effectively than from explicit instructions.

Vary the prompt across generation batches to increase diversity. If you are generating fraud reports, alternate between prompts focused on different fraud types: card-not-present fraud, account takeover, friendly fraud, refund abuse. Each prompt variation explores a different region of the output space, preventing mode collapse.

Avoid leading language that biases the model toward unnatural outputs. "Generate a fraudulent transaction description that clearly indicates suspicious activity" produces descriptions that real fraudsters would never write. Real fraudsters do not describe their own activity as suspicious. Better: "Generate a short message from a customer disputing a charge they claim is fraudulent."

Test your prompts on small batches before scaling to thousands of examples. Generate 100 outputs, manually review 20, and check for common quality issues. Adjust prompt wording, constraints, or examples based on what you observe, then scale generation.

## Temperature and Sampling Strategy

Temperature controls the randomness of model outputs. At temperature 0, the model deterministically selects the highest-probability token at each step, producing identical outputs for identical prompts. At temperature 1, the model samples proportionally to the probability distribution, introducing randomness. At temperature 2, the model samples from a flattened distribution, producing highly diverse but often incoherent outputs.

For synthetic data generation, temperature between 0.7 and 1.2 typically works best. This range produces diverse outputs while maintaining coherence. Temperature below 0.7 risks mode collapse. Temperature above 1.2 risks incoherence and format errors.

Top-p sampling, also called nucleus sampling, controls diversity differently than temperature. Top-p clips the probability distribution to the smallest set of tokens whose cumulative probability exceeds p. At top-p 0.9, the model samples from the top 90 percent of probability mass, ignoring the long tail of unlikely tokens. This prevents the model from selecting bizarre tokens that derail coherence.

Combining temperature 0.9 with top-p 0.95 is a common baseline for data generation. This setting encourages diversity through temperature while preventing incoherence through top-p. Adjust based on observed output quality. If outputs are too repetitive, increase temperature or decrease top-p. If outputs are too incoherent, decrease temperature or increase top-p.

Frequency and presence penalties reduce repetition within a single output. Frequency penalty penalizes tokens based on how often they have appeared so far in the generation. Presence penalty penalizes tokens based on whether they have appeared at all. These penalties prevent the model from repeating phrases or ideas within a single example.

For data generation, apply moderate penalties: frequency penalty 0.3 to 0.7, presence penalty 0.1 to 0.3. Stronger penalties introduce awkward phrasing as the model avoids natural repetitions like articles and common words.

## Controlling Output Format

LLMs do not always produce outputs in the exact format you specify, even when prompted explicitly. Format drift causes downstream pipeline failures when filtering or validation logic expects a specific structure.

The most reliable way to control format is through the system prompt combined with user prompt instructions. The system prompt establishes global formatting rules: "You are a data generation assistant. Always respond with a single JSON object containing the fields: id, content, metadata. Do not include any text outside the JSON object." The user prompt specifies the content: "Generate a customer review for a coffee maker."

Even with explicit instructions, models sometimes add commentary, apologies, or explanations outside the requested format. Filtering must strip these artifacts before passing outputs to validation.

An alternative approach is to use structured output modes if your model provider supports them. GPT-4o and Claude 3.5 Sonnet both support JSON mode, which constrains outputs to valid JSON. This eliminates format drift for structured data but does not apply to free-text generation.

For free-text outputs, specify format constraints in the prompt: "Generate a single paragraph between 50 and 150 words. Do not include a title, heading, or label." Check output format during filtering and reject examples that violate constraints.

When generating multi-field structured data, specify field names, types, and constraints explicitly. "Generate a JSON object with these fields: name as a string between 5 and 50 characters, age as an integer between 18 and 80, email as a valid email address." The more precise your specification, the fewer format errors you will encounter.

## Common Failure Modes

LLM-generated synthetic data suffers from four recurring failure modes: mode collapse, style leakage, factual errors, and label misalignment.

Mode collapse occurs when the model produces low-diversity outputs clustered around a small number of patterns. You prompt for customer complaints and receive 5,000 examples that all follow the structure: "I am very disappointed with product because reason. I expected better." Real customer complaints vary wildly in structure, tone, and specificity. Mode collapse reduces the value of synthetic data because models trained on collapsed data fail to generalize to real diversity.

Style leakage occurs when synthetic data contains stylistic artifacts that distinguish it from real data. LLMs have characteristic phrasings, sentence structures, and vocabulary choices. Outputs often sound slightly too formal, too explanatory, or too polite compared to real user-generated text. Models trained on style-leaked data learn to produce outputs that sound generated rather than natural.

Factual errors occur when the LLM generates content that includes false information, hallucinated entities, or incorrect relationships. A synthetic medical case description might include a drug-disease interaction that does not exist. A synthetic legal contract might reference statutes that were never enacted. Factual errors in training data teach models to hallucinate, compounding the problem.

Label misalignment occurs when you generate inputs and labels separately, and the generated label does not correctly describe the generated input. You prompt the model to generate a spam email, then label it as spam. But the generated email contains no actual spam indicators, making the label incorrect. Models trained on misaligned labels learn spurious patterns.

Each failure mode requires targeted mitigation. Mode collapse: increase temperature, vary prompts, inject real examples. Style leakage: add style constraints to prompts, filter for stylistic artifacts, mix synthetic data with real data. Factual errors: validate against knowledge bases, use retrieval-augmented generation, restrict generation to verified facts. Label misalignment: generate inputs and labels jointly, validate alignment manually, use model-in-the-loop labeling.

## Quality Filtering Pipelines

Filtering transforms raw generated outputs into clean inputs for validation. A production-grade filtering pipeline applies multiple layers of quality checks.

Format validation checks that outputs conform to expected structure. For JSON outputs, validate schema compliance. For text outputs, check length bounds, character encoding, and absence of control characters. Reject malformed outputs immediately to prevent downstream errors.

Content classifiers detect off-topic or inappropriate content. Train a lightweight classifier to distinguish on-distribution outputs from off-distribution outputs using a small set of real examples. Run every generated output through the classifier and reject examples below a confidence threshold.

Deduplication removes identical or near-identical examples. Exact deduplication uses hash-based matching. Near-deduplication uses embeddings and cosine similarity thresholds, typically rejecting pairs with similarity above 0.95. Duplicates waste storage and skew training distributions.

Diversity heuristics enforce minimum variation within batches. Cluster outputs and check that clusters are balanced. If 80 percent of outputs fall into a single cluster, the generation process has collapsed. Reject the batch and adjust generation parameters.

Safety filters detect content that violates usage policies or legal constraints. If you are generating customer interactions, filter for profanity, hate speech, threats, and personal information. Safety filtering is non-negotiable for any data that might appear in training sets used by models serving public users.

Filtering should reject 20 to 40 percent of raw outputs in a well-tuned generation process. Rejection rates above 50 percent indicate poorly calibrated prompts. Rejection rates below 10 percent suggest filtering is too lenient and low-quality outputs are leaking through.

## Validation for Training vs Evaluation Data

Training data and evaluation data require different validation rigor. Training data validation focuses on distribution match and absence of catastrophic errors. Evaluation data validation focuses on correctness and absence of any errors.

For training data, validate a random sample of 5 to 10 percent of generated outputs. Check that examples are coherent, on-topic, and free of obvious factual errors or style artifacts. If sample quality is acceptable, pass the batch to training. If quality is poor, adjust generation parameters and regenerate.

For evaluation data, validate every example. Human reviewers check that each input is realistic and each label is correct. Evaluation data quality directly determines measurement accuracy. A single mislabeled evaluation example can skew metrics and lead to incorrect conclusions about model performance.

Validation workflows differ by task complexity. Simple tasks like sentiment classification can use non-expert validators. Complex tasks like medical diagnosis require domain experts. Budget $0.50 to $5.00 per example for validation labor depending on expertise requirements.

Automated validation supplements human review when you have ground truth or reference data. If you are generating synthetic versions of real data, compare statistical properties: length distribution, vocabulary overlap, n-gram frequency. Significant divergence indicates distribution mismatch.

Inter-annotator agreement measures validation consistency when using multiple reviewers. If three reviewers label the same synthetic example and disagree, the example is ambiguous and should be excluded. Target agreement rates above 85 percent for well-defined tasks.

## When LLM-Generated Data Helps vs Hurts

LLM-generated synthetic data helps when real data is scarce, expensive, or inaccessible, and when the generative model understands the target domain well enough to produce realistic examples. It hurts when the generative model lacks domain knowledge, when task definitions require grounding in facts the model does not possess, or when style leakage introduces artifacts that harm model performance.

Generated data helps for natural language tasks where diversity matters more than factual precision. Customer support, content moderation, sentiment analysis, and text classification all benefit from synthetic augmentation because the model can produce diverse, realistic examples without needing deep domain knowledge.

Generated data hurts for knowledge-intensive tasks where factual correctness is critical. Legal analysis, medical diagnosis, scientific reasoning, and financial forecasting require factually accurate training data. LLM-generated examples will contain hallucinations and errors that degrade model reliability.

Generated data helps for rare events and edge cases that occur infrequently in real data. Security threats, adversarial inputs, policy violations, and system failures happen rarely but must be handled correctly. Synthetic examples let you cover these scenarios comprehensively.

Generated data hurts when used as the sole data source without real data grounding. Models trained exclusively on synthetic data learn the biases and artifacts of the generator, not the true distribution of real-world inputs. Always mix synthetic data with real data to maintain grounding.

Generated data helps during early development when real data is not yet available. You can build initial prototypes, test architectures, and validate approaches using synthetic data, then replace it with real data as collection pipelines come online.

Generated data hurts for evaluation if real data exists. Never evaluate model performance solely on synthetic data. Synthetic evaluation data tells you how well your model handles data similar to the generator's outputs, not how well it handles real user inputs.

## Integration with Real Data

The most effective use of synthetic data combines it with real data in carefully controlled mixing ratios. Start with a baseline of real data, even if small. Add synthetic data to expand coverage, increase diversity, and fill gaps.

Early in development, when real data is scarce, your training set might be 70 percent synthetic and 30 percent real. The real data grounds the distribution. The synthetic data provides volume and coverage. As real data accumulates, shift the ratio toward 50-50, then 30-70, then 10-90.

Track performance as a function of synthetic data ratio. Train models with 0 percent, 25 percent, 50 percent, 75 percent, and 100 percent synthetic data. Evaluate all models on the same real held-out set. This experiment reveals the optimal mixing ratio for your task.

Some tasks benefit from high synthetic ratios. Content moderation systems often train on 60 to 80 percent synthetic data because real examples of policy violations are rare and difficult to collect. Other tasks degrade with synthetic ratios above 20 percent because real-world complexity exceeds what the generator can capture.

Provenance tracking is essential when mixing data sources. Tag every example with metadata indicating whether it is real or synthetic, which generator produced it, and which generation batch it came from. This metadata enables debugging when model performance degrades unexpectedly.

Rebalance mixing ratios over time as real data collection continues. Synthetic data is most valuable when real data is scarce. As real data becomes abundant, synthetic data shifts from a primary source to a supplementary source focused on edge cases and rare events.

## The Infrastructure for LLM-Generated Data

Operating LLM-generated synthetic data at scale requires infrastructure for generation orchestration, filtering, validation, and integration. Build this infrastructure early, before you need to generate tens of thousands of examples under deadline pressure.

Generation orchestration manages API calls, retries, rate limits, and cost controls. Use batch processing frameworks that parallelize generation across multiple API keys or self-hosted model instances. Monitor spend in real time and enforce budget caps to prevent runaway costs.

Filtering infrastructure applies quality checks at scale. Implement filters as modular components that can be added, removed, or reordered without changing the core pipeline. Log rejection reasons for every filtered example so you can diagnose common failure modes.

Validation infrastructure routes examples to human reviewers or automated validation checks based on data type and quality requirements. Build review interfaces that present examples clearly and collect structured feedback. Track reviewer performance and inter-annotator agreement to maintain validation quality.

Integration infrastructure merges validated synthetic data into your data lake or training pipeline with appropriate metadata and access controls. Ensure synthetic data is versioned, reproducible, and auditable for compliance and debugging.

By 2026, teams operating mature synthetic data pipelines generated hundreds of thousands of examples per month at costs below $5,000. The infrastructure investment paid for itself within the first project and provided a reusable capability serving every subsequent project.

Understanding how to design prompts specifically for high-quality data generation, which differs fundamentally from prompting for completion or chat, requires mastering techniques for controlling diversity, enforcing constraints, and validating alignment, which we will explore in the next subchapter on prompt engineering for data generation.
