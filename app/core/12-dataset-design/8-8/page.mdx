# 8.8 — Fairness Metrics for Dataset Evaluation

Three months after deploying a loan eligibility model, a mid-sized bank discovered through a regulatory audit that their approval rates differed by 18 percentage points between two demographic groups — a gap that triggered legal review and public scrutiny. The machine learning team was blindsided. They had tested the model extensively, achieved 89% accuracy on their holdout set, and seen no obvious red flags in standard performance metrics. The problem was not that they failed to measure fairness but that they never defined what fairness meant in their context. They measured overall accuracy when they needed to measure group-specific error rates. They optimized for predictive performance when they needed to balance performance with equity. By the time the audit revealed the disparity, the model had processed 47,000 applications, and the bank faced both remediation costs and reputational damage that far exceeded the cost of proper fairness evaluation during development.

Fairness metrics quantify equity across demographic groups, but fairness itself is not a single concept. It is a family of related but conflicting notions, each reflecting different values and priorities. Demographic parity prioritizes equal treatment. Equalized odds prioritizes equal accuracy. Predictive parity prioritizes equal trustworthiness. Calibration prioritizes equal reliability. No single metric captures all dimensions of fairness, and optimizing for one often degrades another. The central challenge in fairness evaluation is not measuring whether your model is fair — it is deciding what fair means for your product, your users, and your regulatory context.

## The Fairness Metrics Landscape

Demographic parity — also called statistical parity or equal acceptance rate — requires that the proportion of positive predictions is the same across groups. If your model predicts loan approval for 40% of Group A applicants, it must predict approval for 40% of Group B applicants. This metric treats fairness as equal treatment independent of any other factors. It is appealing because it is simple to measure, easy to explain to non-technical stakeholders, and aligns with common intuitions about equality. It is controversial because it ignores differences in base rates between groups. If Group A has a higher true rate of loan repayment than Group B due to socioeconomic factors unrelated to discrimination, demographic parity forces the model to approve lower-quality applicants from Group B or reject higher-quality applicants from Group A.

Demographic parity is appropriate when you believe that outcome differences between groups reflect historical or systemic bias rather than legitimate differences in qualifications. It is common in contexts where the base rate difference itself is viewed as unjust — for example, if criminal justice risk scores show different arrest rates between racial groups, and you believe those arrest rate differences reflect biased policing rather than true criminality. Demographic parity corrects for that bias by enforcing equal positive prediction rates. The cost is that you may sacrifice predictive accuracy.

Equalized odds requires that true positive rates and false positive rates are equal across groups. This means that among applicants who truly qualify for a loan, the model approves the same proportion from each group. Among applicants who do not qualify, the model incorrectly approves the same proportion from each group. Equalized odds treats fairness as equal accuracy, ensuring that the model makes errors at the same rate regardless of group membership. It allows for different positive prediction rates if those differences reflect true differences in qualification rates.

Equalized odds is appropriate when base rates differ legitimately between groups and you want the model to respect those differences while ensuring it does not systematically favor one group over another in its errors. In lending, if Group A genuinely has higher creditworthiness due to income or employment stability, equalized odds allows the model to approve more Group A applicants while requiring that approval accuracy is equal across groups. The challenge is that equalized odds is harder to achieve than demographic parity because it requires balancing two error rates simultaneously — false positives and false negatives.

Equal opportunity is a relaxation of equalized odds that only requires equal true positive rates across groups. It ignores false positive rates. This metric is appropriate when false negatives are much more harmful than false positives. In medical diagnosis, missing a disease case is worse than a false alarm. Equal opportunity ensures that among sick patients, the model detects the disease at equal rates across demographic groups, even if false alarm rates differ. In lending, equal opportunity ensures that among qualified applicants, approval rates are equal, even if unqualified applicants are rejected at different rates.

Predictive parity — also called outcome parity or predictive rate parity — requires that precision is equal across groups. Among applicants the model approves, the proportion who actually repay their loans must be the same across groups. This treats fairness as equal trustworthiness, ensuring that a positive prediction means the same thing regardless of group membership. Predictive parity is appropriate when users rely on model predictions to make decisions and need those predictions to be equally reliable across groups. In hiring, if a resume screening model recommends candidates, predictive parity ensures that recommended candidates are equally likely to succeed regardless of their demographic background.

Calibration requires that predicted probabilities match observed frequencies within each group. If the model predicts 70% loan repayment probability for a set of applicants, then 70% of those applicants should actually repay, and this relationship must hold separately for each demographic group. Calibration treats fairness as equal reliability, ensuring that probability estimates are accurate and interpretable across groups. Calibration is critical when users see probability scores rather than binary predictions and when those scores inform downstream decisions. In medical contexts, if a model predicts 30% risk of readmission, doctors need that 30% to mean the same thing for all patient groups.

Counterfactual fairness is a causal definition requiring that predictions would remain the same if an individual's protected attributes changed but all causally downstream features remained constant. This is the most philosophically rigorous fairness notion but also the hardest to measure and enforce. It requires causal modeling of how protected attributes influence features and outcomes. Counterfactual fairness is rarely used in practice because it demands causal knowledge that most teams lack and because enforcing it often requires dropping informative features that are correlated with protected attributes even if not directly caused by them.

## Why Fairness Metrics Conflict With Each Other

The impossibility results are mathematical proofs, not soft tradeoffs. You cannot simultaneously achieve demographic parity, equalized odds, and predictive parity unless base rates are identical across groups or your model achieves perfect accuracy. If Group A has a 60% base rate of positive outcomes and Group B has a 40% base rate, then any model that achieves high accuracy will necessarily violate demographic parity because it will predict more positives for Group A. Any model that enforces demographic parity will necessarily violate predictive parity because approving equal proportions from unequal base rate groups produces different precision per group.

This is not a technical limitation or an artifact of insufficient data. It is a fundamental constraint that follows from probability theory. The implication is profound: you must choose which fairness definition you prioritize because you cannot satisfy all of them. This choice is a value judgment, not a technical one. It requires input from Legal, Product, domain experts, and affected stakeholders. The machine learning team can measure and optimize for any chosen metric, but the team cannot make the choice alone.

The conflict between fairness and accuracy is another tradeoff, though not an impossibility. Enforcing fairness constraints typically reduces overall model performance because you are adding constraints that the optimizer must satisfy. The magnitude of the accuracy loss depends on the fairness metric, the degree of base rate difference, the model architecture, and the quality of your data. Small base rate differences and flexible models produce small accuracy losses. Large base rate differences and rigid models produce large losses. You must quantify this tradeoff and determine whether the fairness gain justifies the accuracy cost.

Some teams discover that fairness constraints improve generalization because they prevent the model from exploiting spurious correlations. If your model learns to predict loan repayment based on zip code, and zip code correlates with race, then enforcing equalized odds across racial groups forces the model to find more robust predictive features. The model may lose a few points of training accuracy but gain test accuracy and deployment robustness. This is not guaranteed — sometimes fairness constraints simply reduce performance without any generalization benefit — but it is common enough that fairness should be evaluated as a potential regularizer, not just a constraint.

## Choosing the Right Metric for Your Product Context

The metric choice depends on three factors: the downstream impact of predictions, the source of base rate differences, and the regulatory environment. Start with impact. If your model denies someone a loan, what harm does that cause? Denying a qualified applicant is a false negative that prevents someone from accessing credit they deserve. Approving an unqualified applicant is a false positive that leads to default and financial loss for the lender. If false negatives and false positives are equally harmful, equalized odds is appropriate. If false negatives are much worse, equal opportunity is appropriate. If false positives are much worse, you might prioritize equal false positive rates even at the cost of unequal false negative rates.

Consider the source of base rate differences. If you believe those differences reflect historical discrimination — redlining, employment discrimination, educational inequity — then enforcing demographic parity corrects for that injustice. If you believe those differences reflect legitimate factors — income, employment stability, credit history — then equalized odds respects those differences while ensuring equal accuracy. This is a normative judgment that requires domain expertise and stakeholder input. You cannot determine the source of base rate differences purely from data.

Regulatory requirements often dictate metric choice. The Equal Credit Opportunity Act in the United States prohibits lending discrimination but does not specify a single fairness metric. The EU AI Act requires high-risk systems to be evaluated for bias but leaves metric choice to deployers. Some regulators emphasize disparate impact, which resembles demographic parity. Others emphasize equal treatment conditional on qualifications, which resembles equalized odds. You must consult Legal to understand which metrics align with your regulatory obligations.

Product context matters. In content moderation, false negatives allow harmful content to remain visible, and false positives remove legitimate content. The harm asymmetry depends on content type. For misinformation, false negatives are worse. For borderline political speech, false positives are worse. In medical diagnosis, false negatives mean missed diagnoses, and false positives mean unnecessary treatment. The harm asymmetry depends on disease severity and treatment risk. Your fairness metric should align with the harm model your product team has defined.

User trust considerations often push toward calibration and predictive parity. If users see scores or probabilities, those numbers must mean the same thing across groups or users will lose trust in the system. A hiring manager who learns that a 90% suitability score means different things for different demographic groups will stop trusting the model entirely. Calibration ensures that scores are interpretable and comparable.

## Threshold Selection and Its Impact on Fairness

Most classification models produce continuous scores that you convert to binary decisions using a threshold. If the predicted probability exceeds the threshold, you predict positive; otherwise, you predict negative. Threshold choice has enormous impact on fairness metrics. A single model can satisfy demographic parity at one threshold, equalized odds at another threshold, and neither at a third. This gives you a tuning parameter to adjust fairness without retraining the model.

The standard approach is to choose a threshold that optimizes overall accuracy or F1 score on your validation set. This threshold is rarely fair. It optimizes for the majority group because the majority group dominates validation data. A better approach is to grid search over thresholds, measure fairness metrics at each threshold, and choose the threshold that best balances fairness and accuracy according to your priorities. This might mean accepting slightly lower overall accuracy to achieve much better fairness.

Group-specific thresholds are another option. You apply different thresholds to different demographic groups to achieve your fairness goal. If you want demographic parity, you adjust thresholds per group until positive rates are equal. If you want equalized odds, you adjust thresholds until true positive rates and false positive rates are equal. Group-specific thresholds are effective but controversial. Regulators and stakeholders may view different thresholds as discriminatory treatment, even if the goal is equal outcomes. You must consult Legal before deploying group-specific thresholds.

Threshold tuning should happen on a separate validation set, not your test set. If you optimize threshold on your test set, you overfit and your reported fairness metrics are inflated. The proper workflow is train set for model training, validation set for threshold tuning, and test set for final evaluation. If your dataset is small, use cross-validation to avoid overfitting during threshold selection.

## Subgroup Analysis: Going Beyond Binary Protected Attributes

Most fairness analysis starts with binary protected attributes — two racial groups, two gender groups. Real-world populations are not binary. Race is multi-category and context-dependent. Gender includes non-binary identities. Age is continuous. Disability is multi-dimensional. Comprehensive fairness evaluation requires subgroup analysis across all protected attributes and their intersections.

Subgroup analysis measures fairness metrics separately for each demographic subgroup. Instead of comparing Black and white populations, you compare Black, white, Hispanic, Asian, and multiracial populations. Instead of comparing male and female, you compare male, female, and non-binary. The number of comparisons explodes quickly. If you have five racial categories, three gender categories, and four age bands, you have 60 subgroups. Measuring fairness across all pairs of subgroups requires 1,770 comparisons.

The sample size problem becomes severe. If your test set has 10,000 examples and one subgroup represents 0.5% of the population, you have only 50 examples to measure that subgroup's error rate. The confidence intervals are wide, and small random fluctuations produce large metric swings. You cannot determine whether observed unfairness is real or noise. The solution is to prioritize subgroups by prevalence and risk. Measure fairness for large subgroups with high confidence. For small subgroups, collect more data or acknowledge measurement uncertainty.

Subgroup reporting should include confidence intervals, not just point estimates. If Group A has a true positive rate of 0.82 with 95% confidence interval from 0.78 to 0.86, and Group B has a true positive rate of 0.79 with confidence interval from 0.71 to 0.87, you cannot conclude that the groups differ significantly. The intervals overlap. Proper statistical testing — hypothesis tests for rate differences — is mandatory for making fairness claims.

## Reporting Fairness Results to Stakeholders

Stakeholders need to understand fairness metrics, but most stakeholders are not machine learning practitioners. Your reporting must translate technical metrics into business and ethical terms. Instead of saying "we achieved 0.94 demographic parity," say "approval rates differ by 6 percentage points between groups, which is within our target threshold of 10 points." Instead of saying "equalized odds violation of 0.08," say "the model is 8 percentage points more accurate for Group A than Group B among qualified applicants."

Visualizations help. Plot error rates per group with confidence intervals. Plot calibration curves per group showing predicted probability versus observed frequency. Plot precision-recall curves per group to show the accuracy-coverage tradeoff. Show threshold-fairness curves illustrating how different thresholds affect fairness metrics. Visual comparisons make disparities obvious and support decision-making.

The fairness report should include four sections: metric definitions in plain language, baseline performance before mitigation, post-mitigation performance with ablation studies showing the impact of each intervention, and residual gaps with plans for further improvement. The report must be honest about tradeoffs. If you sacrificed two points of overall accuracy to close a 10-point fairness gap, say so explicitly and explain why that tradeoff is justified.

Stakeholders need to approve the fairness metrics you measure and the thresholds you target. If Product expects demographic parity and you deliver equalized odds, you have not met the requirement even if your model is technically fair by a different definition. Alignment on metric choice must happen before training, not after deployment. Document the decision and the rationale so that future teams understand why particular metrics were chosen.

## Continuous Fairness Monitoring

Fairness is not static. Data distribution shifts, user behavior changes, and societal context evolves. A model that is fair at launch may become unfair six months later. Continuous monitoring is mandatory. You must log protected attributes in production, measure fairness metrics on rolling windows of predictions, and alert when metrics degrade beyond acceptable thresholds.

Privacy and compliance considerations complicate monitoring. Some jurisdictions prohibit collecting protected attributes. Some organizations have policies against storing demographic data. You need privacy-preserving fairness monitoring techniques — differential privacy, federated learning, or proxy-based fairness where you infer group membership from correlated features rather than directly observing it. These techniques add noise and reduce measurement precision but allow monitoring in privacy-sensitive contexts.

Fairness monitoring should trigger model retraining. If equalized odds violation increases from 0.03 to 0.10, you need to investigate whether data distribution changed, whether the model drifted, or whether a bug was introduced. Retraining with updated data and recalibrated mitigation strategies restores fairness. Automated retraining pipelines should include fairness checks as deployment gates. If a retrained model improves accuracy but degrades fairness, it should not deploy.

Fairness metrics must evolve with your product. If you launch a lending model targeting a single country and then expand internationally, your fairness metrics must account for cross-country differences in protected attributes, base rates, and regulatory requirements. If you launch a hiring model for technical roles and expand to non-technical roles, your fairness analysis must adapt to different skill distributions and demographic patterns. Fairness evaluation is product-specific and context-dependent, not one-size-fits-all.

## Advanced Metric Considerations: Multicalibration and Individual Fairness

Multicalibration extends standard calibration by requiring that predicted probabilities are accurate not just overall and per protected group, but for every identifiable subpopulation simultaneously. A model is multicalibrated if, for any subgroup you can define — even complex intersections like "Black women over 50 with low income" — the predicted probabilities match observed frequencies. This is stronger than group calibration because it prevents the model from being well-calibrated on average while poorly calibrated on specific subgroups.

Achieving multicalibration is computationally intensive. You cannot simply train a model and check if it is multicalibrated; you must use specialized algorithms that iteratively adjust predictions to satisfy calibration across all possible subgroups. These algorithms are recent research advances, not yet standard practice, but they represent the future of fairness-aware training. Multicalibration is especially valuable in high-stakes domains like criminal justice or healthcare where errors on small subgroups can cause severe harm.

Individual fairness is a metric category that focuses on treating similar individuals similarly, rather than ensuring statistical parity across groups. The core principle is that individuals who are similar with respect to the task should receive similar predictions. If two loan applicants have similar income, employment history, and credit scores, they should receive similar approval probabilities regardless of race or gender. Individual fairness is appealing because it directly operationalizes the idea of non-discrimination.

The challenge with individual fairness is defining similarity. What makes two individuals similar for a loan decision? Should similarity include income, education, employment, credit history, assets, and family status? Should it exclude or include protected attributes? Different similarity definitions lead to different fairness conclusions. Moreover, measuring individual fairness requires access to individual-level data and the ability to compute pairwise similarities, which is computationally expensive for large datasets.

Counterfactual tokens and causal fairness metrics attempt to disentangle fair and unfair uses of features. A feature is unfair if it causally depends on a protected attribute and does not causally influence the outcome. For example, if zip code correlates with race due to historical redlining but does not directly affect creditworthiness, using zip code is unfair. Causal fairness requires building causal graphs that model how protected attributes, features, and outcomes relate. This is methodologically demanding but provides the most principled foundation for fairness.

## Temporal Fairness: Measuring Stability Across Time

Fairness is not just a spatial concept — across groups at a single point in time — but also a temporal one. A model that is fair today may become unfair tomorrow if data distribution shifts differently for different groups. Temporal fairness requires monitoring how group-specific performance evolves over weeks, months, and years, and ensuring that fairness does not degrade over time.

The most common temporal fairness failure is drift asymmetry. When data distribution shifts, it often shifts more for some groups than others. If your lending model is trained on 2024 data and deployed in 2026, economic conditions may have changed more for low-income borrowers than high-income borrowers. The model's accuracy may degrade more for low-income groups, creating unfairness even if the model was fair initially. Temporal monitoring must measure drift per group, not just overall.

Seasonal patterns create cyclical fairness issues. A fraud detection model may perform differently across holiday seasons, tax seasons, and back-to-school seasons. If different demographic groups have different transaction patterns during these periods, fairness metrics will fluctuate seasonally. You need seasonal baselines — knowing that false positive rates increase by 5% for Group A in December is fine if that is consistent across years and explainable by shopping behavior. Unexpected seasonal shifts indicate problems.

Model staleness affects groups asymmetrically. If your model was trained two years ago and has not been retrained, it is stale. But staleness impacts groups differently based on how much their data distributions have changed. Younger users' behavior may change rapidly due to technology adoption or cultural shifts, while older users' behavior may remain stable. A stale model will degrade more for younger users, creating age-based unfairness. Retraining schedules should account for group-specific staleness.

## Legal and Regulatory Perspectives on Metric Choice

Different jurisdictions and domains have different legal standards for fairness, which translate to different metric requirements. In the United States, disparate impact analysis focuses on whether a facially neutral policy has disproportionate adverse effects on a protected class. This aligns with demographic parity — if your model's rejection rates differ substantially across racial groups, you may face disparate impact liability even if the model does not explicitly use race.

The four-fifths rule is a legal heuristic from employment law stating that if the selection rate for a protected group is less than 80% of the selection rate for the highest-performing group, this is evidence of adverse impact. Translated to model fairness, if your approval rate for Group A is 50% and for Group B is 35%, the ratio is 0.7, which fails the four-fifths test. Many organizations use the four-fifths rule as a threshold for demographic parity violations, though it is not a strict legal requirement in all contexts.

The EU AI Act classifies certain AI systems as high-risk and requires technical documentation demonstrating that bias has been tested and mitigated. The Act does not mandate specific fairness metrics but requires that systems are evaluated for bias and that documentation includes the metrics used, the results, and the mitigation strategies applied. This regulatory flexibility means you must choose metrics that align with your domain and justify that choice to auditors.

GDPR's right to explanation intersects with fairness metrics. If an individual is denied credit by an automated system, they have the right to understand why. This pushes toward fairness metrics that are interpretable and explainable — calibration and predictive parity are easier to explain than counterfactual fairness. Your metric choice must consider not just statistical properties but also explainability to end users and regulators.

Sector-specific regulations impose additional constraints. In healthcare, HIPAA and the Affordable Care Act prohibit discrimination based on health status and protected attributes. In financial services, the Equal Credit Opportunity Act, Fair Housing Act, and Community Reinvestment Act create overlapping fairness obligations. In employment, Title VII prohibits discrimination based on race, color, religion, sex, and national origin. Each regulation maps to different fairness metrics, and high-risk systems may need to satisfy multiple metrics simultaneously to ensure compliance across all applicable laws.

## Fairness Metric Selection Checklist

Before training, answer these questions to guide metric choice. First, what is the primary harm your model could cause? Denial of opportunity, allocation of risk, differential treatment, or erosion of trust? Second, who are your protected groups? Race, ethnicity, gender, age, disability, socioeconomic status, geography, or others? Third, what are the base rates of positive outcomes across groups? If they differ substantially, why? Fourth, what are your stakeholders' fairness intuitions? Do they prioritize equal treatment, equal accuracy, or equal outcomes?

Fifth, what are your regulatory obligations? Which laws apply, and what do they require? Sixth, what are your computational and data constraints? Can you support complex metrics that require large sample sizes per group? Seventh, what is your risk tolerance? Can you accept small fairness violations if overall accuracy is high, or do you require strict fairness even at accuracy cost? Eighth, how will you explain fairness to end users? Can you translate your chosen metrics into plain language?

Document the answers and the resulting metric choice. This documentation serves three purposes: it justifies your decision to regulators, it provides context for future teams who inherit the model, and it forces you to think rigorously about fairness rather than defaulting to the first metric you encounter. The documentation should include considered and rejected alternatives — explaining why you chose equalized odds over demographic parity, for example.

## Case Study: Fairness Metrics in a Hiring Model

A technology company building a resume screening model faced the metric choice problem. Legal required compliance with Title VII, which prohibits disparate impact. Product wanted high precision to avoid wasting recruiter time on unqualified candidates. Engineering preferred metrics that were computationally efficient and well-supported by existing libraries. Candidates wanted transparency about how decisions were made.

The team evaluated five metrics: demographic parity, equalized odds, equal opportunity, predictive parity, and calibration. Demographic parity was rejected because base rates of qualifications differed across groups due to pipeline issues in computer science education. Forcing equal selection rates would require lowering the bar for some groups or raising it for others, which Legal determined created reverse discrimination risk. Equalized odds was feasible but complicated to explain to candidates — few understand true positive and false positive rates.

Equal opportunity was chosen as the primary metric because it ensured that among qualified candidates, selection rates were equal across groups. This aligned with Title VII's focus on equal opportunity, satisfied Product's precision requirements by not constraining false positive rates, and was explainable: "if you are qualified, your demographic background does not affect your chances." Calibration was added as a secondary metric to ensure that recommendation scores meant the same thing across groups, supporting transparency.

The team set a threshold of 5 percentage points for equal opportunity violations and 10 percentage points for calibration violations. These thresholds were based on Legal's assessment of defensible disparate impact ratios and Product's tolerance for fairness-accuracy tradeoffs. Monitoring dashboards tracked both metrics weekly, alerting when violations occurred. Over 18 months, the model maintained equal opportunity within threshold while achieving 78% precision, which Product deemed acceptable.

## Fairness Metrics in Multi-Stakeholder Environments

Real-world fairness evaluation often involves conflicting stakeholder priorities. Legal wants metrics that align with regulatory requirements and minimize liability. Product wants metrics that preserve user experience and business outcomes. Engineering wants metrics that are computationally tractable and well-supported by tooling. Users want metrics that match their intuitive sense of fairness. Reconciling these perspectives requires negotiation and compromise.

The stakeholder alignment process should begin early, during problem framing. Convene representatives from Legal, Product, Engineering, Trust and Safety, and affected user communities. Present the menu of fairness metrics with plain-language explanations of what each metric means and what it optimizes for. Walk through concrete examples showing how different metrics lead to different decisions. Facilitate discussion about values and priorities. Document the chosen metrics and the rationale for choosing them.

Expect disagreement. Product may want predictive parity because it preserves precision and minimizes wasted effort on false positives. Legal may want demographic parity because it most clearly demonstrates non-discrimination to regulators. Users may want equal opportunity because it ensures qualified individuals are treated equally. These are not technical disagreements but value disagreements. The resolution must be a negotiated compromise that satisfies hard constraints — legal requirements are non-negotiable — while balancing soft preferences.

Multi-metric evaluation is often the compromise. Instead of choosing a single fairness metric, you evaluate and report multiple metrics, setting thresholds for each. The model must satisfy all thresholds to deploy. This is more conservative than single-metric evaluation because you are imposing multiple constraints, but it addresses stakeholder concerns more comprehensively. The downside is that multi-metric optimization is harder and may not always be feasible when metrics conflict mathematically.

Transparency about tradeoffs builds trust. When you report fairness metrics to stakeholders, include the counterfactual: what would other metrics show if you optimized for different definitions? If you chose equal opportunity, show what demographic parity and predictive parity would be. This demonstrates that you made an informed choice, not that you cherry-picked the metric that made your model look best. Transparency about what you are not optimizing for is as important as reporting what you are optimizing for.

Even with rigorous single-axis fairness analysis — measuring disparities along race, gender, age, or disability independently — you can miss critical inequities that emerge at the intersection of multiple protected attributes. Fairness along one axis guarantees nothing about fairness along two, and that is where we turn next.
