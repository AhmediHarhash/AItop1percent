# 1.4 — Dataset Strategy by AI Product Archetype

The most expensive dataset is not the one that costs too much. It is the one that costs everything but solves the wrong problem. You can spend six months and half a million dollars building a perfect dataset with flawless labels, comprehensive coverage, and rigorous quality control, and still fail completely if you built the wrong type of dataset for your product architecture. A chatbot needs conversation trajectories, not classification labels. A retrieval system needs query-document-answer triplets, not question-answer pairs. An agent needs action sequences with state transitions, not static input-output mappings. Dataset strategy is not universal. It is architecture-specific, and applying the wrong strategy guarantees failure regardless of execution quality.

Dataset strategy is not universal. The dataset you need depends fundamentally on what type of AI product you are building. A chatbot requires conversation trajectories. A retrieval-augmented generation system requires query-document pairs with relevance judgments. An agent requires action sequences with state transitions. A classifier requires labeled examples with clear decision boundaries. A code generation system requires input-output pairs with correctness verification. A content moderation system requires examples at decision thresholds with policy interpretations. A recommendation system requires interaction histories with engagement signals. These are not minor variations on a theme. They are structurally different dataset architectures, and applying the wrong architecture to your product archetype guarantees failure regardless of dataset quality. Understanding the dataset requirements for your specific archetype is the foundation of dataset strategy.

## The Chatbot Archetype: Conversation Trajectories

Chatbots are multi-turn conversational systems. The fundamental unit of evaluation is not a single response but an entire conversation. Your dataset must capture conversation trajectories that include user intent shifts, context accumulation, clarification exchanges, error recovery, and natural conversation endings. A chatbot dataset is not a collection of question-answer pairs. It is a collection of multi-turn dialogues where each turn depends on everything that came before.

Most teams building chatbots make the single-turn mistake. They collect thousands of individual questions with ideal responses, treating each exchange as independent. This produces a chatbot that answers each question well in isolation but fails catastrophically in real conversations. The bot cannot handle follow-up questions because it has no examples of follow-up patterns. It cannot recover from misunderstandings because it has no examples of clarification dialogues. It cannot maintain context across turns because its dataset never modeled context accumulation. You need conversations, not isolated exchanges.

Your chatbot dataset must include examples of intent shifts within a single conversation. A user starts asking about pricing, then pivots to asking about features, then returns to pricing with a refinement based on the feature information. Your dataset needs these trajectories. It must include examples of the bot asking clarifying questions and the user providing additional information that changes the response. It must include examples of the user correcting the bot and the bot adjusting its understanding. It must include examples of natural conversation closings where the user's need has been met and the conversation ends gracefully. These are the conversation patterns your chatbot will encounter in production, and your dataset must reflect them.

The length distribution of your conversations matters. If all your example conversations are three turns, your bot will struggle with conversations that go ten turns deep. If all your conversations resolve quickly, your bot will not know how to handle extended troubleshooting dialogues. Your dataset must cover the full range of conversation lengths you expect in production. Industry data from conversational AI deployments in 2025 consistently shows that real customer service conversations average eight to twelve turns, with a long tail extending to thirty or more turns for complex issues. Your dataset must include examples across this distribution.

The conversation topics in your dataset must match the topic distribution you expect in production. If 40 percent of real conversations are about billing issues, but only 10 percent of your dataset conversations cover billing, your bot will underperform on the most common user need. You cannot assume that a small number of billing examples will generalize. Conversational patterns differ by topic. Billing conversations have different clarification patterns, different user frustration patterns, and different resolution patterns than feature explanation conversations. You need representative coverage.

Your chatbot dataset must also include failure conversations. Real users will ask questions outside your domain. They will provide ambiguous information. They will express frustration. Your dataset needs examples of how the bot should handle these situations: gracefully declining out-of-scope requests, asking for clarification when information is ambiguous, de-escalating frustrated users. If your dataset contains only successful happy-path conversations, your bot will not know how to handle the inevitable failures.

Context window management is a critical aspect of chatbot datasets that teams frequently overlook. Real conversations accumulate context over many turns, and your dataset must show how to maintain, summarize, and reference that context. A conversation about troubleshooting a technical issue might span fifteen turns, with the user providing system details in turn two, error messages in turn five, and previous troubleshooting attempts in turn eight. Your dataset must include examples where the bot correctly recalls and references information from earlier turns. It must show when context should be explicitly summarized for clarity, and when prior statements should be updated based on new information.

The dataset must also capture conversational repair patterns. Users frequently misspeak, provide incomplete information, or change their minds mid-conversation. Your dataset needs examples of the bot recognizing these situations and gracefully handling them. When a user says "actually, I meant the premium plan, not the basic one," the bot must understand this is a correction and adjust the entire conversation context accordingly. When a user provides contradictory information across different turns, the bot must recognize the contradiction and seek clarification rather than proceeding with confused assumptions.

Tone and register variation across conversation trajectories is another dimension that matters. Customer service conversations can range from formal to casual, from friendly to frustrated, from patient to urgent. Your dataset must include conversations that span these tonal variations and show how the bot adapts its responses appropriately. A user who is frustrated and urgent needs a different response style than a user who is relaxed and exploratory. If your dataset contains only neutral, pleasant conversations, your bot will not know how to adjust tone when users signal different emotional states.

## The RAG Archetype: Query-Document-Answer Triplets

Retrieval-augmented generation systems combine search with generation. Your dataset must reflect both components. You need queries, the documents that should be retrieved for those queries, relevance judgments that indicate which documents are helpful, and the answers that should be generated given those documents. This is a triplet structure, not a pair structure. Most teams building RAG systems collect question-answer pairs and assume retrieval will work itself out. It does not.

Your RAG dataset must include queries with multiple relevant documents at different relevance levels. Some documents directly answer the query. Some provide supporting context. Some are tangentially related. Your retrieval component must learn to distinguish these levels, and your dataset must provide the signal. A binary relevant-or-not judgment is insufficient. You need graded relevance: highly relevant, somewhat relevant, not relevant. This allows you to evaluate whether your retrieval is surfacing the best documents first, not just any relevant document.

The query formulation diversity in your dataset determines your system's ability to handle real user queries. Users do not ask questions in a single style. Some ask direct questions. Some provide keywords. Some describe their problem. Some include context. Some are precise. Some are vague. Your dataset must include all these query types for the same underlying information need. If your dataset contains only well-formed questions, your system will fail when users submit keyword searches or rambling descriptions.

Your RAG dataset must explicitly capture negative examples: queries where no document in your corpus contains a good answer. Real users will ask questions outside your knowledge base. Your system must learn to recognize when retrieval has failed to find a sufficient answer, and your dataset must include examples of this. If every query in your dataset has a correct answer in the corpus, your system will hallucinate answers for out-of-scope queries rather than admitting it does not know. You need queries with no good answer, labeled as such.

The document length and structure in your dataset must match your production corpus. If your real documents are lengthy technical manuals with nested sections, but your dataset uses short FAQ entries, your retrieval and generation will not transfer. Long documents require different chunking strategies, different retrieval scoring, and different answer extraction than short documents. Your dataset must represent the document structure you will encounter in production.

Answer quality in your RAG dataset must be evaluated against the retrieved documents, not against external knowledge. The correct answer is not the objectively true answer. The correct answer is the best answer that can be generated from the retrieved documents. Your dataset must include examples where the retrieved documents contain incomplete or partially outdated information, and the answer reflects that limitation. This teaches your system to stay grounded in the retrieved content rather than supplementing with model knowledge that may contradict your documents.

Multi-hop reasoning is a common requirement in RAG systems that many datasets fail to capture. A user asks a question that requires synthesizing information from multiple documents. One document contains the product specifications, another contains the pricing structure, and a third contains the discount policy. The answer requires combining all three. Your dataset must include examples of multi-hop queries with the relevant documents and the synthesis that produces the answer. If your dataset only includes queries answerable from a single document, your system will struggle with queries requiring cross-document reasoning.

Temporal aspects of retrieval matter in many domains. A user asks about current policy, and your corpus contains both outdated policy documents and current policy documents. Your retrieval must surface the current document, not the outdated one. Your dataset must include examples where document recency is a relevance factor. It must show how to distinguish authoritative current documents from historical documents that should not be used for answering current queries.

Conflicting information across retrieved documents is another failure mode that datasets must address. Your corpus contains multiple documents that provide different, potentially contradictory answers to the same question. Your dataset must include examples showing how to handle this situation: acknowledge the conflict, cite both sources, explain the contradiction if possible, or escalate to human review if the conflict is irresolvable. If your dataset assumes retrieved documents are always consistent, your system will produce incoherent answers when faced with real-world document conflicts.

Citation and attribution are critical in RAG systems, particularly in professional contexts like legal, medical, or financial applications. Your dataset must include examples where the generated answer includes citations to specific documents or passages. Users need to know where the information comes from so they can verify it. Your dataset must show proper citation format, when to cite, and how to handle cases where information is synthesized from multiple sources.

## The Agent Archetype: Action Trajectories with State

Agent systems take actions in an environment based on observations. Your dataset must capture trajectories: sequences of state observations, actions taken, and resulting state changes. An agent dataset is not a collection of action examples. It is a collection of multi-step plans where each action depends on the current state and changes the state for the next action.

Most teams building agents collect examples of individual actions: given this situation, take this action. This is insufficient because it does not capture planning, sequencing, error recovery, or state-dependent decision-making. You need full trajectories from initial state to goal state, showing how the agent navigates through the state space. A trajectory dataset shows not just what to do, but when to do it and what to do next based on what happened.

Your agent dataset must include diverse path examples for the same goal. There is rarely a single correct action sequence. Different starting states require different approaches. Different intermediate outcomes require different adaptations. Your dataset must show multiple successful trajectories to the same goal from different starting conditions. This teaches the agent flexible planning rather than rote sequence memorization.

Failure trajectories are critical in agent datasets. Real agents will take actions that do not produce the expected outcome. They will encounter states they did not anticipate. Your dataset must include examples of the agent attempting an action, observing that it failed or produced an unexpected result, and adapting by trying an alternative approach. If your dataset contains only successful trajectories, your agent will not know how to recover from failure. It will continue executing a plan even when early steps have clearly failed.

State representation consistency is essential. Every trajectory in your dataset must represent state in the same format with the same level of detail. If some trajectories include detailed state information and others include minimal state, your agent cannot learn which state features matter for decision-making. The state representation in your dataset must match the state representation your agent will observe in production. If your production environment provides noisy or incomplete state information, your dataset must reflect that. Training on perfect state information and deploying to noisy state information guarantees failure.

Your agent dataset must cover the action space comprehensively. If your agent can take twenty possible actions, your dataset must include examples of all twenty actions in appropriate contexts. Rare actions are still necessary actions. If your dataset never shows the agent using a particular tool or taking a particular action, your agent will not learn when that action is appropriate. You need coverage across the full action space, weighted by real-world action frequency.

Goal decomposition is a sophisticated capability that requires specific dataset support. A user gives the agent a high-level goal: "prepare the quarterly financial report." The agent must decompose this into subgoals: retrieve the data, format it according to templates, verify calculations, generate charts, write the summary. Your dataset must include examples of goal decomposition showing how complex goals are broken down into manageable subgoals, how subgoals are sequenced, and how completion of subgoals is verified before proceeding.

Partial observability is common in real agent environments. The agent cannot see the full state. It must take actions to gather information before it can decide on the next action. Your dataset must include examples of information-gathering actions: the agent queries a database to check current status, calls an API to retrieve additional details, or asks the user for clarification. These are not actions toward the goal. They are actions to reduce uncertainty about the state. If your dataset assumes full observability, your agent will make decisions based on incomplete information without recognizing that gathering more information is an option.

Action dependencies and preconditions must be represented in agent datasets. Some actions can only be taken after other actions have completed. Some actions require specific state conditions to be met. Your dataset must show examples where the agent verifies preconditions before attempting an action, and adapts when preconditions are not met. An agent that does not understand action dependencies will attempt impossible action sequences.

## The Classifier Archetype: Labeled Examples at Decision Boundaries

Classifiers map inputs to discrete categories. Your dataset must provide labeled examples that define the decision boundaries between categories. A classifier dataset is a collection of examples with ground truth labels, but not all examples are equally valuable. The examples near decision boundaries—where categories are similar and distinctions are subtle—are far more valuable than examples in the center of a category where classification is obvious.

Most teams building classifiers collect examples uniformly across categories, aiming for balanced class distribution. This is correct for preventing class imbalance, but it is insufficient for defining decision boundaries. You need deliberate oversampling of boundary cases: examples that are hard to classify, examples that could plausibly belong to multiple categories, examples that represent edge cases. These are the examples that teach your classifier where one category ends and another begins.

Your classifier dataset must include examples of ambiguous cases with clear resolution explanations. Some inputs genuinely sit on the boundary between categories. Your dataset must show how these should be classified and why. If the classification depends on a subtle feature, your labeling must highlight that feature. If the classification depends on policy interpretation, your labeling must reflect that policy. Boundary examples without explanations leave your classifier guessing about decision criteria.

The input diversity in your classifier dataset determines generalization. If you are classifying text, your dataset must include examples with different writing styles, different lengths, different formality levels, different languages or dialects. If you are classifying images, your dataset must include examples with different lighting, different angles, different backgrounds, different resolutions. Classifiers learn from the variation they see. Limited input diversity produces brittle classifiers that fail on inputs that differ from the training distribution.

Your classifier dataset must evolve as the category definitions evolve. In content moderation, policy definitions change. What was acceptable last year may violate policy this year. Your dataset must be updated to reflect current category definitions. Outdated examples with outdated labels will degrade classifier performance. You need a dataset maintenance process that reviews examples when policies change and relabels or removes examples that no longer reflect current definitions.

Classifier datasets must include examples of the none-of-the-above category if your production system will encounter inputs that do not fit any defined category. If your classifier only sees positive examples of defined categories, it will force every input into one of those categories even when none apply. You need examples of out-of-scope inputs labeled as out-of-scope. This teaches the classifier to recognize when it should decline to classify rather than guessing.

Hierarchical classification introduces additional dataset complexity. Categories may have subcategories. An email might be classified first as "customer inquiry" and then as "billing question" within that category. Your dataset must include examples with both the top-level and sub-level labels. It must show how examples at the category boundaries are handled at each level of the hierarchy. If a top-level classification is uncertain, should the system attempt subcategory classification or stop at the top level?

Confidence calibration is important for production classifiers but often neglected in dataset design. Your dataset should include examples across the confidence spectrum: clear cases where the classifier should be highly confident, ambiguous cases where the classifier should express uncertainty, and borderline cases where the classifier should flag for human review. If your dataset contains only clear cases, your classifier will be overconfident even on ambiguous inputs.

## The Code Generation Archetype: Input-Output with Correctness Verification

Code generation systems produce executable code from natural language descriptions. Your dataset must include input specifications, correct code outputs, and verification that the code actually works. A code generation dataset is not just input-output pairs. It is input-output-verification triplets where verification proves correctness.

Most teams building code generation systems collect examples of code with natural language descriptions, treating this as sufficient. It is not sufficient because it does not verify that the code is correct. The description may be wrong. The code may have subtle bugs. The code may work for some inputs but not others. Your dataset must include test cases that verify correctness, and your evaluation must run those test cases. Without verification, you are training on potentially incorrect examples.

Your code generation dataset must cover diverse specification styles. Developers describe what they want in many different ways. Some provide detailed step-by-step specifications. Some provide high-level goals with implicit implementation details. Some provide examples of desired behavior. Some reference existing code patterns. Your dataset must include all these specification styles for similar coding tasks. If your dataset only includes detailed specifications, your system will struggle with high-level requests.

The code complexity distribution in your dataset must match production usage. If your real users need code ranging from simple one-liners to complex multi-function implementations, your dataset must cover that range. Training exclusively on simple examples produces a system that fails on complex requests. Training exclusively on complex examples produces a system that over-engineers simple requests. You need representative complexity distribution.

Your code generation dataset must include examples with constraints: performance requirements, library restrictions, style guidelines, security requirements. Real code generation happens in constrained contexts. Your dataset must show how constraints shape the generated code. If every example in your dataset is unconstrained blue-sky code generation, your system will not learn to respect real-world constraints.

Error patterns matter in code generation datasets. Your dataset should include examples of incorrect code with explanations of why it is incorrect and what the correct version looks like. This teaches the system common error patterns to avoid. It also provides contrast examples that sharpen decision boundaries between correct and incorrect implementations. A dataset of only correct code does not teach what incorrect looks like.

Incremental code generation is a common pattern that requires dataset support. A developer has existing code and wants to add a feature or fix a bug. Your dataset must include examples showing the existing code context, the requested change, and the modified code. This is different from generating code from scratch. The system must understand existing code structure, respect existing patterns and conventions, and integrate the change smoothly.

Documentation and comments are part of code generation quality but often omitted from datasets. Your dataset should include examples where generated code includes appropriate comments explaining complex logic, docstrings for functions, and inline documentation for non-obvious decisions. If your dataset contains only uncommented code, your system will generate uncommented code.

## The Content Moderation Archetype: Threshold Examples with Policy Grounding

Content moderation systems classify content as acceptable or violating along multiple policy dimensions. Your dataset must include examples at and near policy thresholds with explicit grounding in policy language. A moderation dataset is not just violating and non-violating examples. It is examples that show where the line is and why, tied to specific policy clauses.

Most teams building moderation systems collect clear violations and clear non-violations, treating the boundary as self-evident. The boundary is never self-evident. The hardest and most important moderation decisions happen at the threshold. Your dataset must deliberately oversample threshold cases: content that is almost but not quite a violation, content that violates in one context but not another, content that reasonable people might disagree about. These threshold examples define your moderation policy in practice.

Your moderation dataset must include context. The same content may be violating or acceptable depending on context. A medical discussion uses terminology that would be inappropriate in other contexts. A quote from a news article may contain language that would violate policy if posted originally. Your dataset must include examples showing how context changes classification, with explicit labeling of the contextual factors that determine the decision.

Policy justification must be explicit in your moderation dataset. Each labeled example should reference the specific policy clause that determines the label. This grounds your moderation system in policy language rather than in implicit pattern matching. It also makes your dataset auditable. When a policy changes, you can identify all examples justified by that policy clause and review them for relabeling.

Your moderation dataset must reflect policy evolution. Content policies change frequently in response to new abuse patterns, regulatory requirements, and platform decisions. What was acceptable in 2024 may violate policy in 2026. Your dataset must be versioned with timestamps, and you must have a process for reviewing and updating labels when policies change. A moderation dataset is never finished. It requires ongoing curation.

The severity distribution in your moderation dataset matters. Not all violations are equal. Some are severe and require immediate action. Some are minor and may warrant a warning. Your dataset must include examples across the severity spectrum with explicit severity labels. This allows your system to not just detect violations but to triage them appropriately. If your dataset treats all violations as equally severe, your production system will too, leading to either over-enforcement or under-enforcement.

Cultural and linguistic variation is critical in moderation datasets for global platforms. Content that is acceptable in one culture may violate norms in another. Slang and idioms vary by language and region. Your dataset must include examples across the cultural and linguistic contexts you serve. If your dataset is dominated by English-language examples from one region, your moderation will perform poorly on non-English content or content from other regions.

Adversarial evasion is a constant challenge in content moderation. Users attempt to evade moderation by using misspellings, character substitutions, or coded language. Your dataset must include examples of these evasion tactics with labels indicating that they still violate policy despite the obfuscation. If your dataset contains only straightforward violations, your system will be easily defeated by simple evasion techniques.

## The Recommendation Archetype: Interaction Histories with Engagement Signals

Recommendation systems suggest items based on user history and context. Your dataset must include user interaction histories, the recommendations that were made, and the engagement signals that indicate success or failure. A recommendation dataset is not a collection of user-item preference pairs. It is a collection of sequential interaction histories showing how users respond to recommendations over time.

Most teams building recommendation systems collect binary preference data: user liked item or user did not like item. This is insufficient because it does not capture the dynamics of recommendation. Recommendations influence future preferences. User interests shift over time. Engagement depends on context, recency, and what else the user has seen recently. Your dataset must capture these temporal dynamics. You need interaction sequences, not static preferences.

Your recommendation dataset must include negative feedback signals. Users do not just engage with recommendations they like. They actively avoid, dismiss, or express disinterest in recommendations they do not want. Your dataset must capture these negative signals explicitly. If your dataset only includes positive engagement, your system cannot learn what not to recommend. You need examples of recommendations that were shown and ignored, or actively rejected.

Context features are critical in recommendation datasets. The same user may engage with different content depending on time of day, device, location, and recent activity. Your dataset must capture these context features and show how they correlate with engagement. If your dataset strips away context and treats all interactions as equivalent, your recommendations will not adapt to context in production.

Your recommendation dataset must cover cold start scenarios: new users with minimal history, new items with minimal engagement data. If your dataset only includes established users with rich histories, your system will fail when new users arrive. You need examples showing how to make reasonable initial recommendations with limited information, and how recommendations improve as more interaction data accumulates.

The item diversity in your recommendation dataset determines whether your system can recommend across the full catalog or gets stuck recommending popular items. Your dataset must include examples of users engaging with niche, long-tail items, not just popular blockbuster items. If your dataset is dominated by popular item interactions, your system will over-recommend popular items and ignore the long tail, even when the long tail would be more relevant for specific users.

Exploration versus exploitation is a fundamental tension in recommendation systems that datasets must address. Should the system recommend items it is confident the user will like, or should it explore new item types to discover new preferences? Your dataset must include examples of both strategies and their outcomes. Pure exploitation leads to filter bubbles. Pure exploration leads to poor user experience. The dataset must show when to exploit and when to explore.

Session-based patterns matter in many recommendation contexts. Users engage in sessions where they browse, select, and consume items in sequence. A user watching videos may binge-watch a series, then switch to a different genre, then take a break and return hours later with different intent. Your dataset must capture these session patterns, showing how recommendations should adapt within a session and across sessions.

## Archetype Mismatch: The Most Common Dataset Strategy Failure

The most expensive dataset failures happen when teams apply one archetype's dataset strategy to a different archetype's product. A team building a conversational agent collects classification examples. A team building a recommendation system collects static preference pairs. A team building a RAG system collects question-answer pairs without retrieval context. These mismatches guarantee failure regardless of dataset quality within the wrong framework.

Archetype mismatch happens because teams copy dataset strategies from well-known examples without understanding the underlying product differences. A team sees that a famous classifier dataset worked well, so they apply the same labeling approach to their chatbot. The structural mismatch means the dataset cannot support the product. You cannot evaluate a multi-turn conversation with single-turn examples. You cannot evaluate retrieval quality without query-document pairs. You cannot evaluate an agent without state trajectories.

The fix for archetype mismatch is not incremental improvement of the dataset. It is starting over with the correct dataset structure for your product archetype. This is painful when you have already invested months and significant budget in the wrong dataset. But continuing with the wrong structure wastes even more time and money. You must correctly identify your product archetype, understand the dataset structure that archetype requires, and build that dataset. Retrofitting the wrong dataset structure into the right structure rarely works. The underlying examples are usually not reusable because they do not capture the necessary information.

Hybrid products require hybrid dataset strategies. If your product is a conversational agent that uses RAG, you need conversation trajectories that include retrieval steps. Your dataset must show how the agent retrieves documents, how it uses those documents in the conversation, and how the conversation evolves based on retrieved information. You cannot simply combine a chatbot dataset and a RAG dataset. You need an integrated dataset that captures the interaction between conversation and retrieval. Understand your product's full architecture, identify all the archetypes it combines, and design a dataset structure that supports all of them.

The architectural mismatch problem extends beyond just the dataset structure. It affects how you measure success, how you iterate on improvements, and how you diagnose failures. If you are measuring chatbot success with accuracy metrics designed for classifiers, you will optimize for the wrong thing. If you are debugging a RAG system by looking only at generation quality without examining retrieval quality, you will miss half the failure modes. Archetype alignment must extend from dataset structure through evaluation methodology to debugging practices.

Your dataset strategy must align with your product archetype, and your product archetype determines your dataset requirements. There is no universal dataset strategy. There is only the correct strategy for your specific archetype, executed with appropriate depth and coverage. Once you understand what dataset your product requires, the next question is what happens when that dataset is low quality. The cost of bad data is the subject we turn to next.
