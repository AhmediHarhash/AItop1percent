# 6.2 â€” Eval Set Design Principles: Coverage, Balance, and Difficulty Tiers

**A balanced eval set is not a representative eval set.** Most teams confuse these concepts, building evaluation datasets that distribute examples evenly across categories while ignoring every other dimension that determines production performance. Balance addresses one axis of variation. Coverage addresses all axes that matter. You can achieve perfect balance and zero coverage simultaneously, producing metrics that look strong in testing and collapse in production. The distinction is not subtle. It is the difference between measuring performance on the task you designed and measuring performance on the task your users actually give you. When your eval set balances intents but ignores multi-intent queries, emotion, non-standard language, and high-value edge cases, you are optimizing for a fiction. Production does not match that fiction, and your system fails accordingly.

This failure illustrates the central challenge of eval set design: balance and coverage are not the same thing, and optimizing for one without the other produces metrics that lie. Most teams focus exclusively on balancing their eval set to ensure every class or intent is represented equally. This is necessary but insufficient. Balance addresses one dimension of variation. Coverage addresses all dimensions that matter. A well-designed eval set must achieve both, and must explicitly tier difficulty so you understand performance across the capability spectrum. These design principles determine whether your eval set surfaces the failures that matter or hides them behind aggregate metrics.

## Coverage Mapping Across Input Dimensions

Coverage mapping is the systematic process of identifying all dimensions of variation in your input space and ensuring your eval set includes representative examples across each dimension. This starts with the obvious dimensions. For classification tasks, you need coverage across all classes. For extraction tasks, you need coverage across all entity types or fields. For generation tasks, you need coverage across output formats and styles. But these obvious dimensions are just the beginning. Real input spaces vary along dozens of dimensions simultaneously, and your eval set must capture this multi-dimensional variation.

The first step is enumerating dimensions. For a customer support intent classifier, the dimensions might include primary intent, secondary intent if present, customer sentiment, input length, grammatical correctness, use of domain jargon, request complexity, and temporal context. For a document extraction system, dimensions might include document type, document format, field presence, field format variation, OCR quality, layout complexity, and multi-page structure. For a summarization system, dimensions might include input length, topic, structure, technical density, and desired summary style. You identify these dimensions through analysis of production data if available, through interviews with domain experts and users if building something new, and through systematic thinking about what could vary in your input space.

Once you have enumerated dimensions, you map the range of variation along each dimension. For discrete dimensions like intent or document type, this means listing all possible values. For continuous dimensions like input length or technical density, this means defining meaningful buckets. For sentiment, you might define negative, neutral, and positive. For input length, you might define short under 50 tokens, medium 50-200 tokens, and long over 200 tokens. The goal is to discretize your input space into a manageable set of cells that cover the full range of variation.

Then you perform coverage analysis. You create a multi-dimensional grid representing all combinations of dimension values and ask whether your eval set includes examples in each cell. This does not mean you need examples in every possible combination. A twelve-dimensional space with five values per dimension has nearly 250 million cells, and most of those combinations will never appear in production. But you need examples in all combinations that are reasonably likely and in all combinations where failure would be costly. If angry customers asking billing questions represent 8% of production traffic, you need that combination in your eval set. If technical documentation with poor OCR quality appears in 3% of inputs but those inputs come from your highest-value enterprise customers, you need that combination.

Coverage mapping also means identifying and including edge cases. These are inputs that are rare but important, either because they represent high-stakes scenarios or because they expose systemic weaknesses. For the customer support classifier, edge cases might include customers threatening legal action, customers asking about discontinued products, or customers mixing languages. For document extraction, edge cases might include hand-written annotations on printed forms, documents with redacted sections, or documents with embedded tables spanning multiple pages. You cannot enumerate all possible edge cases, but you can identify the categories that matter through analysis of production failures, conversations with domain experts, and adversarial thinking about what could go wrong.

The output of coverage mapping is a structured inventory of your input space with explicit targets for how many examples you need in each cell. This inventory guides both sourcing and curation. When sampling from production logs, you stratify by these dimensions to ensure coverage. When generating synthetic examples, you explicitly target under-covered cells. When curating your final eval set, you verify that you have met your coverage targets and that no critical cells are empty.

## Balanced Representation Versus Production Distribution

Once you have coverage, you must decide how to balance your eval set across the dimensions you have identified. The naive approach is to mirror production distribution. If 60% of production queries are intent A and 5% are intent B, your eval set should have the same ratio. This approach seems principled, but it produces eval sets that hide minority-class failures and waste statistical power on over-represented cases.

The problem with mirroring production distribution is that your eval set becomes dominated by the most common cases, and you have insufficient examples of rare but important cases to measure performance reliably. If intent B appears in 5% of production and you have a 1,000-example eval set, you have 50 examples of intent B. That is barely enough to measure baseline accuracy, and certainly not enough to detect a three-point regression with confidence. Meanwhile, you have 600 examples of intent A, which is far more than you need to measure performance on that intent. You have wasted statistical power on over-sampled cases and have insufficient power for under-sampled cases.

The better approach is **stratified balanced representation**. You ensure sufficient examples of each important category to measure performance reliably, even if that means over-representing rare categories relative to production. For a classification task with twelve intents, you might allocate 60-100 examples per intent rather than mirroring production distribution. This gives you enough statistical power to detect meaningful differences within each intent. For a document extraction task with eight document types, you might ensure 80 examples of each type even if some types are rare in production. The goal is to make your eval set a good measurement instrument, not a perfect mirror of production.

Stratified balancing applies to all dimensions, not just the primary task dimension. If 12% of production inputs contain typos, you might include typos in 15-20% of eval examples to ensure you have sufficient coverage. If 8% of inputs are multi-intent, you might include multi-intent examples in 10-15% of your eval set. You are not trying to match production exactly. You are trying to ensure you can measure performance on each important slice with adequate confidence.

This approach does mean your overall eval accuracy will not directly predict production accuracy. If you have balanced your eval set but production is skewed, your eval accuracy will weigh minority classes more heavily than production does. This is correct. You want to know how well your system handles each class, not just aggregate performance weighted by production frequency. You can always compute production-weighted accuracy by applying production weights to your per-class metrics. But you cannot compute per-class metrics if you do not have enough examples of each class.

The exception to balanced representation is when some categories truly do not matter. If a particular intent appears in 0.3% of production traffic, costs you nothing if you get it wrong, and is not growing or strategically important, you might exclude it from your eval set entirely or include only a handful of examples. The key is making this decision explicitly based on product requirements, not implicitly by mirroring production frequency.

## Difficulty Tiering for Interpretable Performance

Aggregate accuracy on your eval set is a single number that compresses a complex performance profile into a scalar. This compression is useful for tracking trends over time, but it hides critical information. A system that gets 85% accuracy by nailing all the easy cases and failing all the hard cases is very different from a system that gets 85% accuracy by performing adequately across the difficulty spectrum. Difficulty tiering makes this difference visible.

Difficulty tiering means explicitly labeling each example in your eval set as easy, medium, or hard, and reporting performance on each tier separately. This gives you a three-number performance summary instead of one. You might see 97% accuracy on easy cases, 84% on medium cases, and 62% on hard cases. That profile tells you something useful. Your system has strong baseline capability but struggles with complex scenarios. If you improve your prompt and see easy rise to 98%, medium rise to 86%, and hard rise to 74%, you know the improvement was concentrated in hard cases. If you switch models and see easy stay at 97%, medium rise to 91%, but hard drop to 58%, you know the new model trades hard-case performance for medium-case improvement.

The challenge is defining difficulty tiers in a principled way. Difficulty is not a property of the input alone. It is a property of the input relative to your task and your system's capabilities. An input that is easy for a state-of-the-art model might be hard for a weaker baseline. An input that is easy for humans might be hard for models. You need a definition of difficulty that is stable, interpretable, and aligned with your product requirements.

One approach is **expert-based difficulty rating**. Domain experts review each example and label it as easy, medium, or hard based on their judgment of how difficult it would be for a competent system to handle. This approach grounds difficulty in human intuition and domain knowledge. An easy example is one that any reasonable system should get right with minimal sophistication. A hard example is one that requires deep understanding, multi-step reasoning, or handling of ambiguity. This approach works well when you have access to domain experts and when difficulty is relatively clear-cut. It breaks down when experts disagree substantially or when difficulty is highly subjective.

Another approach is **performance-based difficulty calibration**. You run several baseline systems on your eval set and define difficulty based on their performance. Examples that all baselines get right are easy. Examples that all baselines get wrong are hard. Examples where baselines disagree are medium. This approach grounds difficulty in empirical system behavior rather than human judgment. It ensures your difficulty tiers are meaningful relative to actual system capabilities. The downside is that difficulty ratings depend on your choice of baselines and can shift as model capabilities improve.

A third approach is **feature-based difficulty prediction**. You define features that correlate with difficulty, such as input length, syntactic complexity, presence of negation or ambiguity, number of entities or fields to extract, or domain-specific complexity signals. You score each example on these features and assign difficulty tiers based on the scores. This approach is systematic and reproducible, but it requires identifying the right features and validating that they actually predict difficulty.

In practice, the best approach is often a hybrid. You start with expert-based ratings for a subset of examples. You validate those ratings against baseline system performance. You identify features that correlate with expert ratings and system performance. You use those features to assign preliminary difficulty ratings to all examples. You have experts review edge cases and refine ratings. You document the process and the rationale for each tier. The goal is not perfect difficulty calibration. The goal is a stable, interpretable tiering that helps you understand your system's performance profile.

Once you have difficulty tiers, you use them in two ways. First, you report performance by tier in all evaluations. Your standard eval output includes overall accuracy, accuracy on easy cases, accuracy on medium cases, and accuracy on hard cases. This makes performance profiles visible and enables meaningful comparison. Second, you use tiers to guide iteration. If your system is at 94% on easy cases, there is not much room for improvement there. You focus on medium and hard cases. If your system is at 68% on hard cases and your product requirements say hard cases must be handled, you know where to invest.

## How Design Choices Affect What Eval Can Tell You

Every design choice you make in constructing your eval set constrains what the eval can and cannot tell you. These constraints are not bugs. They are fundamental properties of measurement. Understanding them helps you design eval sets that answer the questions you actually need answered.

If you balance your eval set across classes but production is highly skewed, your eval can tell you per-class performance but cannot directly predict production accuracy. You need to apply production weights to get that prediction. If you over-represent hard cases to stress-test your system, your overall eval accuracy will be lower than production accuracy, and you need to communicate that clearly to stakeholders who might otherwise panic about low scores.

If you define difficulty tiers based on expert judgment, your eval can tell you how well your system handles cases experts think are hard, but it cannot tell you how well your system handles cases users find hard unless those two notions of difficulty align. If you define difficulty based on baseline performance, your eval can tell you how your system compares to those baselines, but the tiers will shift as baselines improve.

If you include only examples from production logs, your eval can tell you how well your system handles historical inputs but cannot tell you how it will handle future distribution shift unless your sampling includes representative examples of likely shifts. If you include synthetic examples to fill coverage gaps, your eval can tell you how the system handles those synthetic cases but cannot guarantee the synthetic examples are realistic unless you validate them carefully.

If you use stratified sampling to ensure coverage, your eval can tell you performance across all important slices, but you need enough examples per slice to make those measurements reliable. If you use random sampling to mirror production, your eval can predict aggregate production performance but may lack statistical power for minority slices.

These trade-offs mean you often need multiple eval sets, each optimized for different questions. You might have a **primary eval set** that is stratified and balanced to measure per-slice performance with high confidence. You might have a **production-weighted eval set** that mirrors production distribution to predict aggregate accuracy. You might have a **stress test eval set** that over-represents hard cases and edge cases to find failure modes. You might have a **regression test set** that is small, fast, and stable to catch obvious breaks during development. Each eval set answers different questions, and you use them together to get a complete picture.

The key is making these choices explicitly and documenting them clearly. When you report eval metrics, you explain what the eval set was designed to measure, what it does not measure, and how to interpret the numbers. When stakeholders ask why eval accuracy is lower than production accuracy, you explain that the eval set is designed to stress-test the system and over-represents hard cases. When engineers ask why a three-point improvement on the eval set translated to a one-point improvement in production, you explain that the eval set is stratified and production is skewed. Transparency about design choices builds trust in your metrics and prevents misinterpretation.

## Coverage Validation and Maintenance

Building an eval set with good coverage and balance is not a one-time task. Coverage degrades over time as your product evolves, your users change behavior, and new edge cases emerge. Maintaining coverage requires ongoing validation and updates.

Coverage validation means regularly comparing your eval set to production data along all the dimensions you have identified. You measure the distribution of production inputs across intents, sentiment, length, complexity, and other dimensions, and you compare that to your eval distribution. You look for dimensions where production has shifted significantly or where new values have emerged. If a new intent appears in production, you need examples of it in your eval set. If the distribution of input lengths has shifted, you may need to rebalance. If a new language or locale becomes significant, you need coverage.

This validation should happen on a regular cadence, typically monthly or quarterly depending on how fast your product and user base evolve. You automate as much as possible. You have dashboards that show production distributions across key dimensions and highlight deltas from your eval set. You have alerts that fire when new intents, entity types, or edge cases appear in production above a certain frequency threshold. You have a backlog of eval set updates that are reviewed and prioritized regularly.

Updating your eval set requires the same rigor as the initial build. You source new examples using the same stratified sampling approach. You annotate them using the same guidelines and quality process. You version the updated eval set and document what changed and why. You recompute baseline metrics on the new eval set so you can track performance over time despite the eval set evolving. This versioning is critical. If you continuously modify your eval set without tracking changes, you lose the ability to make longitudinal comparisons.

There is tension between stability and coverage. You want your eval set to be stable so you can track performance over time, but you also want it to evolve to maintain coverage as production changes. The resolution is **controlled evolution**. You do not tweak your eval set weekly. You make deliberate updates on a regular cadence, you version each update, and you maintain historical versions for longitudinal comparison. When you update your eval set, you run all your recent model checkpoints and prompt versions on both the old and new eval sets so you can understand how the update affected metrics. This allows you to continue tracking performance trends even as the eval set evolves.

Coverage maintenance also means retiring examples that are no longer relevant. If a particular intent is deprecated, examples of that intent should be removed from your eval set. If a document format is no longer accepted, examples in that format should be retired. If an edge case was fixed at the product level so the system will never see it again, those examples can be removed. Retiring examples requires the same discipline as adding them. You document why each example was retired, you version the change, and you recompute baselines.

## Balancing Eval Set Size and Engineering Velocity

Larger eval sets provide more statistical power and better coverage, but they are slower to run, more expensive to annotate, and harder to maintain. Smaller eval sets are fast and cheap, but they provide less signal and can miss important edge cases. The right size is the smallest eval set that gives you the confidence you need to make decisions.

For most tasks, this means a primary eval set of 500-2000 examples. This is large enough to detect meaningful performance differences, large enough to ensure coverage across important dimensions, and small enough to run in seconds or minutes rather than hours. We will cover the statistical foundations of sizing in detail in 6.3, but the intuition is straightforward. If you need to detect a three-percentage-point accuracy difference with high confidence, you need several hundred examples. If you need to measure performance on a dozen slices with confidence, you need enough examples per slice, which typically means 800-1500 total examples for reasonable balance.

You may also maintain smaller eval sets for faster iteration. A 100-example smoke test eval set can catch obvious breaks in seconds. A 300-example regression test set can validate that a change did not break existing functionality. These smaller sets are not sufficient for measuring absolute performance or detecting small improvements, but they are sufficient for fast feedback during development.

Conversely, you may maintain larger eval sets for high-stakes measurement. If you are comparing two models and the decision will cost several hundred thousand dollars in inference or licensing, you might use a 3000-5000 example eval set to ensure the performance difference you observe is real. If you are preparing for a launch and need high confidence in your metrics, you might use a larger set. The cost of running a large eval set is small compared to the cost of making the wrong decision.

The key is tailoring eval set size to the decision being made. Fast iteration during development uses small sets. Measuring releases or comparing major alternatives uses larger sets. Routine monitoring uses medium sets. You do not need one eval set that serves all purposes. You need a portfolio of eval sets, each optimized for its use case.

In 6.3, we will examine the statistical foundations that determine how large your eval set needs to be to detect meaningful performance differences and provide reliable confidence in your metrics.
