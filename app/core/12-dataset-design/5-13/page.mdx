# 5.13 â€” Late-Arriving Data and Poison Message Handling

In June 2025, a logistics AI company lost $920,000 in a single week because their dataset pipeline could not handle late-arriving data. The company built a real-time model that predicted delivery times based on streaming event data from trucks, warehouses, and customer orders. Their pipeline processed events in hourly windows, aggregated them into features, and published feature datasets that downstream model serving systems consumed. The pipeline closed each hourly window exactly 60 minutes after the window start time, processed all events that had arrived by then, and moved on to the next window. This worked well when network conditions were normal. But during a regional internet outage affecting one of their major fulfillment centers, event data from 140 trucks was delayed by three to seven hours. When the network recovered, those delayed events arrived and were routed to the pipeline. The pipeline rejected them because their timestamps fell into windows that had already closed. The events were dropped silently. The feature dataset for those hours was missing data from 140 trucks. The model trained on incomplete features and learned incorrect patterns. Delivery time predictions became wildly inaccurate. Customer complaints spiked. The operations team had to manually reroute shipments to compensate for bad predictions, costing significant overtime and expedited shipping fees. The company did not discover the root cause for three days because their monitoring only tracked pipeline failures, not missing data. When they finally identified the issue, they had no mechanism to reprocess the late-arriving events. The events had been dropped, not queued. They were gone. The team spent another four days rebuilding partial feature data from backup logs and retraining models. The root cause was not the internet outage. Outages happen. The root cause was that the pipeline had no strategy for handling late-arriving data. It assumed all data would arrive on time, which is never true in distributed systems. It had no grace periods, no watermarks, no late-data queues, and no reprocessing paths. It optimized for the happy path and failed catastrophically when reality diverged.

Late-arriving data is not an edge case. It is the default behavior of distributed systems. The question is not whether data will arrive late. The question is how you handle it.

## Understanding Late Data in Streaming and Batch Pipelines

Data lateness is the time difference between when an event occurred and when your pipeline processes it. An event generated at 2:00 PM that arrives at your pipeline at 2:05 PM has five minutes of lateness. An event generated at 2:00 PM that arrives at 5:00 PM due to a network partition has three hours of lateness. Lateness is unavoidable because distributed systems experience network delays, clock skew, buffering at intermediate hops, retries after transient failures, and batching for efficiency.

Streaming pipelines process data in time windows. A common pattern is to aggregate events into one-minute, five-minute, or hourly windows based on event timestamps. The pipeline must decide when to close a window and emit results. If you close the window as soon as the window end time passes, you will miss late-arriving events. If you wait indefinitely for late events, you never emit results. The trade-off is latency versus completeness. Lower latency means closing windows quickly but accepting that some late data will be missed. Higher completeness means waiting longer for late data but delaying results.

Batch pipelines process data in discrete partitions. A daily batch pipeline processes all events that occurred on a specific date. But events do not always arrive in the partition corresponding to their timestamp. An event that occurred on 2025-11-15 might not reach your storage system until 2025-11-16 due to delayed uploads, slow source systems, or processing backlogs. If your pipeline runs at midnight on 2025-11-16 to process 2025-11-15 data, it misses events that arrive after midnight but belong to the previous day. You need a grace period: run the 2025-11-15 pipeline at 2:00 AM on 2025-11-16, giving events two hours to arrive. This reduces late-data loss but does not eliminate it.

The severity of late data depends on the use case. For a real-time fraud detection system where decisions must be made within milliseconds, data that arrives 10 seconds late is useless. For a model training dataset where you are aggregating weeks of data, events that arrive a few hours late are still valuable. The grace period and watermarking strategy must match the latency tolerance of downstream consumers.

Clock skew compounds lateness issues. If event producers have unsynchronized clocks, their timestamps may be incorrect. An event generated at 2:00 PM according to the producer's clock might actually have been generated at 1:58 PM or 2:02 PM according to a reference clock. If your pipeline partitions data by event timestamp, clock skew causes events to land in the wrong partitions. You mitigate clock skew by using NTP to synchronize producer clocks, by validating that event timestamps are within a reasonable range of the ingestion timestamp, and by treating event timestamps as estimates rather than ground truth.

## Watermarks and Grace Periods

A watermark is a timestamp that represents the pipeline's belief about how complete the data is up to that point. If the watermark is at 2:00 PM, the pipeline believes it has received most or all events with timestamps before 2:00 PM. Events with timestamps before the watermark that arrive after the watermark advances are considered late. The watermark is not a guarantee. It is a heuristic. Some late events will always slip through.

Watermarks advance based on observed data. In a streaming pipeline reading from Kafka, the watermark advances to the minimum timestamp observed across all active partitions. If you are reading from ten partitions and nine of them have events at 2:05 PM but one partition is lagging at 2:03 PM, the watermark is at 2:03 PM. The watermark does not advance beyond the slowest partition because doing so would cause you to close windows prematurely and drop data from the lagging partition. This is safe but conservative. If one partition is stuck due to a bug or infrastructure issue, the watermark stalls and your entire pipeline stops emitting results.

You add bounded delays to watermarks to account for expected lateness. Instead of setting the watermark to the minimum observed timestamp, you set it to the minimum observed timestamp minus a grace period. If your grace period is five minutes, and the minimum observed timestamp is 2:05 PM, your watermark is at 2:00 PM. This means you keep windows open for five minutes after their nominal end time, allowing late events to be included. The grace period is tuned based on observed lateness in production. If 95 percent of events arrive within one minute and 99 percent arrive within five minutes, a five-minute grace period captures most late data while keeping latency acceptable.

Watermarks trigger window closures. When the watermark advances past the end of a window, the pipeline closes that window, computes the aggregate, and emits the result. For an hourly window from 2:00 PM to 3:00 PM with a five-minute grace period, the window closes when the watermark reaches 3:05 PM. At that point, the pipeline finalizes the aggregation for the 2:00-3:00 window and writes it to the output dataset. Events with timestamps in the 2:00-3:00 range that arrive after 3:05 PM are considered late and handled separately.

Fixed delays are simpler than data-driven watermarks. Instead of computing the watermark based on observed timestamps, you use a fixed delay relative to processing time. If your fixed delay is 10 minutes, the watermark is always 10 minutes behind the current wall clock time. This is easier to reason about and does not stall if a partition stops producing data. It works well when lateness is predictable and bounded. It works poorly when lateness is variable or when your pipeline processes historical data where wall clock time is irrelevant.

Watermarks are not perfect. They trade precision for simplicity. A watermark that is too aggressive closes windows early and drops late data. A watermark that is too conservative keeps windows open indefinitely and delays results. You tune the watermark based on the acceptable trade-off between latency and completeness for your use case. You monitor the actual lateness distribution in production and adjust the grace period if the distribution changes.

## Handling Late Data After Windows Close

Late data that arrives after the watermark has passed must be handled explicitly. You have four options: drop it, route it to a late-data queue, reprocess the affected window, or emit corrections.

Dropping late data is the simplest option. Once the window closes, you ignore any events that arrive late. This is acceptable for use cases where completeness is less important than latency and where the volume of late data is negligible. You monitor how much data you are dropping to ensure the assumption remains valid. If you discover you are dropping 5 percent of events, dropping is no longer acceptable and you need a different strategy.

Late-data queues preserve late events for reprocessing. When an event arrives after its window has closed, the pipeline does not drop it. Instead, it writes it to a separate output path or topic labeled as late data. A separate reprocessing job runs periodically, reads the late-data queue, groups late events by the windows they belong to, and reprocesses those windows to incorporate the late data. The reprocessed windows are published to a versioned output path. Consumers can choose whether to use the initial timely results or wait for the reprocessed results that include late data. This approach maximizes completeness at the cost of complexity and latency.

Window reprocessing emits updated results when late data arrives. Instead of maintaining a separate late-data queue, the pipeline keeps windows open in a special state after the initial result is emitted. When late data arrives, the pipeline updates the affected window's aggregate and emits a new result with a revision number or updated timestamp. Downstream consumers receive multiple versions of the same window and must handle updates. This works well with streaming systems like Kafka where consumers naturally process updates in order. It works poorly with batch systems where consumers expect immutable outputs.

Corrections are explicit compensating records. Instead of reprocessing the entire window, the pipeline emits a correction record that adjusts the aggregate. If the initial window result said there were 1,000 events and 50 late events arrive, the pipeline emits a correction record saying to add 50 to the count. Consumers apply corrections to their local state. This minimizes reprocessing cost but requires that consumers implement correction logic, which not all systems support.

The choice depends on your architecture and requirements. If you are using a streaming platform with native support for updates like Apache Flink or Kafka Streams, window reprocessing is natural. If you are using a batch pipeline writing to immutable storage, late-data queues are more practical. If your downstream consumers are simple dashboards that cannot handle updates, dropping late data may be the only feasible option. The key is to choose deliberately and document the behavior so consumers understand what to expect.

## Poison Messages and Data Validation

A poison message is a record that causes your pipeline to crash, hang, or produce incorrect results. Poison messages arise from bugs in upstream producers, data corruption during transmission, schema violations, malformed encodings, or malicious input. A single poison message can halt your entire pipeline if not handled correctly.

Common poison message patterns include schema violations where a required field is missing, type mismatches where a field expected to be numeric contains a string, encoding errors where a UTF-8 string contains invalid byte sequences, malformed JSON or Avro where the structure does not parse, excessively large payloads that exceed memory limits, null or empty records where the pipeline expected data, and adversarial input designed to exploit bugs in parsing or processing logic.

Your pipeline must validate every record before processing. Validation checks that the record conforms to the expected schema, that required fields are present and non-null, that field values are within acceptable ranges, that string fields are valid UTF-8, that numeric fields are finite and not NaN, that timestamps are reasonable and not in the distant past or future, and that the record size is within limits. Validation happens at the earliest possible stage, immediately after ingestion and before any processing logic runs. If validation fails, the record is marked as invalid and routed to a separate path. It does not enter the main processing pipeline.

Validation failures are logged and monitored. Your pipeline emits metrics counting how many records failed validation, which validation rules were violated, and which upstream sources produced invalid records. You alert if the validation failure rate exceeds a threshold, such as 1 percent of records. Validation failures are a signal of upstream data quality issues. You investigate the root cause and work with upstream teams to fix their producers. You do not silently drop invalid records without visibility.

Invalid records are written to a dead letter queue. The dead letter queue is a separate storage location or topic where invalid records are preserved for later analysis. Each record in the dead letter queue includes metadata explaining why it was rejected: which validation rule failed, the timestamp when it was rejected, the source topic or table it came from, and the full record payload. Engineers and data quality teams review the dead letter queue periodically to identify patterns, diagnose upstream issues, and determine whether validation rules need adjustment.

Partial validation allows processing to continue for valid fields. If a record has ten fields and one field is invalid, you can choose to process the nine valid fields and mark the invalid field as null or missing rather than rejecting the entire record. This is context-dependent. If the invalid field is critical to downstream logic, rejecting the record is safer. If the invalid field is optional metadata, partial processing is acceptable. You configure partial validation rules per field based on downstream requirements.

Retry logic must account for poison messages. If a record causes a crash and your pipeline automatically retries, the poison message will crash the pipeline again, creating an infinite retry loop. You implement retry budgets: each record gets a maximum of three retry attempts. If it fails three times, it is marked as poison and moved to the dead letter queue. The pipeline does not retry it again. This prevents poison messages from stalling the pipeline indefinitely.

## Dead Letter Queues and Quarantine Strategies

A dead letter queue is where records go to die, but in a controlled and observable way. It is not a failure. It is an explicit design choice that separates bad data from good data and preserves bad data for debugging. Every production pipeline must have a dead letter queue.

Dead letter queues store records that failed processing for any reason. Validation failures go to the DLQ. Records that caused exceptions during processing go to the DLQ. Records that exceeded retry limits go to the DLQ. Records that arrived too late and were dropped go to the DLQ. The DLQ is append-only. You never delete records from it without manual review. This ensures that no data is lost silently.

DLQ records include diagnostic metadata. Each record stores the original payload, the error message or exception that caused the failure, the timestamp when the failure occurred, the pipeline stage where the failure occurred, the retry count, and the source identifier such as Kafka topic and partition or S3 object path. This metadata allows engineers to diagnose issues without needing to reproduce the failure in a test environment.

You monitor DLQ volume and alert on anomalies. You track how many records are sent to the DLQ per hour or per day. You alert if the DLQ volume spikes above baseline. A sudden increase in DLQ volume indicates an upstream data quality regression, a new bug in your pipeline logic, or an infrastructure issue. You investigate immediately. You do not wait until the DLQ fills up and causes storage issues.

DLQ records are reviewed and triaged. You run a weekly review process where engineers examine the DLQ, categorize failure types, and decide what to do with each record. Some records are genuinely invalid and are discarded. Some records failed due to a transient bug that has since been fixed and should be reprocessed. Some records reveal a bug in validation logic that should be relaxed. You track triage decisions in a ticket system and implement fixes based on the analysis.

Reprocessing from the DLQ is a first-class operation. You build tooling that allows you to extract records from the DLQ, optionally fix them manually or with automated scripts, and resubmit them to the pipeline. The reprocessing tool tracks which DLQ records have been reprocessed to avoid double-processing. This allows you to recover from incidents where valid data was incorrectly rejected due to a bug.

Quarantine is a temporary holding area distinct from the DLQ. When you suspect a data quality issue but are not certain, you quarantine records rather than processing them or rejecting them. Quarantined records are stored separately and reviewed manually before deciding whether to process, reject, or fix them. This is common when deploying new validation rules. You quarantine records that fail the new rule for 24 hours, review a sample, verify the rule is correct, and then either process the quarantined records or move them to the DLQ.

## Reprocessing Late Data and Backfilling Gaps

When late data arrives after a window or partition has closed, you must decide whether to reprocess. Reprocessing late data is expensive. It requires recomputing aggregates, rewriting outputs, and potentially invalidating downstream consumers who already processed the initial results. You only reprocess when the value of including late data exceeds the cost of reprocessing.

You define reprocessing policies per dataset. For a training dataset where model quality improves with more complete data, you reprocess windows when late data volume exceeds 1 percent of the window's record count. For a real-time monitoring dataset where results are consumed immediately and never revisited, you do not reprocess. For a compliance dataset where completeness is legally required, you reprocess every late record regardless of volume. The policy is explicit and documented.

Reprocessing is triggered automatically based on late-data volume. Your pipeline tracks how many late records arrive for each closed window. When the count exceeds the reprocessing threshold, the pipeline adds the window to a reprocessing queue. A separate reprocessing job runs hourly or daily, reads the queue, reprocesses the affected windows by reading the original input data plus late data from the late-data queue, and writes updated outputs to a versioned path. Downstream consumers are notified that a new version is available.

Backfilling gaps is distinct from reprocessing. A gap occurs when an entire partition or window is missing because the pipeline failed, the source data was unavailable, or the partition was accidentally deleted. You detect gaps by auditing partition completeness: you list expected partitions based on your schedule and compare them to actual partitions in storage. Missing partitions are flagged. You backfill by running the pipeline for the missing partition's date range, reading from archived source data, and writing outputs to the missing partition path.

Gap detection is automated. You run a daily audit job that checks partition completeness for all critical datasets over the last 30 days. The job emits metrics for each dataset showing how many expected partitions exist, how many are missing, and which specific partitions are missing. It alerts if gaps are detected. It files tickets for backfills. This ensures gaps do not go unnoticed for weeks until a downstream consumer complains.

Backfilling gaps requires coordination with downstream consumers. You do not silently backfill a missing partition without notifying consumers. Some consumers may have already adapted to the missing data by using a fallback or skipping the partition. If you backfill without warning, you may cause unexpected behavior. You announce backfills in advance, publish the backfill schedule, and notify consumers when backfilled partitions are available.

## Monitoring Data Completeness and Lateness

You cannot manage what you do not measure. Monitoring data completeness and lateness is critical for detecting issues before they cause downstream failures.

Lateness distributions are the first metric. You measure the difference between event timestamp and ingestion timestamp for every record. You emit this as a histogram or percentile distribution: p50 lateness, p95 lateness, p99 lateness. You track these metrics per source and per partition. You alert if p99 lateness exceeds your grace period, indicating that your watermark is closing windows too early and dropping data. You review lateness trends weekly to identify sources with increasing latency.

Completeness metrics track how much data arrived versus how much was expected. If your source system publishes a count of how many events it produced for a given time window, you compare that count to how many events your pipeline received. If you received 980 events but the source produced 1,000, you are missing 2 percent of data. You alert if completeness drops below 95 percent. You investigate why data is missing: network issues, bugs in the source producer, or bugs in your ingestion pipeline.

Watermark lag is the difference between the current wall clock time and the watermark. If the watermark is at 2:00 PM and the current time is 2:10 PM, the watermark lag is 10 minutes. You alert if watermark lag exceeds your target latency. High watermark lag indicates that your pipeline is not keeping up with the data rate, that a partition is stalled, or that your grace period is too long.

Late-data volume tracks how many records arrive after their window closes. You count late records per window and per source. You alert if late-data volume spikes. A spike indicates increased latency from a source, clock skew issues, or network problems. You correlate late-data spikes with infrastructure events to identify root causes.

Gap detection metrics track missing partitions. You count how many partitions are expected versus how many exist. You track the age of the oldest gap. You alert if gaps exist for more than 24 hours. Gaps are high-severity issues because they indicate data loss or pipeline failures.

DLQ metrics track invalid record volume. You count records sent to the DLQ per hour, categorize them by failure reason, and track the top sources producing invalid records. You alert if DLQ volume exceeds 1 percent of ingested records. DLQ spikes indicate upstream data quality regressions.

All of these metrics feed into dashboards that engineers review daily. The dashboards show completeness, lateness, watermark lag, late-data volume, gap count, and DLQ volume for all critical datasets. Anomalies are immediately visible. Trends are tracked over weeks and months. The dashboards are the operational heartbeat of your dataset infrastructure.

## Designing for Data Uncertainty and Partial Availability

The assumption that all data will arrive on time and in order is false. Distributed systems are asynchronous, unreliable, and subject to partitions. You must design pipelines that produce useful results even when data is incomplete.

Approximate aggregates tolerate missing data better than exact counts. If you are counting events and 2 percent of events arrive late, an exact count will be wrong until you reprocess. An approximate count using a probabilistic data structure like HyperLogLog will be close enough for most use cases and does not require reprocessing. You trade perfect accuracy for resilience to late data.

Windowed aggregates with revisions allow results to improve over time. You emit an initial result when the window closes based on the data available at that time. As late data arrives, you emit revised results. Consumers use the latest revision. This approach acknowledges that data completeness improves over time and builds that into the data model rather than pretending the first result is final.

Fallback strategies handle missing partitions gracefully. If downstream consumers need a dataset partition and it is missing, they fall back to the previous day's partition, use a cached version, or skip the partition and log a warning. Fallbacks prevent cascading failures where a single missing partition halts the entire downstream pipeline.

Idempotent processing allows safe retries without corruption. If your pipeline is idempotent, you can reprocess late data by rerunning the pipeline for the affected partition. The reprocessed output replaces the initial output cleanly. If your pipeline is not idempotent, reprocessing creates duplicates and corruption.

Uncertainty metadata makes incompleteness explicit. Your output dataset includes fields indicating whether the data is complete or partial, what the watermark was when the data was processed, how many late records are known to exist, and when the next revision is expected. Downstream consumers check this metadata and adjust their behavior. They may choose to wait for the complete revision before training a model or proceeding with analysis.

Designing for uncertainty is harder than assuming perfect data. It requires more engineering time, more sophisticated monitoring, and more coordination with downstream consumers. But it is the only approach that works in production. Late-arriving data, poison messages, and missing partitions are not edge cases. They are the normal operating conditions of distributed data systems. Pipelines that ignore this reality fail. Pipelines that embrace it remain reliable under real-world conditions. Next, we build on these reliability and late-data handling strategies to design dataset lineage systems that track how datasets evolve and depend on each other across the entire organization.
