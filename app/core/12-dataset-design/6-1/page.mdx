# 6.1 â€” What Makes a Good Evaluation Dataset

In mid-2025, a legal technology company launched a contract clause extraction system after six months of development. The team had built a comprehensive training pipeline, implemented careful prompt engineering, and achieved 94% accuracy on their evaluation set of 200 annotated contracts. Three weeks after launch, their enterprise customers began reporting systematic failures. The system consistently missed arbitration clauses in international agreements, misclassified force majeure provisions in construction contracts, and failed entirely on contracts written before 2020. The root cause was not the model or the prompts. The evaluation dataset contained only standard commercial agreements from 2023-2024, all drafted by three law firms, none longer than twelve pages. The system had been optimized for a narrow slice of contract types that represented less than 30% of production traffic. When the team audited their real inference logs, they discovered their evaluation set shared almost no characteristics with actual usage. They had spent six months optimizing for a test that told them nothing about production performance.

This failure reveals the foundational truth about evaluation: your eval dataset is not just a measurement tool, it is the definition of what success means. If your eval set is unrepresentative, too clean, or too easy, your metrics will be precise measurements of the wrong thing. You will ship systems that pass all tests and fail in production. Most teams treat eval dataset construction as a quick sampling task. You grab a few hundred examples, annotate them, and start measuring. This approach produces eval sets that are too small to detect real regressions, too clean to surface real failure modes, and too different from production data to predict actual performance. A good evaluation dataset is not a random sample. It is a carefully constructed artifact designed to expose the full range of behaviors your system must handle, surface the edge cases that matter most, and remain stable enough to track performance over time.

## The Properties of Good Evaluation Datasets

A good evaluation dataset has five core properties. First, it is **representative** of your production distribution across all dimensions that matter. This means not just input types and intents, but also input quality, ambiguity levels, edge case frequency, and contextual variation. If 15% of your production queries are misspelled, your eval set should reflect that. If 8% of your documents contain tables or charts that must be interpreted, your eval set must include them. If your users submit queries in seventeen languages, your eval set cannot be English-only. Representativeness is not about random sampling. It is about deliberate construction to match the statistical properties of real usage. You cannot achieve this by grabbing the first 500 examples from your logs. You achieve it by stratified sampling, targeted edge case inclusion, and continuous validation against production metrics.

Second, a good eval set is **challenging**. It should include the cases where your system is most likely to fail. This does not mean filling your eval set with impossible examples or adversarial inputs designed to break any system. It means including the boundary cases, the ambiguous inputs, the multi-step reasoning chains, and the contextual nuances that separate good systems from bad ones. If your eval set is too easy, you will see 95% accuracy and have no idea whether you are ready for production. A well-designed eval set should have a difficulty distribution that stresses your system. You want 30-40% of your examples to be straightforward cases that any reasonable approach should handle. You want 40-50% to be moderate difficulty cases that require some sophistication. You want 10-20% to be hard cases that expose real capability gaps. This distribution gives you signal across the full performance spectrum and prevents you from over-optimizing for easy cases.

Third, a good eval set is **stable**. The same input should produce the same expected output over time, and the distribution of inputs should not drift unless your product requirements change. Stability is critical because evaluation is fundamentally about comparison. You want to know whether today's prompt performs better than yesterday's, whether the new model improves over the old one, whether the latest code change introduced regressions. If your eval set changes every week, you lose the ability to make these comparisons. Stability does not mean your eval set is frozen forever. It means changes are deliberate, versioned, and documented. When you add new examples or retire old ones, you understand the impact on your baseline metrics. Most teams regenerate their eval sets constantly because they do not separate eval from exploratory analysis. Your eval set is not a notebook experiment. It is production infrastructure.

Fourth, a good eval set is **uncontaminated**. It contains no examples that were used during prompt engineering, fine-tuning, or model training. Contamination is the silent killer of evaluation. If your eval set includes examples you iterated on while tuning your prompts, your metrics will overestimate production performance. If your eval set includes examples from public datasets that were in your model's training data, your metrics will reflect memorization rather than generalization. Contamination is especially insidious because it produces metrics that look good and feel right. You see steady improvement as you iterate, you ship with confidence, and you discover in production that none of it generalizes. Preventing contamination requires discipline. You set aside your eval set before you start building. You never look at individual eval examples while tuning. You track the provenance of every example and retire any that might have been seen during development.

Fifth, a good eval set is **versioned and documented**. You know exactly what is in it, why each example was included, when it was added, and what it is meant to test. Every example has metadata explaining its source, its difficulty tier, the edge cases it covers, and any special annotation considerations. The eval set as a whole has a changelog documenting every modification. This level of documentation feels like overhead when you are moving fast, but it is non-negotiable for production systems. Three months from now, when your metrics suddenly drop by four percentage points, you need to know whether the eval set changed or your system regressed. Six months from now, when a new team member asks why a particular example is labeled a certain way, you need an answer beyond "that's what we decided in March." Documentation is not busywork. It is the foundation of reproducible evaluation.

## Common Evaluation Dataset Failures

The most common eval dataset failure is **size mismatch**. Teams use eval sets that are far too small to provide statistical confidence. A 50-example eval set might feel like enough when you are prototyping, but it cannot tell you whether a two-percentage-point accuracy difference is real signal or random noise. If your baseline accuracy is 85% on 50 examples, the 95% confidence interval is roughly plus or minus seven percentage points. You cannot distinguish an 85% system from a 90% system. You cannot detect a three-point regression. You are flying blind with precise instruments that measure nothing. For most tasks, you need at least 200-300 examples to detect meaningful differences, and often much more. We will cover the statistical foundations in detail in 6.3, but the principle is simple: if your eval set is too small, your metrics are noise.

The second common failure is **distribution mismatch**. Your eval set looks nothing like production. This happens in several ways. Sometimes your eval set is too clean. You annotate examples carefully, fix typos, remove duplicates, and standardize formatting. Then you deploy to production where 20% of inputs are malformed, 10% are duplicates, and 5% are in the wrong language. Your eval metrics said 92% accuracy. Production is 74%. Sometimes your eval set is too narrow. You focus on the happy path cases and ignore the edge cases that drive production failures. Your eval set contains only single-intent queries when 30% of production queries are multi-intent. Your eval set contains only well-formed documents when 15% of production documents are scanned PDFs with OCR errors. Sometimes your eval set is too old. You built it in early 2025 with data from 2024, and now it is late 2025 and your users have moved on. The language has shifted, the product has evolved, and your eval set measures a system that no longer exists.

The third common failure is **contamination**. Your eval set includes examples you trained on, tuned on, or iterated on during development. This produces metrics that look great and predict nothing. The most obvious contamination is including fine-tuning examples in your eval set. If you fine-tuned on 1,000 examples and 200 of them appear in your eval set, your eval metrics will be artificially high. The more subtle contamination is iterating on eval examples during prompt engineering. You look at a failing example, adjust your prompt to fix it, check the eval metrics, and repeat. Every example you looked at is now contaminated. Your prompt has been tuned to those specific inputs, and your eval metrics no longer measure generalization. The most insidious contamination is using public benchmark datasets that were in your model's training data. GPT-4 was trained on massive web scrapes that included MMLU, HellaSwag, and dozens of other public benchmarks. If you eval on those same benchmarks, you are measuring memorization, not capability.

The fourth common failure is **label quality issues**. Your eval set has inconsistent annotations, ambiguous ground truth, or outright errors. This happens because annotation is hard and most teams underinvest in it. You hire contractors, give them minimal instructions, and accept the first-pass labels without validation. Or you have domain experts annotate examples, but they disagree on 25% of cases and you never reconcile the disagreements. Or you derive ground truth from heuristics or weak signals that are themselves noisy. Label quality issues produce metrics that oscillate randomly as you fix examples, inflate confidence in systems that are actually failing, and waste engineering time debugging apparent regressions that are actually annotation errors. If your ground truth is wrong, your metrics are meaningless.

The fifth common failure is **lack of difficulty calibration**. Your eval set is all easy examples or all hard examples, and you have no way to understand where your system is strong and where it struggles. An eval set of only easy examples will show 95% accuracy for both a barely functional system and a state-of-the-art one. You will have no signal for improvement. An eval set of only hard examples will show 45% accuracy even for good systems, and you will have no sense of whether you are ready to ship. A well-calibrated eval set includes a range of difficulties so you can see performance across the spectrum. You want to know that your system gets 98% of easy cases right, 85% of medium cases, and 60% of hard cases. That distribution tells you something. A flat 82% across an uncalibrated mix tells you nothing.

## How Evaluation Quality Determines Metric Trust

The quality of your evaluation dataset directly determines whether you can trust your metrics, and therefore whether you can make good decisions. If your eval set is high quality, your metrics are reliable signals. When accuracy drops three points, you investigate because it likely represents a real regression. When a new prompt improves performance by five points, you ship it because the improvement is real. When you compare two models and one scores seven points higher, you choose it with confidence. Your metrics are connected to production outcomes. You can predict with reasonable accuracy how a change that improves eval performance by X points will affect production performance.

If your eval set is low quality, your metrics are noise. That three-point drop might be random variation, an annotation error, or a real regression, and you have no way to tell. That five-point improvement might be overfitting to your specific eval examples. That seven-point model difference might evaporate in production. You cannot make decisions based on these metrics because they do not predict anything. Teams in this situation fall into two failure modes. Either they ignore metrics entirely and ship based on vibes, or they obsess over metrics and waste weeks optimizing for noise. Both paths lead to production failures.

The difference between high-quality and low-quality eval sets compounds over time. With a high-quality eval set, every iteration teaches you something. You build institutional knowledge about what works, what doesn't, and why. You develop intuition for how eval improvements translate to production wins. You can onboard new team members by pointing them to eval results and having those results actually mean something. With a low-quality eval set, every iteration is random walk. You have no institutional knowledge because you have no reliable signal. You cannot develop intuition because the connection between eval and production is broken. New team members learn that metrics don't matter, and you lose the ability to do principled engineering.

This is why eval dataset construction is not a task you do once and forget. It is ongoing infrastructure work that requires the same rigor as your production code. You version it, you test it, you monitor it, you improve it. When your product evolves, your eval set evolves with it. When you discover new failure modes in production, you add examples to your eval set. When you find annotation errors, you fix them and document the fix. When your eval set grows too large to run efficiently, you stratify and sample intelligently rather than just grabbing random subsets. A good eval set is a living artifact that grows more valuable over time. A bad eval set is technical debt that makes every decision harder.

## The Eval Set as Product Requirement

The best way to think about your evaluation dataset is as a concrete specification of your product requirements. Your PRD says "the system should handle multi-step queries." Your eval set contains 47 multi-step queries that define exactly what that means. Your PRD says "the system should be robust to typos." Your eval set contains 83 examples with realistic typos across different severity levels. Your PRD says "the system should respect regional content policies." Your eval set contains 104 examples spanning twelve regions with ground truth for each. The eval set makes abstract requirements concrete and testable.

This perspective changes how you build eval sets. You are not trying to create a statistically perfect sample of production data. You are trying to encode your product requirements in executable form. This means your eval set is often more structured than a random sample would be. You deliberately over-represent edge cases that matter to your product. You include examples that stress specific capabilities you care about. You balance different input types to ensure coverage even if production is skewed. You add examples that test for known failure modes even if those failure modes are rare. The goal is not to mirror production distributions perfectly. The goal is to create a test that, if passed, gives you confidence your system meets its requirements.

This also means your eval set should be comprehensible to non-engineers. Product managers should be able to read through your eval examples and say "yes, these are the cases we need to handle." Domain experts should be able to validate that the examples are realistic and the ground truth is correct. Stakeholders should be able to look at per-example results and understand where the system is strong and where it struggles. If your eval set is a black box that only ML engineers understand, it is not serving its purpose as a product specification.

The other implication is that eval set construction is a cross-functional effort. Engineering cannot build a good eval set alone because you do not have complete knowledge of product requirements, user behavior, or domain constraints. Product cannot build it alone because you need to understand what is technically feasible to test and how to structure examples for reliable evaluation. Domain experts cannot build it alone because you need to balance domain accuracy with engineering practicality. The best eval sets come from collaboration. Product defines the requirements and priorities. Domain experts provide realistic examples and validate ground truth. Engineering ensures the eval set is technically sound, statistically powered, and operationally maintainable. This collaboration is not a one-time meeting. It is an ongoing partnership that evolves as the product evolves.

## Starting with Eval Set Construction

For new projects, the right time to build your evaluation dataset is before you write any prompt engineering code or train any models. This feels backward to many teams. You want to start experimenting, iterating, and building. But if you start iterating before you have a solid eval set, you will waste weeks tuning on examples that do not represent your real requirements, optimizing for metrics that do not predict production performance, and building institutional knowledge that turns out to be wrong. Starting with eval set construction forces you to answer the hard questions up front. What are we actually trying to do? What does success look like? What are the edge cases we must handle? What is the distribution of inputs we will see? What is our ground truth and how confident are we in it?

The process starts with requirements gathering. You work with product, domain experts, and stakeholders to define the full scope of what your system must handle. This is not a one-paragraph PRD. This is a detailed enumeration of input types, task variations, edge cases, constraints, and success criteria. You document the distribution of production inputs as best you can, either from historical data or from user research if you are building something new. You identify the cases that matter most, either because they are frequent or because failures are costly. You define ground truth and how it will be obtained. This work takes days or weeks, and it is the foundation of everything that follows.

Next, you source candidate examples. For existing products, you sample from production logs using stratified sampling to ensure coverage across input types, intents, and edge cases. For new products, you generate synthetic examples, pull from adjacent domains, or conduct user research to collect realistic inputs. You aim for breadth first. You want hundreds or thousands of candidate examples covering the full input space, knowing you will filter down to a smaller eval set. You also want examples across the difficulty spectrum. Easy cases that any system should handle, moderate cases that require sophistication, hard cases that push the boundaries.

Then you annotate. This is the most time-consuming and most critical step. You define clear annotation guidelines that specify exactly how to label each example, how to handle ambiguous cases, and how to document uncertainty. You have multiple annotators label the same examples to measure agreement and surface ambiguities. You reconcile disagreements through discussion, not by forcing consensus. When experts disagree, that often signals a genuinely ambiguous case, and you need to decide whether to exclude it, refine the guidelines, or accept multiple valid labels. You validate annotations with domain experts to ensure they are correct and realistic. You track annotation time and difficulty to understand label quality. This process typically requires several rounds of iteration before you have labels you trust.

Finally, you curate and version your eval set. You filter your candidate examples down to a manageable set that provides good coverage, appropriate difficulty distribution, and statistical power. You add metadata to each example documenting its source, difficulty tier, edge cases covered, and any annotation notes. You version the entire eval set with a clear identifier and changelog. You set it aside in a protected location where it cannot be contaminated by prompt engineering or fine-tuning. You write documentation explaining what the eval set covers, what it does not cover, how it was constructed, and how it should be used. This documentation is as important as the examples themselves.

Only after you have this eval set in place do you start building. Now every experiment produces meaningful signal. Every prompt iteration can be evaluated reliably. Every model comparison is grounded in real requirements. You move faster because you are not constantly questioning whether your metrics are real. You make better decisions because your metrics are connected to outcomes. You build a better system because you are optimizing for the right thing.

In 6.2, we will examine the specific design principles that govern how you construct an eval set to ensure coverage, balance, and appropriate difficulty distribution across your input space.
