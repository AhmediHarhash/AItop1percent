# 2.11 â€” Multi-Source Merging: Deduplication, Conflict Resolution, and Priority

Six weeks into deployment, the model started repeating itself. Responses showed bizarre bias toward outdated insurance verification procedures that the company had discontinued two years earlier. A routine data audit revealed the cause: 40% of the training data was duplicates and near-duplicates the merge pipeline had failed to detect. The same patient interactions appeared multiple times with conflicting field values, and the pipeline had no conflict resolution strategy. It merged everything, amplifying whichever version appeared most frequently, which happened to be the oldest, least representative records from the legacy system. Three months of remediation, two lost contracts worth $4.2 million combined, and a delayed product roadmap followed. The team had merged data sources without explicit deduplication and conflict resolution policies.

The failure was not a lack of data engineering skill. It was the absence of a deliberate strategy for multi-source merging. Most production datasets do not come from a single clean source. They are assembled from multiple systems, each with different update frequencies, quality characteristics, and coverage profiles.

When you merge these sources without explicit deduplication, conflict resolution, and priority policies, you create a dataset that reflects not the reality you want to model, but the accidents of your data infrastructure. Multi-source merging is not optional cleanup work. It is foundational dataset architecture that determines whether your training data represents truth or chaos.

## The Three Types of Duplication

Duplication appears in three forms, and you must address all three. Exact duplication occurs when the same record appears multiple times with identical content across all fields. This is the easiest to detect and the most common outcome of merging overlapping data sources.

A customer support ticket exported from both your CRM and your ticketing system will often be byte-for-byte identical except for system-generated metadata fields. Exact deduplication uses hash-based matching or full-field comparison and removes all but one copy of each record.

Fuzzy duplication occurs when the same underlying entity appears with minor variations in formatting, spelling, or field values. The same customer review might appear with different timestamps, slightly different text due to encoding issues, or variation in metadata fields while remaining substantively identical. A patient record might appear with the name formatted as "Smith, John" in one system and "John Smith" in another, with the same date of birth and medical record number.

Fuzzy deduplication requires similarity matching based on field subsets, normalized representations, or approximate string matching. You define a threshold for what constitutes "close enough" and choose one canonical version.

Semantic duplication occurs when different records describe the same underlying event, conversation, or entity but with different levels of detail or different perspectives. A customer interaction might appear as a brief chat transcript in one system and as a detailed case summary in another, both referring to the same support session.

A product return might appear as a warehouse receiving record in one database and as a customer service complaint in another. These are not duplicates in content, but they are duplicates in what they represent. Semantic deduplication requires entity resolution, contextual matching, and judgment calls about whether to merge, keep both, or choose one based on which provides better training signal.

Most teams detect exact duplicates and stop there. Professional dataset engineering addresses all three types with explicit policies documented in the pipeline configuration.

## Deduplication Strategies and When to Use Each

Exact deduplication uses content hashing or field-by-field comparison to identify records that match completely. You compute a hash of the record content, excluding system-generated fields like timestamps or database IDs, and remove all records with duplicate hashes. This is fast, deterministic, and should be the first pass in every multi-source merge.

The policy decision is which copy to keep when you find duplicates. You might choose the earliest record by collection timestamp, the record from the highest-priority source, or the most recently updated version. Document this choice explicitly because it affects reproducibility when you rebuild the dataset.

Fuzzy deduplication uses similarity scoring to identify records that are nearly identical but not exact matches. You define a subset of fields that constitute identity, normalize their values, and compute similarity scores using edit distance, token overlap, or embedding cosine similarity.

For customer records, identity might be email address plus name with normalized whitespace and case. For support tickets, it might be customer ID plus creation date within a 24-hour window plus text similarity above 0.85. The threshold you choose determines precision versus recall in deduplication.

A threshold of 0.95 catches only near-exact matches but misses subtle duplicates. A threshold of 0.75 catches more duplicates but risks merging distinct records. Calibrate this threshold on a sample of labeled duplicate pairs from your specific data sources.

Semantic deduplication uses entity resolution to identify records that refer to the same real-world event or entity even when content differs substantially. You define entity keys, such as transaction ID, session ID, or user ID plus timestamp range, and group records that share these keys.

Then you decide whether to merge them into a single composite record, keep the most detailed version, or keep all versions with tags indicating they are related. The choice depends on what your model needs to learn. If you are training a summarization model, keeping multiple perspectives on the same event might provide useful variation.

If you are training a classification model, multiple records representing the same event will create label leakage and inflate performance metrics artificially.

You run these strategies in sequence, not in parallel. Exact deduplication first to remove the obvious redundancy. Fuzzy deduplication second to catch formatting variations and near-duplicates. Semantic deduplication last to resolve entity-level overlaps. Each pass reduces the dataset size and changes the distribution of what remains, so the order matters for both performance and correctness.

## Conflict Resolution When Sources Disagree

When the same entity appears in multiple sources with conflicting field values, you must choose which version to trust. A customer support interaction might have a sentiment label of "negative" in your CRM system and "neutral" in your quality assurance database. A product description might have different text in your e-commerce catalog versus your content management system.

A medical diagnosis code might differ between the electronic health record and the billing system. You cannot ignore conflicts. Keeping both versions creates inconsistency that confuses models and invalidates evaluation. You need an explicit conflict resolution policy.

Source priority hierarchies assign each data source a priority level and automatically prefer values from higher-priority sources when conflicts occur. Your conflict resolution logic checks which source provided each conflicting field and selects the value from the highest-priority source.

Priority is not arbitrary. It reflects your knowledge of which systems are more authoritative, more recently updated, or more directly tied to ground truth. For customer data, your primary CRM might take priority over third-party enrichment services. For content data, your human-reviewed editorial database might take priority over automated scraping.

For transactional data, your system of record takes priority over downstream analytics databases that may contain stale copies.

Field-level conflict policies apply different resolution strategies to different field types. For timestamp fields, you might take the earliest or most recent value depending on what the timestamp represents. For categorical labels, you might take the majority vote across sources or defer to a designated authoritative source.

For free-text fields, you might take the longest version, the most recently updated version, or concatenate all versions with source attribution. For numeric fields, you might take the median, the value from the most reliable source, or flag the record for manual review if values differ beyond a threshold.

Manual review queues handle conflicts that cannot be resolved automatically. When sources disagree on high-stakes fields, such as ground truth labels for evaluation data or personally identifiable information that affects compliance, you route the conflicting records to a review queue where a human decision-maker selects the correct value or marks the record as unresolvable.

This is not scalable for all conflicts, but it is essential for the subset of conflicts that materially affect model behavior or legal obligations. You track the rate of manual review as a dataset health metric. If more than 2% of your records require manual conflict resolution, your source priority rules are too weak or your data sources are too misaligned.

Conflict resolution policies are code, not documentation. They live in your merge pipeline configuration, versioned and testable, so that you can reproduce the same conflict resolution decisions when you rebuild the dataset or when new team members onboard.

## Maintaining Provenance Through the Merge

Every record in your merged dataset should carry metadata indicating which source it came from, whether it was deduplicated, and how conflicts were resolved. This is provenance tracking, and it is non-negotiable for production dataset engineering.

Without provenance, you cannot diagnose distribution drift, cannot trace model errors back to data sources, and cannot selectively refresh or exclude data from specific sources when quality issues emerge.

Provenance metadata includes source identifiers for each contributing system, source-specific record IDs that allow you to trace back to the original record, the timestamp when the record was extracted from each source, and flags indicating whether the record was the survivor of a deduplication operation or the result of conflict resolution.

When a record is created by merging multiple source records, you log all contributing source IDs and the resolution policy that determined the final field values. This metadata does not become part of the model input. It lives alongside the dataset in auxiliary tables or metadata files, queryable for analysis but excluded from training.

Provenance enables source-level analysis of model performance. You can compute metrics broken down by originating source to detect whether one source contributes disproportionately to model errors. You can track the percentage of your dataset contributed by each source over time to detect unexpected shifts when upstream systems change.

You can exclude or down-weight records from sources that are later discovered to be biased or unreliable without reprocessing the entire dataset from scratch.

Provenance also supports compliance and auditability. When regulators or customers ask where a specific training example came from, you can trace it to the exact source system, extraction timestamp, and processing decisions. When a data breach or quality incident affects one of your source systems, you can identify and remove all affected records from your dataset immediately.

Provenance transforms your dataset from an opaque artifact into an auditable, maintainable asset.

## Deduplication Pipelines and Merge Architecture

Deduplication and conflict resolution happen in a dedicated stage of your data collection pipeline, after source extraction but before any labeling, filtering, or augmentation. The merge pipeline ingests raw records from all sources, applies deduplication in sequence, resolves conflicts, and outputs a unified dataset with provenance metadata.

This stage is idempotent, meaning you can rerun it on the same source data and produce the same output, which is essential for reproducibility and debugging.

The pipeline architecture uses staging tables or intermediate file formats to hold the output of each deduplication pass. After exact deduplication, you write the results to a staging table and compute summary statistics on how many duplicates were removed and from which sources.

After fuzzy deduplication, you write the updated results to another staging table and log the similarity threshold and match rate. After semantic deduplication and conflict resolution, you write the final merged dataset along with provenance metadata. Each stage is independently testable and rerunnable.

You version the merge logic alongside your dataset versions. When you change your deduplication threshold, conflict resolution policy, or source priority hierarchy, you increment the dataset version and regenerate the merged dataset from the original source extracts.

This allows you to compare the impact of merge policy changes on downstream model performance and rollback if a policy change degrades quality.

Merge pipelines are scheduled on the same cadence as your source data refreshes. If you pull data from one source daily and another source weekly, your merge pipeline runs daily to incorporate the new data, reapplying deduplication and conflict resolution to the combined dataset.

This keeps your dataset fresh without manual intervention, but it requires monitoring to detect when source changes introduce new duplication patterns or conflicts that your existing policies do not handle.

## Prioritizing Sources Based on Quality and Recency

Not all data sources are equal. Some are more accurate, more complete, or more representative of the distribution you want your model to learn. Source prioritization encodes this reality into your merge logic. You assign priority levels to each source based on explicit quality criteria, and your conflict resolution and deduplication logic respects these priorities.

Priority criteria include accuracy, measured by how often records from this source match ground truth or human review. Completeness is measured by the percentage of fields populated and the coverage of your target domain. Recency is measured by how recently the data was generated or updated.

Provenance trust is measured by whether the source is a system of record or a derived copy. A source that is directly recorded by users or domain experts typically has higher priority than a source that is automatically scraped or inferred. A source that is updated in real time has higher priority than a batch export from a legacy system.

You document source priorities in a configuration file or database table that your merge pipeline reads at runtime. Each source has a numeric priority level, and the conflict resolution logic automatically defers to the highest-priority source when fields conflict.

When you add a new source to your pipeline, you explicitly assign its priority level based on quality analysis of a sample of its records compared to your existing sources. This prevents new sources from silently overwriting higher-quality data.

Source priority is not static. You monitor source quality over time using metrics like duplicate rate, conflict rate, and schema compliance. If a previously high-priority source begins producing malformed records or conflicting data at a higher rate, you lower its priority or exclude it from the merge until the upstream issue is resolved.

This dynamic prioritization prevents quality degradation from propagating into your dataset automatically.

## Handling Schema Evolution Across Sources

Data sources evolve their schemas over time. New fields are added, old fields are deprecated, data types change, and field semantics shift. When you merge data from sources with evolving schemas, you must handle these changes without breaking your pipeline or corrupting your dataset.

Schema mapping defines how fields from each source map to the unified dataset schema. When a source adds a new field, you decide whether to incorporate it into the unified schema, map it to an existing field, or ignore it. When a source renames a field, you update the mapping to preserve continuity.

When a source changes a field's data type, you add type conversion logic to normalize it to the unified schema's expected type.

You version schema mappings alongside your pipeline code. When a source schema changes, you create a new version of the mapping that handles both the old and new schema versions. Your pipeline detects which schema version each source record conforms to and applies the appropriate mapping.

This allows you to reprocess historical data with the old schema and new data with the new schema using the same pipeline.

Schema validation catches breaking changes before they corrupt your dataset. Each pipeline run validates that incoming records conform to one of the known schema versions for their source. Records that fail validation are quarantined for manual review rather than merged automatically.

You receive alerts when validation failures exceed a threshold, indicating that a source has changed its schema in an unexpected way.

Default value policies handle missing fields gracefully. When a record from one source lacks a field that other sources provide, you decide whether to leave it null, populate it with a default value, or derive it from other fields. The policy depends on whether the missing field is critical for training, whether null values are interpretable by your model, and whether deriving the value from other fields is semantically valid.

## Deduplication at Scale and Performance Optimization

Deduplication becomes computationally expensive as dataset size grows, especially for fuzzy and semantic deduplication that require pairwise comparisons or similarity scoring. Exact deduplication scales linearly with dataset size using hash-based matching, but fuzzy deduplication naively scales quadratically if you compare every record to every other record.

Professional deduplication pipelines use blocking and indexing to reduce the comparison space.

Blocking partitions records into groups based on coarse features, such as first letter of a name field, date range, or customer ID prefix, and only compares records within the same block. This reduces the number of comparisons from N squared to the sum of block sizes squared, which is much smaller if blocks are balanced.

You choose blocking keys based on what fields are most predictive of duplicates in your data. For customer records, email domain might be a good blocking key. For support tickets, customer ID plus month might work. The risk is that duplicates spanning different blocks will be missed, so you validate blocking effectiveness on a labeled sample.

Indexing uses specialized data structures like locality-sensitive hashing or nearest neighbor search indexes to find similar records without exhaustive comparison. You compute a compact representation of each record, such as a MinHash signature or an embedding vector, and index these representations for fast similarity queries.

When processing a new record, you query the index for candidates above your similarity threshold and only compare the new record to those candidates. This scales sub-linearly with dataset size but introduces approximation error. You tune the index parameters to balance recall against computational cost.

Incremental deduplication processes only new records when adding to an existing dataset, comparing new records against the existing deduplicated dataset rather than re-deduplicating everything from scratch. This requires maintaining an index of the existing dataset and updating it as new records are added.

Incremental deduplication is essential for continuous pipelines where data arrives daily or hourly and reprocessing the entire dataset on each update is prohibitively expensive.

You monitor deduplication performance using metrics like records processed per second, percentage of comparisons avoided by blocking, and index query latency. When deduplication becomes a pipeline bottleneck, you optimize by tightening blocking keys, switching to faster similarity metrics, or parallelizing comparisons across workers.

## Testing Deduplication and Conflict Resolution Logic

Deduplication and conflict resolution are deterministic processes that must be tested like any other critical pipeline logic. You build test datasets with known duplicates and conflicts and assert that your pipeline produces the expected output.

Test cases include exact duplicates with identical content, fuzzy duplicates with minor formatting differences, semantic duplicates representing the same entity with different detail levels, and conflict scenarios where sources disagree on field values.

For each test case, you define the expected behavior: which record should survive deduplication, which field values should be chosen in conflicts, and what provenance metadata should be attached. You run your merge pipeline on the test dataset and assert that the output matches expectations.

When you change deduplication thresholds or conflict resolution policies, you rerun the tests to verify that the changes produce the intended behavior and do not introduce regressions.

You also test deduplication recall and precision on a labeled sample of real data. Recall measures what percentage of true duplicates your pipeline detects. Precision measures what percentage of flagged duplicates are actually duplicates.

You manually label a few hundred pairs of records as duplicate or distinct, run your deduplication logic, and compute recall and precision. This validates that your similarity thresholds and blocking logic are calibrated correctly for your actual data distribution.

Conflict resolution tests verify that priority hierarchies are enforced correctly and that field-level policies apply as intended. You create synthetic records where sources disagree on specific fields, run the conflict resolution logic, and assert that the output contains the expected values from the correct sources.

You test edge cases like ties in priority levels, missing fields in high-priority sources, and conflicts on fields with no defined policy.

Testing deduplication and conflict resolution is not a one-time activity. You add new test cases whenever you discover a deduplication or conflict pattern that your existing logic handles incorrectly, and you rerun the full test suite on every pipeline change.

## When to Merge Sources and When to Keep Them Separate

Not every multi-source dataset should be merged into a single unified dataset. Sometimes keeping sources separate and training separate models or using source labels as features produces better results. The decision depends on whether the sources represent the same underlying distribution or meaningfully different distributions that should be modeled differently.

If your sources cover the same task and domain but come from different systems, merging makes sense. Customer support tickets from your CRM and your ticketing system both represent customer support interactions and should be merged with deduplication and conflict resolution.

If your sources cover different tasks or domains, keeping them separate may be better. Customer support tickets and sales call transcripts both involve customer interactions, but they serve different functions and may require different models. Merging them dilutes the signal for both tasks.

If sources have dramatically different quality profiles, you might keep them separate during initial model development and merge them only after you have validated that the lower-quality source does not degrade performance. You train a baseline model on the high-quality source alone, then incrementally add data from the lower-quality source and measure the impact on evaluation metrics.

If adding the lower-quality source improves coverage without degrading accuracy, you merge. If it degrades accuracy, you either exclude it or use it only for specific slices where it adds value.

Source separation also supports A/B testing of data sources. You train two models, one on each source, and compare their production performance. The winning source becomes the primary data source, and the losing source is deprioritized or excluded.

This empirical approach prevents dogmatic assumptions about which source is better and lets real-world performance guide your merge decisions.

## Monitoring Merge Quality Over Time

Multi-source merges degrade over time as sources evolve, as new sources are added, and as upstream systems change. You monitor merge quality continuously to detect when deduplication effectiveness declines, when conflict rates spike, or when source contributions shift unexpectedly.

Deduplication rate tracking measures what percentage of incoming records are flagged as duplicates and removed. You track this metric per source and overall. A sudden increase in deduplication rate indicates that a source is producing redundant exports or that an upstream system is malfunctioning.

A sudden decrease suggests that your deduplication logic is failing to detect duplicates that it previously caught.

Conflict rate tracking measures what percentage of merged records have conflicts across sources and which sources are involved in conflicts most frequently. High conflict rates between two sources suggest schema drift, data quality issues, or semantic misalignment that needs investigation.

Conflict rates that increase over time indicate that sources are diverging and may need priority adjustments or separate handling.

Source contribution tracking measures what percentage of your final dataset comes from each source after deduplication and conflict resolution. You compare current contributions to historical baselines. If a source that previously contributed 40% of your data now contributes only 10%, either that source is producing less data or deduplication is removing more of its records as duplicates from other sources.

Both scenarios warrant investigation.

Provenance audits randomly sample merged records and manually verify that deduplication and conflict resolution decisions were correct. You review a few hundred records per month, checking that duplicates were actually duplicates, that the chosen version in conflict resolution was the right choice, and that provenance metadata accurately reflects the merge decisions.

This catches systematic errors that automated metrics miss.

## Merge Policies as Living Configuration

Deduplication thresholds, conflict resolution rules, and source priorities are not set once and forgotten. They evolve as your data sources change, as new sources are added, and as you learn from model performance and data quality issues. You treat merge policies as living configuration, versioned and updated based on ongoing monitoring and analysis.

You review deduplication metrics quarterly or whenever you add a new data source. If the duplicate rate increases unexpectedly, you investigate whether a source is producing redundant records or whether your blocking logic is failing to catch duplicates.

If conflict rates increase, you investigate whether sources have diverged in schema or semantics and whether your priority hierarchy needs adjustment.

You review conflict resolution decisions on a sample basis, manually inspecting a few dozen records where conflicts were resolved to verify that the chosen values are correct. If you find systematic errors, you update the conflict resolution policy and regenerate the dataset.

You track the frequency of each resolution rule being applied, and if a rule is never triggered or always triggered, you investigate whether it is misconfigured or obsolete.

Merge policies are documented in code and configuration files, not in wiki pages or runbooks. The pipeline reads the configuration at runtime, and changes to the configuration trigger a dataset rebuild and version increment. This makes merge logic auditable, testable, and reproducible.

Multi-source merging is not a data janitorial task. It is dataset architecture that determines whether your training data reflects coherent ground truth or the chaotic overlap of misaligned systems. You build deduplication and conflict resolution into the core of your collection pipeline, test it rigorously, monitor it continuously, and evolve it as your data sources evolve.

The next challenge is deciding how often to run that pipeline, which determines whether your dataset is a static snapshot or a living reflection of production reality.
