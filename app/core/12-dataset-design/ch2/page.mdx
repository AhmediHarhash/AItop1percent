# Chapter 2 — Data Collection Pipelines

How data enters your system determines everything downstream. Collection pipelines are the intake infrastructure that spans from raw sources through initial routing into storage. A poorly designed pipeline introduces latency, duplicates, schema mismatches, and lost context about where each record originated. A well-designed pipeline is transparent, testable, and auditable at every stage. This chapter covers the full spectrum of data sources—production logs, synthetic generation, external procurement, user feedback, and expert elicitation—along with the architectural patterns that make each source reliable at scale.

The choice between streaming and batch ingestion, between one-time and continuous collection, and between raw sources and processed intermediates shapes your entire dataset lifecycle. Early decisions about event timestamps, sampling strategies, and multi-source deduplication become expensive to reverse once pipelines are production-grade. Teams that defer these decisions pay the price in lost records, misaligned timestamps, and months of downstream rework.

Provenance tracking—knowing exactly where every record came from, when it arrived, what transformations it underwent, and who touched it—separates professional data engineering from amateur practice. It is not optional. It is the foundation of debugging, compliance, and learned models that can be audited and explained.

---

- **2.1** — The Three Sources: Production Logs, Synthetic Generation, External Procurement
- **2.2** — Harvesting Production Data: Logging, Sampling, and Consent
- **2.3** — Building Intake Pipelines: Schema, Validation, and Routing
- **2.4** — Streaming vs Batch Ingestion: When Real-Time Changes Everything
- **2.5** — Event Time vs Processing Time: Getting Timestamps Right
- **2.6** — Web Scraping and Crawling in 2026: Legal, Ethical, and Technical Realities
- **2.7** — Purchasing and Licensing Third-Party Datasets
- **2.8** — API-Based Data Collection: Rate Limits, Costs, and Reliability
- **2.9** — User-Submitted Data: Feedback Loops, Corrections, and Opt-In Flows
- **2.10** — Domain Expert Elicitation: Structured Knowledge Capture
- **2.11** — Multi-Source Merging: Deduplication, Conflict Resolution, and Priority
- **2.12** — Collection Cadence: One-Shot vs Continuous Pipelines
- **2.13** — Data Collection for Low-Resource Domains
- **2.14** — Multimodal Data Collection: Audio, Image, and Video Pipelines
- **2.15** — Collection Pipeline Monitoring and Alerting
- **2.16** — Data Provenance: Tracking Where Every Record Came From

---

*We begin with the three fundamental sources of data and when each one matters most.*
