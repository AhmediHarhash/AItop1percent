# 9.6 â€” GDPR Data Subject Rights and Dataset Obligations

A data subject submits a GDPR deletion request. You must delete their personal data from your databases within 30 days. Easy enough. But their data is also in your training dataset. The training dataset was used to fine-tune a model six months ago. The model weights now encode patterns learned from that individual's data. The model is deployed in production, serving 40,000 requests per day. Do you have to retrain the model to honor the deletion request? Can you prove the individual's data is no longer influencing outputs? Do you even have the infrastructure to identify which training examples belong to that individual across versioned snapshots, cloud buckets, and archived datasets? Most teams cannot answer these questions, and that gap between legal obligation and technical capability is where enforcement actions will focus. Ignoring data subject rights because they are hard to implement is not a strategy. It is a violation waiting to be discovered.

The General Data Protection Regulation grants data subjects seven core rights, and every one of them creates friction with the way AI systems are built. Data subjects can ask what data you hold about them, but your training dataset is distributed across storage tiers, cloud buckets, and versioned snapshots. Data subjects can demand deletion, but your model was trained on their data six months ago and you do not have a mechanism to remove individual records from trained weights. Data subjects can request correction, but correcting one record in a dataset that has already been used for training does not correct the patterns the model learned from the wrong data. The law assumes that data is stored in queryable databases where records can be identified, retrieved, updated, and deleted. AI datasets are not structured that way, and teams that treat data subject rights as someone else's problem will face enforcement actions that force them to rebuild their entire data pipeline.

## The Seven Data Subject Rights

Under GDPR Articles 12 through 22, data subjects have the **right of access** (Article 15), the **right to rectification** (Article 16), the **right to erasure** (Article 17), the **right to restriction of processing** (Article 18), the **right to data portability** (Article 20), the **right to object** (Article 21), and the **right not to be subject to automated decision-making** including profiling (Article 22). Each right has specific requirements, timelines, and exceptions, and failing to honor any of them is a separate violation.

The **right of access** requires that you provide data subjects with confirmation of whether you are processing their personal data, and if so, access to that data along with information about the purposes of processing, the categories of data, the recipients of the data, the retention period, and the existence of their other rights. You must respond within one month, extendable by two additional months if the request is complex. You must provide the information free of charge unless the request is manifestly unfounded or excessive. You must verify the identity of the requester to prevent unauthorized disclosure.

For AI systems, the right of access is difficult to implement because training data is often not indexed by individual. You may have millions of records stored in Parquet files, sharded across buckets, with no efficient way to query which records belong to a specific user. If a user asks what data of theirs is in your training set, can you answer? If you cannot, you are violating Article 15. The obligation is to provide access to the data, not to explain why access is technically difficult.

The **right to rectification** requires that you correct inaccurate personal data and complete incomplete personal data without undue delay. For transactional databases, this is straightforward. For training datasets, it is not. If a user reports that their email address, birthdate, or demographic attribute was recorded incorrectly, you must correct it in your active data stores. But if that incorrect data was used to train a model, the model has learned patterns from the wrong data. Correcting the source record does not correct the model. You must either retrain the model without the incorrect data or accept that the model is subtly wrong and document that risk.

The **right to erasure**, also known as the right to be forgotten, requires that you delete personal data without undue delay when consent is withdrawn, when the data is no longer necessary for the purposes for which it was collected, when the data subject objects to processing and there are no overriding legitimate grounds, when the data was unlawfully processed, or when erasure is required to comply with a legal obligation. Erasure must be complete. It must cover active datasets, backup datasets, archived datasets, and any derived datasets or models that contain the data in a recoverable form.

For training datasets, erasure is operationally complex. You must identify every dataset version that contains the user's data, delete the records, and rebuild any downstream artifacts like aggregated statistics or model checkpoints that were computed from the deleted data. If the model was trained on the data and the model can reproduce or reveal the data, you may need to retrain the model. If the model cannot reproduce the data and the data has been fully aggregated into statistical patterns, the model is arguably not personal data and does not need to be retrained, but you must document that reasoning and be prepared to defend it.

The **right to restriction of processing** requires that you suspend processing of personal data under certain conditions, such as when the accuracy of the data is contested, when the processing is unlawful but the data subject does not want erasure, when you no longer need the data but the data subject needs it for legal claims, or when the data subject has objected to processing and you are verifying whether your legitimate grounds override theirs. Restricted data can be stored but not processed, which means you cannot use it for training, evaluation, or inference until the restriction is lifted.

The **right to data portability** requires that you provide data subjects with their personal data in a structured, commonly used, and machine-readable format, and transmit it to another controller if technically feasible. This right applies only to data that the data subject provided to you and that you process based on consent or contract. It does not apply to data you derived or inferred, and it does not apply to processing based on legitimate interest or legal obligation. For training datasets, portability is rarely requested, but when it is, you must be able to extract the user's contributed data in JSON, CSV, or another portable format.

The **right to object** allows data subjects to object to processing based on legitimate interest or for direct marketing purposes. When a user objects, you must stop processing unless you can demonstrate compelling legitimate grounds that override the user's interests, rights, and freedoms, or unless the processing is necessary for the establishment, exercise, or defense of legal claims. The right to object is absolute for direct marketing. If a user objects to marketing, you must stop immediately. For other processing, the right is conditional, but the burden is on you to prove that your grounds are compelling.

The **right not to be subject to automated decision-making** prohibits decisions based solely on automated processing, including profiling, that produce legal effects or similarly significantly affect the individual, unless the decision is necessary for entering into or performing a contract, is authorized by law, or is based on explicit consent. Even when automated decision-making is permitted, you must implement suitable safeguards including the right to obtain human intervention, to express their point of view, and to contest the decision. For AI systems that make hiring decisions, creditworthiness assessments, insurance pricing, or content moderation decisions, this right is directly applicable and requires human review mechanisms.

## The Right of Access Applied to Training Data

The right of access is where most teams first realize that their dataset infrastructure is not compliant. A user submits a Subject Access Request asking what data you hold about them. You check your production database and provide a report of their account information, transaction history, and profile settings. The user asks about the training data. Your training datasets are stored in S3 buckets, organized by collection date, not by user ID. You do not have an index mapping users to training records. You cannot efficiently answer the question.

The legal obligation is clear: if you process personal data, you must be able to identify and retrieve it on request. If you cannot, you are in violation of Article 15. The technical solution is to build an index at the time of dataset creation that maps user identifiers to record identifiers, storage locations, and dataset versions. When a user requests access, you query the index, retrieve the records, and return them. If you did not build that index, you must scan the entire dataset, which may take hours or days, and you will miss the one-month response deadline.

Some teams argue that training data is not subject to access rights because it has been aggregated into a model and no longer exists in a recoverable form. That argument works only if you have actually deleted the training data after training. If you retain the training data for retraining, for evaluation, for debugging, or for compliance, it is still personal data and access rights still apply. If you retain the model and the model can reproduce individual training examples through memorization or inversion attacks, the model itself may be subject to access rights.

The practical approach is to treat training data as subject to access rights unless you can affirmatively demonstrate that the data has been anonymized, deleted, or aggregated to the point where it is no longer personal data. Build data lineage tracking so that when a user requests access, you can identify all datasets that contain their data, retrieve the records, and provide a complete response. If you cannot do that, you are not ready to collect training data at scale.

## The Right to Erasure Applied to Models

The right to erasure is the most disruptive right for AI systems because it requires deletion of data that has already been used to train models, and deleting data from a trained model is not a standard operation. When a user exercises the right to erasure, you must delete their data from your active datasets, your backup datasets, your archived datasets, and any derived datasets or models that contain the data in a recoverable form.

The question is whether trained models contain data in a recoverable form. For traditional machine learning models trained on structured data, the answer is usually no. A logistic regression model, a decision tree, or a random forest aggregates training data into weights and thresholds that do not allow reconstruction of individual records. Deleting one training record and retraining the model would produce nearly identical weights. The model does not memorize training data. It generalizes from it. Under that analysis, the model is not personal data and does not need to be retrained when data is deleted.

For large language models, embedding models, and generative models, the answer is less clear. Research has shown that large models can memorize training examples, especially rare or unusual examples, and reproduce them verbatim on request. If a model was trained on a user's email, blog post, or support ticket, and the model can be prompted to reproduce that content, the model contains the user's personal data in a recoverable form. Deleting the source data without retraining the model does not satisfy the right to erasure because the data still exists in the model weights.

The legal test is not whether the data can be easily recovered. It is whether recovery is reasonably likely. If researchers have published techniques for extracting training data from models, if red teams have demonstrated that your model memorizes specific types of content, or if the model is fine-tuned on a small dataset where memorization is likely, the model is at risk of being classified as personal data. If the model is trained on billions of diverse examples and does not memorize individual records, the risk is lower, but you still need to document your analysis and be prepared to defend it.

The safest technical approach is to implement **machine unlearning**, which allows you to retrain a model efficiently without specific data points. Techniques like SISA (Sharded, Isolated, Sliced, and Aggregated learning) partition training data into shards and train separate models on each shard. When a user requests erasure, you retrain only the shard that contains their data and re-aggregate the models. This is more efficient than full retraining but requires upfront architectural decisions that most teams have not made.

The next safest approach is to track which data was used to train which models and commit to retraining models when erasure requests exceed a threshold. If you receive ten erasure requests per month and you retrain your model quarterly, you can batch the requests and retrain once per quarter with the deleted data excluded. If you receive ten thousand erasure requests per month, batching does not solve the problem and you need machine unlearning or a commitment to more frequent retraining.

The riskiest approach is to delete the source data, ignore the model, and hope that no one challenges you. That approach works until a regulator audits your compliance, a user sues for failure to honor their rights, or a researcher publishes evidence that your model memorizes training data. At that point, you have no defense, and the fines for failing to honor erasure requests are severe.

## The Right to Rectification and Downstream Propagation

The right to rectification requires that you correct inaccurate data without undue delay, and that you notify all recipients of the data about the correction unless notification is impossible or involves disproportionate effort. For AI systems, the recipients include downstream datasets, aggregated statistics, cached embeddings, and trained models. Correcting data in the source database does not automatically correct the data in those downstream artifacts, and failing to propagate corrections is a violation.

If a user reports that their age, location, or demographic attribute was recorded incorrectly, you must correct it in your production database. If that data was exported to a training dataset, you must correct it there as well. If the training dataset was used to compute aggregate statistics like mean age or location distribution, you may need to recompute those statistics. If the dataset was used to train a model and the incorrect data influenced the model's predictions for that user or for similar users, you must decide whether to retrain the model.

The legal standard is whether the inaccuracy materially affects the processing. If a single incorrect age in a dataset of ten million records does not materially affect the model's behavior, retraining may not be necessary. If the incorrect data is a sensitive attribute like health status or creditworthiness and the model is used to make decisions about the user, retraining is necessary. The burden is on you to assess the impact and document your decision.

The practical solution is to implement **data lineage tracking** that records which datasets were derived from which source data, which models were trained on which datasets, and which predictions were made using which models. When a user requests rectification, you query the lineage graph, identify all downstream artifacts that depend on the incorrect data, assess the impact of the correction, and decide whether to rebuild, retrain, or document the risk. Without lineage tracking, you cannot comply with rectification rights at scale.

## Data Protection Impact Assessments for AI Datasets

Under GDPR Article 35, you must conduct a **Data Protection Impact Assessment** before processing that is likely to result in a high risk to the rights and freedoms of individuals. High-risk processing includes systematic and extensive profiling with legal or similarly significant effects, large-scale processing of special category data, and systematic monitoring of publicly accessible areas. AI systems that profile users, train on health data, or analyze biometric data almost always require a DPIA.

A DPIA must describe the processing, assess the necessity and proportionality of the processing, assess the risks to individuals, and describe the safeguards and measures to mitigate those risks. It must be conducted before processing begins, and it must be reviewed when the processing changes materially. If the DPIA identifies high residual risk after mitigation, you must consult with the supervisory authority before proceeding.

For training datasets, a DPIA must assess the risk of re-identification, the risk of unauthorized access, the risk of data leakage through model outputs, the risk of bias or discrimination, and the risk of harm if the model makes incorrect predictions. It must document the legal basis for collecting the data, the consent mechanisms or legitimate interest assessment, the security controls for protecting the data, the anonymization or pseudonymization techniques applied, and the processes for honoring data subject rights.

DPIAs are not optional. If you are training models on health data, financial data, biometric data, or any data that could result in profiling with legal effects, you must conduct a DPIA. If you are processing data of children, if you are using automated decision-making, if you are processing on a large scale, you must conduct a DPIA. Failing to conduct a DPIA when required is a violation under Article 83, and regulators treat it as evidence that you did not take privacy seriously.

## The Data Protection Officer Role

Under GDPR Article 37, you must appoint a **Data Protection Officer** if you are a public authority, if your core activities involve large-scale systematic monitoring, or if your core activities involve large-scale processing of special category data. Most AI companies meet at least one of these criteria. If you process user behavior data at scale, you are conducting systematic monitoring. If you train models on health data, biometric data, or demographic data, you are processing special category data. You need a DPO.

The DPO is responsible for monitoring compliance, advising on data protection obligations, conducting DPIAs, cooperating with the supervisory authority, and serving as the point of contact for data subjects. The DPO must be independent, must report directly to the highest level of management, and must not be dismissed or penalized for performing their duties. The DPO does not need to be an employee. You can hire an external DPO, but they must have expert knowledge of data protection law and practices.

Many teams treat the DPO role as a formality, appointing someone from Legal or Compliance who has no technical understanding of AI systems and no authority to block projects. That is a mistake. The DPO must understand your dataset pipelines, your model training processes, your data subject rights workflows, and your security controls. They must be involved in decisions about dataset collection, consent mechanisms, anonymization techniques, and cross-border transfers. If your DPO does not know what data you are training on or how you handle erasure requests, they cannot do their job, and you are not compliant.

## Cross-Border Transfer Mechanisms

GDPR restricts the transfer of personal data to countries outside the European Economic Area unless the destination country ensures an adequate level of data protection. **Adequacy decisions** have been issued for a limited number of countries including the UK post-Brexit, Canada, Japan, and others, but not for the United States, China, Russia, or most of the world. If you transfer data to a non-adequate country, you must use one of the transfer mechanisms in GDPR Chapter V.

The most common mechanism is **Standard Contractual Clauses**, which are pre-approved contract templates published by the European Commission that impose data protection obligations on the data importer. SCCs must be supplemented with a **transfer impact assessment** that evaluates whether the laws of the destination country provide sufficient protection and whether additional safeguards are necessary. If the destination country has surveillance laws that could compel access to the data, you must implement technical measures like encryption, pseudonymization, or data minimization to protect against government access.

For AI training datasets, cross-border transfers are common. Training data is collected in Europe, transferred to cloud storage in the US, processed by training infrastructure in multiple regions, and accessed by engineering teams distributed globally. Every one of those transfers is subject to GDPR restrictions. If you transfer training data to the US without SCCs and a transfer impact assessment, you are violating GDPR Article 44. If you transfer data to China, where national security laws require disclosure of data on government request, you may not be able to implement sufficient safeguards, and the transfer may be unlawful even with SCCs.

The safest approach is to process European data in Europe using infrastructure located in the EEA or in countries with adequacy decisions. If that is not feasible, use SCCs, conduct transfer impact assessments, and implement encryption, access controls, and audit logging to protect the data in transit and at rest. Document every transfer, every assessment, and every safeguard, because you will need that documentation under audit.

## Practical Compliance Frameworks for AI Teams

Building a GDPR-compliant dataset pipeline requires engineering, process, and documentation. You need **data subject rights workflows** that allow users to submit access, rectification, erasure, restriction, portability, and objection requests through a web interface, email, or API. You need **identity verification** to prevent unauthorized disclosure of data in response to fraudulent requests. You need **request tracking** to ensure that requests are acknowledged within 72 hours and resolved within one month. You need **automated data retrieval** that queries all your datasets, indexes, and backups to identify records belonging to the requester.

You need **data lineage tracking** that records which datasets were derived from which source data, which models were trained on which datasets, and which predictions were made using which models. You need **version control** for datasets so that when a user requests erasure, you can identify all versions that contain their data and delete them. You need **audit logs** that record every access, every modification, and every deletion, so you can demonstrate compliance under investigation.

You need **consent management infrastructure** that records when consent was obtained, what the user consented to, and when consent was withdrawn. You need **policy enforcement** that prevents processing data when consent has been withdrawn or when a restriction is in place. You need **training and awareness programs** so that engineers, data scientists, and product managers understand their obligations and do not bypass controls.

And you need **regular compliance reviews** that test whether your workflows work, whether your documentation is current, whether your safeguards are effective, and whether your team is following the processes. Compliance is not a one-time checkbox. It is an ongoing discipline, and teams that treat it as an afterthought will fail when the first rights request arrives or the first audit begins.

## The Gap Between Legal Requirements and Technical Capabilities

The uncomfortable truth is that most AI teams cannot fully comply with GDPR data subject rights using their current infrastructure. They cannot efficiently identify all training data belonging to a specific user. They cannot retrain models to remove individual data points. They cannot propagate corrections from source data to trained models. They cannot demonstrate that their models do not memorize training data. The gap between what the law requires and what the technology supports is real, and it is not an excuse.

Regulators do not care that compliance is hard. They care that you are processing personal data and you owe data subjects their rights. If you cannot honor those rights, you should not be collecting the data. If you are collecting the data anyway, you are accepting the risk that a rights request will expose your non-compliance, and that risk will materialize.

The solution is not to ignore the problem. The solution is to build compliance into your dataset engineering from the beginning. Index training data by user. Track data lineage. Implement machine unlearning. Conduct DPIAs. Appoint a DPO. Use SCCs for cross-border transfers. Document everything. Compliance is not a legal problem to be solved by lawyers. It is an engineering problem to be solved by engineers, and the time to solve it is before you collect the first record of training data.

GDPR is the baseline for data subject rights in AI systems, but it is not the only privacy regime that applies to datasets. Healthcare data is governed by additional requirements that are even stricter, and those requirements create obligations that go beyond what GDPR demands.

