# Chapter 4 — Data Quality and Cleaning

Raw data is not usable data. Every record that enters your pipeline carries noise, duplicates, missing fields, inconsistent formatting, and context loss. Data quality is not a one-time cleaning step performed before model training. It is a continuous discipline with automated gates, observability systems, service-level objectives, and economics. Teams that treat data quality as a checkbox instead of a practice pay the cost in silently degraded models, missed regressions, and months spent debugging why a system that worked last quarter no longer works today.

The full lifecycle of data quality spans detection through remediation through monitoring. You must know what bad data looks like in your domain—whether that means duplicate user interactions, grammatically broken text, out-of-range numeric values, or corrupted media files. You must have automated pipelines that catch problems before bad records reach your training or evaluation sets. You must track quality metrics with the same rigor you apply to model metrics, including SLOs for correctness, freshness, completeness, and availability. And you must understand the economics of when fixing a record is worth the cost versus when dropping it is the right choice.

Multimodal datasets add complexity: audio files with dropped samples, images with compression artifacts, videos with resolution mismatches. Each modality requires its own quality checks. But the principle is universal. Every dataset degrades over time as sources change, new noise patterns emerge, and the ground truth itself shifts. The teams that stay ahead of degradation use instrumentation, observability, and proactive monitoring—not post-hoc audits of stale data.

---

- **4.1** — Defining Data Quality for AI Systems
- **4.2** — The Quality Inspection Pipeline: Automated Checks Before Human Review
- **4.3** — Deduplication: Exact, Near-Duplicate, and Semantic
- **4.4** — Noise Detection and Removal
- **4.5** — Handling Missing, Incomplete, and Contradictory Records
- **4.6** — Language Quality: Grammar, Fluency, and Register Consistency
- **4.7** — Format Normalization: Dates, Numbers, Entities, Encodings
- **4.8** — Outlier Detection and Boundary Cases
- **4.9** — Quality Scoring and Tiering: Not All Records Are Equal
- **4.10** — Automated Quality Gates in Data Pipelines
- **4.11** — Data Observability: Distribution Shift Detection, Freshness Alerts, and Volume Anomalies
- **4.12** — Data Quality SLOs: Availability, Freshness, Correctness, and Completeness
- **4.13** — Multimodal Quality Checks: Corruption, Duration, Sample Rate, Resolution
- **4.14** — The Economics of Data Cleaning: When to Fix vs When to Drop
- **4.15** — Quality Regression: How Datasets Degrade Over Time

---

*We begin by defining what data quality actually means for AI systems, because it is not what most teams assume.*
