# 6.12 — Adversarial Eval Sets: Stress-Testing Edge Cases

In early 2025, a legal technology company deployed a contract clause classification system that achieved 96% accuracy on their standard evaluation set. The system categorized clauses into types like indemnification, confidentiality, termination, and liability limitation. It performed beautifully in internal testing and early production trials. Three weeks after full launch, a law firm using the system discovered that it consistently misclassified indemnification clauses when the word "indemnify" didn't appear explicitly. Instead of "Party A shall indemnify Party B," the contract used "Party A shall hold harmless and defend Party B against all claims." The system tagged these as general liability clauses rather than indemnification clauses. The law firm found fourteen similar failures across their contract portfolio. When the legal tech company investigated, they realized their evaluation set contained only standard-phrasing examples. Every indemnification clause in the eval set used the word "indemnify." No one had tested whether the system could recognize indemnification by semantic content rather than keyword presence. They had optimized for the common case and never stress-tested the paraphrase edge case. Their standard eval set measured performance on well-formed, prototypical examples. They needed an adversarial eval set that specifically targeted the ways their system could break.

This is the adversarial evaluation problem. Standard eval sets measure performance on representative examples sampled from the expected distribution of inputs. They tell you how the system performs on typical cases. But typical cases aren't where systems fail in production. Systems fail on edge cases, adversarial inputs, distribution shifts, and inputs specifically constructed to exploit model weaknesses. An adversarial eval set is built to stress-test these failure modes. It's not trying to estimate average performance. It's trying to find the boundaries where the system breaks, measure how robust the system is to perturbations, and identify specific vulnerability patterns that need to be addressed. This chapter teaches you how to build adversarial eval sets that expose weaknesses, how to distinguish between adversarial examples and edge cases, and how to maintain these sets as your models improve and develop new failure modes.

## Adversarial Examples vs Edge Cases vs Corner Cases: The Distinction

The terms adversarial examples, edge cases, and corner cases are often used interchangeably, but they represent different categories of difficult inputs with different construction methods and different purposes in evaluation.

An edge case is an input that sits near the boundary of the task definition or the boundary between categories. It's a legitimate real-world input that happens to be difficult because it has characteristics of multiple categories or lacks clear distinguishing features. A customer support message that asks both a billing question and a technical question is an edge case. It's a real user behavior that doesn't fit cleanly into the single-intent classification the system expects. Edge cases test whether your system handles ambiguity and boundary conditions. They appear naturally in production data, though at lower frequency than central cases.

A corner case is an input that involves the rare combination of multiple unusual features. Each feature individually might be handled correctly, but the combination creates unexpected behavior. A medical symptom description that mentions both pregnancy and chest pain is a corner case for a symptom triage system. Pregnancy is handled. Chest pain is handled. But the combination requires different triage logic than either alone. Corner cases test whether your system handles feature interactions correctly. They appear very rarely in production but can cause severe failures when they occur.

An adversarial example is an input specifically constructed to fool the system. It's designed by someone who understands the system's decision boundary and deliberately crafts an input that crosses that boundary in a way that causes a wrong prediction. A spam classifier might learn that emails containing certain product names are legitimate. An adversarial example would be spam that includes those product names specifically to evade detection. Adversarial examples often don't occur naturally in production unless someone is actively attacking the system. But they reveal fundamental weaknesses in the model's learned representations.

The construction methods differ. Edge cases are found through domain analysis and production sampling. You identify where category boundaries are fuzzy and sample examples from those regions. Corner cases are found through combinatorial analysis. You identify which features interact and construct examples that combine them. Adversarial examples are found through perturbation methods, gradient-based attacks, or manual red teaming. You take a correctly classified example and modify it minimally until it's misclassified.

The purposes in evaluation differ as well. Edge cases in your eval set measure how well the system handles realistic ambiguity. This predicts production performance on difficult but common inputs. Corner cases measure robustness to rare feature combinations. This predicts how the system behaves in low-data regions. Adversarial examples measure vulnerability to deliberate attacks or systematic weaknesses. This predicts how the system behaves under adversarial conditions or when distribution shifts expose learned shortcuts.

A complete adversarial eval set includes all three categories. You need edge cases to test boundary robustness. You need corner cases to test interaction robustness. You need true adversarial examples to test decision boundary robustness. The relative proportions depend on your threat model and production risk profile. A content moderation system facing active adversarial users needs a higher proportion of true adversarial examples. A medical triage system with no adversarial users but high stakes on corner cases needs a higher proportion of corner cases.

## Techniques for Generating Adversarial Inputs: Perturbation, Paraphrase, Semantic Inversion

Building an adversarial eval set requires systematic generation techniques. You can't rely on random sampling from production because adversarial examples are rare in production. You need to actively construct examples that target known or hypothesized weaknesses.

Perturbation techniques start with a correctly classified example and apply small modifications designed to flip the classification without changing the semantic meaning. For text, perturbations include synonym replacement, word order changes, insertion of irrelevant clauses, typos, and spacing changes. A sentiment classifier that correctly labels "This product is excellent" as positive should also correctly label "This product is great" and "This product is really excellent" and "This product is excellent." Perturbation tests whether the model has learned robust representations or surface-level pattern matching.

Effective perturbations target linguistic variation without changing task-relevant semantics. Synonym replacement using WordNet or contextual embeddings creates paraphrases that preserve meaning. A legal clause saying "within thirty days" should be recognized equivalently to "within 30 days" and "within a period of thirty days" and "no later than thirty days following." If the model fails on these variants, it's relying on exact phrasing rather than semantic content. You generate these variants systematically for every example in your seed set, creating a perturbation set that's five to ten times larger than the original set.

Paraphrase generation uses language models to rewrite examples in different styles while preserving meaning. You take a correctly classified example and prompt GPT-4 or Claude to rewrite it in formal style, informal style, technical jargon, simplified language, or verbose style. A customer complaint written informally—"ur app keeps crashing when i try to upload pics"—should be classified the same as the formal version—"I am experiencing repeated application crashes during image upload operations." If classification accuracy drops significantly across paraphrase styles, the model has learned style cues rather than content understanding.

Semantic inversion techniques create adversarial examples by flipping the meaning while preserving surface structure. For sentiment analysis, you take a positive review and replace key sentiment-bearing words with antonyms while keeping the structure. "The food was delicious and the service was excellent" becomes "The food was terrible and the service was awful." This should flip the classification from positive to negative. If it doesn't, the model is relying on structural cues rather than word meanings. For more subtle inversion, you use negation insertion. "I recommend this product" becomes "I do not recommend this product." This is a minimal change—insertion of a single word—that completely reverses meaning.

Insertion techniques add irrelevant or distracting information to see if the model maintains correct classification. A spam detector that correctly identifies a phishing email should still identify it as phishing when a paragraph of legitimate-looking content is inserted in the middle. A contract clause classifier that correctly identifies a termination clause should still identify it when surrounded by paragraphs about unrelated topics. Insertion tests whether the model can extract task-relevant content from noisy inputs.

Deletion techniques remove potentially redundant information to test whether the model relies on it. If you delete the subject line from an email, does the spam classifier still work? If you delete the first sentence from a document summary, does the topic classifier still work? Deletion reveals which parts of the input the model depends on. If deleting seemingly irrelevant context causes failures, the model has learned spurious correlations with that context.

Adversarial typo generation tests robustness to input quality. Real production data contains typos, misspellings, and grammatical errors. Your eval set should include them. You systematically introduce realistic typos—character swaps, missing letters, doubled letters, homophone errors—to correctly classified examples. A medical symptom description "I have severe headache" should be recognized equivalently to "I have severe hedache" and "I have severe headacke." If accuracy drops significantly with typos, the model isn't production-ready.

## Red Team Contributions to Eval Sets

Automated perturbation techniques generate large volumes of adversarial examples, but they miss creative attack patterns that humans discover. Red teaming brings human creativity and adversarial thinking to eval set construction. A red team is a group of people whose job is to break your system. They approach it with an attacker's mindset, trying every manipulation they can think of to cause failures.

Red team members are given access to your system and asked to find inputs that produce wrong outputs, offensive outputs, unsafe outputs, or outputs that violate policy. They're not constrained to realistic inputs. They can try anything. They experiment with prompt injection, role-playing scenarios, multi-language mixing, encoded inputs, indirect requests, and social engineering. Every failure they find becomes a candidate for the adversarial eval set.

The value of red teaming is that humans find failure modes that automated techniques miss. A red team member might discover that your content moderation system can be bypassed by framing policy-violating content as a hypothetical question. "If someone were to describe explicit violence, how would that sound?" becomes a way to get the system to generate the prohibited content. This isn't a perturbation of an existing example. It's a novel attack pattern that emerges from understanding how the system interprets context and intent.

Red team sessions are time-boxed sprints. You give the team a week to find as many failures as possible. They document each failure with the input that caused it, the output produced, the expected output, and the attack pattern used. After the sprint, you review the documented failures, classify them by attack type, and select representative examples for the adversarial eval set. You don't include every failure. You include examples that represent distinct attack patterns.

Red team membership should include diverse perspectives. Include domain experts who understand the task and can craft domain-specific attacks. Include security researchers who understand adversarial techniques. Include end users who can think like real users, including malicious users. Include people from communities who might be harmed by system failures, so they can identify failures that your core team wouldn't notice. Diversity in red teams leads to diversity in discovered attack patterns.

Red teaming is particularly valuable for subjective tasks like content moderation, toxicity detection, and safety classification. Automated perturbations don't capture the creative ways people try to evade filters. Humans invent new slang, use coded language, employ sarcasm and irony, and frame prohibited content in ways that technically comply with rules while violating their spirit. Red teams find these patterns. You document them in the adversarial eval set to ensure that model updates don't regress on known evasion techniques.

Red team findings also inform adversarial generation strategies. Once a red team discovers an attack pattern, you can automate variations of that pattern. If a red team finds that content moderation can be bypassed by adding "asking for a friend" to requests, you can systematically generate eval examples that test whether that bypass still works. The red team discovers the pattern. The automated generation scales the pattern into a comprehensive eval subset.

## Domain-Specific Adversarial Patterns in Medical, Legal, and Financial Contexts

Adversarial patterns are domain-dependent. The ways a medical AI system can fail are different from the ways a legal AI system can fail. Effective adversarial eval sets target domain-specific vulnerabilities.

In medical domains, adversarial patterns include symptom masking, where serious conditions are described using casual language that downplays severity. A patient describing a heart attack might say "I have some tightness in my chest and feel a bit sweaty." The casual language—"some," "a bit"—masks the urgency. A symptom triage system trained on clearly serious descriptions might deprioritize this input. Your adversarial eval set should include serious conditions described in minimizing language. Polypharmacy interactions create corner cases where multiple medications combine to produce symptoms not associated with any single medication. Symptom overlap creates edge cases where benign and serious conditions present identically in early stages.

In legal domains, adversarial patterns include clause obfuscation, where standard legal concepts are expressed in non-standard language to evade detection. Indemnification clauses that don't use the word "indemnify." Non-compete clauses embedded in confidentiality sections. Termination rights buried in amendment provisions. Your adversarial eval set should include these disguised clauses. Conflicting provisions create corner cases where different sections of a contract contradict each other, requiring precedence rules or interpretation. Implied terms create edge cases where legal obligations are implied by context rather than stated explicitly.

In financial domains, adversarial patterns include transaction structuring designed to evade fraud detection. Splitting a large transaction into many small transactions below detection thresholds. Using third-party intermediaries to obscure the flow of funds. Timing transactions to span reporting periods. These patterns are adversarial by nature—they're designed by people trying to evade detection. Your eval set should include synthetic examples of these patterns even if they rarely appear in training data. Regulatory edge cases appear at boundaries between jurisdictions or regulatory categories. A transaction might be legal in one jurisdiction but illegal in another. A financial instrument might be classified differently depending on subtle structural features.

Domain-specific adversarial patterns also include terminology shifts. Medical terminology evolves. New diseases get named. Existing conditions get reclassified. Your system needs to handle both current and legacy terminology. Legal terminology varies by jurisdiction. A "fixture" in property law means something different in California than in New York. Financial terminology varies by market. "Prime" means different things in mortgage lending than in securities trading. Your adversarial eval set should include examples using non-standard, archaic, or jurisdiction-specific terminology to test whether the system has learned concepts rather than keyword matching.

Cultural and linguistic variation creates adversarial patterns in all domains. A medical system serving diverse populations encounters symptom descriptions influenced by cultural beliefs about illness. Some cultures describe mental health symptoms in physical terms. Some describe pain using metaphors that don't translate literally. Your eval set should include culturally varied descriptions if your production population is diverse. Legal systems encounter contracts drafted in translated English where idiomatic legal phrases are rendered literally and awkwardly. Financial systems encounter transactions described using local conventions and units.

## Maintaining Adversarial Sets as Models Improve

Adversarial eval sets have a lifecycle. When you first build an adversarial set targeting weaknesses in model version 1, the set exposes many failures. You iterate on the model, addressing those weaknesses. By model version 3, many of the original adversarial examples are correctly handled. The eval set has become less adversarial. You need to evolve the adversarial set to continue stress-testing the current model's boundaries.

The maintenance process starts with saturation detection. After each model update, you run the full adversarial eval set and identify which adversarial subsets have become saturated—achieved accuracy above 95% for three consecutive model versions. Saturated subsets are no longer stressing the model. They're testing solved problems. You can retire these subsets or archive them as historical markers of vulnerabilities that have been addressed.

New adversarial generation replaces retired subsets. You analyze current model failures in production to identify new vulnerability patterns. You run red team sessions focused on the current model's weaknesses. You apply perturbation techniques that target errors the current model makes. These new adversarial examples become the next generation of the adversarial eval set. The process is continuous. As the model improves, the adversarial set evolves to remain challenging.

Versioning adversarial eval sets separately from standard eval sets is important. Your standard eval set measures production performance on representative inputs. Your adversarial eval set measures robustness to attacks and edge cases. These serve different purposes and have different baselines. A standard eval set might target 95% accuracy as the production threshold. An adversarial eval set might target 80% accuracy, accepting that adversarial examples are inherently harder. Reporting the two metrics separately makes it clear that you're measuring different things.

Adversarial set maintenance also includes adjusting difficulty. As models improve, you can increase the aggressiveness of perturbations. Early models might fail on single-word synonym replacement. Later models require multi-word paraphrase and structural changes to fail. You calibrate perturbation severity to maintain a target failure rate. If your adversarial set is too easy and the model scores 98%, you're not learning anything. If it's too hard and the model scores 45%, you can't distinguish between model versions. Target a range where model improvements produce measurable gains—somewhere between 70% and 90% accuracy on adversarial examples.

Tracking adversarial pattern evolution provides product insight. When a specific adversarial pattern that used to cause failures is now handled robustly, you can document this as a capability improvement. When a new adversarial pattern emerges that causes failures, you can document this as a vulnerability requiring attention. The history of adversarial patterns and model responses becomes a record of the model's evolving robustness. This is valuable for trust and safety reviews, security audits, and regulatory compliance documentation.

## Using Adversarial Eval Results to Prioritize Model Improvements

Adversarial eval results don't just measure robustness. They guide model development. When your adversarial eval set reveals a category of failures, you have a prioritization decision. Do you address this vulnerability immediately, or do you accept the risk and focus on other improvements? The decision depends on the severity of the failure, the frequency in production, and the cost of mitigation.

Severity assessment asks what happens when this adversarial example causes a failure in production. A content moderation system that fails to detect a coded hate speech message has high severity. A document classifier that misclassifies a benign edge case has low severity. Severity determines urgency. High-severity failures become immediate priorities. Low-severity failures are tracked but may not justify model changes.

Frequency estimation asks how often this adversarial pattern appears in production. If red teamers found a creative attack pattern that requires ten specific conditions to align, it might never occur in real usage. If the adversarial pattern is a simple paraphrase of common inputs, it will occur frequently. Frequency determines impact. High-frequency adversarial patterns affect many users. Low-frequency patterns affect few users. You prioritize high-frequency, high-severity failures first.

Mitigation cost asks how much effort is required to address the vulnerability. Some adversarial patterns can be fixed with data augmentation—adding more training examples of that pattern. Some require architectural changes, new features, or human-in-the-loop workflows. You balance cost against severity and frequency. A high-severity, high-frequency vulnerability justifies expensive mitigation. A low-severity, low-frequency vulnerability might not.

The prioritization framework produces a roadmap. High-severity, high-frequency failures become P0 issues addressed in the next model iteration. High-severity, low-frequency failures become P1 issues addressed when feasible. Low-severity, high-frequency failures become P2 issues that improve user experience but aren't critical. Low-severity, low-frequency failures are documented but not actively addressed. This prevents you from chasing every possible edge case while ensuring that critical vulnerabilities get attention.

Adversarial eval sets also inform data collection priorities. If the eval set reveals that your model fails on medical symptom descriptions using informal language, you prioritize collecting more informal symptom descriptions for training. If the eval set reveals failures on contracts using non-standard clause language, you prioritize finding real contracts with those patterns. The adversarial eval set becomes a specification for what training data you need.

## Balancing Adversarial and Representative Eval Sets

A common mistake is to conflate adversarial eval performance with production performance expectations. Teams see 73% accuracy on the adversarial eval set and panic, thinking their production system will achieve 73% accuracy. But adversarial eval sets are deliberately harder than production data. They're not meant to estimate production accuracy. They're meant to identify vulnerabilities.

You maintain two separate eval sets with two separate purposes and two separate performance targets. The representative eval set samples from the expected production distribution. It estimates production performance. Your target might be 94% accuracy. The adversarial eval set samples from the adversarial and edge case distribution. It measures robustness. Your target might be 78% accuracy. Both metrics are reported, and both are tracked over time, but they're not compared directly.

The relative size of the two sets depends on your risk profile. A high-stakes system in a regulated domain might have a large adversarial set—500 examples—and a smaller representative set—300 examples—because you care more about robustness than average performance. A consumer application might have a large representative set—1,000 examples—and a smaller adversarial set—200 examples—because you care more about typical user experience than edge case handling.

The reporting cadence differs too. You might run the representative eval set daily or weekly as part of continuous integration. You run the adversarial eval set monthly or per release, treating it as a deep robustness check rather than a continuous monitor. This reflects the different purposes. Representative eval is a sanity check. Adversarial eval is a stress test.

Building and maintaining adversarial eval sets requires a mindset shift. You stop trying to build an eval set that represents your data and start trying to build an eval set that breaks your model. You embrace red team thinking. You actively seek out edge cases and construct examples designed to fail. This feels counterintuitive. But it's the only way to find the boundaries of your model's capabilities before your users find them in production. An adversarial eval set is your controlled experiment in failure. It lets you discover, measure, and address vulnerabilities in a safe environment before they cause production harm.

Next, we conclude the chapter on evaluation dataset construction by examining how to document, version, and share your eval sets to ensure reproducibility, enable collaboration, and support auditing and compliance requirements.
