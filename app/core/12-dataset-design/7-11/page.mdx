# 7.11 — Transfer and Multi-Task Dataset Design

Most teams build separate datasets for every task. This is waste.

By mid-2025, a legal technology company had fine-tuned six different models: one for contract clause extraction, one for risk summarization, one for compliance checking, one for deadline extraction, one for party identification, and one for obligation tracking. Each model had its own dataset, its own training pipeline, its own evaluation set. Each dataset required specialized labeling. The models achieved acceptable performance on their individual tasks, but the company faced escalating costs and maintenance burden. When a new task emerged—extracting payment terms—they started building a seventh dataset and training an eighth model.

The alternative is **transfer learning**: training one model on multiple related tasks simultaneously or using data from one task to improve performance on another. The legal tech company eventually rebuilt their system around a single multi-task model trained on a unified dataset covering all six original tasks plus four new ones. The combined dataset was smaller than the sum of the original six. Performance on individual tasks held steady or improved. Maintenance burden dropped by half.

## The Transfer Learning Premise

Transfer learning works because tasks share underlying structure. Contract clause extraction and risk summarization both require understanding legal language, identifying key concepts, and reasoning about document structure. These skills are not task-specific. A model that learns them in one context can apply them in another.

The degree of transfer depends on task similarity. Extracting clauses from contracts transfers well to extracting sections from legislation—both involve identifying boundaries in legal text. It transfers poorly to generating marketing copy—the domain, structure, and objectives are unrelated. Effective transfer requires shared input types, shared reasoning patterns, or shared output formats.

Data efficiency is the primary benefit. Training a model on task A often improves its performance on task B, even if task B's training set is small. A model trained on 50,000 customer support conversations and 5,000 technical troubleshooting exchanges will handle troubleshooting better than a model trained on troubleshooting alone. The support data teaches conversational structure, context tracking, and user intent recognition—skills directly applicable to troubleshooting.

Multi-task training amplifies this effect. Instead of training separate models and hoping for implicit transfer, you train one model on all tasks simultaneously. The model learns a shared representation that captures commonalities across tasks and task-specific parameters that handle differences. This approach works best when tasks are related but not identical.

## Shared Formats and Consistent Quality

Multi-task datasets require format consistency. If task A represents inputs as questions and outputs as answers, while task B represents inputs as commands and outputs as structured logs, the model must learn two different formatting conventions. This adds complexity without adding capability.

A unified format simplifies learning. One common structure is the instruction-following format: each example includes a task description, an input, and an expected output. The task description specifies what to do—"extract contract clauses," "summarize risks," "identify parties"—and the model learns to condition its behavior on this instruction. The input and output structure remains constant across tasks. This format scales easily to dozens of tasks.

Consistency in quality standards is harder to achieve but equally important. If your contract extraction dataset is labeled by legal experts with strict quality requirements, while your summarization dataset is labeled by generalist annotators with loose guidelines, the model learns that different tasks have different quality bars. This can lead to uneven performance: high precision on extraction, sloppy outputs on summarization.

Quality consistency requires unified labeling guidelines across tasks. Define what constitutes a correct output, an acceptable output, and an unacceptable output in terms that apply to all tasks. For text generation tasks, this might mean: correct outputs are factually accurate, complete, and well-formed; acceptable outputs have minor phrasing issues but preserve meaning; unacceptable outputs contain errors or omissions. Apply these standards uniformly.

Dataset mixing ratios determine how much the model learns from each task. If you combine 100,000 examples of task A with 1,000 examples of task B and train without weighting, the model will overwhelmingly optimize for task A. Task B will be undertrained. To balance learning, you can oversample task B, downsample task A, or assign higher loss weight to task B examples. The right ratio depends on task importance, difficulty, and similarity.

## When Transfer Helps vs When It Introduces Noise

Not all multi-task combinations improve performance. Sometimes combining datasets degrades results on individual tasks compared to single-task training.

Negative transfer occurs when learning one task interferes with learning another. If task A requires brevity and task B requires verbosity, training on both simultaneously can confuse the model about when to be concise and when to elaborate. If task A requires formal tone and task B requires casual tone, the model may blend the two inappropriately. The more dissimilar the tasks, the higher the risk of negative transfer.

Task conflict is most severe when outputs for the same input should differ across tasks. Imagine training a model on two tasks: answering medical questions for patients and answering medical questions for doctors. The input might be identical—"What is the prognosis for stage 2 hypertension?"—but the output should differ in technicality, detail, and terminology. If your dataset doesn't clearly signal which task is active, the model will produce a blended response appropriate for neither audience.

Shared adversarial patterns can propagate across tasks. If your summarization dataset includes examples where the model must avoid editorializing, and your sentiment analysis dataset includes examples where the model must explicitly editorialize, the model receives contradictory training signals. One task punishes a behavior the other rewards. Resolution depends on whether task conditioning is strong enough to override the contradiction. Often it's not.

Transfer helps most when tasks share skills but differ in application. Code generation and code review share understanding of syntax, semantics, and common patterns but apply that understanding differently. Training on both improves the shared representation—better code understanding—while task-specific parameters handle generation versus critique. The shared foundation is stronger than either task would produce alone.

Domain coherence amplifies positive transfer. A model trained on five different legal tasks will learn legal reasoning and terminology that benefit all five. A model trained on one legal task, one medical task, one customer support task, one software engineering task, and one creative writing task will learn little that transfers across all five. The domains are too disparate. Multi-task training works best within a domain, not across unrelated domains.

## The Catastrophic Forgetting Problem

Catastrophic forgetting is what happens when you fine-tune a model on task B after training it on task A, and it forgets how to do task A. This is the central challenge in sequential multi-task learning.

The mechanism is straightforward. Fine-tuning updates the model's parameters to minimize loss on the new task. If the new task's data distribution differs significantly from the old task's, those parameter updates can overwrite the knowledge needed for the old task. The model's capacity is finite. When you optimize aggressively for B, you discard some of what it learned for A.

The severity of forgetting depends on several factors. Large distribution shifts cause more forgetting than small ones. Long fine-tuning runs cause more forgetting than short ones. High learning rates cause more forgetting than low rates. Models with more parameters forget less because they have more capacity to store task-specific knowledge in separate subnetworks.

Mitigation strategies fall into three categories: data replay, regularization, and architecture.

Data replay means including examples from task A when fine-tuning on task B. Instead of training exclusively on new data, you mix in a subset of old data—perhaps 10% to 30% of each batch. This continually reminds the model how to perform task A, preventing full forgetting. The cost is slower adaptation to task B because you're splitting gradient updates between tasks.

Regularization approaches penalize changes to parameters that were important for task A. Elastic Weight Consolidation is the best-known technique: after training on task A, you identify which parameters contributed most to task A's performance, then add a regularization term during task B training that discourages large changes to those parameters. The model can still adapt to task B but must do so without disrupting task A's critical weights.

Architecture approaches dedicate different parameters to different tasks. Adapter layers are small, task-specific modules inserted into a frozen base model. When you train on task B, you freeze task A's adapter and train a new adapter for task B. The base model's shared knowledge remains intact, and task-specific behavior is isolated. This approach scales well to many tasks but requires more careful engineering.

For most teams, data replay is the simplest and most effective solution. Maintain a reservoir of examples from all previous tasks, and sample from it during training on new tasks. The reservoir doesn't need to be large—5,000 to 10,000 examples per task is often sufficient to prevent catastrophic forgetting even when training on 100,000 new examples.

## Auxiliary Tasks That Improve Primary Task Performance

Some tasks don't matter in themselves but improve performance on other tasks when trained jointly. These are auxiliary tasks, and adding them to your dataset can be a high-leverage improvement.

Denoising is a common auxiliary task. You take clean text, corrupt it with random deletions, substitutions, or shuffles, and train the model to reconstruct the original. This teaches robust representation learning—the model must learn to ignore noise and extract meaning from degraded inputs. A model trained on denoising alongside a primary task like summarization will often produce better summaries because it's learned to focus on semantic content rather than surface form.

Next-token prediction on domain-relevant text is another effective auxiliary task. If your primary task is legal contract analysis, including a next-token prediction objective on a large corpus of legal text helps the model build stronger legal language understanding. This is essentially continued pretraining framed as an auxiliary task. The model learns domain knowledge and terminology that improve primary task performance even though next-token prediction isn't directly useful.

Classification tasks can serve as auxiliary tasks for generation tasks. If your primary task is generating product descriptions, an auxiliary task might be classifying product categories or predicting product attributes. The classification tasks teach the model to extract and represent product features, which improves the coherence and accuracy of generated descriptions. The auxiliary task provides grounding.

Contrastive tasks are useful for retrieval and similarity judgments. You train the model to distinguish similar pairs from dissimilar pairs—for example, paraphrase pairs versus random pairs. This teaches the model to build representations where semantically similar inputs are close together. If your primary task involves semantic search or duplicate detection, a contrastive auxiliary task sharpens the model's similarity judgments.

The auxiliary task ratio should be modest. If 80% of your training is the primary task and 20% is auxiliary tasks, the auxiliary tasks provide useful grounding without distracting from the main objective. If you flip that ratio, the model optimizes for auxiliary tasks at the expense of the primary task. The auxiliary tasks are supporting actors, not leads.

## Multi-Task Dataset Construction Workflow

Building a multi-task dataset is not just concatenating single-task datasets. It requires deliberate design to balance tasks, align formats, and ensure quality consistency.

Start by defining the task set. Which tasks will the model handle? The answer should be driven by user needs and task relatedness. If users need contract extraction, summarization, and compliance checking, those three tasks belong together. Adding a fourth unrelated task—say, sentiment analysis of product reviews—dilutes focus without adding value unless your users actually need it.

Next, standardize the format. Choose an input-output structure that accommodates all tasks. The instruction format works for most cases: a task instruction, a context or input, and an expected output. Map each task's existing data into this format. Extraction becomes "Extract all payment terms from the following contract" plus the contract text plus the extracted terms. Summarization becomes "Summarize the key risks in the following contract" plus the contract text plus the summary. The structure is uniform even though the tasks differ.

Then align quality standards. Review a sample of examples from each task and define what good looks like. Write guidelines that apply across tasks. Train your labeling team on all tasks using these unified guidelines, not task-specific instructions. This ensures that the quality bar is consistent.

Sample and mix the datasets. Decide how many examples of each task to include in the final dataset. This depends on task importance, difficulty, and data availability. If task A is twice as important as task B, or twice as hard to learn, you might include twice as many examples. If task C has limited data, you might oversample it to prevent it from being undertrained.

Validate transfer empirically. Train a multi-task model on your combined dataset and evaluate it on each task individually. Compare performance to single-task baselines. If multi-task performance matches or exceeds single-task performance, transfer is working. If it lags significantly on any task, investigate. You may need to adjust mixing ratios, improve format consistency, or add task-specific signals.

Iterate based on performance gaps. If the model struggles with task C, add more task C examples, increase task C's loss weight, or include auxiliary tasks that strengthen the skills task C requires. Multi-task dataset design is empirical. The first version rarely achieves optimal balance. Refinement based on evaluation results is essential.

## Task Conditioning and Instruction Clarity

The model must know which task it's performing. This seems obvious but is often mishandled.

Explicit task instructions are the clearest signal. Each example begins with a directive: "Summarize the following text," "Extract all dates from the following email," "Classify the sentiment of the following review." The instruction disambiguates the task. At inference time, you provide the same instruction format, and the model knows what to do.

Implicit task signals are riskier. If you train on task A using one input format and task B using another, the model might learn to infer the task from the format. This works until you encounter an ambiguous input that could belong to either task. The model guesses, sometimes wrongly. Explicit instructions eliminate this ambiguity.

Instruction phrasing consistency matters. If your training data uses ten different phrasings for the same task—"Summarize this text," "Write a summary," "Provide a brief summary," "Condense the following," and so on—the model must learn that all ten mean the same thing. This is learnable but inefficient. Using a single canonical phrasing per task accelerates learning and reduces variability.

Task-specific prompts at inference time must match training-time instructions. If you trained with "Extract all named entities from the following text" and at inference time you prompt with "Find the entities in this document," the model may underperform due to distribution shift. The phrasing doesn't need to be perfectly identical—models generalize across small variations—but large deviations hurt.

## Multi-Task Evaluation and Task-Specific Metrics

Evaluating a multi-task model requires task-specific metrics aggregated into an overall score. You cannot evaluate summarization, extraction, and classification with the same metric.

Task-specific evaluation uses the same metrics you'd use for single-task models. Summarization might be evaluated with ROUGE, human preference ratings, or factual consistency checks. Extraction might use precision and recall. Classification uses accuracy or F1. Each task gets the metrics appropriate to its output type and quality requirements.

Aggregating these metrics into a single score is necessary for model selection but introduces trade-offs. A simple average treats all tasks equally. A weighted average lets you prioritize important tasks. A minimum-score threshold ensures no task falls below acceptable performance. The right aggregation depends on your deployment requirements.

Overall performance can mask task-specific failures. A model might achieve 85% average accuracy across five tasks but only 60% on one critical task. The average looks acceptable, but the weak task is a blocker. Always examine per-task metrics, not just aggregates. Identify underperforming tasks and address them specifically, either by adding data, adjusting loss weights, or including auxiliary tasks that strengthen relevant skills.

Generalization to new tasks is the ultimate test of transfer learning. If your multi-task model performs well on tasks it was trained on, that's expected. If it adapts quickly to new, related tasks with minimal fine-tuning, that's evidence of strong transfer. Test this by holding out one task during training, then fine-tuning on a small sample of that task and measuring performance. A model with good transfer will reach acceptable performance with far fewer examples than a model trained from scratch.

## Bridge to Active Learning

Multi-task datasets grow over time as you add tasks, refine examples, and respond to model failures. The question is which examples to add next. Random sampling is inefficient. You want to prioritize the examples that will improve the model most. The next subchapter introduces the integration loop: the system that routes production failures back into your dataset, applies active learning to select high-value examples for labeling, and prioritizes annotation effort where it matters. This loop is how training datasets evolve from static artifacts into living systems that improve continuously.
