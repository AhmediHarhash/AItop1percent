# 1.6 — Dataset Ownership: Who Builds, Maintains, and Governs

Six months of degraded performance and a one-hundred-eighty-thousand-dollar emergency rebuild. That was the cost of a single organizational mistake: no one owned the dataset. The fraud detection system worked perfectly at launch because a cross-functional team had built a comprehensive, high-quality dataset. Then the team dissolved. Engineering thought Product was maintaining it. Product thought Engineering was handling it. Domain experts moved on. Legal was never told the dataset needed regulatory updates. The dataset became an orphan. New fraud patterns emerged and went undetected. Transaction distributions shifted and the dataset did not shift with them. Regulations changed and the labels became non-compliant. By the time anyone noticed, the damage was severe and the fix was expensive. The lesson is brutal but simple: datasets without clear ownership decay rapidly, and the decay is invisible until it becomes a crisis.

Datasets are not static artifacts. They are living systems that require ongoing maintenance, governance, and evolution. Without clear ownership, datasets decay. Labels become outdated. Coverage becomes stale. Distributions drift away from production reality. Compliance requirements change and the dataset falls out of alignment. Ownership means someone is accountable for dataset quality, freshness, coverage, and compliance. Ownership means someone has the authority to allocate resources to dataset work and the responsibility to ensure that work happens. Ownership means someone monitors dataset health and acts when problems emerge. Without ownership, datasets fail slowly and expensively.

## The Ownership Vacuum Problem

The ownership vacuum happens when everyone assumes someone else is responsible for the dataset. Engineering teams assume Product defines what data is needed and ensures quality. Product teams assume Engineering handles data collection and maintenance because it is technical work. Domain experts assume they are consultants who provide input when asked, not owners who drive dataset decisions. Legal assumes datasets are an engineering concern unless a specific compliance question arises. The result is that no one is accountable, and the dataset drifts into dysfunction.

This vacuum is common because datasets span multiple disciplines. A good dataset requires product judgment about what use cases to cover, engineering capability to collect and process data, domain expertise to label accurately, and legal oversight to ensure compliance. No single role naturally owns all these dimensions. In the absence of explicit ownership assignment, each discipline handles its piece reactively rather than proactively. Product specifies initial requirements but does not monitor ongoing relevance. Engineering builds collection pipelines but does not validate that collected data meets product needs. Domain experts label batches when asked but do not track labeling consistency over time. Legal reviews datasets when someone escalates a concern but does not proactively audit for emerging compliance risks.

The ownership vacuum manifests in specific failure modes. Datasets are not versioned properly because no one owns versioning. Labeling guidelines are not updated when policies change because no one owns guideline maintenance. Coverage gaps are not identified and filled because no one owns coverage monitoring. Data drift between training and production is not detected because no one owns drift monitoring. Compliance reviews do not happen on a regular schedule because no one owns compliance cadence. Each failure is a coordination failure, and coordination failures compound.

The cost of the ownership vacuum is not just the immediate cost of dataset decay. It is the cost of rediscovering the same problems repeatedly. Every few months, someone notices that dataset quality has degraded. A task force is assembled. The dataset is patched. The task force dissolves. A few months later, the cycle repeats. This is expensive and demoralizing. Teams lose confidence in the dataset. They start building shadow datasets for specific projects. The dataset fragments into multiple incompatible versions. The fragmentation creates new coordination problems. The ownership vacuum creates a downward spiral.

The vacuum also creates knowledge loss. The people who built the initial dataset understand the design decisions, the labeling edge cases, and the known limitations. When those people move on and no one is assigned to maintain institutional knowledge about the dataset, that knowledge evaporates. New team members cannot understand why the dataset is structured a certain way. They make changes that violate original design principles. They introduce inconsistencies because they do not understand the historical context. Knowledge loss compounds dataset decay.

The political dimension of the ownership vacuum is significant. Datasets often become political battlegrounds when ownership is unclear. Product wants the dataset to prioritize certain use cases. Engineering wants the dataset to fit existing infrastructure. Domain experts want the dataset to reflect domain purity. Legal wants the dataset to minimize compliance risk. Without a clear owner to adjudicate these competing demands, dataset decisions are made through organizational power dynamics rather than through principled analysis. The result is datasets that satisfy no one and serve no purpose well.

## RACI for Datasets

The RACI framework clarifies dataset ownership by assigning four roles: Responsible, Accountable, Consulted, and Informed. Responsible means doing the work. Accountable means ensuring the work gets done and is done correctly. Consulted means providing input and expertise. Informed means being kept updated on progress and decisions. Every dataset must have a clear RACI assignment for every major dataset activity.

For dataset strategy and roadmap, Product is typically Accountable. Product defines what use cases the dataset must support, what coverage is required, and what quality thresholds are necessary. Product prioritizes dataset work against other product work. Engineering is Responsible for building the systems that enable dataset work, but Product is Accountable for ensuring those systems support product needs. Domain experts are Consulted on coverage and quality requirements. Legal is Consulted on compliance constraints. Leadership is Informed of dataset strategy and resource requirements.

For dataset creation and initial collection, Engineering is typically both Responsible and Accountable. Engineering builds the data pipelines, collection tools, and storage systems. Engineering ensures data is collected with appropriate metadata, formatting, and structure. Product is Consulted to ensure collected data aligns with product requirements. Domain experts are Consulted to ensure collected data is suitable for labeling. Legal is Consulted to ensure collection complies with privacy and licensing requirements.

For dataset labeling and annotation, domain experts are typically Responsible, with Product or a dedicated data team Accountable. Domain experts do the labeling because they have the expertise to make correct judgments. But domain experts are often not trained in dataset management, so someone must be Accountable for ensuring labeling happens on schedule, follows guidelines, and maintains consistency. Product can play this role if dataset work is a core product activity. Alternatively, a dedicated data team or ML engineering team can be Accountable. Engineering is Consulted on labeling tooling needs. Legal is Consulted on labeling guidelines for sensitive or regulated content.

For dataset maintenance and updates, the ownership is often split. A data steward role, which may sit in Product, Engineering, or a dedicated data team, is Accountable for ongoing dataset health. This role monitors dataset quality metrics, identifies when coverage gaps emerge, and initiates labeling or collection work to address issues. Engineering is Responsible for maintaining the infrastructure that supports dataset updates. Domain experts are Responsible for executing relabeling when policies or definitions change. Product is Consulted on prioritization of maintenance work. Legal is Consulted on compliance-driven updates.

For dataset governance and compliance, Legal is typically Accountable, with a data governance team or privacy team Responsible for audits and reviews. Legal ensures that datasets comply with GDPR, HIPAA, copyright law, and other regulations. Legal ensures that consent and licensing are appropriate for dataset use. Engineering is Responsible for implementing technical controls that support governance, such as data retention policies and access controls. Product is Consulted on the impact of compliance requirements on dataset utility. Domain experts are Informed of compliance constraints that affect labeling.

The key principle is that Accountable roles must be singular and empowered. You cannot have shared accountability. If Product and Engineering are both Accountable for dataset quality, neither is truly Accountable. One role must have final authority and responsibility. That role must have the resources and organizational support to execute. Assigning accountability without providing budget and headcount is performative, not real.

The RACI framework must be documented and communicated. A RACI matrix should be created during dataset planning and shared with all stakeholders. The matrix should be reviewed quarterly to ensure it remains accurate as teams and priorities evolve. When new team members join, they should be onboarded to the RACI framework so they understand their role and who to coordinate with. The framework is only valuable if it is known and followed.

The escalation paths must be clear within the RACI framework. When the Responsible party encounters a blocker, who do they escalate to? When the Accountable party needs to make a decision that affects other teams, what is the decision-making process? When Consulted parties disagree on a recommendation, how is the disagreement resolved? These questions must be answered as part of the RACI definition, or the framework will break down when coordination is most needed.

## Who Should Own What: Eval vs Training vs Production Datasets

Different dataset types require different ownership models. Evaluation datasets, training datasets, and production logging datasets serve different purposes and have different ownership needs. Trying to apply a single ownership model to all dataset types creates misalignment.

Evaluation datasets should be owned by Product or by a dedicated evaluation team that reports to Product. Evaluation datasets define success. They operationalize product requirements into measurable criteria. Product is best positioned to ensure that evaluation datasets reflect real user needs and real product priorities. If Engineering owns evaluation datasets, there is a risk that eval datasets optimize for what is easy to measure rather than what matters to users. If domain experts own evaluation datasets, there is a risk that eval datasets reflect domain purity rather than product pragmatism. Product ownership ensures that evaluation datasets answer the question: does this system deliver the product value we committed to?

Product ownership of eval datasets means Product is Accountable for ensuring eval datasets are representative of production traffic, cover edge cases that matter to users, and evolve as product requirements evolve. Engineering is Responsible for building the systems that make eval dataset collection and versioning easy. Domain experts are Consulted to ensure eval criteria are correctly interpreted. Legal is Consulted to ensure eval datasets comply with privacy and licensing requirements.

Training datasets should be owned by Engineering or by a dedicated ML engineering team. Training datasets are technical artifacts that feed model training pipelines. Engineering is best positioned to ensure that training datasets are formatted correctly, versioned properly, and integrated into training workflows. If Product owns training datasets, there is a risk of misalignment between dataset format and training infrastructure. If domain experts own training datasets, there is a risk that datasets are annotated beautifully but are not in a format that training code can consume.

Engineering ownership of training datasets means Engineering is Accountable for ensuring training datasets are clean, deduplicated, and structured for efficient training. Product is Consulted to ensure training datasets support product use cases. Domain experts are Responsible for labeling, with Engineering providing the tools and processes that make labeling efficient and consistent. Legal is Consulted on compliance, and Engineering is Responsible for implementing data retention, access controls, and other compliance mechanisms.

Production logging datasets should be owned by a cross-functional team with Engineering Responsible for collection and storage, and Product Accountable for defining what to log and how to use logged data. Production logs are both a technical system and a product intelligence source. Engineering must ensure logs are collected reliably, stored securely, and accessible for analysis. Product must ensure logs capture the information needed to understand user behavior, identify failure modes, and inform dataset improvements.

Production logging ownership is often the most neglected. Teams treat logging as an afterthought. They log whatever is easy to log, not what is important to log. This creates a cold start problem: when you need production data to improve your dataset, you discover you have not been logging the necessary information. Establishing clear ownership of production logging from the start prevents this problem. Product defines logging requirements. Engineering implements logging infrastructure. Together they review logged data regularly to ensure it is complete and actionable.

The distinction between these dataset types must be maintained in organizational structure and processes. A single dataset team that owns all three types will face conflicting priorities. Evaluation datasets need stability so benchmarks remain comparable over time. Training datasets need frequent updates to incorporate new examples. Production logging needs real-time responsiveness to capture evolving user behavior. These needs conflict. Separate ownership allows each dataset type to be optimized for its purpose.

The interfaces between these dataset types must be managed carefully. Production logs feed into training dataset creation. Evaluation datasets must be kept separate from training datasets to prevent overfitting. Training dataset updates must be validated against evaluation datasets before deployment. These workflows cross ownership boundaries. Clear RACI definitions prevent these interfaces from becoming coordination bottlenecks.

## The Role of Product in Dataset Governance

Product's role in dataset governance is to ensure datasets serve product goals. This means Product must be able to answer several questions about any dataset: What product use cases does this dataset support? What quality thresholds does this dataset need to meet for the product to succeed? How do we know if this dataset is representative of real user needs? What happens to the product if this dataset decays?

Product must advocate for dataset investment in roadmap and resource allocation discussions. Engineering roadmaps often prioritize features over datasets because features are visible to users and datasets are not. Product must articulate why dataset work is foundational to feature success. If a new feature requires a new dataset or an expansion of an existing dataset, Product must ensure that dataset work is scoped, resourced, and scheduled as part of the feature delivery plan. Treating dataset work as an engineering detail that will somehow happen automatically is a recipe for failure.

Product must also own the definition of dataset quality metrics. What does good look like for this dataset? Is it accuracy of labels? Is it coverage of edge cases? Is it recency of examples? Is it demographic representativeness? Product translates user needs and business requirements into dataset quality criteria, and then holds the team accountable to those criteria. Without Product-owned quality metrics, dataset quality becomes a subjective engineering judgment rather than a measurable product requirement.

Product participates in dataset reviews. Just as Product reviews feature specs and design mocks, Product should review dataset samples, labeling guidelines, and coverage analyses. This is not a rubber-stamp process. This is Product verifying that the dataset being built aligns with the product being built. Misalignments caught early are cheap to fix. Misalignments discovered after months of labeling work are expensive.

Product is also the voice of the user in dataset discussions. Engineers and domain experts can optimize for technical elegance or domain correctness while missing what users actually care about. Product brings user research, usage data, and customer feedback into dataset decisions. If users are struggling with a particular interaction pattern, Product ensures the dataset includes examples of that pattern. If a particular failure mode is causing user churn, Product ensures the dataset is strengthened in that area. Product keeps the dataset grounded in product reality.

Product must balance short-term feature delivery pressure against long-term dataset health. It is tempting to deprioritize dataset maintenance when there are urgent feature requests. Product must resist this temptation. Dataset decay is slow and invisible until it causes a crisis. Product must maintain discipline about dataset maintenance even when the immediate pressure is elsewhere. This requires organizational maturity and long-term thinking.

Product must also manage the tension between dataset comprehensiveness and dataset focus. A dataset that tries to cover every possible use case becomes unwieldy and expensive to maintain. A dataset that is too narrowly focused cannot support product evolution. Product must define the appropriate scope: comprehensive enough to support current and near-term planned use cases, focused enough to remain manageable. This scoping decision requires product judgment about what use cases matter most.

## The Role of Engineering in Dataset Infrastructure

Engineering's role in dataset governance is to build the infrastructure that makes good dataset practices easy and bad dataset practices hard. This means Engineering must provide tools for dataset versioning, annotation, quality monitoring, and compliance. Engineering must build pipelines that collect production data reliably. Engineering must build systems that make it easy to compare dataset versions, audit labeling decisions, and track dataset lineage.

Engineering must also enforce technical standards for datasets. All datasets must be versioned. All datasets must have metadata describing provenance, labeling guidelines, and intended use. All datasets must be stored in formats that support efficient access and analysis. All datasets must have associated quality metrics that are computed automatically and surfaced in dashboards. Engineering builds the systems that make these standards enforceable rather than aspirational.

Engineering provides the annotation tooling that domain experts use for labeling. This tooling must be designed with labeling quality in mind. It should surface labeling guidelines inline with examples. It should flag inconsistencies, such as when a labeler is labeling similar examples differently. It should support multi-labeler workflows with adjudication for disagreements. It should track labeler performance and identify when a labeler needs additional training or when guidelines need clarification. Good tooling raises labeling quality. Bad tooling allows inconsistency to creep in undetected.

Engineering also builds the monitoring systems that detect dataset decay. These systems compare current production traffic distributions to dataset distributions and alert when drift exceeds thresholds. They track model performance on recent production data and alert when performance degrades. They monitor data freshness and alert when the dataset has not been updated within expected intervals. Monitoring turns dataset maintenance from a reactive scramble into a proactive process.

Engineering's role is not to make dataset decisions, but to make those decisions executable. Product and domain experts decide what data to collect and how to label it. Engineering ensures those decisions can be implemented reliably, at scale, with quality controls. This is a partnership, not a handoff. Engineering must push back when Product or domain experts request dataset practices that are technically infeasible or that create compliance risk. But Engineering must also proactively propose technical solutions that make dataset work easier and more reliable.

Engineering must build dataset infrastructure with long-term maintainability in mind. The systems that support datasets will need to evolve as dataset needs evolve. The infrastructure must be modular, well-documented, and designed for change. Infrastructure that is tightly coupled to current dataset structure will become a bottleneck when dataset structure needs to change. Engineering must anticipate evolution and build for flexibility.

Engineering must also build dataset infrastructure with security and compliance in mind. Datasets often contain sensitive information. Access controls must be in place to ensure only authorized users can view or modify datasets. Audit logs must track who accessed what data and when. Encryption must protect datasets at rest and in transit. Data retention policies must be enforced automatically to ensure datasets are deleted when required. These are not optional features. They are foundational requirements.

## The Role of Domain Experts in Dataset Quality

Domain experts are the guardians of label correctness. They have the expertise to distinguish correct from incorrect, to interpret ambiguous cases, and to apply policy consistently. Domain experts are typically Responsible for labeling, but they must also be Consulted on labeling guideline creation, coverage assessment, and quality reviews.

Domain experts must be trained in dataset practices, not just in domain knowledge. A medical expert who is an excellent clinician may not be an excellent dataset annotator without training. Dataset annotation requires consistency, which means applying guidelines rigorously even when domain intuition suggests flexibility. It requires speed, which means making labeling decisions efficiently without excessive deliberation. It requires calibration, which means aligning with other annotators to ensure consistency across the team. These are skills that must be developed through training and practice.

Domain experts must also provide feedback on labeling guidelines. When guidelines are ambiguous or incomplete, domain experts are the first to know because they encounter edge cases that guidelines do not address. Domain experts must surface these issues so guidelines can be clarified. If domain experts silently apply their own judgment to fill guideline gaps, labeling consistency will suffer because different experts will fill gaps differently.

Domain experts should participate in dataset coverage assessments. They can identify gaps that data analysis alone might miss. A domain expert reviewing a medical dataset might notice that certain rare conditions are underrepresented, or that examples are biased toward certain demographics, or that certain clinical contexts are missing. This domain perspective is essential for ensuring datasets are comprehensive and representative.

Domain experts also play a critical role in dataset validation. Before a dataset is used for training or evaluation, domain experts should review a sample to verify that labels are correct and that examples are appropriate. This is a quality gate. A dataset that passes domain expert review is far more trustworthy than a dataset that has only been reviewed by engineers or product managers who lack domain expertise.

Domain experts must understand the downstream use of their labeling work. When domain experts understand how their labels will be used—what decisions the model will make, what consequences incorrect labels will have—they can make better labeling decisions. This requires Product and Engineering to communicate the product context to domain experts, not just hand them examples to label.

Domain experts must also participate in error analysis. When the model makes mistakes in production, domain experts should review those mistakes to determine whether they stem from labeling errors, guideline ambiguities, or model limitations. This feedback loop from production failures back to dataset improvements is critical for continuous quality improvement.

## The Role of Legal in Dataset Compliance

Legal's role in dataset governance is to ensure datasets comply with all applicable regulations and do not create legal risk. This means Legal must review datasets for privacy compliance, copyright compliance, data retention compliance, and fairness compliance. Legal must also review dataset collection practices, labeling practices, and usage practices to ensure they align with legal requirements.

Legal must be involved early in dataset planning, not late in dataset review. If Legal is only consulted after a dataset has been collected and labeled, Legal's feedback may require expensive rework. If Legal is consulted during dataset design, Legal can shape collection and labeling practices to ensure compliance from the start. Early Legal involvement is a risk mitigation strategy.

Legal must provide clear guidance on data retention. How long can you keep this dataset? When must it be deleted? What data must be excluded from the dataset due to privacy or consent limitations? Legal must translate regulatory requirements into operational requirements that Engineering can implement. Vague guidance like "comply with GDPR" is not actionable. Specific guidance like "delete all datasets containing EU personal data after 18 months unless explicit consent for longer retention has been obtained" is actionable.

Legal must also review labeling guidelines for regulated content. In content moderation, labeling guidelines interpret platform policies, which have legal implications. In healthcare, labeling guidelines interpret clinical standards, which may be defined by regulation. In finance, labeling guidelines interpret compliance categories, which are defined by law. Legal review ensures that labeling guidelines align with legal definitions and do not create liability.

Legal should conduct periodic audits of datasets to ensure ongoing compliance. Regulations change. A dataset that was compliant in 2024 may not be compliant in 2026 if new regulations have been enacted. The EU AI Act, which came into force in 2024 and began full enforcement in 2025, imposes new requirements on datasets used for high-risk AI systems. Legal must audit datasets against these evolving requirements and ensure datasets are updated or retired as necessary.

Legal's role is not to block dataset work. Legal's role is to enable dataset work within legal boundaries. Legal should propose compliant alternatives when initial dataset plans create legal risk. Legal should help Product and Engineering understand trade-offs between dataset utility and legal risk. Legal is a partner in dataset governance, not an obstacle.

Legal must also ensure that dataset licenses are appropriate for the intended use. If you are using third-party data, does the license permit AI training? Does it permit commercial use? Does it require attribution? Legal must review and approve all data sources before they are incorporated into datasets. Failure to do this creates copyright and licensing risk that can materialize years later.

Legal must coordinate with Privacy and Security teams on dataset governance. Privacy teams ensure compliance with data protection regulations like GDPR and CCPA. Security teams ensure datasets are protected from unauthorized access. These teams must work together with Legal to create a comprehensive governance framework. Siloed governance creates gaps where risks fall through the cracks.

## Organizational Patterns That Work

The most successful dataset governance models create a dedicated dataset stewardship role. This role, which may be called a data product manager, a dataset owner, or a data steward, is Accountable for dataset health across the full lifecycle. This person coordinates between Product, Engineering, domain experts, and Legal. This person monitors dataset quality metrics, initiates maintenance work, and ensures dataset evolution aligns with product evolution. This role is not a domain expert or an engineer or a product manager. It is a hybrid role that requires understanding of all three disciplines plus strong project management skills.

Organizations that lack a dedicated stewardship role often embed dataset ownership within product management. A product manager owns not just the feature roadmap but also the datasets that support those features. This works when dataset work is a small fraction of the product manager's responsibilities. It breaks down when datasets are large, complex, or require frequent updates. In those cases, dataset stewardship is a full-time job, and trying to add it to an already-full product management role leads to neglect.

Some organizations create a centralized data team that owns all datasets across the company. This team provides dataset infrastructure, annotation services, quality monitoring, and compliance reviews as a shared service. Product teams and engineering teams consume these services but do not build their own dataset capabilities. This model works well in large organizations where dataset work is substantial enough to justify a dedicated team. It works less well in small organizations where a centralized team becomes a bottleneck.

Other organizations use a federated model where each product team owns its own datasets, but shared standards, tooling, and governance processes ensure consistency. A central platform team provides the infrastructure and tooling. Each product team uses that infrastructure to build and maintain their own datasets according to shared standards. A central governance committee, with representatives from Legal, Privacy, and senior Engineering, reviews datasets for compliance and quality. This model preserves product team autonomy while preventing fragmentation.

The worst organizational pattern is the implicit model where no one is explicitly assigned ownership and everyone assumes it will somehow work out. It never works out. Datasets decay. Compliance lapses. Quality suffers. The implicit model is the ownership vacuum, and the ownership vacuum is expensive.

The organizational structure must also address career paths for dataset roles. If dataset stewardship is not a recognized career path with clear growth opportunities, talented people will not stay in these roles. Organizations must create leveling frameworks for dataset roles, define promotion criteria, and provide professional development opportunities. Dataset stewardship is specialized, valuable work. It should be recognized and rewarded accordingly.

## What Good Ownership Looks Like in Practice

Good ownership means dataset quality metrics are tracked and reviewed regularly. The dataset owner looks at label consistency rates, coverage metrics, data freshness, and production-to-dataset distribution comparisons every week or every month. When metrics degrade, the owner initiates corrective action. When coverage gaps are identified, the owner schedules annotation work to fill them. When production traffic shifts, the owner ensures the dataset shifts to match.

Good ownership means dataset evolution is planned and resourced. The dataset roadmap is integrated with the product roadmap. When a new feature is planned, the dataset implications are scoped as part of the feature spec. When a policy changes, the dataset updates are scheduled and executed. Dataset work is not an afterthought. It is planned work with allocated resources.

Good ownership means cross-functional collaboration is structured and effective. Product, Engineering, domain experts, and Legal have clear roles and clear communication channels. Dataset decisions are documented. Disagreements are resolved through defined escalation paths. Collaboration is not ad-hoc. It is systematic.

Good ownership means accountability is real. When a dataset fails, the owner is responsible. When a dataset succeeds, the owner is credited. This accountability creates the right incentives. Owners take dataset quality seriously because their performance is evaluated on it. They advocate for resources because they are held accountable for results. They build systems and processes that prevent failures because failures are their responsibility.

Good ownership means datasets are treated as products, not as byproducts. They have roadmaps, quality metrics, user feedback loops, and continuous improvement processes. They are versioned, documented, and maintained. They are investments that are managed actively, not assets that are built once and forgotten.

Good ownership means proactive communication about dataset status. The owner regularly communicates dataset health to stakeholders. When issues arise, stakeholders are notified promptly. When major changes are planned, stakeholders are consulted. Communication prevents surprises and builds trust.

Good ownership means learning from failures and successes. When a dataset-related failure occurs, the owner conducts a post-mortem to understand root causes and prevent recurrence. When a dataset improvement delivers exceptional results, the owner documents what worked so it can be replicated. Continuous learning drives continuous improvement.

Datasets without clear ownership decay rapidly. Datasets with clear, empowered ownership remain healthy and continue to deliver value over time. Ownership is not optional. It is foundational. Once ownership is established, the next step is assessing dataset maturity and planning the path to higher maturity. That is the subject we turn to next.
