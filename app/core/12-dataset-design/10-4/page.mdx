# 10.4 â€” Distribution Descriptions: What Your Data Looks Like

Two datasets can have the same size, the same schema, the same labels, and wildly different value. A fraud detection dataset with 500,000 transactions might be useless if 499,950 are legitimate and 50 are fraud. A customer support classification dataset with 100,000 labeled tickets might fail immediately in production if 95% of the training data came from one product line and production traffic is evenly distributed across twelve product lines. The difference is not in what the data contains, but in how it is shaped. Models do not learn from examples. They learn from distributions. If you cannot describe the distribution of your dataset, you cannot predict what your model will learn from it.

Distribution documentation is where most teams fail silently. They build datasets, label them, train models, and never once write down what the data actually looks like as a statistical population. They assume balance without measuring it. They assume coverage without mapping it. They assume representativeness without validating it. When the model fails in production, they add more data without understanding what shape of data they are missing. When eval performance diverges from production performance, they blame the model instead of recognizing that their training distribution and production distribution are different worlds. Distribution descriptions are not optional metadata. They are the single most predictive piece of documentation for whether a dataset will produce a useful model.

## What Distribution Means in Practice

Distribution is the shape of your data across every dimension that matters. It is not a single number. It is a multi-dimensional description of how your examples are spread across classes, features, languages, geographies, time periods, input lengths, output types, and every other axis that affects model behavior. A well-documented distribution tells you what the model will see a lot of, what it will see rarely, and what it will never see at all. It tells you where the model will be confident and where it will guess. It tells you which subpopulations are over-represented and which are invisible.

For a classification dataset, class distribution is the foundation. If you have five classes and 80% of your examples are in one class, your model will learn to predict that class most of the time. If you have a binary classifier with 99% negative examples and 1% positive examples, your model will achieve 99% accuracy by predicting negative for everything. Class imbalance is not inherently bad, but undocumented class imbalance is negligence. You must write down the counts and percentages for every class. You must state whether the distribution matches production or is artificially balanced for training purposes. You must describe how you handled imbalance: did you upsample, downsample, weight losses, or leave it natural? A distribution description for a five-class sentiment classifier might read: "Class distribution is 42% neutral, 28% positive, 18% negative, 8% very positive, 4% very negative. This matches production distribution from Q4 2025. No resampling applied."

For text datasets, length distribution matters enormously. If your training data is mostly short inputs and production receives long inputs, the model will fail. If your training data includes 10,000-token documents and your production system truncates at 2,000 tokens, you wasted compute training on irrelevant examples. Length distribution should describe percentiles: median length, 25th percentile, 75th percentile, 95th percentile, maximum. It should describe whether length correlates with difficulty, label type, or quality. A length distribution description for a summarization dataset might read: "Input documents range from 200 to 8,000 tokens. Median is 1,200 tokens. 95th percentile is 4,500 tokens. Longer documents are predominantly news articles, shorter documents are product reviews. All inputs fit within the 8,192-token context window of the target model."

For datasets with categorical features, feature distribution describes how examples spread across each feature's possible values. If you have a language feature and 90% of your data is English, your model will fail on other languages unless you explicitly design for that. If you have a geographic feature and all your data is from North America, your model will fail in Europe and Asia. Feature distribution documentation lists the top values for each feature and their percentages. It flags sparse features where most values appear only once or twice. It identifies correlated features where certain feature combinations dominate. A feature distribution description for a product recommendation dataset might read: "User country: 65% United States, 12% United Kingdom, 8% Canada, 15% other. Product category: 40% electronics, 25% clothing, 20% home goods, 15% other. 85% of electronics purchases are from US users, indicating geographic-category correlation."

For datasets that span time, temporal distribution describes when the data was collected and whether it is evenly distributed or clustered. If you collect all your data in November and December, your model will overfit to holiday behavior. If you collect data in 2023 and deploy in 2026, your model will fail because the world changed. Temporal distribution should state the date range, the density over time, and any known temporal biases. It should describe whether the dataset includes seasonal variation, trend changes, or event-driven spikes. A temporal distribution description for a financial forecasting dataset might read: "Data collected from January 2024 through December 2025. Weekly resolution. Includes two full seasonal cycles. March 2025 shows a spike in volatility due to banking sector stress. Data collection is continuous and this dataset will be extended quarterly."

For multi-task or multi-domain datasets, task distribution and domain distribution describe how examples are allocated across tasks and domains. If you have a joint dataset for question answering, summarization, and classification, you must document how many examples support each task. If you have a multi-domain dataset covering healthcare, finance, and e-commerce, you must document how many examples come from each domain and whether performance requirements differ by domain. A task distribution description for a multi-task dataset might read: "60% classification, 30% extraction, 10% generation. Classification examples are evenly split between five classes. Extraction examples have 2 to 8 entities per example, median 4. Generation examples have target lengths between 50 and 300 tokens, median 120."

Distribution descriptions are prose, not charts. This book has no images, and your dataset documentation should not rely on images either. Write the numbers. Write the percentiles. Write the comparisons. Describe the shape in words that someone can understand without seeing a histogram. Say "class A is four times more common than class B" instead of showing a bar chart. Say "input lengths follow a long-tail distribution with median 500 tokens and 95th percentile 3,000 tokens" instead of showing a density plot. Prose forces you to interpret the distribution, not just display it. Prose forces you to say what the distribution means, not just what it is.

## The Assumed Uniform Anti-Pattern

The most common distribution failure is assuming your data is balanced, representative, and evenly distributed without ever checking. Teams build datasets by collecting whatever data is easiest to collect, assume it covers all the cases they care about, and discover during production that entire subpopulations are missing. A healthcare AI company in mid-2025 built a diagnostic assistant trained on 200,000 clinical notes from their pilot hospital. They assumed the data was representative of general practice. They launched to twelve hospitals. Within two weeks, accuracy dropped from 89% in eval to 62% in production at the non-pilot sites. The root cause was distribution: the pilot hospital was a tertiary care research hospital that saw rare and complex cases, and the model learned to recognize rare disease patterns. The other hospitals were community hospitals that saw common conditions, and the model had almost no training data for common primary care presentations. The team had never documented the case complexity distribution, specialty distribution, or patient demographic distribution. They assumed uniform coverage across all clinical scenarios. They were wrong by a factor of ten.

Assumed uniformity fails in language distribution. A customer support dataset might be 95% English simply because English-speaking customers are more likely to contact support in writing, but the team assumes the dataset represents all customers. Assumed uniformity fails in geographic distribution. A fraud detection dataset might be 80% US transactions simply because the company started in the US, but the team assumes the dataset represents fraud patterns globally. Assumed uniformity fails in time distribution. A content moderation dataset might be 60% collected during a specific policy crisis when reporting volume spiked, but the team assumes the dataset represents steady-state moderation needs.

You prevent assumed uniformity by measuring distribution for every dimension you care about before you finalize the dataset. You count classes. You count languages. You count geographies. You plot lengths. You check correlations. You compare your dataset distribution to your production distribution. If they match, you document the match. If they do not match, you document the difference and explain why it is acceptable or describe your plan to close the gap. You do this before training the first model, not after the first production failure.

## Distribution Shift as a Silent Failure Mode

Models trained on one distribution and deployed on another distribution fail silently. The model runs. It returns predictions. The predictions are confidently wrong. A legal document classification system trained on contracts from 2020 through 2023 failed in early 2025 because contract language shifted after a major regulatory change in 2024. The distribution of clause types changed. The model's training distribution was pre-regulation, and production distribution was post-regulation. The model's accuracy dropped from 94% to 78%, but nobody noticed for three months because the system had no distribution monitoring.

Distribution shift happens in class distribution. If your training data is 50% class A and 50% class B, but production is 80% class A and 20% class B, your model's precision and recall on class B will degrade because the decision boundary was optimized for a different base rate. Distribution shift happens in feature distribution. If your training data has feature X present in 30% of examples and production has feature X present in 70% of examples, the model's reliance on feature X will be under-calibrated. Distribution shift happens in input length distribution. If your training data has median length 500 tokens and production has median length 1,500 tokens, the model may struggle with longer contexts if it learned to rely on early tokens.

You document distribution shift risk by comparing training distribution to expected production distribution at the time you create the dataset. You write down the differences. You assess whether each difference is acceptable, risky, or disqualifying. You design your eval set to include production distribution, not just training distribution, so you can measure the impact of distribution shift before deployment. If you cannot obtain production-representative data for training, you document that limitation explicitly and describe how you will monitor for distribution shift in production.

A credit risk modeling team in late 2025 built a dataset from loan applications in 2024 and early 2025, a period of low interest rates and high approval rates. They documented that their dataset had an approval rate of 68% and a default rate of 2.1%. They predicted that production in 2026 would have an approval rate of 50% and a default rate of 3.5% due to tightening credit conditions. They documented this expected distribution shift and designed their eval set to include a synthetic stress scenario with higher-risk applicants. When they deployed in early 2026, production distribution matched their prediction, and the model performed within expected bounds because they had anticipated and measured the shift in advance.

## Connecting Distribution to Eval Set Design

Your eval set must cover the same distribution as production, not the same distribution as training. If your training data is imbalanced but production is balanced, your eval set should be balanced. If your training data is artificially augmented but production is natural, your eval set should be natural. If your training data is from 2024 but production is in 2026, your eval set should include 2026 data. Eval set design is distribution design, and you cannot design an eval set distribution without first documenting your training set distribution and your production distribution.

A common failure is to split training and eval randomly from the same source dataset, ensuring that eval has the same distribution as training. This measures overfitting, but it does not measure generalization to production. If your training data has 90% short inputs and production has 50% short inputs, a random split gives you an eval set with 90% short inputs, and you will not discover that your model fails on long inputs until production. You must intentionally design your eval distribution to match production distribution, even if that means your eval distribution differs from training distribution.

Distribution documentation for training data enables distribution design for eval data. If you document that your training data is 95% English, you can intentionally design your eval data to be 70% English and 30% other languages if that matches production. If you document that your training data has median input length 400 tokens, you can intentionally design your eval data to have median input length 800 tokens if that matches production. If you document that your training data is from Q1 through Q3 2025, you can intentionally collect eval data from Q4 2025 and Q1 2026 to test temporal generalization.

A fraud detection team in early 2025 documented their training data distribution: 99.2% legitimate transactions, 0.8% fraud. They documented their production distribution: 99.5% legitimate, 0.5% fraud, but with different fraud type distribution because their training data was from card-present fraud and production included card-not-present fraud. They designed their eval set with 95% legitimate and 5% fraud, with the 5% fraud heavily weighted toward card-not-present patterns. This eval set had a very different distribution from training, but it matched production distribution and allowed them to measure the model's ability to generalize to the fraud types that mattered.

## What to Measure and What to Skip

You do not need to document every possible distribution. You need to document the distributions that affect model behavior and the distributions that affect production performance. Measure class distribution for classification. Measure length distribution for text tasks. Measure language distribution for multilingual systems. Measure temporal distribution for time-sensitive tasks. Measure feature distribution for structured data. Measure task distribution for multi-task models. Measure domain distribution for multi-domain models.

You skip distributions that do not vary meaningfully or do not affect the model. If every example in your dataset is English, you do not need to document language distribution beyond stating "100% English". If every example is between 400 and 600 tokens, you do not need detailed length percentiles beyond stating "median 500 tokens, range 400 to 600". If your dataset has no temporal component, you do not need temporal distribution. If your dataset has no categorical features, you do not need feature distribution. Document what varies and what matters.

For each distribution you document, include the measurement date and the measurement method. State whether the distribution is measured on the full dataset or a sample. State whether the distribution is measured before or after preprocessing, filtering, or augmentation. A distribution description that says "40% class A" is incomplete. A distribution description that says "40% class A, measured on the full dataset of 500,000 examples after deduplication and filtering, as of January 15, 2026" is complete.

## Writing Distribution Descriptions Without Jargon

Distribution descriptions are read by engineers, product managers, data scientists, auditors, and executives. They must be clear to all audiences. Avoid statistical jargon. Do not say "the distribution exhibits positive skew". Say "most examples are short, but a long tail of very long examples pulls the mean higher than the median". Do not say "the distribution is bimodal". Say "examples cluster into two groups: short queries under 50 tokens and long documents over 1,000 tokens, with very few examples in between". Do not say "kurtosis is 4.2". Say what the number means in plain language or skip the metric.

Use comparisons to make distributions concrete. Say "class A is three times more common than class B" instead of listing percentages without context. Say "input lengths range from tweet-length to article-length, with most examples closer to paragraph-length" instead of listing tokens without reference points. Say "English is the dominant language at 85%, followed by Spanish at 8% and French at 4%, with all other languages under 1% each" instead of listing fifteen languages with decimals.

Use benchmarks to make distributions meaningful. Say "our class imbalance is 10-to-1, which is moderate compared to fraud detection datasets that often see 100-to-1 or higher" instead of stating the imbalance without context. Say "our median input length of 1,200 tokens is well within the 8,192-token limit of GPT-5 and Claude Opus 4, but would require chunking for older models with 4,096-token limits" instead of stating the length without implications.

## Distribution Documentation as Diagnostic Tool

When a model fails, the first place to look is distribution mismatch. If eval accuracy is 92% but production accuracy is 78%, check whether production distribution matches eval distribution. If the model performs well on class A but poorly on class B, check whether class B is under-represented in training. If the model performs well in English but poorly in Spanish, check whether Spanish is under-represented in training. Distribution documentation turns debugging from guesswork into measurement.

A content moderation team in mid-2025 saw production precision drop from 88% to 71% over six weeks. They checked their distribution documentation and compared it to production logs. They discovered that production traffic had shifted from 60% text-only posts to 60% posts with embedded links, and their training data was only 20% posts with embedded links. The model had learned patterns for text-only content and was failing on link-heavy content. They updated their dataset to match the new production distribution and retrained. Precision returned to 87%. The diagnosis took two hours because they had distribution documentation. Without it, they would have spent weeks adding random data and retraining blindly.

Distribution documentation also guides data collection priorities. If you document that your dataset has 5% representation for a category that accounts for 20% of production traffic, you know exactly where to focus your next data collection effort. If you document that your dataset has strong coverage for common cases but weak coverage for edge cases, you know to prioritize edge case collection. If you document that your dataset is clustered in time and missing recent examples, you know to prioritize recent data.

## Updating Distribution Documentation Over Time

Datasets evolve. You add examples, remove examples, relabel examples, merge datasets, filter datasets. Every change that affects distribution requires updating the distribution documentation. If you add 50,000 examples and shift class balance from 60-40 to 70-30, you update the class distribution description. If you filter out all examples shorter than 100 tokens and shift the median from 300 to 400 tokens, you update the length distribution description. If you merge two datasets and introduce a new geographic distribution, you document the new distribution.

Distribution documentation is versioned with the dataset. Every dataset version has a distribution description that matches that version. You do not overwrite the old distribution description when you update the dataset. You create a new distribution description for the new version and keep the old one for the old version. This allows you to understand how the dataset evolved and to diagnose issues that trace back to specific versions. A team in late 2025 discovered that a model trained on dataset version 4 performed better than a model trained on dataset version 5, despite version 5 being larger. They compared distribution documentation between versions and discovered that version 5 had introduced a class imbalance by adding a large batch of class A examples without proportionally adding class B examples. They reverted to version 4 distribution and rebalanced version 5.

You review distribution documentation quarterly or whenever production distribution changes. If production launches in a new geography, you check whether your dataset covers that geography. If production adds a new product line, you check whether your dataset includes examples from that product line. If production traffic shifts from mobile to desktop or from short to long inputs, you check whether your dataset matches the new distribution. Distribution drift between dataset and production is a leading indicator of model performance degradation.

Your distribution description is complete when a new team member can read it and predict what the model will be good at, what the model will struggle with, and what the model has never seen. It is complete when an auditor can read it and assess whether the dataset is representative of the intended use case. It is complete when a product manager can read it and decide whether the dataset supports their launch plan. Distribution documentation is not a statistical appendix. It is the map that shows where your data takes the model and where it leaves the model blind.

The next step after documenting what your data looks like is defining who decides what happens to your data. Governance structures establish who has authority to approve, modify, and retire datasets, and how those decisions are made at scale.
