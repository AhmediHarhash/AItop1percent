# 6.11 — Evolving Eval Sets as Your Product Changes

The product launches in 2024 supporting residential mortgages in English. By mid-2025, it supports commercial real estate loans, multi-currency documents, covenant extraction, and Spanish and French language processing. The evaluation set still contains only residential mortgages in English because no one updated it. When the team tests a new model, the eval set shows 94 percent accuracy on a task the product no longer performs, and production accuracy is 81 percent on the tasks users actually run. This is eval set obsolescence, and it happens to every team that treats evaluation as a one-time artifact instead of a living system that evolves with the product. Your eval set is a snapshot of what you thought mattered six months ago. Your product is what you shipped last week. The gap between those two realities means your metrics measure nothing useful.

This is the eval set obsolescence problem. Your product changes. Features are added. Models are updated. User behavior shifts. New edge cases emerge. The evaluation set that once represented your production task becomes a historical artifact that measures performance on a task you no longer perform. If you continue using the obsolete eval set, you get misleading metrics. Your evaluation scores trend upward while production performance stagnates or degrades. You optimize for a test that no longer reflects reality. The solution isn't to throw away the old eval set and start over. The solution is to evolve your eval set in parallel with your product, maintaining historical comparability while adapting to new requirements.

## Why Static Eval Sets Become Stale

The fundamental problem is that evaluation sets are static snapshots of a task definition that is constantly changing. When you build an eval set, you're encoding a specific understanding of what the system should do, what inputs it will receive, what edge cases matter, and what success looks like. This understanding is valid at the moment of creation. But it becomes less valid every week as the product evolves.

Feature additions are the most obvious source of staleness. When you add a new capability, the eval set doesn't automatically test it. A customer support system that originally handled billing questions and then adds technical support questions needs eval examples covering technical support. If the eval set still contains only billing questions, it's testing half the product. Performance metrics become uninterpretable. A model update might improve technical support accuracy from 76% to 84% while maintaining billing accuracy at 92%. But if your eval set only measures billing, you see 92% before and after. The improvement is invisible to your evaluation process.

Model updates also change what the eval set measures. When you upgrade from GPT-4 to GPT-5.1, the model's capabilities shift. Some tasks that were hard for GPT-4 become easy for GPT-5.1. Some error patterns disappear. New error patterns emerge. If your eval set was designed to stress-test GPT-4's weaknesses, it might not stress-test GPT-5.1 effectively. The eval set becomes too easy, showing artificially high scores because it's no longer calibrated to the current model's failure modes.

User behavior shifts create another source of staleness. The way users interact with your system changes over time. They learn what works and what doesn't. They develop new phrasings. They start using the system for tasks you didn't originally anticipate. A chatbot launched in 2024 might have received mostly simple, well-formed questions. By 2026, users have learned that the chatbot can handle complex multi-part questions, so they start asking them. Your eval set, built on 2024 interaction patterns, doesn't include these complex multi-part questions. It's testing the 2024 user population, not the 2026 user population.

Production edge cases that emerge over time don't appear in static eval sets. When you launch, you try to anticipate edge cases based on domain knowledge and pre-launch testing. But real production traffic reveals edge cases you never imagined. A medical symptom checker might not have anticipated that users would describe symptoms using trending TikTok health terminology. A legal contract analyzer might not have anticipated that users would upload scanned images of contracts with handwritten annotations. These edge cases become common in production but remain absent from the eval set. Performance metrics don't reflect how the system handles these real-world patterns.

Policy and requirement changes also obsolete eval sets. If you launch with one set of content moderation policies and then update those policies, the eval set needs to reflect the new policies. What was labeled as acceptable in 2024 might be unacceptable in 2026. If your eval set still uses the old labels, you're measuring adherence to obsolete policies. You might score high on the eval set while violating current policy in production.

## Adding New Test Cases for New Features Without Invalidating Old Benchmarks

The core challenge is maintaining comparability across time while adapting to change. You want to be able to compare a model's performance in 2024 against its performance in 2026, even though the product is different in 2026. This requires a versioned approach where you preserve historical test cases while adding new ones.

The versioned eval set structure organizes test cases into generations. Version 1.0 contains the original launch eval set. Version 1.1 adds new test cases for the first feature expansion while preserving all Version 1.0 cases. Version 1.2 adds cases for the next feature while preserving 1.0 and 1.1. Each version is a superset of the previous version. This allows you to report performance on multiple metrics: performance on the complete current eval set, performance on the historical baseline set, and performance on each incremental addition.

When you add new test cases for a new feature, you create a clearly labeled subset within the eval set. If you add technical support capability to a customer service system, you create a Technical Support subset with 150 examples. This subset has its own performance metric. You report overall accuracy across the entire eval set, billing accuracy on the original subset, and technical support accuracy on the new subset. This lets you track whether the model update that enabled technical support degraded billing performance. If overall accuracy holds at 91% but you see billing drop from 92% to 88% and technical support at 85%, you know you've introduced a regression on the original task.

The subset structure also enables feature-level performance tracking. Product teams care about per-feature accuracy. If you add five features over twelve months, each feature gets its own eval subset. You can report a performance dashboard showing accuracy for each feature independently. This makes it clear which features are production-ready and which need more work. It also makes it possible to disable or deprecate features based on evaluation data. If a feature consistently scores below 75% accuracy and doesn't improve after multiple model iterations, you have quantitative evidence to sunset it.

When adding new examples, you maintain the same annotation quality bar and schema as the original eval set. If the original set required dual annotation with adjudication, the new examples require the same process. If the original set used a three-label scheme, the new examples use the same labels. Consistency in construction ensures that performance is comparable across subsets. If you lower the quality bar for new examples, high performance on new subsets doesn't mean the same thing as high performance on old subsets.

You also avoid discarding old examples when adding new ones. Teams sometimes feel that old examples are no longer relevant and remove them to make room for new examples. This breaks historical comparability. You can no longer compare current performance to baseline performance because you've changed the benchmark. The old examples remain valuable even as the product evolves. They ensure that new features and model updates don't regress on original functionality. They serve as a stability test. If performance on the original subset degrades as you add new features, you're experiencing feature interference.

## Versioning Eval Sets Alongside Product Versions

The most rigorous approach is to align eval set versions with product release versions. Every time you ship a major product release, you ship a corresponding eval set version. The eval set version includes all test cases relevant to that release, including both historical cases and new cases introduced for new features.

Version numbering follows semantic versioning principles. Major version changes—2.0, 3.0—reflect major product changes that redefine the task. Adding an entirely new capability, changing the core model, or restructuring the output schema justifies a major version change. Minor version changes—2.1, 2.2—reflect incremental feature additions or expansions. Adding support for a new document type, expanding to a new language, or covering a new edge case category justifies a minor version change. Patch version changes—2.1.1, 2.1.2—reflect bug fixes in existing test cases, re-annotations based on updated guidelines, or corrections of labeling errors.

Each versioned eval set is stored as a complete artifact. You don't just store diffs. You store the full set of examples for each version. This allows you to rerun historical evaluations exactly. If you want to know how your current model would have performed on the 2024 product, you run it against eval set version 1.0. If you want to know how your 2024 model performs on the 2026 product, you run the old model against version 3.0. This enables time-series analysis of model progress and product evolution.

Versioned eval sets also enable A/B testing of models across product versions. When you're considering a model upgrade, you can evaluate the new model on the current eval set version and on several historical versions. If the new model improves on current tasks but regresses on historical tasks, you have a decision to make. Sometimes regression on old tasks is acceptable because those tasks are no longer relevant. Sometimes it's a warning sign that the new model has lost important capabilities.

The version history serves as documentation of product evolution. By reading the changelog for eval set versions, you can see when features were added, when requirements changed, and when new edge cases emerged. This creates institutional memory. New team members can understand how the product evolved by studying eval set history. Product managers can see when specific capabilities were introduced and track how their accuracy evolved over time.

Versioning also enables rollback. If a product release causes performance degradation, you can roll back to the previous release and verify that the previous eval set version shows restored performance. This confirms that the regression was caused by the new release rather than by production data drift or infrastructure changes.

## Retirement Criteria: When to Sunset Eval Cases

Not all eval cases should live forever. Some become irrelevant as the product changes. Some become too easy as models improve. Some test features that have been deprecated. Removing these cases keeps the eval set focused and prevents it from growing unbounded.

The first retirement criterion is feature deprecation. If you remove a feature from the product, you should remove eval cases that specifically test that feature from the primary eval set. You don't delete them entirely. You move them to an archived subset. This preserves historical comparability but removes them from current performance calculations. A company that deprecated support for fax number extraction can remove fax extraction cases from the active eval set. If they later re-introduce fax support, they can restore the archived cases.

The second criterion is task redefinition. Sometimes the way you define a task changes fundamentally. A sentiment analysis system that originally classified text as positive, negative, or neutral might evolve to a five-point scale from very negative to very positive. The old three-label eval cases don't map cleanly to the new five-label task. You can reannotate them using the new schema or retire them and build new cases designed for the new schema. Retirement is often cleaner because reannotation introduces inconsistency—older annotators used different context and guidelines than current annotators.

The third criterion is saturation. If an eval case has been answered correctly by every model version for the past twelve months, it's no longer providing signal. It's testing a solved problem. Retiring saturated cases keeps the eval set difficult enough to drive improvement. You want your eval set to have a mix of easy cases that establish baseline competence and hard cases that distinguish between models. If too many cases become saturated, the eval set loses discriminative power. A 95% accuracy score doesn't tell you much if 90% of cases are trivially easy.

The fourth criterion is contamination suspicion. If you suspect that eval cases have leaked into training data—either because they were publicly shared, included in documentation, or inadvertently uploaded to model training data sources—you should retire them. Contaminated cases don't measure the model's task performance. They measure the model's memorization. You can't prove contamination definitively in most cases, but if a case shows a sudden performance jump that doesn't correlate with general task improvement, contamination is a plausible explanation.

The fifth criterion is annotation error discovery. As you review eval set performance, you sometimes discover that certain examples were mislabeled. The annotator misunderstood the task, the guidelines were ambiguous, or the example itself was inherently ambiguous. When you discover these errors, you have two options: reannotate with the correct label or retire the example. Retirement is safer if the example is fundamentally ambiguous or if correcting it would break comparability with historical results.

Retirement doesn't mean deletion. Retired cases are moved to an archive with metadata explaining why they were retired and when. The archive is available for historical analysis but excluded from current performance metrics. This allows you to reconstruct historical eval set versions exactly, including retired cases, if you need to reproduce old results or understand how the eval set evolved.

## Maintaining Historical Comparability While Evolving

The tension between evolution and comparability is real. Every time you add new cases or retire old cases, you change what you're measuring. Completely rigid adherence to historical comparability means never evolving the eval set, which leads to obsolescence. Completely unconstrained evolution means losing the ability to track progress over time. The solution is to maintain a stable core set that never changes and allow evolution in the expansion sets.

The stable core set is defined at launch and frozen. It contains 200 to 500 examples that represent the central, enduring aspects of the task. These examples are never modified, never retired, and never reannotated unless you discover a critical annotation error. Performance on the core set is your long-term benchmark. It allows you to compare the 2026 model against the 2024 model on exactly the same test. The core set provides the anchor for historical comparability.

The expansion sets are versioned and evolving. Each product release or major feature addition creates a new expansion set. These sets test new capabilities, new edge cases, and new requirements. Performance on expansion sets tracks how well the system handles the evolving product. The combination of core set performance and expansion set performance gives you a complete picture: the core set shows whether you're maintaining baseline capability, and the expansion sets show whether you're successfully handling new requirements.

Reporting practices reinforce this structure. Every evaluation report includes three metrics: core set accuracy, current expansion set accuracy, and combined accuracy. Core set accuracy is the long-term trendline. It should be stable or improving. If it degrades, you've introduced a regression on fundamental capabilities. Expansion set accuracy tracks new feature performance. Combined accuracy reflects overall production performance estimation. By reporting all three, you make it clear whether performance changes are driven by improvements on the core task, successful addition of new capabilities, or both.

The stable core set also serves as a calibration reference. If you change annotation guidelines for new expansion sets—because you've refined your understanding of the task or because new features require new label definitions—the core set remains annotated according to the original guidelines. This creates a bridge between old and new annotation schemes. You can measure inter-annotator agreement between old-guideline and new-guideline annotations on the core set to quantify how much the guidelines have shifted.

## The Eval Set Review Cadence: Quarterly, Per-Release, Continuous

The frequency of eval set updates depends on your release velocity and the rate of product change. High-velocity teams with monthly releases need more frequent eval set updates than low-velocity teams with annual releases. But even low-velocity teams benefit from regular eval set review to catch drift, detect obsolescence, and identify new edge cases.

The per-release cadence ties eval set updates to product releases. Every time you ship a new product version, you review the eval set. You add new cases covering new features. You review whether any existing cases should be retired. You update documentation to reflect the new eval set version. This ensures that the eval set and the product stay synchronized. It also creates a natural forcing function. You can't ship a release without defining how you'll evaluate it.

The quarterly cadence decouples eval set updates from releases. Every quarter, regardless of how many product releases occurred, you conduct an eval set review. You sample recent production data to identify new patterns. You review production errors to see which ones aren't represented in the eval set. You analyze performance trends to identify saturated cases and underperforming categories. You make incremental additions and retirements. This approach works well for products with variable release schedules or for products where production drift happens independently of releases.

The continuous cadence treats the eval set as a living dataset that's updated weekly or monthly. New production edge cases are added as soon as they're discovered and annotated. Saturated cases are retired as soon as saturation is detected. This requires more infrastructure—automated annotation pipelines, continuous performance tracking, and dynamic eval set versioning—but it provides the tightest coupling between production reality and evaluation. Continuous eval set evolution is most common in high-stakes production systems where evaluation must reflect current production conditions in near real-time.

Regardless of cadence, the review process should be systematic. You don't add random examples or remove examples based on intuition. You follow a defined process: analyze production performance data, identify gaps between production patterns and eval set patterns, sample new examples from production to fill those gaps, annotate new examples using current guidelines, add new examples to the appropriate eval set version, review existing examples for saturation or obsolescence, retire cases that meet retirement criteria, document changes in the eval set changelog, and publish the new version. This systematic process ensures that eval set evolution is intentional and traceable rather than ad-hoc.

Evolving your eval set in parallel with your product requires discipline, infrastructure, and ongoing investment. But the alternative—using a static eval set that becomes increasingly obsolete—leads to invisible regressions, misleading metrics, and degraded production performance. Treating your eval set as a living artifact that evolves with your product keeps your evaluation aligned with reality and ensures that the metrics you optimize are the metrics that matter.

Next, we turn to adversarial evaluation sets, which are specifically designed to stress-test your system, expose edge cases, and find the boundaries where your model breaks.
