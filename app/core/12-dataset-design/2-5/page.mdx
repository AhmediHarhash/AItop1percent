# 2.5 â€” Event Time vs Processing Time: Getting Timestamps Right

In a 2024 study of production AI system failures across healthcare, finance, and e-commerce, timestamp confusion accounted for 23% of train-serve skew incidents, making it the second most common cause of production performance degradation after label noise. The pattern is consistent: teams train models using infrastructure timestamps that reflect when their systems processed data, then deploy to environments where only domain timestamps are available. The models achieve strong offline metrics by learning from information that will not exist at decision time, then fail in production when that future information disappears. Event time versus processing time is not a minor technical distinction. It is the difference between models that generalize and models that hallucinate causality from infrastructure lag.

This failure reveals the critical distinction between event time and processing time. Event time is when something actually happened in the real world. Processing time is when your system recorded or processed that event. Confusing these two timestamps is one of the most common and dangerous errors in dataset engineering. It causes data leakage, stale training sets, incorrect evaluation windows, and models that perform well in backtests but fail in production. Getting timestamps right is not a minor technical detail. It is a foundational requirement for building reliable AI systems that operate in the temporal reality of production environments.

## The Two Clocks: Event Time and Processing Time

Every data point in your pipeline exists in two temporal dimensions simultaneously. Event time is the timestamp of when the event occurred in the real world, according to the domain you are modeling. For a user clicking a button, event time is when the click happened. For a financial transaction, event time is when the transaction was authorized. For a patient discharge, event time is when the patient left the hospital. Event time is the ground truth timestamp, the moment that matters for your domain logic and your model's understanding of causality.

Processing time is when your system observed, received, or processed that event. For a user click, processing time might be when the event reached your logging service, or when it landed in your data warehouse, or when your training pipeline read it from storage. Processing time can lag event time by milliseconds, seconds, hours, or even days depending on your infrastructure, network latency, batching intervals, and data propagation delays. Processing time is infrastructure time, the timestamp your systems use to track their own operations.

Most teams naturally gravitate toward using processing time because it is readily available in their logs and databases. The warehouse insert timestamp, the Kafka message timestamp, the S3 file creation time, these are all processing time markers. They are convenient and always present. But using processing time as a proxy for event time introduces systematic errors that corrupt your training data and invalidate your evaluations.

When you train a model using processing time, you are teaching it patterns based on when your infrastructure happened to process data, not when events actually occurred. This creates three critical problems: data leakage from late-arriving information, temporal misalignment between training and production, and incorrect evaluation windows that do not reflect real decision boundaries.

## Why Processing Time Fails

Processing time fails as a modeling timestamp because it conflates infrastructure delays with domain reality. Your model needs to understand the world as it unfolds in event time. A recommendation system must predict what a user will click based on their history up to the moment of the recommendation request. If you train that model using processing time, you might include clicks that happened after the recommendation was served but were processed before the training pipeline ran. The model learns to use future information, achieving high offline accuracy but failing in production where that future does not exist yet.

Processing time also varies unpredictably based on infrastructure behavior. A network outage might delay data by hours. A batch job might run late. A retry mechanism might reprocess old data. These infrastructure artifacts create temporal noise that has nothing to do with the patterns your model should learn. Training on processing time teaches the model to respond to infrastructure behavior rather than domain patterns.

The most insidious problem is that processing time errors are silent. Your pipeline runs without errors. Your model trains successfully. Your evaluation metrics look good. The only signal that something is wrong appears in production when the model performs worse than expected. By then, you have invested weeks or months in a fundamentally flawed approach.

## The Solution: Event Time Semantics

The solution is to treat event time as the primary temporal dimension for all dataset operations. Every record in your pipeline must carry an explicit event time field that represents when the event occurred in the domain. Your training set construction, your train-eval splits, your feature windows, and your evaluation logic must all operate on event time, not processing time. Processing time remains useful for operational monitoring, debugging pipeline delays, and handling late-arriving data, but it must never drive your modeling decisions.

Event time semantics require discipline at every stage of your pipeline. When data enters your system, you must capture or preserve the event timestamp from the source. When you transform data, you must propagate event time correctly through joins and aggregations. When you build training examples, you must ensure that features use only data with event time strictly before the example's label event time. When you split data for evaluation, you must split on event time to create valid temporal boundaries.

This discipline pays off in models that generalize correctly from training to production. Your offline evaluations measure the same temporal relationships that exist in production. Your feature engineering respects causality. Your model learns patterns that hold across time rather than artifacts of your infrastructure.

## Data Leakage Through Temporal Confusion

The most damaging consequence of confusing event time and processing time is subtle data leakage that allows future information to contaminate your training examples. Consider a fraud detection system that processes credit card transactions. A transaction occurs at event time T, but the fraud label arrives at processing time T plus six hours after the cardholder reports the fraud. If you build training examples using processing time, you might inadvertently include features computed from transactions that occurred between T and T plus six hours, transactions that happened after the event you are trying to predict but before your system processed the label. The model learns to use information from the future, achieving high offline accuracy but failing completely in production where that future information does not exist yet.

This leakage is not always obvious because your pipeline does not explicitly reference future data. The leakage happens implicitly through timestamp boundaries. When you filter data for a training example using processing time windows, you are asking what data had my system processed by this processing time rather than what data had occurred by this event time. These two questions have different answers whenever there is processing delay, and that difference is precisely the leakage. The model sees data that arrived late, data that represents events from after the decision point but before the processing point.

A recommendation system trained on user engagement data faces the same hazard. A user watches a video at event time T. The system logs the view, computes engagement metrics, and writes the record to the data warehouse at processing time T plus 30 minutes. If your training pipeline pulls user history based on processing time, a recommendation request at event time T might include features from videos watched up to 30 minutes after T, because those videos were processed within the same time window. The model learns patterns that depend on seeing future user behavior, patterns that do not hold when making real-time recommendations.

The fix requires discipline in every stage of your pipeline. Every feature computation must use event time windows. Every join must align on event time. Every aggregation must bucket by event time. Your training set builder must filter data using event time predicates, ensuring that for a training example with event time T, all features use only data with event time strictly before T. This means you cannot rely on simple database queries that use insertion timestamps or update timestamps. You must explicitly track and propagate event time through every transformation, and you must validate that your temporal logic respects causality.

## Point-in-Time Correctness

Point-in-time correctness is the property that a feature value for an event at time T reflects only information that was known at time T, using event time as the definition of known. Many feature pipelines fail this test because they compute features using the latest available data at processing time rather than reconstructing the historical state at event time.

Consider a feature that counts how many purchases a user made in the last 30 days. For a training example with event time T, the point-in-time correct value is the count of purchases with event time in the range from T minus 30 days to T. But if your feature pipeline computes this count at processing time P, it might accidentally include purchases that occurred between T and P if those purchases were processed before P. The feature becomes contaminated with future information, and your model learns dependencies on data it will not have in production.

Maintaining point-in-time correctness requires versioned feature stores or careful temporal logic in your feature queries. For each training example at event time T, you must query feature values as they existed at event time T, not as they exist now. This is straightforward for immutable event data, you simply filter by event time. But for mutable entity data like user profiles or account balances, you need either temporal tables that track the history of changes with event time timestamps, or a snapshot-based approach that periodically captures the state of all entities and allows you to query historical snapshots.

Many teams discover point-in-time violations only after deploying a model that performs worse in production than in evaluation. The offline metrics looked strong because features inadvertently included future information. In production, without that future information, the model's performance degrades. Debugging these issues is difficult because the problem is not in the model architecture or hyperparameters, it is in the temporal logic of the feature pipeline, buried in SQL queries or data transformations that nobody suspected were wrong.

## Train-Eval Splits and Temporal Boundaries

The event time versus processing time distinction becomes especially critical when constructing train-eval splits. The purpose of a temporal split is to simulate production conditions where you train on the past and evaluate on the future. If you split based on processing time, you are simulating the wrong thing. You are asking how well does the model perform on data my system processed later rather than how well does the model perform on events that happened later. These are different questions with different answers.

Imagine you split your dataset at processing time P, using all data processed before P for training and all data processed after P for evaluation. An event that occurred before P but was processed after P due to a delay will land in your evaluation set even though it chronologically belongs in the training set. Conversely, an event that occurred after P but was processed before P due to early arrival will land in your training set even though it chronologically belongs in the evaluation set. Your split no longer represents a clean temporal boundary, and your evaluation metrics no longer measure the model's ability to generalize to the future.

The healthcare readmission example from the opening illustrates this problem perfectly. The team split their data at a processing time boundary, assuming it represented a meaningful temporal division. But patient records from earlier discharges that took longer to process ended up in the evaluation set, while records from later discharges that processed quickly ended up in the training set. The evaluation set was not truly future data, it was a random mix of past and future events based on processing delays. The model appeared to generalize well because it was tested on a temporally scrambled dataset that did not reflect the true production scenario.

You must split on event time to create valid temporal boundaries. Define a cutoff event time T, use all examples with event time before T for training, and use all examples with event time after T for evaluation. This ensures that your evaluation set represents events that genuinely happened after your training set, creating a realistic test of the model's ability to predict the future from the past. Processing time becomes irrelevant for the split decision, though you may still use it to filter out examples that arrived so late they would not be available for training in a production scenario.

## Watermarking and Late-Arriving Data

In real production systems, data does not arrive in perfect event time order. Network delays, retry logic, batch processing, and system outages cause events to arrive out of order or significantly delayed. A transaction from 10 AM might arrive at your pipeline at 10:02 AM, while a transaction from 10:01 AM arrives at 10:05 AM. This disorder creates a fundamental challenge: when can you consider a time window complete and safe to use for training?

Watermarking provides a solution. A watermark is a processing time assertion that all events with event time before some threshold T have been observed. If your watermark advances to T, you are declaring that you do not expect to see any more events with event time before T, or that any such late arrivals will be ignored or handled specially. Watermarks allow your pipeline to make progress even in the presence of out-of-order data, closing time windows and finalizing training batches with bounded uncertainty about completeness.

The challenge is setting watermark policies that balance timeliness against completeness. An aggressive watermark policy advances quickly, allowing your pipeline to close time windows and produce training data with minimal delay. But aggressive watermarks risk dropping late-arriving data, creating training sets that are missing events and no longer representative of the true distribution. A conservative watermark policy waits longer before advancing, ensuring you capture nearly all late arrivals, but delays training data availability and increases pipeline latency. The right policy depends on your domain's latency distribution and your tolerance for missing data.

For most systems, a percentile-based watermark works well. Measure the lag between event time and processing time across your data stream, compute the 95th or 99th percentile lag, and set your watermark to trail processing time by that amount. If 99% of your events arrive within five minutes, set your watermark to processing time minus five minutes. Events that arrive more than five minutes late are either dropped or routed to a late-arrival handling process that updates existing training examples or triggers retraining. This policy ensures you capture the vast majority of data while still allowing your pipeline to make progress.

Late-arriving data also affects model retraining schedules. If you retrain daily using yesterday's data but 2% of events from yesterday arrive today, your daily training sets are systematically incomplete. You either need to delay retraining to allow late arrivals to settle, or you need to implement a correction process that backfills training data when late events arrive. Many teams run two training processes: a fast daily retrain using watermarked data for quick iteration, and a slower weekly retrain using fully settled data for production deployment. This dual-speed approach balances agility and correctness.

## Handling Out-of-Order Events

Out-of-order events complicate feature computation and aggregation. If you maintain a rolling window of the last hour of user activity, and events arrive out of order, you cannot simply append new events to the window. You must insert them at the correct position based on event time, potentially invalidating previously computed aggregations.

Stream processing frameworks like Apache Flink and Apache Beam handle out-of-order events using windowing and triggers. You define windows based on event time, such as hourly or daily windows. The framework buffers events until the watermark advances past the window boundary, then triggers computation of the window's aggregation. Events that arrive after the watermark has passed are either dropped or trigger late-data handling logic that updates previous results.

For batch pipelines, out-of-order handling is simpler because you process data after it has settled. You sort data by event time before processing, ensuring that aggregations and joins operate on correctly ordered sequences. The tradeoff is latency: you wait for data to settle before processing, increasing the delay between event occurrence and model training.

Some domains have inherently unreliable event timestamps that require special handling. Mobile applications generate events with client-side timestamps that may be incorrect due to clock skew or manipulation. IoT devices in remote locations may buffer events for hours before uploading them. In these cases, you need fallback logic that validates event timestamps against processing timestamps, rejecting events with implausible event times and potentially using processing time as a proxy when event time is unavailable or untrustworthy.

## Processing Time Use Cases: When the Infrastructure Clock Matters

While event time should drive your modeling decisions, processing time remains essential for operational and infrastructure concerns. Processing time tells you when data arrived in your system, which is critical for monitoring pipeline health, detecting delays, debugging data quality issues, and managing storage and compute resources. You track processing time to answer questions like how far behind is my pipeline or which events were affected by yesterday's outage or how much data did we ingest in the last hour.

Processing time also drives your pipeline's scheduling and triggering logic. You typically trigger training jobs based on processing time milestones, such as after all data from yesterday has been processed or every six hours of processing time. You use processing time to decide when to close batches, when to advance watermarks, and when to compact or archive historical data. All of this is infrastructure orchestration, and infrastructure naturally operates on its own clock, processing time.

The key is to keep event time and processing time concerns separated and explicit. Your pipeline should track both timestamps for every record: an event time field that represents domain time and a processing time field that represents infrastructure time. Your modeling code uses event time. Your operational code uses processing time. You never allow one to substitute for the other, and you document clearly which timestamp is used for each decision. This separation prevents confusion and ensures that temporal logic remains correct even as your infrastructure evolves.

Some pipelines also track multiple processing timestamps to represent different stages of data flow: ingestion time when data first entered the system, warehouse time when it landed in storage, and training time when it was read for model training. Each of these markers serves a different operational purpose. Ingestion time helps you measure end-to-end latency from event occurrence to pipeline entry. Warehouse time helps you manage retention policies and query performance. Training time helps you track which data versions were used for which model versions. All are processing time variants, all are useful for operations, and none should replace event time for modeling.

## Practical Timestamp Management in Pipelines

Implementing robust event time handling requires explicit design decisions throughout your data pipeline. The first decision is where event time originates. Ideally, the system that generates the event assigns the event time timestamp at the moment of occurrence. For user interactions, the client application timestamps the event before sending it to your backend. For server-side events, the service handling the request timestamps the event immediately. For batch imports from external systems, you preserve the original timestamp from the source system rather than assigning a new one at import time.

The second decision is how event time propagates through transformations. Every stage of your pipeline that creates derived data must preserve or compute appropriate event times. If you aggregate click events into sessions, each session's event time might be the timestamp of the first click or the last click, depending on your semantics. If you join user events with account data, the join output inherits the event time from the event side, not from the account data which may have a different temporal lifecycle. You document these choices explicitly in your pipeline schema and validate them in testing.

The third decision is how you handle events with missing or unreliable timestamps. Client-generated timestamps can be incorrect due to clock skew, timezone confusion, or deliberate manipulation. You need validation logic that detects implausible timestamps, such as events claiming to occur in the future or far in the past, and a policy for handling them. Some systems fall back to processing time for invalid event times, but this undermines your temporal guarantees. A better approach is to reject invalid events entirely or route them to a quarantine for investigation, keeping your main pipeline free of temporal corruption.

The fourth decision is how you represent event time in storage and in your training set format. Use explicit, high-precision timestamp fields, not implicit orderings or integer counters. Store timestamps in UTC with timezone information preserved separately if needed for display purposes. Include both event time and processing time fields in your schemas so that downstream consumers can choose the appropriate timestamp for their needs. When serializing training examples, embed the event time directly in the example rather than relying on external metadata, ensuring that the temporal context travels with the data.

## Timezone and Precision Considerations

Event timestamps often originate from systems in different timezones. A global application receives events from users around the world, each with their local time. Converting these to a consistent timezone, typically UTC, is essential for correct temporal ordering. Timezone conversion must happen at ingestion time, not later in the pipeline, to avoid confusion about whether a timestamp represents local time or UTC.

Timestamp precision matters for high-frequency domains. Financial trading systems need microsecond precision to correctly order transactions. Click stream analytics might need millisecond precision to understand user interaction sequences. Storage systems and data formats must preserve this precision throughout the pipeline. Rounding or truncating timestamps to seconds can scramble event order and invalidate temporal logic.

Daylight saving time transitions create temporal anomalies that your pipeline must handle gracefully. When clocks spring forward, an hour disappears. When they fall back, an hour repeats. If your event timestamps use local time without timezone information, these transitions create ambiguous or missing timestamps. Always store event time in UTC to avoid DST complications, and convert to local time only for display or analysis purposes.

## Debugging Temporal Issues in Production

When your production model behaves differently than your offline evaluation predicted, timestamp confusion is one of the first places to investigate. Start by comparing the event time distribution in your training data versus your production traffic. If training examples cluster around certain times of day or days of week differently than production requests, you may have inadvertently filtered or sampled based on processing time patterns rather than event time patterns. Check whether your training set includes weekends and holidays proportionally to their real occurrence, or whether batch processing schedules caused some periods to be over- or under-represented.

Next, audit your feature computation logic for point-in-time violations. For a sample of production requests, manually reconstruct what feature values should have been available at the event time of the request, then compare against what your production feature service actually returned. Discrepancies indicate that your feature pipeline is leaking future information or using stale data. This audit is tedious but often reveals subtle bugs in join logic, caching policies, or aggregation windows that are invisible in standard testing.

Also examine your watermark and late-arrival policies. If you are dropping a significant fraction of late-arriving data in training but that same data is available in production through retry mechanisms or eventual consistency, your model is trained on a subset that does not match the production distribution. Check the lag distribution between event time and processing time in your logs. If the distribution has a long tail or shows bimodal patterns, you may need to adjust your watermark policy or implement late-arrival correction procedures.

Finally, validate that your train-eval split truly represents a temporal boundary. Sample examples from both sides of the split and verify that evaluation examples have event times strictly after training examples. Look for overlaps caused by processing time sorting or timestamp precision issues. A single violated temporal boundary can allow leakage that inflates your evaluation metrics and masks the model's true generalization performance.

## Testing Temporal Correctness

Testing temporal logic requires dedicated validation strategies. Unit tests should verify that feature computations respect event time boundaries. For a feature that aggregates the last 30 days of user activity, create test cases with events spanning 60 days, compute the feature for a specific event time in the middle, and verify that only events with event time before the query time are included. Test with out-of-order events to ensure aggregation logic sorts by event time, not arrival order.

Integration tests should validate end-to-end temporal correctness across your pipeline. Run your training pipeline on historical data where you know the ground truth event timestamps. Generate training examples for specific event times and manually verify that features reflect only data available at those times. Compare feature values computed by your pipeline against reference values computed by a simple, obviously correct implementation that explicitly filters by event time.

Shadow mode testing helps catch temporal bugs before production deployment. Run your new feature pipeline in parallel with production, computing features for live traffic using event time semantics. Compare the feature values your pipeline computes at decision time against the values that would have been available if you had used processing time or violated point-in-time constraints. Large discrepancies indicate temporal logic errors that would cause train-serve skew.

Regression tests should lock in temporal correctness once achieved. When you fix a temporal bug, add a test case that reproduces the bug with the old logic and passes with the corrected logic. These tests prevent future refactoring from reintroducing temporal errors. They also serve as documentation of the event time contracts your pipeline maintains.

## Building Temporal Discipline Into Your Workflow

Getting timestamps right is not a one-time fix, it is an ongoing discipline that must be embedded in your team's workflow. Every new data source that enters your pipeline must be evaluated for its event time semantics. What does the timestamp represent? Is it when the event occurred, when it was logged, when it was sent, or when it was received? If the source does not provide a reliable event time, can you infer one from other fields, or do you need to request a schema change from the upstream system?

Every feature addition must include a temporal correctness review. Does this feature use only information available at event time, or does it inadvertently depend on future data? If the feature joins multiple datasets, are the join keys and timestamp logic correct for point-in-time semantics? Document the event time assumptions for each feature explicitly so that future engineers understand the temporal contract and do not break it during refactoring.

Every model training run should log both the event time range and the processing time range of the data used. This metadata allows you to correlate model performance with data recency and detect issues where processing delays caused training sets to be stale. If a model trained on data processed last week performs worse than a model trained on data processed yesterday, even though both use the same event time range, you have a processing time dependency that needs investigation.

Temporal discipline also extends to your evaluation framework. Your evaluation splits must be defined and validated by event time. Your metrics dashboards should display both event time and processing time axes, allowing you to see whether model performance varies with data lag. Your A/B testing framework should bucket users or requests by event time, not by when the system happened to process them, ensuring that your experiments measure real user experience rather than infrastructure artifacts.

## Code Review Checklist for Temporal Logic

Code reviews must include specific checks for temporal correctness. When reviewing a pull request that touches data pipelines or feature engineering, verify that all timestamp references use event time fields, not processing time fields. Check that aggregation windows are defined using event time ranges. Confirm that joins align datasets on event time keys. Look for any use of database insertion timestamps, system clocks, or other processing time proxies where event time should be used.

Review temporal filtering logic carefully. Queries should filter data using event time predicates that enforce point-in-time correctness. A training example at event time T should include only features computed from data with event time strictly before T. Look for off-by-one errors where the boundary is inclusive when it should be exclusive, or where the comparison uses less-than-or-equal when it should use strictly less-than.

Validate that schema changes preserve event time semantics. If a pull request adds a new field or modifies an existing timestamp field, confirm that event time is correctly assigned and propagated. Check that documentation explains what the timestamp represents and which clock it uses. Ensure that downstream consumers are updated if the temporal semantics change.

Flag any code that mixes event time and processing time without clear separation. If processing time is used, it should be for operational concerns like monitoring or triggering, never for modeling decisions. The variable names, function signatures, and documentation should make it obvious which timestamp is which.

## Training New Team Members

New engineers joining your team need explicit training on event time versus processing time. This distinction is not intuitive, and most engineers have not encountered it in traditional software development. Onboarding should include documentation that explains the two clocks, provides examples of temporal errors and their consequences, and establishes the team's standards for temporal correctness.

Pair new engineers with experienced team members on their first few tasks involving timestamps. Walk through existing feature logic together, explaining why event time is used at each step and what would break if processing time were used instead. Review pull requests together, pointing out temporal correctness checks and common mistakes to avoid.

Create internal reference examples that demonstrate correct temporal patterns. Show how to implement a rolling window aggregation with event time semantics. Show how to construct point-in-time correct features from slowly changing dimension tables. Show how to handle late-arriving data and out-of-order events. These reference implementations serve as templates that new engineers can adapt for their own work.

Run internal workshops or brown bag sessions where the team discusses temporal correctness. Present case studies of temporal bugs from your own history or from industry incidents. Discuss the debugging process and the lessons learned. This shared understanding builds a culture where temporal correctness is taken seriously and mistakes are caught early.

## Documentation Standards

Every dataset and feature in your pipeline should have documentation that specifies its temporal semantics. For each timestamp field, document whether it represents event time or processing time, what specific moment it captures, and how it should be used. For features, document the event time window over which they are computed and the point-in-time guarantees they provide.

Schema documentation should include examples of correct and incorrect timestamp usage. Show a query that correctly filters data using event time. Show a common mistake where processing time is used inappropriately and explain why it fails. These examples help engineers apply the temporal rules correctly without needing to consult senior team members for every decision.

Pipeline documentation should describe watermarking policies, late-arrival handling, and out-of-order event processing. Explain why the team chose specific watermark thresholds and what tradeoffs they represent. Document the expected latency distribution between event time and processing time so that engineers know what delays are normal and what delays indicate infrastructure problems.

Model cards and experiment documentation should record the event time range of training data, not just the processing time range. This information helps debug performance regressions and understand whether model improvements come from better algorithms or simply from training on more recent data. It also supports reproducibility, allowing future engineers to reconstruct the exact temporal slice of data used for training.

## The Cultural Shift

Establishing temporal correctness as a core discipline requires a cultural shift in how your team thinks about data. Engineers must move from thinking about data as static rows in tables to thinking about data as events unfolding in time. They must internalize the distinction between when something happened and when the system learned about it. They must develop intuition for how temporal logic affects model behavior.

This shift takes time and leadership commitment. Engineering managers must allocate time for temporal correctness work, recognizing that it is foundational infrastructure, not a nice-to-have feature. Product managers must accept that building temporal discipline adds upfront cost that pays off in reduced production incidents and more reliable model performance. Executives must understand that temporal errors can cause models to fail in ways that are invisible in development but catastrophic in production.

The payoff is substantial. Teams with strong temporal discipline ship models that perform as expected in production. They avoid entire categories of data leakage and train-serve skew. They debug issues faster because they can trust their temporal logic and focus on other potential causes. They build stakeholder confidence by consistently delivering models that meet offline promises.

Getting timestamps right is foundational. Event time represents reality. Processing time represents your infrastructure's view of reality. Never confuse the two. With temporal correctness established, you can turn to another major data collection challenge that has transformed dramatically in the last two years: web scraping and crawling in 2026's new legal and ethical landscape.
