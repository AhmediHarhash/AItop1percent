# 9.2 â€” PII Detection: Automated Scanning and Classification

Manual PII detection does not scale and never will. A human reviewer reading 100 support tickets per hour, eight hours per day, five days per week can review 200,000 tickets per year. A production training dataset for a modern language model contains 50 billion tokens drawn from hundreds of millions of documents. At 100 documents per hour, you would need 2,500 human-years of effort to review the dataset once. By the time the review finished, the dataset would be obsolete and your model would be five generations behind your competitors. Manual review is appropriate for spot-checking, for validating automated detection, and for investigating flagged edge cases. It is not appropriate as your primary detection strategy.

Automated PII detection is the only viable approach at scale, but automated detection is not a solved problem. It is a problem with known techniques, well-understood tradeoffs, and failure modes that you must design for explicitly. The techniques fall into three broad categories: regex-based, named entity recognition-based, and LLM-based classification. Each has strengths. Each has blind spots. Production PII detection pipelines in 2026 use all three in sequence, with human review reserved for high-confidence ambiguous cases and low-confidence high-risk samples.

## Regex-Based Detection: Fast and Brittle

Regular expressions are the oldest and fastest PII detection technique. A regex pattern can match Social Security numbers, credit card numbers, phone numbers, email addresses, IP addresses, and other structured identifiers with high precision and zero inference latency. Regex scanning scales linearly with text volume: scanning a billion tokens takes a billion pattern matches, but modern regex engines process millions of matches per second. For PII that follows a fixed format, regex is the correct tool.

The limitation is that regex matches syntax, not semantics. A regex for US Social Security numbers will match any nine-digit string formatted as three digits, hyphen, two digits, hyphen, four digits. It will match real SSNs. It will also match fake SSNs used as examples in documentation, test data formatted like SSNs, and random numbers that happen to fit the pattern. It will not match SSNs written without hyphens, SSNs with spaces instead of hyphens, or SSNs embedded in prose like "my social is nine one two eight three four five six seven eight." The regex does not understand context. It does not know whether "123-45-6789" is a real identifier or a placeholder. It flags everything that matches, and you must decide whether each match is a true positive.

False positives create operational burden. If your regex flags 10,000 potential PII matches in a dataset and 90 percent are false positives, a human reviewer must examine 9,000 examples to find the 1,000 real leaks. If the cost of review is too high, teams start ignoring flags or tuning the regex to reduce false positives. Tuning for precision reduces recall: you catch fewer false positives but you also miss more true positives. The tradeoff is unavoidable. The correct calibration depends on your risk tolerance, your review capacity, and the cost of a miss.

False negatives are more dangerous because they are invisible. A regex tuned to match only nine-digit strings with hyphens will silently pass over SSNs written as "SSN 912834567" or "social security number nine one two eight three four five six seven eight." Those examples are rare in clean structured data but common in unstructured text where humans write naturally. Support tickets, chat logs, forum posts, and user-generated content are full of PII written in formats that regex cannot handle. Regex-based detection is necessary but never sufficient.

## Named Entity Recognition: Context-Aware but Noisy

Named entity recognition models detect PII by classifying spans of text as person names, organizations, locations, dates, and other entity types. Modern NER models are transformer-based classifiers trained on labeled corpora to recognize entities in context. An NER model can distinguish between "Jordan" as a person name, "Jordan" as a country, and "Jordan" as a shoe brand based on surrounding words. It can detect names that do not appear in any gazetteer. It can handle misspellings, abbreviations, and informal text.

Production NER models in 2026 are fine-tuned versions of GPT-5, Claude Opus 4, Llama 4, or domain-specific transformers like BioBERT for medical text and FinBERT for financial text. These models achieve F1 scores above 90 percent on benchmark datasets for common entity types in clean text. Performance degrades on noisy text, domain-specific jargon, and rare entity types. An NER model trained on news articles will perform worse on customer support tickets, medical notes, or social media posts. An NER model trained on English will fail on multilingual datasets unless explicitly trained for cross-lingual transfer.

The tradeoff between precision and recall remains. An NER model tuned for high recall will flag many entity mentions, including non-PII entities like product names, fictional characters, and historical figures. A model tuned for high precision will miss ambiguous cases: names that are also common nouns, acronyms that might be organizations or meaningless letter strings, and partial names that appear without context. The calibration depends on your risk model. If you are training a public-facing consumer model, you tune for high recall and accept the false positive burden. If you are curating a dataset for internal tooling, you might tune for precision and accept the risk that some PII slips through.

NER-based detection requires compute. A transformer-based NER model processes thousands of tokens per second on a GPU, which sounds fast until you are scanning 50 billion tokens. Scanning a large dataset with NER takes hours or days even on distributed infrastructure. The cost is not prohibitive, but it is not negligible. You must budget for it in your data pipeline and plan for periodic rescanning as models improve and new PII patterns emerge.

## LLM-Based Classification: Flexible but Expensive

Large language models can classify arbitrary text spans as PII or non-PII given a prompt and a few examples. An LLM-based PII detector uses a model like GPT-5, Claude Opus 4.5, or a fine-tuned open-source model to read a piece of text and answer the question: does this contain PII? The model can reason about context, disambiguate edge cases, and handle PII types that were not seen during training. It can detect PII in natural language descriptions: "the patient is a 34-year-old woman from Brooklyn who works as a nurse" is flagged as quasi-identifying even though it contains no direct identifiers.

The strength of LLM-based detection is flexibility. You can adapt the detector to new PII types by changing the prompt. You can tune sensitivity by adjusting the instructions: "flag anything that could be used to identify a person" versus "only flag direct identifiers like names and SSNs." You can incorporate domain knowledge by providing examples in the prompt: "in this dataset, employee ID numbers are six-digit codes starting with E." The model generalizes from the instructions and examples without retraining.

The limitation is cost. LLM inference is orders of magnitude slower and more expensive than regex or NER. GPT-5 in February 2026 costs approximately 0.50 dollars per million input tokens. Scanning a 50 billion token dataset would cost 25,000 dollars in API fees, assuming you send every token through the LLM. Latency is also a constraint: even with batch processing, scanning billions of tokens takes days. Self-hosted open-source models reduce cost but add infrastructure complexity and still require significant GPU resources.

The practical approach is to use LLMs selectively: run regex and NER first, flag ambiguous cases, and send only those cases to an LLM for final classification. This multi-pass pipeline reduces LLM cost by two orders of magnitude while preserving the flexibility to handle edge cases. A dataset of 50 billion tokens might have 10 million flagged spans after regex and NER. Sending 10 million spans to an LLM for classification costs hundreds of dollars instead of tens of thousands.

## PII Taxonomies: Direct, Quasi, and Sensitive

Not all PII is equally identifying. Regulatory frameworks and privacy research distinguish between direct identifiers, quasi-identifiers, and sensitive attributes. Understanding the taxonomy changes how you design detection and how you prioritize remediation.

Direct identifiers uniquely identify an individual in isolation: Social Security numbers, passport numbers, driver's license numbers, email addresses, full names in most contexts, biometric data, and account numbers. A single direct identifier is sufficient to re-identify a person. If your dataset contains direct identifiers, removal is non-negotiable. Detection must prioritize recall: missing even one SSN or email address is a violation.

Quasi-identifiers do not uniquely identify an individual alone but can identify someone when combined. Zip code, age, and gender are the canonical example: 87 percent of the US population can be uniquely identified by the combination of five-digit zip code, birthdate, and gender according to research by Latanya Sweeney published in 2000 and replicated many times since. Other quasi-identifiers include employer name, job title, race, diagnosis codes, and temporal patterns like admission dates or purchase timestamps. Quasi-identifiers are harder to detect because they are not inherently sensitive in isolation. Detection requires understanding which attributes might combine to enable re-identification.

Sensitive attributes reveal information that individuals might not want disclosed even if it does not directly identify them: health conditions, sexual orientation, religious beliefs, political opinions, genetic data, and financial status. GDPR designates these as special categories of personal data requiring heightened protection. The EU AI Act imposes additional restrictions on AI systems that process sensitive attributes for high-risk use cases. Detection is context-dependent: the word "diabetic" in a medical dataset is a sensitive attribute; the same word in a product review of glucose monitors is not.

Your PII detection pipeline must handle all three categories. Direct identifiers get flagged and removed or redacted unconditionally. Quasi-identifiers get flagged for generalization or suppression depending on re-identification risk. Sensitive attributes get flagged for compliance review and potential exclusion from training data depending on use case and jurisdiction. The taxonomy drives the detection logic: regex and NER focus on direct identifiers, LLM-based classification handles quasi-identifiers and sensitive attributes that require contextual reasoning.

## Precision vs Recall: The Unavoidable Tradeoff

Every detection system operates on a precision-recall curve. High recall means catching most PII but flagging many false positives. High precision means most flags are real PII but some real PII slips through. You cannot maximize both. You must choose a point on the curve based on the cost of false positives versus the cost of false negatives.

False positives waste human review time and potentially remove useful training data. If your detector flags 100,000 examples and 80 percent are false positives, your reviewers will spend days or weeks confirming that 80,000 examples are clean. If you choose to remove flagged examples without review to save time, you lose 80,000 legitimate training samples, which degrades model quality. The cost of false positives scales with the volume of data and the scarcity of review resources.

False negatives leak PII into your training data, which creates regulatory, legal, and reputational risk. A single missed SSN in a training dataset can become a GDPR violation if the model memorizes it and an attacker extracts it. A single missed patient name can become a HIPAA breach. The cost of false negatives is not linear with volume: one catastrophic leak can trigger the same regulatory response as a hundred leaks. The probability of catastrophic failure increases with false negative rate.

The correct calibration depends on your risk tolerance and use case. If you are training a public-facing model that will be adversarially probed, you tune for high recall and accept the false positive burden. You would rather remove 10,000 clean examples than miss one SSN. If you are curating a dataset for internal analytics where the model is not exposed and extraction risk is low, you might tune for balanced precision and recall or even favor precision to preserve data utility. There is no universal answer. The answer is a risk decision that engineering cannot make alone: it requires input from legal, compliance, and leadership.

## Multi-Pass Detection Pipelines

Production PII detection in 2026 uses staged pipelines where each stage applies a different technique and handles different tradeoffs. The standard architecture is a three-stage pipeline: regex scan, NER scan, LLM classification.

Stage one is regex scanning for high-confidence direct identifiers. You run pattern matching for SSNs, credit card numbers, phone numbers, email addresses, IP addresses, and other fixed-format identifiers. Regex matches are flagged immediately for removal or redaction. False positives are acceptable because regex is fast enough to run on the entire dataset and most regex patterns have high precision. You review regex flags manually only for ambiguous formats like dates or numeric strings that might be identifiers or might be product codes.

Stage two is NER scanning for names, locations, organizations, and dates. You run a transformer-based NER model on the full dataset or on a sample if the dataset is too large. NER flags are classified by entity type: person names are high priority, organization names might be acceptable depending on context, locations are reviewed case by case. You deduplicate NER flags against regex flags to avoid double-flagging. You send high-confidence NER matches for removal and low-confidence matches to stage three.

Stage three is LLM-based classification for ambiguous cases and quasi-identifiers. You send examples that passed regex and NER but contain potential PII based on keyword heuristics, examples that were flagged by NER with low confidence, and a random sample of unflagged examples for validation. The LLM prompt asks: does this text contain information that could identify an individual or that is sensitive under GDPR? The LLM response is yes, no, or uncertain. Yes answers trigger removal. Uncertain answers go to human review. No answers are logged for periodic audit.

The pipeline is tuned by adjusting thresholds at each stage. If you are seeing too many false positives, you raise the confidence threshold for NER or tighten the regex patterns. If you are seeing PII in production that should have been caught, you lower thresholds, expand the LLM review sample, or add new regex patterns for the missed cases. Tuning is iterative and never finished: new PII patterns emerge, new data sources are added, and adversaries find new extraction techniques.

## Handling PII in Unstructured vs Structured Fields

Structured data is easier to scan because identifiers live in predictable places. A database table with columns for email, phone, and address can be scanned by running detection on those columns only. You know the schema. You know which fields are high-risk. You can apply field-level redaction or encryption and be confident you caught the PII.

Unstructured data is harder because PII appears anywhere in freetext. A customer support ticket might have the customer's email in the from field, but it might also have the email repeated in the body, the phone number in the signature, the address in a quoted reply, and the account number in the subject line. Detection must scan the entire text, not just labeled fields. The same text might be stored in multiple formats: original email, processed ticket, summarized case note, extracted intent label. You must scan all representations or accept the risk that PII in one format leaks through.

Semi-structured data like JSON, XML, or CSV files combines both problems. Field names might indicate PII: a JSON key called "ssn" is high-risk. But field names might be generic: a key called "notes" could contain anything. You must scan field names as metadata and field values as unstructured text. You must handle nested structures, arrays, and escaped characters. A JSON object serialized as a string inside a CSV field is unstructured text even though it has structure. Your detection logic must parse before scanning or treat everything as text.

Log files are a particularly insidious source of PII. Application logs capture user activity, error messages, API requests, and system events. Developers log request payloads for debugging. Support engineers log session IDs and user inputs for troubleshooting. Logs are not curated. They are not reviewed before archival. They accumulate in S3 buckets or logging platforms for months or years. If you ingest logs into training datasets without scanning, you are almost certainly ingesting PII: email addresses in login events, IP addresses in access logs, session tokens in API logs, and freetext user inputs in error traces.

## The Hidden PII Problem

The hardest PII to detect is PII that does not look like PII until you combine it with other data. A dataset of purchase timestamps is not PII. A dataset of product categories is not PII. A dataset of zip codes is not PII. A dataset containing all three for the same users is highly identifying: the combination of when you shop, what you buy, and where you live creates a behavioral fingerprint that can re-identify you even without your name.

File metadata is another hidden source. Image files contain EXIF data with GPS coordinates, timestamps, and device identifiers. PDFs contain author names, creation timestamps, and editing history. Office documents contain tracked changes with usernames. If you extract text from documents without stripping metadata, that metadata propagates into your training data. An LLM trained on PDFs with author metadata can memorize and regurgitate author names.

URLs are often overlooked. A URL like "https://example.com/user/profile?id=12345" contains a user identifier. A URL like "https://example.com/orders/2024/03/15/johndoe" contains a name and a date. If your dataset includes web scrapes, forum archives, or user-submitted links, those URLs might embed PII. Standard PII detection focuses on text content and ignores URLs. You must scan URLs separately or accept the leakage.

Log messages written by engineers often embed PII unintentionally. A debug log that says "User jdoe@example.com failed login attempt from 192.168.1.1" contains an email and an IP address. A performance log that says "Query took 3.2 seconds for account 9876543" contains an account number. Engineers write logs for readability, not for privacy. Unless your logging framework automatically redacts PII or your detection pipeline scans log text, those identifiers leak.

## Tools and Approaches in 2026

The PII detection tooling landscape in February 2026 includes open-source libraries, cloud-managed services, and custom pipelines. Microsoft Presidio is the most widely used open-source framework, providing regex and NER-based detection with extensible recognizers for custom PII types. Presidio supports 20 languages and integrates with spaCy, Transformers, and Azure AI. It is production-ready but requires tuning for domain-specific PII.

Google Cloud Data Loss Prevention API offers managed PII detection with over 150 built-in detectors covering identifiers from 30 countries. DLP API handles structured and unstructured data, integrates with BigQuery and Cloud Storage, and supports custom detectors defined by regex or dictionaries. Pricing is per API call, which becomes expensive at scale but eliminates infrastructure management.

AWS Comprehend DetectPiiEntities is a managed NER service for PII detection in text. It detects names, addresses, credit cards, SSNs, and other identifiers with language support for English, Spanish, French, German, Italian, and Portuguese. Comprehend is faster and cheaper than DLP API but less customizable. Both cloud services handle detection but do not handle remediation: you must build your own redaction or generalization logic.

LLM-based detection is custom-built in most production systems because commercial LLM APIs do not offer PII-specific detection endpoints. Teams fine-tune Llama 4, Mistral, or Qwen models on annotated PII datasets or use GPT-5 and Claude Opus 4.5 with few-shot prompting. The fine-tuning approach reduces inference cost but requires labeled training data. The few-shot prompting approach is faster to deploy but more expensive per inference.

The build-versus-buy decision depends on scale, customization needs, and compliance requirements. If you are scanning millions of documents per month and need custom PII types, you build a pipeline using Presidio or a custom NER model. If you are scanning thousands of documents per month and need compliance certifications, you use a managed service. If you are scanning billions of documents or need real-time detection in a streaming pipeline, you optimize for throughput and build custom GPU-accelerated detection with batch processing.

## What Happens After Detection

Detection is the gate. What you do with flagged PII is the remediation decision. The options are removal, redaction, masking, generalization, or synthetic replacement. Each option has different implications for data utility, re-identification risk, and regulatory compliance.

Removal deletes the entire example containing PII. This is the safest option for privacy but the most destructive for data utility. If 10 percent of your support tickets contain PII and you remove them, you lose 10 percent of your training data including the non-PII context in those tickets. Removal is appropriate for direct identifiers that cannot be safely redacted, for examples where PII is central to the content, and for cases where re-identification risk is unacceptably high.

Redaction, masking, and generalization preserve the example while removing or obscuring the PII. These techniques balance privacy and utility but introduce re-identification risk if not applied carefully. The specifics of these techniques and the risk calculus around them are the subject of the next subchapter, which covers the spectrum of de-identification approaches and the tradeoffs that determine which technique is appropriate for which PII type and use case.
