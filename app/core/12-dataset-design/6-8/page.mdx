# 6.8 â€” Cross-User Leakage Prevention: Same User Across Splits

In August 2025, a personalized learning platform launched a content recommendation model that achieved 87% precision on their held-out test set. They had carefully split their dataset by user ID, ensuring that no user appeared in both training and test. The model went live. Within ten days, the data science team noticed something wrong: production precision was 71%, a full sixteen points below the test estimate. They audited the test set and confirmed that the split was correct by user ID. No user ID appeared in both splits. But the gap was real.

The failure was identity fragmentation. The platform allowed users to access content as guests before creating accounts. A guest session received a temporary device ID. If the guest later registered, they received a permanent user ID. The two IDs were linked in the product database, but not in the evaluation dataset. The data team had split by user ID, but many registered users had prior guest sessions under different device IDs. The training set contained guest sessions from users whose registered sessions were in the test set. The model learned user preferences from the guest data, then was tested on the same users under their registered IDs. The split appeared clean, but the user was the same. Precision was inflated by fifteen points.

This is cross-user leakage: when the same real-world user appears in multiple splits under different identifiers. It is one of the most common and most invisible bugs in evaluation datasets. User-based splits are only effective if you correctly define user identity. If your identity linking is incomplete, your splits leak. If your identity linking is wrong, your splits leak. If your identity definition does not match the way users actually interact with your system, your splits leak. The test metrics look strong, but they are testing the model's ability to recognize users it has already seen, not its ability to generalize to new users.

## Why User Identity Is Harder Than It Looks

User identity seems straightforward. Your system has user IDs. You split by user ID. But user identity in production systems is fragmented across devices, sessions, accounts, and contexts. A single human user might interact with your system through multiple identities, and your dataset must recognize them as the same user to prevent leakage.

The most common fragmentation is device-based. A user accesses your product on their phone, their laptop, and their tablet. Each device has a distinct device ID. If you split by device ID, the same user appears in train and test under three different IDs. A music streaming service building a playlist recommendation model split by device ID, reserving 15% of devices for testing. They assumed each device represented a distinct user. In reality, 40% of their active users accessed the service on multiple devices. The test set contained many users whose other devices were in the training set. The model learned user preferences from one device, then was tested on the same user's other device. Recommendation precision was inflated by twelve points.

Another fragmentation is account-based. Some users create multiple accounts. A user might have a work account and a personal account. A user might create a second account after forgetting the password to their first. A user might create throwaway accounts for privacy reasons. If you split by account ID, the same user appears under multiple accounts. A customer support automation tool built by an e-commerce company split by account ID. They did not realize that 8% of their users had created multiple accounts, often to take advantage of new-user discounts. The model learned customer issue patterns from one account, then was tested on the same customer's alternate account. The test set showed 84% resolution accuracy. Production showed 78%. The six-point gap was account leakage.

Session-based fragmentation is also common. Some systems assign a new session ID every time a user visits. If sessions are short-lived and users visit frequently, a single user might have hundreds of session IDs. If you split by session ID, you are not splitting by user; you are splitting by visit. A news personalization system split by session ID, treating each session as an independent user. The majority of their traffic came from returning users with many sessions. The training set contained earlier sessions from users whose later sessions were in the test set. The model learned user reading preferences from the earlier sessions and was tested on the later sessions. Test click-through rate was 19%. Production click-through rate for new users was 11%. The eight-point gap was session leakage.

Even when you have stable account IDs, users can share accounts. A family shares a Netflix account. A team shares a corporate Slack workspace. A classroom shares a learning platform account. If multiple humans use the same account, the account ID does not represent a single user. It represents a group. If you split by account ID, you are testing whether the model can generalize to new groups, not whether it can generalize to new individuals. This might be the correct test for your product, but you must define it explicitly.

## Building a Unified User Identity Graph

Preventing cross-user leakage requires you to link all identifiers that belong to the same real-world user into a unified user identity graph. This graph maps device IDs, session IDs, account IDs, email addresses, phone numbers, IP addresses, and any other identifiers to a single canonical user ID. When you split your dataset, you split by canonical user ID, ensuring that all identifiers linked to the same user are assigned to the same split.

The identity graph is built by joining logs from multiple sources. Your authentication logs link session IDs to account IDs. Your device fingerprinting logs link device IDs to session IDs. Your email verification logs link email addresses to account IDs. Your billing logs link payment methods to account IDs. You join these logs to create a many-to-one mapping from all identifiers to canonical user IDs. A single canonical user ID might map to three device IDs, ten session IDs, two email addresses, and one account ID.

A financial services company building a transaction categorization model in late 2024 initially split by account ID. They later discovered that many users accessed their accounts from multiple devices and browsers, generating distinct device fingerprints. They built an identity graph by linking device fingerprints to account IDs via login events. The graph revealed that 35% of accounts were accessed from multiple devices. They re-split by canonical user ID, ensuring that all devices linked to the same account were in the same split. Test accuracy dropped from 91% to 86%. The five-point drop was device leakage that the account-only split had missed.

The identity graph must handle ambiguity. Sometimes two identifiers are linked with high confidence: a login event directly ties a session ID to an account ID. Sometimes the link is probabilistic: two sessions from the same IP address within ten minutes might belong to the same user, or they might belong to two users on the same network. You must decide how aggressively to merge identities. Aggressive merging reduces leakage but risks merging distinct users. Conservative merging preserves distinct users but risks missing links that cause leakage.

One strategy is to use deterministic links for splitting and probabilistic links for analysis. Deterministic links are events where you know with certainty that two identifiers belong to the same user: a login event, an email verification, a device handoff within the same session. Probabilistic links are heuristics: same IP address, same geolocation, same user agent, same behavioral pattern. You build your canonical user IDs from deterministic links only, ensuring that you do not incorrectly merge distinct users. You then analyze probabilistic links to estimate residual leakage. If 5% of your test set has probabilistic links to training set users, you know your test metrics might be inflated by up to 5%.

## Account Linking Across Login and Guest Sessions

Many products allow users to interact as guests before requiring account creation. The guest session generates examples under a guest ID. The registered session generates examples under an account ID. If you do not link guest and account IDs, the same user appears in both train and test.

A ride-sharing app built a demand forecasting model in early 2025. Users could request quotes as guests before creating accounts to book rides. The data team split by account ID, assuming that guest sessions were anonymous and uncorrelated. They missed that 60% of guest sessions converted to account creation within three days, and the conversion event was logged with both the guest session ID and the new account ID. The training set contained guest sessions from users whose account sessions were in the test set. The model learned demand patterns from the guest sessions, then was tested on the same users after registration. Forecast accuracy was overstated by nine points.

The fix is to link guest IDs to account IDs at conversion time. When a guest creates an account, you write a mapping from the guest session ID to the new account ID. You then propagate this mapping back through your dataset, reassigning all examples from the guest session to the canonical user ID of the account. If a user had five guest sessions before registering, all five sessions are now attributed to the registered user's canonical ID. When you split by canonical ID, all guest and registered sessions for that user are assigned to the same split.

Some products use device-based linking even without account creation. If a guest session on a mobile device has a persistent device ID, and the user later registers on the same device, you link the guest session to the account via the device ID. This is probabilistic if users share devices, but deterministic if you use device fingerprints that include app installation ID or hardware identifiers.

## Handling Shared Accounts and Household Identities

Shared accounts complicate user identity. If two people use the same account, the account ID represents two users. If you split by account ID, you might place one user's early examples in train and the same user's later examples in test, or you might place one user's examples in train and the other user's examples in test. Both are forms of leakage if the two users have correlated behavior.

A video streaming platform built a watch-next recommendation model in 2025. Many accounts were shared by families or roommates. The platform's identity system assigned a single account ID to each subscription, but users could create separate profiles under the same account. The data team initially split by account ID, treating each account as a single user. This leaked profile-level preferences across splits. Two profiles under the same account often had correlated viewing patterns: they watched the same shows, or they watched shows in the same genre. The model learned preferences from one profile in train and was tested on the other profile under the same account in test. Recommendation precision was inflated by fourteen points.

The fix was to split by profile ID, treating each profile as a distinct user. This required redefining canonical user ID to be profile ID, not account ID. The identity graph linked device IDs and session IDs to profile IDs, not to account IDs. When the data team re-split by profile ID, they found that test metrics dropped significantly, because they were now testing generalization to truly new users, not just new profiles under known accounts.

If your product does not have profiles but does have shared accounts, you must decide whether to test generalization to new accounts or generalization to new individuals. If you test generalization to new accounts, you accept that the test set includes new individuals who share accounts with training set individuals. If you test generalization to new individuals, you must infer individual identity from behavioral signals: distinct viewing times, distinct device IDs, distinct genres. This is harder, but it gives you a more accurate estimate of cold-start performance.

## Cross-Device Tracking and Identity Persistence

Users access products across devices, and each device generates a distinct identifier unless you explicitly link them. Cross-device tracking links device IDs to the same user, preventing device-based leakage.

The most reliable cross-device link is login. When a user logs into their account on a new device, you record the device ID and link it to the account ID. From that point forward, all activity on that device is attributed to the account. If the user later uses the same device without logging in, you can still attribute activity to the account if the device ID is persistent.

Some products use deterministic device identifiers: Apple's IDFV, Google's Android Advertising ID, browser fingerprints that include installed fonts and canvas signatures. These identifiers are stable across sessions on the same device. If you link them to account IDs at login time, you can track the user across logged-in and logged-out sessions on the same device. If you do not link them, you will treat the logged-in and logged-out sessions as distinct users, leaking user identity across splits.

A health and fitness app built an activity recommendation model in mid-2025. Users could track workouts while logged out, and the app stored activity data locally on the device. When the user later logged in, the app synced local data to their account. The data team split by account ID, but they did not link pre-login device IDs to post-login account IDs. The training set contained pre-login activity from users whose post-login activity was in the test set. The model learned workout patterns from the device data, then was tested on the same users after login. Test accuracy was inflated by eleven points. The fix was to link device IDs to account IDs at sync time, ensuring that all pre-login and post-login data from the same device was attributed to the same canonical user.

Cross-device tracking also requires you to handle device churn. Users replace phones, switch browsers, clear cookies, and reinstall apps. Each time a device identifier is reset, the user appears as a new device. If you split by device ID, the same user appears under multiple device IDs across splits. You must link the new device ID to the old device ID via login events, ensuring that both devices are attributed to the same canonical user ID.

## Temporal Aspects of Identity Linking

Identity linking is not static. Users create accounts over time. They add new devices over time. They merge accounts, delete accounts, and change email addresses. Your identity graph must reflect the state of identity at the time each example was created.

A common mistake is to use the current identity graph to label historical data. You export today's identity graph, which includes all links created up to today, and you apply it to examples from six months ago. But six months ago, some of those links did not exist. A user who created a second account last month should not have that account linked to examples from six months ago, because the second account did not exist then. If you link them retroactively, you are creating a temporal leak: you are using future information to label past examples.

The correct approach is to snapshot the identity graph at the timestamp of each example. For each example, you use the set of identity links that existed at the time the example was created. This requires you to maintain a time-series identity graph, where each link has a creation timestamp. When you build your dataset, you join examples to the identity graph as of their example timestamp, not as of today.

A social media platform building a friend recommendation model in late 2024 used the current identity graph to link user IDs across devices. The graph included account merges that had occurred after some of the examples were created. The training set contained examples from a user before they merged their accounts, and the test set contained examples from the same user after the merge, but under a different canonical user ID because the merge had reassigned the canonical ID. The split appeared clean, but the user was the same. The model was tested on users it had already seen. Recommendation precision was overstated by eight points. The fix was to use time-stamped identity links, ensuring that examples were attributed to the canonical user ID that was active at the time the example was created.

## Detecting User Leakage in Existing Splits

If you have already split your dataset and you suspect user leakage, you can audit the split by analyzing example similarity across splits. If examples in the test set are highly similar to examples in the training set, and the similarity is concentrated in specific users, you likely have user leakage.

One method is to compute pairwise similarity between test examples and training examples using embedding vectors. You embed each example using a pre-trained model, then for each test example, you find the nearest training example by cosine similarity. If the nearest training example is from a different user, the similarity is cross-user. If the nearest training example is from the same user, the similarity is within-user, and you have leakage. You measure the distribution of nearest-neighbor user IDs. If a significant fraction of test examples have nearest neighbors from the same user, your split has leaked.

A legal document classification system built by a document automation company in early 2025 split by client account ID. They later ran a leakage audit by embedding documents and finding nearest neighbors. They found that 22% of test documents had their nearest training neighbor from the same client, but under a different matter ID. The client had multiple legal matters, and the data team had treated each matter as a distinct user. The matters were correlated: they used similar contract templates and legal language. The model had learned client-specific language from one matter and was tested on another matter from the same client. The split was leaking client identity through matter IDs. They re-split by client ID, ensuring that all matters from the same client were in the same split, and test accuracy dropped by seven points.

Another detection method is to measure test performance conditional on user activity in the training set. For each test user, count how many examples from linked user IDs appear in the training set. If linked IDs have zero training examples, the test user is truly held-out. If linked IDs have many training examples, the test user is partially leaked. You plot test performance as a function of training set exposure. If performance increases with exposure, you have leakage. A flat relationship suggests the split is clean.

## Edge Cases: Bots, Fraud, and Adversarial Users

Some identifiers in your dataset do not represent real users. Bots generate automated traffic. Fraudulent users create disposable accounts. Adversarial users create accounts to test your system. If you do not filter these identifiers, they will appear in your splits and distort your metrics.

Bots often have distinctive behavioral patterns: they generate requests at inhuman speed, they access endpoints in non-human sequences, they use automated user agents. If you split by user ID without filtering bots, your test set will include bot traffic, and your model will be tested on bot behavior rather than human behavior. If your model learns to detect bots during training, it will perform well on bot examples in the test set, but this does not reflect performance on real users.

A content delivery platform built a caching recommendation model in mid-2025. They split by request session ID, treating each session as a user. They did not filter bot traffic. Approximately 18% of sessions were bots: search engine crawlers, monitoring services, and scraping scripts. The bots had highly predictable request patterns. The model learned to predict bot requests with 96% accuracy. Human requests were predicted with 78% accuracy. The overall test set showed 84% accuracy, a blend of bot and human performance. Production performance, measured only on human traffic, was 78%. The six-point gap was bot leakage. The fix was to label and exclude bot sessions from the dataset before splitting.

Fraudulent users create many short-lived accounts. A fraud prevention system must test generalization to new fraud patterns, not memorization of known fraudster IDs. If you split by account ID, and a fraudster has created dozens of accounts, some of those accounts will be in train and some in test. The model can learn the fraudster's behavioral signature from the training accounts and detect the test accounts, even though the account IDs are different. This is within-user leakage across accounts.

The solution is to link fraud accounts by behavioral fingerprints: IP address, device fingerprint, payment method, shipping address, behavioral timing. If multiple accounts share these signals, they likely belong to the same fraudster. You assign them the same canonical user ID, ensuring that all accounts from the same fraudster are in the same split. This tests whether your model can detect new fraudsters, not whether it can recognize known fraudsters under new account IDs.

## Identity Linking as a Data Engineering Dependency

Building and maintaining a unified identity graph is not a one-time task. It is an ongoing data engineering dependency. User identities evolve. New devices are added. Accounts are merged. Email addresses change. Your identity graph must be updated continuously, and your dataset splits must be regenerated when identity links change.

This means your evaluation pipeline must be reproducible. You cannot manually assign examples to splits. You must define the split logic as code: load the identity graph, load the examples, join them, assign canonical user IDs, sort by canonical ID, assign splits. When the identity graph updates, you re-run the split logic, and your splits update to reflect the new links. If you do not automate this, your splits will drift out of sync with your identity graph, and leakage will reappear.

A B2B SaaS company building a feature usage prediction model maintained an identity graph that linked user emails to company domains. When an employee left a company and their email was deactivated, the identity graph removed the link. When a company was acquired and employees migrated to a new domain, the identity graph added new links. The evaluation dataset was re-split quarterly to reflect identity graph updates. This ensured that splits remained clean as user identities evolved.

Identity linking is also a privacy and compliance concern. Linking device IDs, email addresses, and IP addresses to create a unified user identity graph is exactly the kind of tracking that privacy regulations restrict. You must ensure that your identity graph complies with GDPR, CCPA, and other privacy laws. You must document what identifiers you link, why you link them, and how long you retain the links. You must allow users to request deletion of their identity links. If a user requests deletion, you must remove their canonical user ID from your identity graph and re-split your dataset to ensure their examples are no longer used for evaluation.

## The Split Is Only As Good As the Identity Graph

A user-based split prevents leakage only if the user definition is correct. If your identity graph is incomplete, your splits leak. If your identity graph is incorrect, your splits leak. If your identity graph is stale, your splits leak. The quality of your evaluation depends on the quality of your identity linking.

This means you must invest in identity infrastructure before you invest in evaluation infrastructure. You cannot build a clean user-based split on top of fragmented user IDs. You must first build the identity graph, validate that it links all identifiers for the same user, and maintain it over time. This is data engineering work, not model work, but it is prerequisite to accurate evaluation.

The next challenge is a different form of leakage, specific to synthetic evaluation datasets: when your evaluation examples are generated from templates or prompts, and the model recognizes the template structure rather than solving the task. This is near-duplicate leakage, and it is invisible in traditional deduplication methods.
