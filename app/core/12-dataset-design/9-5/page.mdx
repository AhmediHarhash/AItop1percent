# 9.5 â€” Consent Management for Dataset Collection

Industry data consistently shows that a substantial portion of AI training datasets rely on consent obtained for unrelated purposes before the AI project existed. Users consented to receive customer support, not to have their transcripts used for training sentiment analysis models. Users consented to terms of service that mentioned analytics, not fine-tuning language models. Users posted content publicly, but never agreed to have it scraped for commercial AI training. Every one of these consent scenarios is legally defective. Consent for one purpose does not transfer to another purpose. Consent obtained under old terms does not cover new processing activities. Public posting does not constitute consent to all uses. When your dataset rests on defective consent, your entire model is tainted, your legal basis collapses, and regulators can force you to delete everything and start over.

Consent in the AI context is more complex than consent for traditional data processing because AI systems repurpose data in ways that were not contemplated when the data was originally collected. Data gathered for product functionality is used for model training. Data collected from one user cohort is used to improve services for another. Data captured in one jurisdiction is processed in another. Every one of those shifts triggers new consent requirements, and teams that do not understand the consent hierarchy, the jurisdictional variations, and the operational mechanics of consent management will find themselves defending enforcement actions that could have been avoided with better planning.

## The Consent Hierarchy

Not all consent is created equal, and the legal strength of your consent determines what you can do with the data. The strongest form is **explicit consent**, which requires a clear affirmative action by the data subject indicating agreement to the processing of their personal data for a specific purpose. Under GDPR, explicit consent is required for processing special category data, which includes biometric data for identification, health data, genetic data, and data revealing racial or ethnic origin, political opinions, religious beliefs, or sexual orientation. Explicit consent cannot be inferred from inaction, pre-ticked boxes, or bundled terms. It must be freely given, specific, informed, and unambiguous.

Explicit consent in practice means opt-in mechanisms with clear, plain-language explanations of what data will be collected, how it will be used, and who will have access to it. It means separate consent for separate purposes: training consent is distinct from inference consent, and evaluation consent is distinct from both. It means users can refuse consent without losing access to unrelated services. It means consent can be withdrawn at any time, and withdrawal must be as easy as granting consent. Explicit consent is hard to obtain and hard to maintain, but when you have it, your legal position is strong.

The next tier is **consent** under the general GDPR standard, which does not require the same level of formality as explicit consent but still requires a freely given, specific, informed, and unambiguous indication of the data subject's wishes. This can be an opt-in checkbox, a button click, or a documented verbal agreement. It cannot be silence, inactivity, or pre-checked boxes. It cannot be bundled with unrelated consents or made a condition of service unless the processing is strictly necessary for that service.

Below consent, you have **legitimate interest**, which is a legal basis for processing that does not require consent but does require that your interests or the interests of a third party outweigh the data subject's interests, rights, and freedoms. Legitimate interest is a balancing test, not a free pass. You must conduct a **Legitimate Interest Assessment** documenting the purpose of the processing, the necessity of the processing, and the balancing of interests. You must still provide transparency, honor opt-out requests, and implement appropriate safeguards. Legitimate interest is commonly used for fraud prevention, network security, and direct marketing to existing customers, but it is not a safe harbor for AI training on sensitive data or for processing that involves children, special category data, or high-risk profiling.

The weakest basis is **contractual necessity**, which allows processing that is necessary to perform a contract with the data subject or to take pre-contractual steps at their request. This is the basis for processing payment information to complete a purchase, or processing delivery addresses to ship a product. It does not extend to AI training unless training is genuinely necessary to deliver the service the user signed up for. If you offer a service that works without AI and then use customer data to train a model that improves the service for future users, that is not contractual necessity. That is repurposing, and repurposing requires new consent.

The hierarchy matters because higher tiers impose stricter requirements but provide stronger defenses. If you rely on legitimate interest and a regulator disagrees with your balancing test, you lose your legal basis and the processing becomes unlawful. If you rely on explicit consent and the data subject withdraws consent, you lose the data but you do not face retroactive liability for the period when consent was valid. If you rely on contractual necessity for processing that was not genuinely necessary, you have no legal basis at all and every day of processing is a violation.

## Training Consent vs Inference Consent vs Evaluation Consent

AI systems involve multiple types of processing, and each type may require separate consent. **Training consent** is consent to use data to train, fine-tune, or improve a model. This is often the most controversial form because users do not directly benefit from contributing their data to training. They are providing value to you so that future users or other use cases can benefit. Training consent must be specific about the type of model being trained, the purpose of the model, and whether the model will be used internally, sold, or released publicly. Users who consent to training for an internal customer service model may not consent to training for a third-party recommendation engine.

**Inference consent** is consent to use a model to process the user's data and generate predictions, classifications, or outputs. This is typically easier to obtain because the user receives a direct benefit: a personalized recommendation, a fraud alert, a diagnostic suggestion. Inference consent can often be implied from the use of the service, but only if the user was clearly informed that the service uses AI and what the AI does. If your terms of service mention AI in passing but do not explain that user inputs will be processed by a model to generate outputs, you do not have informed consent for inference.

**Evaluation consent** is consent to use data in evaluation datasets, benchmark sets, or red-teaming exercises. Evaluation data is often more sensitive than training data because it is reviewed by humans, shared with third-party evaluators, or published in research papers. Users who consent to automated processing may not consent to human review. Users who consent to internal use may not consent to external publication. If you plan to release evaluation datasets publicly, you need explicit consent for that specific purpose, and you need to anonymize the data to the point where re-identification is not reasonably likely.

Many teams use a single blanket consent for all AI processing and assume that covers training, inference, and evaluation. It does not. Consent must be specific and informed, and specificity requires breaking down the processing into discrete purposes and obtaining consent for each. Bundling consents is a dark pattern. It is legal only if the user can refuse individual consents without being denied access to unrelated services, and in practice, few teams implement that granularity.

You also face the question of whether consent obtained for one model can be transferred to another. If a user consents to training a fraud detection model and you later build a credit scoring model, can you use the same data? No, unless the original consent was broad enough to cover credit scoring, which it almost certainly was not. If you retrain the fraud model with a new architecture, is that a new purpose requiring new consent? Probably not, if the purpose and risk profile are the same. If you fine-tune the model on a new domain, is that a new purpose? Probably yes, if the domain shift changes the type of predictions or the sensitivity of the processing. These are judgment calls, and the safe answer is always to seek new consent when the purpose or risk changes materially.

## Jurisdictional Variations in Consent Requirements

Consent requirements vary significantly across jurisdictions, and teams operating internationally must comply with the strictest standard applicable to their processing. Under **GDPR**, consent must be freely given, specific, informed, and unambiguous, and it must be documented. The controller must be able to demonstrate that consent was obtained and that the data subject understood what they were consenting to. Consent cannot be a condition of service unless the processing is strictly necessary for that service. Consent must be as easy to withdraw as to give, and withdrawal must stop the processing going forward without affecting the lawfulness of processing that occurred before withdrawal.

Under **CCPA and CPRA** (California Consumer Privacy Act and California Privacy Rights Act), consent requirements are less stringent for general processing but stricter for the sale of personal information and for processing data of minors. Businesses must provide clear notice of data collection and use, and they must honor opt-out requests for the sale or sharing of personal information. For users under 16, affirmative opt-in consent is required before selling or sharing their data. For users under 13, parental consent is required. CPRA also introduces the concept of sensitive personal information, which includes biometric data, precise geolocation, and personal information revealing racial or ethnic origin, religious beliefs, or sexual orientation, and users have the right to limit the use of sensitive personal information to purposes necessary to provide the requested service.

Under **LGPD** (Brazil's General Data Protection Law), consent must be free, informed, and unambiguous, and it must be provided in a format that is clearly distinguishable from other terms. Consent for processing sensitive data must be specific and prominent. Under **PIPL** (China's Personal Information Protection Law), consent must be informed and voluntary, and separate consent is required for processing sensitive personal information, transferring data cross-border, and using personal information for automated decision-making that significantly affects individuals.

The practical consequence is that if you operate globally, you must design your consent mechanisms to meet GDPR standards at minimum, because GDPR is typically the strictest. You must also implement jurisdiction-specific controls for California users, Chinese users, and Brazilian users where their local laws impose additional requirements. You cannot have one global consent flow that ignores jurisdictional variation. You need consent management infrastructure that detects the user's jurisdiction, presents the appropriate consent language, enforces the appropriate opt-in or opt-out rules, and logs the user's decision with sufficient detail to defend it under audit.

## Consent for Repurposing

The most common consent failure in AI systems is repurposing data that was collected for one purpose and using it for another without obtaining new consent. A SaaS company collects customer support emails to resolve support tickets. That is the original purpose, and users implicitly or explicitly consented to that processing by submitting the ticket. Two years later, the company decides to train a sentiment analysis model on the support emails to detect dissatisfaction patterns. That is a new purpose. The original consent does not cover it. New consent is required.

The test for whether new consent is required is whether the new purpose is **compatible** with the original purpose. Under GDPR Article 6, compatibility is assessed based on the link between the original and new purposes, the context of collection, the nature of the data, the consequences for data subjects, and the safeguards applied. If the new purpose is closely related to the original purpose, involves similar risks, and was reasonably anticipated by the data subject, it may be compatible. If the new purpose is unrelated, involves new risks, or was not anticipated, it is incompatible and requires new consent.

Training a model on support emails to improve support routing is arguably compatible with the original purpose. Training a model to analyze employee sentiment or to predict churn is not. Using product usage telemetry to improve product features is compatible. Using the same telemetry to build a user profiling model for third-party advertisers is not. The distinction is not always clear, but the burden is on you to demonstrate compatibility, and if you are wrong, the processing is unlawful.

Repurposing is common in AI because training data is expensive and teams look for opportunities to reuse existing data. But reuse without consent is not efficiency. It is a violation. If you want to repurpose data, you have three options: obtain new consent, anonymize the data to the point where it is no longer personal data, or rely on an alternative legal basis like legitimate interest and document why the new purpose is compatible. The first option is the safest. The second is technically difficult. The third is risky.

## Consent Withdrawal and the Right to Be Forgotten

One of the hardest problems in AI consent management is handling withdrawal. Under GDPR Article 7, data subjects have the right to withdraw consent at any time, and withdrawal must be as easy as granting consent. Once consent is withdrawn, you must stop processing the data for the purposes covered by that consent. If the user consented to training and withdraws that consent, you must stop using their data for training. But what about the model that was already trained on their data? Do you have to retrain the model without their data? Do you have to delete the trained weights because they were derived from data that is no longer consented?

The legal answer is unsettled. GDPR does not explicitly address whether trained models are personal data or whether they must be updated when consent is withdrawn. The prevailing interpretation is that if the model can be shown to contain or reproduce the withdrawn data, it may need to be retrained or deleted. If the model has fully aggregated the data into statistical patterns that do not allow reconstruction of individual inputs, the model is not personal data and does not need to be updated. The practical answer is that very few teams can demonstrate that their models do not retain individual data, especially for large language models, embedding models, or generative models that can memorize training examples.

The safest approach is to implement **machine unlearning** mechanisms that allow you to retrain models without specific data points, or to use training techniques that prevent memorization and allow for efficient retraining. The next safest approach is to log which data was used to train which models, so that when consent is withdrawn, you can assess whether retraining is necessary and document your reasoning. The riskiest approach is to ignore withdrawal requests on the theory that the model is not personal data, because if a regulator disagrees, you have no defense.

The **right to be forgotten** under GDPR Article 17 is even more aggressive than consent withdrawal. It requires erasure of personal data when the data is no longer necessary for the purposes for which it was collected, when consent is withdrawn and there is no other legal basis, when the data subject objects to processing based on legitimate interest and there are no overriding legitimate grounds, when the data was unlawfully processed, or when erasure is required to comply with a legal obligation. The right to be forgotten applies to training data, evaluation data, and arguably to trained models that contain the data.

If a user exercises the right to be forgotten, you must delete their data from active datasets, backup datasets, archived datasets, and any derived datasets or models that contain the data in a recoverable form. If you cannot demonstrate that the model does not contain recoverable personal data, you must retrain the model without that data or delete the model. The right to be forgotten is not absolute. It can be overridden by obligations to retain data for legal compliance, for public health purposes, for scientific research under appropriate safeguards, or for the establishment, exercise, or defense of legal claims. But those exceptions are narrow, and the burden is on you to demonstrate that they apply.

## Practical Consent Management Systems

Managing consent at scale requires infrastructure, not good intentions. You need a **consent management platform** that records when consent was obtained, what the user consented to, how the consent was presented, and when consent was withdrawn. You need to link consent records to data records so that when a user withdraws consent, you can identify all the data covered by that consent and delete it or stop processing it. You need to track which datasets were built under which consent regimes so that when consent changes, you can assess which datasets remain lawful and which must be rebuilt.

Your consent management system must support **granular consent**, allowing users to consent to some purposes and refuse others. It must support **temporal consent**, recording when consent was obtained and when it expires or is withdrawn. It must support **jurisdictional consent**, presenting different consent flows and language based on the user's location. It must support **audit trails**, logging every consent grant, every withdrawal, every data access, and every deletion so that you can demonstrate compliance under investigation.

The system must also support **consent refresh**, prompting users to reconfirm consent when the purposes of processing change, when new types of data are collected, or when a material amount of time has passed since the original consent. Consent obtained in 2022 for processing that looked very different than processing in 2026 is probably not valid for current use. You need to go back to users and obtain fresh consent under current terms.

And the system must integrate with your dataset pipelines, your model training infrastructure, and your rights request processes. When a user withdraws consent, the consent management system must trigger deletion in your training datasets, your evaluation datasets, and your backup systems. When a user requests access to their data, the system must identify all the datasets that contain their data and generate a report. When a user requests rectification, the system must propagate the corrected data to all systems that hold copies. None of this happens automatically. It requires engineering, integration, and testing.

## The Opt-In vs Opt-Out Debate in 2026

In 2026, the consent landscape is shifting toward opt-in as the default. GDPR has always required opt-in for explicit consent and for processing special category data. CPRA requires opt-in for selling or sharing the data of minors. The EU AI Act requires opt-in for high-risk AI systems that involve biometric identification or emotion recognition. Jurisdictions are increasingly rejecting opt-out as insufficient for processing that involves AI, profiling, or sensitive data.

Opt-out is still permissible under legitimate interest in GDPR, and it is the standard under CCPA for general processing, but relying on opt-out for AI training is legally risky and reputationally damaging. Users expect to be asked before their data is used to train models, especially if the models will be used for purposes beyond the service they signed up for. Using opt-out consent for AI training signals that you value efficiency over user autonomy, and that signal is noticed by regulators, privacy advocates, and customers.

The trend is toward affirmative, informed, granular opt-in consent for AI training, with opt-out available for less sensitive processing like analytics and product improvement. If you are building consent mechanisms in 2026, build for opt-in. Design interfaces that clearly explain what data will be used, for what purpose, and with what safeguards, and ask users to actively confirm their agreement. Do not bury consent in terms of service. Do not use dark patterns like pre-checked boxes or double negatives. Do not bundle AI consent with unrelated consents. Make consent a first-class part of your user experience, not an afterthought in your privacy policy.

Consent is the legal foundation of your dataset, and defective consent makes everything built on that dataset unlawful. Once you have consent right, you still have to comply with the specific rights that data protection law grants to data subjects, and those rights create operational obligations that most AI teams are not prepared to meet.

