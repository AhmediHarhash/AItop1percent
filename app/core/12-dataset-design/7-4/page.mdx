# 7.4 â€” Dataset Size Planning: How Much Data You Actually Need

In mid-2025, a conversational AI company spent four months assembling a training dataset of 180,000 customer service dialogues for fine-tuning their support bot. The project consumed $340,000 in annotation costs and tied up six contractors full-time. When they finally ran the fine-tuning job, they achieved 91.2% accuracy on their eval set. Out of curiosity, they tested a smaller ablation using only the first 8,000 examples from the same dataset. The result: 90.8% accuracy, a difference of 0.4 percentage points. The final 172,000 examples had cost them $310,000 and three additional months but delivered almost no measurable improvement. The root cause was not a data quality problem or a modeling failure. It was dataset size planning negligence. No one had modeled the accuracy curve before committing to the full dataset. No one had tested whether 10,000 examples would saturate performance for their specific task. The team had operated on the assumption that more data always yields better models, and they paid dearly for that assumption.

Dataset size planning is the process of determining how many training examples you need to reach your target performance level for a specific task. It is not a guess. It is not a rule of thumb applied blindly. It is a deliberate analysis that balances task complexity, model capacity, quality standards, budget constraints, and diminishing returns. In 2026, models fine-tune faster and generalize better than ever before, which means the old heuristics from 2022 and 2023 no longer apply. GPT-4o and Claude 3.5 Sonnet can learn many tasks with a few hundred high-quality examples, while Llama 3 and Gemini 2 may require thousands for the same task. Your job is to determine the minimum viable dataset size that meets your performance requirements, then add a margin for safety, not to blindly chase the largest dataset you can afford.

## The More Data Myth and Where It Breaks Down

The belief that more data always improves model performance is rooted in the scaling laws observed during pretraining. For base model training, adding more tokens generally yields better perplexity and broader knowledge up to trillions of tokens. But fine-tuning operates under different dynamics. Fine-tuning is task-specific adaptation, not general knowledge acquisition. Once the model has seen enough examples to learn the task distribution, additional examples yield rapidly diminishing returns. The marginal improvement per additional example decays exponentially, not linearly. A task that achieves 88% accuracy with 1,000 examples might reach 91% with 5,000 examples and 92% with 20,000 examples. The gap between 91% and 92% cost you 15,000 additional examples for a single percentage point gain. Whether that tradeoff is worth it depends entirely on your performance requirements and budget.

The myth breaks down in three distinct ways. First, it breaks down when data quality is low. Adding 50,000 noisy examples does not improve performance if those examples confuse the model or introduce conflicting signals. A dataset of 2,000 clean, well-labeled examples will outperform a dataset of 20,000 poorly labeled examples every time. Second, it breaks down when the task is simple. A binary classification task with clear decision boundaries does not require 100,000 examples. It requires enough examples to cover the variation in inputs and edge cases, which for many tasks is fewer than 5,000 examples. Third, it breaks down when the model is already highly capable. Fine-tuning GPT-4o on a task it already performs reasonably well at requires far fewer examples than fine-tuning a smaller model from scratch. You are teaching it refinements and preferences, not teaching it the task from zero knowledge.

You must test the scaling curve empirically for your specific task. This means assembling a small initial dataset, training a model, measuring performance, then incrementally adding more data and retraining to observe the accuracy curve. If you see the curve flattening after 3,000 examples, you know that 10,000 examples will not deliver proportional gains. If you see the curve still climbing steeply at 10,000 examples, you know you need more data. There is no shortcut to this empirical testing. The heuristics published in model provider documentation are starting points, not final answers. Your task has unique characteristics, your data has unique quality, and your performance bar has unique constraints. Model the curve before committing to a final dataset size.

## Scaling Laws for Fine-Tuning: Diminishing Returns Curves

The relationship between dataset size and model performance follows a predictable pattern in fine-tuning. Performance improves rapidly with the first few hundred examples, then the rate of improvement slows as you add thousands more, and eventually performance plateaus no matter how much additional data you provide. This is the diminishing returns curve, and it governs every fine-tuning project. The shape of the curve depends on task complexity, model capacity, and data quality, but the overall pattern holds universally. Your goal is to identify the point on the curve where additional data stops being worth the cost.

For simple classification tasks with well-defined categories, the curve typically flattens after 2,000 to 5,000 examples. A sentiment classification task with three categories might reach 92% accuracy at 1,500 examples and 93.5% accuracy at 8,000 examples. The incremental gain of 1.5 percentage points required more than five times as much data. For moderately complex tasks like entity extraction or intent classification with ten to twenty categories, the curve flattens between 5,000 and 15,000 examples. For highly complex tasks like multi-turn dialogue generation or nuanced content moderation with context-dependent decisions, the curve may not flatten until 20,000 to 50,000 examples. These are not hard rules. They are observed patterns from production fine-tuning projects in 2025 and 2026 across multiple model families.

The steepness of the early curve also varies by model. GPT-4o demonstrates faster learning on small datasets than GPT-4, meaning the curve rises more steeply in the first 500 examples. Claude 3.5 Sonnet shows similar fast adaptation. Llama 3 models require more examples to reach the same performance level, meaning the curve rises more gradually and flattens later. Gemini 2 falls somewhere in between. If you are fine-tuning a smaller open-source model, expect to need 50% to 100% more data to match the performance of a fine-tuned frontier model. If you are fine-tuning a frontier model on a task it already handles reasonably well in the base version, expect to need far fewer examples than the heuristics suggest.

You measure the curve by training models at multiple dataset sizes and plotting accuracy against size on a log scale. Start with 200 examples, then 500, then 1,000, 2,000, 5,000, 10,000, and so on, doubling each time. For each dataset size, train the model with identical hyperparameters and measure performance on a held-out eval set. Plot the results. If the curve is still climbing steeply at your current dataset size, you know you need more data. If the curve has flattened and the slope is nearly zero, you know additional data will not help. If the curve shows a gentle upward slope, you make a cost-benefit decision: is the marginal gain worth the marginal cost of additional examples?

## Task Complexity Determines Data Requirements

Not all tasks are created equal. A task that requires the model to apply nuanced judgment across ambiguous contexts requires far more training data than a task with binary outputs and clear decision rules. Task complexity is the primary determinant of dataset size requirements, and you must assess complexity honestly before committing to a data collection plan. Complexity is not a vague intuition. It is a measurable property based on the number of decision points, the ambiguity of edge cases, the diversity of input formats, and the precision required in outputs.

Simple tasks are those with fewer than five output categories, clear decision boundaries, and minimal context dependence. A spam filter classifying emails as spam or not spam is a simple task. A sentiment classifier labeling reviews as positive, neutral, or negative is a simple task. For these tasks, 1,000 to 3,000 high-quality examples are often sufficient to reach production-grade accuracy. The model learns the patterns quickly because the patterns are consistent and the decision space is small. If you collect 20,000 examples for a simple task, you are wasting resources unless your accuracy requirements are extraordinarily high or your data quality is unusually low.

Moderate tasks are those with ten to twenty output categories, some context dependence, and occasional ambiguity. Intent classification for a customer service bot with fifteen intents is a moderate task. Named entity recognition across eight entity types with overlapping boundaries is a moderate task. For these tasks, 5,000 to 15,000 examples are typically required. The model needs enough data to learn the distinctions between similar categories and to handle edge cases where two categories might apply. Underfitting is a real risk below 5,000 examples, and overfitting is a real risk if you do not maintain diversity in the dataset.

Complex tasks are those with many output categories, heavy context dependence, and significant ambiguity. Multi-turn dialogue generation where the model must maintain context across five or more turns is a complex task. Content moderation decisions that depend on cultural context, user history, and platform policy nuance are complex tasks. For these tasks, 20,000 to 50,000 examples may be required, and in some cases even more. The model must learn not just the categories but the subtle interactions between features that determine the correct output. If you attempt to fine-tune a complex task with 3,000 examples, you will see poor generalization and high variance in outputs.

You assess complexity by counting decision points, measuring inter-annotator agreement, and observing model behavior on small datasets. If two expert annotators disagree more than 10% of the time on the correct label, the task is complex. If a model trained on 1,000 examples shows high variance between training runs, the task is complex. If the task description requires more than two paragraphs to explain the decision logic, the task is complex. Complexity drives dataset size, and ignoring complexity leads to either under-collection or over-collection of data.

## Quality Versus Quantity Tradeoffs at Different Dataset Sizes

The relationship between data quality and dataset size is not linear. At small dataset sizes, quality is everything. A dataset of 500 perfectly labeled examples will outperform a dataset of 5,000 carelessly labeled examples. At large dataset sizes, quality still matters, but quantity begins to compensate for moderate quality issues through volume. A dataset of 50,000 examples with 5% label noise will often outperform a dataset of 5,000 perfect examples, because the model can learn to ignore the noise if the signal is strong enough. Your strategy must shift based on where you are on the size spectrum.

When you are working with fewer than 2,000 examples, invest heavily in quality. Every mislabeled example has outsized impact on model behavior. A single bad example in a dataset of 200 examples represents 0.5% of the training data, and if that example appears in a critical decision region, it can corrupt the learned boundary. Annotation should be done by experts, not crowd workers. Every label should be reviewed by a second annotator. Ambiguous cases should be discussed and resolved with clear criteria. You cannot afford sloppiness at this scale. The cost of high-quality annotation is justified because the dataset is small and every example counts.

When you are working with 5,000 to 20,000 examples, you can tolerate moderate quality issues as long as the error rate is below 3%. The model will learn the dominant patterns and treat mislabeled examples as noise. You should still aim for high quality, but you do not need expert annotation on every single example. A mix of expert annotation and trained contractor annotation with spot-checking is appropriate. The focus shifts from perfection to consistency. If your annotators apply the labeling criteria consistently, even if the criteria are not perfect, the model will learn the consistent patterns. Inconsistency is more damaging than imperfection at this scale.

When you are working with more than 20,000 examples, quantity begins to dominate. A dataset of 100,000 examples with 5% label noise will train a strong model because the signal-to-noise ratio is favorable. The model sees each decision boundary thousands of times, and the mislabeled examples become statistical outliers. You should still aim for quality, but the marginal cost of increasing quality from 95% to 98% may not be worth it if it slows down data collection by months. The tradeoff shifts toward speed and volume. You can collect more data faster with slightly lower quality standards, and the model performance will often be better than collecting less data with higher quality standards.

This is not a license to collect garbage data. It is a recognition that the optimal quality bar shifts based on dataset size. At 500 examples, you need 99% accuracy in labels. At 50,000 examples, 95% accuracy may be sufficient. The model is robust to noise at scale in a way it is not robust at small scale. Plan your annotation process and budget accordingly.

## Cost Modeling: The Budget-Constrained Dataset Size Decision

Most dataset size decisions are not purely technical. They are budget-constrained optimization problems. You have a fixed budget for annotation, a fixed timeline, and a performance target. Your job is to determine the maximum dataset size you can afford, estimate the performance you will achieve at that size, and decide whether that performance meets your requirements. If it does not, you either increase the budget, relax the performance target, or simplify the task. There is no magic. There is only math.

Annotation costs vary by task complexity and annotation method. Simple classification tasks cost $0.10 to $0.50 per example when using trained contractors on platforms like Scale AI or Labelbox. Moderate tasks like entity extraction or intent labeling cost $0.50 to $2.00 per example. Complex tasks like dialogue quality rating or nuanced content moderation cost $2.00 to $10.00 per example, especially if you require expert annotators. These are 2026 market rates for quality annotation with reasonable SLAs. If you are annotating in-house, your costs are higher due to salary overhead but your quality control is tighter.

A budget of $50,000 for annotation buys you 5,000 to 10,000 examples for a moderate task at $5 per example, or 25,000 to 50,000 examples for a simple task at $1 per example. You must decide which task you are building for and what quality bar you need. If your task is moderate complexity and your budget buys you 8,000 examples, you need to check whether 8,000 examples will meet your performance target. If your empirical scaling curve shows that you need 15,000 examples to hit your target, you have three options: increase the budget, lower the target, or improve data efficiency through better annotation guidelines or active learning.

The timeline constraint is often as binding as the budget constraint. Annotation takes time. A team of ten contractors can label 500 examples per day for a simple task or 100 examples per day for a complex task. If you need 20,000 examples and your team can label 200 per day, you need 100 days of annotation time. If your product launch is in 60 days, you cannot collect 20,000 examples. You must either expand the annotation team, simplify the task, or accept a smaller dataset. The timeline forces the size decision as much as the budget does.

You build a cost model by estimating annotation cost per example, multiplying by candidate dataset sizes, and comparing the total cost to your budget. You build a timeline model by estimating annotation throughput per day and dividing candidate dataset sizes by throughput. You overlay the performance curve from your empirical tests to find the dataset size that fits both the budget and the timeline while meeting the performance target. If no such size exists, you have a resourcing problem, not a data problem. Fix the resourcing or adjust the requirements.

## Practical Heuristics for 2026: Starting Points by Task Type

While empirical testing is the gold standard, you need starting points to plan initial data collection. The following heuristics are derived from production fine-tuning projects in 2025 and 2026 across GPT-4o, Claude 3.5 Sonnet, Llama 3, and Gemini 2. They are not guarantees. They are reasonable starting points that you refine through testing.

For binary classification tasks with clear decision boundaries, start with 1,000 to 2,000 examples. This includes spam detection, toxicity classification, and simple yes-no decisions. If your eval set performance is below target after training on 2,000 examples, add another 2,000 and retest. If performance is at target, stop. For multi-class classification with three to five categories, start with 2,000 to 4,000 examples. This includes sentiment analysis, topic classification, and simple intent detection. For multi-class classification with ten to twenty categories, start with 5,000 to 10,000 examples. This includes detailed intent classification, product categorization, and entity type classification.

For structured extraction tasks like named entity recognition or slot filling, start with 3,000 to 8,000 examples depending on the number of entity types and the ambiguity of boundaries. A task with three entity types and clear boundaries needs 3,000 examples. A task with ten entity types and overlapping boundaries needs 8,000 examples or more. For single-turn generation tasks like summarization, question answering, or instruction following, start with 2,000 to 5,000 examples if the task is narrow and well-defined. If the task is open-ended or requires creative generation, start with 10,000 to 20,000 examples.

For multi-turn dialogue tasks, start with 5,000 to 15,000 dialogues depending on the average number of turns and the complexity of the domain. A simple FAQ bot with two-turn dialogues needs 5,000 dialogues. A complex support bot with five-turn dialogues across multiple domains needs 15,000 dialogues or more. For content moderation tasks with context-dependent decisions, start with 10,000 to 30,000 examples depending on the number of policy categories and the cultural and contextual nuance required.

These heuristics assume high-quality data with consistent labeling and representative coverage of the input distribution. If your data quality is lower, increase the starting point by 50%. If your model is a smaller open-source model rather than a frontier model, increase the starting point by 50% to 100%. If your task is at the boundary between two complexity levels, start at the higher size. It is easier to discover you needed less data than to discover you needed more after you have already trained a model and missed your performance target.

## The Right Dataset Size Is a Decision, Not a Guess

Dataset size planning is not a one-time decision made at the beginning of the project. It is an iterative decision informed by empirical data, budget constraints, and performance requirements. You start with a heuristic, collect an initial dataset, train a model, measure performance, and then decide whether to stop or collect more data. If you are meeting your performance target, stop. If you are not meeting the target and the scaling curve shows you are still in the steep part of the curve, collect more data. If you are not meeting the target and the scaling curve has flattened, more data will not help. You have a data quality problem or a model capacity problem, not a data quantity problem.

The discipline of dataset size planning saves you from two failure modes. It saves you from under-collection, where you launch a model that underperforms because you stopped data collection too early. And it saves you from over-collection, where you waste months and hundreds of thousands of dollars collecting data that delivers no marginal performance gain. Both failures are common. Both are preventable. Model the curve, set a target, test empirically, and make the size decision based on evidence, not intuition.

In 2026, the models are better and the data requirements are often lower than you expect. Do not assume you need 100,000 examples because that is what you needed in 2023. Test with 5,000 examples first. If you hit your target, you just saved yourself $200,000 and three months. If you do not hit your target, you have a baseline to build from and a curve to guide your next decision. Dataset size planning is where engineering discipline meets resource optimization, and getting it right is the difference between a fine-tuning project that ships on time and on budget and one that drags on for months while burning through your runway.

The next challenge is not just how much data to collect, but in what order to present it during training. The sequence in which the model sees examples can have as much impact on final performance as the total number of examples, and that is where curriculum design becomes critical.
