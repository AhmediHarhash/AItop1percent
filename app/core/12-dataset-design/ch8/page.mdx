# Chapter 8 — Bias, Fairness, and Representativeness

Every dataset encodes the biases of its sources, its collectors, and its annotators. Most teams treat bias as a problem to patch during model training, overlooking the fact that no amount of post-hoc fairness tuning can recover from a fundamentally skewed dataset. Bias is not a bug you fix once. It is a property you measure continuously and mitigate deliberately, from the moment you decide what to collect through the day you deprecate the dataset.

This chapter equips you to identify where bias enters your pipelines, measure its presence with precision, and take structural action to reduce it. You will learn to distinguish between seven distinct bias sources that most teams only catch two of. You will measure bias across demographic, geographic, and linguistic dimensions. You will understand the annotator demographics that shape training labels and the temporal drift that makes old datasets unreliable. By the end of this chapter, you will know which metrics matter, how to document bias for regulatory compliance, and how to build the business case that justifies investment in fairness work.

---

- **8.1** — How Bias Enters Datasets: The Seven Sources
- **8.2** — Measuring Representation: Demographic, Geographic, and Linguistic
- **8.3** — Selection Bias in Production Data Pipelines
- **8.4** — Annotation Bias and Annotator Demographics
- **8.5** — Temporal Bias: When Your Dataset Is Stuck in the Past
- **8.6** — Bias Detection Techniques and Tooling in 2026
- **8.7** — Mitigation Strategies: Resampling, Reweighting, and Augmentation
- **8.8** — Fairness Metrics for Dataset Evaluation
- **8.9** — Intersectional Analysis: Beyond Single-Axis Fairness
- **8.10** — Bias Documentation: What to Measure and Report
- **8.11** — The Business Case for Fair Datasets
- **8.12** — Regulatory Requirements: EU AI Act Dataset Obligations

---

*We start with the seven sources of bias and why most teams only catch two of them.*
