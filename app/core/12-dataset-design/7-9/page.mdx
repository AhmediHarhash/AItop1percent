# 7.9 â€” Training Data for RAG Systems: Retrieval Pairs and Passage Sets

You cannot train a retrieval model for pharmaceutical research using Wikipedia question-answering pairs and expect it to work. The query "what is the capital of France" has nothing in common with "what are the off-target effects of CRISPR base editors in primary T cells." The vocabulary is different. The query length is different. The relevance criteria are different. The document types are different. A retrieval model trained on generic QA learns to match short factual questions to encyclopedia articles. It does not learn to match complex scientific queries to technical papers. When deployed in production, it returns highly cited but irrelevant results, misses recent publications, and the downstream generation model hallucinates citations because the retrieved context is useless. This is not a model quality problem. It is a training data mismatch problem. RAG training data must reflect the actual retrieval task, not a convenient proxy.

## How RAG Training Data Differs from Standard Fine-Tuning Data

Training data for retrieval-augmented generation systems has a fundamentally different structure than training data for end-to-end generation models. In standard fine-tuning, each training example is an input-output pair where the input is the user query or prompt and the output is the complete response. The model learns to map inputs to outputs directly. In RAG systems, the training data must teach two separate but coupled tasks: retrieving the right passages given a query, and generating the right response given the query and the retrieved passages.

This creates a two-part training data requirement. First, you need query-passage pairs for training the retrieval component. Each pair consists of a query and one or more passages that are relevant to that query according to your task definition of relevance. Second, you need query-passage-response triples for training the generation component. Each triple consists of a query, the passages the retrieval system would return for that query, and the correct response synthesized from those passages. These two datasets are related but not identical, and constructing both requires understanding how retrieval and generation interact in your production system.

The retrieval training data defines what counts as relevant. A passage is relevant if it contains information that helps answer the query, but relevance is task-dependent. For a question answering system, relevance means the passage contains the factual answer. For a summarization system, relevance means the passage contains information that should appear in the summary. For a citation recommendation system, relevance means the passage represents prior work that should be cited in the context of the query topic. You cannot reuse retrieval training data across tasks because the definition of relevance changes.

The generation training data must reflect realistic retrieval outputs, including both correct and incorrect retrievals. If you train the generation model only on examples where the retrieved passages perfectly answer the query, the model will fail in production when the retrieval system returns partially relevant or irrelevant passages, which happens frequently. A well-constructed generation training set includes examples where the retrieved passages are fully relevant, examples where they are partially relevant and the model must synthesize information from multiple passages, examples where they are marginally relevant and the model must hedge or request clarification, and examples where they are irrelevant and the model must decline to answer based on the provided passages.

This means you cannot construct RAG training data by simply taking an existing question-answering dataset and treating the context as retrieved passages. Those datasets were constructed with gold-standard contexts that fully answer the questions. Production retrieval systems do not return gold-standard contexts. They return whatever the retrieval model scores highest, which includes errors, omissions, and noise. Your training data must reflect that reality.

## Query-Passage Pair Construction for Retrieval Model Training

Query-passage pairs are the foundation of retrieval model training. Each pair teaches the model that a specific query and a specific passage are related, and the model learns to embed queries and passages such that related pairs have high similarity in the embedding space. The quality of these pairs determines the quality of your retrieval.

The first challenge is sourcing queries that reflect production traffic. Generic queries like "what is machine learning" do not teach the model to retrieve for your domain. You need queries that match the phrasing, specificity, and intent distribution of real user queries in your system. For a scientific literature search system, queries are technical, multi-term, often phrased as full questions or research objectives. For a customer support knowledge base, queries are colloquial, often incomplete, sometimes phrased as complaints rather than questions. For a legal research system, queries include citation strings, statute numbers, and legal terminology.

If you have an existing search system with logs, your queries come from those logs. You sample queries that resulted in user engagement, like click-throughs or explicit relevance feedback, because those are the queries where users found what they were looking for. You filter out navigational queries where the user was looking for a specific known document rather than exploring a topic, because those queries do not teach the model about semantic relevance. You filter out malformed queries, test queries, and adversarial queries that do not represent real information needs.

If you do not have search logs, you synthesize queries by having domain experts write questions they would ask given specific passages. This is more expensive and less realistic than mining real queries, but it is viable for bootstrapping. You show an expert a passage and ask them to write three to five questions that the passage answers. This produces query-passage pairs where the passage is guaranteed to be relevant, but the queries may not reflect the phrasing or specificity of real user queries. You mitigate this by having multiple experts write queries for each passage and by explicitly instructing them to vary query style, length, and specificity.

The second challenge is determining which passages are relevant to each query. For queries mined from search logs, you use behavioral signals: passages the user clicked on, passages the user spent time reading, passages the user explicitly marked as helpful. These signals are noisy. Users sometimes click on irrelevant results and sometimes ignore relevant results. You improve signal quality by requiring multiple engagement signals, like a click followed by dwell time above a threshold, or by aggregating signals across multiple users who issued the same or similar queries.

For synthesized queries, relevance is defined by the construction process: the passage you showed the expert when they wrote the query is relevant by definition. But you also want to identify other relevant passages that were not used during query generation. You do this by using the initial query-passage pairs to train a first-iteration retrieval model, then using that model to retrieve candidate passages for each query, and having experts label those candidates as relevant or not relevant. This expands your positive set and also generates negative examples.

The third challenge is balancing specificity and generalization. If all your query-passage pairs are highly specific, like queries containing exact phrases from the passage, the model learns to do phrase matching rather than semantic retrieval. If all your pairs are highly abstract, like queries that only relate to the passage at a topical level, the model learns coarse topic matching and misses fine-grained relevance. You need pairs that span the specificity spectrum, from exact match queries to paraphrased queries to topically related but differently phrased queries.

A mature query-passage dataset has multiple levels of relevance labeled explicitly. Binary relevance, where a passage is either relevant or not, is insufficient for training high-quality retrieval models. You need graded relevance: perfectly relevant passages that fully answer the query, highly relevant passages that answer most of the query, partially relevant passages that provide some useful information, and marginally relevant passages that are on-topic but do not help with the specific query. These gradations let you train the model with ranking losses that prioritize perfect matches over partial matches, which improves top-k retrieval quality.

## Passage Chunking Strategies and Their Impact on Training Quality

Retrieval models operate on passage embeddings, not document embeddings, which means you must chunk your documents into passages before constructing training data. The chunking strategy directly affects training quality because it determines what the model learns to retrieve. Poor chunking creates passages that are too short to be informative, too long to be focused, or poorly aligned with the information boundaries in the text.

The simplest chunking strategy is fixed-length chunking, where you split documents into passages of a fixed token count, like 256 or 512 tokens, with optional overlap between adjacent chunks. This is fast and simple, but it produces passages that often split sentences, paragraphs, or logical sections in awkward places. A passage that begins mid-sentence and ends mid-paragraph is harder to embed meaningfully because it lacks context for the beginning and closure for the end. Fixed-length chunking works reasonably well for documents with uniform structure, like news articles or blog posts, but fails for documents with hierarchical structure, like legal contracts or technical manuals.

Semantic chunking splits documents at natural boundaries, like paragraph breaks, section headers, or topic shifts. This produces passages that are coherent units of meaning, which improves embedding quality. The challenge is identifying semantic boundaries automatically. For structured documents with explicit headers, you can chunk at section boundaries. For unstructured documents, you need a model to detect topic shifts, which adds complexity and can introduce errors. A semantic chunking strategy might split a document every time a new topic is introduced based on sentence embedding similarity, creating passages of variable length but consistent topical focus.

Sentence-window chunking creates passages by taking a target sentence plus a fixed number of sentences before and after it. This ensures that every sentence appears in multiple overlapping passages with different surrounding contexts. Sentence-window chunking is effective for question answering tasks where the answer might be a single sentence but understanding that sentence requires surrounding context. The downside is that it produces more passages per document, increasing retrieval latency and storage costs.

The chunking strategy must align with your query characteristics. If queries are typically answered by information that spans multiple paragraphs, you need longer passages. If queries are typically answered by single sentences or short spans, you need shorter passages with more focused context. A scientific literature retrieval system might chunk papers at section boundaries, creating passages that each cover a coherent part of the research like the introduction, methods, results, or discussion. A customer support retrieval system might chunk articles at the question level, creating passages that each address a single FAQ.

Chunking also interacts with negative example construction. If you chunk too finely, passages from the same document become hard negatives for each other, because they share topic and vocabulary but may not all be relevant to the same query. This is actually beneficial for training because it forces the model to learn fine-grained distinctions. If you chunk too coarsely, passages are so long that they cover multiple topics, and it becomes unclear which part of the passage is relevant to the query. The model learns to retrieve based on partial topic overlap, which produces low precision.

You evaluate chunking strategies by training retrieval models on differently chunked versions of the same corpus and measuring retrieval quality on a held-out test set. The best chunking strategy is the one that maximizes retrieval precision at the rank cutoffs you care about, typically precision at k where k is the number of passages you send to the generation model. If your RAG system retrieves five passages, you optimize for precision at five. Chunking strategies that maximize overall recall but produce low precision at five are not useful.

## Negative Passage Mining for Contrastive Learning

Retrieval models learn by contrasting positive query-passage pairs with negative query-passage pairs. A positive pair is a query and a relevant passage. A negative pair is a query and an irrelevant passage. The model learns to assign higher similarity scores to positive pairs than to negative pairs. The quality of negative examples determines how well the model learns to distinguish relevant from irrelevant passages.

Random negatives, passages sampled uniformly from the corpus, are easy negatives that teach the model very little. A query about machine learning paired with a random passage about cooking recipes is trivially distinguishable. The model learns to reject passages with completely different vocabulary and topic, but it does not learn the fine-grained distinctions that matter in production. Random negatives are useful for initializing training, but they should be a small fraction of the negative set.

Hard negatives are passages that share topic, vocabulary, or structure with the query but are not actually relevant. For a query about the side effects of a specific drug, a hard negative might be a passage about the side effects of a different drug in the same class, or a passage about the efficacy of the same drug, or a passage about side effects but in a different patient population. Hard negatives force the model to learn the specific features that distinguish relevant from irrelevant passages in contexts where the distinction is subtle.

You mine hard negatives from multiple sources. The most direct source is retrieval errors from your production system or from an initial retrieval model. Every time the system retrieves an irrelevant passage, that passage is a hard negative for the query. You log these errors, review them to confirm they are actually irrelevant, and add them to your training set. This creates a feedback loop where production errors become training signal for the next model iteration.

Another source is BM25 or keyword-based retrieval. You issue each query to a keyword search system and take the top-ranked passages that are not labeled as relevant. These passages have high keyword overlap with the query but may lack semantic relevance. They are hard negatives because they would confuse a naive keyword-matching model. Training your neural retrieval model to rank them below the truly relevant passages teaches the model to prioritize semantic meaning over keyword overlap.

A third source is same-document negatives. For each query-passage pair where the passage is relevant, you sample other passages from the same document that are not relevant to the query. These passages share the document's topic and vocabulary but do not address the specific information need expressed in the query. Same-document negatives are particularly valuable for training models to distinguish between different aspects of a topic.

The ratio of hard negatives to random negatives depends on your training setup. Early in training, when the model has not yet learned basic topic-level distinctions, you use more random negatives to establish coarse boundaries. As training progresses, you shift toward hard negatives to refine the decision boundary. A typical training batch might include one positive passage, one to three hard negatives, and one random negative per query. This ratio ensures the model learns both coarse and fine-grained distinctions.

You also vary the difficulty of hard negatives over the course of training. Initially, you use moderately hard negatives that are clearly distinguishable once the model learns basic semantic features. Later, you introduce extremely hard negatives that require nuanced understanding to distinguish from positives. This curriculum approach prevents the model from getting stuck in local optima where it learns to reject some categories of hard negatives but never develops the capacity to handle the hardest cases.

## Training Data for Rerankers vs Retrievers

RAG systems often use a two-stage retrieval architecture: a fast first-stage retriever that narrows the candidate set from millions of passages to hundreds, and a slower second-stage reranker that refines the ranking of those candidates. The training data for these two components is related but distinct because they optimize for different objectives under different computational constraints.

The retriever must be fast, which typically means it uses dense embeddings computed offline and retrieved using approximate nearest neighbor search. The training data for a retriever consists of query-passage pairs with binary or graded relevance labels. The model learns to embed queries and passages such that relevant pairs have high dot product or cosine similarity. The loss function is typically a contrastive loss like InfoNCE or triplet loss, which trains the model to rank positive passages above negative passages in the embedding space.

The reranker operates on a small candidate set and can afford to be slower and more expressive. It typically uses cross-attention between the query and passage, which allows it to model fine-grained interactions that are not captured by independent embeddings. The training data for a reranker consists of query-passage pairs where the passage is drawn from the retriever's output, not from the full corpus. This is critical. If you train a reranker on random query-passage pairs, it learns to distinguish relevant passages from random irrelevant passages, but it does not learn to refine the ranking of passages that were already scored highly by the retriever.

You construct reranker training data by running your retriever on every training query and taking the top k retrieved passages, like the top 50 or top 100. You then label each retrieved passage as relevant or irrelevant based on your ground truth labels. The reranker's training set consists of each query paired with its top-k retrieved passages, with labels indicating which of those passages are actually relevant. This teaches the reranker to correct retriever errors: promoting false negatives that the retriever scored too low and demoting false positives that the retriever scored too high.

The reranker's negative examples are retriever false positives, which are inherently hard negatives because they scored highly enough to be retrieved. This makes reranker training more challenging than retriever training, but it also makes the reranker more effective at the margin. A well-trained reranker can improve precision at five by 15 to 30 percentage points over the retriever alone, because it is specifically optimized to distinguish the hardest cases.

The training pipeline has dependencies. You cannot train a reranker until you have a retriever, because the reranker training data depends on retriever outputs. This means you train the retriever first on query-passage pairs, then use the trained retriever to generate candidate sets for reranker training, then train the reranker on those candidates. If you later retrain the retriever, you also need to regenerate the reranker training data, because the retriever's output distribution has changed.

Some systems skip the reranker and rely solely on retrieval. This is viable when retrieval quality is already high, when latency constraints are tight, or when the candidate set is small enough that the generation model can handle all retrieved passages. If you do use a reranker, you need to allocate training data construction effort appropriately. The retriever needs broad coverage of query types and passage types. The reranker needs deep coverage of retriever errors and near-miss cases.

## End-to-End RAG Training: Combining Retrieval and Generation Data

Retrieval and generation are coupled in RAG systems, and training them in isolation can produce suboptimal results. A retrieval model trained to maximize precision on standalone relevance judgments might retrieve passages that are technically relevant but not useful for generation because they lack the specific details the generation model needs. A generation model trained on gold-standard passages might produce high-quality outputs when given perfect retrieval but fail catastrophically when given realistic noisy retrieval.

End-to-end RAG training constructs training data that reflects the full pipeline. Each training example consists of a query, the passages the retrieval system would return for that query, and the correct response generated from those passages. The retrieval component is trained to retrieve passages that improve generation quality, not just passages that match relevance labels. The generation component is trained to generate correct responses from realistic retrieved passages, including handling cases where the retrieved passages are incomplete or contradictory.

The construction process starts with queries and ground truth responses. For each query, you have a target response that represents the correct output of the full RAG system. You then run your retrieval system to get candidate passages. If the retrieved passages are sufficient to generate the target response, you use them as-is. If not, you either augment the retrieved passages with additional relevant passages from your corpus or you adjust the target response to reflect what is actually achievable given the retrieved passages.

This creates three categories of training examples. First, examples where retrieval is perfect and the generation model just needs to synthesize the information from the retrieved passages into the target format. These examples teach the generation model how to combine information from multiple sources. Second, examples where retrieval is incomplete and the generation model needs to hedge, acknowledge uncertainty, or request clarification. These examples teach the generation model to recognize when the retrieved context is insufficient. Third, examples where retrieval includes irrelevant or contradictory passages and the generation model needs to filter out the noise and focus on the relevant parts. These examples teach the generation model robustness to retrieval errors.

End-to-end training also enables joint optimization, where you train the retrieval and generation components together so that retrieval learns to fetch passages that maximize generation quality. This requires differentiable retrieval, which is technically complex, or reinforcement learning, where the generation model's output quality serves as the reward signal for the retrieval model. Joint optimization is most valuable when the definition of relevance is task-specific and cannot be easily encoded in standalone relevance labels.

For example, in a summarization RAG system, a relevant passage is one that contains information that should appear in the summary. But relevance is not absolute. A passage might be relevant for a detailed summary but not for a concise summary. A passage might be relevant for a summary focused on financial results but not for a summary focused on strategic initiatives. Joint training teaches the retrieval model to adapt its notion of relevance based on the summarization task's specific requirements, which are encoded in the generation training examples.

End-to-end RAG training data is expensive to construct because it requires running the retrieval system for every training query and then labeling or generating responses based on the actual retrieved passages. You cannot reuse standard question-answering datasets without modification. You must process every example through your retrieval pipeline and verify that the training target is achievable given the retrieved context. This is why many RAG systems start with separate retrieval and generation training, and only move to end-to-end training once they have deployed an initial version and collected production data showing how retrieval and generation interact in practice.

## Evaluation Data for RAG Systems: Beyond Component Metrics

Training data teaches the model what to learn. Evaluation data measures whether it learned correctly. For RAG systems, evaluation data must test both retrieval quality and generation quality, but more importantly, it must test the interaction between them. A retrieval model with 90% precision and a generation model with 90% accuracy can still produce a RAG system with 70% end-to-end accuracy if retrieval errors cascade into generation errors.

Retrieval evaluation data consists of queries and labeled passage relevance judgments. For each query, you have a list of passages labeled as relevant or irrelevant, ideally with graded relevance. You measure retrieval quality using metrics like precision at k, recall at k, mean reciprocal rank, and normalized discounted cumulative gain. These metrics tell you whether the retrieval component is surfacing the right passages in the right order.

Generation evaluation data consists of query-passage-response triples where the passages are realistic retrieval outputs, not gold-standard contexts. You measure generation quality using metrics like exact match, F1 score, BLEU, ROUGE, or BERTScore for tasks with deterministic outputs, and human evaluation for tasks where outputs are open-ended. The key is that the passages must reflect what the retrieval system actually returns, including errors, so you are measuring the generation model's ability to handle realistic inputs.

End-to-end evaluation data consists of queries and target responses, and you measure the full RAG pipeline by issuing the query, retrieving passages, generating a response, and comparing that response to the target. This is the only evaluation that matters for production, because it is what users experience. Component metrics are useful for debugging and for understanding where improvements are needed, but they do not directly measure user-facing quality.

You also need adversarial evaluation data that tests edge cases and failure modes. For retrieval, this includes queries where many passages are topically related but only one is truly relevant, queries where the relevant passage uses very different vocabulary than the query, and queries where no passage in the corpus is relevant. For generation, this includes cases where the retrieved passages contradict each other, cases where the passages are relevant but lack the specific detail needed to answer the query, and cases where the passages contain misleading or incorrect information.

Constructing high-quality evaluation data for RAG systems is as important as constructing training data, and it requires the same attention to domain alignment, difficulty distribution, and coverage of edge cases. Evaluation data must be held out from training, must represent production query distributions, and must include enough examples to measure performance reliably on the subpopulations you care about.

Understanding how to construct training and evaluation data for RAG systems is essential to building retrieval and generation components that work together effectively. But training data construction is only part of the dataset engineering challenge. As datasets grow and evolve, you also need strategies for versioning, managing drift, and maintaining quality over time. The next subchapter examines dataset versioning and evolution: tracking changes and managing drift in training datasets.
