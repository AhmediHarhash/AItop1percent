# 5.11 â€” Breaking Change Governance: Who Approves, Rollout Plans, Migration Windows

In March 2025, a conversational AI company serving 140 enterprise customers pushed a schema change to their evaluation dataset format without coordinating across teams. The change seemed innocuous: they renamed a field from "confidence" to "confidence_score" and changed the timestamp format from Unix epoch to ISO 8601 strings. The dataset engineering team thought they were improving clarity. They updated their internal pipelines, published the new dataset version, and moved on. Within six hours, 22 downstream systems broke. The ML platform team's automated retraining pipeline crashed because it expected numeric timestamps. The monitoring system stopped ingesting quality metrics because it parsed the old field name. The customer analytics dashboard went dark because the schema validator rejected every new record. The incident took 31 hours to fully resolve, cost the company an estimated $480,000 in lost productivity and emergency engineering time, and triggered three customer escalations from Fortune 500 clients whose production systems depended on stable data feeds. The root cause was not technical incompetence. It was governance failure. No one asked who else depended on the dataset schema. No one defined what constituted a breaking change. No one required approval before publishing. No one planned a migration window or maintained backwards compatibility. The company had built sophisticated dataset versioning infrastructure but treated schema changes like internal refactoring instead of API contracts with downstream consumers.

Breaking changes are not bugs. They are planned disruptions that you choose to inflict on every system and team that consumes your data. The question is whether you manage that disruption deliberately or let it cascade uncontrolled through your infrastructure.

## What Constitutes a Breaking Change

You need explicit definitions. A breaking change is any modification to a dataset that causes existing consumers to fail, produce incorrect results, or require code changes to continue functioning. This is not subjective. Removing a field is a breaking change. Renaming a field is a breaking change. Changing a field's data type is a breaking change. Changing the format of a string field from one standard to another is a breaking change. Changing the range or distribution of numeric values can be a breaking change if downstream systems have hardcoded thresholds. Reordering fields in a CSV is a breaking change if consumers parse by position. Changing the primary key structure is a breaking change. Changing the partitioning scheme is a breaking change if consumers rely on partition paths.

Additive changes are usually safe but not always. Adding a new field to the end of a schema is generally backwards compatible. Adding a new partition to an existing partitioning scheme is generally safe. Adding new enum values to a field that already existed is context-dependent: if downstream systems use exhaustive switch statements without default cases, you just broke them. If they handle unknown values gracefully, you did not. You cannot assume. You must verify.

Schema evolution patterns matter. Some changes appear breaking but can be implemented non-destructively. Renaming a field does not require deleting the old one immediately. You can publish both the old field and the new field in parallel, mark the old one as deprecated, give consumers six months to migrate, then remove it. Changing a data type can follow the same pattern: publish both versions of the field, deprecate the old one, monitor until usage drops to zero, then remove it. This costs storage and processing overhead but preserves stability for consumers who are not ready to migrate on your timeline.

The definition of breaking vs. non-breaking must be written down and shared across every team that produces datasets. It must be part of onboarding documentation for new engineers. It must be enforced by automated schema validators that run in CI/CD pipelines before any dataset version is published. If your validator does not flag these changes and halt the pipeline until a human approves, you will ship breaking changes by accident.

## Approval Workflows and Ownership Models

Someone must have veto power. Breaking changes cannot be self-service. If any engineer can publish a breaking change without review, you will have incidents. The approval workflow depends on your organizational structure, but the principle is universal: breaking changes require sign-off from representatives of every major consumer category.

Start with ownership registration. Every dataset must have a declared owner team and a declared set of known consumers. This is not optional. If you cannot list who consumes your dataset, you are not ready to version it. The consumer list does not need to be exhaustive, but it must include the major internal teams and external systems that depend on the data. For internal consumers, you register team names and Slack channels. For external consumers, you register customer segments or integration partners. This registry lives in your dataset catalog and is updated every time a new consumer onboards.

The approval workflow is staged. First, the dataset owner proposes the breaking change by opening a formal change request. The request includes the old schema, the new schema, a diff highlighting breaking fields, a justification for why the change is necessary, and a proposed migration plan with timelines. This is not a Slack message. It is a document or ticket that persists and can be reviewed asynchronously.

Second, the change request is routed to representatives from each major consumer team. These representatives are not junior engineers. They are senior ICs or tech leads who understand their team's dependencies and can assess migration complexity. Each representative has 72 hours to review and respond. If they approve, they commit to a migration timeline. If they object, they explain why and propose alternatives. If they do not respond within 72 hours, you escalate to their manager. Silence is not consent.

Third, if all consumer representatives approve, the change request goes to a dataset platform governance committee. This is a rotating group of three to five senior engineers from across the organization who review breaking changes weekly. They verify that the migration plan is realistic, that the timelines are reasonable, that backwards compatibility was considered, and that the justification is sound. They have final veto power. If they reject the change, the dataset owner must revise the proposal or abandon it.

This process sounds bureaucratic. It is. Bureaucracy exists because informal processes fail under scale. When you have 60 datasets and 200 consuming systems, you cannot coordinate breaking changes over Slack. You need formal workflows, written records, and accountability.

## Migration Plans and Backwards Compatibility Windows

Every approved breaking change must include a migration plan. The plan specifies three dates: the deprecation announcement date, the backwards compatibility end date, and the old version shutdown date. These dates are non-negotiable once approved. Moving them requires reopening the approval process.

The deprecation announcement date is when you publish both the old schema and the new schema in parallel. You update your dataset catalog to mark the old schema as deprecated. You send notifications to all registered consumers via email and Slack. You update documentation to show the new schema as preferred and the old schema as supported but deprecated. From this point forward, new consumers should onboard to the new schema, but existing consumers can continue using the old schema without disruption.

The backwards compatibility window is the period during which you support both schemas simultaneously. The length depends on the complexity of the migration and the number of consumers. For internal consumers with small, agile teams, three months is typical. For external consumers or large internal platforms with quarterly release cycles, six to twelve months is more realistic. The window must be long enough that no consumer is forced to drop everything and migrate immediately. If you only give consumers two weeks, you are not managing a migration. You are creating an emergency.

During the backwards compatibility window, you publish data in both formats. This usually means writing to two different output paths or maintaining both field versions in the same records. The implementation depends on your pipeline architecture. If you use Parquet files, you can include both the old field and the new field in every record. If you use separate CSV exports, you can publish two versions of the file to different S3 prefixes. If you use a streaming system like Kafka, you can publish to two topics or include both fields in the message schema with one marked deprecated. The key is that consumers see no disruption. Existing pipelines continue to work without changes.

You monitor adoption during the migration window. Your pipeline emits metrics tracking how many consumers are reading from the old schema path versus the new schema path. You publish a weekly dashboard showing migration progress. You send reminders to teams that have not started migrating. You offer office hours where dataset engineers help consumer teams update their code. You do not wait until the end of the window and hope everyone migrated. You actively drive the migration.

The old version shutdown date is when you stop publishing the old schema. This date is communicated at the start of the migration window and reiterated in weekly reminders. One month before shutdown, you send a final warning to any teams still using the old schema. One week before shutdown, you send a final final warning and escalate to their management if they have not responded. On the shutdown date, you stop writing to the old schema path, mark it as deprecated in your catalog, and archive the old data. If a consumer missed the migration, their pipeline breaks. This sounds harsh, but it is necessary. If you keep extending the deadline every time someone is not ready, the migration never completes and you maintain two schemas indefinitely.

## Staged Rollouts and Canary Consumers

Not all breaking changes can be flipped at once across all consumers. For high-risk changes affecting dozens of downstream systems, you need staged rollouts. You identify a small set of canary consumers who migrate first. These are typically internal teams with good monitoring, fast iteration cycles, and engineers who can respond quickly if something breaks. You work with them to migrate to the new schema while all other consumers remain on the old schema.

The canary phase lasts two to four weeks. During this time, you monitor the canary consumers closely. You check for data quality issues, schema validation errors, downstream pipeline failures, and unexpected behavior. You collect feedback from the canary teams about migration pain points, documentation gaps, and tooling issues. If the canary phase reveals problems, you fix them before expanding the rollout. If everything is stable, you move to the next stage.

The second stage is a broader rollout to 20 to 30 percent of consumers. These are typically less critical systems or teams that volunteered to migrate early. You continue monitoring. You measure migration success rate, time to migrate per team, support requests, and incident rate. If the second stage is clean, you proceed to general availability.

General availability means all remaining consumers are expected to migrate within the backwards compatibility window. You publish an announcement, update documentation, send migration guides, and offer support. You still maintain both schemas during the window, but the expectation is that teams migrate proactively rather than waiting until the deadline.

Staged rollouts reduce blast radius. If your breaking change has an unforeseen consequence, you discover it when ten teams are affected, not when 80 teams are affected. You have time to fix the issue, communicate clearly, and adjust your migration plan before most consumers are impacted. This is especially important for schema changes that affect data semantics rather than just structure. If you change how a field is calculated or what values are valid, you cannot predict every downstream dependency. Staged rollouts give you empirical feedback before you commit the entire organization.

## Communication Plans and Stakeholder Notifications

Breaking changes fail when consumers do not know they are coming. You cannot assume that publishing a changelog or updating a schema registry is sufficient. You must actively notify stakeholders through multiple channels over the entire migration timeline.

The notification plan starts at approval. As soon as the breaking change is approved, you send an announcement email to all registered consumer teams. The email includes the change summary, the justification, the migration timeline, links to updated documentation, example code for migrating, and contact information for support. You post the same announcement in a dedicated Slack channel for dataset changes. You update the dataset catalog page with a prominent deprecation notice.

You send reminders at regular intervals. At the start of the backwards compatibility window, you send a reminder. At 25 percent of the way through the window, you send another. At 50 percent, another. At 75 percent, a more urgent reminder highlighting the upcoming deadline. At 90 percent, a final warning to teams that have not migrated. Each reminder includes updated migration progress statistics showing how many teams have completed the migration and how many have not.

You hold office hours. You schedule weekly or biweekly sessions where consumer teams can ask questions, get help debugging migration issues, and review their updated pipelines. You publish a FAQ document addressing common migration questions. You create a Slack channel specifically for migration support and staff it with engineers from the dataset team who can respond quickly.

You escalate when necessary. If a critical consumer team is not responding to reminders and the deadline is approaching, you escalate to their management. You do not wait until the last week. You escalate when the team is 30 days away from the deadline and has shown no progress. You frame it as a partnership, not a threat: we want to help you migrate successfully, but we need to know if you are blocked or need more time so we can adjust the plan.

You document the outcome. After the migration is complete and the old schema is shut down, you publish a post-mortem summarizing what went well, what went poorly, how many teams migrated on time, how many required extensions, what issues were discovered, and what you will do differently next time. This becomes institutional knowledge that improves future migrations.

## Backwards Compatibility Strategies

The best breaking change is the one you avoid. Before approving any breaking change, you explore whether you can achieve the same goal with a backwards-compatible evolution. In many cases, you can.

Field additions are backwards compatible if consumers ignore unknown fields. If you need to add data, you add new fields and leave old fields unchanged. Consumers that do not need the new fields continue working without modification. Consumers that want the new fields update their schemas to include them. This is the safest evolution pattern.

Field deprecation without removal is backwards compatible. You mark a field as deprecated in documentation and in schema metadata, you publish a new recommended field, but you continue populating the old field indefinitely. This allows consumers to migrate on their own timeline without coordination. The cost is storage and processing overhead. The benefit is zero disruption. For high-value datasets with many external consumers, this cost is worth it.

Dual writes are backwards compatible during transitions. If you need to change how a field is calculated or formatted, you can write both the old version and the new version to separate fields. Consumers gradually migrate from the old field to the new field. Once all consumers have migrated, you stop writing the old field. This is the pattern used by major API providers when evolving public endpoints. It works equally well for internal datasets.

Versioned schemas in the same dataset are backwards compatible if consumers specify which version they want. You publish multiple schema versions to different partitions or paths within the same dataset. Consumers request the version they are compatible with. New consumers use the latest version. Old consumers continue using older versions until they are ready to migrate. This requires more sophisticated catalog and discovery infrastructure, but it decouples consumer migration timelines entirely. Some data platforms support this natively through schema registries with versioned entries.

The question is always whether the backwards-compatible approach is feasible given your constraints. If the old schema has a fundamental flaw that causes data quality issues, you may need to break it. If maintaining both versions doubles your storage cost and you are already over budget, you may need to force migration. If the old schema exposes PII that should not be in the dataset, you must remove it immediately. Backwards compatibility is preferred but not absolute. The governance process exists to make these trade-offs explicit and approved.

## Rollback Plans and Incident Response

Even with perfect planning, breaking changes can fail. Your migration plan must include a rollback strategy. If you deploy the new schema and discover a critical bug, data quality issue, or unforeseen downstream breakage, you need to revert quickly without making things worse.

The simplest rollback is to extend the backwards compatibility window. If consumers are hitting issues with the new schema and you have not yet shut down the old schema, you delay the shutdown date. You notify all stakeholders that the migration is paused, you investigate the issues, you fix them, and you resume the migration when stability is restored. This is low-risk because the old schema is still available.

If you already shut down the old schema, rollback is harder. You need to restart the old pipeline and republish old-format data. This means you must retain the pipeline code and configuration for the old schema even after shutdown. You do not delete it immediately. You archive it in version control with clear tags indicating it is the rollback version. If you need to revert, you redeploy the old pipeline, reprocess recent data, and republish to the old schema path. This takes hours, not minutes, so you only do it for critical incidents.

You define rollback triggers in advance. A rollback is triggered if more than 20 percent of consumers report breaking issues within 48 hours of the new schema launch. A rollback is triggered if a high-severity data quality issue is discovered in the new schema that affects downstream business metrics. A rollback is triggered if a compliance or security issue is identified. These triggers are written into the migration plan and reviewed during the approval process. They give you objective criteria for when to roll back rather than relying on subjective judgment during an incident.

You rehearse rollbacks. Before launching a high-risk breaking change, you conduct a tabletop exercise where you simulate a failed migration and walk through the rollback process. You verify that the old pipeline code still runs, that the old schema is still documented, that you know how to republish old data, and that stakeholders know who to contact. This rehearsal surfaces gaps in your rollback plan before you need it in production.

## Cross-Team Coordination and Dependency Mapping

Breaking changes expose hidden dependencies. A dataset you thought only served your team may also feed three other teams you did not know about. A schema field you thought was unused may be critical to a dashboard that executives review daily. You discover these dependencies during governance review, not during incidents.

You build a dependency graph. Your dataset catalog tracks not just which teams own which datasets but which datasets depend on which other datasets. When Team A's pipeline reads from Team B's dataset, that dependency is registered. When Team C's model training job uses Team A's processed dataset, that dependency is registered. When Team D's dashboard queries Team C's model outputs, that dependency is registered. Over time, you accumulate a graph of data lineage across your entire organization.

This graph powers impact analysis. When a dataset owner proposes a breaking change, the catalog automatically identifies all downstream consumers by traversing the dependency graph. It generates a list of teams that must review and approve the change. It estimates the blast radius by counting how many datasets, pipelines, models, and dashboards are transitively affected. This turns governance from a manual coordination problem into an automated workflow.

You enforce registration. Teams cannot consume a dataset without registering their dependency in the catalog. This is enforced by access control policies: to get read access to a dataset, you must declare your team, your use case, and your pipeline identifier. This creates visibility. It also creates accountability. If you register as a consumer, you are on the notification list for breaking changes. If you do not register, you do not get notified, and your pipeline may break without warning.

You audit orphaned consumers. Periodically, you scan access logs to identify systems reading datasets that are not in the registry. You reach out to the owners and ask them to register. If they do not respond, you revoke access. This keeps the registry accurate. It also prevents shadow consumers who bypass governance and then complain when breaking changes break their undocumented pipelines.

Cross-team coordination is not about slowing down innovation. It is about making coordination costs visible and manageable. If a breaking change affects 40 teams, that is important information. It tells you the change should be planned carefully, migrated slowly, and communicated extensively. If a breaking change affects two teams, you can move faster. The governance process adapts to the actual risk.

## Regulatory and Compliance Constraints on Schema Changes

Some breaking changes are not just engineering decisions. They are compliance decisions. If your dataset contains PII, financial data, or health information, schema changes may require legal or compliance review before approval. If you are subject to GDPR, you cannot unilaterally change how you store or process personal data without updating your data processing agreements. If you are subject to SOX, you cannot change financial dataset schemas without documenting the change in your audit trail. If you are subject to HIPAA, you cannot change how you anonymize patient data without verifying that the new schema still meets de-identification standards.

You identify regulated datasets during the approval process. Your dataset catalog marks datasets that contain PII, financial data, health data, or other regulated content. When a breaking change is proposed for a regulated dataset, the approval workflow automatically routes the request to your legal or compliance team in addition to technical reviewers. They verify that the change does not introduce compliance risk.

Compliance review is not rubber-stamping. Your compliance team checks whether the schema change affects data retention policies, whether it changes how PII is pseudonymized, whether it impacts audit logging, whether it requires updating data processing agreements with customers or partners, and whether it triggers notification obligations under GDPR or other regulations. If the answer to any of these is yes, the compliance team works with the dataset owner to address the issue before approval.

Some schema changes require customer notification. If you provide datasets to external customers under contract, and the contract specifies a schema format, you cannot break that schema without amending the contract or notifying the customer with sufficient advance notice. Your legal team reviews the contracts and determines notification requirements. This can extend your migration timeline significantly. If you need 90 days to notify customers and get consent, your backwards compatibility window must account for that.

You document compliance decisions. Every breaking change that touches regulated data includes a compliance section in the change request explaining what regulations apply, what compliance review was conducted, what mitigations were implemented, and who signed off. This documentation is retained for audit purposes. If regulators ask how you manage schema changes for datasets containing personal data, you can show them the governance process and the records of compliance review.

## Tooling and Automation for Change Management

Governance does not scale without tooling. You cannot manage breaking changes for 200 datasets across 60 teams using spreadsheets and Slack. You need automated workflows that enforce process, track progress, and provide visibility.

Your schema registry is the foundation. It stores every version of every dataset schema, tracks which versions are active, which are deprecated, and which are retired. It enforces schema validation on every write: if a pipeline tries to publish data that does not match the registered schema, the write is rejected. It provides APIs that consumers use to fetch the current schema for a dataset before reading data. It emits metrics on schema version adoption so you can track migration progress.

Your change management system integrates with the schema registry. When a dataset owner wants to propose a breaking change, they submit a change request through a web UI or CLI tool. The system compares the old schema and new schema, identifies breaking fields, and generates a diff report. It looks up registered consumers from the catalog and routes the request to their representatives for approval. It tracks approval status and sends reminders to reviewers who have not responded. It enforces that all required approvals are collected before the change is marked as approved.

Your CI/CD pipeline enforces schema compatibility. When a dataset pipeline is updated, the CI system runs a schema compatibility check comparing the new schema to the currently published schema. If the check detects breaking changes and there is no approved change request on file, the build fails. The engineer must either revert the breaking change or file a change request and wait for approval before the pipeline can deploy. This prevents accidental breaking changes from reaching production.

Your monitoring system tracks migration progress. It counts how many consumers are reading from the old schema path versus the new schema path. It measures read volumes, error rates, and schema validation failures for each path. It generates dashboards showing migration progress over time. It sends alerts if a consumer starts reading from a deprecated schema path after the backwards compatibility window has ended.

Your notification system automates stakeholder communication. When a breaking change is approved, the system sends the initial announcement email. It schedules reminder emails at predefined intervals. It tracks which teams have migrated by monitoring schema path usage and automatically removes them from the reminder list. It escalates to managers if teams are approaching the deadline without progress. This eliminates the manual overhead of tracking and reminding dozens of teams.

The tooling investment is significant. Building this infrastructure takes months and requires dedicated platform engineering resources. But it is non-negotiable at scale. Without it, governance becomes a bottleneck that either slows down every change or gets bypassed entirely. With it, governance becomes a lightweight, automated process that protects stability without blocking progress.

Breaking changes are not inherently bad. They are necessary as your data models evolve, your business requirements change, and your understanding deepens. The difference between professional and amateur dataset engineering is not whether you make breaking changes. It is whether you manage them deliberately, communicate them clearly, and protect downstream consumers from unnecessary disruption. The governance process, migration plans, backwards compatibility strategies, and tooling are the mechanisms that separate controlled evolution from chaos. Next, we examine how to build dataset pipelines that survive failures without corruption: idempotency, backfills, and reprocessing strategies that make your infrastructure resilient.
