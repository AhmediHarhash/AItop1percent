# 7.7 â€” Negative Examples and Hard Negatives

In August 2025, a legal technology company launched a fine-tuned model for contract clause classification after training on 12,000 carefully labeled positive examples across 47 clause types. The model performed beautifully in initial testing, achieving 94% accuracy on their validation set. Two weeks into production, the Legal Operations team reported that the model was flagging nearly every paragraph in standard employment agreements as containing intellectual property assignment clauses, even obvious boilerplate like severability provisions and notice requirements. The classification precision had collapsed from 94% to 31% in the wild. The issue was immediately obvious to the ML team once they examined the training data: they had shown the model thousands of examples of what each clause type looked like, but never shown it what those clause types were not. The model had learned to recognize IP assignment language patterns, but had no concept of what to reject. It took three months and 18,000 negative examples to rebuild the training set and retrain the model to production quality.

This failure represents one of the most common mistakes in fine-tuning dataset construction: the assumption that positive examples alone are sufficient for a model to learn meaningful boundaries. Models do not inherently understand what you do not want them to do. They learn from the decision boundaries you explicitly encode in your training data. Without negative examples, models develop overly permissive decision boundaries that trigger on surface-level pattern matches rather than deep semantic understanding. Negative examples are not optional augmentation. They are foundational to teaching models what to reject.

## Why Models Need to See What Not to Do

Positive examples teach a model what a category, format, or response pattern looks like. Negative examples teach the model what that category is not. This distinction is critical because language models learn by modeling probability distributions over text sequences. When you fine-tune a model on positive examples only, you are effectively teaching it to increase the probability of generating text that resembles those examples whenever the input context is even vaguely similar. You are not teaching it to decrease the probability of generating text that superficially resembles the target but is semantically wrong.

Consider a content moderation model trained to flag hate speech. If you train it exclusively on examples of hate speech, the model learns the linguistic patterns, vocabulary, and syntactic structures common in hateful content. But it does not learn to distinguish hate speech from heated political debate, from academic discussion of discrimination, from quoted speech in news articles, from reclaimed slurs used within communities, or from false positives that happen to share vocabulary with hateful content. Each of these categories requires explicit negative examples showing the model contexts where the superficial markers of hate speech are present but the content should not be flagged.

The same principle applies across task types. A summarization model trained only on good summaries will happily generate fluent text that sounds like a summary but fabricates facts not present in the source document. A code generation model trained only on correct implementations will generate syntactically valid code that produces the wrong result. A named entity recognition model trained only on positive entity spans will tag everything that looks like a name, including fictional characters in hypothetical examples, placeholder text, and quoted speech. Negative examples carve out the space where the model should withhold the behavior you have trained.

The absence of negative examples creates models that are overconfident and underdiscriminating. They learn to trigger on cues rather than meaning. You see this most clearly in classification tasks, where models without negative examples often default to the majority class for anything ambiguous, or worse, spread predictions across multiple plausible categories because they have never learned the boundaries that separate them. Negative examples force the model to learn those boundaries explicitly.

## Negative Examples in Classification, Generation, and Retrieval Tasks

The role and structure of negative examples varies significantly by task type, but the underlying principle remains constant: you must show the model what to reject in contexts that are plausibly confusable with what to accept.

In classification tasks, negative examples are instances that superficially resemble the positive class but belong to different categories or to no category at all. For a sentiment classifier, negative examples for the positive class include neutral statements, sarcastic statements, conditional positive statements, and negative statements. For each class you define, you need negative examples that could plausibly be confused with that class based on surface features but are definitively not members. A well-constructed negative set for a five-class intent classifier includes examples from each of the other four classes, examples that fall outside all five classes entirely, and examples that blend intents in ways that should map to a specific resolution strategy.

In generation tasks, negative examples are outputs that are fluent and contextually plausible but factually wrong, stylistically inappropriate, or structurally malformed according to your task specification. These are harder to construct than classification negatives because you cannot simply sample from a different category. You need to generate outputs that are wrong in the specific ways your production system is likely to fail. For a customer support response generator, negative examples include responses that are polite and grammatically correct but provide incorrect information, responses that answer a different question than the one asked, responses that include appropriate policies but apply them to the wrong scenario, and responses that are factually accurate but violate tone guidelines.

In retrieval tasks, negative examples are passages or documents that are topically related to the query but do not contain the information the query seeks. A retrieval model for a medical knowledge base needs negative examples showing documents that discuss the same condition but do not answer the specific clinical question, documents that mention the same drugs but in different contexts, and documents that share vocabulary with the query but address different patient populations or treatment goals. The key is relevance without correctness. Random negatives teach the model nothing because they are too easy to distinguish. Topically related negatives teach the model the fine-grained distinctions that matter in your domain.

The construction strategy differs, but the requirement is universal: negative examples must be plausibly confusable with positives based on features the model can observe, but definitively distinguishable based on features the model should learn to prioritize.

## Hard Negatives: Examples That Are Almost Correct but Subtly Wrong

Not all negative examples provide equal training signal. Easy negatives, instances that are obviously distinct from the positive class, teach the model very little because the decision boundary is trivial. A spam classifier does not need many examples of empty emails or emails in foreign languages to learn that marketing emails in English are spam. The model needs examples of legitimate transactional emails that share vocabulary, structure, and formatting with promotional emails but serve a fundamentally different purpose. These are hard negatives.

Hard negatives are negative examples that are maximally similar to positive examples on all features except the decision-critical ones. They sit close to the decision boundary you want the model to learn. Training on hard negatives forces the model to learn fine-grained distinctions rather than relying on superficial cues. A contract clause classifier needs hard negatives showing limitation of liability clauses in contexts where they might be confused with indemnification clauses, warranty disclaimers, or force majeure provisions. Each of these shares legal language, conditional structures, and risk allocation concepts with the target clause type, but represents a distinct legal function that requires separate handling.

Hard negatives are especially critical in domains where precision matters more than recall. In content moderation, a hard negative might be a news article reporting on hate speech that quotes hateful language but is clearly journalistic in context. In medical diagnosis support, a hard negative might be a patient presentation that shares symptoms with the target condition but has a different underlying cause. In financial fraud detection, a hard negative might be an unusual but legitimate transaction pattern that resembles known fraud typologies but has a coherent business explanation.

The challenge is that hard negatives are expensive to source. Easy negatives can be randomly sampled or synthetically generated. Hard negatives require domain expertise to identify and label. You need annotators who understand the decision boundary well enough to recognize near-miss cases. In practice, this means your hard negative mining strategy must be tightly coupled to your production error analysis. Every false positive your production model generates is a candidate hard negative. Every user correction, every escalated edge case, every support ticket that begins with the phrase "the system incorrectly classified" is pointing you toward the hard negatives your training set lacks.

A mature hard negative mining pipeline includes automated extraction of production errors, prioritization by frequency and business impact, expert review to confirm the negative label and identify the confounding features, and systematic injection back into the training pipeline with versioning to track which hard negative categories have been addressed. This is not a one-time dataset construction step. It is a continuous learning loop where production failures become training signal for the next model iteration.

## Mining Hard Negatives from Production Errors and Near-Misses

The most valuable hard negatives come from your production system, not from synthetic generation or random sampling. Production errors reveal the specific confusions your model exhibits in real user contexts with real data distributions. Near-misses, cases where the model almost made the wrong decision but was corrected by a downstream process or human reviewer, reveal decision boundary weaknesses before they become visible failures.

In mid-2025, a healthcare analytics company built a hard negative mining pipeline for their clinical coding model by instrumenting every human override in their coding review workflow. When a medical coder corrected an AI-suggested code, the system logged the original note text, the suggested code, the correct code, and the coder's reason for the change. Over six months, this pipeline surfaced 14,000 hard negatives across 200 code categories. The most common pattern was condition codes suggested for symptom descriptions, a classic confusion between observed presentation and underlying diagnosis. The second most common pattern was laterality errors, where the model correctly identified the condition and body system but assigned it to the wrong side of the body. These hard negatives were then prioritized by downstream cost, because laterality errors in surgical codes trigger insurance denials and patient billing issues.

The mining process has four stages: error capture, error categorization, example extraction, and training integration. Error capture requires instrumentation at every point where a human corrects or overrides a model output. This includes explicit corrections through a review interface, implicit corrections through user behavior like immediately editing a generated draft, and escalations where a user routes a task to manual handling because the model output is insufficient. You cannot mine what you do not measure.

Error categorization clusters errors by type so you can identify systematic boundary weaknesses rather than treating each error as an independent event. A contract analysis model might categorize errors into entity recognition failures, clause classification confusions, date parsing errors, cross-reference resolution failures, and definition extraction misses. Within each category, you further segment by the confounding factor: entity recognition failures might split into missed entities, hallucinated entities, incorrect entity type assignments, and boundary errors where the entity span is partially correct. This categorization tells you which types of hard negatives to prioritize.

Example extraction pulls the specific input-output pairs that triggered each error category, along with the correct output and any available explanation of why the model output was wrong. For classification errors, this is straightforward: the input, the incorrect predicted class, and the correct class. For generation errors, you need to capture not just that the output was wrong but specifically what made it wrong, because generation models can fail in many ways on the same input. An incorrect summary might be wrong because it fabricated facts, because it omitted critical information, because it included irrelevant details, or because it misrepresented the tone or framing of the source. Each failure mode requires different hard negatives.

Training integration decides how to use hard negatives in the next training run. The simplest approach is to add hard negatives to the training set with the correct label and retrain. This works if the hard negatives represent gaps in coverage, but it does not work if the model is confusing two similar categories due to overlapping features. In that case, you need contrastive learning, where the model is explicitly trained to distinguish the positive from the hard negative in paired examples. You might also adjust the loss function to penalize hard negative errors more heavily than easy negative errors, forcing the model to prioritize learning the difficult boundaries.

## The Ratio Question: How Many Negatives per Positive

The ratio of negative to positive examples in your training set directly shapes the decision boundary your model learns. Too few negatives and the model becomes overly permissive, triggering on weak signals. Too many negatives and the model becomes overly conservative, requiring overwhelming evidence before committing to the positive class. The optimal ratio depends on your task structure, your data distribution, and your downstream cost model.

For binary classification tasks where the positive class is rare in production, you typically need a higher ratio of negatives to positives in training than you observe in production. A fraud detection model might see fraud in 0.2% of transactions in production, but training on a 1:500 negative-to-positive ratio will produce a model that almost never predicts fraud because the learned prior is so heavily weighted toward the negative class. Instead, you train on something closer to 5:1 or 10:1 negatives to positives, which gives the model enough signal to learn the positive class decision boundary while still teaching it to reject the negative class.

For multi-class classification, the ratio question applies to each class independently. A document classification model with 20 categories needs negative examples for each category, and those negative examples should be drawn disproportionately from the categories most likely to be confused with the target. A legal brief classification model might have ten times as many motion-to-dismiss examples in the negative set for summary judgment motions than it has examples from unrelated categories like discovery motions, because the confusion risk is asymmetric.

For generation tasks, the ratio concept is less clear because you are not training a binary decision boundary. Instead, you are training the model to assign higher probability to correct outputs than to incorrect outputs. Here, the ratio question becomes: how many incorrect outputs should you show per correct output? The answer depends on the diversity of ways the task can fail. A code generation model benefits from seeing multiple incorrect implementations for each correct implementation, with each incorrect version demonstrating a different type of error: wrong algorithm, correct algorithm with off-by-one errors, correct logic with syntax errors, correct implementation for a different interpretation of the requirements. Five to ten incorrect examples per correct example is common in code generation training sets.

For retrieval tasks, the negative ratio is typically much higher because the space of non-relevant documents is vast. A retrieval model for question answering might train on one positive passage per query and 50 to 100 negative passages, with the negatives stratified by difficulty. Ten might be easy negatives randomly sampled from the corpus, 30 might be topically related but non-responsive negatives, and ten might be hard negatives that nearly answer the question but have critical missing information or incorrect details. This ratio ensures the model learns to distinguish true relevance from topical similarity.

The ratio is not static. Early in training, when the model has weak decision boundaries, you can train on easier ratios with more easy negatives. As the model improves, you shift the negative distribution toward harder negatives that challenge the current decision boundary. This curriculum learning approach prevents the model from getting stuck in local optima where it learns to reject easy negatives but never develops the capacity to handle hard ones.

## When Negative Examples Hurt: Over-Correction and Refusal Behavior

Negative examples are essential, but they can be misused. Too many negatives, particularly too many hard negatives, can induce over-correction where the model becomes overly cautious and refuses to commit to the positive class even when the evidence is clear. This is especially problematic in generation tasks, where excessive exposure to incorrect outputs can teach the model to avoid entire categories of responses rather than learning to generate them correctly.

In late 2024, an education technology company fine-tuned a writing feedback model on 8,000 student essays with detailed critiques. To prevent the model from generating generic praise, they included 12,000 negative examples showing inappropriate feedback: overly harsh critiques, factually incorrect suggestions, feedback on the wrong aspect of the essay, and feedback that misunderstood the assignment. After deployment, teachers reported that the model frequently responded with variations of "I cannot provide feedback on this essay" or "This essay is too complex for me to evaluate." The model had learned that generating feedback was risky because most of the feedback examples it had seen during training were labeled as wrong. It developed a refusal strategy as a form of self-protection.

This over-correction pattern appears when negative examples outnumber positive examples by more than 20:1 in generation tasks, or when negative examples are systematically more salient or memorable than positive examples. Salience can come from length, from emotional intensity, from specificity, or from the diversity of negative patterns. If your positive examples are homogeneous but your negative examples span a wide range of failure modes, the model learns that the negative space is larger and more complex than the positive space, and it defaults to avoidance.

Refusal behavior also emerges when hard negatives are too hard. If the distinction between a positive and a hard negative requires context or reasoning that the model cannot reliably perform, the model learns that it cannot trust its own judgment and begins refusing to make predictions. A medical diagnosis model shown hard negatives where the distinction between the correct diagnosis and the hard negative diagnosis depends on lab values not mentioned in the clinical note will learn that clinical notes are insufficient evidence for diagnosis, and it will start hedging or refusing to output a diagnosis at all.

The solution is not to remove hard negatives, but to ensure that for every hard negative, there are multiple positive examples that demonstrate the correct handling of similar contexts. If you show the model a hard negative where a limitation of liability clause is confused with an indemnification clause, you also need to show it three positive examples of limitation of liability clauses in similar contractual contexts and three positive examples of indemnification clauses in similar contexts, so the model learns the distinguishing features rather than learning that these contexts are ambiguous.

Another failure mode is negative transfer, where training the model to reject certain outputs in one context causes it to reject similar outputs in different contexts where they are appropriate. A content moderation model trained to flag political advocacy in customer support responses might start flagging political content in user-generated posts where political discussion is allowed. A code generation model trained to avoid certain deprecated APIs might refuse to generate code that discusses those APIs even in comments or documentation. This happens when your negative examples are not sufficiently contextualized. The model needs to learn not just that an output is wrong, but that it is wrong in this specific context for this specific reason.

You prevent negative transfer by pairing every category of negative examples with explicit positive examples showing the same output type in contexts where it is appropriate. If you are training a model not to generate medical advice in a general chatbot, you also train it on examples where generating medical information is appropriate: responses to explicit health questions in a clinical decision support tool, patient education materials in a healthcare app, explanatory content in a medical reference database. The model learns the contextual boundaries, not a blanket prohibition.

## Integrating Negative Examples into the Training Pipeline

Negative example integration is not a one-time dataset construction step. It is a continuous process where you systematically mine production errors, categorize failure modes, extract hard negatives, and retrain models with updated negative sets. The pipeline has three components: negative example generation, negative example validation, and negative example versioning.

Negative example generation has multiple sources. Production error mining, as discussed earlier, provides hard negatives directly from real-world failures. Synthetic generation creates negative examples by applying rule-based transformations to positive examples: introducing factual errors, swapping entity references, altering dates or quantities, changing sentiment markers, or applying style transformations that violate task constraints. Adversarial generation uses a separate model to generate outputs designed to fool your primary model, then labels those outputs as negatives if they successfully confuse the model during evaluation. Each source contributes different types of negatives with different levels of difficulty.

Negative example validation ensures that examples labeled as negative are actually negative according to your task definition, and that they are teaching the distinction you intend. This requires expert review, especially for hard negatives where the boundary is subtle. A validation workflow might route hard negatives to multiple annotators and require agreement, or route them to senior domain experts who can explain the decision boundary in annotation notes. Those notes then inform the next round of positive example collection, because if annotators struggle to articulate why a hard negative is wrong, your positive examples probably do not cover the distinguishing features clearly enough.

Negative example versioning tracks which categories of negatives have been introduced in which training runs, so you can measure the impact of each negative category on model performance and diagnose regressions. A versioned negative set might include 2,000 base negatives from the initial dataset, 500 hard negatives from Q4 2025 production errors, 300 adversarial negatives from red team exercises, and 150 edge case negatives from customer escalations. When you evaluate the model, you break down performance by negative category to identify which boundaries are well-learned and which remain weak.

This versioning also enables rollback when a category of negatives causes over-correction. If adding 200 hard negatives for a specific confusion pattern causes the model to become overly conservative in related contexts, you can remove those negatives, adjust the positive examples to better demonstrate the boundary, and reintroduce the hard negatives in the next training cycle with better positive coverage.

The integration cadence depends on your model update frequency and your error accumulation rate. A high-stakes production model updated monthly might integrate new hard negatives every two weeks, while a lower-frequency model might batch hard negatives quarterly. The key is that hard negative mining is always active. You are always collecting production errors, always categorizing them, always building the next batch of training signal.

Understanding how to construct, mine, and integrate negative examples is foundational to fine-tuning models that make precise distinctions rather than superficial pattern matches. But negative examples alone are not sufficient for high-quality training data. You also need to filter the training set to remove examples that are confusing, contradictory, or otherwise unsuitable for learning. The next subchapter examines dataset filtering for training: quality thresholds that matter.
