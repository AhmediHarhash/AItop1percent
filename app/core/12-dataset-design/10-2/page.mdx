# 10.2 â€” The Dataset Specification Template

A dataset card describes what exists. A dataset specification describes what you need. The distinction matters because most dataset failures begin not during collection or labeling, but during the planning phase, when stakeholders have different assumptions about what the dataset should contain and no one writes those assumptions down. A dataset spec is a contract between the people who need the dataset and the people who will build it. It defines the requirements, constraints, and acceptance criteria before anyone spends time or money on data collection. Without a spec, you get scope creep, misaligned expectations, and datasets that do not support the models or evaluations they were meant to support.

Dataset specs are written before collection begins. They are reviewed by all stakeholders: the data scientists who will use the data, the engineers who will process it, the product managers who define the use case, the legal team that ensures compliance, and the labeling team that will annotate it. The spec is approved, version-controlled, and treated as the source of truth throughout the collection process. If requirements change, the spec is updated, and the change is documented. The spec is not a formality. It is the tool that prevents wasted effort and ensures that what you build matches what you need.

## Required Fields in a Dataset Specification

A dataset spec contains both required and optional fields. The required fields are the minimum information needed to define what the dataset is, what it covers, and how you will know whether it is good enough. Every dataset spec must include a purpose statement, target distribution, size requirements, quality thresholds, format and schema, labeling instructions, and acceptance criteria. Each of these fields serves a distinct function.

The purpose statement defines why the dataset is being built and what it will be used for. It is specific about the task, the domain, the model or system that will consume it, and the product context. For example: "This dataset supports training and evaluation of the invoice extraction model, which extracts vendor name, invoice number, line items, and total amount from scanned PDF invoices submitted by small business customers in the United States. The model is part of the automated bookkeeping pipeline launched in Q1 2026. The dataset must support supervised training, validation during development, and ongoing evaluation in production." This level of detail ensures that everyone understands the context and prevents the dataset from being built for a use case that no longer exists.

The target distribution defines what the dataset should cover. It specifies the population, the input types, the edge cases, and the proportions. For example: "The dataset must represent the distribution of invoices submitted by small business customers in the United States. Target coverage: seventy percent single-page invoices, twenty percent multi-page invoices, ten percent invoices with handwritten annotations. Target vendor diversity: at least five hundred unique vendors with no single vendor exceeding five percent of examples. Target document quality: sixty percent high-quality scans, thirty percent medium-quality scans with minor artifacts, ten percent low-quality scans with significant noise or skew. Geographic coverage: proportional to customer base, with at least ten percent representation from each of the four major US census regions." This section makes implicit assumptions explicit. If the model needs to handle handwritten annotations, the dataset must include them. If the model needs to generalize across vendors, the dataset must include sufficient vendor diversity.

The size requirements specify how many examples are needed, broken down by category if relevant. For example: "Minimum dataset size: five thousand labeled invoices. Target size: ten thousand labeled invoices. Distribution: at least five hundred examples per label category (vendor_name, invoice_number, line_items, total_amount). At least one thousand examples of multi-page invoices. At least five hundred examples of low-quality scans. At least two hundred examples of invoices with handwritten annotations." Size requirements are not arbitrary. They are derived from model requirements, evaluation requirements, and the need to support train-validation-test splits with sufficient statistical power. Undersized datasets lead to overfitting and poor generalization. Oversized datasets waste money and time.

The quality thresholds define the minimum acceptable quality for the dataset as a whole and for individual examples. For example: "Labeling accuracy: minimum ninety-five percent agreement with expert review on a ten percent holdout sample. Schema compliance: one hundred percent of examples must conform to the defined schema with no missing required fields. Data completeness: zero tolerance for examples with corrupted files, unreadable text, or missing ground truth. Noise tolerance: no more than two percent of examples should require manual correction after automated preprocessing." Quality thresholds make acceptance criteria measurable. Without them, quality becomes subjective, and disagreements about whether the dataset is ready become unresolvable.

The format and schema section specifies the technical structure of the dataset. It defines the file format, the encoding, the directory structure, the naming conventions, and the schema for each record. For example: "Dataset stored as JSONL with one invoice per line. Each record contains the following fields: invoice_id (string, unique identifier), file_path (string, relative path to PDF), page_count (integer), vendor_name (string, ground truth label), invoice_number (string, ground truth label), line_items (array of objects, each containing item_description, quantity, unit_price, total_price), total_amount (float, ground truth label), scan_quality (string, one of: high, medium, low), has_handwritten_annotations (boolean), collection_date (ISO 8601 datetime), source (string, anonymized customer identifier). All currency values in USD. All dates in UTC." This level of detail ensures that the dataset can be ingested, processed, and versioned without ambiguity.

The labeling instructions section defines how examples should be labeled. It specifies the labeling interface, the guidelines, the quality assurance process, and the expected output. For example: "Labeling performed using internal annotation tool version 3.2. Labelers extract vendor_name from the header section, invoice_number from the top-right or header section, line_items from the itemized list, and total_amount from the summary section. If a field is not present or illegible, labelers mark it as null. Each invoice is labeled by two independent labelers. Disagreements are resolved by a senior labeler. Labelers are provided with a style guide and a set of fifty reference examples with gold-standard labels. Labeling accuracy is spot-checked on a rotating ten percent sample reviewed by the dataset owner." This section ensures consistency across labelers and makes it possible to audit labeling quality.

The acceptance criteria section defines the conditions under which the dataset is considered complete and ready for use. It is a checklist of requirements that must be met before the dataset is delivered. For example: "Dataset is accepted if and only if: it contains at least five thousand labeled invoices meeting the target distribution requirements, labeling accuracy on the holdout sample exceeds ninety-five percent, schema compliance is one hundred percent, all required fields are populated with valid values, no corrupted or unreadable files are present, the dataset card is complete and reviewed, and the dataset has been approved by the data science lead, the product manager, and the legal team." Acceptance criteria prevent premature handoff. If the dataset does not meet the criteria, it is not delivered, and the collection or labeling process continues until it does.

## Optional Fields That Add Rigor

The required fields define the minimum viable dataset spec. The optional fields add rigor, accountability, and constraints that improve the likelihood of success. Not every dataset needs every optional field, but including them makes planning more concrete and reduces the risk of surprises during collection.

The cost budget field specifies the maximum amount of money that can be spent on dataset collection and labeling. For example: "Maximum budget: fifty thousand dollars, including vendor fees, labeling costs, and data acquisition. Target cost per labeled example: five dollars. If cost exceeds budget, reduce dataset size proportionally or negotiate lower labeling rates." Cost budgets force tradeoffs between dataset size, quality, and coverage. They prevent runaway spending and ensure that the dataset is built within organizational constraints.

The timeline field specifies the deadline for dataset delivery and the key milestones along the way. For example: "Dataset collection begins March 1, 2026. Labeling begins March 15, 2026. First quality review completed April 1, 2026. Final dataset delivered April 30, 2026. Milestones: five hundred examples labeled by March 22, two thousand examples labeled by April 5, five thousand examples labeled by April 25." Timelines make it possible to track progress, identify delays early, and adjust scope or resources if the schedule slips.

The source priority field specifies which data sources should be prioritized if multiple sources are available. For example: "Primary source: production invoice submissions from small business customers who opted in to data use for model training. Secondary source: synthetic invoices generated using template library. Tertiary source: publicly available invoice datasets with compatible schema. Prefer production data over synthetic data. Use public data only if production and synthetic sources are insufficient." Source priority ensures that the dataset is built from the most representative and highest-quality sources first, with fallback options if those sources are unavailable.

The diversity requirements field specifies the minimum representation for demographic, geographic, or categorical dimensions. For example: "At least fifteen percent of invoices must come from minority-owned businesses as identified by customer self-reporting. At least ten percent of invoices must come from rural areas as defined by USDA rural-urban classification. At least five percent of invoices must be in Spanish or bilingual. No single industry category may exceed twenty percent of the dataset." Diversity requirements ensure that the dataset does not overfit to a dominant demographic or category and that the model generalizes across populations.

Optional fields also include risk mitigation plans, data retention policies, and privacy review checkpoints. These fields are particularly important for datasets that contain sensitive information, personally identifiable information, or data subject to regulatory requirements. For example: "All personally identifiable information must be redacted prior to labeling. Labelers must sign confidentiality agreements. Data stored in encrypted S3 buckets with access restricted to named personnel. Dataset subject to quarterly privacy review. Dataset must be deleted or anonymized within twenty-four months of collection unless explicit consent for longer retention is obtained." These fields ensure that dataset collection complies with legal and ethical requirements from the start, rather than as an afterthought.

## How Specs Prevent Scope Creep and Misaligned Expectations

Scope creep is the silent killer of dataset projects. It begins when someone says, "While we are at it, can we also collect examples of receipts?" or "Can we add a field for tax identification number?" or "Can we expand coverage to include Canada and Mexico?" Each request seems reasonable in isolation, but collectively they double the time, cost, and complexity of the project. Scope creep happens when there is no written spec to point to, or when the spec is treated as a suggestion rather than a contract.

A dataset spec prevents scope creep by making the requirements explicit and version-controlled. When a new requirement is proposed, the first question is: "Is this in the spec?" If the answer is no, the requirement is evaluated against the timeline, budget, and priorities. If it is critical, the spec is updated, and the impact on cost and timeline is assessed. If it is not critical, it is deferred to a future version of the dataset. The spec provides a neutral arbiter that prevents every stakeholder from adding their wishlist items without accountability.

Misaligned expectations are equally damaging. The data science team expects ten thousand labeled examples by the end of the month. The labeling team expects six weeks to complete labeling at the required quality level. The product team expects the dataset to cover edge cases that were never discussed. Legal expects full anonymization, which was not budgeted. Each team has a different mental model of what the dataset is and when it will be ready. Without a spec, these misalignments are discovered late, after time and money have been spent in the wrong direction.

A dataset spec aligns expectations by requiring all stakeholders to review and approve the requirements before work begins. The review process surfaces disagreements, clarifies ambiguities, and forces tradeoffs to be made explicitly. For example, if the data science team wants ten thousand examples and the labeling team can only deliver five thousand within the timeline, the spec review forces the team to choose: delay the timeline, reduce the size requirement, or increase the labeling budget. The decision is made once, documented, and not revisited unless circumstances change.

Version-controlled specs also create accountability. If requirements change, the spec is updated, and the version history shows who requested the change and when. This prevents retroactive blame when a dataset does not meet expectations that were never written down. The spec is the record of what was agreed to, and it protects both the team building the dataset and the team using it.

## The Review and Approval Process for Dataset Specs

A dataset spec is not written in isolation. It is a collaborative document that reflects the needs and constraints of multiple stakeholders. The review and approval process ensures that all stakeholders have input, that conflicts are resolved before collection begins, and that the final spec is realistic and achievable.

The process begins with a draft spec written by the person or team responsible for the dataset, typically a data scientist or data engineer. The draft includes all required fields and as many optional fields as are relevant. It is circulated to stakeholders for review: the model owner, the product manager, the engineering lead, the legal team, and the labeling team if labeling is involved. Each stakeholder reviews the spec from their perspective and provides feedback.

The model owner reviews the purpose, target distribution, size requirements, and quality thresholds. They ensure that the dataset will support the model's training and evaluation needs. They flag any gaps in coverage or edge cases that are missing. The product manager reviews the purpose, target distribution, and timeline. They ensure that the dataset aligns with product priorities and that the timeline supports the product roadmap. The engineering lead reviews the format, schema, and acceptance criteria. They ensure that the dataset can be ingested, processed, and versioned by the existing infrastructure. The legal team reviews the source, labeling instructions, and privacy requirements. They ensure that the dataset complies with privacy regulations, consent requirements, and data retention policies. The labeling team reviews the labeling instructions, size requirements, and timeline. They ensure that the labeling workload is achievable within the timeline and budget.

Feedback is consolidated, conflicts are resolved, and the spec is revised. For example, if the model owner wants ten thousand examples but the labeling team can only deliver seven thousand within the timeline, the team discusses the tradeoff. They might decide to extend the timeline, reduce the size requirement, or increase the labeling budget. The decision is documented in the spec. If the legal team flags a privacy concern with the proposed data source, the team identifies an alternative source or adjusts the anonymization process. The spec is updated to reflect the resolution.

Once all stakeholders have approved the spec, it is finalized and version-controlled. The approved spec is stored in a shared repository, typically alongside the dataset itself or in a documentation system. The spec is treated as the contract for the dataset project. Changes to the spec after approval require the same review and approval process, ensuring that scope changes are deliberate and transparent.

## Version-Controlled Specs That Evolve with the Product

Products evolve, and datasets must evolve with them. A dataset that was fit for purpose in March 2026 may be inadequate by September 2026 if the product has expanded to new markets, added new features, or changed its user base. Version-controlled dataset specs make it possible to track how dataset requirements have changed over time and to maintain alignment between datasets and products.

Each version of a dataset spec corresponds to a version of the dataset. When a new version of the dataset is planned, a new version of the spec is created. The new spec documents what is changing and why. For example: "Version 2.0 spec: Expand geographic coverage to include Canada and Mexico. Add support for invoices in Spanish and French. Increase dataset size from five thousand to eight thousand examples to maintain statistical power across new regions. Update labeling instructions to include currency conversion guidelines for CAD and MXN. Update acceptance criteria to require at least fifteen percent representation from each new region." The version history shows the evolution of the dataset and the rationale for each change.

Version-controlled specs also make it possible to reproduce past datasets. If a model was trained on dataset version 1.0, the spec for version 1.0 documents exactly what that dataset contained, how it was collected, and what quality thresholds it met. If the model needs to be retrained or if evaluation results need to be reproduced, the spec provides the blueprint. Without version control, you lose the ability to reconstruct what the dataset was at a specific point in time.

Version control also supports rollback. If a new version of a dataset introduces unforeseen issues, you can roll back to the previous version and the corresponding spec. For example, if version 2.0 of a dataset introduces label noise due to updated labeling instructions, you can revert to version 1.0 while the labeling process is debugged. The spec for version 1.0 documents the process that produced the stable dataset, making rollback safe and reproducible.

Dataset specs should be stored in the same version control system as code and model configurations. This ensures that specs are subject to the same change management, review, and audit processes as other critical artifacts. Specs are not disposable planning documents. They are durable records of intent and requirements that live as long as the datasets they describe.

## How Specs Connect to Evaluation Requirements

The relationship between dataset specs and evaluation requirements is direct. The dataset you collect determines what you can measure, and what you need to measure determines what you need to collect. A dataset spec that is not aligned with evaluation requirements will produce a dataset that cannot support the evaluations you need to run.

Evaluation requirements define the metrics, test cases, and coverage dimensions that determine whether a model is ready to ship. For example, an evaluation plan might specify: "The invoice extraction model must achieve ninety-five percent precision and ninety percent recall on vendor name extraction, ninety percent precision and eighty-five percent recall on line item extraction, and ninety-eight percent precision and ninety-five percent recall on total amount extraction. Metrics must be reported separately for single-page and multi-page invoices, for high-quality and low-quality scans, and for invoices with and without handwritten annotations." These evaluation requirements directly inform the dataset spec. The dataset must include sufficient examples of each category to compute metrics with statistical confidence. It must include representative distributions of document quality and complexity. It must include ground truth labels that match the evaluation metrics.

If the evaluation plan requires performance breakdowns by geographic region, the dataset spec must include geographic diversity requirements. If the evaluation plan requires stress testing on rare edge cases, the dataset spec must include minimum representation for those edge cases. If the evaluation plan requires tracking performance over time, the dataset spec must include a plan for periodic data collection to create longitudinal test sets.

Aligning dataset specs with evaluation requirements prevents the common failure mode where a dataset is collected, a model is trained, and only then does the team realize that they cannot compute the metrics they need because the dataset lacks the necessary coverage or labels. By defining evaluation requirements first and deriving dataset requirements from them, you ensure that the data supports the measurements that matter.

The dataset spec should reference the evaluation plan explicitly. For example: "This dataset supports the evaluation plan documented in eval-plan-invoice-v2.md. It provides sufficient examples to compute precision and recall for each extraction field with a confidence interval of plus or minus three percentage points. It includes stratified subsets for single-page versus multi-page invoices, high-quality versus low-quality scans, and invoices with versus without handwritten annotations, as required by the evaluation plan." This cross-reference ensures that the dataset and the evaluation plan remain aligned as both evolve.

Dataset specs are the bridge between intent and execution. They capture what you need before you build it, they align stakeholders before disagreements become expensive, and they create accountability for delivering datasets that meet defined requirements. A dataset card documents what exists. A dataset spec ensures that what you build is what you need. The next step is to be explicit about the boundaries of that dataset: what it is intended for and what it cannot do.
