# 7.6 â€” Data Mixing: Combining Tasks, Domains, and Difficulty Levels

Three tasks, 43,000 training examples, one model. Task A has 15,000 examples, Task B has 8,000 examples, Task C has 20,000 examples. You shuffle them together and train. The model achieves 87 percent on Task A, 79 percent on Task B, and 92 percent on Task C. Task C dominates because it represents 46 percent of the training data, so most gradient updates optimize for Task C performance. Tasks A and B underfit because they contribute less to the loss. You have three choices: collect more data for Tasks A and B until they match Task C in volume, accept the performance imbalance, or rebalance the mixing ratios so each task contributes equally to training regardless of raw example counts. The third option costs nothing and often works better than collecting more data, because the problem is not data scarcity but gradient allocation.

Data mixing is the process of combining training examples from multiple tasks, domains, or difficulty levels into a unified training dataset with deliberate sampling ratios that align the gradient signal with your performance priorities. It is not a detail. It is a first-order architectural decision in multi-task fine-tuning and domain-mixed training. In 2026, most production models serve multiple use cases, handle multiple domains, and must perform well across a range of difficulty levels. Random mixing treats all examples equally, which means high-frequency tasks and easy examples dominate the training signal. Deliberate mixing gives you control over what the model optimizes for, allowing you to balance performance across tasks, prevent catastrophic forgetting, and ensure that hard examples receive enough gradient signal to be learned. Your job is to define mixing ratios based on your performance requirements, test those ratios empirically, and adjust them dynamically during training if necessary.

## Why Multi-Task Mixing Matters for General Capability

Multi-task training is the practice of training a single model on multiple related tasks simultaneously. The advantage is parameter sharing: the model learns representations that generalize across tasks, improving sample efficiency and reducing the total amount of task-specific data required. The disadvantage is task interference: the gradient signals from different tasks can conflict, causing the model to underfit on some tasks while overfitting on others. Data mixing is the mechanism that controls this tradeoff.

When you train a model on multiple tasks with random mixing, each task contributes to the gradient in proportion to its dataset size. If task A has 20,000 examples and task B has 2,000 examples, task A contributes ten times as much gradient signal as task B. The model optimizes primarily for task A, and task B becomes an afterthought. If your performance requirements are the same for both tasks, this is unacceptable. You need to rebalance the mix so that task B receives equal or greater weight despite having fewer examples.

The simplest rebalancing strategy is equal sampling, where you sample the same number of examples from each task in each training batch regardless of the task's dataset size. If task A has 20,000 examples and task B has 2,000 examples, you sample 1,000 examples from each task per epoch, upsampling task B by repeating its examples. This ensures that both tasks contribute equally to the gradient signal. The model optimizes for both tasks with equal priority. The cost is that task B examples are seen more frequently, which can lead to overfitting on task B if the dataset is small or low-diversity.

A more sophisticated rebalancing strategy is proportional sampling with a temperature parameter, where you sample from each task according to a weighted probability that is not directly proportional to dataset size. You compute the sampling probability for task i as the dataset size raised to the power of a temperature parameter, then normalize across all tasks. A temperature of 1.0 gives you proportional sampling. A temperature of 0.5 gives you square-root sampling, which upweights smaller tasks. A temperature of 0.0 gives you equal sampling. You tune the temperature based on your performance requirements and the relative importance of each task.

Multi-task mixing also affects transfer learning. Tasks that are closely related can share useful representations, improving performance on both tasks. Tasks that are unrelated or conflicting can interfere with each other, degrading performance on both tasks. You must assess task relatedness before deciding on mixing ratios. If two tasks share similar input structures and decision boundaries, mix them aggressively to leverage positive transfer. If two tasks are unrelated, consider training separate models or using domain tags to partition the model's capacity.

## Mixing Ratios: How to Weight Different Data Sources

Mixing ratios are the proportions in which you sample examples from different data sources during training. If you have three tasks with datasets of size 10,000, 5,000, and 15,000, a proportional mixing ratio is 33%, 17%, and 50%. An equal mixing ratio is 33%, 33%, and 33%. A priority-weighted mixing ratio might be 50%, 30%, and 20% if you care most about the first task. The mixing ratio determines what the model optimizes for, and it is one of the most important hyperparameters in multi-task training.

You set mixing ratios based on three factors: performance requirements, dataset size, and task difficulty. If all tasks have the same performance requirements, use equal mixing or square-root mixing to give smaller tasks more weight. If one task has higher performance requirements than the others, increase its mixing ratio to give it more gradient signal. If one task is inherently harder than the others, increase its mixing ratio to give the model more opportunities to learn it.

A common mistake is to set mixing ratios once at the beginning of training and never adjust them. Mixing ratios should evolve as the model learns. In early epochs, you might use equal mixing to give all tasks a fair chance to establish foundational representations. In later epochs, you might shift the ratio toward tasks that are underperforming or toward hard examples within each task. Dynamic mixing allows you to respond to the model's learning trajectory and optimize for final performance across all tasks.

You can also use mixing ratios to control the balance between in-domain and out-of-domain data. If you have 20,000 in-domain examples and 50,000 out-of-domain examples from a related task, you might use a 70% in-domain, 30% out-of-domain ratio to ensure that the model prioritizes in-domain performance while still benefiting from the additional out-of-domain signal. If you use a 50-50 ratio, the out-of-domain data will dominate the gradient signal and the model may underfit on the in-domain task. If you use a 90-10 ratio, the out-of-domain data contributes little and the benefit of including it is marginal.

You test mixing ratios empirically by training models with different ratios and measuring per-task performance on held-out eval sets. Start with proportional mixing as a baseline. Then test equal mixing, square-root mixing, and priority-weighted mixing. Compare final performance across all tasks and choose the ratio that meets your performance requirements with the best overall balance. If one task consistently underperforms regardless of mixing ratio, you have a data quality or task difficulty problem, not a mixing problem.

## Domain Balance: Preventing Catastrophic Forgetting of One Domain

Catastrophic forgetting occurs when a model forgets previously learned patterns as it learns new patterns, particularly when the new patterns are presented with high intensity. In multi-domain training, this manifests as the model losing performance on one domain while improving on another. Data mixing is the primary defense against catastrophic forgetting. By ensuring that all domains remain present in the training signal throughout training, you prevent the model from overwriting domain-specific representations.

If you train a model on domain A for two epochs, then switch to domain B for two epochs, the model will forget domain A as it adapts to domain B. The gradients from domain B push parameters away from the values that were optimal for domain A. By the end of training, the model performs well on domain B but poorly on domain A. This is pure catastrophic forgetting. You prevent it by mixing domains throughout training. In every batch, include examples from both domain A and domain B. The model continues to receive gradient signal from domain A even while learning domain B, which stabilizes the parameters and prevents forgetting.

The mixing ratio between domains determines the degree of forgetting. If you mix domain A and domain B equally, neither domain will be forgotten. If you mix them in a 10-90 ratio favoring domain B, domain A will experience some forgetting because the gradient signal is weak. If you use sequential training with no mixing, forgetting is severe. You must choose a mixing ratio that balances your performance requirements across domains.

One technique for managing domain balance is to use a replay buffer, where you store a subset of examples from earlier domains and continue to sample from them in later epochs even as you introduce new domains. For example, if you train on domain A in epoch one, you might store 1,000 examples from domain A in a replay buffer. In epoch two, when you introduce domain B, you sample 80% from domain B and 20% from the replay buffer. This ensures that domain A remains in the training signal without requiring you to keep the full domain A dataset in every batch. Replay is particularly useful when you have many domains and limited batch size.

Another technique is to use domain-specific learning rates, where you apply a lower learning rate to parameters associated with earlier domains and a higher learning rate to parameters associated with new domains. This reduces the magnitude of gradient updates for earlier domains, making them more resistant to forgetting. This technique requires architectural support, such as domain-specific adapter layers, and is more complex to implement than simple data mixing. Use it only if standard mixing strategies fail to prevent forgetting.

## Difficulty Distribution: The Right Balance of Easy, Medium, Hard

Within a single task or domain, training examples vary in difficulty. Easy examples are those the model can learn quickly with low loss. Hard examples are those the model struggles with even after many epochs. The distribution of easy, medium, and hard examples in your training data affects convergence speed, final performance, and generalization. Too many easy examples and the model overfits to simple patterns without learning the nuance required for hard cases. Too many hard examples and the model struggles to converge because the gradient signal is noisy and unstable.

The optimal difficulty distribution depends on the task and the stage of training. In early epochs, bias toward easy and medium examples to establish foundational patterns. In later epochs, shift toward hard examples to refine the decision boundaries and improve performance on edge cases. A common distribution for the first epoch is 50% easy, 40% medium, 10% hard. For the second epoch, shift to 30% easy, 40% medium, 30% hard. For the third epoch, shift to 20% easy, 30% medium, 50% hard. By the final epoch, the model is spending most of its capacity on the hardest examples, which is where the marginal performance gains are largest.

You can also use difficulty-based upsampling, where you sample hard examples more frequently than easy examples to give them more gradient signal. This is the inverse of standard curriculum learning, where easy examples are presented first. Difficulty-based upsampling is appropriate when you have already trained the model on easy examples and you want to focus the remaining training budget on improving performance on hard examples. You upsample hard examples by repeating them or by adjusting the sampling probability to favor them.

Another approach is to use a difficulty-aware loss function, where you weight the loss for each example by its difficulty. Easy examples receive low weight, and hard examples receive high weight. This ensures that the gradient signal is proportional to difficulty, not frequency. The challenge is calibrating the weights. If you weight hard examples too heavily, the gradient signal becomes noisy and training destabilizes. If you weight them too lightly, the effect is negligible. A common weighting scheme is to use the inverse of model confidence: examples where the model is highly confident receive low weight, and examples where the model is uncertain receive high weight.

You measure the difficulty distribution in your training data by computing per-example loss on a validation set after each epoch. Bin the examples into easy, medium, and hard based on loss percentiles. Track the proportion of examples in each bin over time. If the proportion of hard examples is decreasing, the model is learning them and they are becoming easier. If the proportion is not decreasing, the model is not learning them and you need to adjust the mixing ratio or the training strategy.

## Dynamic Mixing: Adjusting Ratios During Training Based on Loss Signals

Static mixing ratios are set once at the beginning of training and remain fixed throughout. Dynamic mixing ratios are adjusted during training based on observed model performance. Dynamic mixing is more complex to implement but can deliver better final performance by adapting the training signal to the model's changing needs.

The simplest dynamic mixing strategy is to increase the sampling weight for tasks or domains where validation loss is high and decrease the weight for tasks where validation loss is low. After each epoch, you evaluate the model on a validation set and compute per-task loss. If task A has a loss of 0.5 and task B has a loss of 1.2, you increase the sampling ratio for task B in the next epoch to give it more gradient signal. You decrease the ratio for task A because it is already performing well. This adaptive approach ensures that the model focuses its learning capacity on the tasks that need it most.

A more sophisticated dynamic mixing strategy is to use a loss-based temperature schedule, where the mixing temperature decreases over time to shift from exploration to exploitation. In early epochs, you use a high temperature to give all tasks equal weight and encourage exploration. In later epochs, you decrease the temperature to focus on tasks with the highest loss and exploit the model's capacity where it is most needed. The temperature schedule is a hyperparameter you tune based on task diversity and performance requirements.

You can also use dynamic mixing to implement automatic curriculum learning, where the model controls the difficulty distribution based on its own performance. After each epoch, you compute the model's accuracy on easy, medium, and hard examples. If accuracy on hard examples is below a threshold, you increase the proportion of hard examples in the next epoch. If accuracy on easy examples is degrading, you reintroduce more easy examples to prevent forgetting. This closed-loop approach allows the training process to self-regulate and adapt to the model's learning trajectory.

Dynamic mixing requires infrastructure to compute per-task and per-difficulty metrics after each epoch and to regenerate training files with adjusted sampling ratios. This infrastructure is nontrivial to build and maintain. Use dynamic mixing only if static mixing strategies fail to meet your performance requirements or if you are training on a large, diverse dataset where the optimal mixing ratio is not obvious from the start.

## Common Mixing Mistakes and How to Diagnose Them

The most common mixing mistake is proportional sampling without considering task difficulty or performance requirements. If task A has 50,000 examples and task B has 5,000 examples, proportional sampling gives task A 90% of the gradient signal and task B 10%. If both tasks have the same performance requirements, task B will underperform. You diagnose this by evaluating per-task performance on a validation set. If one task consistently underperforms, check the mixing ratio and rebalance.

Another common mistake is equal sampling for tasks with very different dataset sizes. If task A has 50,000 examples and task B has 500 examples, equal sampling forces you to upsample task B by 100x, repeating its examples many times. The model overfits to task B because it sees the same examples repeatedly. You diagnose this by measuring per-task loss over epochs. If task B shows decreasing training loss but flat or increasing validation loss, you are overfitting. The solution is to reduce the upsampling factor or to collect more data for task B.

A third mistake is static mixing ratios for tasks with different learning rates. Some tasks are learned quickly, while others require many epochs to converge. If you use the same mixing ratio throughout training, you waste gradient signal on tasks that have already converged and underallocate signal to tasks that are still learning. You diagnose this by tracking per-task loss curves. If one task's loss plateaus early while another task's loss is still decreasing, adjust the mixing ratio to shift toward the slower-learning task.

A fourth mistake is mixing conflicting tasks without domain tags or task-specific layers. If two tasks require different decision boundaries or conflicting representations, mixing them in the same training batch can cause interference. The model attempts to learn both tasks simultaneously and ends up learning neither well. You diagnose this by comparing multi-task performance to single-task performance. If single-task models outperform the multi-task model by a large margin, the tasks are interfering. The solution is to use task-specific adapter layers, domain tags, or separate models.

## Data Mixing as Strategic Optimization

Data mixing is not a one-time configuration. It is a strategic optimization process that you revisit throughout training. You start with a baseline mixing ratio based on dataset sizes and performance requirements. You train a model and evaluate per-task performance. You adjust the mixing ratio based on observed performance. You retrain and reevaluate. You iterate until you find the mixing strategy that meets your requirements.

The discipline of data mixing is recognizing that not all training examples are created equal. Some examples contribute more to performance than others. Some tasks require more gradient signal than others. Some difficulty levels require more attention than others. Your job is to allocate the training budget across examples, tasks, and difficulty levels in a way that maximizes overall performance. Random mixing is the default, but it is rarely optimal. Deliberate mixing is more work, but it delivers better models.

In 2026, the tooling for data mixing is mature. Frameworks like Hugging Face Transformers support weighted sampling and dynamic batch construction. OpenAI's fine-tuning API allows you to specify mixing ratios via dataset composition. You have no excuse for ignoring mixing strategy. Test it, measure it, and optimize it. The payoff is models that perform well across all tasks, all domains, and all difficulty levels, not just the easy cases.

Once you have determined the size, order, and mix of your training data, the final challenge is to ensure that the data you have collected actually covers the distribution you will encounter in production. That is the problem of distribution coverage, and it determines whether your fine-tuned model generalizes or fails on the first real-world input it has never seen before.
