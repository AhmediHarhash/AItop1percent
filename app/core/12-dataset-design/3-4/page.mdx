# 3.4 â€” Controlling Diversity and Distribution in Synthetic Sets

In mid-2025, a legal technology company launched a contract analysis tool trained on 40,000 synthetically generated business agreements. The model performed well in early testing, achieving 91% accuracy on standard commercial contracts. But within three weeks of deployment across their enterprise client base, the system began failing catastrophically on contracts from smaller vendors, international partners, and non-standard deal structures.

The pattern was clear: the synthetic training data had converged on the most common contract types, missing entire categories of legitimate variation. The company spent $1.8 million over five months rebuilding their dataset with proper diversity controls, while their market position eroded to competitors who had avoided the convergence trap.

The root cause was not a failure of synthetic generation itself. The failure was treating synthetic data as a generic output process rather than a distribution engineering problem. When you generate synthetic examples without explicit diversity controls, language models naturally drift toward the statistical mode of their training distribution.

They produce the most probable examples, the most common patterns, the safest variations. This mode collapse creates training sets that appear superficially diverse but are actually clustered tightly around a narrow range of the possible space.

Your model learns to handle the common case extremely well and fails completely on everything else. Controlling diversity and distribution in synthetic sets is not an optimization problem. It is a foundational requirement for any synthetic data strategy that aims to produce production-ready systems.

## The Mode Collapse Problem in Synthetic Generation

Synthetic data generation through language models exhibits a natural tendency toward statistical centrality. When you prompt a model to generate examples without explicit diversity constraints, it samples from the high-probability regions of its learned distribution.

For contract generation, this means standard boilerplate, common clause structures, and typical term ranges. For customer support conversations, it means the most frequent issue types, the most common tone patterns, and the most standard resolutions. For medical documentation, it means the most prevalent conditions, the most typical symptom presentations, and the most common treatment paths.

This convergence happens even when you vary the generation prompts. Asking for "diverse" examples or "varied" scenarios does not overcome the fundamental statistical bias. The model's concept of diversity is itself bounded by the high-probability regions of its training data.

You get variation within the mode, not variation across the full distribution. A synthetic dataset of 10,000 customer support tickets might contain 50 different surface-level phrasings but only represent five underlying issue patterns.

A synthetic dataset of medical notes might include hundreds of patients but only a dozen distinct clinical presentations. The surface diversity masks the underlying convergence.

## Impact on Model Training

The impact on model training is severe and non-obvious. When you train a model on mode-collapsed synthetic data, validation metrics often look acceptable because your validation set likely shares the same mode bias. The model achieves high accuracy on the common cases that dominate both training and validation.

The failure only becomes visible in production, where the real distribution includes all the tail cases, edge patterns, and unusual combinations that were absent from your synthetic set. Your model has been trained to be extremely confident about a narrow slice of the problem space and has no learned behavior for everything else.

Detection requires measuring the actual distribution of your synthetic data against the known or estimated distribution of production data. This is not a single metric. You need to measure diversity across multiple dimensions simultaneously.

You need lexical diversity at the token level, structural diversity in example composition, semantic diversity in meaning space, and distributional diversity across known categorical and continuous variables. A synthetic dataset can score well on lexical diversity while having collapsed semantic diversity. It can show structural variation while missing critical distributional coverage.

## Measuring Diversity Across Dimensions

Lexical diversity measures the range and distribution of vocabulary used in your synthetic examples. The simplest metric is type-token ratio: the number of unique tokens divided by the total token count.

But this metric is misleadingly optimistic for synthetic data because language models naturally use varied vocabulary even when expressing the same underlying concepts. A more informative metric is n-gram diversity: measuring the distribution of bigrams, trigrams, and longer sequences.

Mode-collapsed synthetic data shows low n-gram diversity even when type-token ratio looks acceptable. You see the same phrases, the same sentence structures, and the same transition patterns repeated with only surface-level lexical substitution.

The repetition is not exact duplication. It is statistical clustering where the same patterns appear with minor variations. Real human-authored text shows much greater n-gram diversity because different authors have different stylistic preferences and phrasings.

## Structural and Semantic Diversity

Structural diversity measures variation in the compositional patterns of your examples. For text data, this includes sentence count distribution, paragraph structure patterns, and document organization. For conversational data, it includes turn-taking patterns, conversation length distribution, and topic transition structures.

For structured outputs like JSON or database records, it includes field presence patterns, nesting depth, and relationship cardinality. Measuring structural diversity requires defining structural features for your domain and then measuring the entropy of those features across your synthetic set.

High structural diversity means your synthetic data covers the full range of compositional patterns seen in production. Low structural diversity means your synthetic examples all follow the same template with only content variation.

Semantic diversity measures variation in the actual meaning and content of your examples, independent of surface form. This is the hardest dimension to measure but the most important for training robustness.

One approach is embedding-based diversity: generate embeddings for all synthetic examples using a strong language model, then measure the distribution of those embeddings in semantic space. Collapsed synthetic data clusters tightly in embedding space even when lexical and structural metrics look diverse.

You can measure this clustering using metrics like average pairwise cosine distance, coverage of semantic space using volume estimates, or entropy of cluster assignments when you partition the embedding space. A well-distributed synthetic dataset shows broad coverage with no dense clusters.

## Distributional Diversity

Distributional diversity measures how well your synthetic data matches the known or estimated distribution of production data across categorical and continuous variables. For categorical variables like contract type, customer segment, or medical condition, you can directly compare the frequency distribution in synthetic versus production data.

Use metrics like KL divergence or Jensen-Shannon distance to quantify the difference. Low divergence means your synthetic distribution closely matches production. High divergence means you are over-representing some categories and under-representing others.

For continuous variables like dollar amounts, response times, or patient ages, you compare the full distributional shape using tests like Kolmogorov-Smirnov or by comparing quantiles across the distribution. Real distributions often have skew, multiple modes, or heavy tails that synthetic generation might miss.

Low distributional diversity means your synthetic data is missing entire categories or has incorrect probability mass in continuous ranges. The legal tech company's synthetic contracts matched production frequency for the five most common contract types but completely missed fifteen less common types that represented 20% of production volume.

You need all four dimensions. A synthetic dataset that scores well on lexical and structural diversity but poorly on semantic and distributional diversity will produce models that appear to perform well on surface metrics but fail on the actual task.

## Distribution Matching Techniques

The most direct approach to controlling distribution is stratified generation with explicit quotas. You define the target distribution across your key categorical variables based on production data or domain requirements, then generate synthetic examples in proportions that match that target.

For a customer support dataset, you might specify that 30% of examples should be billing issues, 25% technical problems, 20% account management, 15% product questions, and 10% edge cases. You then generate examples in each category until you hit the quota, ensuring your synthetic distribution matches your target.

Stratification requires defining the strata carefully. The categories must be mutually exclusive, collectively exhaustive, and aligned with the actual decision boundaries your model needs to learn. Poorly chosen strata create artificial boundaries that do not reflect production complexity.

The legal tech company initially stratified only by contract type but did not stratify by deal size, party sophistication, or jurisdictional variation. This gave them correct proportions of contract types but incorrect distributions within each type. Their synthetic contracts for each type were all clustered around median deal sizes with standard clauses.

## Sampling from Target Distributions

For continuous variables, distribution matching requires sampling from target distributions rather than letting the model generate freely. If you know that contract values in production follow a log-normal distribution with specific parameters, you sample contract values from that distribution and then condition your synthetic generation on those values.

If you know that customer support response times follow a bimodal distribution with peaks at immediate responses and delayed escalations, you sample response times from that bimodal distribution and generate conversations that reflect those timings.

The conditioning mechanism matters. Simply including the target value in the generation prompt is not sufficient because language models do not reliably honor numerical constraints in prompts. You need verification and rejection: generate a synthetic example conditioned on the target value, parse the generated example to extract the actual value, and reject examples where the extracted value deviates from the target beyond acceptable tolerance.

This rejection rate tells you how well your generation process respects distributional constraints. High rejection rates indicate your prompts are not effectively constraining the model. You need to revise your prompts or add post-generation filtering to enforce the constraints.

## Temperature and Sampling Parameters

Temperature and sampling parameters affect distributional control. Lower temperature makes generation more deterministic and increases mode collapse. The model always selects the highest-probability continuation, which means always generating the most typical example.

Higher temperature increases variation but can produce incoherent or invalid examples. The model samples from lower-probability continuations, which introduces diversity but also introduces errors and nonsense.

The optimal temperature depends on your task and your diversity goals. For structured data generation, you typically want lower temperature with explicit stratification. The structure constraints prevent mode collapse, and low temperature ensures coherence.

For unstructured text generation, you want moderate temperature with diversity-promoting sampling like nucleus sampling or top-k sampling. These sampling methods bound randomness to prevent nonsense while still allowing variation.

But sampling parameters alone cannot overcome fundamental distributional bias in your prompts and conditioning strategy. If your prompts always ask for standard examples, high temperature will produce noisy standard examples, not diverse examples.

## Adversarial Prompting for Tail Coverage

The tail of your distribution contains the cases that are rare in production but critical for robustness. These are not errors or noise. They are legitimate examples that occur infrequently but matter when they do occur.

A customer support system must handle the rare customer who has experienced three unrelated issues in sequence. A contract analysis system must handle the unusual deal structure where payment terms are tied to performance milestones. A medical coding system must handle the patient presenting with symptom combinations that suggest multiple concurrent conditions.

Standard synthetic generation misses the tail because language models are trained to maximize probability, which means generating likely examples. Adversarial prompting inverts this bias by explicitly requesting unlikely but valid examples.

Instead of prompting for "a customer support conversation about a billing issue," you prompt for "a customer support conversation where the customer has been incorrectly billed three times in three different ways and has escalated to a supervisor after the first-line agent failed to understand the compound issue." This adversarial prompt pushes generation into the tail.

## Constraints for Adversarial Prompts

Adversarial prompting requires domain expertise to distinguish between rare-but-valid and invalid. Asking for "unlikely" examples without constraints produces invalid examples that violate domain rules. The prompt must specify what makes the example unusual while maintaining all validity constraints.

For legal contracts, an adversarial prompt might request a contract with unusual payment terms but must still require that those terms are legally enforceable and internally consistent. For medical documentation, an adversarial prompt might request a complex multi-condition presentation but must still require that the symptom combination is clinically plausible.

The proportion of adversarial examples in your synthetic set depends on your robustness requirements and your production tail frequency. If tail cases represent 5% of production volume but 40% of user-reported issues, you want tail cases to be overrepresented in your training data relative to their production frequency.

This is not a distributional matching failure. It is a deliberate choice to weight training data by impact rather than by frequency. Your model learns to handle tail cases with confidence rather than treating them as out-of-distribution surprises.

Validation of adversarial examples requires expert review. You cannot assume that examples generated from adversarial prompts are valid just because they were generated. Language models can produce plausible-sounding tail cases that violate subtle domain constraints.

A contract with unusual payment terms might be syntactically correct but commercially nonsensical. A medical case with rare symptom combinations might be individually valid but collectively impossible. You must have domain experts review adversarial synthetic examples to confirm they represent the legitimate tail rather than model hallucinations.

## Diversity Metrics and Monitoring

The primary diversity metric is coverage: the proportion of your known or estimated production distribution that appears in your synthetic training set. For categorical variables, coverage is straightforward: what percentage of production categories appear at least N times in your synthetic data.

For the customer support example, if production includes 50 distinct issue subcategories, coverage measures how many of those 50 appear in your synthetic set with sufficient frequency to support learning. Full coverage means all 50 appear with at least 20 examples each. Partial coverage means only 30 of 50 appear.

For continuous variables, coverage requires defining meaningful ranges or quantiles. If contract values in production span from $5,000 to $50 million, you might divide this range into deciles and measure how many deciles are represented in your synthetic data.

Full coverage means all ten deciles appear with sufficient examples. Partial coverage means only the middle deciles appear, missing both small contracts and large contracts. Collapsed coverage means 80% of synthetic examples fall in a single decile.

## Entropy and Cluster Analysis

Entropy measures the evenness of distribution across categories or bins. Maximum entropy means all categories are equally represented. Minimum entropy means all examples fall in a single category.

Real production distributions are rarely maximum entropy, so you do not want maximum entropy in your synthetic data. You want entropy that matches production. The metric is relative entropy: how much does the entropy of your synthetic distribution differ from the entropy of your production distribution.

Large differences indicate distributional mismatch. If production entropy is 3.2 bits and synthetic entropy is 1.8 bits, your synthetic data is much more concentrated than production.

Cluster analysis in embedding space provides a dimensionality-reduced view of semantic diversity. Generate embeddings for all synthetic examples, cluster them using k-means or hierarchical clustering, and measure cluster properties.

Collapsed synthetic data produces a small number of large, tight clusters. Diverse synthetic data produces many clusters with more even size distribution and greater between-cluster distance. You can compare cluster properties between synthetic and production data to identify semantic gaps.

Monitoring diversity during generation prevents collapse before it accumulates. Instead of generating your entire synthetic dataset in one batch and then measuring diversity, you generate in increments and measure diversity after each increment.

If diversity metrics degrade as generation continues, you adjust your prompts, increase temperature, or switch to more aggressive stratification. The legal tech company generated all 40,000 contracts in a single run and only measured diversity afterward. By then, mode collapse had already occurred and they had to discard most of the dataset.

## Detecting and Correcting Distribution Collapse

Distribution collapse manifests as unexpectedly high model performance on synthetic validation data and unexpectedly low performance on production data. The gap between these two performance measurements is the primary signal.

If your model achieves 94% accuracy on held-out synthetic data but only 76% accuracy on production data, your synthetic distribution has collapsed relative to production. The model has overfit to the narrow mode present in your synthetic set.

Secondary signals include low error diversity in production failures. When your model fails in production, do the failures span a wide range of scenarios or do they cluster tightly around specific underrepresented patterns?

Collapsed synthetic training leads to clustered failures. All the production errors involve small contracts, or non-US jurisdictions, or multi-party agreements. These clusters point to the distributional gaps in your synthetic data.

## Correcting Through Targeted Generation

Correcting collapse requires targeted generation for the identified gaps. You do not regenerate your entire dataset. You identify the specific regions of the distribution that are underrepresented, generate additional synthetic examples specifically for those regions, and augment your existing dataset.

For the legal tech company, this meant generating synthetic contracts specifically for small vendors, international jurisdictions, non-standard payment terms, and multi-party structures. The augmentation brought distributional coverage from 60% to 93% and closed the performance gap between synthetic validation and production testing.

The augmentation ratio depends on gap size and impact. If a missing region represents 2% of production volume but 15% of production failures, you generate enough synthetic examples to bring that region to 10-15% of your training set.

This deliberate oversampling for high-impact regions improves production robustness more than proportional sampling would. You are optimizing for impact-weighted performance, not frequency-weighted performance.

Rebalancing existing synthetic data is an alternative to augmentation when you have sufficient diversity but incorrect proportions. If your synthetic set includes all relevant categories but in the wrong ratios, you can downsample overrepresented categories and upsample underrepresented categories without generating new examples.

This is cheaper than new generation but only works when the underrepresented categories are present at all. If a category is completely missing, downsampling others does not help. You must generate new examples.

## Diversity Control in Multi-Stage Generation

Many synthetic generation workflows use multiple stages: first generating high-level structure, then filling in details, then adding variation. Distribution control must operate at all stages. Collapsing at any stage propagates through the entire pipeline.

If your first stage generates collapsed contract types, the later stages that fill in clauses and terms cannot recover distributional diversity. The structural collapse constrains all downstream generation.

Stage-specific stratification addresses this by applying distributional constraints at each stage independently. At the contract type generation stage, you stratify by contract category, jurisdiction, and party count.

At the clause generation stage, you stratify by clause type, complexity, and conditionality. At the term generation stage, you stratify by value ranges, date ranges, and penalty structures. Each stage respects its own target distribution rather than hoping that early-stage diversity will propagate.

Validation between stages catches collapse early. After the first stage generates high-level structures, you measure the distribution of those structures before proceeding to detail generation.

If the structures show mode collapse, you regenerate with adjusted prompts or stratification rather than continuing through the pipeline. This fail-fast approach prevents wasted compute on detailed generation for collapsed structures.

## Feedback Loops Across Stages

Feedback loops from later stages to earlier stages improve overall diversity. If detail generation fails frequently for certain structure types because those structures are underspecified or invalid, that failure signal should increase the frequency of those structure types in the first stage.

The generation process becomes iterative: generate structures, attempt detail generation, measure success rates, adjust structure generation probabilities, repeat. This adaptive approach converges on structures that support successful downstream generation while maintaining distributional targets.

## Balancing Diversity with Validity

Maximizing diversity can conflict with maintaining validity. As you push generation into less common regions of the distribution, you increase the risk of producing invalid examples that violate domain constraints.

A contract with unusual payment terms might be invalid because those terms create logical contradictions. A customer support conversation with rare issue combinations might be invalid because one issue precludes the other. Diversity without validity produces training data that teaches your model to accept invalid patterns.

The validity-diversity tradeoff requires explicit constraints in your generation prompts. You specify what dimensions should vary and what constraints must hold. For contract generation, you might specify that payment terms can vary widely but must always result in deterministic total amounts and must never create circular dependencies.

For medical documentation, you might specify that symptom combinations can vary but must never include mutually exclusive findings. These constraints bound the diversity space to the valid subset.

## Automated and Expert Validity Checking

Automated validity checking catches violations early. After generating each synthetic example, you run validation rules that check domain constraints. Invalid examples are rejected and regenerated with adjusted prompts.

The rejection rate by validity check tells you how well your diversity goals align with validity requirements. High rejection rates mean you are pushing diversity too far beyond the valid space and need to tighten constraints.

Expert review provides a second layer of validity checking for examples that pass automated rules but may violate subtle constraints. You sample from your synthetic data, especially from the tail and adversarial examples, and have domain experts review for plausibility and correctness.

Expert rejection rate is a separate metric from automated rejection rate. High expert rejection with low automated rejection means your automated rules are insufficient. You need to formalize the subtle constraints that experts use into automated checks.

The balance point between diversity and validity depends on your use case and risk tolerance. For a production system where invalid outputs cause immediate user-facing failures, you want very high validity thresholds and accept narrower diversity.

For an internal tool where invalid outputs are filtered by downstream processes, you can tolerate lower validity and pursue wider diversity. The legal tech company initially prioritized diversity without sufficient validity checking, producing contracts that included clauses that were mutually contradictory.

This created training data that taught the model to generate invalid outputs with high confidence. They had to implement stricter validity rules and reject 35% of generated examples to maintain quality.

Your synthetic data should reflect production distribution not just in frequency but in complexity, coverage across the full range of valid scenarios, and representation of high-impact tail cases. Controlling diversity and distribution is not a post-generation cleanup step. It is a design requirement that shapes your generation strategy from the first prompt.

The next subchapter examines how to generate synthetic data specifically for rare and edge cases, where the value of synthetic generation is highest and the control challenges are most acute.
