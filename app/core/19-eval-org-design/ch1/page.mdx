# Section 19 — Evaluation Organization & Team Design

## Chapter 1

### Plain English

This section answers a critical question:

**"Who owns quality, and how is that ownership structured so it actually works?"**

Because in real organizations:
- tools don't enforce quality
- dashboards don't enforce quality
- metrics don't enforce quality

**People, roles, incentives, and structure do.**

Eval systems fail not because of technology,
but because no one is clearly responsible.

---

### Why Org Design Matters for Evals

Early-stage teams:
- rely on founders
- rely on intuition
- rely on hero engineers

At scale:
- intuition doesn't scale
- founders become bottlenecks
- heroics become outages

Evaluation must become:
- owned
- distributed
- enforceable
- resilient to turnover

That requires organization design.

---

### The Core Principle (2026)

**Quality must have owners, not just advocates.**

If everyone is responsible, no one is.

---

### The Three Eval Ownership Models

In 2026, orgs converge on **three viable models**.

Each has tradeoffs.

---

#### 1) Centralized Evaluation Team

A dedicated team owns:
- eval frameworks
- metrics
- tooling
- standards
- governance

Pros:
- consistency
- deep expertise
- strong governance
- reusable infrastructure

Cons:
- risk of bottlenecks
- slower iteration if isolated

Best for:
- large platforms
- regulated domains
- multi-product companies

---

#### 2) Embedded Evaluation Ownership

Eval responsibility lives inside product teams:
- each team owns its evals
- closer to product context

Pros:
- faster iteration
- better domain understanding

Cons:
- inconsistency
- duplicated effort
- weaker governance

Best for:
- fast-moving startups
- low-risk products (initially)

---

#### 3) Hybrid Model (2026 Best Practice)

A central eval platform team:
- builds tooling
- defines standards
- enforces gates

Product teams:
- write evals
- interpret results
- own outcomes

This model scales best.

---

### Core Roles in an Eval Organization

#### Evaluation Platform Lead
Owns:
- eval infrastructure
- tooling roadmap
- cross-team alignment

This is often a **Staff or Principal engineer**.

---

#### Evaluation Engineers
Build:
- eval pipelines
- metrics
- dashboards
- automation

They are not QA.
They are system engineers.

---

#### Product AI Engineers
Own:
- task-specific evals
- domain metrics
- interpretation of results

They are accountable for quality in their area.

---

#### Human Review Coordinators
Design:
- human eval workflows
- reviewer guidelines
- sampling strategies

They ensure human signal stays high quality.

---

#### Governance / Risk Owners
Approve:
- risk acceptance
- exceptions
- overrides

They are accountable when things go wrong.

---

### Clear Ownership Boundaries

Every eval must answer:
- Who wrote it?
- Who maintains it?
- Who reviews failures?
- Who can override it?
- Who is accountable if it's ignored?

If any answer is unclear, fix the org, not the code.

---

### Evaluation as a First-Class Responsibility

In mature orgs:
- eval work is planned
- eval work is staffed
- eval work is rewarded
- eval work affects promotion

If eval work is invisible, it will be skipped.

---

### Incentives & Performance

Bad incentive:
- reward speed only

Good incentives:
- reward quality preservation
- reward catching issues early
- reward improving eval coverage
- reward incident prevention

People optimize what they're rewarded for.

---

### Ownership During Incidents

When something fails:
- eval owners participate in postmortems
- gaps in evals are treated as system failures
- new evals are added as prevention

Eval debt is tracked like technical debt.

---

### Cross-Team Coordination

At scale:
- multiple teams share models
- prompts
- platforms

Org design must enable:
- shared standards
- local autonomy
- fast escalation
- conflict resolution

Coordination beats control.

---

### Documentation as Org Glue

Strong eval orgs rely on:
- clear docs
- decision records
- evaluation playbooks

This reduces:
- tribal knowledge
- onboarding time
- dependency on individuals

---

### Hiring for Evaluation Maturity

In 2026, strong orgs hire people who:
- think in systems
- care about quality
- can design evals
- can interpret signals
- can explain tradeoffs

This is a differentiator in interviews.

---

### Founder Perspective

For founders:
- eval org design enables delegation
- reduces founder bottlenecks
- supports scale
- increases trust with enterprises

Founders who own everything forever don't scale.

---

### Interview-Grade Talking Points

You should be able to explain:

- centralized vs embedded vs hybrid models
- why hybrid wins
- how ownership is enforced
- how incentives align with quality
- how evals survive team growth

This is **Staff / Principal / Head-of-AI level** thinking.

---

### Completion Checklist

You are done with this section when you can:

- design an eval org for a startup
- design an eval org for an enterprise
- assign clear ownership
- align incentives with quality
- explain governance interactions

If this is clear, you can scale people as well as systems.

---

### What Comes Next

Now that evals scale and teams are organized, the next challenge is:

**How do we choose models and route traffic intelligently?**

That is Section 20 — Model Selection & Routing.
