# 4.2 — Few-Shot Example Economics: When Examples Cost More Than They Improve

The assumption is nearly universal: more few-shot examples means better output. Add three examples of the desired format and the model follows it. Add five and it follows it more reliably. Add eight and you are virtually guaranteed compliance. This assumption was reasonable in 2023, when models needed heavy guidance to follow complex instructions. It is expensive and often wrong in 2026. Frontier models like GPT-5, Claude Opus 4.6, and Gemini 3 Pro have been trained on vast instruction-following datasets. They understand formatting requirements, tone specifications, and structured output constraints from zero-shot instructions alone. The few-shot examples that once provided essential guidance now frequently provide marginal quality improvement at non-marginal cost. Every example you include in a prompt travels with every request. Three examples at 300 tokens each adds 900 tokens to every API call. At 100,000 requests per day, those examples consume 90 million input tokens daily. On GPT-5 at $1.25 per million input tokens, that is $112 per day, $3,375 per month, $40,500 per year — just for the examples. The question is not whether examples help. The question is whether they help enough to justify their cost at your traffic volume.

## The Diminishing Returns Curve

Few-shot examples follow a steep diminishing returns curve. The first example provides the largest quality lift because it anchors the model's understanding of the expected output format, length, tone, and content structure. The model goes from interpreting your instructions abstractly to seeing a concrete instantiation. That shift from abstract to concrete is the most valuable transition in prompt design.

The second example adds less. It confirms the pattern established by the first, reduces ambiguity about edge cases, and narrows the variance in output style. The quality improvement is real but smaller than the first example provided. The third example adds even less — it reinforces a pattern the model has already learned from the first two. By the fourth and fifth examples, the marginal quality improvement per example is often statistically insignificant. You are paying the same token cost for each additional example, but the quality return is approaching zero.

This is not theoretical. A fintech company running a financial report summarization system tested this systematically in mid-2025. They started with five few-shot examples in every request, consuming 1,750 tokens. They removed examples one at a time, measuring summarization quality against a 400-case eval suite after each removal. Removing the fifth example produced zero change in any quality metric. Removing the fourth produced no statistically significant change. Removing the third dropped one metric — format compliance — by 1.2%, within their acceptable tolerance. Removing the second dropped format compliance by an additional 2.8% and summary accuracy by 1.5%. Removing the first example dropped format compliance by 11% and introduced noticeable tone inconsistencies. The team settled on two examples: the minimum required to maintain quality at their threshold. That decision removed 1,050 tokens from every request, saving $47,000 per year on their Claude Sonnet 4.5 deployment.

The curve shape matters for your cost model. Token cost per example is linear — each example costs the same. But quality gain per example is sublinear and approaches zero rapidly. This means there is a crossing point where the marginal cost of the next example exceeds the marginal value of the quality improvement it provides. Finding that crossing point is the core skill of few-shot example economics.

## How to Measure the Marginal Value of Each Example

Measuring marginal value requires a structured process, not intuition. Engineers routinely add examples because they "feel like they help" and remove none because they "might be important." This is how teams end up with seven examples consuming 2,100 tokens per request, with no data on whether any individual example contributes meaningfully to output quality.

The process is straightforward. Start with your current set of examples. Run your eval suite and record baseline scores across all quality dimensions: accuracy, format compliance, tone consistency, factual grounding, whatever matters for your task. Remove the last example. Run the eval suite again. Compare scores. If no metric drops below your quality threshold, the removed example was not earning its tokens. Continue removing examples from the end, one at a time, until a removal causes a meaningful quality drop. That is your minimum viable example set.

Then reverse the process. Start with zero examples. Add examples one at a time from the beginning. After each addition, run the eval suite and record the quality improvement. Calculate the dollar cost of that example across your daily request volume. Compare the cost to the quality gain. This gives you the cost-per-quality-point for each example position. The first example might cost $15,000 per year and improve format compliance by 11 percentage points — a cost of $1,364 per point. The second might cost $15,000 per year and improve format compliance by 2.8 points — a cost of $5,357 per point. The third might cost $15,000 per year and improve format compliance by 1.2 points — a cost of $12,500 per point. At that third example, you are paying $12,500 for a single percentage point of format compliance. Whether that is worth it depends on your quality requirements and your budget. But you cannot make that decision without the data.

The common objection is that this process takes too long. It does not. For a system with a well-built eval suite, removing an example, running the eval, and recording results takes less than an hour per example. A five-example prompt can be fully analyzed in a single day. The annualized savings from removing even one unnecessary example at high traffic volumes justify days of analysis, not just hours.

## The Zero-Shot Threshold: When No Examples Are Needed

Here is the claim that makes teams uncomfortable: for many tasks in 2026, zero-shot prompting with frontier models produces quality that is indistinguishable from few-shot prompting. You do not always need examples. Sometimes you need none.

This was not true two years ago. GPT-4 in 2024 benefited meaningfully from few-shot examples on complex formatting tasks, structured extraction, and style-constrained generation. But the frontier has moved. GPT-5, Claude Opus 4.6, and Gemini 3 Pro have been trained specifically to follow complex, multi-constraint instructions without examples. Their instruction-following capability has improved to the point where a well-written zero-shot system prompt achieves the same quality that required three to five examples with the previous generation. The examples that were essential in 2024 are overhead in 2026 for many task types.

The task types most likely to cross the zero-shot threshold include straightforward classification, sentiment analysis, simple summarization, question answering with provided context, and standard format generation like JSON or structured text. For these tasks, a clear system prompt that specifies the expected output format, any constraints, and the decision criteria is often sufficient. The model does not need to see an example of a correctly classified support ticket. It needs clear instructions on the classification categories and criteria.

The task types that still benefit from few-shot examples in 2026 include highly domain-specific output formats that the model has not encountered in training, nuanced tone calibration that is difficult to specify in instructions alone, and complex multi-step reasoning patterns where the example serves as a reasoning template. If your task involves an unusual output structure — say, a proprietary risk assessment format with twelve interlocking fields — the model may genuinely need to see an example to understand the structure. But if your task involves standard formats and clear instructions, test zero-shot before defaulting to few-shot. You may be paying for examples you do not need.

A healthcare technology company made this discovery in late 2025 when they migrated from GPT-4o to GPT-5 for their clinical note summarization system. Their legacy prompt included four few-shot examples consuming 1,400 tokens. After migration, an engineer ran their eval suite with and without the examples. Without examples, GPT-5 scored within 0.5% of the example-augmented version on every quality metric. The team removed all four examples, saving $63,000 per year in input token costs. The model had surpassed the capability level where those examples provided value. The examples were legacy cost, not current quality investment.

## Dynamic Few-Shot Selection: Higher Quality Per Token

For tasks where examples genuinely improve quality, static examples — the same examples in every request — are often the wrong approach. **Dynamic few-shot selection** retrieves examples relevant to the specific query rather than using a fixed set. Instead of including the same three examples for every customer support question, the system retrieves the three most similar historical interactions to the current query from an example bank. The examples are more relevant, which means each example provides a larger quality improvement per token.

The economics shift in two ways. First, because each dynamically selected example is more relevant, you often need fewer of them. A team that required four static examples to achieve 92% format compliance might achieve the same 92% with two dynamically selected examples, because the selected examples are closer matches to the current task. Fewer examples means fewer tokens. Second, the quality ceiling rises. Static examples provide a generic template. Dynamic examples provide a specific, task-relevant template. The model produces better output because its examples are better aligned with the task at hand.

The cost trade-off is that dynamic selection requires a retrieval step. You need an embedding model to encode the query, a vector index to store your example bank, and a retrieval call to fetch the top-K matches. This adds latency — typically 20 to 80 milliseconds — and cost — typically $0.001 to $0.01 per query for the embedding and retrieval. But the retrieval cost is a fraction of the token savings. If dynamic selection lets you reduce from four static examples at 1,200 tokens to two dynamic examples at 600 tokens, you save 600 tokens per request. At 100,000 requests per day on GPT-5, that is $75 per day in token savings versus $1 to $3 per day in retrieval costs. The net savings are $72 to $74 per day, or approximately $26,000 per year.

Dynamic selection also solves the problem of example staleness. Static examples are written once and updated rarely. They reflect the state of your product, your data, and your user base at the time they were written. As your product evolves, static examples drift out of alignment with current behavior. Dynamic selection pulls from a continuously updated example bank, ensuring that examples always reflect current patterns. The cost of maintaining this bank is minimal — you are already generating production responses that can serve as future examples after quality review.

## The Example Audit: A Systematic Approach

Every team that uses few-shot examples should conduct an **Example Audit** at least twice per year, and ideally after every model upgrade. The audit answers three questions. Are all current examples still necessary? Is the example count optimal? Would dynamic selection be more cost-effective than static examples?

The audit begins with an inventory. List every prompt in your system that includes few-shot examples. Record the number of examples, the token count per example, and the total example token count per prompt. Multiply each by daily request volume and per-token cost to calculate the annualized cost of examples for each prompt. This inventory alone often produces surprises. Teams frequently discover that their example token costs exceed their system prompt costs, especially for tasks with long, detailed examples.

Next, run the marginal value analysis described above. Remove examples one at a time, measure quality impact, and calculate cost-per-quality-point for each example position. Identify examples that are below the marginal value threshold — where the annualized cost of the example exceeds the dollar value you assign to its quality contribution. Remove those examples.

Then evaluate whether remaining examples should be static or dynamic. If your request types are diverse — customer queries about different products, summarization of different document types, classification into many categories — dynamic selection is likely more cost-effective. If your request types are uniform — every request is essentially the same task with minor variations — static examples may be sufficient.

Finally, test zero-shot. Remove all examples and measure quality against your eval suite. If zero-shot quality meets your threshold, you have eliminated your entire example cost. This test takes thirty minutes and can save tens of thousands of dollars per year. The only reason not to run it is if you already know your task requires examples based on recent testing. "We have always used examples" is not evidence that you need them.

## Model Upgrades as Example Reset Points

Every model upgrade is an opportunity to re-evaluate your example strategy. When you migrate from one model generation to the next — from GPT-4o to GPT-5, from Claude Sonnet 4 to Claude Sonnet 4.5, from Gemini 2 to Gemini 3 — the model's instruction-following capability has likely improved. Examples that were necessary for the previous generation may be unnecessary for the current one. The examples that GPT-4o needed to produce consistently formatted JSON output may be redundant for GPT-5, which has stronger native structured output capabilities.

Treating model upgrades as reset points means running your example audit as part of every migration. Before migrating, record your current quality baselines with examples. After migrating, test the new model with the same examples and without examples. If the new model matches quality without examples, remove them. If it matches quality with fewer examples, reduce them. If it still needs all current examples, keep them but re-evaluate at the next upgrade.

Teams that do not reset their example strategy at model upgrades accumulate what amounts to **legacy prompt debt** — instructions and examples designed for a less capable model, carried forward into a more capable one. This debt costs real money. A team that migrated through three model generations without re-evaluating their five-example prompt is likely paying for three to four examples that the current model does not need. That legacy debt compounds with every request, every day, for the life of the deployment.

## Building Cost Awareness Into Example Design

The final principle is cultural. Engineers who design prompts need to understand the cost of their examples, not as abstract knowledge but as a concrete number they can see. When an engineer adds a 350-token example to a prompt that serves 100,000 requests per day on GPT-5, they should know they are adding approximately $16,000 per year to the system's operating cost. That does not mean they should not add the example. It means the decision should be conscious. The example should justify its cost through measurable quality improvement, documented in the eval suite, not through intuition.

Some teams implement automated cost annotations in their prompt management systems. When an engineer modifies a prompt, the system displays the estimated annual cost impact of the change based on current traffic and model pricing. "Adding this example increases estimated annual input token cost by $14,200." That annotation does not block the change. It informs the change. The engineer weighs the quality benefit against the cost and makes a deliberate decision. Over time, this practice builds a team that thinks about examples as economic assets with measurable return on investment, not as free improvements that you can pile on without consequence.

The next subchapter extends this economic thinking from the examples you choose to send to the context you retrieve from external sources — and why most retrieval systems send far more context than the model needs, at a cost that far exceeds its value.
