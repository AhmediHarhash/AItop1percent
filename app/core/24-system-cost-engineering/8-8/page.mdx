# 8.8 — Agent Loop Amplification: Recursive Tool Calls That Drain Budgets

In November 2025, a four-person startup building an autonomous coding agent discovered something was wrong when their weekly OpenAI invoice arrived at $47,000. The system had been running for eleven days by that point. Two agents in a multi-agent research workflow had entered a recursive loop — one agent asked the other to verify a result, the second agent asked the first to re-check a prerequisite, and the first agent sent the verification request back. Neither agent had termination logic for circular dependencies. Neither agent tracked how many times it had already processed the same request. The loop ran continuously for eleven days, twenty-four hours a day, consuming tokens at a steady rate that did not trigger any existing alert thresholds because each individual request looked normal. The total: $47,000 in API charges for zero useful output. The team had no cost circuit breaker. They had no per-session budget cap. They had no loop detection. The agents simply did what agents do when they encounter a problem they cannot solve — they kept trying.

This is **Agent Loop Amplification**, the cost pathology specific to agentic AI systems where an agent enters a recursive cycle of reasoning and tool calls that consumes resources without making progress. It is one of the most expensive failure modes in production AI, and it is becoming more common as agent architectures become more prevalent. By early 2026, security researchers had formalized the concept as **"agentic resource exhaustion"** — a category of vulnerability distinct from traditional denial-of-service because the system is attacking its own budget, not an external target. The agent is functioning exactly as designed. It just never stops.

## The Mechanism: Why Agents Loop

To understand why agents loop, you need to understand how agents work. An agentic system receives a task, decomposes it into steps, executes each step using tools or model calls, evaluates the result, and decides whether the task is complete. If the result is unsatisfactory, the agent modifies its approach and tries again. This retry behavior is not a bug — it is the core value proposition of agents. An agent that gives up after one failed attempt is not very useful. An agent that persists through failures, tries different approaches, and adapts its strategy is what makes agentic AI powerful.

The problem is that persistence without termination conditions is indistinguishable from an infinite loop. The agent encounters a failing test. It modifies the code and reruns the test. The test fails again, for a different reason. The agent modifies the code again. The test fails for a third reason. The agent has no mechanism to recognize that it has been trying for forty minutes and made no meaningful progress. It has no concept of "stuck." It only knows that the task is not complete and that it should try another approach. So it does. And it does again. And again. Three hundred and forty times, at $0.37 per iteration, until the session timeout triggers at $127 — if you were lucky enough to set a session timeout.

Three conditions create the majority of agent loops. **Ambiguous task specifications** are the first. When the task description can be interpreted multiple ways, the agent may oscillate between interpretations. "Make the output better" is a recipe for infinite iteration because "better" is never fully achieved. The agent improves the output, evaluates it, decides it could be better still, and loops. **Tool failures with retry logic** are the second. When a tool returns an error — an API timeout, a file not found, a permission denied — the agent retries with different parameters. If the failure is permanent but the agent does not recognize it as permanent, it retries indefinitely, trying variation after variation of an approach that can never succeed. **Contradictory constraints** are the third. When the task requires satisfying conditions that are mutually exclusive — "make the response shorter and include all of the following details" — the agent alternates between optimizing for one constraint and the other, never reaching a state that satisfies both. Each iteration consumes tokens. None produces a resolution.

## The Cost Profile of a Looping Agent

A normal agent session has a predictable cost profile. The agent receives a task, makes three to eight tool calls and model invocations, produces a result, and terminates. The total cost is typically $0.05 to $0.50 for a standard task, or $1 to $5 for a complex multi-step task using a frontier model. The cost grows roughly linearly with task complexity, and the session completes within seconds to a few minutes.

A looping agent session has a fundamentally different profile. The cost grows linearly with time because the agent keeps making calls at a steady rate. There is no natural termination point. The session continues until an external constraint stops it — a timeout, a token limit, a cost cap, or a human noticing the anomaly. Without any of those constraints, the session runs until the account balance is exhausted.

The per-iteration cost of a loop depends on what the agent is doing in each cycle. A simple reasoning loop where the agent calls the model to reconsider its approach costs $0.01 to $0.05 per iteration. An agent that calls external tools on each iteration — running code, querying APIs, writing files — costs $0.10 to $0.50 per iteration because each tool call involves additional model calls for planning and result interpretation. A multi-agent loop where two or more agents exchange messages costs $0.20 to $1.00 per iteration because every message in both directions is a separate inference call.

At a rate of one iteration every two to ten seconds, the math adds up fast. A simple reasoning loop running for one hour at $0.03 per iteration and one iteration every five seconds generates 720 iterations at a total cost of $21.60. That same loop running for twenty-four hours costs $518. A tool-calling loop at $0.30 per iteration and one iteration every fifteen seconds generates 240 iterations per hour at a total cost of $72 per hour or $1,728 per day. A multi-agent loop at $0.50 per iteration and one iteration every twenty seconds runs for three iterations per minute, 180 per hour, and costs $90 per hour or $2,160 per day. The $47,000 incident over eleven days works out to roughly $4,270 per day — consistent with a multi-agent loop running at moderate intensity around the clock.

Now multiply by concurrency. A production system running 200 concurrent agent sessions at any given time might have a loop incidence rate of 0.5 percent — one in every 200 sessions enters a loop. That is one looping session at all times. If looping sessions cost 100x normal sessions, that single persistent loop accounts for one-third of total agent compute cost. At a 2 percent loop incidence rate — four concurrent loops — the looping sessions dominate the budget.

## Common Triggers in Production

The triggers that cause agent loops in development are different from the triggers in production. In development, loops are usually caused by obvious bugs — a missing termination condition, a broken tool, an infinite recursion in the planner. These are caught during testing. Production loops are caused by edge cases that only emerge at scale, with real user inputs, under real-world conditions.

**Ambiguous user intent** is the most frequent production trigger. A user asks the agent to "research everything about this topic and write a comprehensive report." The agent generates a report, evaluates it, decides it is not comprehensive enough, adds more sections, evaluates again, and loops because "comprehensive" has no measurable threshold. Another user asks the agent to "fix this code until all tests pass." One test has a flaky dependency that fails intermittently. The agent fixes the code, the test passes, the agent runs the suite again, the flaky test fails, the agent tries a different fix, the test passes on the next run, another flaky test fails — the agent is chasing nondeterministic failures that have nothing to do with its code changes.

**Tool integration failures** are the second most common trigger. The agent calls an external API that returns a 429 rate-limit error. The agent waits and retries. The API returns 429 again. The agent tries a different approach — maybe a different endpoint or a reformulated query. That also returns 429 because the rate limit is account-wide, not endpoint-specific. The agent does not understand rate limits as a category of failure. It sees each failed attempt as requiring a new strategy, and each new strategy triggers another rate-limited request. This pattern is especially dangerous with agents that have filesystem access. An agent that writes a file, reads it back to verify, finds an encoding error, rewrites it, reads it back again, and encounters the same encoding error because the underlying issue is in the file system configuration — not in the file content — will loop indefinitely.

**Escalation cascades in multi-agent systems** are the third trigger. Agent A encounters a problem it cannot solve and escalates to Agent B. Agent B determines that it needs information from Agent A to proceed and sends a request back. Agent A sees this as a new task, processes it, and encounters the same original problem, which it escalates to Agent B again. Each exchange is a legitimate-looking message between two agents. Neither agent sees a loop because each individual message is different from the previous one — different wording, different framing, slightly different context. But the underlying cycle repeats. This was the exact mechanism behind the $47,000 incident: two agents caught in a circular dependency that looked like a productive conversation at the message level but was an infinite loop at the workflow level.

## Prevention: Building Circuit Breakers

Preventing agent loops requires multiple defense layers, because no single mechanism catches all loop types. The most resilient systems combine hard limits, behavioral detection, and economic controls.

**Step limits** are the simplest and most essential defense. Set a hard maximum on the number of reasoning steps, tool calls, or iterations an agent can perform in a single session. The right number depends on your use case. A customer support agent that routes inquiries might need 5 to 10 steps. A coding agent that writes and tests code might need 20 to 40 steps. A research agent that gathers and synthesizes information might need 30 to 60 steps. Whatever the number, it must be an absolute ceiling that the agent cannot override. When the agent hits the limit, the session terminates with a summary of what was accomplished and what remains incomplete. The user can start a new session if needed — which resets the counter and creates a natural breakpoint.

**Cost circuit breakers** add an economic layer to the step limit. Instead of counting steps, track the cumulative cost of the session in real time. Set a dollar threshold that triggers termination. For a standard task, the threshold might be $2. For a complex task, $10. For an enterprise batch job, $50. The threshold should be set at a level where legitimate tasks virtually never reach it, but runaway loops are caught within minutes. A session that reaches 80 percent of its cost budget should receive an explicit warning injected into the agent's context: "You have consumed 80 percent of the budget allocated for this task. Evaluate whether you are making progress and consider terminating if the remaining steps are unlikely to succeed." This warning does not guarantee the agent will stop, but it introduces a decision point that breaks many loops.

**Loop detection through action similarity** catches loops that step limits might miss. Track the sequence of actions the agent takes — the tools it calls, the parameters it passes, the types of operations it performs. If the last five actions are substantially similar to the five actions before them, the system flags a potential loop. "Substantially similar" means the same tools called in the same order with parameters that differ by less than a defined threshold. This detection catches the common pattern where an agent retries the same approach with minor variations — editing line 42 instead of line 41, querying with "search query version 3" instead of "search query version 2." The detection does not need to be perfect. A 70 percent accuracy rate is sufficient if the consequence of a false positive is a warning rather than a termination. Inject a reflection prompt into the agent's context: "Your recent actions appear similar to your previous actions. Are you making progress toward the goal, or are you stuck in a cycle? If you are stuck, summarize what you have tried and terminate."

**Mandatory reflection checkpoints** force the agent to pause and evaluate its own progress at regular intervals. Every 10 steps, or every $0.50 in cost, the agent must answer three questions: What have I accomplished since the last checkpoint? Is my current approach likely to succeed? Should I continue, change strategy, or terminate? The answers are logged. If the agent's self-assessment indicates no progress over two consecutive checkpoints, the system escalates to a cost circuit breaker. This approach works because it uses the agent's own reasoning capability to detect situations that mechanical step limits cannot. An agent that has been trying ten different approaches to fix a permissions error might recognize at a reflection checkpoint that the error is environmental rather than code-related — something it would not have recognized in the middle of a retry cycle.

**Timeout walls** provide a final backstop. Set a maximum wall-clock duration for any agent session — 30 minutes for standard tasks, 2 hours for complex tasks, 8 hours for batch processing. When the timeout triggers, the session terminates regardless of progress. Timeouts are blunt instruments that catch everything, including legitimate long-running tasks. But they are the last line of defense against the scenario that burned the $47,000: a loop that runs for days because every other safeguard was absent. A timeout set at 2 hours limits the maximum damage from any single looping session to whatever the agent can spend in 2 hours. At $90 per hour for a multi-agent loop, that is $180 — expensive, but survivable.

## Designing for Graceful Degradation

The best loop prevention does not just terminate runaway sessions. It degrades gracefully, preserving whatever value the agent has created and giving the user a path forward.

When a cost circuit breaker or step limit terminates a session, the system should produce a structured output: a summary of what the agent accomplished, a list of approaches it tried and why they failed, a diagnosis of why it could not complete the task, and a recommendation for how the user might proceed — whether that means providing more specific instructions, breaking the task into smaller subtasks, or escalating to a human. This output costs one additional model call, but it transforms a frustrating dead end into a useful diagnostic. The user understands what happened and can decide how to proceed rather than simply seeing "task terminated."

For multi-agent systems, graceful degradation requires coordination. When Agent A detects that it is in a potential loop with Agent B, it should not just terminate itself. It should send a structured termination message to Agent B that includes the loop diagnosis, the accumulated context, and a recommendation to stop the current workflow. Without this coordination, Agent B may interpret Agent A's termination as a failure and attempt to restart the workflow, creating a new loop at the orchestration level.

## The Organizational Discipline

Technical safeguards are necessary but insufficient. The organizational discipline around agent cost management matters just as much. Every agent deployment needs an owner who is accountable for the cost budget. Every agent type needs a documented cost profile that includes normal session cost, maximum expected cost, and the circuit breaker thresholds. Every week, someone should review the distribution of agent session costs and investigate any session that exceeded 5x the median. This review is not optional overhead. It is how you find the loops that your automated detection missed, the new edge cases your step limits did not anticipate, and the emerging patterns that require updated thresholds.

Teams that treat agent loop prevention as a one-time configuration exercise discover new loop patterns every month. The agent that was safe with 30-step limits starts looping at step 28 when a new tool integration introduces a failure mode the original limits did not account for. The cost circuit breaker set at $5 becomes insufficient when the team upgrades to a more expensive model. The loop detection that caught repetitive tool calls misses a new pattern where the agent alternates between two different tools in a cycle. Agent cost management is an ongoing practice, not a deployed feature.

The cost of building these safeguards is small compared to the cost of a single undetected loop. One engineer spending two weeks building step limits, cost circuit breakers, loop detection, and reflection checkpoints prevents the kind of incident that can consume a quarter's worth of compute budget in a single week. The $47,000 lesson has been learned. The question is whether your team learns it from reading about it or from living it. The next subchapter examines a cost pathology that is entirely self-inflicted — the spiral where your own quality evaluation system consumes as much budget as the product it evaluates.
