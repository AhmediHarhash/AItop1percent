# 6.7 — Search Index Costs: Maintaining Freshness Without Blowing the Budget

How fresh does your search index actually need to be? Not how fresh would be ideal in a perfect world. How fresh does it need to be for your users to get correct, useful results? The answer to that question determines whether you spend $500 or $5,000 a month maintaining your search infrastructure. Most teams never ask it. They default to the freshest option their architecture supports, rebuild indexes on every change, re-embed every document on every update, and treat real-time indexing as a requirement rather than a choice. Then they wonder why their retrieval costs rival their inference costs. The economics of search index maintenance are not about whether you can keep the index fresh. They are about whether you should, at what cadence, and for which data.

Every time a document is added, updated, or deleted in a retrieval system, the index needs to reflect that change. For a vector index, that means generating new embeddings for the changed content, inserting those embeddings into the index structure, and potentially rebuilding parts of the index to maintain search quality. For a keyword index, it means tokenizing the new content, updating the inverted index, and recalculating relevance scores. For a hybrid search system that combines both, every change triggers work on both sides. Each of those operations has a cost — in compute, in API calls, in storage writes, and in engineering complexity. The question is never whether to keep your index up to date. It is how often to pay that cost.

## The Four Cost Drivers of Index Maintenance

**Re-embedding changed documents** is usually the largest single cost in vector index maintenance. Every time a document changes, the modified content must be sent through an embedding model to generate new vectors. If you use an external embedding API like OpenAI's text-embedding-3-small or Cohere's embed-v4, you pay per token. A 2,000-token document costs roughly $0.00002 to embed with text-embedding-3-small at $0.02 per million tokens. That sounds negligible until you consider scale. A knowledge base with 500,000 documents where 5% change daily means 25,000 re-embeddings per day. At 2,000 tokens per document, that is 50 million tokens, costing about $1 per day. Manageable. But a product catalog with 10 million items where 15% update daily — common for e-commerce with fluctuating prices, availability, and descriptions — means 1.5 million re-embeddings per day. At 2,000 tokens per item, that is 3 billion tokens daily. On text-embedding-3-small, that costs $60 per day, $1,800 per month. Switch to a more capable embedding model at five to ten times the price, and you are looking at $9,000 to $18,000 per month just in re-embedding costs.

**Updating vector indexes** is the second cost driver, and it is more complex than most teams anticipate. Modern vector indexes — HNSW graphs, IVF structures, and their variants — are not simple lookup tables where you swap one entry for another. When you insert a new vector into an HNSW graph, the index must connect that vector to its approximate nearest neighbors by traversing existing graph connections. This is a CPU-intensive operation. When you delete a vector, most implementations use soft deletes — they mark the vector as inactive but leave it in the graph structure. Over time, these soft deletes accumulate, bloating the index and degrading search performance. Periodic compaction or rebuilding is required to clean them out, and that rebuild is expensive. A full rebuild of an HNSW index with 10 million vectors can take hours on a standard machine and requires enough memory to hold both the old and new indexes simultaneously.

**Rebuilding keyword indexes** adds a third layer. If your system uses hybrid search — combining vector similarity with keyword matching — the keyword side has its own maintenance overhead. Inverted indexes need to be updated when documents change, segment merges need to happen in the background, and the scoring models need to account for shifting term frequencies as the corpus evolves. Systems like Elasticsearch or OpenSearch handle this automatically through background segment merging, but "automatic" does not mean "free." Those merge operations consume CPU and I/O bandwidth that compete with query-serving resources. During heavy indexing periods, query latency can spike by 20% to 40% as the system diverts resources to index maintenance.

**Maintaining hybrid search infrastructure** is the fourth driver and the one most teams underestimate. Running both a vector database and a keyword search engine means maintaining two separate systems with their own scaling, monitoring, and operational requirements. Pinecone for vectors and Elasticsearch for keywords, for example, doubles your infrastructure footprint compared to either alone. By 2026, several platforms — Weaviate, Qdrant, and Elasticsearch with its native vector search — offer integrated hybrid search in a single system, reducing operational complexity. But even integrated systems have higher resource requirements when running hybrid queries versus either mode alone. The query must search both indexes, combine scores, and return merged results. Each hybrid query costs more in compute than a pure vector or pure keyword query.

## The Freshness-Cost Spectrum

Not all freshness strategies cost the same. The spectrum runs from real-time indexing at the expensive end to weekly batch rebuilds at the cheap end, with several intermediate options that offer different trade-offs.

**Real-time indexing** means every document change is reflected in the index within seconds. When a product price changes, the index updates immediately. When a support article is edited, the new version is searchable within moments. This requires streaming infrastructure — a change data capture pipeline, an event bus, an incremental indexing service that processes changes as they arrive. The embedding model must be available on demand, not in batch mode. The vector database must support real-time upserts without manual compaction delays. The cost is high not just in compute but in architecture. You need a reliable streaming pipeline, retry logic for failed updates, monitoring to detect indexing lag, and enough embedding capacity to handle burst updates. A team running real-time indexing on a 10-million-document corpus typically spends $3,000 to $8,000 per month on the indexing infrastructure alone, separate from query-serving costs.

**Near-real-time indexing** introduces a small deliberate delay — typically one to fifteen minutes — between a document change and its appearance in the index. Changes are collected in micro-batches and processed together. This is dramatically cheaper than true real-time because it allows batching of embedding calls (reducing per-call overhead), batching of index updates (reducing write amplification in the vector database), and smoothing of traffic spikes (a burst of 1,000 changes processes as one batch rather than 1,000 individual operations). A team that moves from real-time to near-real-time indexing with five-minute batches typically reduces indexing costs by 30% to 50% while adding less than fifteen minutes of staleness.

**Scheduled batch indexing** processes all accumulated changes at fixed intervals — every hour, every six hours, every night. This is the cheapest approach because it maximizes batching efficiency, allows the use of batch-pricing APIs for embedding generation (which can be 50% cheaper than real-time API calls), and concentrates all index maintenance into a single window where resources can be provisioned temporarily rather than running continuously. A nightly batch indexing pipeline for a 500,000-document knowledge base typically costs $200 to $500 per month. The trade-off is staleness. If a critical document changes at 9 AM and the batch runs at midnight, users see stale results for fifteen hours.

**Full periodic rebuilds** are the cheapest per-execution option for small to medium corpora but the most expensive for large ones. Instead of tracking individual changes, you re-embed and re-index the entire corpus on a schedule. For a corpus under 100,000 documents, a weekly full rebuild might cost $20 to $50 in embedding and compute. For a corpus of 10 million documents, a full rebuild costs $1,000 to $3,000 per execution. Full rebuilds are simple to implement — no change tracking, no incremental logic — but they waste money re-embedding documents that have not changed.

## Matching Cadence to Use Case

The right freshness cadence depends on what the index serves, and different categories of content within the same system may warrant different cadences.

**Customer support knowledge bases** are the most forgiving. Support articles change infrequently — a few updates per week, maybe a handful of new articles per month. The content is reference material, not time-sensitive data. A daily batch update, or even twice-weekly, keeps the index sufficiently fresh. Most customer questions are about stable topics: how to reset a password, what the return policy is, how to configure a feature. The answer does not change hour by hour. Teams that run real-time indexing on support knowledge bases are paying a premium for freshness that provides no measurable improvement in user satisfaction. Shift to daily batch indexing and reinvest the savings elsewhere.

**Product catalogs** are more nuanced because they contain both time-sensitive and stable data. Product prices and availability change frequently — sometimes hundreds of times per day across a large catalog. Product descriptions, specifications, and images change rarely. The cost-effective approach is to split the index. Use real-time or near-real-time indexing for structured fields like price and stock status, which can be updated without re-embedding because they are typically handled by filters or metadata rather than vector search. Use daily or weekly batch indexing for product descriptions and other content that feeds into the embedding layer. This split approach can reduce indexing costs by 60% to 80% compared to re-embedding the entire product record every time a price changes.

**News, financial data, and market intelligence** demand near-real-time or real-time indexing. A financial analyst searching for information about a company needs today's news, not yesterday's. A trading system that uses retrieval-augmented generation to surface relevant market context cannot work with stale data. For these use cases, the freshness cost is not optional. It is a product requirement. The optimization lever here is not cadence — you cannot slow it down — but efficiency: smaller embedding models, more aggressive change detection, and careful scoping of what actually needs to be in the index versus what can be queried from a live database at retrieval time.

**Internal documentation and corporate wikis** are the most tolerant of staleness. Confluence pages, internal policy documents, and engineering wikis change slowly. A weekly batch update is almost always sufficient. Some teams index internal documentation monthly without complaints. The cost savings are substantial: a system that processes 200,000 internal documents weekly instead of in real time avoids the entire streaming infrastructure, reduces embedding costs by batching, and runs the indexing pipeline on spot instances during off-peak hours. For a corpus this size, the difference can be $2,000 or more per month.

## Incremental Indexing: Only Pay for What Changed

The single most impactful optimization for index maintenance cost is **incremental indexing** — processing only the documents that actually changed since the last index update, rather than re-processing everything. This sounds obvious, but a surprising number of teams run full re-indexing pipelines because incremental logic is harder to implement and "it just works" to re-index everything.

The cost difference is dramatic. Consider a knowledge base of 500,000 documents where 2% change daily. A full re-index processes 500,000 documents. An incremental approach processes 10,000 documents. That is a 50-to-1 cost reduction in embedding and indexing compute. At $0.02 per million embedding tokens and 2,000 tokens per document, the full re-index costs $20 per run. The incremental re-index costs $0.40 per run. Over a month of daily updates, the full approach costs $600. The incremental approach costs $12. The gap widens as the corpus grows and the change rate stays low.

Implementing incremental indexing requires change detection — knowing which documents changed since the last run. The most reliable approach is a **change tracking table** that records document IDs, last-modified timestamps, and content hashes. When the indexing pipeline runs, it queries this table for documents modified since the last successful run. Only those documents are re-embedded and re-indexed. Content hashing adds an extra layer of protection: even if a document's timestamp changed (perhaps it was opened and saved without modification), the hash confirms whether the actual content differs. This prevents wasting embedding calls on documents whose text has not changed even though their metadata has.

Some teams take change detection further with **differential embedding.** Instead of re-embedding an entire document when one paragraph changes, they embed at the chunk level and only re-embed the specific chunks that were modified. If a 20-page document has one paragraph updated, differential embedding re-processes one chunk instead of twenty. For large documents with small, frequent edits — legal contracts with amendment histories, technical manuals with version updates — differential embedding can reduce re-embedding costs by another 80% to 90% on top of the savings from document-level change detection.

## Index Partitioning: Rebuild Only What You Must

**Index partitioning** is the structural optimization that makes selective rebuilding possible. Instead of maintaining one monolithic index for your entire corpus, you split the index into partitions based on logical boundaries — by content type, by data source, by update frequency, or by customer tenant.

The cost benefit comes during maintenance. When the product catalog partition needs a full rebuild because of a schema change, only that partition rebuilds. The knowledge base partition, the internal docs partition, and the support articles partition remain untouched. Without partitioning, any operation that requires a full rebuild affects the entire corpus. With partitioning, the blast radius is limited to the affected segment.

Partitioning also enables different freshness cadences for different content types within the same search system. The product catalog partition can run near-real-time indexing. The internal documentation partition can run weekly batch updates. Each partition carries only the maintenance cost appropriate to its freshness requirements. A team running a unified search system across four content types with partitioned indexes typically spends 40% to 60% less on index maintenance than a team running the same content through a single monolithic index with a one-size-fits-all update cadence.

The trade-off is query complexity. Searching across partitions requires a federation layer that queries each partition and merges results. This adds query latency — typically 10 to 30 milliseconds per partition — and introduces scoring normalization challenges when different partitions use different embedding models or indexing parameters. For most production systems, the maintenance savings outweigh the query overhead, but teams should benchmark both before committing to a partitioning strategy.

## The Hidden Cost: Index Fragmentation and Compaction

A cost that sneaks up on teams running incremental indexing for months is **index fragmentation.** As vectors are inserted, updated, and soft-deleted over time, the internal structure of a vector index degrades. HNSW graph connections become suboptimal — new vectors are connected based on the graph state at insertion time, not the current optimal neighborhood. Soft-deleted vectors remain in the graph, adding traversal overhead to every search. Memory usage creeps upward as dead entries accumulate.

The symptom is gradual search quality degradation paired with increasing query latency. Neither metric changes dramatically on any given day, but over weeks and months, 95th-percentile latency drifts upward by 15% to 30%, and recall at the same latency budget drops by 5% to 10%. Teams that do not monitor these metrics closely attribute the degradation to other causes — model drift, changing user queries, increased traffic — and miss the actual problem.

The fix is periodic compaction or rebuilding of the affected index segments. Most managed vector databases — Pinecone, Weaviate, Qdrant — handle compaction automatically in the background, but the compute cost of that compaction shows up in your bill. Self-hosted indexes require manual scheduling. A sensible policy is to monitor the ratio of active to total vectors in each index segment and trigger compaction when the ratio drops below 80%. This balances the cost of compaction against the performance penalty of fragmentation.

Budget for compaction as a recurring cost. For a moderately active index with 5 million vectors and 5% daily churn, expect compaction to add 10% to 15% to your baseline index maintenance costs. Ignore it, and you pay the cost anyway — in slower queries, worse results, and eventually a forced full rebuild when the fragmentation becomes unbearable.

## Monitoring What You Spend on Freshness

You cannot optimize what you do not measure. Index maintenance costs are notoriously difficult to track because they blend into general infrastructure spending. The embedding API calls for indexing look the same as embedding calls for query-time operations. The compute consumed by index updates shares instances with query serving. The storage cost of the index includes both the useful vectors and the dead weight of soft deletes.

Separate your indexing costs from your query-serving costs. Tag embedding API calls with a source label — "indexing" versus "query" — so you can see what fraction of your embedding spend goes to keeping the index fresh versus answering user questions. Track the number of documents re-embedded per day, the number of index write operations, and the compute hours consumed by index maintenance jobs. Calculate a **cost-per-fresh-document metric**: divide your total monthly indexing cost by the number of unique documents that were actually updated. If you are spending $1,800 per month to keep an index fresh and only 50,000 unique documents change per month, your cost per fresh document is $0.036. If that number is higher than the value of having that document fresh, your cadence is too aggressive.

Set freshness SLAs by content type, not for the system as a whole. Your product pricing data might need a 15-minute freshness SLA. Your support articles might need a 24-hour SLA. Your internal documentation might need a 7-day SLA. Then engineer each pipeline to meet its SLA and no more. The team that treats every document as equally urgent pays the freshness cost of their most demanding content type across their entire corpus.

## The Freshness Budget Framework

Put it all together with a framework that any team can apply. Start by inventorying every content type in your search system. For each type, answer three questions. First, what is the business impact of stale data? If a customer sees a product price from yesterday, does that cause a support ticket, a failed transaction, or just mild inconvenience? Second, what is the change velocity — what percentage of documents in this category change per day, per week? Third, what does it cost to maintain each freshness level — real-time, near-real-time, daily, weekly?

Map each content type to the cheapest freshness cadence that meets its business impact threshold. You will almost always find that only 10% to 20% of your content requires real-time or near-real-time freshness. Another 30% to 40% can live with daily updates. The rest — often 40% to 60% — is fine with weekly refreshes. Implementing this tiered freshness strategy on a mixed-content search system with 2 million total documents typically reduces index maintenance costs by 50% to 70% compared to a uniform real-time approach, with no measurable impact on user satisfaction for the content types moved to slower cadences.

The savings are not just in compute and API calls. The architectural simplicity of a batch pipeline versus a streaming pipeline means less engineering time maintaining the infrastructure, fewer operational incidents from streaming failures, and a smaller surface area for bugs. The cheapest pipeline is not just the one that processes the fewest documents. It is the one that requires the fewest engineers to keep running.

The next subchapter examines where all this data lives after it is indexed and generated — and how tiering your storage across hot, warm, cold, and archive layers can cut your AI data storage bill by half or more.