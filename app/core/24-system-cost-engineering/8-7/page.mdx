# 8.7 — Token Exhaustion and Billing Abuse: Economic Exploitation Patterns

The script has been running for six hours. A single user on the free tier, registered with a disposable email address, is sending requests at a rate of 10,000 per hour. Each request is engineered to generate maximum output — the prompt instructs the model to "write a comprehensive 5,000-word essay on the following topic" followed by a random Wikipedia article title. The responses average 3,800 tokens of output each. The system's rate limiter counts requests per minute and the user stays just below the threshold. By the time the on-call engineer notices the anomaly in the dashboard, this one user has consumed $4,800 worth of inference — more than the previous week's total spend across 23,000 legitimate users. The user's account was created fourteen minutes before the first request. There is no payment method on file. There never was.

This is not a security breach. No data was stolen. No system was compromised. But the economic damage is real, immediate, and entirely preventable. **Token exhaustion** and **billing abuse** are the cost pathologies that sit at the intersection of system design and adversarial behavior. They happen when users — sometimes malicious, sometimes just clever, sometimes genuinely naive — consume disproportionate resources relative to what they are paying or what the system was designed to support. Understanding these patterns is not optional for any team running AI in production. Every unprotected endpoint is an invitation.

## The Asymmetry That Creates the Vulnerability

The fundamental vulnerability is economic asymmetry. It costs the attacker almost nothing to send a request — a few lines of script, a few seconds of compute, a disposable email address for a free account. It costs you real money to fulfill that request. A single API call to a frontier model with a long output can cost $0.05 to $0.15. Ten thousand such calls cost $500 to $1,500. A hundred thousand cost $5,000 to $15,000. The attacker bears zero marginal cost. You bear all of it. In traditional software, this asymmetry matters less because the cost of serving a request is negligible — a database read, a template render, a few microseconds of compute. In AI systems, the cost of serving a request is measured in tokens, and tokens cost money at every scale.

This asymmetry is why AI billing abuse has become a distinct attack category since 2024. Security researchers coined the term **"denial-of-wallet"** to distinguish it from denial-of-service. A denial-of-service attack aims to make your system unavailable. A denial-of-wallet attack aims to make your system expensive. The system keeps running. The users keep getting responses. But the bill spirals because one or more actors are consuming resources far beyond what your economic model assumed. By late 2025, the OWASP Top 10 for LLM Applications included denial-of-wallet as a named vulnerability, and major cloud providers began adding specific billing abuse protections to their AI service tiers.

## The Four Patterns of Billing Abuse

Billing abuse is not a single behavior. It follows four distinct patterns, each with its own mechanism, its own attacker profile, and its own defenses.

**Free-tier farming** is the most common. A user creates multiple accounts to bypass per-account limits. The mechanics are simple: register with a disposable email, use a VPN to rotate IP addresses, and run each account up to its free-tier limit before creating the next. A single person with a basic script can operate fifty to a hundred accounts simultaneously. If your free tier offers 1,000 requests per month, a hundred accounts give the attacker 100,000 free requests. The cost to you is 100,000 model calls that generate zero revenue. Teams that discovered this pattern in production often found clusters of accounts created within minutes of each other, all originating from similar IP ranges, all exhibiting identical usage patterns — perfectly regular request intervals, identical prompt structures, identical session durations.

**Output maximization** is more subtle. The attacker does not need multiple accounts. They craft prompts specifically designed to generate the longest possible responses. "Write a comprehensive, detailed, 5,000-word essay" generates more output tokens than "summarize this in one paragraph." If your pricing model charges the user a flat rate per request but you pay per token to the model provider, every output-maximized request erodes your margin. Some attackers go further and chain requests: they send the model's output back as input for the next request, creating an artificial conversation that accumulates context and maximizes both input and output tokens simultaneously. A single user running this pattern for eight hours can consume more tokens than a thousand normal users consume in a day.

**API proxy abuse** is the pattern where users exploit your AI product as a free or cheap proxy to the underlying model API. If you offer an AI feature that calls GPT-5 or Claude Opus 4.6 behind the scenes, and your pricing is lower than the direct API cost — or worse, if the feature is included in a flat-rate subscription — sophisticated users will script against your endpoint to get model access at below-market rates. This is particularly common with products that expose a chat interface. The attacker builds a wrapper around your chat API, routes their own application's traffic through it, and pays your subscription rate instead of the API provider's per-token rate. One SaaS company discovered in early 2025 that 4 percent of their users were consuming 61 percent of their model spend. Investigation revealed that twelve of those users were running automated pipelines that treated the company's chat feature as a cheap alternative to direct API access.

**Resource hoarding** is the least obvious pattern. The attacker opens long-running sessions or connections and holds them open to consume capacity. In systems that maintain conversation state in memory or reserve GPU capacity per session, a user who opens fifty concurrent sessions and sends occasional keep-alive messages can tie up resources that would otherwise serve hundreds of legitimate users. The direct cost is the capacity those sessions consume. The indirect cost is the degraded performance or increased infrastructure spend required to maintain capacity for legitimate traffic.

## The Scale of the Problem

A single abusive user can consume more resources than 10,000 legitimate users. This is not hyperbole. Consider the math. A typical legitimate user of a conversational AI product sends 5 to 15 messages per day, with an average input of 200 tokens and an average output of 400 tokens. At the median, a legitimate user consumes about 6,000 tokens per day. An abusive user running an output-maximization script sends 500 to 2,000 messages per day, with crafted inputs of 500 tokens and outputs that average 3,000 tokens. At the median, an abusive user consumes 3.5 million tokens per day. That single user consumes roughly 580 times the tokens of a normal user. Ten such users consume as much as 5,800 normal users. If you have 50,000 legitimate users and ten abusers, those ten abusers account for 10 percent of your total token spend while generating zero revenue.

The economic impact compounds because abusive usage often targets your most expensive resources. Abusers prefer frontier models because those models produce higher-quality outputs. They prefer long conversations because they get more value per session. They prefer agentic features because multi-step workflows deliver more capability per request. Every design decision you make to improve your product for legitimate users also improves the value proposition for abusers. The features that attract your best customers also attract your most expensive exploiters.

## Detection: Spotting Abuse Before the Invoice Arrives

The good news is that abusive patterns are distinctive. They look different from legitimate usage in ways that are detectable if you have the right instrumentation.

**Cost-per-user tracking** is the foundation. Calculate the daily and weekly cost attributed to each user. Plot the distribution. In a healthy system, the distribution follows a long tail — most users cluster around the median, some power users are 3x to 5x above median, and a tiny fraction is 10x or above. Abusive users show up as extreme outliers: 50x, 100x, or 500x the median. Set an alert threshold at 10x the median daily cost. Any user who crosses that threshold gets flagged for review. Not automatically blocked — flagged. Some of your highest-value customers are also high-consumption users, and you do not want to suspend a customer paying you $50,000 per year because they had a busy day. But you do want to know about them.

**Behavioral fingerprinting** catches scripted abuse. Humans are messy. They send requests at irregular intervals. They rephrase questions. They pause, backtrack, abandon sessions. Scripts are clean. They send requests at perfectly regular intervals — every 3.2 seconds, or every 500 milliseconds. They use identical prompt templates with minor variations. They never pause, never abandon a session, never make typos. If a user's request pattern has lower variance in timing than any human could produce, it is almost certainly automated. Track the standard deviation of inter-request intervals for each user. Legitimate users have high variance. Bots have near-zero variance.

**Session pattern analysis** catches multi-account farming. Look for clusters of accounts that were created within a narrow time window, share similar registration patterns like sequential email addresses or disposable email domains, and exhibit identical usage behaviors. Device fingerprinting and IP clustering help here, but even without sophisticated tracking, the behavioral similarity between farmed accounts is usually obvious in the data. Twenty accounts that all send the same prompt template at the same rate from the same IP range are not twenty independent users.

**Output-to-input ratio monitoring** catches output maximization. Calculate the ratio of output tokens to input tokens for each user. Legitimate users typically have an output-to-input ratio between 1:1 and 4:1 — the model's response is one to four times the length of the user's input. Output maximizers have ratios of 8:1, 15:1, or higher, because they send short prompts designed to trigger long responses. A user whose average output-to-input ratio is three standard deviations above the population mean is almost certainly gaming your system.

## Prevention: Building Economic Defenses

Detection tells you where the problem is. Prevention keeps it from becoming a problem in the first place. Effective prevention combines multiple layers, because no single defense stops all abuse patterns.

**Progressive rate limiting** is the first layer. Instead of a flat rate limit like 60 requests per minute for everyone, implement a tiered system where the rate limit adjusts based on the user's trust level. New accounts start with a conservative limit — perhaps 10 requests per minute. After a week of normal usage with a verified email, the limit increases to 30. After a month with a payment method on file, it increases to 60. After three months of consistent usage with no anomalies, it increases to 100. This approach is generous for legitimate users who demonstrate normal behavior and restrictive for new accounts that have not earned trust. Most abusers never get past the first tier because they create accounts, hit the conservative limit, and move on.

**Cost-based quotas** are more effective than request-count quotas. A request count limit treats a simple "What time is it?" query the same as a "Write a 5,000-word research paper" prompt. Both count as one request, but the second costs 20x more in tokens. Cost-based quotas assign each user a daily cost budget instead of a request count. A free-tier user might get $0.50 per day in inference cost. A paid user might get $5.00 per day. An enterprise user might get $50.00 per day. When the user's accumulated cost hits the daily cap, further requests are either throttled to a cheaper model or queued until the next day. This approach directly aligns the defense mechanism with the resource you are protecting — cost — rather than using a proxy measure like request count.

**Authentication gates for expensive features** add friction where it matters most. Your basic chat feature might be available to any registered user. Your agent feature, which can consume 20x more tokens per interaction, requires a verified email and a payment method on file. Your research mode, which triggers multi-step retrieval and synthesis, requires a paid subscription. The principle is simple: the more expensive the feature, the higher the authentication bar. This does not eliminate abuse, but it dramatically reduces the pool of potential abusers by filtering out anyone unwilling to attach a real identity or a real payment method to their usage.

**Output length controls** address output maximization directly. Set a maximum output token limit per request that matches your product's legitimate use cases. If your product is a customer support chatbot, no legitimate response needs to be 5,000 tokens. Cap output at 1,000 tokens. If the user legitimately needs a longer response, they can ask a follow-up question. This single control eliminates the most common output-maximization pattern. Pair it with a per-session total token limit, and you also prevent the accumulated-context attack where users chain requests to grow the conversation window.

**Automated suspension with human review** is the final layer. When monitoring detects a user who exceeds multiple abuse indicators — high cost, scripted behavior, output-maximization ratios — the system automatically suspends the account and creates a review ticket. The suspension is immediate but temporary. A human reviewer examines the account within 24 hours to determine whether the behavior is genuinely abusive or just an unusually active legitimate user. This approach stops the bleeding fast while avoiding permanent bans of high-value customers who happen to have a busy day. The key is speed: the automated suspension prevents damage, and the human review prevents false positives.

## The Legal Foundation

Technical defenses work best when backed by clear legal language. Your terms of service must explicitly address abusive usage patterns, and the language must be specific enough to enforce. Vague terms like "excessive usage" are difficult to act on because they invite disputes about what counts as excessive. Instead, define specific prohibited behaviors: automated or scripted access without prior authorization, creating multiple accounts to circumvent usage limits, deliberately crafting inputs to maximize resource consumption, and using the service as a proxy for direct API access to underlying model providers.

Include a clause that reserves your right to suspend or terminate accounts that exhibit patterns consistent with these behaviors. Include a clause that specifies you may recover costs from users whose usage demonstrates clear abuse of pricing terms. These clauses may never be enforced — most abusers are anonymous and unreachable — but they establish the legal basis for automated suspension, give your support team clear authority to act, and protect you in the rare case where an abuser disputes their suspension.

The EU AI Act's transparency obligations, enforced as of 2025, add a layer of complexity. If your system monitors user behavior to detect abuse, you may need to disclose this monitoring in your terms of service, particularly if you operate in jurisdictions that regulate automated decision-making. Consult with legal counsel to ensure your abuse detection and automated suspension systems comply with local regulations on algorithmic enforcement.

## The Organizational Challenge

The hardest part of defending against billing abuse is not the technology. It is the organizational will to enforce limits. Product teams resist rate limits because they fear hurting the user experience. Sales teams resist cost caps because they worry about enterprise customers hitting limits during evaluations. Leadership resists automated suspension because they do not want negative publicity from false positives. These concerns are legitimate. But the alternative is an unprotected system where a single script can consume thousands of dollars per day.

The resolution is graduated enforcement with clear escalation. Start with monitoring and alerting. Then add soft limits — warnings to users who approach thresholds. Then add hard limits — automatic throttling or suspension for users who exceed thresholds. At each stage, measure the impact on legitimate users. In practice, well-designed progressive rate limits and cost-based quotas affect fewer than 0.1 percent of legitimate users while blocking 95 percent of abusive patterns. The data makes the case. Run the numbers, show the abuse you prevented, and the organizational resistance dissolves.

Every dollar you lose to billing abuse is a dollar you cannot invest in your product. Defending against exploitation patterns is not a security luxury — it is a cost engineering requirement. The next subchapter examines a different kind of cost pathology: what happens when your own AI agents turn against your budget through recursive loops that drain resources from the inside.
