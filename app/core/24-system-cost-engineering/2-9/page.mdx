# 2.9 — The Token Price Deflation Curve: How Pricing Changes Across Model Generations

Token prices have dropped roughly ten times every twelve to eighteen months since large language models became commercially available. This is not a guess. It is observable history. And it changes how you should think about every cost optimization decision you make.

GPT-3 became publicly accessible through OpenAI's API in late 2020 at roughly sixty dollars per million tokens. When GPT-3.5 Turbo launched in March 2023, it cost two dollars per million input tokens — a thirty-fold reduction for a more capable model. GPT-4 launched in March 2023 at thirty dollars per million input tokens for the 8K context version, a premium tier for its substantially greater capabilities. By mid-2024, GPT-4o brought that frontier capability down to five dollars per million input tokens. GPT-4o mini, released in July 2024, pushed the floor even lower at fifteen cents per million input tokens. When GPT-5 launched in August 2025, it arrived at $1.25 per million input tokens — roughly four times cheaper than GPT-4o for a model significantly more capable. As of early 2026, GPT-5-mini costs twenty-five cents per million input tokens, and GPT-5-nano costs five cents. The cheapest model that matches GPT-3's original performance level now costs roughly one-thousandth of what GPT-3 cost five years ago.

The pattern holds across providers. Anthropic's Claude 3 Opus launched at fifteen dollars per million input tokens in early 2024. Claude Opus 4.5, released in 2025, costs five dollars per million input tokens — a 67% reduction for superior capabilities. Claude Sonnet 4.5 costs three dollars per million input tokens. Claude Haiku 4.5 costs under one dollar. Google's Gemini 3 Flash sits at fifty cents per million input tokens. Open-source models running on hosted inference push prices even lower.

This is **the Token Price Deflation Curve**, and it is the single most important external force shaping AI cost engineering.

## The Historical Deflation Rate

Epoch AI, a research organization that tracks AI compute trends, documented that for a model achieving equivalent performance to a given benchmark score, inference costs have dropped by a factor of roughly ten every year. That is faster than Moore's Law, which historically doubled computing power every eighteen to twenty-four months. Token price deflation is driven by multiple compounding factors: hardware improvements in GPU and custom silicon, algorithmic efficiency gains in model architecture, increased competition among providers, and open-source models that create pricing pressure on commercial APIs.

The deflation is not uniform across capability tiers. Frontier models — the most capable models from any given provider — have deflated more slowly because each new frontier model offers substantially more capability. You are paying less per unit of intelligence, but the frontier keeps advancing. Mid-tier models have deflated faster because competition is fiercer in the middle of the market. Budget-tier models have deflated fastest because specialized small models and open-source alternatives create intense downward pressure.

This stratification matters for cost engineering. If you are locked into a frontier model because your task requires maximum capability, your deflation rate is slower — perhaps five to seven times over eighteen months. If your task can be served by a mid-tier model, deflation is closer to ten times. If your task can be served by a budget model, deflation exceeds ten times, and open-source hosted alternatives may reduce your cost to a fraction of a cent per million tokens.

## What Deflation Means for Today's Cost Decisions

The deflation curve creates a strategic tension. Every dollar you spend on cost optimization today may be unnecessary in eighteen months because prices will have dropped enough to make the optimization irrelevant. But today's bill is due today, and eighteen months of overspending at current prices is a lot of money.

The wrong conclusion is to wait and do nothing. A team spending $120,000 per month on API costs cannot afford to wait eighteen months for prices to drop by ten times. Even if prices drop exactly as predicted, the team will have spent $2.16 million in the interim. Optimization that saves 30% today saves $648,000 over those eighteen months. The optimization is worth doing even if prices eventually make it unnecessary.

The other wrong conclusion is to over-invest in micro-optimizations that price drops will erase. Spending three months of engineering time to save $5,000 per month through aggressive token shaving is a poor investment if a model upgrade in six months will save $20,000 per month automatically. The engineering time would have been better spent on product features.

The right approach is to distinguish between **structural optimizations** and **micro-optimizations**. Structural optimizations are changes to your architecture that compound with price deflation. Micro-optimizations are narrow tweaks that price deflation makes irrelevant.

## Structural Optimizations That Compound with Deflation

Structural optimizations improve cost efficiency in ways that persist and multiply as prices drop. They are worth investing in regardless of the deflation curve because they make your system cheaper at every price point.

**Caching** is the purest structural optimization. A cache that prevents 40% of your requests from hitting the API saves 40% of your cost at any token price. When prices drop by ten times, your cached system costs one-tenth of its current cost. Without caching, your system also costs one-tenth — but one-tenth of a bigger number. The cache advantage persists through every generation of pricing.

**Model routing** is another structural optimization. A router that sends 70% of queries to a cheap model and 30% to an expensive model reduces your effective cost per query. When both models get cheaper, the router's proportional savings persist. The absolute savings per query decrease, but you are still saving the same percentage, and that percentage applies to a higher volume of queries because cheaper models encourage more usage.

**Context management** — summarization, selective history, sliding windows — reduces the tokens consumed per request. This savings compounds with price deflation because you are reducing the volume of tokens, not just the price. When prices drop, you pay less per token and you send fewer tokens. The savings multiply.

**Prompt optimization** — eliminating waste, tightening instructions, reducing output verbosity — similarly reduces token volume. Every token you remove from your prompt stays removed through every pricing generation. The savings from removing 500 tokens from a system prompt compound with every price drop and every volume increase.

These structural optimizations are the ones to invest in. They make your system leaner, and a leaner system benefits more from price deflation than a bloated one.

## Micro-Optimizations That Deflation Erases

Micro-optimizations are narrow, low-impact tweaks that produce small savings at current prices and become meaningless as prices drop. They are not worthless — they save money today — but the engineering effort required to implement and maintain them may not be justified when you factor in the deflation timeline.

Shaving ten tokens from a prompt by abbreviating instructions saves a fraction of a cent per request. At current prices, across high volume, this might save a few hundred dollars per month. In eighteen months, when the same model costs one-tenth the price, the savings are a few dozen dollars per month. The engineering time spent implementing, testing, and maintaining the abbreviation was worth more than the savings.

Switching between two models that differ by a fraction of a cent per million tokens is another micro-optimization. The model that costs $0.50 per million tokens versus the one that costs $0.45 saves you 10% — but if both prices drop to $0.05 and $0.045 in a year, the absolute savings are negligible.

Compressing retrieved context by 5% through aggressive text cleaning is a micro-optimization. The effort to build and maintain the cleaning pipeline exceeds the savings once prices drop.

The test for whether an optimization is structural or micro is simple: will this optimization still save meaningful money if token prices drop by ten times? If yes, it is structural. If no, it is micro.

## The Moving Floor: Why Total Spend Often Stays Flat

One of the most counterintuitive aspects of the deflation curve is that total AI spending at most companies does not decrease proportionally with prices. Prices drop by ten times, but total spend drops by two to three times — or sometimes increases. This is the **Moving Floor** phenomenon.

The mechanism is straightforward. When tokens get cheaper, teams use more of them. Features that were too expensive to build at five dollars per million tokens become viable at fifty cents. Products that were limited to premium tiers get rolled out to free users. Conversations that were capped at ten turns are allowed to run to fifty. Models that were called once per user action get called three times with different prompts for different analysis angles. Quality controls that required a cheaper model get upgraded to a more capable one because the capable model is now affordable.

Jevons Paradox — the observation that increased efficiency of resource use tends to increase total consumption of that resource — applies directly to token pricing. Every price drop unlocks new use cases, new features, and new volume that partially or fully offsets the per-unit savings. A company that was spending $80,000 per month on API calls at 2024 prices might spend $60,000 per month at 2025 prices — not the $8,000 that a naive ten-times deflation would suggest. The per-token cost dropped ten times, but usage grew five to six times.

This has profound implications for cost planning. Do not assume that next year's bill will be one-tenth of this year's bill just because prices are dropping. Assume that your team will find ways to use the cheaper tokens. Budget accordingly. The floor moves up as the ceiling of what is affordable expands.

A media company experienced this directly. In 2024, they spent $45,000 per month on API calls for their content recommendation engine, using GPT-4o at five dollars per million input tokens. When they migrated to GPT-5 at $1.25 per million input tokens in late 2025, their per-query cost dropped by 75%. But the product team immediately expanded the recommendation engine from serving premium users to serving all users, tripled the number of model calls per recommendation to include personalization and explanation, and increased the context window per call by 60% to improve quality. Monthly spend stabilized at $38,000 — a 16% decrease despite a 75% price reduction. The product got dramatically better. The savings were real but flowed into product quality rather than the budget line.

## Strategic Implications for Architecture Decisions

The deflation curve should influence which architecture decisions you make and when. Some decisions become more urgent because deflation creates opportunities. Others become less urgent because deflation will solve the problem for you.

Building a caching layer is urgent regardless of deflation. Caching saves money at every price point and scales with volume. The sooner you build it, the more money it saves across all future price regimes.

Building a model routing layer becomes more valuable as deflation creates a wider range of model price points. In 2023, you had one or two API options. In 2026, you have dozens of models across five or more pricing tiers. A router that matches each query to the cheapest adequate model captures value from the entire pricing spectrum. As new, cheaper models emerge, the router automatically routes traffic to them.

Investing in fine-tuning becomes a moving calculation. Fine-tuning a smaller model to match the performance of a larger model saves the cost differential. But if the larger model's price drops by five times in a year, the differential shrinks, and the fine-tuning investment may not pay back. Fine-tune when the cost differential is large enough to justify the investment over a twelve-to-eighteen-month payback horizon, not over a three-year horizon that price deflation will disrupt.

Migrating from proprietary to open-source models is a structural decision that interacts with deflation in complex ways. Open-source model costs are primarily infrastructure — GPU instances and serving overhead. These costs deflate differently from API prices. Hardware costs deflate slowly, perhaps 30 to 40% per year. API prices deflate ten times per year or faster. The cost advantage of self-hosting narrows over time as API prices fall faster than hardware costs. A self-hosted solution that is 50% cheaper than the API today may be only 10% cheaper in two years. Factor this convergence into your build-versus-buy analysis.

## Predicting the Next Eighteen Months

Nobody can predict exact pricing. But the historical pattern is remarkably consistent, and you can use it for directional planning.

If frontier model input tokens cost two to five dollars per million today, expect them to cost twenty cents to one dollar per million in eighteen months. If mid-tier models cost fifty cents to two dollars today, expect them to cost five to twenty cents in eighteen months. If budget models cost five to twenty-five cents today, expect them to cost one to five cents in eighteen months.

These are not precise predictions. They are planning ranges based on the historical deflation rate. Use them to stress-test your cost projections. Build three scenarios: optimistic deflation at fifteen times in eighteen months, expected deflation at ten times, and pessimistic deflation at five times. See how your business model performs under each scenario. If your product is profitable only under optimistic deflation, your pricing is too aggressive. If your product is profitable even under pessimistic deflation, you have a solid cost structure.

The teams that plan for deflation have a structural advantage. They build lean architectures that benefit from every price drop. They avoid over-investing in micro-optimizations that deflation will erase. They structure their pricing to capture the upside of deflation rather than passing all savings to customers. And they reserve engineering capacity for the features that cheaper tokens will unlock, rather than spending all their engineering time squeezing savings from the current price regime.

## When Deflation Stops

Every deflationary trend eventually slows. Moore's Law sustained its pace for decades but began slowing in the 2010s. Token price deflation will also slow at some point, though predicting exactly when is difficult.

The most likely deceleration scenario is that frontier models become so expensive to train that providers need to maintain high prices to recoup investment. Training costs for the most capable models have been increasing even as inference costs decrease. If training a frontier model costs billions of dollars, the provider needs substantial revenue from API pricing to justify the investment. This creates a floor beneath frontier model prices even as mid-tier and budget models continue to deflate.

Another scenario is market consolidation. If the number of viable providers shrinks, competitive pressure decreases and deflation slows. Open-source models counteract this by maintaining competitive pressure from below, but if regulatory requirements make deploying open-source models in regulated industries impractical, the competitive pressure diminishes.

For planning purposes, assume deflation continues at roughly the historical rate for the next two to three years, with gradual deceleration after that. This gives you a planning window in which the strategies described in this chapter remain valid.

## Building Deflation into Your Financial Model

Your AI cost financial model should include a deflation assumption. Without it, your multi-year projections are meaningless because they assume today's prices persist indefinitely.

The model structure is simple. Start with current monthly spend. Apply a monthly usage growth rate — typically 10 to 25% for growing products. Apply a quarterly or semi-annual price adjustment when you migrate to newer, cheaper models. The net cost trajectory is usage growth minus price deflation.

If your usage grows at 15% per month and you capture a 50% price reduction every twelve months through model migration, your monthly spend grows at roughly 5 to 8% per month despite the price drops. If you also implement structural optimizations that reduce per-query token consumption by 30%, the effective growth rate drops to near zero. That is the goal: usage grows, prices drop, and your optimizations keep total spend flat or growing slower than revenue.

The financial model also helps you time your optimization investments. If a structural optimization will save $20,000 per month and takes two months to build, but prices will drop 50% in six months, the optimization saves $20,000 for four months at current prices, then $10,000 per month at reduced prices. Total savings over eighteen months: roughly $260,000, well worth the two-month investment. Run this analysis for every major optimization project to ensure the investment pays back even after deflation.

The next subchapter addresses the operational mechanism for turning all of these cost insights into enforceable constraints: token budgets, and how to set per-request and per-session limits that actually stick.