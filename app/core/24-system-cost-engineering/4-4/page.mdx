# 4.4 — Summarization as Cost Control: Compressing Conversation History and Retrieved Documents

Every token you send to the model costs money. In conversational systems, that cost grows with every turn. Turn one might be 500 tokens. Turn ten might be 4,000 tokens. Turn twenty, if you are sending the full conversation history, can blow past 8,000 tokens before the user even asks their question. The same problem appears in retrieval-augmented systems: you retrieve three documents at 2,000 tokens each, and you have spent 6,000 tokens on context before the model reads a single instruction. **Summarization as cost control** is the technique of compressing this accumulated context — conversation history, retrieved documents, or both — into a shorter representation that preserves the meaning your model needs while eliminating the tokens your budget does not. It is not a quality optimization. It is a cost optimization that, when done well, has a negligible quality impact and a dramatic cost impact.

The trade-off is real, though. Summarization is not free compression. You are making an extra model call to generate each summary. You are losing detail that the model might need for edge-case questions. You are introducing a new failure mode where the summary misrepresents the original content. The economics only work when the cost of the summarization call is less than the cost of sending the full context across all subsequent requests that would have used it. Get the math right and summarization is one of the most powerful levers in your cost engineering toolkit. Get the math wrong and you are paying more for worse quality.

## The Conversation Cost Growth Problem

Conversational AI systems have a structural cost problem that no other optimization fully solves: context grows with every turn. In a typical multi-turn system, each new request includes the system prompt, the entire conversation history, any retrieved context, and the user's latest message. The system prompt is fixed. The retrieved context varies but stays roughly constant. The conversation history, however, grows linearly with every exchange. If each turn — one user message and one assistant response — averages 400 tokens, the conversation history at turn five is 2,000 tokens, at turn ten it is 4,000 tokens, and at turn twenty it is 8,000 tokens.

The cost implications are direct. On a model priced at $3 per million input tokens, a request at turn five costs $0.006 in conversation history alone. At turn twenty, that same request costs $0.024 — four times as much — and the conversation history has become the dominant cost component, exceeding the system prompt, the retrieval context, and the user message combined. Multiply this across 100,000 conversations per day, with an average of twelve turns each, and the growing conversation history adds tens of thousands of dollars per month to your input token bill.

The problem compounds in customer support and enterprise assistant applications where conversations routinely reach twenty or thirty turns. A support conversation that starts as a simple question evolves into troubleshooting, escalation, and resolution. Each turn adds context. By the final turns, you are paying to send the entire troubleshooting history with every request, even though the model only needs to understand the current state of the problem and the most recent question.

Without intervention, conversation cost grows quadratically with length. Not because individual turns get more expensive, but because each turn carries the accumulated cost of all previous turns. Turn one sends one turn of history. Turn two sends two turns. Turn ten sends ten turns. The total tokens spent on conversation history across a ten-turn conversation is the sum of one through ten, which is fifty-five turns worth of tokens. For a twenty-turn conversation, the total is 210 turns worth. This is the quadratic curve that summarization breaks.

## How Conversation Summarization Works

The mechanism is straightforward. Instead of carrying the full conversation history forward indefinitely, you periodically compress it into a summary. After every N turns — typically five to ten — you make a separate model call that takes the full conversation history and produces a condensed version. That summary replaces the history in all subsequent requests. The model now sees the summary plus the recent turns, not the entire raw history.

A ten-turn conversation with a summarization window of five turns works like this. Turns one through five proceed normally, with the full history growing with each turn. After turn five, you trigger a summarization call. The model condenses the five turns into a summary of roughly 200 to 400 tokens, depending on conversation density. From turn six onward, the context includes the summary (300 tokens) plus the turns since the last summary (growing from zero to five turns). At turn ten, you summarize again: this time the model takes the previous summary plus turns six through ten and produces a new cumulative summary. The process repeats throughout the conversation.

The result is that context size never exceeds the system prompt plus the summary plus the unsummarized recent window. If your summary target is 300 tokens and your recent window is five turns at 400 tokens each, your conversation context is capped at roughly 2,300 tokens regardless of whether the conversation is ten turns or fifty turns. Without summarization, a fifty-turn conversation would carry 20,000 tokens of history. With summarization, it carries about 2,300. That is a reduction of more than 85%.

## The Summarization Call Economics

Summarization is not free. Every time you trigger a summarization call, you are making an additional model request. That request consumes input tokens — the conversation context being summarized — and generates output tokens — the summary itself. The summarization call has a cost, and that cost must be less than the savings it produces for the optimization to make economic sense.

The calculation is a break-even analysis. Suppose your conversation context at the summarization trigger point is 2,000 tokens. The summarization call costs 2,000 input tokens plus roughly 300 output tokens for the summary. On a model priced at $3 per million input tokens and $15 per million output tokens, that summarization call costs $0.006 in input and $0.0045 in output, totaling about $0.0105. That is the investment.

The return comes from every subsequent turn that uses the summary instead of the full history. Each subsequent turn saves 1,700 tokens in input — the difference between the 2,000-token full history and the 300-token summary. At $3 per million input tokens, each subsequent turn saves $0.0051. After just three turns, the savings exceed the cost of the summarization call. After ten turns, the net savings are $0.0405 — nearly four times the cost of the summarization. For a conversation that continues for another twenty turns, the net savings are $0.091 per conversation.

This is why the economics improve with conversation length. Short conversations of three to five turns rarely justify summarization because there are not enough subsequent turns to recoup the summarization cost. Long conversations of fifteen or more turns almost always justify it because every turn after the summarization point saves tokens. The optimal trigger point depends on your expected conversation length distribution. If most conversations are under eight turns, a summarization trigger at turn five only benefits the minority of longer conversations. If most conversations are fifteen to twenty-five turns, a trigger at turn five pays for itself many times over.

## Document Summarization for Retrieval Systems

The same principle applies to retrieved documents in RAG systems, but the economics are different. In a conversational system, summarization saves tokens across future turns within the same conversation. In a retrieval system, document summarization saves tokens across every request that retrieves that document.

Consider a knowledge base of 10,000 articles with an average length of 2,000 tokens each. When a user query triggers retrieval, the system fetches three relevant articles and includes them in the prompt. That is 6,000 tokens of retrieval context per request. If you pre-summarize each article to 400 tokens, the retrieval context drops to 1,200 tokens — a reduction of 4,800 tokens per request. At $3 per million input tokens, each request saves $0.0144. At 50,000 requests per day, that is $720 per day or $21,600 per month in input cost savings.

The summarization cost is a one-time investment per document. Summarizing 10,000 documents at 2,000 input tokens and 400 output tokens each costs approximately $0.012 per document, or $120 total. The entire corpus summarization pays for itself in less than five hours of production traffic. The ratio is so favorable because each document summary is used across thousands of requests. The cost is amortized across all retrievals.

Pre-computed document summaries also reduce latency because fewer tokens means faster model processing. And they reduce the risk of context window overflow for systems with many retrieval results. The trade-off is that summaries lose detail. A 2,000-token article that becomes a 400-token summary necessarily omits 80% of its content. If a user's question depends on a detail that was dropped during summarization, the model will produce an incorrect or incomplete answer. This is the quality risk that must be measured and managed.

## The Quality Trade-Off: What Summaries Lose

Summarization is lossy compression. A summary preserves the gist but drops specifics. For conversation history, this means the model loses access to exact phrasings, minor clarifications, and tangential topics that were discussed and then abandoned. For retrieved documents, it means the model loses access to supporting details, nuanced caveats, and specific data points that might be relevant to certain queries.

The quality impact depends on the task. For customer support conversations where the current issue and its status matter more than the exact wording of turn three, summarization has minimal quality impact. In testing across multiple production systems, teams consistently report that summarized conversation history produces responses that are indistinguishable from full-history responses for 85% to 95% of turns. The remaining 5% to 15% are cases where the user references something specific from early in the conversation — an exact error message they described, a specific product name, a date they mentioned — and the summary did not preserve that detail.

For document retrieval, the quality impact is more variable. Summaries work well when the user's question is about the document's main topic. They work poorly when the question is about a specific detail buried in a paragraph that the summary omitted. A legal document summary might capture the key obligations and deadlines but drop a specific exception clause. A technical document summary might capture the architecture overview but drop a performance benchmark. If the user's question is about that exception clause or that benchmark, the summary fails.

The mitigation is not to avoid summarization but to design your summarization strategy around the quality risk. The most effective approach is the **hybrid strategy**: use summaries as the default but fall back to full documents when the model's confidence is low or when the user explicitly requests detail. Some teams implement a two-pass retrieval: the first pass retrieves summarized documents to identify relevant candidates, and the second pass retrieves the full text of only the top candidate. This gives you the cost savings of summarization for the filtering step and the quality of full context for the generation step.

## The Hybrid Approach for Conversations

The most production-proven conversation summarization strategy is not pure summarization. It is a hybrid: summarize old context but keep recent turns verbatim. This approach recognizes that recent turns are disproportionately important. The user's most recent question, the model's most recent answer, and the two or three turns before that capture the current state of the conversation far better than any summary could.

The hybrid approach works as a **sliding window with summary prefix**. You maintain a window of the most recent N turns in full verbatim form. Everything before that window is summarized into a running summary. As the conversation progresses, turns age out of the verbatim window and get incorporated into the next summarization cycle. The model always sees a summary of the distant past and the full text of the recent past.

In practice, a window of three to five recent turns plus a summary of everything before that captures over 95% of the conversational context that matters. The user's short-term memory aligns with the window: they remember what they just said and what the model just answered. They do not typically reference exact wording from ten turns ago. When they do, they usually restate it. The summary handles the backstory — what the conversation is about, what has been resolved, what is still pending. The verbatim window handles the immediate context — the current question, the current thread of discussion.

This hybrid approach also simplifies the summarization prompt. Instead of asking the model to summarize the entire conversation in a way that preserves every possible detail, you ask it to summarize the older portion in a way that provides the backstory. The recent turns provide the detail. The cognitive load on the summarization call is lower, which means the summary is more reliable and more consistent.

## When Summarization Does Not Pay

Summarization is not universally beneficial. There are scenarios where the economics do not work and scenarios where the quality risk is too high.

Short conversations are the clearest case. If your average conversation is five turns or fewer, summarization after turn three saves tokens on only two subsequent turns. The summarization call itself might cost more than it saves. For systems where most interactions are quick question-and-answer exchanges, the overhead of summarization exceeds the benefit. You are better off sending the full history and accepting the linear cost growth because the conversations are short enough that the total cost is manageable.

Single-query retrieval systems are another case. If each user request is independent, with no conversation history and no reuse of retrieved documents, there is no opportunity for summarization to amortize its cost. You retrieve fresh documents for every query, use them once, and discard them. Summarizing them saves no future cost because there is no future request that benefits from the summary. In these systems, the cost optimization levers are retrieval precision and chunk size, not summarization.

High-precision domains where every detail matters are a third case. In medical, legal, and financial applications, a summary that omits a critical detail can cause real harm. If a legal assistant summarizes a contract and drops a liability clause, the user might make a decision based on incomplete information. In these domains, the quality risk of summarization may exceed the acceptable threshold. The cost savings of $20,000 per month look small next to a single legal liability caused by a missing clause.

Even in these domains, though, summarization is not entirely off the table. You can use summaries for routing and filtering, determining which documents or conversation threads are relevant, while using full text for the final generation. The cost savings are smaller because you still send full text at the end, but the filtering step eliminates a significant number of unnecessary full-text inclusions.

## Measuring Summarization Quality

You cannot deploy summarization without measuring its impact on response quality. The measurement approach has two components: summary faithfulness and downstream task quality.

**Summary faithfulness** measures whether the summary accurately represents the original content. Does it include the key points? Does it omit anything critical? Does it introduce any claims that were not in the original? You measure this by running a sample of summaries through an evaluation pipeline that compares the summary against the source material. Automated evaluation works well here: you can use a model-based judge to score summaries on completeness, accuracy, and conciseness. Target a faithfulness score above 90% and monitor it continuously. If faithfulness drops, your summarization prompt needs revision.

**Downstream task quality** measures whether summarized context produces responses as good as full context. This is the measurement that actually matters. You run an A/B evaluation where one variant uses full context and the other uses summarized context. You compare response quality across both variants using your standard eval metrics — accuracy, helpfulness, groundedness. If the summarized variant scores within 2% to 3% of the full-context variant, the quality impact is acceptable. If it drops by more than 5%, you need to adjust your summarization parameters: increase the summary length, expand the verbatim window, or restrict summarization to less detail-sensitive conversation types.

Run this evaluation before launch and continuously in production. The quality impact of summarization can drift over time as conversation patterns change. New product features, new customer segments, or seasonal traffic shifts can change what users ask about, which changes what details the summary needs to preserve. A quarterly re-evaluation of summarization quality catches drift before it becomes a customer-facing problem.

## Implementation Priorities

Start with the optimization that has the clearest economics. If you run a conversational system with average conversation lengths above ten turns, conversation summarization with a hybrid sliding window is your highest-return investment. The math is straightforward: calculate the token cost of your average conversation with and without summarization. The difference is your savings. Subtract the summarization call cost. If the result is positive, ship it.

If you run a RAG system where the same documents are retrieved frequently, pre-computed document summaries are your highest-return investment. The one-time summarization cost is trivial compared to the per-request savings across thousands of retrievals. Start with your most-retrieved documents — the top 10% by retrieval frequency — and measure the quality impact before expanding to the full corpus.

If you run both a conversational system and a RAG system, prioritize whichever one contributes more to your total input token cost. Pull the numbers from your cost attribution system. If conversation history accounts for 40% of input tokens and retrieval context accounts for 30%, start with conversation summarization. The numbers will tell you where the money is. Follow the money.

The next subchapter shifts from reducing input cost to reducing output cost — the more expensive side of the token equation. Output Length Control is where you constrain what the model generates to what the user actually needs and stop paying for words nobody reads.
