# 10.8 — Open-Source vs Proprietary Cost Analysis: The Full Total Cost of Ownership

Open-source models are free to download. They are not free to run, not free to maintain, not free to secure, and not free to keep current. This single misunderstanding has led more teams into budget overruns than any other decision in AI cost engineering. The moment you download a model, you accept a set of ongoing costs that are invisible in the download but very visible in your quarterly infrastructure bill.

The question is not whether open-source is cheaper — sometimes it is, dramatically so. The question is whether you have accounted for every cost category before making the decision, because the teams that get burned are always the ones who compared the API price of a proprietary model against the download price of an open-source one and concluded the math was obvious.

It isn't. The math is subtle, context-dependent, and full of costs that only appear after you have committed.

## The Proprietary Cost Model: Simplicity at a Price

The proprietary model — calling an API from Anthropic, OpenAI, Google, or another provider — has a cost model that is simple to understand and simple to predict. You pay per token. Input tokens have one price. Output tokens have another. You multiply your volume by the rate and you have your bill.

As of early 2026, the pricing tiers are well established. A frontier model like Claude Opus 4.6 costs $5 per million input tokens and $25 per million output tokens. A mid-tier model like Claude Sonnet 4.5 costs $3 and $15 respectively. A small model like Claude Haiku 4.5 costs $1 and $5. GPT-5's pricing follows a similar tiered structure. Gemini 3 Pro offers competitive rates, and Gemini 3 Flash provides a budget tier. The per-token prices have dropped substantially since 2023, with frontier-equivalent capability costing roughly 80 percent less than it did two years ago.

Batch processing discounts, prompt caching, and committed-use agreements reduce costs further — sometimes by 50 percent or more. Anthropic's prompt caching charges roughly 90 percent less for cached input tokens. Batch processing, where you submit requests and receive results hours later, typically costs 50 percent less than real-time inference. Committed-use agreements with minimum monthly spend can unlock an additional 10 to 30 percent discount depending on volume.

The simplicity of this model is itself a feature. You do not hire GPU infrastructure engineers. You do not manage Kubernetes clusters for model serving. You do not optimize CUDA kernels or tune vLLM configurations. You do not handle model version upgrades, quantization, or serving framework patches. You do not build disaster recovery for your inference infrastructure. The provider does all of this and amortizes the cost across thousands of customers, which is why they can offer per-token pricing that is lower than what most individual organizations could achieve running the same model themselves.

The downsides are equally clear. Cost scales linearly with volume. Every additional request costs the same as the last. There are no economies of scale on your side — the provider captures those economies, not you. You have no control over pricing changes, though competitive pressure has historically driven prices down. You have limited control over latency and throughput during peak demand. And you are subject to the provider's data processing practices, which may be a compliance concern in regulated industries.

## The Open-Source Cost Model: Freedom with Hidden Bills

The open-source cost model looks cheaper on paper because the most visible cost — the model license — is zero. You download Llama 4 Maverick, DeepSeek V3.2, Mistral Large 3, or any other open-weight model without paying a licensing fee. But the costs you take on in exchange for that zero are substantial, and they fall into five categories that most teams undercount.

**Infrastructure cost** is the largest and most predictable. Running a large language model requires GPUs, and GPUs are expensive. A 70-billion-parameter model like Llama 4 Scout requires at least two A100-80GB GPUs for inference at reasonable throughput, and more for high-concurrency production workloads.

Cloud GPU pricing has dropped significantly since the 2023 peak — H100 instances that cost $8 to $11 per hour in 2023 now run $2.85 to $3.50 per hour on major cloud providers, with budget providers like RunPod offering rates as low as $1.99. But the cost is still continuous. A two-GPU setup running 24 hours a day, 7 days a week costs roughly $4,100 to $5,000 per month on cloud instances. Scaling to handle peak traffic means provisioning additional capacity, which means paying for GPUs that sit partially idle during off-peak hours. Reserved instances reduce the hourly rate by 30 to 40 percent, but they require one-year or three-year commitments.

**Engineering cost** is the category teams underestimate most severely. Deploying an open-source model is not downloading a file and calling an endpoint. It requires setting up a serving infrastructure — typically vLLM, TGI, or a similar serving framework. It requires configuring quantization to fit the model onto available hardware while maintaining acceptable quality. It requires load balancing, auto-scaling, health checking, and restart logic. It requires monitoring for model performance, GPU utilization, memory usage, and request latency. It requires security hardening of the serving endpoint.

Each of these tasks requires an engineer with specific expertise in ML infrastructure. Industry data from late 2025 suggests that a production-grade open-source model deployment typically requires 0.5 to 1.5 full-time-equivalent engineers for ongoing maintenance — not for initial setup, but for the ongoing work of keeping the system healthy, updated, and performing. At a fully loaded engineering cost of $200,000 to $300,000 per year, that is $100,000 to $450,000 per year in engineering labor alone.

A common pattern: a team expects their ML engineer to spend 20 percent of their time on model serving and 80 percent on model improvement. Within three months, the ratio inverts. Serving infrastructure demands constant attention — GPU memory issues, driver updates, scaling problems during traffic spikes, performance regressions after framework updates. The engineer who was supposed to be improving the product spends most of their time keeping the infrastructure running.

**Maintenance cost** is what keeps the engineering number high over time. Models get updated. Llama 4 replaces Llama 3. Serving frameworks release new versions with critical bug fixes. GPU drivers need updates. Quantization techniques improve and the old quantized weights become suboptimal. Each update requires testing, validation, and rollout.

If you have fine-tuned the model, each base model update potentially requires re-running the fine-tuning process — re-creating the training pipeline, running the training job, evaluating the result, and deploying the new version. Unlike proprietary APIs where the provider handles all of this transparently, open-source maintenance falls entirely on your team.

**Opportunity cost** is the most insidious category because it never shows up on an invoice. Every hour your engineers spend maintaining model infrastructure is an hour they are not spending on product features, quality improvements, or customer-facing work. For a startup with five engineers, dedicating one to model infrastructure means 20 percent of the engineering team is solving inference plumbing instead of building the product. For a larger organization, the opportunity cost is lower as a percentage but still real: those ML infrastructure engineers could be fine-tuning models, building evaluation systems, or improving the product's AI capabilities.

**Compliance cost** exists because when you deploy a model yourself, you own the full stack. Under the EU AI Act, deployers of AI systems bear specific obligations around risk management, transparency, and human oversight. When you use a proprietary API, the provider handles compliance for the base model and you handle compliance for your application. When you deploy open-source, you are both the provider and the deployer for the model serving layer. This adds audit requirements, documentation requirements, and potentially the need for legal counsel to evaluate your obligations. These costs are not large in absolute terms, but they are non-zero and teams that assume open-source means less regulatory burden often find the opposite.

## The Crossover Calculation: When Does Open-Source Win

The **total cost of ownership** comparison comes down to a crossover point: the volume at which the lower marginal cost of self-hosted inference outweighs the higher fixed costs of infrastructure and engineering.

Consider a concrete example. A mid-tier proprietary model costs $3 per million input tokens and $15 per million output tokens. For a typical request with 800 input tokens and 400 output tokens, the cost is roughly $0.0084 per request. At 100,000 requests per day, the monthly proprietary cost is approximately $25,200.

Now model the same workload on self-hosted infrastructure. A two-GPU cloud instance running vLLM with a quantized 70B model costs $4,500 per month in compute. Monitoring, logging, and supporting infrastructure add $500 per month. Engineering maintenance costs $12,500 per month, which is half of one engineer's fully loaded cost. Total monthly cost: $17,500, with the capacity to handle 100,000 requests per day at acceptable latency. The per-request cost is $0.0058, roughly 31 percent cheaper than proprietary.

At this volume, open-source wins by about $7,700 per month. But the margin is thinner than the naive calculation suggested. Without the engineering cost, the infrastructure-only cost of $5,000 per month looks like an 80 percent savings over the $25,200 proprietary bill. With the engineering cost, the savings are 31 percent. And if the engineer's time turns out to be closer to full-time than half-time — which happens more often than teams plan for — the savings shrink to 10 percent or vanish entirely.

At lower volumes, the math flips. At 20,000 requests per day, the monthly proprietary cost drops to approximately $5,040. The self-hosted cost stays at roughly $17,500 because the infrastructure and engineering costs are mostly fixed — you still need the GPUs running and you still need the engineer maintaining them. Open-source costs 3.5x more than proprietary at this volume.

The crossover point for this specific scenario is approximately 60,000 to 70,000 requests per day, depending on how you account for engineering time. The crossover point is not universal. It shifts based on model size, GPU costs in your region, engineering labor costs, and whether you need the engineer for other work anyway. But the pattern is consistent: open-source requires volume to justify the fixed costs.

## The Scenarios Where Open-Source Clearly Wins

Certain conditions make the open-source cost advantage overwhelming, turning the marginal savings into massive annual numbers.

**Very high volume** is the clearest case. A product processing one million requests per day at $0.0084 per request spends $252,000 per month on proprietary inference. Self-hosting with a scaled GPU cluster, even including four to six GPUs and one full-time engineer, might cost $35,000 to $55,000 per month. The savings of $200,000 or more per month justify significant engineering investment. Companies processing tens of millions of requests per day — search engines, large consumer products, high-traffic platforms — almost always self-host because the volume makes the per-request economics irresistible.

**Stable requirements** amplify the advantage. If your model, prompt structure, and serving configuration rarely change, the ongoing engineering cost drops because maintenance is minimal. A product that fine-tuned a model once, deployed it, and handles the same type of requests month after month incurs lower engineering overhead than a product that constantly iterates on model versions, prompt designs, and tool integrations.

**Strong existing ML infrastructure team** eliminates the incremental engineering cost. If your organization already employs GPU infrastructure engineers who manage training clusters, adding model serving to their responsibilities is marginal work, not a new hire. The engineering cost in the TCO calculation drops from $150,000 to $300,000 per year to a fraction of that because the capability already exists.

**Data residency requirements** make self-hosting a compliance necessity rather than a cost choice. If your regulatory environment requires that data never leave your infrastructure — common in healthcare, government, and financial services operating under GDPR, HIPAA, or national security regulations — proprietary APIs may not be an option regardless of cost. In this case, self-hosting is the only path, and the cost comparison is against the alternative of not building the product at all.

## The Scenarios Where Proprietary Clearly Wins

Other conditions make the proprietary cost model the clear winner, even when the per-token math looks unfavorable.

**Low to medium volume** is the most common scenario. A product handling 10,000 to 50,000 requests per day spends $2,500 to $12,600 per month on a mid-tier proprietary model. Self-hosting the equivalent capability costs $15,000 to $20,000 per month when you include infrastructure and engineering. Proprietary wins by a wide margin, and the gap widens further when you account for the faster time-to-market and the absence of infrastructure risk.

**Small engineering teams** cannot afford to allocate an engineer to model infrastructure. A five-person startup that dedicates one engineer to model serving has reduced its product engineering capacity by 20 percent. The savings from self-hosting rarely justify that trade-off at startup scale. Use the proprietary API, focus every engineer on the product, and revisit self-hosting when volume justifies it.

**Fast-moving requirements** make proprietary APIs more cost-effective because switching models is trivial. If you need to evaluate GPT-5.2 versus Claude Opus 4.6 versus Gemini 3 Pro for a new feature, you change an API key and a model identifier. On self-hosted infrastructure, evaluating a new model means downloading it, quantizing it, deploying it to a serving framework, load-testing it, and potentially provisioning new hardware if the model requires different GPU configurations. The flexibility cost of self-hosting is real and it gets paid every time you need to change direction.

**Regulated industries** where the provider's compliance certifications reduce your burden favor proprietary. Major providers carry SOC 2, HIPAA BAA, ISO 27001, and other certifications that would cost your organization hundreds of thousands of dollars to achieve independently. When your compliance team asks "who is responsible for securing the model serving infrastructure," the answer "Anthropic, under their SOC 2 certification" is simpler and cheaper than "our team, under our own security program."

## The Hybrid Strategy

The most sophisticated cost engineers do not choose one side. They run a **hybrid strategy** that uses proprietary APIs for some workloads and self-hosted models for others, routing each request to the option that optimizes cost for that specific use case.

The pattern typically looks like this: high-volume, stable, latency-tolerant workloads run on self-hosted open-source models where the per-request economics are favorable. Low-volume, complex, or frontier-quality workloads run on proprietary APIs where the fixed costs of self-hosting are not justified. A routing layer directs each request to the appropriate backend based on the request type, required model capability, and current load.

This approach captures the best economics of both worlds. The high-volume workloads that drive most of the cost run at self-hosted rates. The complex workloads that need frontier capability get it without requiring frontier-class hardware. The routing layer adds modest complexity, but for organizations at sufficient scale — typically processing more than 200,000 diverse requests per day across multiple use cases — the savings justify the architecture.

A healthcare analytics company ran this hybrid approach through 2025. Patient intake summarization, which was high volume and stable, ran on a fine-tuned Llama 4 model on their own GPU cluster at $0.003 per request. Complex diagnostic reasoning, which was low volume and required frontier accuracy, ran on Claude Opus 4.5 at $0.028 per request. Clinical trial matching, which required the latest model knowledge and changed quarterly, ran on GPT-5 at $0.019 per request. The blended per-request cost across all workloads was $0.0068 — lower than any single option could achieve alone.

## The Middle Option: Managed Open-Source Platforms

The binary choice between "run it yourself" and "use a major provider's API" is no longer the only option. A third category has matured significantly since 2024: **managed open-source platforms** that host open-source models with API-style pricing.

Providers like Together, Fireworks, Anyscale, and others offer hosted Llama, Mistral, and DeepSeek models at prices 40 to 70 percent below frontier proprietary pricing, with none of the infrastructure management overhead. You get API simplicity — send a request, get a response, pay per token — but at rates closer to self-hosted economics because the underlying models are open-source and the provider achieves scale across many customers.

The per-token pricing on these platforms in early 2026 typically runs $0.10 to $0.30 per million input tokens and $0.30 to $1.00 per million output tokens for 70B-class models — a fraction of frontier proprietary pricing. Quality varies by model and task, but for use cases where a Llama 4 or Mistral Large 3 model meets quality requirements, the economics are compelling.

The trade-off is reduced flexibility compared to self-hosting and reduced capability compared to frontier proprietary models. You cannot fine-tune models on managed platforms as easily as on your own infrastructure. You do not get the latest frontier capabilities from Anthropic or OpenAI. But for the large middle ground of use cases where open-source quality suffices and self-hosting overhead is not justified, managed platforms offer a pragmatic option.

## The Decision Framework

Before choosing proprietary, open-source, managed, or hybrid, run the full TCO calculation for your specific workload. Here is the framework, broken into four steps.

First, calculate your proprietary cost. Take your current or projected daily request volume, multiply by the per-request cost of the model tier that meets your quality requirements, and extend to monthly. Include any committed-use discounts or batch processing savings you can negotiate.

Second, calculate your self-hosted cost. Include GPU compute at your target utilization rate, including the cost of excess capacity for peak handling. Include serving infrastructure — load balancers, monitoring, logging, storage. Include engineering labor at the number of FTEs required for initial deployment and ongoing maintenance. Include the one-time setup cost amortized over twelve months. Include the cost of model updates and re-quantization, typically two to four times per year.

Third, calculate the crossover. At what volume does the self-hosted total drop below the proprietary total? Is your current volume above or below that crossover? Where will your volume be in twelve months?

Fourth, assess the non-cost factors. Do you have the engineering team to support self-hosting? Are there data residency requirements that mandate it? Does your product require the flexibility to switch models frequently? Is your organization prepared for the compliance burden of deploying models directly?

The framework forces an honest comparison. When teams skip it and go with their gut — "open-source is free, obviously it's cheaper" or "APIs are simpler, obviously they're better" — they make decisions that cost tens or hundreds of thousands of dollars over the following year. Five hours of analysis prevents twelve months of regret.

## The Evolving Landscape

The open-source versus proprietary cost equation is not static. Three trends are reshaping it in 2026.

First, proprietary API prices continue to fall. The competitive pressure among Anthropic, OpenAI, Google, and others has driven frontier model prices down by roughly 80 percent since early 2024. Every price reduction raises the crossover volume, making proprietary viable for higher-volume workloads that would have required self-hosting two years ago.

Second, open-source model quality has converged with proprietary for many use cases. Llama 4 Maverick, DeepSeek V3.2, and Mistral Large 3 deliver quality that is within 5 to 7 points of frontier proprietary models on standard benchmarks. This convergence means the quality tax for using open-source has shrunk, making the cost comparison more relevant because you are comparing against models of similar capability rather than settling for a significant quality discount.

Third, the managed open-source category continues to grow, offering an expanding set of models at prices that challenge both self-hosting and proprietary APIs. These platforms shift the crossover point again, offering proprietary-style simplicity at open-source-adjacent pricing.

Revisit your TCO calculation every six months. The answer that was correct in January may be wrong by July. The team that locked into a three-year GPU reservation in 2024 based on that year's pricing may be paying more than the API rate by 2026. The team that committed to proprietary APIs at 2024 prices is paying a fraction of what they expected because prices dropped faster than anyone forecasted.

The decision about which models to run is a one-time calculation that needs periodic revisiting. Keeping costs controlled over time requires ongoing discipline. The next subchapter defines the recurring review cadence that keeps cost engineering alive in your organization week after week, month after month.
