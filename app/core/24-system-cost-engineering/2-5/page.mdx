# 2.5 â€” Batch API Pricing: When Asynchronous Processing Cuts Costs in Half

The pipeline finishes at 2:47 AM. Fourteen hours earlier, the team submitted 280,000 customer support tickets to an asynchronous batch endpoint for sentiment classification. The job ran overnight, each ticket classified into one of seven categories with a confidence score, and the results sit waiting in an output file. Total cost: $340. The same workload processed through the real-time API during business hours would have cost $680. Same model, same prompts, same quality. Half the price. The only difference is that the team did not need the results in milliseconds. They needed them by morning.

This is the economics of patience. Every major LLM provider in 2026 offers a batch API that trades latency for cost. You submit a collection of requests, the provider processes them asynchronously within a 24-hour window, and you pay 50 percent less on both input and output tokens. The discount is not modest. It is exactly half. For teams with any volume of work that does not require real-time responses, ignoring the batch API is the same as volunteering to pay double.

## How Batch APIs Work

The mechanics are straightforward. Instead of sending individual API requests and waiting for each response, you assemble a batch of requests into a single submission. Each request in the batch is a self-contained unit: it has its own system prompt, its own user message, its own parameters. The provider accepts the batch, queues it for processing, and returns a batch identifier. You poll the batch status or register a webhook. When processing completes, you retrieve the results, which map each request in the batch to its corresponding response.

OpenAI's Batch API accepts up to 50,000 requests per batch, with a total file size limit that scales with your tier. Anthropic's Message Batches API accepts up to 10,000 requests per batch. Google's Gemini batch processing works through Vertex AI with similar volume limits. All three guarantee completion within 24 hours, though in practice most batches complete in one to six hours depending on volume and model load.

The 50 percent discount applies to both input and output tokens. At standard Claude Sonnet 4.5 pricing of $3 per million input tokens and $15 per million output tokens, the batch pricing drops to $1.50 and $7.50 respectively. For Claude Opus 4.5, standard pricing of $5 input and $25 output becomes $2.50 and $12.50 through the batch API. OpenAI applies the same 50 percent discount across its model lineup. These are not promotional rates. They are a permanent pricing tier that reflects the operational efficiency providers gain from batch processing.

Why can providers offer this discount? Because batch requests are easier to schedule. The provider does not need to guarantee sub-second response times. They can queue batch requests during off-peak periods, fill spare GPU capacity, and process requests in whatever order is most efficient for their infrastructure. Real-time requests demand reserved capacity, low latency, and immediate scheduling. Batch requests demand only eventual completion. That flexibility has enormous value to the provider, and they pass half of it back to you.

## Identifying Batch-Eligible Workloads

Not every API call belongs in a batch. The deciding factor is latency tolerance. If the end user is waiting for the response, the request is not batch-eligible. If the response feeds into a pipeline that runs on a schedule, it almost certainly is.

The clearest batch candidates are data enrichment tasks. You have a database of customer records and you want to add sentiment tags, intent classifications, or quality scores. None of these need to happen in real time. You can process them overnight, write the results back to the database, and have enriched data ready for the morning's dashboards. A company processing 100,000 records per night at an average of 300 input tokens and 50 output tokens per request pays roughly $1.05 for input and $1.75 for output at standard Claude Sonnet 4.5 pricing, totaling $2.80 per nightly run. Switch to batch pricing and the same run costs $1.40. Over a year, that daily $1.40 savings adds up to $511. Now scale the numbers to a real enterprise workload of 500,000 records per night with a more capable model, and the annual savings reach well into five figures.

Bulk classification is another natural fit. Any time you need to categorize a large set of documents, messages, tickets, or records, batch processing is the default choice unless the classification result must be available within seconds of the input arriving. Teams doing compliance screening, content moderation on backlogs, or lead scoring on CRM imports should be running these through batch endpoints.

Synthetic data generation is perhaps the highest-value batch use case. Generating training data for fine-tuning, eval datasets, or test fixtures involves thousands to hundreds of thousands of generation requests. None of these are user-facing. None require real-time responses. A team generating 50,000 synthetic training examples at an average of 200 input tokens and 300 output tokens per example would pay approximately $30 for input and $225 for output at standard Claude Sonnet 4.5 pricing, totaling $255. Through batch pricing, the same generation costs $127.50, saving $127.50 on a single training data generation run. Teams that regenerate datasets weekly or monthly save thousands over the course of a year.

Evaluation runs are another major category. Running your eval suite across hundreds or thousands of test cases means hundreds or thousands of API calls. These can almost always be batched. Your eval pipeline does not need real-time responses. It needs correct responses at the lowest cost. Running evals through batch APIs cuts your eval infrastructure costs in half, which removes a common excuse for running evals less frequently than you should.

Report generation, content generation for email campaigns, document summarization for internal knowledge bases, and bulk translation jobs all follow the same pattern. If the work can wait hours instead of demanding milliseconds, it belongs in a batch.

## What Does Not Belong in a Batch

The boundary is clear but worth stating explicitly. Anything in the user's critical path cannot be batched. When a customer types a question into your support chatbot, they expect an answer in seconds, not hours. When a developer asks your code assistant for a suggestion, they expect it before they finish their next thought. When a patient interacts with a medical triage tool, the response must be immediate.

Conversational interfaces are inherently real-time. Each turn depends on the previous turn, and the user is waiting. You cannot batch a conversation. You also cannot batch any workflow where the next step depends on the model's response. If your pipeline calls the model to make a routing decision, and the routed request needs to happen immediately, the routing call must be real-time.

The gray area is near-real-time workloads. Some applications can tolerate latency of minutes but not hours. A content moderation system that needs to flag posts within five minutes of publication is not truly real-time, but a 24-hour batch window is too slow. For these workloads, batch APIs are not the right tool. You need the real-time API, potentially with other cost optimizations like prompt caching or model routing.

The key discipline is to audit your traffic and honestly categorize each endpoint. Most teams discover that 30 to 60 percent of their total API volume could be shifted to batch processing without any impact on user experience. The volume that can shift is almost always higher than teams initially estimate, because teams default to real-time processing out of habit, not out of necessity.

## Calculating Annual Savings at Scale

Let us work through a concrete scenario. A mid-size SaaS company uses Claude Sonnet 4.5 for three workloads. First, real-time customer chat handling 20,000 requests per day with an average of 800 input tokens and 400 output tokens per request. Second, nightly data enrichment processing 150,000 records per day with 500 input tokens and 100 output tokens per record. Third, weekly eval runs of 10,000 test cases with 600 input tokens and 200 output tokens per case.

The chat workload is real-time and stays on the standard API. At $3 per million input and $15 per million output, the daily cost is $48 for input and $120 for output, totaling $168 per day or roughly $61,300 per year.

The data enrichment workload is a perfect batch candidate. At standard pricing, daily cost is $225 for input and $225 for output, totaling $450 per day. Switched to batch pricing at $1.50 and $7.50, the daily cost drops to $112.50 for input and $112.50 for output, totaling $225 per day. The saving is $225 per day, or $82,125 per year.

The eval workload runs weekly. At standard pricing, each run costs $18 for input and $30 for output, totaling $48 per week or $2,496 per year. At batch pricing, each run costs $9 and $15, totaling $24 per week or $1,248 per year. The saving is $1,248 per year.

Total annual savings from shifting batch-eligible traffic: $83,373. The chat workload, which cannot be batched, stays at $61,300 per year. The total annual bill drops from $145,921 to $62,548 for the batch-eligible portion, plus the unchanged $61,300 for real-time, totaling $123,848 versus $145,921. That is a 15 percent reduction in the total bill, achieved by changing nothing about the model, the prompts, or the quality. You changed only the timing.

For companies with larger batch-eligible volumes, the savings scale linearly. A company processing a million records per night saves over half a million dollars annually just by routing those requests through the batch API. At that scale, the batch API is not an optimization. It is a requirement.

## Operational Trade-offs

Batch processing is not free of complexity. It introduces operational patterns that your team must handle correctly.

The first trade-off is error handling. In a real-time API call, you get an error immediately and can retry, fall back to a different model, or return a graceful degradation to the user. In a batch, errors surface hours after submission. A malformed request in a batch of 50,000 does not fail at submission time. It fails during processing, and you discover it when you retrieve the results. Your pipeline needs to handle partial batch failures gracefully, retrying failed requests without reprocessing the entire batch.

The second trade-off is lack of streaming. Batch responses arrive as complete objects, not as streams. If your downstream processing benefits from streaming, such as parsing partial output or making early termination decisions, you lose that capability with batch processing. For most batch workloads this does not matter, but it is worth noting.

The third trade-off is eventual consistency. When you process data in batch, the results are not available until the batch completes. If another system depends on those results, it needs to tolerate the delay. If your product dashboard shows enriched data, the enrichment reflects the last completed batch, not the current moment. You need to design your systems around this eventual consistency model, which may require status indicators, progress tracking, and clear communication to users about data freshness.

The fourth trade-off is batch size management. Submitting one batch of 200,000 requests is operationally different from submitting four batches of 50,000. Larger batches take longer to complete. If a large batch fails partway through, you may need to resubmit the entire remainder. Breaking work into smaller batches provides more granular progress tracking and faster partial results, but increases submission overhead. Finding the right batch size for your workload requires experimentation.

The fifth trade-off is debugging difficulty. When a single request produces an unexpected result in a real-time system, you can inspect the request and response immediately. In a batch system, tracing a problematic result back to its input requires good request identifiers and logging. Every request in your batch should have a unique identifier that you can use to match inputs to outputs and trace issues back to their source.

## Combining Batch Pricing with Other Discounts

Batch pricing stacks with other discount mechanisms, and the combination can produce dramatic savings.

Prompt caching, offered by both Anthropic and OpenAI, reduces input token costs for repeated content. If your batch of 100,000 requests all share the same 1,500-token system prompt, prompt caching ensures you pay full price for those 1,500 tokens only once. Subsequent requests pay a reduced rate for the cached portion, typically 90 percent less. When you combine prompt caching with batch pricing, you get the 50 percent batch discount on the already-reduced cached input price. For workloads with heavy prompt reuse, this combination can reduce input costs by 90 to 95 percent compared to standard real-time pricing.

The math is striking. Standard input pricing for Claude Sonnet 4.5 is $3 per million tokens. Prompt caching reduces the cached portion to $0.30 per million tokens. Batch pricing further halves that to $0.15 per million tokens. If 80 percent of your input tokens are the shared system prompt, and 20 percent are the unique query, your blended input cost per request drops from $3 per million to roughly $0.72 per million tokens. That is a 76 percent reduction in input costs before you even consider the output savings.

Teams that design their batch pipelines to maximize cache hit rates, by ordering requests to keep the system prompt consistent across large blocks of sequential requests, extract the maximum benefit from this stacking. The combination of batch pricing, prompt caching, and output constraints can reduce total per-request cost by 60 to 80 percent compared to an unoptimized real-time approach.

## Designing Your System to Identify Batch-Eligible Traffic

The batch discount is only useful if your system knows which traffic to route through the batch API. This requires architectural awareness.

Start with a traffic audit. For every endpoint in your system that calls an LLM, answer three questions. First, does the end user wait for this response? Second, does the next processing step depend on this response within seconds? Third, can this work be deferred by hours without business impact? If the answer to the first two questions is no and the answer to the third is yes, the endpoint is batch-eligible.

Most teams discover that their traffic falls into three clean categories. The first is unambiguously real-time: chat, search, live recommendations, anything where a human is waiting. The second is unambiguously batch-eligible: nightly jobs, weekly reports, bulk processing, eval runs, data enrichment. The third is the interesting one: workloads that are currently running in real time but do not need to be. A feature that updates a customer's risk score on every login could run nightly instead. A report that generates on demand could generate on a schedule. An enrichment that runs on record creation could run in hourly sweeps.

The third category is where the money is. These are the workloads hiding in your real-time traffic, paying full price, when they could be deferred and pay half. Identifying them requires conversations with product managers and stakeholders about what "real-time" actually means for each feature. Often it means "we built it that way because it was simpler," not "the user requires sub-second results."

Build your routing layer to make the batch-versus-realtime decision explicit. Tag each API call with its latency requirement. Route batch-eligible calls to a queue that accumulates requests and submits them as batches on a schedule. Route real-time calls to the standard API. This separation makes your cost structure visible and gives you a clear lever to pull when you need to reduce spend: move more workloads into the batch tier.

## The Organizational Discipline of Patience

Batch processing requires a cultural shift as much as a technical one. Engineers default to real-time because it is simpler to build and easier to debug. Product managers default to real-time because "faster is better." Stakeholders default to real-time because they do not think about the cost difference. Nobody asks "could this be asynchronous?" because nobody is incentivized to ask.

Building the discipline of patience means making the cost difference visible. When a product manager requests a new feature that involves LLM calls, the engineering team should present two cost estimates: real-time and batch. If the feature does not require real-time responses, the batch estimate will be half the real-time estimate. Presenting both numbers forces the conversation about whether real-time is truly necessary, and it gives the product manager a clear incentive to accept asynchronous processing when the feature allows it.

The teams that extract the most value from batch APIs are the ones that ask the question early. Not "should we use the batch API?" after the system is built, but "does this need to be real-time?" during the design phase. The answer shapes the architecture, the cost profile, and the operational model. Getting it right at design time is easy. Retrofitting batch processing into a system built for real-time is hard.

The next subchapter tackles a cost that is even more hidden than the batch discount is obvious: reasoning tokens, where your model thinks on your dime and the thinking never appears in the response.
