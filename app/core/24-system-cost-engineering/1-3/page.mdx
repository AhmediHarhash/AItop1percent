# 1.3 — The Six Cost Surfaces: Where Money Goes in AI Systems

Most teams believe model inference is where the money goes. They are wrong. Ask any engineering manager where their AI spend is concentrated, and they will point to the API invoice from OpenAI or Anthropic. That invoice is real, and it is large. But it is rarely the majority of total spend. Across hundreds of production AI systems surveyed and benchmarked in 2024 and 2025, model inference accounts for 25 to 45 percent of total cost in mature systems. The rest — the majority — hides in five other surfaces that most teams do not track, do not report, and do not optimize. The result is that engineering teams spend weeks squeezing ten percent out of their inference cost while ignoring the retrieval pipeline that costs more than inference, the evaluation layer that doubles the model bill, and the operational overhead that nobody budgets for.

**The Cost Surface Map** is the framework that makes all six surfaces visible. A cost surface is a distinct area of your AI system where money is spent. Each surface has its own cost drivers, its own scaling behavior, its own optimization levers, and its own monitoring requirements. When you map your spend across all six surfaces, you see where the money actually goes — not where you assume it goes. That visibility changes everything. It changes which optimizations you prioritize, which team owns which cost, and which surfaces are growing faster than revenue.

## Surface One: Model Inference

Model inference is the cost of calling language models to generate responses. It is the most visible cost surface because it comes as a line item on an API invoice or as a GPU utilization metric on your infrastructure dashboard. It is also the surface that gets the most optimization attention, partly because it is visible and partly because the levers are straightforward: switch to a cheaper model, reduce token consumption, cache frequent responses.

For API-based systems, inference cost is a function of model tier, input token count, and output token count. A system running Claude Opus 4.6 at $5 per million input tokens and $25 per million output tokens has a fundamentally different cost structure than one running Claude Haiku 4.5 at $1 per million input tokens and $5 per million output tokens. That is a 5x difference in cost for the same token volume. The model selection decision made in the first week of a project echoes through every invoice for the life of the product.

For self-hosted models, inference cost is a function of GPU type, GPU count, utilization rate, and hosting provider pricing. Running Llama 4 405B on eight NVIDIA H100 GPUs costs roughly $24 to $32 per hour depending on the cloud provider. At an average throughput of 15 requests per second with reasonable batching, that translates to approximately $0.0006 per request — dramatically cheaper than API pricing for the same model class. The catch is that you are paying the $24 to $32 per hour whether you serve 15 requests per second or zero. The per-request cost depends entirely on utilization. At 80 percent utilization, self-hosting is 5x to 10x cheaper than API pricing. At 20 percent utilization, it is only marginally cheaper and comes with the operational burden of managing GPU infrastructure.

Inference cost typically represents 25 to 45 percent of total cost per request in production systems that include retrieval, evaluation, and monitoring. In simple prompt-response systems without those components, inference cost can be 70 to 85 percent of total cost. The percentage tells you how mature your system is: if inference is 80 percent of your cost, you probably don't have evaluation, retrieval, or monitoring — and you should be worried about quality, not cost.

Streaming adds a subtle overhead that teams often miss. When you stream tokens to the user for real-time display, the connection stays open for the entire generation. This increases infrastructure cost because your servers hold connections longer, it complicates load balancing because long-lived connections cannot be rebalanced, and it increases your exposure to timeout-related retries. The streaming overhead is typically 3 to 8 percent on top of the base inference cost when you account for connection management and retry behavior.

## Surface Two: Tool and API Execution

Every time your AI system calls an external tool or API, that call costs money. A customer support agent that looks up order status calls your database API. A research agent that gathers current information calls a web search API. A coding assistant that validates syntax calls a linting service. A financial agent that checks stock prices calls a market data API. Each of these calls has its own pricing, its own latency, its own rate limits, and its own failure modes.

Tool execution cost is the second most underestimated surface because teams think of tool calls as "free" — they're calling their own APIs, after all. But those APIs run on infrastructure that costs money. A database query that runs on an RDS instance contributes to that instance's utilization. A search query that runs on an Elasticsearch cluster contributes to that cluster's cost. When AI systems start making thousands or millions of tool calls per day, the infrastructure serving those calls needs to scale accordingly, and that scaling has a cost.

External tool calls are more obviously expensive. A web search API like Brave Search or Bing charges $3 to $10 per thousand queries. A code execution sandbox charges $0.01 to $0.05 per execution depending on complexity and runtime. A document parsing API charges per page. A translation API charges per character. An agent that calls three to five external tools per task generates $0.02 to $0.15 in tool execution cost per task, on top of the inference cost.

In agentic systems, tool execution cost can rival or exceed inference cost. An agent that plans a ten-step task, calls two tools per step, and retries three of those calls generates twenty-three tool calls for a single user request. If each tool call averages $0.008, that is $0.184 in tool execution cost alone. A customer support agent that checks order status, looks up return policy, searches the knowledge base, and drafts a response generates four tool calls minimum. A research agent that searches multiple sources, extracts information, cross-references claims, and synthesizes a report can generate twenty to forty tool calls.

Tool execution cost typically represents 10 to 25 percent of total cost in agent-heavy systems and 5 to 10 percent in simpler systems. The surface grows disproportionately as you add agentic capabilities because agents, by definition, use tools — and every tool call costs money.

## Surface Three: Retrieval and Storage

Retrieval cost covers everything involved in finding, storing, and delivering context to the model. This surface includes four sub-components: embedding generation, vector storage, search queries, and reranking.

**Embedding generation** is the cost of converting text into vector representations for semantic search. Every document you ingest needs to be embedded. Every user query needs to be embedded. The embedding model charges per token. OpenAI's text-embedding-3-large charges $0.13 per million tokens. Smaller models like text-embedding-3-small charge $0.02 per million tokens. For a knowledge base with 500,000 documents averaging 2,000 tokens each, the initial embedding cost is $130 with the large model or $20 with the small model. That is a one-time cost for ingestion. But every user query also needs to be embedded, and that cost recurs with every request. At 100,000 queries per day averaging 200 tokens, query embedding costs $2.60 per day with the large model — $78 per month. Not enormous, but not zero.

**Vector storage** is the ongoing cost of keeping your embedded documents in a searchable database. Pinecone charges $0.096 per hour for a p1 pod that stores roughly 1 million vectors with 1,536 dimensions. That is $70 per month per million vectors. For a knowledge base with 5 million vectors, the storage cost is $350 per month before any queries. Weaviate, Qdrant, and Milvus have different pricing structures — self-hosted options reduce the monthly fee but add operational overhead. The choice of embedding dimensionality directly affects storage cost: a 768-dimensional embedding uses half the storage of a 1,536-dimensional embedding, which means half the vector database cost at the same document count.

**Search queries** have their own cost. Vector databases charge per query, per compute unit, or per resource allocation depending on the provider and plan. A high-traffic system making 500,000 vector searches per day needs sufficient compute to handle the query load with acceptable latency. The cost varies widely — from $200 per month for a small self-hosted deployment to $5,000 per month or more for a managed service handling millions of daily queries with sub-100ms latency.

**Reranking** is the cost of running a second model to reorder search results by relevance before passing them to the generation model. Not every system uses reranking, but those that do see meaningful quality improvements. A reranking model like Cohere Rerank charges per search, typically $1 to $2 per thousand searches. At 100,000 searches per day, that is $100 to $200 per day, or $3,000 to $6,000 per month. Cross-encoder reranking models that are self-hosted cost less per query but require dedicated GPU compute.

Retrieval cost as a whole typically represents 15 to 30 percent of total cost in RAG-based systems. Teams consistently underestimate this surface because each sub-component feels small in isolation. The embedding cost seems trivial. The vector database subscription seems like a fixed cost. The reranking cost seems marginal. But when you sum them — embedding generation plus storage plus search compute plus reranking — the retrieval surface often exceeds the inference surface, especially in systems with large knowledge bases and high query volumes.

## Surface Four: Infrastructure and Networking

Infrastructure cost covers the compute, storage, networking, and orchestration that runs your AI system independent of model inference. This surface includes the servers that host your API gateway, request queues, result caches, orchestration logic, and preprocessing pipelines. It includes the networking cost of moving data between services, between regions, and between your system and the model providers. It includes the autoscaling infrastructure that adds and removes capacity based on demand.

For API-first architectures where inference runs on the provider's infrastructure, the infrastructure surface is relatively small — typically 5 to 15 percent of total cost. You are paying for your own orchestration layer, not for the GPUs that run the models. But that 5 to 15 percent grows quickly as your system becomes more complex. Every microservice in your pipeline needs compute. Every queue needs memory. Every cache needs storage. Every inter-service call generates network traffic. A simple prompt-response architecture might have three services: gateway, orchestrator, and post-processor. A full RAG agent architecture might have twelve: gateway, router, classifier, retriever, reranker, context assembler, generator, evaluator, guardrail, formatter, logger, and delivery. Each service adds infrastructure cost.

Networking cost is the stealth expense on this surface. Cloud providers charge for data egress — typically $0.08 to $0.12 per gigabyte leaving a region. AI systems move substantial data: prompts with full context windows, retrieved documents, model responses, telemetry payloads. A system that sends 50,000 requests per day with an average payload of 20 kilobytes generates 1 gigabyte of egress per day to the model provider alone. At $0.09 per gigabyte, that is only $2.70 per day. But add response payloads, logging, cross-region replication, and CDN distribution, and the egress cost can reach $200 to $500 per month for a medium-traffic system. For multi-region deployments serving global users, egress costs scale with the number of regions.

Autoscaling adds cost variance. When traffic spikes, autoscaling launches additional instances. Those instances run at on-demand pricing, which is 2x to 3x reserved pricing. A traffic spike that lasts two hours might cost $50 to $200 in burst compute. If spikes happen daily — morning rush, product launch, marketing campaign — the burst compute cost accumulates. Teams that size their infrastructure for peak load waste money during off-peak hours. Teams that size for average load pay burst pricing during peaks. The optimal strategy is reserved capacity for the baseline plus autoscaling for peaks, but getting the baseline right requires historical traffic analysis that many teams do not perform.

Infrastructure cost typically represents 5 to 15 percent of total cost in API-first architectures and 30 to 50 percent in self-hosted architectures. The self-hosted number is higher because it includes GPU infrastructure, which is expensive whether the GPUs are busy or idle.

## Surface Five: Evaluation and Monitoring

Evaluation and monitoring cost is where teams are most consistently blind. They build evaluation into their system because they know quality matters, but they rarely track how much the evaluation itself costs.

**LLM-as-judge evaluation** is the largest component. When you use a language model to grade the quality of another model's output, you are making an additional model call for every evaluated request. If you evaluate 5 percent of production traffic using Claude Opus 4.6 as the judge, each evaluation costs approximately $0.06 to $0.12 depending on context length. On a system serving a million requests per month, that is 50,000 evaluation calls at $0.06 to $0.12 each — $3,000 to $6,000 per month in evaluation cost alone. That is often more than the cost savings from whatever optimization you ran last quarter. Research consistently shows LLM-as-judge offers 500x to 5,000x cost savings over human review, but it is not free, and the cost scales with both traffic and the quality standards you enforce.

**Human review** is more expensive per instance but typically covers fewer requests. If human reviewers audit 1 percent of traffic at a cost of $0.50 to $2.00 per review, the amortized cost is $0.005 to $0.02 per request. That sounds small until you calculate the monthly total. At a million requests per month, it is $5,000 to $20,000 per month for human review. The cost of the review team — salaries, training, tooling, management — is usually budgeted under operations, not under AI cost. This accounting separation means the true cost of quality assurance is invisible to the team tracking AI spend.

**Observability infrastructure** adds ongoing cost. Collecting, storing, and querying the telemetry data from an AI system is more expensive than for a traditional web service because AI telemetry is larger per event. You are storing prompts, completions, token counts, latency breakdowns, quality scores, and metadata for every request or a sampled subset. Observability platforms like Datadog, Honeycomb, or self-hosted solutions charge based on data ingestion volume. A well-instrumented AI system generating 10 kilobytes of telemetry per request at a million requests per month produces 10 terabytes of annual telemetry data. The storage, indexing, and querying cost for that volume ranges from $1,000 to $8,000 per month depending on the platform and retention policy.

Evaluation and monitoring cost typically represents 8 to 20 percent of total cost in systems that take quality seriously. Systems that skip evaluation save this cost — and pay for it in undetected quality degradation, customer complaints, and incident response. The money does not disappear. It shifts from evaluation cost to support cost and reputation cost.

## Surface Six: Operational Overhead

Operational overhead is the cost of running the humans and processes that keep the AI system working. It is the most difficult surface to quantify and the most commonly ignored in cost calculations.

**Incident response** is the most acute operational cost. When an AI system fails — a quality degradation, a hallucination spike, a cost anomaly, a security incident — engineers drop what they are doing and investigate. A medium-severity incident involving two engineers for four hours costs $600 to $1,200 in engineering time. A high-severity incident involving four engineers, a product manager, and a communication lead for eight hours costs $3,000 to $6,000. If your system experiences one medium incident and one high incident per month, the monthly incident response cost is $3,600 to $7,200. Amortized across a million requests, that is $0.004 to $0.007 per request. Not large, but not zero, and not tracked.

**Compliance labor** includes the time spent maintaining documentation, responding to audits, updating data processing agreements, and reviewing regulatory changes. For teams operating under HIPAA, GDPR, or the EU AI Act, compliance labor can consume one to two full-time equivalent headcount. At a fully loaded cost of $180,000 to $250,000 per year per headcount, that is $180,000 to $500,000 per year in compliance labor. Amortized across a high-volume system, the per-request cost is small. For a lower-volume system, it can be significant.

**Manual review and escalation** includes the cost of human agents handling AI escalations, quality reviewers checking flagged outputs, and domain experts providing corrections to improve the system. A customer support system that escalates 12 percent of conversations to human agents generates a manual review cost that, when amortized, can add $0.03 to $0.10 per request. A medical AI that requires physician review of 5 percent of outputs generates an even higher per-request overhead because physician time costs $150 to $300 per hour.

**Model management labor** includes the engineering time spent evaluating new models, testing model updates, managing prompt regressions, and maintaining the evaluation pipeline. A model update from a provider — which happens every four to eight weeks for major providers — triggers a regression testing cycle that consumes two to five engineering days. With three to four updates per quarter across multiple providers, the ongoing model management labor cost is one to two engineering weeks per quarter. At a fully loaded engineering cost of $120 per hour, that is $4,800 to $9,600 per quarter just for model update management.

Operational overhead typically represents 10 to 20 percent of total cost in mature AI systems. Immature systems undercount this surface because they have not yet built the processes — incident response, compliance, review — that generate the cost. As the system matures and these processes are formalized, operational overhead becomes visible and often becomes the second or third largest cost surface.

## Mapping Your Own Surfaces

The percentages given above are ranges based on industry patterns. Your system will have its own distribution. The first step in cost engineering is to map your actual spend across all six surfaces. Here is how.

For Surface One, pull your API invoices and GPU utilization data. This is the number most teams already know.

For Surface Two, instrument your tool calls. Track the number, type, and cost of every external API call and function invocation triggered by your AI system. Most orchestration frameworks — LangChain, LlamaIndex, custom agent loops — can be instrumented to emit cost telemetry for each tool call.

For Surface Three, sum your embedding costs, vector database costs, and reranking costs. These costs are often spread across multiple invoices and dashboards, which is why they are easy to miss. Pull them into a single view.

For Surface Four, allocate your infrastructure cost by service. What fraction of your compute, storage, and networking serves the AI system versus other products? For teams with dedicated AI infrastructure, this is straightforward. For teams sharing infrastructure, it requires tagging and allocation.

For Surface Five, track your evaluation model calls separately from your production model calls. If your evaluation calls go through the same API key as your production calls, the cost is invisible — it is just part of the API invoice. Set up separate tracking, separate API keys if possible, or at minimum separate cost tags for evaluation versus production inference.

For Surface Six, estimate your operational labor cost. How many engineering hours per month are spent on incident response, model updates, compliance, and manual review related to the AI system? Multiply by the fully loaded hourly rate. Divide by the number of requests to get the per-request overhead.

When you have all six surfaces mapped, you will see your system's cost profile clearly for the first time. The distribution will surprise you. The optimization opportunities will be obvious. And you will finally be able to answer the question that every finance team, every board member, and every product leader eventually asks: where does the money go?

The next subchapter dives deeper into the four cost categories — variable, semi-variable, fixed, and risk-adjusted — and teaches you how each category behaves as your system scales from hundreds to millions of requests.
