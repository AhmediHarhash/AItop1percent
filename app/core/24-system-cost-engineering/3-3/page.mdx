# 3.3 — Cost-Driven Model Routing: Sending Cheap Queries to Cheap Models

The goal of model routing is not to pick the best model. It is to pick the cheapest model that is good enough. This distinction sounds subtle but it changes everything about how you design the system. A team optimizing for "best" runs every request through a frontier model and hopes the budget holds. A team optimizing for "cheapest adequate" builds a routing layer that evaluates each request and sends it to the most economical model that meets the quality bar. The first team spends three to ten times more than the second team for output that users rate identically on 85% to 95% of requests.

Cost-driven model routing has matured rapidly through 2025 and into 2026. What was once a research curiosity — the LMSYS RouteLLM framework demonstrated 85% cost reductions on MT-Bench while maintaining 95% of GPT-4 quality — has become a standard production pattern. Every major AI gateway platform now supports some form of model routing. The question is no longer whether to route. It is which routing strategy fits your traffic, your latency constraints, and your tolerance for routing errors.

## The Anatomy of a Routing Decision

Every routing decision answers the same question: given this incoming request, which model tier should handle it? The answer depends on three variables. The complexity of the request — how much reasoning, context, or nuance does it require? The quality threshold for the task — what is the minimum acceptable output quality? And the cost of each candidate model — how much does each tier charge for this request's expected token profile?

The routing decision happens at the gateway layer, before the request reaches any model. The router inspects the request, makes a tier assignment, and forwards the request to the assigned model. The user does not know or care which model handles their request. They experience a response that meets the quality bar. You experience a cost per request that reflects the actual complexity of the work.

A well-designed router makes this decision in single-digit milliseconds. It adds negligible latency to the overall request. It adds a small per-request cost for the classification itself. And it saves multiples of that cost by avoiding frontier pricing on requests that do not need frontier capability. The economics are overwhelmingly favorable. The question is how to build a router that makes accurate tier assignments.

## Strategy One: Rule-Based Routing

Rule-based routing is the simplest approach and the one most teams should start with. The router applies a set of deterministic rules to classify each request and assign it to a model tier. The rules are based on observable features of the request: which API endpoint received it, how long the input is, what task type it belongs to, which customer or tier is making the request, or whether specific keywords or patterns are present.

The strength of rule-based routing is its predictability. You write the rules. You know exactly what they do. You can trace any routing decision back to a specific rule. There are no black boxes, no training data dependencies, and no model inference costs for the routing itself. A rule that says "all requests to the classification endpoint go to GPT-5-nano" costs nothing to evaluate and adds zero latency.

The practical implementation starts with your task taxonomy. If your product has distinct features — search, summarization, extraction, analysis, generation — each feature maps to a task type, and the eval work from the previous subchapter tells you which model tier handles each task type adequately. The routing rules encode these assignments. Requests tagged as classification go to the small tier. Requests tagged as summarization go to the mid tier. Requests tagged as complex analysis go to the frontier tier.

You can add complexity gradually. Within a task type, input length serves as a secondary routing signal. A summarization request with 500 input tokens might route to Haiku, while the same task type with 15,000 input tokens might route to Sonnet because the longer context benefits from a more capable model. Customer tier adds another dimension: enterprise customers on a premium plan might route to a higher model tier than standard customers, aligning inference cost with revenue.

The limitation of rule-based routing is rigidity. The rules capture what you knew when you wrote them. They do not adapt to new patterns in your traffic. A request that looks simple based on its endpoint and input length might actually require complex reasoning because of the content. A request that looks complex based on length might be simple because it is mostly boilerplate with a straightforward question at the end. Rule-based routing misclassifies these edge cases because it operates on surface features, not on understanding.

For most teams, the misclassification rate of a well-designed rule-based router is 10% to 20%. That means 10% to 20% of requests go to a tier that is either too expensive — wasting money — or too cheap — producing subpar output. Whether that error rate is acceptable depends on the cost savings. If rule-based routing saves 60% of your inference spend and produces quality failures on 5% of total requests, the economics strongly favor the router. If it saves 30% and produces failures on 15%, the case is weaker.

## Strategy Two: Classifier-Based Routing

Classifier-based routing replaces hand-written rules with a trained model that predicts which tier each request needs. The classifier takes the request as input and outputs a tier assignment. Because the classifier is itself a model, it can learn patterns that rules cannot express — subtle relationships between vocabulary, structure, query intent, and the complexity of the expected response.

The classifier is typically a small model. You do not route with a frontier model — that would defeat the purpose. A fine-tuned BERT-class model, a small transformer trained on your labeled routing data, or even a logistic regression model over engineered features can serve as the router. The inference cost of the classifier is negligible — typically $0.0001 to $0.001 per request, depending on the model size and hosting approach. The latency overhead is 5 to 20 milliseconds, which is imperceptible in the context of a model inference call that takes 500 to 3,000 milliseconds.

Training the classifier requires labeled data. You need a dataset of requests, each labeled with the correct tier assignment. The best way to build this dataset is to run a sample of production requests through all candidate model tiers, score the outputs with your eval suite, and label each request with the cheapest tier that met the quality threshold. This is the same evaluation process described in the previous subchapter, applied at the individual request level rather than the task-type level.

The strength of classifier-based routing is adaptability. The classifier captures nuances that rules miss. It learns that certain phrasings, certain topic areas, or certain request structures predict complexity better than input length alone. It adapts to your specific traffic distribution. And it can be retrained periodically as your traffic patterns evolve, keeping the router aligned with the actual complexity profile of your requests.

The limitation is the training and maintenance overhead. You need labeled data. You need a training pipeline. You need a serving infrastructure for the classifier. You need a monitoring system that detects when the classifier's accuracy degrades — because it will degrade as traffic patterns shift. This overhead is justified at scale but may be overkill for a team processing 20,000 requests per day. The typical threshold where classifier-based routing pays for itself over rule-based routing is somewhere around 100,000 to 200,000 daily requests, where the improved routing accuracy translates to enough absolute dollar savings to justify the investment.

A well-trained routing classifier achieves 85% to 92% accuracy on tier assignment, compared to 80% to 90% for a well-designed rule-based system. That 5 to 10 percentage point improvement in accuracy sounds modest, but at 500,000 daily requests it means 25,000 to 50,000 fewer misrouted requests per day. If each misrouted request costs an average of $0.02 in wasted frontier spend, the improved accuracy saves $500 to $1,000 per day, or $15,000 to $30,000 per month. Subtract the classifier's hosting cost — typically $500 to $1,500 per month — and the net savings are substantial.

## Strategy Three: Confidence-Based Cascading

Confidence-based routing takes a fundamentally different approach. Instead of predicting which tier a request needs before inference, it tries the cheapest model first and evaluates the result. If the cheap model's output meets a confidence threshold, the system returns that output. If it does not, the system escalates to the next tier and tries again. The process cascades upward through the tiers until a model produces output that meets the confidence bar, or until the system reaches the frontier tier as the final fallback.

The mechanism works like this. A request arrives. The router sends it to the small model — say, GPT-5-nano. The nano model generates a response. The system evaluates the response using a confidence signal. This signal might be the model's own token-level probabilities, a separate quality classifier that scores the output, or a structural check that verifies the output meets format and completeness requirements. If the confidence score exceeds the threshold, the nano response is returned to the user. If it falls below the threshold, the request is forwarded to the mid-tier model — say, Claude Sonnet 4.5. The same evaluation happens. If Sonnet's output passes, it is returned. If not, the request escalates to Opus.

The economic advantage of confidence-based routing is precision. The system does not predict which tier a request needs. It discovers which tier a request needs by empirically testing the cheapest option first. This eliminates the classification error that plagues rule-based and classifier-based approaches. If the cheap model handles the request well, the system confirms this directly rather than inferring it from proxy features.

The disadvantage is latency and cost overhead for complex requests. A request that ultimately needs the frontier tier is processed by the small model, evaluated, processed by the mid-tier model, evaluated again, and finally processed by the frontier model. That is three model calls for a single request. The total latency is the sum of all three inference calls plus evaluation overhead. For a request where the small model takes 300 milliseconds, the mid-tier model takes 800 milliseconds, and the frontier model takes 2,000 milliseconds, the cascaded path is over 3,100 milliseconds — compared to 2,000 milliseconds if the system had routed directly to the frontier model.

The latency penalty only applies to requests that escalate. If 65% of your requests are handled by the small model on the first try, 25% escalate to mid-tier, and only 10% reach the frontier tier, the average latency is dominated by the majority of requests that are handled quickly at the small tier. The median latency actually improves compared to sending everything to the frontier model, because most requests are served by a faster, smaller model. The tail latency — the worst case for the hardest requests — is worse. Whether this trade-off is acceptable depends on your latency requirements. Real-time conversational products where users stare at a loading indicator may not tolerate the tail latency. Batch processing or asynchronous workflows where latency is measured in seconds, not milliseconds, are ideal candidates.

The cost overhead of cascading also matters. Every escalated request pays for two or three model calls instead of one. The small model's cost on the escalated request is wasted — it produced output that was rejected. If the escalation rate is 35%, that means 35% of requests pay for at least one wasted model call. At GPT-5-nano's pricing, the wasted call costs about $0.0003 per request. At Haiku's pricing, it costs about $0.002. These are small amounts individually but they accumulate at scale and must be factored into the net savings calculation.

## Comparing the Three Strategies

Each routing strategy occupies a different point on the trade-off curve between implementation complexity, routing accuracy, latency impact, and cost savings.

Rule-based routing is the simplest to implement, requires no training data, adds zero latency, and typically saves 50% to 65% of inference costs compared to routing everything to a frontier model. It is the right starting point for every team and the right long-term solution for teams with clear task taxonomies and stable traffic patterns.

Classifier-based routing requires a training pipeline and labeled data, adds 5 to 20 milliseconds of latency, and typically saves 60% to 75% of inference costs. The improvement over rule-based routing comes from more accurate classification of ambiguous requests. It is the right choice for teams with high traffic volumes, mixed-complexity endpoints, and the engineering capacity to maintain a classifier.

Confidence-based cascading requires no training data but adds significant latency for escalated requests. It typically saves 55% to 70% of inference costs but with a wider variance depending on the escalation rate and the cost of evaluation. It is the right choice for asynchronous workloads where latency is flexible and for teams that want to avoid the training overhead of a classifier while still capturing the precision of empirical quality verification.

Many production systems combine strategies. A rule-based router handles the easy cases — requests to well-defined endpoints where the tier assignment is obvious. A classifier or cascade handles the ambiguous cases — requests to mixed-complexity endpoints where the optimal tier is not clear from surface features. This hybrid approach captures most of the cost savings from routing while limiting the latency and complexity overhead to the subset of requests that benefit from it.

## The Router's Own Economics

The router itself has costs. Rule-based routing has negligible per-request cost — the rules execute in microseconds on commodity compute. Classifier-based routing costs $0.0001 to $0.001 per request depending on the classifier's size and hosting. Confidence-based routing costs the price of the initial cheap model call plus the evaluation on every request, and the price of the intermediate model call on every escalated request.

These costs must be offset by routing savings. The net savings calculation is straightforward: take the total savings from routing requests to cheaper tiers, subtract the cost of the router itself and the cost of any misrouted requests. A positive number means the router pays for itself. A negative number means the routing overhead exceeds the savings, which happens only when the traffic volume is too low for routing savings to accumulate or when the routing accuracy is so poor that most requests end up at the frontier tier anyway.

A worked example makes this concrete. Consider a product handling 100,000 requests per day. Without routing, all requests go to Claude Sonnet 4.5 at an average cost of $0.018 per request, totaling $1,800 per day or $54,000 per month. With a rule-based router, 60% of requests route to Haiku at $0.006 per request and 40% stay on Sonnet at $0.018 per request. The routed daily cost is $360 plus $720, totaling $1,080 per day or $32,400 per month. The savings are $21,600 per month. The router itself costs nothing beyond the initial implementation effort.

Now add a classifier-based router for the 40% of requests that the rule-based router assigned to Sonnet. The classifier determines that half of those — 20% of total traffic — can actually go to Haiku. The classifier costs $500 per month to host. The additional 20,000 daily requests rerouted to Haiku save $240 per day, or $7,200 per month. Subtract the $500 classifier hosting cost, and the net additional savings are $6,700 per month. Total monthly savings with the hybrid approach: $28,300, or 52% of the original bill.

## Routing Accuracy and the Cost of Getting It Wrong

Routing errors come in two flavors, and they have very different consequences. An upward error routes a simple request to a more expensive model than necessary. The output quality is fine — probably excellent — but you overpaid. A downward error routes a complex request to a cheaper model that cannot handle it adequately. The output quality suffers, and the user has a bad experience.

Upward errors cost money. Downward errors cost trust. In most products, downward errors are far more dangerous because a single bad response can erode user confidence in a way that takes dozens of good responses to repair. The asymmetry means your routing system should be biased toward caution. When the router is uncertain about a request's complexity, it should err toward a more capable model, not a cheaper one. A 5% upward error rate is an acceptable cost of safety. A 5% downward error rate is a product quality problem.

The way to manage this asymmetry is to set different confidence thresholds for routing down versus routing up. The threshold for sending a request to a cheaper model should be high — you need strong confidence that the cheap model will handle it. The threshold for keeping a request at a more expensive model should be low — any uncertainty about the cheap model's ability is enough to justify the premium. This asymmetric threshold structure means you will overspend slightly on some requests but you will almost never underserve a user.

Monitoring routing accuracy in production requires a feedback loop. Sample a percentage of routed requests — 2% to 5% is typical — and run them through your eval suite on the assigned model tier. If the quality scores for a tier start dropping below the quality floor, the router is sending requests to that tier that it cannot handle. Investigate whether the traffic pattern has shifted, whether the quality threshold needs adjustment, or whether the routing rules or classifier need retraining.

## Building the Business Case for Routing

If you are trying to convince your organization to invest in model routing, the business case builds itself from the numbers. Calculate your current monthly inference spend. Estimate the percentage of traffic that could be handled by a cheaper tier — even a conservative estimate of 50% is usually defensible. Multiply the rerouted traffic volume by the cost difference between your current model and the cheaper tier. Subtract the implementation cost, which is typically two to four weeks of engineering effort for rule-based routing or four to eight weeks for classifier-based routing. The payback period for most teams is one to three months.

Present the case in annual terms. Saving $15,000 per month is a $180,000 annual saving. That is a headcount. That is a product launch budget. That is the margin that makes a feature viable or an enterprise contract profitable. CFOs and VPs of Engineering do not get excited about token pricing. They get excited about hundred-thousand-dollar line items that disappear from the budget.

The case becomes even stronger when you frame it as a prerequisite for growth. Without routing, your inference costs scale linearly with traffic. Double the traffic, double the bill. With routing, your costs scale at a fraction of the traffic growth rate because the marginal cost of each new request reflects its actual complexity, not the price of your most expensive model. Routing is not just a cost optimization. It is the mechanism that makes your unit economics survive at scale.

The next subchapter takes routing one step further with cascading inference — the pattern where your system tries the cheapest model first, checks whether the output is good enough, and escalates only when it is not.
