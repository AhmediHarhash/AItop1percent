# 3.9 â€” Agent Inference Costs: Why Autonomous Systems Are Unpredictably Expensive

An agent does not have a cost per request. It has a cost distribution per request, and the tail of that distribution will surprise you. A multi-model pipeline, no matter how complex, has a bounded cost: you know how many steps there are, you know which models each step uses, and you can calculate the total. An agent does not give you that luxury. The agent decides, at runtime, how many model calls it needs to complete the task. A simple question might take two calls. A moderately complex task might take eight. An ambiguous request with tool failures and retry loops might take forty-five calls before the agent either succeeds or hits a timeout. You cannot predict the cost of a single request because the agent's behavior is contingent on the input, the tool responses, the intermediate reasoning, and sometimes random variation in the model's decision-making.

This unpredictability is not a flaw in agent design. It is the fundamental nature of autonomous reasoning systems. An agent that always takes the same number of steps regardless of task complexity is not really reasoning. It is following a fixed pipeline with a different name. True agency means adaptive behavior, and adaptive behavior means variable cost. The engineering challenge is not eliminating the variability. It is understanding it, bounding it, and building financial controls around it so that your system remains economically viable even when agents take unexpected paths.

## Why Agent Costs Follow Heavy-Tailed Distributions

**Agentic systems** generate cost distributions that are heavy-tailed, meaning a small percentage of requests consume a disproportionately large share of total inference spend. In a typical production agent deployment, the median request might cost $0.04. The 90th percentile might cost $0.25. The 99th percentile might cost $1.50. And the 99.9th percentile might hit $8.00 or more. These are not hypothetical numbers. They reflect the reality that most tasks an agent handles are straightforward, requiring two to five reasoning steps, while a minority of tasks are genuinely hard, requiring extended reasoning chains, multiple tool calls, error recovery, and iterative refinement.

The heavy tail emerges from several reinforcing dynamics. The first is **reasoning depth variability**. When a user asks "what's the weather in Tokyo," the agent makes one tool call and one generation call. When a user asks "compare the quarterly revenue growth of the top five SaaS companies by market cap and identify which one has the best gross margin trajectory," the agent needs to identify the companies, look up financial data for each, calculate growth rates, compare them, and synthesize a response. The second query might require fifteen to twenty model calls, each building on the output of the previous one.

The second dynamic is **tool call frequency and failure rates**. Agents use tools: search APIs, databases, calculators, code interpreters, file systems. Each tool call requires at least one model call to decide which tool to use and how to call it, plus another model call to interpret the result. If a tool call fails, the agent reasons about the failure and retries, adding two or more model calls per failure. In production, tool failure rates of five to fifteen percent are common. An agent that makes ten tool calls and experiences two failures might spend six additional model calls on error recovery alone.

The third dynamic is **context accumulation**. As an agent works through a multi-step task, it accumulates context: previous reasoning steps, tool call results, intermediate outputs. This context grows with each step, and the model processes the full context on every subsequent call. Step one might process 500 tokens. Step five might process 3,000 tokens. Step fifteen might process 12,000 tokens. The per-step cost increases as the agent works longer because the input token count grows with each iteration. This means that long-running agent sessions are not just more expensive because they have more steps. They are more expensive per step because each step processes more context.

The fourth dynamic is **exploration and backtracking**. Sophisticated agents sometimes explore multiple approaches to a problem, abandoning unsuccessful paths and trying alternatives. This exploration is valuable when it leads to better outcomes, but it multiplies cost. An agent that tries two approaches before finding the right one has roughly tripled its model call count for that portion of the task. An agent that tries three approaches has quadrupled it. Exploration is especially common with ambiguous inputs where the right approach is not obvious from the initial request.

## The Cost Anatomy of an Agent Session

Breaking down the cost of a single agent session reveals where the money goes. Consider a production research agent that receives the request: "Find the three most recent peer-reviewed studies on the effect of sleep deprivation on cognitive performance in shift workers, and summarize the key findings."

Step one: the agent parses the request and formulates a search strategy. One model call, approximately 400 input tokens and 200 output tokens. Step two: the agent calls a search tool with the first query. One model call to formulate the query, one tool call, one model call to interpret results. Approximately 600 input tokens and 300 output tokens across the model calls. Step three: the agent evaluates the search results and determines that only one of the results is peer-reviewed. It refines the search query and searches again. Two model calls, growing context. Step four: the agent identifies three candidate papers and calls a document retrieval tool for each. Three tool calls, three model calls to interpret, growing context at each step. Step five: the agent reads the abstracts and determines one paper is about sleep deprivation in medical residents, not shift workers specifically. It searches for a replacement. Two more model calls. Step six: the agent now has three valid papers and begins summarizing. One model call for each paper's summary, then one model call to synthesize the overall summary. Four model calls with substantial context.

Total: approximately fourteen model calls, with cumulative context growing from 400 tokens on step one to over 8,000 tokens by the final synthesis. The total cost depends on the model, but at GPT-5-mini pricing the session might cost $0.12. At GPT-5 pricing, the same session might cost $0.85. And this was a successful session with only one backtracking step. If the search tool had returned poor results repeatedly, or if the papers required full-text analysis instead of abstract-level summarization, the session could easily have doubled or tripled in cost.

Now consider the same agent handling a harder query: "Analyze the methodology quality of the five most cited meta-analyses on sleep deprivation and cognitive performance published between 2020 and 2025, and identify common limitations." This query requires deeper tool interaction, more document retrieval, more complex reasoning, and more synthesis. The agent might make thirty to forty model calls with context growing to 15,000 or 20,000 tokens by the final steps. The cost could reach $3.00 to $5.00 at frontier model pricing. Both queries arrived through the same interface, were handled by the same agent, and look similar to the end user. But one costs forty times more than the other.

## Cost Drivers You Must Monitor

Four metrics determine whether your agent system stays within budget or spirals into uncontrolled spending.

The first is **steps per session**. Track the number of model calls per agent session as a distribution, not just an average. The average hides the tail. If your average is six steps but your 99th percentile is thirty-eight steps, you have a cost control problem that the average obscures. Plot the distribution weekly and watch for shifts. A rising 95th percentile often signals that agents are encountering harder tasks, experiencing more tool failures, or getting stuck in reasoning loops.

The second is **tokens per step**. As agents accumulate context, each successive step processes more tokens. Track the average tokens per step across the session, broken down by step number. If step one averages 500 tokens but step ten averages 9,000 tokens, the later steps cost eighteen times more per step than the early ones. This context growth is the silent cost multiplier in agent systems. Teams that only track total steps miss the fact that the later steps are dramatically more expensive per step.

The third is **tool failure rate**. Every tool failure triggers at least two additional model calls: one to diagnose the failure and one to retry or adapt. A tool with a ten percent failure rate adds roughly twenty percent more model calls to sessions that use that tool. Monitor failure rates per tool and prioritize reliability improvements for the tools with the highest failure rates and the highest usage frequency. A search API that fails eight percent of the time and is called in ninety percent of sessions is a more urgent fix than a calculator tool that fails two percent of the time and is called in five percent of sessions.

The fourth is **session timeout rate**. Sessions that hit the timeout or step limit without completing represent both wasted cost and failed user experience. Track the percentage of sessions that reach your step limit or token budget. If more than three to five percent of sessions are timing out, either your limits are too tight, your agent is not efficient enough, or certain task types are fundamentally beyond your agent's capabilities at the current cost constraints.

## Cost Controls: Bounding the Unbounded

Because agent costs are inherently variable, you need hard controls that prevent runaway spending. Soft guidance and optimization are not enough. You need circuit breakers.

The most fundamental control is a **step limit**. Set a maximum number of model calls per agent session. When the agent reaches this limit, it must either return its best current answer or escalate to a human. The step limit should be set based on your cost tolerance and the value of a completed task. If a completed task is worth $2.00 to your business and your per-step cost averages $0.06, a step limit of thirty gives you a maximum session cost of $1.80, which is within your value threshold. If a completed task is worth $0.50, a step limit of eight keeps you at $0.48. The step limit must be a hard constraint, not a suggestion. Agents optimized for task completion will cheerfully exceed any soft limit if the task remains incomplete.

The second control is a **token budget per session**. Instead of limiting steps, limit the total tokens consumed across all model calls in the session. This is more precise than step limits because it accounts for context growth. A session with five steps and 50,000 total tokens consumed costs the same as a session with twenty steps and 50,000 total tokens. The token budget lets you cap cost directly regardless of how the agent distributes its reasoning across steps. Set the budget based on your per-session cost target: if your target is $0.30 and your blended token cost is $6 per million tokens, your budget is 50,000 tokens.

The third control is a **cost circuit breaker** that monitors cumulative session cost in real time and terminates sessions that exceed a threshold. Unlike step limits or token budgets, which are proxies for cost, the circuit breaker tracks actual dollar cost by multiplying token consumption by price at each step. This is the most accurate control because it accounts for model pricing differences, context growth, and input-output token ratio changes across steps. The circuit breaker should fire a warning at eighty percent of the threshold and hard-terminate at one hundred percent.

The fourth control is **timeout enforcement**. Set a maximum wall-clock time for agent sessions. Even if the step limit and token budget are not exceeded, a session that has been running for ten minutes is consuming compute resources, holding state in memory, and potentially blocking downstream processing. Timeout enforcement prevents zombie sessions that the other controls might miss, particularly sessions where tool calls take unexpectedly long to return.

## The Fundamental Tension: Autonomy vs Cost

Every agent cost control reduces the agent's ability to complete complex tasks. A step limit of eight means the agent cannot handle tasks that genuinely require twelve steps. A token budget of 30,000 means the agent cannot accumulate the context needed for deep analysis. A tight cost circuit breaker means the agent will abandon high-value tasks that happen to be expensive. This is the fundamental tension in agent cost engineering: the controls that protect your budget also limit your agent's capabilities.

The resolution is not to remove controls. It is to make them adaptive. Implement tiered cost limits based on task value. A customer service agent handling a routine question gets a step limit of five and a cost budget of $0.15. The same agent handling an enterprise customer's complex escalation gets a step limit of twenty-five and a cost budget of $2.00. A research agent running a low-priority background task gets strict limits. The same agent running a user-initiated high-priority investigation gets relaxed limits. The tier is determined at session start based on the input characteristics, the user context, or explicit priority flags.

Some teams implement **progressive budgets** where the agent starts with a small budget and can request extensions. The agent completes the first five steps within an initial budget of $0.15. If the task is complete, it returns the result. If the task is incomplete and the agent believes more steps would be productive, it submits a budget extension request. The extension request includes the agent's estimate of additional steps needed and the likely cost. The system approves or denies the extension based on task priority, remaining daily budget, and historical completion rates for similar extensions. This approach gives agents autonomy to handle complex tasks while maintaining financial oversight.

## Modeling Agent Unit Economics

To build a viable business around an agentic product, you need to model your unit economics around the cost distribution, not the average. If your median session cost is $0.04 and you price your product at $0.10 per request, you are profitable on the median request. But if your 95th percentile cost is $0.80, five percent of requests are deeply unprofitable, and those five percent can consume more total spend than the other ninety-five percent combined.

Calculate your blended cost per request by summing cost across the full distribution: take the total inference spend for a month and divide by total sessions. If you spent $42,000 on inference and served 600,000 sessions, your blended cost is $0.07 per session. Your pricing must exceed $0.07 with enough margin to cover infrastructure, engineering, and profit. But you also need to ensure that the tail does not grow. If a product change or user behavior shift causes the 99th percentile to move from $1.50 to $4.00, your blended cost might jump from $0.07 to $0.11, eliminating your margin.

The safest approach is to price based on the 90th or 95th percentile cost rather than the median or mean. If your 90th percentile cost is $0.25, pricing at $0.50 per request gives you margin even on expensive sessions. You will be highly profitable on simple requests and modestly profitable on complex ones, which creates a sustainable blended margin. Some teams use tiered pricing that charges more for complex requests, aligning price with the cost of fulfillment.

Monitor your cost distribution weekly. Plot the 50th, 90th, 95th, and 99th percentile costs over time. Watch for upward drift in any percentile. A rising 99th percentile often precedes a rising 95th, which precedes a rising mean. By the time the mean increases noticeably, you have been losing money on the tail for weeks.

## Real-World Agent Cost Patterns

Production agent deployments in 2026 show consistent patterns. Customer service agents, which handle structured queries against a knowledge base, have relatively tight cost distributions. The median cost is $0.03 to $0.08, the 95th percentile is $0.20 to $0.50, and the 99th percentile is $0.60 to $1.50. The distribution is tight because most customer service queries are similar in complexity and the tool set is constrained.

Research and analysis agents have much wider distributions. The median cost is $0.10 to $0.30, the 95th percentile is $1.00 to $3.00, and the 99th percentile can reach $5.00 to $15.00. The distribution is wide because research tasks vary enormously in complexity, require multiple external data sources, and often involve iterative refinement.

Coding agents have the widest distributions of all. A simple code change might cost $0.05. A complex multi-file refactoring might cost $10.00 or more, particularly when the agent runs test suites, interprets failures, and iterates on fixes. The cost distribution of a coding agent looks less like a bell curve and more like a power law, where a small fraction of tasks dominate total spend.

Understanding which cost pattern your agent follows is essential for pricing, budgeting, and setting appropriate controls. A customer service agent can tolerate tighter controls because the distribution is narrow. A research agent needs wider controls and more generous budgets because tight controls will truncate too many legitimate tasks. A coding agent needs the widest controls of all, combined with the most aggressive monitoring, because the tail extends further than any other agent type.

Agent systems are the most expensive and least predictable inference workloads in the AI cost landscape. But they are also, increasingly, the most valuable. The key is not to avoid agent costs but to understand, measure, and control them. The tools for doing so are step limits, token budgets, cost circuit breakers, adaptive tiers, and relentless monitoring of the cost distribution.

Agents are unpredictable at the individual request level, but at the portfolio level, patterns emerge. Those patterns, and the volume thresholds where they demand entirely different strategies, are the subject of the next subchapter.
