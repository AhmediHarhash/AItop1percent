# 2.8 — Token Waste: Identifying and Eliminating Tokens That Add No Value

**The Copy-Paste Prompt** is the most common and most expensive anti-pattern in production AI systems. It works like this: an engineer writes a system prompt for a feature. It works. Another engineer needs a prompt for a different feature. They copy the first prompt, change a few lines, and ship it. Six months later, every feature in the product has a system prompt that carries 400 tokens of instructions originally written for a completely different use case. The model reads those instructions on every request, processes them, and ignores most of them because they are irrelevant to the current task. The team pays for those 400 tokens on every single API call. At 100,000 requests per day, that is 40 million wasted tokens per day — tokens that add absolutely nothing to the output quality.

This is **token waste**: tokens that are sent to or generated by the model but contribute zero value to the quality of the response. Token waste is not a rounding error. In high-volume production systems, it routinely accounts for 15 to 35% of total token spend. A team spending $80,000 per month on API costs may be wasting $12,000 to $28,000 per month on tokens that could be eliminated with no impact on output quality. The money goes to processing instructions the model does not need, context the model does not use, and output formatting the user does not read.

## The Six Sources of Token Waste

Token waste comes from predictable sources. Once you know what to look for, you find it everywhere.

The first source is **duplicated instructions**. System prompts accumulate instructions over time. Someone adds "always respond in a professional tone." Someone else adds "maintain a professional and helpful demeanor." A third person adds "your responses should be professional." Three instructions that say the same thing, consuming tokens three times. In mature systems, prompt audits routinely find 20 to 30% of system prompt tokens are duplicated or near-duplicated instructions. These instructions were added at different times by different people, and nobody removed the older versions when the newer ones were added.

The second source is **irrelevant retrieved context**. In RAG systems, the retrieval step pulls documents or passages that are semantically similar to the query. But semantic similarity does not guarantee relevance. A user asks about cancellation policies for annual subscriptions, and the retriever pulls three passages: one about annual subscription cancellation, one about monthly subscription cancellation, and one about subscription pricing. The third passage is semantically similar — it mentions subscriptions — but it does not answer the question. The model reads 200 tokens of pricing information, determines they are irrelevant, and ignores them. You paid for those 200 tokens for nothing.

The third source is **few-shot examples that do not match the task**. Few-shot examples are powerful when they are relevant. They become waste when they are generic. A classification prompt includes five examples covering five categories. The incoming query clearly belongs to category three. The model needs only the category-three example to understand the pattern. The other four examples consume 400 tokens and add no incremental value because the model already understood the task from the system prompt. Static few-shot examples that never change regardless of the input are a common source of waste.

The fourth source is **verbose output formatting**. If you instruct the model to "provide a detailed explanation with context, reasoning, and a final recommendation" when the user needs only the recommendation, you are generating output tokens you do not need. Output tokens are three to five times more expensive than input tokens in most pricing models. Every unnecessary sentence in the model's response costs three to five times what an unnecessary sentence in the prompt costs. Teams that prompt for verbose responses and then discard most of the text in their application layer are paying premium prices for text nobody reads.

The fifth source is **defensive repetition in prompts**. Engineers who do not trust the model add the same instruction multiple times in different phrasings. "Remember, you must not disclose internal pricing. Under no circumstances should you share pricing details. If the user asks about pricing, do not reveal internal numbers." Three sentences that say the same thing. This pattern comes from anxiety about model behavior, and it is understandable — but the model does not become more obedient because you repeated the instruction three times. One clear instruction works as well as three redundant ones. The other two are pure waste.

The sixth source is **stale conversation history**. As covered in the previous subchapter, multi-turn conversations carry forward the entire history. But within that history, many turns are no longer relevant. The user asked about shipping in turn three, the model answered, and the conversation moved on to billing. Turns three and four are sitting in the context window, consuming tokens on every subsequent request, contributing nothing to the billing discussion. This is conversation-level token waste, distinct from the compounding problem — it is not that the history is too long, it is that specific parts of the history have zero remaining value.

## Detecting Token Waste: The A/B Removal Test

The most reliable way to detect token waste is the **A/B removal test**. The method is straightforward: remove a suspected source of waste and measure whether output quality changes. If quality stays the same, the removed tokens were waste. If quality drops, they were not.

The test requires a quality measurement. You need an eval set — a collection of representative queries with known-good responses — and a scoring mechanism. Send each query through the system with the full prompt and through the system with the suspected waste removed. Compare the quality scores. If the scores are statistically equivalent, the removed tokens are confirmed waste and can be permanently eliminated.

Start with the largest suspected sources. If your system prompt is 1,200 tokens, test removing sections of it. Remove the duplicated instructions and run the eval. Remove the generic boilerplate at the top and run the eval. Remove the defensive repetitions and run the eval. Each test isolates a specific source of potential waste.

A customer support platform ran this test on their system prompt. The original prompt was 1,400 tokens. They identified three sections as potential waste: a 180-token section of general instructions duplicated from an older prompt, a 120-token section of examples that did not match any current query patterns, and a 90-token section that repeated the response format instruction in three different ways. They removed all three sections — 390 tokens total — and ran their eval suite of 500 queries. Quality scores were identical to the original prompt on 498 of 500 queries, and within acceptable variance on the remaining two. They permanently removed those 390 tokens, saving 28% of their system prompt tokens on every request. At 200,000 requests per day, this saved roughly 78 million input tokens per day. At two dollars per million input tokens, that is $156 per day or approximately $4,700 per month — from removing instructions the model never needed.

## Token Marginal Value: A Framework for Every Token

The concept of **token marginal value** gives you a principled way to evaluate whether any token in your system deserves to be there. Token marginal value is the quality improvement attributable to a specific token or group of tokens. If adding a sentence to your prompt improves quality by 2%, those tokens have positive marginal value. If adding a sentence changes nothing, those tokens have zero marginal value. If adding a sentence actually confuses the model and reduces quality — which happens more often than you might expect — those tokens have negative marginal value.

Every token in your system should have positive marginal value. If a token has zero or negative marginal value, it should be removed. This is a straightforward principle, but most teams never apply it because they never measure it. They write prompts, add context, include examples, and assume everything is contributing. The assumption is wrong more often than it is right.

Measuring token marginal value does not require testing every individual token. You test at the level of prompt sections, context chunks, example blocks, and formatting instructions. Group related tokens together and test the group. If the group has zero marginal value, remove the whole group. If the group has positive marginal value, you can optionally test subsets to find the minimum set of tokens that provides the value.

A fintech company applied token marginal value analysis to their loan application processing system. The system prompt was 2,100 tokens and included instructions for handling twelve different loan types. They measured the marginal value of each loan-type instruction block by removing it and running the eval for that loan type. They found that six of the twelve instruction blocks had zero marginal value — the model handled those loan types correctly based on its pre-training knowledge alone without any prompt instructions. The other six blocks had measurable positive value. They removed the six zero-value blocks, cutting the system prompt from 2,100 to 1,250 tokens, a 40% reduction. Quality scores across all twelve loan types remained unchanged. Monthly savings: approximately $8,200.

## Eliminating Waste in Retrieved Context

RAG systems are particularly prone to token waste because the retrieval step is imprecise. The retriever returns passages based on similarity scores, but similarity does not equal usefulness. A passage can be highly similar to the query — sharing keywords, topics, and semantic content — and still add nothing to the model's ability to answer the question, either because the passage contains information the model already knows or because the passage is about a related but different topic.

The standard approach is to retrieve the top K passages and include all of them in the context. But K is usually set once and never adjusted. If K is five, every query gets five passages regardless of whether one passage would have been sufficient or whether none of the five are actually useful. Setting K too high wastes tokens on irrelevant passages. Setting K too low risks missing relevant information.

A smarter approach uses **adaptive retrieval with relevance thresholds**. Instead of retrieving a fixed number of passages, you retrieve passages above a relevance threshold and include only those. If three passages score above the threshold, include three. If one passage scores above the threshold, include one. If no passages score above the threshold, skip the retrieval context entirely and let the model answer from its own knowledge. This ensures you never pay for passages the retriever is not confident about.

The relevance threshold must be calibrated. Too high, and you exclude useful passages. Too low, and you include irrelevant ones. Calibrate by running your eval set with different thresholds and measuring quality. Find the threshold below which including passages does not improve quality, and set the threshold there.

A legal research platform reduced their retrieved context by 35% by switching from fixed-K retrieval to threshold-based retrieval. They had been retrieving eight passages per query. After analysis, they found that on average only four to five passages per query scored above the relevance threshold they calibrated. Three to four passages per query were below the threshold and added no value. The token savings were significant: approximately 45 million tokens per day, translating to roughly $5,400 per month in input token costs.

## Eliminating Waste in Few-Shot Examples

Few-shot examples are valuable when they are relevant to the current input. They are waste when they are generic boilerplate included in every request regardless of the input. The fix is **dynamic few-shot selection**: instead of including the same examples every time, select examples that are most similar to the current input.

Dynamic few-shot selection works by maintaining a library of examples and using a lightweight retrieval step to find the most relevant ones for each query. If your library has fifty examples covering ten categories, and the current query is about billing disputes, you retrieve the three most relevant billing dispute examples rather than sending five generic examples that cover five unrelated categories.

The quality impact is often positive, not neutral. The model performs better with three relevant examples than with five generic ones, because relevant examples provide stronger signal. You pay fewer tokens and get better quality — a rare case where cost optimization and quality optimization align perfectly.

The implementation cost is modest. You need an embedding index over your example library and a retrieval step that adds a few milliseconds of latency. For most systems, this is trivial to implement and maintains itself as you add new examples to the library.

A content moderation system switched from static to dynamic few-shot examples. Their original prompt included eight examples covering the eight most common violation types. Each request sent all eight examples regardless of the content being moderated. After switching to dynamic selection, each request received two to three examples matching the violation type most likely present in the content. Prompt tokens dropped by 55% for the few-shot section. Moderation accuracy actually improved by 1.3 percentage points because the model received more targeted examples.

## Eliminating Waste in Output Generation

Output token waste is expensive because output tokens cost three to eight times more than input tokens. Every unnecessary sentence, every redundant explanation, every verbose formatting element in the model's response is waste at premium prices.

The most common source of output waste is prompting for more than you need. If your application extracts a yes-or-no decision from the model's response and discards the rest, every token beyond "yes" or "no" is waste. If your application parses a structured response and uses only three fields, every token the model spends on other fields is waste. If your application displays only the first paragraph of the response, every token in subsequent paragraphs is waste.

The fix is to prompt for exactly what you need and nothing more. If you need a classification label, prompt for the label only. If you need a score, prompt for the score only. If you need a summary, specify the maximum length. Use instructions like "respond with only the category name" or "provide your answer in one sentence" or "limit your response to 50 words." These instructions reduce output tokens dramatically.

Be specific about what you do not want. "Do not explain your reasoning" saves dozens of output tokens per request. "Do not include a preamble" eliminates the "Sure, I would be happy to help with that" that many models produce before the actual answer. "Do not repeat the question" prevents the model from restating the user's query before answering.

An e-commerce company discovered that their product recommendation system was generating 350 output tokens per request on average. The model produced a recommendation, then explained why it chose that product, then listed alternative products, then added a closing sentence. The application used only the product ID from the recommendation. By adding "respond with only the product ID" to the prompt, they reduced output tokens from 350 to 15 per request. At 500,000 requests per day and eight dollars per million output tokens, this saved approximately $1,340 per day, or roughly $40,000 per month. The quality of recommendations was unchanged because the application never used the explanatory text.

## The Prompt Audit Process

Token waste accumulates over time. Prompts grow as engineers add instructions, context, and examples to handle new edge cases. Nobody removes old instructions when new ones are added. Nobody tests whether existing instructions are still necessary after the model has been updated. The prompt becomes a geological record of every problem the team has ever encountered, with each layer preserved even when it is no longer relevant.

The solution is a regular **prompt audit**. Once per quarter, review every production prompt. For each section of the prompt, ask: does this section contribute to output quality? Has the model changed since this section was added? Is this instruction duplicated elsewhere? Is this example still representative of current queries?

The audit should be data-driven, not opinion-driven. Do not rely on engineers' intuitions about what is necessary. Run the A/B removal test on each suspicious section. Let the eval scores decide.

A mid-size SaaS company instituted quarterly prompt audits after discovering that their main product's system prompt had grown from 600 tokens at launch to 2,400 tokens over eighteen months. Each quarter, they found 15 to 25% of tokens could be removed without quality impact. Over three audit cycles, they reduced the prompt from 2,400 to 1,100 tokens — a 54% reduction. Annualized savings were approximately $168,000.

## Building a Culture of Token Efficiency

Token waste is an organizational problem as much as a technical one. Engineers add tokens because adding is easier than optimizing. Nobody gets credit for removing 200 tokens from a prompt. Everybody gets credit for shipping a fix that adds 200 tokens. The incentives favor accumulation.

Changing this requires making token efficiency visible. Track prompt sizes over time. Report token-per-request metrics in engineering dashboards. Set targets for maximum prompt sizes per feature. Include token efficiency in code reviews — when a pull request adds tokens to a prompt, the reviewer should ask whether those tokens have been tested for marginal value.

Some teams assign a **token budget** to each feature, similar to performance budgets in frontend engineering. The product recommendation feature gets a budget of 1,500 input tokens and 100 output tokens per request. If the feature needs more tokens, the team must justify the increase by showing the quality improvement from the additional tokens. This creates accountability and prevents the silent accumulation that leads to bloated prompts.

The culture shift is worth the effort. Teams that treat tokens like money — because tokens are money — build leaner, more efficient systems. They ask "do we need this?" before adding tokens instead of asking "can we afford this?" after the bill arrives. They test the marginal value of every prompt section, every retrieved passage, and every output format. They treat token waste the same way they treat code bloat: something to be identified, measured, and eliminated.

The next subchapter examines a force that reshapes all of these calculations over time: the token price deflation curve, and what it means for the cost optimizations you invest in today.