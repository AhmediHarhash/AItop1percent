# 8.1 — The Cost Pathology Catalog: Failure Modes Only Experienced Teams Recognize

Every AI system in production develops cost pathologies. Not might develop. Develops. The question is whether you find them before the finance team does. A **cost pathology** is a systematic failure mode where your system silently burns money through architectural flaws, operational neglect, or emergent behavior that nobody designed and nobody monitors. Unlike bugs that crash your application or quality issues that generate user complaints, cost pathologies run quietly. The system works. The responses are fine. The metrics are green. The only symptom is an invoice that grows faster than your traffic, and by the time someone investigates, the damage is months old and five or six figures deep.

Cost pathologies differ from simple overspending in a critical way. Overspending is a pricing problem — you chose an expensive model when a cheaper one would work, or you over-provisioned your infrastructure. A cost pathology is a structural problem embedded in your architecture, your operational processes, or your team's behavior. Fixing overspending requires changing a configuration. Fixing a cost pathology requires changing how your system works. The distinction matters because configuration changes take a day and cost pathologies take a quarter.

This subchapter catalogs the major pathologies. Each one has a mechanism that causes it, a detection method that reveals it, a typical cost impact that justifies the effort, and a fix that resolves it. Treat this as a diagnostic checklist. The teams that run this checklist quarterly catch pathologies early. The teams that don't find out about them from their CFO.

## The Creeping Prompt

**The Creeping Prompt** is the pathology where system prompts grow continuously as the team addresses edge cases, adds guardrails, and refines behavior — each addition increasing per-request cost without any single change feeling expensive enough to question. The mechanism is simple and human. A user finds a case where the model gives a bad answer. A product manager files a ticket. An engineer adds three sentences to the system prompt: instructions to check order status first, to avoid suggesting the user contact support unless automated options fail, and to include the order number in every response. Problem solved. Cost of the fix: an extra 45 tokens per request. At $3 per million input tokens, that is $0.000135 per request. Nobody notices.

But this happens every week. Fifty-two weeks of edge-case patches, each adding 30 to 80 tokens. After a year, the system prompt has grown from 800 tokens to 4,200 tokens. The prompt that started as a crisp set of instructions is now a sprawling document of accumulated rules, many of which contradict each other, several of which address edge cases that no longer exist, and a few of which duplicate instructions the model already follows from its training. The per-request input cost has increased by roughly 5x just from prompt growth. On a system processing 200,000 requests per day using Claude Sonnet 4.5, that extra 3,400 tokens per request adds $2.04 per thousand requests in input cost alone. Over a month, the prompt bloat costs an extra $12,240 — not from any decision anyone made, but from the accumulation of decisions nobody tracked.

Detection is straightforward but requires discipline. Track the token count of your system prompt over time. Set an alert if the prompt grows by more than 10 percent in any thirty-day period. Review the full prompt quarterly and ask a simple question for every paragraph: does removing this change model behavior on our eval suite? If removing a paragraph produces identical evaluation scores, the paragraph is dead weight. In practice, teams that run this audit find that 20 to 40 percent of their accumulated prompt instructions are redundant, outdated, or have no measurable effect on output quality.

The fix goes beyond trimming. The root cause is that prompt modification is unmetered. Anyone can add instructions, but nobody tracks the cost impact. The solution is to treat the system prompt like a budget. Every addition requires a justification: what edge case does this address, how many users hit it, and what is the annual cost of the extra tokens? A rule that costs $8,000 per year in extra tokens to address an edge case that affects 0.3 percent of queries is probably not worth it. A rule that costs $800 per year to prevent a compliance violation is cheap insurance. The math is not hard. The discipline to do it is the hard part.

## The Abandoned Experiment

**The Abandoned Experiment** is the pathology where old model endpoints, staging environments, prototype infrastructure, or decommissioned features continue running and billing long after anyone stopped using them. The mechanism is institutional: someone spins up a resource for a test, a prototype, or a comparison, and when the experiment ends, nobody tears it down. The resource is not in anyone's operational runbook. It does not appear in any team's cost dashboard because it was created outside the production system. It runs silently, billing your account month after month.

An enterprise AI team at a financial services company discovered this pathology during a cost audit in late 2025. They found eleven active model endpoints on their cloud account that nobody could attribute to any current project. Three were staging deployments for a product feature that had been cancelled eight months earlier. Two were A/B test endpoints that had been running since the test concluded in January. Four were developer sandbox environments that the original developers had left the company without decommissioning. Two were monitoring pipelines that sampled production traffic and sent it to evaluation endpoints that no longer existed — the evaluation model had been swapped months ago, but the sampling pipeline was still running and sending requests to the old endpoint, which was still active and billing. Combined, these eleven abandoned resources cost $14,800 per month. They had been running for an average of six months. The total waste was approximately $89,000.

Detection requires a resource audit that goes beyond your production infrastructure. Production systems are usually well-tracked. The pathology lives in the shadow infrastructure: developer accounts, staging environments, experimental namespaces, CI/CD pipelines that spin up GPU instances for testing and never spin them down. The audit should cover every API key your organization has issued, every cloud resource tagged with your project identifiers, every model endpoint that is receiving traffic or incurring idle charges, and every scheduled job that calls model APIs. If you cannot attribute a resource to a current project with an active owner, it is a candidate for termination.

The fix is a combination of governance and automation. Every resource that incurs model API charges or GPU costs should have an expiration date at creation time. Developer sandboxes expire after fourteen days. Staging environments expire after thirty days. Experimental endpoints expire after the experiment's planned duration. Automated cleanup scripts decommission expired resources. If someone needs to extend a resource, they explicitly renew it with a justification. This pattern — borrowed from cloud security practices around temporary credentials — eliminates the "spin up and forget" behavior that creates abandoned experiments. Teams that implement expiration-by-default typically reduce their shadow infrastructure costs by 60 to 80 percent within the first quarter.

## The Debug-Mode-in-Production

**The Debug-Mode-in-Production** pathology occurs when development-time configurations ship to production without anyone catching the difference. The mechanism is familiar to any engineer who has ever left a print statement in production code, but the cost consequences in AI systems are orders of magnitude worse because every extra token costs money.

The most common variant is verbose logging. During development, engineers often configure the system to log full prompts, full model responses, intermediate reasoning chains, and retrieved documents for every request. This logging frequently involves sending the full output through a secondary model call for structured extraction or classification. In development, where request volume is 50 to 100 per day, the extra cost is negligible. In production, where volume is 50,000 to 500,000 per day, it can double the inference bill. A customer support team discovered they were spending $22,000 per month on "observability" model calls that were actually debug-level logging from their development configuration, carried forward through three deployment cycles because nobody verified the logging level in the production environment variable set.

The second variant is extended reasoning or chain-of-thought that was enabled for debugging but never disabled. Models like Claude Opus 4.6 and GPT-5.2 offer extended thinking modes that produce detailed internal reasoning before the final answer. These modes are invaluable for debugging: you can see why the model made each decision. They are also expensive, typically consuming 3x to 10x the output tokens of the standard response. A team that enables extended thinking for debugging and deploys the configuration to production is paying for internal reasoning on every request — reasoning that users never see and the application never uses.

The third variant is unoptimized prompts. During development, engineers write prompts for clarity and debuggability. They include verbose examples, detailed explanations, and redundant instructions that help them understand the model's behavior. In production, a prompt-optimized version would be shorter and cheaper. But the optimization step often gets skipped because the verbose prompt "works fine" and there is always something more urgent to build. A prompt that includes six few-shot examples when two would achieve the same quality adds unnecessary tokens to every request. At scale, those extra examples cost real money — a system processing 300,000 requests daily with four unnecessary few-shot examples of roughly 200 tokens each wastes 240 million tokens per day, which at $3 per million input tokens is $720 per day or $21,600 per month.

Detection requires comparing your development configuration to your production configuration on every deployment. The specific items to check: logging level, reasoning mode, prompt version, evaluation sampling rate, retry count, and timeout settings. Any of these that differ between development and production should be intentional, documented, and reviewed. The most effective detection method is a deployment checklist that explicitly verifies cost-affecting configurations before any release.

The fix is environment-aware configuration management. Your system should not allow a single configuration to be used across development, staging, and production. Each environment should have its own configuration profile with cost-affecting settings explicitly declared. Log verbosity, reasoning mode, prompt version, and evaluation settings should be part of your deployment manifest and reviewed as part of every production release. Some teams go further and implement a "cost diff" step in their CI/CD pipeline that compares the estimated per-request cost of the new deployment against the current production cost. If the new deployment is more than 15 percent more expensive per request, the pipeline flags it for manual review.

## The Unmonitored Pipeline

**The Unmonitored Pipeline** is the pathology where a multi-step AI pipeline has per-request cost tracking at the aggregate level but not at the step level, hiding expensive bottlenecks behind a blended average. The mechanism is that teams measure "cost per request" as a single number — the total API spend divided by the total request count — without decomposing that cost into the individual steps of the pipeline. When total cost rises, they don't know which step caused the increase. When one step becomes ten times more expensive due to a configuration change, the blended average rises by a modest percentage that doesn't trigger any alerts.

Consider a document analysis pipeline with five steps: document chunking, embedding generation, retrieval, answer generation, and quality evaluation. The team tracks total cost per request, which averages $0.08. One day, an engineer modifies the retrieval step to pull fifteen documents instead of five for better recall. The retrieval cost triples. The answer generation cost also increases because the retrieved context is three times longer. But the total cost per request rises from $0.08 to $0.14 — a 75 percent increase that, when blended across the 500,000 daily requests, adds $30,000 per month. The team notices the monthly increase but attributes it to traffic growth. They don't investigate the per-step breakdown because they don't have one.

The pathology is especially dangerous in agentic systems. An agent that calls tools in a loop has no fixed number of steps. Request A might complete in three steps costing $0.05. Request B might require twelve steps costing $0.42. Request C might get stuck in a reasoning loop and consume forty steps costing $2.80 before hitting the timeout. The blended average across all three is $1.09 — a number that tells you nothing about the distribution. Without per-step cost tracking, you cannot identify that 2 percent of your requests are consuming 40 percent of your budget.

Detection requires per-step cost instrumentation. Every model call, every embedding generation, every retrieval query, and every tool invocation should log its own token count and cost. Your monitoring dashboard should show cost breakdown by pipeline step, not just total cost per request. It should also show the cost distribution — the median, the 90th percentile, the 99th percentile, and the maximum — so that expensive outlier requests become visible. When a single pipeline step accounts for more than 50 percent of total per-request cost, that step deserves its own optimization investigation.

The fix is to treat each pipeline step as an independently metered service. Each step has its own cost budget, its own alerts, and its own optimization targets. When the retrieval step's cost exceeds its budget, the retrieval team investigates. When the evaluation step's cost exceeds its budget, the evaluation team investigates. Without step-level budgets, cost optimization devolves into finger-pointing where every team claims their step didn't change and the increase must be coming from somewhere else. With step-level budgets, accountability is clear and investigations start immediately.

## The Gold-Plated Eval

**The Gold-Plated Eval** is the pathology where evaluation systems use the most expensive models on every request, or evaluate every request when sampling would produce equivalent statistical confidence. The mechanism is a quality mindset applied without cost awareness. The eval team wants the best possible quality signal. The best quality signal comes from the best model judging every request. So they configure LLM-as-judge evaluation using Claude Opus 4.6 at $5 per million input tokens and $25 per million output tokens, running on 100 percent of production traffic.

Do the arithmetic. If your production system processes 300,000 requests per day and your average eval prompt is 2,000 input tokens with a 400-token judgment, the daily eval cost is: 300,000 requests times 2,000 input tokens equals 600 million input tokens, at $5 per million, that is $3,000 per day in eval input alone. The output cost for 120 million tokens at $25 per million is another $3,000. Total eval cost: $6,000 per day, $180,000 per month. If your production inference cost is $150,000 per month, your evaluation system costs more than the system it is evaluating.

The waste is twofold. First, a mid-tier model like Claude Sonnet 4.5 or GPT-5 produces eval judgments that agree with Opus-level models 92 to 96 percent of the time on most quality dimensions. You are paying 5x to 8x more per evaluation for a 4 to 8 percent improvement in judgment accuracy. Second, sampling 5 percent of traffic instead of 100 percent reduces your eval cost by 95 percent while still providing statistically significant quality signals. A 5 percent sample of 300,000 daily requests gives you 15,000 evaluated requests per day — more than enough to detect a 2 percent quality regression with high confidence. The monthly eval cost drops from $180,000 to $4,500.

Detection requires comparing your eval spend to your production inference spend. If eval costs exceed 20 percent of inference costs, your evaluation system is likely over-engineered. Track the eval model tier and the sampling rate independently. Review both quarterly with a simple question: would reducing the eval model tier or the sampling rate materially change your ability to detect quality regressions? In most cases, the answer is no.

The fix is a tiered evaluation architecture. Use a fast, cheap model — GPT-5-nano at $0.05 per million input tokens or Claude Haiku 4.5 at $1 per million input tokens — as your primary evaluator, running on a 5 to 10 percent sample. Use a mid-tier model for deeper evaluation of requests flagged by the primary evaluator. Reserve frontier-model evaluation for weekly or monthly deep audits where you run the most capable judge against a curated sample to calibrate your cheaper evaluators. This tiered approach typically reduces eval costs by 90 to 95 percent while maintaining equivalent quality detection capability.

## The Phantom Migration

Beyond the five major pathologies, several less obvious patterns drain budgets in ways that compound over months. **The Phantom Migration** is the pattern where a team starts migrating from one model to another — say, from GPT-5 to Claude Sonnet 4.5 — and runs both models in parallel during the transition. The transition was supposed to take two weeks. It takes three months because the team discovers edge cases, gets pulled onto other priorities, and never finishes the cutover. For those three months, the team is paying for two model endpoints serving the same traffic type. The parallel period doubles the inference cost for the affected traffic, and because the migration is "in progress," nobody questions the dual billing.

Detection is a matter of tracking model endpoint counts and traffic routing. If you have two active endpoints serving the same traffic type — one labeled "current" and one labeled "migration target" — and both have been active for more than thirty days, the migration has stalled. Set a hard deadline for every migration: if the cutover is not complete by the deadline, roll back to the original model and reschedule. Open-ended parallel running is one of the most expensive forms of indecision in AI operations.

## The Feature Nobody Uses

**The Feature Nobody Uses** is the pattern where an AI-powered feature — summarization, auto-categorization, sentiment analysis — runs on every request because it was built as a default-on component of the pipeline, even though only a small fraction of users ever consume the output. A SaaS company built an automatic meeting summary feature that ran Sonnet-level inference on every meeting recording. It cost $28,000 per month in model calls. Product analytics showed that only 12 percent of users ever opened the summary. The company was spending $24,640 per month generating summaries that nobody read.

Detection requires correlating feature usage with feature cost. For every AI-powered feature, track two numbers: the cost to generate the feature's output and the percentage of users who actually consume it. If you are spending $10,000 per month on a feature that 8 percent of users engage with, you need to ask whether generating the output on demand — only when a user requests it — would deliver the same user experience at a fraction of the cost. In most cases, switching from run-by-default to run-on-demand eliminates 80 to 90 percent of the waste. Generate the summary when the user clicks "summarize," not when the meeting ends.

## The Compound Pathology Problem

In practice, cost pathologies rarely appear in isolation. A system suffering from The Creeping Prompt is also likely suffering from The Unmonitored Pipeline, because the team that doesn't track prompt size also doesn't track per-step costs. A team with abandoned experiments probably also has debug-mode configurations in production, because both stem from the same root cause: weak operational discipline around cost-affecting infrastructure.

A mid-size AI company that ran a comprehensive cost audit in early 2026 found all five major pathologies simultaneously. Their system prompts had grown by 260 percent over eighteen months. They had seven abandoned endpoints costing $9,200 per month. Their production system was running debug-level evaluation on 100 percent of traffic. Their multi-step pipeline had no per-step cost tracking. And their evaluation system used Opus-class models for every judgment. The combined waste was $67,000 per month — nearly 40 percent of their total AI spend. They had been paying this tax for months without anyone connecting the individual symptoms to a systemic pattern.

The lesson is that cost pathologies are not one-time problems to fix. They are ongoing failure modes that recur unless you build systems to prevent them. Quarterly cost audits, automated expiration of experimental resources, deployment checklists for cost-affecting configurations, per-step cost monitoring, and tiered evaluation architectures are not luxuries. They are the cost equivalent of monitoring and alerting for production systems. You would never run a production system without health checks. You should never run an AI budget without cost pathology detection.

## Building the Audit Practice

The pathology catalog is only useful if you actually use it. The most effective teams build a quarterly cost audit practice with a structured checklist. The audit takes one engineer two to three days and typically uncovers savings worth ten to fifty times the cost of the audit itself.

The audit has four phases. First, pull the full resource inventory: every API key, every model endpoint, every GPU instance, every scheduled job that involves model inference. Attribute each resource to a project, a team, and an owner. Any resource without an owner is an abandoned experiment until proven otherwise. Second, measure per-step costs for your top five traffic-volume pipelines. Decompose total cost into individual steps and identify which steps account for disproportionate spend. Third, review system prompt sizes, logging configurations, evaluation sampling rates, and model tier selections against your quality requirements. Fourth, check for parallel deployments, stalled migrations, and features with low user engagement running expensive inference.

Document the findings. Assign each pathology to a team with a remediation deadline. Track the cost impact before and after the fix. Share the results with engineering leadership. The act of sharing creates accountability, and accountability is the only sustainable defense against cost pathologies. The teams that treat cost engineering as a recurring practice — not a one-time project — are the ones that keep their AI margins healthy as they scale.

## What Comes Next

The pathology catalog gives you the diagnostic framework. But some pathologies deserve deeper treatment because they are especially common, especially expensive, or especially seductive. The next subchapter examines one of the most financially dangerous: The Premature Self-Hosting Trap, where teams convince themselves that running their own GPUs will save money — and discover too late that they've traded a predictable API bill for an unpredictable infrastructure project that costs more.
