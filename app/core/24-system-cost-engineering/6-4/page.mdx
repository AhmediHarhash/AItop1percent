# 6.4 — Embedding Generation Costs: Calculating the Price of Your Vector Pipeline

Every document you embed, every query you vectorize, every chunk you re-index has a price. Most teams have never calculated it. They treat embedding generation as a rounding error, a line item so small it does not deserve a dashboard. And for the first few months, they are right. But embedding costs are deceptive. They start small, scale linearly with corpus size and query volume, and compound every time you change models, update documents, or re-index your entire collection. A team that embeds five million chunks once and runs a hundred thousand queries a day is generating billions of tokens through its embedding pipeline every month. Those tokens are cheap individually, but they are not free, and the infrastructure surrounding them — the storage, the compute, the index maintenance — often costs five to ten times more than the embedding generation itself.

The teams that get blindsided are not the ones with small corpora. They are the ones that grew. They started with fifty thousand documents and a quick prototype using OpenAI's embedding API. The monthly bill was negligible. Then the corpus grew to a million documents. Then they needed to re-index when they switched from text-embedding-ada-002 to text-embedding-3-large. Then they added a second product line with its own corpus. Then they realized their chunking strategy was wrong and needed to re-chunk and re-embed everything. Each of those events triggered a full re-embedding pass, and suddenly the "rounding error" line item was a five-figure annual cost that nobody had budgeted for.

## The Two Cost Streams

**Embedding generation** has two distinct cost streams, and you need to model both to understand your total spend. The first is index-time embedding — the cost of converting your document corpus into vectors. This happens when you first build your index and again every time you update it. The second is query-time embedding — the cost of converting each user query into a vector for similarity search. This happens on every single search request your system handles.

Index-time embedding is a batch workload with a predictable cost profile. You know how many documents you have, you know how you chunk them, and you can calculate the total token count before you start. Query-time embedding is a real-time workload that scales with traffic. You do not control it directly — it grows as your user base grows.

The mistake teams make is optimizing for one stream while ignoring the other. A team obsessed with query-time efficiency that caches query embeddings aggressively might never look at the index-time cost that spikes every time they re-chunk their corpus. A team focused on corpus management that carefully minimizes re-indexing might ignore the fact that query-time embedding costs have quietly grown to ten times their index-time spend because traffic tripled.

## Calculating Index-Time Cost

Start with the numbers. Your corpus has one million documents. Your chunking strategy produces an average of five chunks per document, so you have five million chunks. Each chunk averages 500 tokens. That means embedding your entire corpus requires processing 2.5 billion tokens through an embedding model.

At OpenAI's text-embedding-3-small pricing of $0.02 per million tokens, that full corpus embedding costs $50. At text-embedding-3-large pricing of $0.13 per million tokens, it costs $325. At Cohere's embed-v4 pricing of $0.12 per million tokens, it costs $300. These are one-time costs for a single full index build, and they are genuinely cheap. This is why teams dismiss embedding costs during prototyping — the initial build is trivially affordable.

But the initial build is not the last build. You will re-embed your corpus. The question is not whether, but how often and at what cost. Here are the events that trigger a full or partial re-index. Changing your embedding model, because the new model produces different vectors that are incompatible with the old index. Changing your chunking strategy, because different chunk sizes produce different chunks that need new embeddings. Updating documents, because changed content needs new embeddings. Adding new documents, which need to be embedded and added to the index. And correcting errors, because if you discover a bug in your preprocessing pipeline, you need to re-embed everything that was affected.

A team that re-embeds their full corpus four times in a year because of model upgrades, chunking experiments, and pipeline fixes pays four times the initial build cost. At $325 per full build with text-embedding-3-large, that is $1,300 per year just in embedding generation, before accounting for storage, compute, or the engineering time to orchestrate the rebuild. For larger corpora, the numbers grow proportionally. A ten-million-document corpus with fifty million chunks at 500 tokens each requires 25 billion tokens per full embedding pass. At $0.13 per million tokens, that is $3,250 per full build, or $13,000 annually if you rebuild quarterly.

## The Re-Indexing Tax

The real cost of re-indexing is not the embedding generation. It is the operational overhead. A full re-index of a fifty-million-chunk corpus takes hours of sustained API calls, requires orchestration to handle rate limits and failures, and creates a window where your index is stale or inconsistent. During the re-index, queries hit either the old index, a partially updated index, or require a complex blue-green deployment where you build the new index alongside the old one and swap them atomically.

The blue-green approach is the right answer, but it doubles your storage cost during the transition period. You are running two complete indexes simultaneously — the old one serving live traffic and the new one being built. For a large corpus, that temporary storage duplication can cost hundreds to thousands of dollars in vector database fees. And if the re-index fails partway through, you might need to restart from scratch, paying the embedding cost again and extending the dual-storage period.

Teams that change embedding models frequently pay a compounding penalty. Every model swap requires a full re-embed and a full re-index. If you started with text-embedding-ada-002, switched to text-embedding-3-small for cost savings, then switched to text-embedding-3-large for quality, and are now considering Cohere embed-v4 for multilingual support, you have rebuilt your entire index three times in eighteen months. Each rebuild costs embedding tokens plus storage duplication plus engineering time plus a quality assurance cycle to validate that retrieval accuracy did not degrade. The cumulative cost of model churn far exceeds the sticker price of the embedding API.

The principle here is simple: treat embedding model selection as a high-inertia decision. The switching cost is real, even if the API pricing looks trivial. Choose your embedding model based on a thorough evaluation — not just accuracy benchmarks, but also dimension count, multilingual support, and long-term provider stability — and commit to it for at least twelve months.

## Calculating Query-Time Cost

Query-time embedding is the smaller per-request cost but the larger annual cost for most systems, because it scales with traffic rather than corpus size. Every search request requires embedding the user's query into the same vector space as your document chunks. The query is typically short — fifty to two hundred tokens for a natural language question, sometimes shorter for keyword-style queries.

At 100,000 queries per day with an average query length of 50 tokens, you are processing 5 million tokens per day through the embedding API. At OpenAI's text-embedding-3-small pricing of $0.02 per million tokens, that costs $0.10 per day, or about $3 per month. At text-embedding-3-large pricing of $0.13 per million tokens, it costs $0.65 per day, or about $20 per month. These numbers are still small in isolation.

Now scale to a serious production system. At one million queries per day with an average of 100 tokens per query, you are processing 100 million tokens per day. Monthly, that is 3 billion tokens. At text-embedding-3-small, the monthly cost is $60. At text-embedding-3-large, it is $390. Still manageable. But consider a platform serving multiple products with aggregate traffic of ten million queries per day. At 100 tokens per query, that is 1 billion tokens per day, 30 billion per month. At text-embedding-3-large pricing, monthly query embedding cost reaches $3,900. At Cohere embed-v4, it is $3,600. Now the "rounding error" is a $45,000 annual line item, and it grows linearly with traffic.

The insight is that query-time embedding cost is directly proportional to query volume and query length. If your product grows from one million to ten million daily queries, your query embedding cost grows by ten times. No optimization changes this linear relationship except caching, which we will cover shortly.

## Embedding Model Pricing Landscape in 2026

The embedding model market in 2026 offers a wide pricing range, and choosing the right model for your use case can reduce embedding costs by an order of magnitude without meaningful quality loss.

At the low end, OpenAI's text-embedding-3-small costs $0.02 per million tokens. It produces 1536-dimension embeddings by default but supports dimension reduction to 512 or 256. For many workloads — internal search, FAQ matching, simple document retrieval — this model provides sufficient quality at the lowest API price on the market. OpenAI also offers a 50% batch discount, bringing the effective price to $0.01 per million tokens for index builds that can tolerate twenty-four-hour turnaround.

In the mid range, OpenAI's text-embedding-3-large costs $0.13 per million tokens and Cohere's embed-v4 costs $0.12 per million tokens. These models produce higher-quality embeddings with better performance on multilingual and domain-specific tasks. Cohere's embed-v4 also supports image embeddings at $0.47 per million image tokens, making it a strong choice for multimodal corpora. The six-times price premium over text-embedding-3-small is justified only if your retrieval quality measurements show a meaningful accuracy difference on your specific data.

At the self-hosted end of the spectrum, open-source models like BGE-M3, GTE-Qwen2, and sentence-transformers variants cost nothing per token — your cost is the GPU compute to run them. A single mid-range GPU can generate embeddings at thousands of tokens per second, and the cost per million tokens is effectively $0.001 to $0.005 depending on your hardware and utilization. For teams processing tens of billions of tokens per month, self-hosting an embedding model on a dedicated GPU can reduce embedding cost by 95% compared to API pricing. The trade-off is operational complexity: you own the infrastructure, the scaling, the failover, and the model updates.

The decision framework is straightforward. If your monthly embedding token volume is under one billion tokens, use an API. The cost is negligible and the operational simplicity is worth the premium. If your volume is between one and ten billion tokens per month, evaluate whether self-hosting a single GPU for embedding generation is cheaper than the API, accounting for the engineering time to maintain it. If your volume exceeds ten billion tokens per month, self-hosting almost certainly wins on cost, and you should invest in the infrastructure.

## The Dimension Decision

Embedding dimensionality directly affects two costs: the vector storage in your database and the query compute for similarity search. A 1536-dimension embedding stored as 32-bit floats takes 6,144 bytes per vector. A 384-dimension embedding takes 1,536 bytes. A 256-dimension embedding takes 1,024 bytes. For a corpus of five million vectors, the storage difference between 1536 and 384 dimensions is 23 gigabytes versus 5.8 gigabytes — a four-times reduction.

Storage cost is only part of the equation. Similarity search performance degrades with higher dimensions because each distance calculation involves more multiplications. A vector database serving queries against 1536-dimension vectors requires roughly four times the compute per query compared to 384-dimension vectors. At high query volumes, this compute difference translates directly to infrastructure cost. A managed vector database that charges per query compute unit will cost more for higher-dimensional searches. A self-hosted database will need more powerful hardware to maintain the same query latency.

Modern embedding models make the dimension decision easier than it used to be. OpenAI's text-embedding-3 models support Matryoshka-style dimension reduction: you can request a 3072-dimension model to return only the first 256, 512, or 1024 dimensions, and the model preserves most of its accuracy because the most important semantic information is packed into the leading dimensions. OpenAI's own documentation notes that text-embedding-3-large shortened to 256 dimensions outperforms the older text-embedding-ada-002 at its full 1536 dimensions on the MTEB benchmark. You get better quality at six times less storage and compute.

The cost optimization is clear. Start with a quality benchmark on your own data at multiple dimension settings — 256, 512, 768, and 1536. Measure retrieval precision and recall at each setting. If 512 dimensions delivers retrieval accuracy within 2% of 1536, use 512. You save three times on storage, three times on query compute, and the quality difference is undetectable to your users. If 256 dimensions is within 5% of 1536, that may also be acceptable depending on your quality bar. Test on your data, not on public benchmarks. Your domain-specific retrieval patterns may tolerate dimension reduction better or worse than the average benchmark.

## Embedding Caching for Query-Time Savings

If the same queries appear repeatedly, you are paying to embed them repeatedly. **Embedding caching** stores the vector representation of previously seen queries and returns the cached vector instead of calling the embedding API again. For systems with high query repetition — search interfaces, customer support, FAQ-style queries — caching can reduce query-time embedding cost by 40% to 70%.

The implementation is a lookup layer between your application and your embedding API. Before sending a query to the API, hash the query text and check a cache store. If the hash exists, return the cached vector. If not, call the API, store the result, and return the vector. The cache can be an in-memory store like Redis for low-latency lookups or a persistent store for durability across restarts.

The economics depend on your cache hit rate. If 60% of queries are repeats or near-duplicates, caching eliminates 60% of your query-time embedding API calls. At 100,000 queries per day and $0.13 per million tokens, a 60% cache hit rate saves $0.39 per day, or about $12 per month. Modest. But at one million queries per day, that same cache hit rate saves $3.90 per day, or $117 per month. At ten million queries per day, savings reach $1,170 per month, or $14,000 per year. The savings scale linearly with traffic, and the cache infrastructure cost — typically a small Redis instance — is negligible by comparison.

Near-duplicate detection extends caching beyond exact matches. Two queries that differ only in capitalization, punctuation, or trivial word variation produce nearly identical embeddings. Normalizing queries before hashing — lowercasing, stripping punctuation, removing stop words — increases the cache hit rate by catching these near-duplicates. More sophisticated approaches use locality-sensitive hashing to identify queries that are semantically similar enough to reuse the same embedding. These techniques push cache hit rates above 70% for systems with repetitive query patterns.

## Batch Embedding for Index Builds

When you are building or rebuilding your index, never embed chunks one at a time through real-time API calls. Use batch embedding, where you submit thousands or millions of chunks in a single batch job and receive results within a twenty-four-hour window. OpenAI's Batch API offers a 50% discount on embedding generation, cutting the cost of a full corpus build in half. For a five-million-chunk corpus at 500 tokens per chunk, batch embedding with text-embedding-3-large costs $162 instead of $325. For a fifty-million-chunk corpus, batch saves $1,625 per full index build.

Beyond the API pricing discount, batch embedding is more reliable for large index builds. Real-time API calls at high volume run into rate limits, require retry logic, and can fail partway through with partial results. Batch jobs handle throttling internally, process at a sustainable pace, and return complete results with clear success or failure status. The twenty-four-hour turnaround is acceptable for index builds because nobody is waiting in real time for the result.

If you are self-hosting your embedding model, the batch principle still applies but the mechanism differs. Instead of API batch endpoints, you load chunks into a GPU processing pipeline that runs at maximum throughput. Batch processing on a self-hosted model achieves near-100% GPU utilization, compared to the sporadic utilization of an on-demand endpoint. The cost saving comes from higher utilization rather than a pricing discount, but the magnitude is comparable — roughly a 30% to 50% cost reduction for batch versus real-time self-hosted embedding generation.

## The Total Pipeline Cost

Embedding generation is one component of your total vector pipeline cost. The full cost includes embedding generation, vector storage, index maintenance, query compute, and infrastructure overhead. Understanding how these components relate helps you optimize the total cost rather than optimizing one piece in isolation.

For a mid-sized deployment — five million vectors at 1536 dimensions with 100,000 queries per day — the annual cost breakdown looks approximately like this. Embedding generation for the initial build and two re-indexes per year costs around $975 using text-embedding-3-large at standard pricing. Query-time embedding at 100,000 queries per day costs about $240 per year. Vector storage in a managed database costs $600 to $3,000 per year depending on provider. Query compute in the managed database costs $1,200 to $6,000 per year depending on volume pricing. Infrastructure overhead — monitoring, orchestration, failover — adds another $500 to $2,000 per year in engineering tool costs.

The total is roughly $3,500 to $12,000 per year for a mid-sized RAG deployment. Embedding generation itself accounts for only 10% to 30% of that total. Vector database costs typically dominate, accounting for 50% to 70% of the total. This means that the most impactful cost optimization for your vector pipeline is not choosing a cheaper embedding model — it is right-sizing your vector database, reducing dimensionality to lower storage and query costs, and avoiding unnecessary re-indexes.

For a large deployment — fifty million vectors, one million queries per day, frequent index updates — the total annual pipeline cost can reach $50,000 to $150,000. At this scale, the cost structure shifts: query-time embedding and query compute become larger relative shares, and optimizations like embedding caching and dimension reduction deliver significant absolute savings. A large deployment that reduces dimensionality from 1536 to 512 and implements query caching with a 55% hit rate can cut total pipeline cost by 30% to 40%, saving $15,000 to $60,000 per year.

## Cost Models by Deployment Size

Small deployments — under 500,000 vectors, under 10,000 queries per day — should not overthink embedding costs. Use an API provider, pick whichever embedding model your cloud vendor offers natively, and move on. Your total annual vector pipeline cost is under $1,000. Optimization effort is better spent on inference cost, which is one to two orders of magnitude larger.

Medium deployments — 500,000 to ten million vectors, 10,000 to 500,000 queries per day — should evaluate embedding model pricing, implement dimension reduction if retrieval quality holds, and ensure index builds use batch pricing. Your annual pipeline cost is $3,000 to $25,000, and thoughtful optimization can save $1,000 to $8,000 per year. The savings justify a few days of engineering analysis but not a major infrastructure project.

Large deployments — over ten million vectors, over 500,000 queries per day — should seriously evaluate self-hosted embedding models, implement comprehensive caching, optimize dimensionality based on rigorous retrieval benchmarks, and negotiate volume pricing with their vector database provider. Your annual pipeline cost is $25,000 to $200,000, and optimization can save $10,000 to $80,000 per year. At this scale, a dedicated engineer spending a quarter on vector pipeline cost optimization pays for themselves several times over.

The pattern across all three tiers is the same: embedding generation cost is the visible expense, but vector database cost is the dominant expense. The next subchapter examines vector database economics in detail — the storage pricing, the query cost models, and the managed-versus-self-hosted decision that determines whether your vector infrastructure costs $200 per month or $8,000 per month for the same workload.
