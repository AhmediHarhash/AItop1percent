# 7.2 — GPU Economics in 2026: On-Demand, Reserved, Spot, and Inference-Specific Accelerators

The GPU you rent today is the most expensive component in your self-hosted AI stack, and the procurement model you choose for that GPU determines whether you pay full retail or get a structural discount that compounds every month for years. In early 2026, a single NVIDIA H100 SXM GPU rents for anywhere between $1.49 per hour and $6.98 per hour depending on the provider and the commitment level. That spread — nearly five-to-one — is not a quality difference. It is a procurement difference. The same silicon, running the same model, producing the same output, at dramatically different costs depending on how you agreed to pay for it. Understanding the four procurement models and the emerging hardware alternatives is not optional knowledge for anyone running self-hosted inference. It is the difference between a cost-optimized deployment and one that hemorrhages money through the wrong purchasing decision.

## The Four Procurement Models

Every major cloud provider and GPU-specialized provider offers GPU compute through some combination of four models: on-demand, reserved, spot, and inference-specific accelerators. Each model trades a different resource — flexibility, commitment, risk tolerance, or hardware specificity — for a different price point. The right choice depends on your workload's characteristics, not on a blanket preference.

**On-demand** is the simplest model. You spin up a GPU instance when you need it, you pay by the hour, and you shut it down when you are done. No commitment, no contracts, instant availability subject to capacity. This is the default for most teams starting out, and it is the most expensive option per hour. An H100 SXM on-demand from a specialized provider like Lambda Labs or Hyperstack runs $2.40 to $3.29 per hour. The same GPU on a hyperscaler like AWS, Azure, or Google Cloud runs $4.00 to $6.98 per hour. The hyperscaler premium buys you deeper integration with their ecosystem — managed Kubernetes, monitoring dashboards, compliance certifications — but the raw GPU performance is identical.

**Reserved instances** trade commitment for discount. You sign a one-year or three-year contract promising to pay for a fixed amount of GPU capacity regardless of whether you use it. In return, the provider cuts your hourly rate by thirty to sixty percent. A three-year reservation on an H100 can bring the effective hourly cost down to $1.50 to $2.00 per hour on specialized providers and $2.50 to $3.50 on hyperscalers. The savings are enormous — a team spending $3.00 per hour on-demand that switches to a one-year reservation at $2.00 per hour saves $8,760 per GPU per year. Multiply that by eight GPUs in a node and you save $70,000 annually on a single serving node. The risk is commitment. If your traffic drops, if you switch models, if a better GPU becomes available at a lower price, you are locked in. You pay the reserved rate whether you use the capacity or not.

**Spot instances** — called preemptible instances on Google Cloud and spot instances on AWS — offer the deepest discounts. You bid on unused GPU capacity that the provider would otherwise leave idle. Prices run seventy to ninety percent below on-demand rates. An H100 that costs $3.00 per hour on-demand might be available as a spot instance for $0.50 to $1.00 per hour. The catch is brutal: the provider can reclaim your instance at any time with as little as thirty seconds of warning. Your workload gets interrupted, your model gets evicted from GPU memory, and your serving node disappears. For training jobs and batch inference that can checkpoint and restart, spot instances are a gift. For real-time inference serving that cannot tolerate interruption, they are unusable as your primary capacity. Some teams use spot instances as burst capacity behind a queue — if the spot instance gets reclaimed, the request waits or falls back to on-demand capacity.

**Inference-specific accelerators** are the fourth model and the one shifting the landscape most aggressively in 2026. These are purpose-built chips designed specifically for inference rather than training, and they trade general-purpose flexibility for dramatically better price-per-token on inference workloads. AWS Inferentia2 offers inference at roughly $0.50 to $2.00 per hour with throughput that competitive benchmarks show delivering up to seventy percent cost reduction compared to general-purpose GPUs on supported models. Google's TPU v5e and the latest Trillium generation provide similar inference economics, particularly for models that have been optimized for the TPU architecture. Groq's LPU chips deliver extremely low latency — often under 100 milliseconds for first token — at competitive per-token costs, with pricing around $0.05 to $0.79 per million output tokens depending on the model. The constraint with all inference-specific hardware is model compatibility. Not every model runs on every chip. A model optimized for NVIDIA CUDA may need significant adaptation to run on Inferentia or TPU. If your model of choice is well-supported on an inference-specific accelerator, the economics are compelling. If it is not, the engineering cost of porting may negate the savings.

## The 2026 GPU Hardware Landscape

The GPU market in 2026 looks fundamentally different from even two years ago. NVIDIA still dominates, but the product line has expanded and the competitive landscape has intensified.

The NVIDIA H100 remains the workhorse of production inference and is now available at commodity pricing from specialized cloud providers. On-demand rates have settled into the $2.40 to $3.50 range on providers like Hyperstack, Lambda, DataCrunch, and CoreWeave. Hyperscaler rates remain higher but include managed services and compliance certifications that enterprise customers value. The H100 delivers excellent price-performance for models up to about 70 billion parameters on a single GPU and can serve larger models across multi-GPU configurations with NVLink high-speed interconnect.

The NVIDIA H200 — the H100's successor with increased memory bandwidth and larger HBM3e memory — is now broadly available on cloud providers at $3.72 to $5.50 per hour on-demand. The additional memory is critical for serving larger models without quantization, and the increased bandwidth improves throughput by fifteen to thirty percent on memory-bandwidth-bound workloads like long-context inference. For teams serving models in the 70-billion-parameter range and above, the H200 often delivers lower cost per request despite the higher hourly rate, because the throughput improvement more than compensates for the price premium.

NVIDIA's Blackwell generation — the B100 and B200 — began shipping to cloud providers in late 2025 and is now available on select providers at $6.00 to $8.00 per hour. These GPUs represent a generational leap in both training and inference performance, with the B200 delivering roughly two to three times the inference throughput of an H100 on large language models. The per-hour cost is higher, but the per-token cost is lower. Early adopters running high-volume inference on B200 instances report cost-per-request reductions of forty to sixty percent compared to H100 deployments, even at the early-adopter pricing premium. As availability increases through 2026, Blackwell pricing is expected to soften, and B200 instances will likely become the default for new high-performance inference deployments.

The A100 — NVIDIA's previous generation — is now the budget option. On-demand rates have fallen to $1.29 to $2.00 per hour, and reserved pricing can bring A100 costs below $1.00 per hour on some providers. For smaller models, for workloads that don't need the throughput of an H100, and for teams optimizing cost above all else, the A100 remains a solid choice. A team serving a 7-billion-parameter model on A100 instances at $1.50 per hour achieves excellent cost-per-request economics because the model is small enough to saturate the GPU's compute capacity.

## AMD MI300X: The Memory-Rich Alternative

NVIDIA's dominance in the GPU market is no longer unchallenged. AMD's Instinct MI300X has emerged as the most credible alternative for production AI workloads, and its economics deserve serious evaluation from any team running self-hosted inference at scale.

The MI300X's headline advantage is memory. Each accelerator ships with 192 GB of HBM3e — more than double the 80 GB on an H100 and substantially more than the 141 GB on an H200. For teams serving models in the 70-billion-parameter range and above, this memory advantage is not a vanity spec. It means you can serve larger models without quantization, or serve quantized models with much larger KV caches, which directly translates to higher concurrent user capacity per GPU. A model that requires two H100 GPUs because it exceeds a single card's memory can sometimes fit on a single MI300X, immediately halving your per-request GPU cost for that workload.

On-demand cloud pricing for the MI300X has settled into the $1.85 to $4.89 per hour range in early 2026, depending on the provider. Providers like Vultr and TensorWave have pushed pricing below $2.00 per hour, while RunPod offers self-service MI300X access under $3.00 per hour. Compare that to H100 SXM pricing at $2.40 to $3.50 per hour from similar providers. At the hardware purchase level, the economics are even more striking: MI300X units sell to enterprise customers in the $10,000 to $15,000 range, while H100 units peaked above $40,000 and still trade at significant premiums. For teams building or leasing private clusters, the capital expenditure difference is enormous.

Performance-per-dollar is where the picture gets nuanced. On large models — Llama 4 Maverick at 400-plus billion parameters, DeepSeek V3 at 670 billion parameters — the MI300X beats the H100 in both absolute throughput and cost-per-token. The memory headroom lets it handle these massive models more efficiently. On smaller models and at larger batch sizes, the H100 and especially the H200 often pull ahead due to NVIDIA's mature CUDA software stack and more aggressive kernel optimizations. Benchmarks consistently show the MI300X achieving between sixty and one hundred percent of H100 performance depending on model size and batch configuration, with the gap narrowing on memory-bound workloads and widening on compute-bound ones.

The software story is the critical factor. AMD's ROCm ecosystem was a dealbreaker for most production teams as recently as 2024, but by early 2026, the situation has materially improved. vLLM — the most widely used open-source inference engine — now treats ROCm as a first-class platform. A dedicated ROCm CI pipeline went live in late 2025, and as of January 2026, ninety-three percent of vLLM's AMD test groups pass consistently, up from thirty-seven percent just two months earlier. Pre-built Docker images for MI300X inference are available directly from Docker Hub — you no longer need to build from source. That said, the CUDA ecosystem still has a deeper bench of optimized kernels, more community support, and broader tool compatibility. Teams should plan for an extra two to four weeks of engineering time when bringing up a new model on MI300X compared to H100.

When does the MI300X make economic sense? Three scenarios stand out. First, if you serve models that exceed 80 GB in memory and would require multi-GPU setups on H100, the MI300X's 192 GB lets you consolidate onto fewer cards. Second, if your workload is primarily memory-bandwidth-bound — long-context inference, large KV caches, models with high token counts — the MI300X's bandwidth advantage pays off directly. Third, if you are building a new cluster from scratch and the capital expenditure savings of thirty to fifty percent over equivalent NVIDIA hardware justifies the additional software ecosystem risk. AMD's MI350X, which began shipping in the second half of 2025 with 288 GB of HBM3e and support for FP4 precision, pushes this value proposition even further — offering up to thirty-five times faster inference than the MI300X generation on supported workloads. The AMD hardware roadmap is no longer a speculative bet. It is a cost lever that serious infrastructure teams cannot afford to ignore.

## Google TPU: The Vertically Integrated Option

Google's Tensor Processing Units occupy a unique position in the accelerator market because they are both a cloud service and a hardware architecture that Google controls end to end. You cannot buy a TPU. You rent one from Google Cloud. This vertical integration means the economics work differently from GPUs — you are buying into an ecosystem, not just renting silicon.

The TPU v5p, Google's current training-focused accelerator, is priced at $4.20 per chip per hour at on-demand rates. But training is not where most teams feel the cost pressure. The more relevant product for inference-heavy teams is the TPU v6e — marketed under the Trillium brand — which became generally available in 2025. TPU v6e pricing starts as low as $0.39 per chip-hour with committed-use discounts, scaling up to roughly $1.38 per chip-hour at on-demand rates. Compare that to H100 instances at $2.40 to $6.98 per hour and the cost differential is immediately obvious.

The performance story backs up the pricing. Google claims the TPU v6e delivers up to four times better performance per dollar compared to the H100 for large language model serving, recommendation systems, and large-batch inference. Real-world validation supports this: Midjourney reportedly reduced its monthly inference spend from $2.1 million to under $700,000 after migrating to TPU v6e — an annualized saving of roughly $16.8 million while maintaining the same output volume. Even accounting for Google's incentive to present favorable comparisons, the directional savings are too large to dismiss as marketing.

The constraint is lock-in. Models must be compiled for the TPU architecture using JAX or TensorFlow, and the effort to port a model that was developed and optimized for CUDA is non-trivial. Teams deeply embedded in the PyTorch and CUDA ecosystem face a real migration cost. However, for teams already running on Google Cloud, using JAX-based frameworks, or starting new projects without legacy CUDA dependencies, TPUs offer inference economics that are difficult to match on any GPU. Google's upcoming TPU v6p generation, expected at roughly $8,000 per chip, will push further into high-performance training territory while the v6e continues to serve as the price-performance leader for inference.

The practical decision framework for TPUs is straightforward. If you are already on Google Cloud and your models are JAX-compatible, run the numbers on TPU v6e before you provision a single GPU — the pricing gap is wide enough that even a modest migration effort pays for itself within one to three months. If you are multi-cloud or CUDA-native, the migration cost is real but not prohibitive for high-volume inference workloads where the per-token savings at scale will dwarf the one-time porting investment. The teams that dismiss TPUs without running the comparison are often the same teams that overpay for inference by forty percent or more.

## Inference-Specific Accelerators: Purpose-Built Economics

Beyond AMD and Google's general-purpose alternatives, a class of hardware designed exclusively for inference is reshaping the cost floor for serving workloads. These accelerators sacrifice training capability and sometimes model flexibility in exchange for dramatically better price-per-token on the workloads they support.

**AWS Inferentia2 and Trainium2** represent Amazon's strategy to undercut GPU pricing for customers already running on AWS. Inferentia2 instances are available at approximately $1.30 per hour — roughly half the cost of an H100 on the same cloud. AWS claims Inferentia2 delivers up to four times the throughput of its predecessor with ten times lower latency, and customers have reported cost reductions of fifty to eighty percent compared to GPU-based inference on supported models. Trainium2, originally designed for training but increasingly used for inference on larger models, runs at approximately $4.80 per hour — still roughly half the cost of an H100 instance on AWS. The constraint is model support: not every architecture compiles cleanly to AWS's Neuron SDK, and models that require custom CUDA kernels or non-standard operators may need significant adaptation work. For teams running standard transformer architectures on AWS — particularly Llama variants, which Amazon has invested heavily in optimizing — the Inferentia2 economics are compelling enough to justify the porting effort.

**Groq's LPU** takes a fundamentally different approach. Rather than using a GPU architecture at all, Groq's Language Processing Unit is built around a deterministic dataflow design that eliminates the memory bottleneck plaguing conventional inference hardware. The result is extraordinary latency — often under 100 milliseconds to first token — at API pricing of roughly $0.05 to $0.79 per million output tokens depending on the model, with an additional fifty percent discount available for batch workloads. The speed advantage is real and measurable: for applications where time-to-first-token is a competitive differentiator, such as real-time conversational AI or interactive coding assistants, Groq's latency profile is unmatched by any GPU-based solution. The trade-off is flexibility. Groq's hardware supports a limited set of models, you access it as a managed API rather than bare metal, and model customization options are constrained compared to running your own GPU fleet. In early 2026, NVIDIA acquired Groq, raising questions about the long-term independence of the platform but also suggesting that the LPU architecture will be integrated into future NVIDIA inference products.

**Cerebras** has carved out a different niche with its Wafer-Scale Engine — a single chip the size of an entire silicon wafer that eliminates the memory-bandwidth bottleneck by keeping the entire model on-chip. Cerebras offers inference pricing at $0.10 per million tokens for Llama 3.1 8B and $0.60 per million tokens for Llama 3.1 70B, which undercuts most GPU-based inference providers by a wide margin. In January 2026, Cerebras signed a $10 billion deal with OpenAI to deliver 750 megawatts of inference computing power through 2028, a validation of the Wafer-Scale approach at the highest level of industry demand. For teams that can use supported models and are comfortable with an API-based access model, Cerebras offers some of the most aggressive per-token pricing available in 2026.

The pattern across all three — Inferentia, Groq, Cerebras — is identical. Each trades generality for economics. You lose the ability to run any model on any framework with any custom kernel. You gain per-token costs that are thirty to eighty percent lower than general-purpose GPUs. The right mental model is not "should we switch all inference to custom silicon" but rather "which of our inference workloads run supported models at high enough volume that the migration cost pays back in under six months." For most teams, the answer is at least one workload, and often their highest-volume one.

The strategic takeaway across all of these alternatives is the same: the NVIDIA GPU monopoly on inference economics is over. Teams that automatically default to H100 or B200 instances without evaluating AMD MI300X, Google TPU, AWS Inferentia, Groq, or Cerebras are leaving money on the table — potentially thirty to seventy percent of their inference compute bill. The evaluation cost is a few weeks of engineering time. The savings compound every month for as long as you run inference.

## The Cost-Per-Request Calculation

Raw GPU hourly cost is meaningless without throughput context. The metric that matters is cost per request — or more precisely, cost per one million tokens processed — because that is what you compare directly against API pricing.

The formula is straightforward when stated in prose. Take your GPU cost per hour, divide by the number of requests your serving setup processes per hour at your actual utilization rate, and you get cost per request. If an H100 costs $2.50 per hour and your vLLM deployment processes 180,000 requests per hour at an average of 1,200 input tokens and 600 output tokens per request, your GPU cost per request is $2.50 divided by 180,000, which equals approximately $0.000014 per request. That is $0.014 per thousand requests, or roughly $0.008 per million tokens processed.

But GPU cost per request is not total cost per request. You need to add the amortized cost of your serving infrastructure — the load balancers, the networking, the storage for model weights, the monitoring stack. You need to add the amortized cost of the engineering team maintaining the deployment. You need to account for GPU idle time if your utilization is below one hundred percent. At fifty percent utilization, your effective GPU cost per request doubles. At thirty percent utilization, it more than triples.

A realistic total cost per request for a well-optimized H100 inference deployment running Llama 4 Maverick at seventy percent utilization, including infrastructure overhead and amortized engineering cost, lands in the range of $0.15 to $0.40 per million tokens. Compare that to API pricing for Claude Sonnet 4.5 at $3.00 per million input tokens and $15.00 per million output tokens, or even GPT-5 at $1.25 and $10.00. The self-hosted cost is dramatically lower — but only if your utilization is high and your operational overhead is controlled.

## Matching Procurement Model to Workload

The choice between on-demand, reserved, spot, and inference-specific accelerators is not a one-time decision. It maps directly to the characteristics of each workload you run.

Stable, predictable production inference traffic — your always-on customer-facing chatbot, your real-time recommendation engine, your classification pipeline that processes a steady stream of events — belongs on reserved instances. The traffic is predictable enough that you can commit with confidence, and the thirty to sixty percent discount on reserved pricing compounds into hundreds of thousands of dollars in annual savings. A team running four H100 GPUs for production inference that switches from on-demand at $3.00 per hour to a one-year reservation at $2.00 per hour saves $35,000 per year on those four GPUs alone.

Burst and overflow traffic — the spikes above your reserved baseline — belongs on on-demand instances or, if your architecture supports it, spot instances behind a queue. You spin up additional GPU capacity when traffic exceeds what your reserved fleet can handle, and you shut it down when the spike subsides. The per-hour cost is higher, but you only pay for the hours you use. For most production systems, burst traffic accounts for ten to thirty percent of total compute needs. Paying on-demand rates for that slice while paying reserved rates for the baseline is the optimal split.

Training and fine-tuning workloads — which can be checkpointed and restarted — are ideal for spot instances. A fine-tuning job that takes eight hours at $3.00 per hour on-demand costs $24.00. The same job on spot instances at $0.80 per hour costs $6.40 — even if the job gets interrupted once and needs to restart from its last checkpoint. The economics are so favorable that running training exclusively on spot instances, even with interruptions, typically saves sixty to eighty percent over on-demand.

Batch inference — processing a queue of requests overnight or during low-traffic periods — also suits spot instances well. If a batch job processes 500,000 documents and can tolerate a two-hour interruption where it picks up where it left off, the savings from spot pricing are enormous. Batch inference is inherently fault-tolerant because the requests are in a queue and no user is waiting in real time for the result.

Inference-specific accelerators belong wherever your model is supported and your latency requirements are met. If you serve a model that runs well on AWS Inferentia2, Google TPU, or Groq hardware, and your deployment doesn't require the flexibility to switch models frequently, the cost advantage of inference-optimized silicon is too large to ignore. A team that moved their Llama-based classification pipeline from H100 instances to Inferentia2 reported a fifty-five percent reduction in per-request cost, even after accounting for the engineering time spent adapting their serving stack.

## The Shifting Economics: Why GPU Procurement Is Not Set-and-Forget

The GPU market is in a period of rapid price deflation. H100 on-demand rates dropped approximately sixty to seventy-five percent from their 2023 peaks to early 2026 pricing. A100 rates are now below what H100 rates were twelve months ago. As Blackwell GPUs become widely available, H100 and H200 prices will continue to soften. Custom ASIC shipments from cloud providers — AWS Inferentia, Google TPU, Groq LPU — are projected to grow over forty percent in 2026 while GPU shipment growth is closer to sixteen percent. The hardware landscape is diversifying, and competition is pushing prices down across every tier.

This deflation means that locking into long-term reserved commitments carries real opportunity cost. A three-year reservation on H100 instances at $1.80 per hour might look attractive today, but if H100 on-demand rates drop to $1.50 within eighteen months as B200 instances commoditize, you are paying a premium for an older GPU on a contract you cannot exit. The optimal strategy in a deflationary market is to use shorter commitment windows — one-year reservations rather than three-year, or even quarterly commitments if your provider offers them — and to re-evaluate your hardware mix every six months.

Teams that treat GPU procurement as a quarterly review cycle rather than a one-time decision consistently achieve fifteen to twenty-five percent lower compute costs than teams that sign multi-year contracts and forget about them. The savings come not from heroic negotiation but from systematically shifting workloads onto the best-priced hardware available at each review point.

## Building Your GPU Cost Model

Every self-hosted inference team needs a living spreadsheet — a GPU cost model that tracks the true cost of every GPU in the fleet, updated quarterly. The model should include the following for each GPU type in your deployment: the hourly cost at your actual procurement rate, the maximum throughput at your model's configuration, the actual utilization averaged over the past 30 days, the resulting cost per request at actual utilization, and the all-in cost per request including amortized infrastructure and engineering overhead.

This cost model serves two purposes. First, it gives you the accurate per-request cost you need to compare against API alternatives. Most teams that think self-hosting is cheaper are relying on theoretical throughput at one hundred percent utilization, which never matches reality. The cost model forces you to use real numbers. Second, the quarterly update cadence surfaces optimization opportunities — a GPU running at forty percent utilization that should be consolidated, a workload that should shift from H100 to A100, a spot instance opportunity for a batch workload still running on reserved capacity.

The teams that run GPU infrastructure efficiently are the ones that measure it relentlessly. The teams that run it expensively are the ones that provisioned it once and moved on.

## What Comes Next

Procurement model and hardware choice determine your GPU cost per hour. But the variable that has the largest impact on your cost per request is not the price on the invoice — it is how much of that GPU's capacity you actually use. Most teams waste between thirty and fifty percent of their GPU compute through low utilization. The next subchapter covers GPU utilization optimization: why most teams waste half their compute, how to measure it, and the specific strategies that reclaim that wasted capacity.
