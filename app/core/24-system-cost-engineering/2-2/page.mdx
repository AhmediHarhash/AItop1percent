# 2.2 â€” Calculating Cost Per Query: The Complete Token Accounting Method

In September 2025, a mid-stage insurance technology company launched an AI-powered claims processing assistant. Their back-of-the-envelope cost estimate was straightforward: average user message of 150 words, model response of about 200 words, roughly 500 total tokens per query, GPT-5 pricing. They projected two cents per query, which at twenty thousand queries per day came to four hundred dollars daily, about twelve thousand dollars monthly. The CFO approved it. The system shipped. The first invoice was thirty-seven thousand dollars. Not twelve thousand. Thirty-seven thousand. Three times the projection. The engineering lead spent two days tracing the discrepancy. The user message was indeed about 150 words. The visible response was about 200 words. But the team had not counted the system prompt, which was 900 tokens. They had not counted the retrieved policy documents injected by their RAG pipeline, which averaged 2,400 tokens. They had not counted the few-shot examples included for response formatting, which added another 600 tokens. They had not counted the reasoning tokens the model consumed internally before producing the visible response, which averaged 1,200 tokens. They had not counted the 18% retry rate, where failed requests still incurred token costs before the successful retry. And they had not counted the per-request evaluation call that ran a second LLM to score response quality, which consumed its own tokens. The true cost per query was not two cents. It was six point two cents. The team had measured only the tokens the user could see and ignored everything else.

This is one of the most common cost engineering failures. The tokens you see in the user interface, the prompt and the response, are a fraction of the tokens you pay for. A complete cost accounting requires tracking every token that flows through your system, visible or not. This subchapter walks through the **Token Accounting Method**: a systematic approach to identifying, measuring, and costing every token in a request lifecycle.

## The Seven Token Flows in a Typical Query

Every request to a language model involves multiple categories of tokens, and each category has different characteristics. Some are fixed per request. Some vary with the user. Some are invisible. All of them cost money.

The first flow is the **system prompt**. This is the instruction set that defines the model's behavior, persona, constraints, and output format. In most production systems, the system prompt is identical across all requests. It is a fixed cost. But fixed does not mean small. A well-crafted system prompt for a customer service agent might be 600 to 1,200 tokens. A system prompt for a complex document analysis task might be 2,000 tokens. A system prompt that includes behavioral rules, output schemas described in prose, tone guidelines, and safety constraints can easily reach 1,500 tokens. These tokens are processed as input on every single request. At GPT-5 pricing of one dollar twenty-five cents per million input tokens, a 1,000-token system prompt costs one eighth of a cent per request. At ten thousand requests per day, that is twelve dollars fifty cents per day just for the system prompt. At Claude Opus 4.5 pricing of five dollars per million, the same system prompt costs five cents per thousand requests, or fifty dollars per day at ten thousand requests. This is before a single user has typed a single word.

The second flow is the **user input**. This is the actual query, question, or document the user submits. It is variable per request and typically the smallest token component in a well-designed system. A short question might be 30 tokens. A paragraph of context might be 100 tokens. A pasted document might be 2,000 tokens. The key characteristic of user input is that you cannot control its length without limiting your users. You can set maximum input lengths, but within those limits, the cost varies with each request.

The third flow is **retrieved context**. In any system using retrieval-augmented generation, the model receives chunks of retrieved documents alongside the user query. A typical RAG setup might retrieve three to five chunks, each 300 to 600 tokens long, adding 900 to 3,000 tokens of input per request. This is often the largest single token component. A customer service system that retrieves relevant knowledge base articles might inject 2,000 tokens of context. A legal research system that retrieves case law excerpts might inject 4,000 to 8,000 tokens. Every additional retrieved chunk adds cost, and the retrieval pipeline itself has no awareness of token cost. It retrieves whatever meets the relevance threshold, regardless of how many tokens that represents.

The fourth flow is **few-shot examples**. Many systems include example input-output pairs in the prompt to guide the model's behavior. Three examples, each with a 50-token input and a 100-token output, add 450 tokens to every request. Five examples with longer outputs can add 1,000 tokens or more. Like the system prompt, few-shot examples are typically fixed per request, making them a constant cost multiplier. Teams often add examples during development to fix edge cases and never remove them, even after the model has been updated and may no longer need the guidance. This is token waste that compounds over every request.

The fifth flow is **output tokens**. These are the tokens the model generates in response. As covered in the previous subchapter, output tokens cost three to eight times more than input tokens. A 500-token response on GPT-5 costs five thousandths of a cent per token, while a 500-token input costs about one-sixth of a thousandth of a cent per token. Output is where The Output Premium hits your budget hardest. And output length is partially under your control. Asking the model to "explain in detail" produces more output tokens than asking it to "answer briefly." Requesting structured output with verbose field names produces more tokens than requesting terse formats. Allowing the model to hedge, qualify, and caveat produces more tokens than instructing it to be direct.

The sixth flow is **reasoning tokens**. This is the cost category that did not exist before 2024 and now accounts for a significant share of bills for teams using reasoning-capable models. When you use OpenAI's o-series models, Claude with extended thinking, or Gemini with reasoning mode, the model generates internal reasoning tokens before producing the visible response. These reasoning tokens are billed as output tokens at the full output rate, even though the user never sees them. A model that generates 800 tokens of internal reasoning and then 400 tokens of visible response costs you for 1,200 output tokens, not 400. For complex queries, reasoning tokens can outnumber visible output tokens by ten to one or more. A math problem that requires extended thinking might generate 3,000 reasoning tokens and 200 visible output tokens. You pay for 3,200 output tokens. At Claude Opus 4.5 rates of twenty-five dollars per million output tokens, those 3,000 invisible reasoning tokens cost seven and a half cents. For a single query. The insurance company in the opening story was using a reasoning model and had not accounted for reasoning tokens at all.

The seventh flow is **overhead tokens**: retries, evaluations, and monitoring. When a request fails due to a malformed response, a content filter trigger, or a timeout, you still pay for every token the model processed before the failure. If 18% of your requests require a retry, you are paying for roughly 18% more tokens than your successful request count suggests. On top of retries, many production systems run per-request evaluations where a second LLM call scores the quality of the primary response. That evaluation call has its own token cost. If you run evaluation on every request, you effectively double your LLM spend. If you sample 10% of requests for evaluation, you add 10% to your total cost. Monitoring systems that log full prompts and responses consume storage and processing tokens. Guardrail checks that run a classifier model on every output add their own token cost. None of these show up in a naive cost calculation that only counts the primary request and response.

## The Token Accounting Worksheet

To calculate the true cost per query, you need to sum all seven flows and apply the correct pricing to each. Walk through this in concrete terms for a realistic system.

Consider a RAG-based customer service assistant running on GPT-5. The system prompt is 900 tokens. The average user query is 120 tokens. The RAG pipeline retrieves three chunks averaging 500 tokens each, for 1,500 tokens of context. There are two few-shot examples totaling 350 tokens. The average visible response is 400 tokens. There are no reasoning tokens because the team is using the standard GPT-5, not a reasoning model.

Total input tokens per request: 900 plus 120 plus 1,500 plus 350 equals 2,870. Total output tokens per request: 400. At GPT-5 pricing, input cost is 2,870 divided by one million times one dollar twenty-five cents, which equals roughly 0.36 cents. Output cost is 400 divided by one million times ten dollars, which equals 0.4 cents. Base cost per query: 0.76 cents, or just under a penny.

Now add the hidden costs. The system has a 15% retry rate due to occasional content filter triggers and timeout errors. That means for every 100 successful queries, there are roughly 18 additional partial requests that consume tokens before failing. Assume retries consume on average 70% of the tokens of a successful request before failing. That adds 18 times 70% times the base cost, which is roughly 9.5% additional cost. The system also runs an LLM-as-judge evaluation on 20% of queries using GPT-5-mini to check response quality. Each evaluation call consumes about 1,200 input tokens, including the original query, the response, and the evaluation prompt, plus 150 output tokens for the judgment. At GPT-5-mini pricing of twenty-five cents per million input and two dollars per million output, each evaluation costs about 0.06 cents. Running on 20% of queries adds 0.012 cents per query on average.

Adjusted cost per query: 0.76 cents base, plus roughly 0.07 cents for retries, plus 0.012 cents for evaluation, totaling about 0.84 cents. That is 11% higher than the base calculation. The insurance company in the opening story saw a much larger gap because they were using a reasoning model, had a higher retry rate, and ran evaluation on every request.

## Why Naive Estimates Always Undercount

The pattern is consistent: naive cost estimates capture 60% to 75% of the actual cost. The gap comes from four sources that teams routinely ignore.

The first source is system prompt and context inflation. Teams build their system prompt iteratively. They start with a simple instruction. Then they add output format requirements. Then safety constraints. Then persona guidelines. Then edge case handling. Six months later the system prompt is 1,800 tokens and nobody has measured its length since the first version. Similarly, RAG pipelines start by retrieving two chunks and gradually increase to five or six as the team tries to improve answer quality. Each addition seems small. In aggregate, the input token count per request can double over six months without anyone noticing because no one is tracking input token consumption as a metric.

The second source is output length drift. Models do not produce consistent output lengths. The same prompt might generate 300 tokens one time and 600 the next. Teams estimate output length based on a few test queries and then multiply by their query volume. But the distribution has a long tail. 5% of queries might generate responses three times the average length. Those outlier responses are disproportionately expensive because they hit the output premium hardest. If your average output is 400 tokens but 5% of responses are 1,500 tokens, those 5% of queries account for about 15% of your output cost.

The third source is retry and failure costs. Most teams track their success rate but do not track the token cost of failed requests. A request that times out after the model has generated 300 tokens still costs you for those 300 output tokens. A request that triggers a content filter after processing 2,000 input tokens still costs you for those 2,000 input tokens. If your failure rate is 10%, your actual token consumption is roughly 5% to 8% higher than your successful query count times average tokens per successful query.

The fourth source is auxiliary LLM calls. Modern production systems rarely make a single LLM call per user query. There is the primary generation call, but there might also be a classification call to route the query, a guardrail call to check the output, an evaluation call to score quality, a summarization call to compress conversation history, or a tool-use call where the model decides which function to invoke. Each auxiliary call has its own token cost. A system with a router, a generator, and a guardrail makes three LLM calls per user query. The user sees one response. The bill reflects three calls.

## Building the Cost Model: From Estimates to Measurements

Estimates get you started. Measurements keep you honest. The Token Accounting Method has two phases.

Phase one is the estimate. Before you launch, calculate the expected cost per query using the seven token flows described above. List each flow, estimate its token count, apply the per-million pricing, and sum. Add a 20% buffer for retries, output length variance, and auxiliary calls. This gives you a launch estimate that is at least in the right order of magnitude.

Phase two is measurement. After launch, instrument your system to log the actual token counts for every request. Every API response from OpenAI, Anthropic, and Google includes token usage data: prompt tokens consumed, completion tokens generated, and for reasoning models, reasoning tokens generated. Log these numbers alongside each request. Calculate the actual average cost per query daily. Compare it to your estimate. Identify which token flows are higher than expected and investigate why.

The teams that succeed at cost engineering are the ones that treat cost per query as a first-class metric, right alongside latency, accuracy, and error rate. They have dashboards that show cost per query trending over time. They set alerts when cost per query exceeds a threshold. They investigate cost spikes the same way they investigate latency spikes. When a product change increases cost per query by 30%, they know about it the same day, not at the end of the month when the invoice arrives.

## The Multiplier Effect at Scale

Cost per query matters because it multiplies. A one-cent difference in cost per query sounds trivial until you multiply it by your query volume. At one thousand queries per day, one cent per query is ten dollars daily, three hundred dollars monthly. At one hundred thousand queries per day, one cent per query is one thousand dollars daily, thirty thousand dollars monthly. At one million queries per day, one cent per query is ten thousand dollars daily, three hundred thousand dollars monthly.

The insurance company that miscalculated their cost per query by four cents was processing twenty thousand queries per day. That four-cent gap cost them eight hundred dollars per day, twenty-five thousand dollars per month, more than the entire projected budget. They did not have a pricing problem. They did not have a volume problem. They had a measurement problem. They measured the tokens they could see and ignored the tokens they could not.

This is why the Token Accounting Method exists. It forces you to account for every token before you launch and then verify those accounts with real data after you launch. The insurance company could have avoided the entire budget overrun with two hours of work before launch: listing all seven token flows, estimating each one, summing the total, and adding a buffer. Instead, they estimated from the user-visible tokens and were surprised by three times their projection. The method is not complex. It is just thorough. And thoroughness is the difference between controlling your costs and being surprised by them.

## From Cost Per Query to Cost Per Feature

The Token Accounting Method does not stop at individual queries. It extends to features, workflows, and user sessions. A single feature in your product might trigger multiple queries. A "summarize this document" feature involves an initial chunking step, multiple LLM calls to summarize each chunk, and a final LLM call to merge the summaries. If the document is ten thousand tokens long and you chunk it into five parts, you are making six LLM calls for one user action. The cost of the feature is six times the cost per query, not one.

A conversation feature is even more expensive because of the compounding context problem, which a later subchapter in this chapter covers in detail. Each turn in a multi-turn conversation includes all previous turns in the input, which means input tokens grow with every exchange. The first turn might cost 0.5 cents. The fifth turn might cost 2.5 cents. The tenth turn might cost 5 cents. A ten-turn conversation does not cost ten times the first turn. It costs roughly thirty to fifty times the first turn because of the accumulating context.

Map your product's features and workflows to their token consumption. Identify which features are cheap per use, which are expensive per use, and which appear cheap but trigger many underlying LLM calls. This map is the foundation for the cost-quality tradeoff decisions covered in Section 25. You cannot make intelligent tradeoffs if you do not know what each feature actually costs at the token level.

## The Weekly Cost Review

Cost per query should be reviewed weekly by the engineering team and monthly by product and finance. The weekly review checks three things. First, has the average cost per query changed since last week? If it went up, why? A new system prompt addition, a RAG pipeline change, or an increase in output verbosity can all shift cost per query. Second, what is the distribution of cost per query? The average hides outliers. If 5% of queries cost ten times the average, those outliers might represent a bug, a specific query type that triggers excessive reasoning tokens, or a retrieval pipeline that sometimes returns far more context than needed. Third, are the actual costs tracking to the estimates? If the estimate said 0.8 cents and the actual is 1.2 cents, there is a gap to investigate. The gap will not close on its own.

The monthly review rolls the per-query cost up to total spend and compares it to the budget. It also projects forward: if query volume is growing 15% month over month and cost per query is stable, what does next quarter look like? If a new feature is launching that adds an auxiliary LLM call per query, what will that do to the total bill? The monthly review is where engineering and finance meet. The engineer speaks in tokens per query. The finance team speaks in dollars per month. The Token Accounting Method provides the translation layer between them.

The next subchapter examines what happens when teams try to solve quality problems by throwing more context at the model, and why that decision is far more expensive than it appears.