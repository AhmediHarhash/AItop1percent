# 8.6 — Denial-of-Wallet Attacks: When Adversaries Target Your Cost Layer

A denial-of-service attack takes your system offline. A denial-of-wallet attack keeps your system running but makes it unprofitable. The second is harder to detect and, for many companies, just as damaging. Traditional DDoS protection focuses on availability — keeping the lights on, keeping requests flowing. But in AI systems where every request costs real money in model inference, an attacker does not need to crash your servers. They just need to make you process enough expensive requests to blow through your budget. Your system stays up. Your dashboards look healthy. Your users experience no interruption. And your monthly bill arrives with a number that puts a hole in your P-and-L.

**Denial-of-wallet (DoW)** is not a theoretical risk. The OWASP LLM Top 10 for 2025 elevated this threat to its own category — LLM10: Unbounded Consumption — recognizing that the pay-per-token economics of AI create an entirely new attack surface that traditional security tools do not cover. As agentic systems that reason, plan, and execute multi-step workflows became standard production deployments in 2025 and 2026, the cost attack surface expanded from single inference calls to entire agent workflows that can be triggered by a single carefully crafted request.

## The Attack Surface: Every Inference Endpoint Is a Cost Target

In a traditional web application, the cost of processing a request is relatively uniform. A REST API call that queries a database and returns JSON costs roughly the same amount of compute whether the query is simple or complex. The cost difference between the cheapest and most expensive request might be a factor of two or three.

In an AI system, the cost difference between requests can be a factor of a hundred or more. A simple classification query that sends 200 input tokens and receives 10 output tokens costs roughly $0.0008 at mid-tier API pricing. An agent workflow that triggers five tool calls, each involving a separate model inference, processes 15,000 input tokens across the chain, and generates 5,000 output tokens can cost $0.12. That is a 150-to-1 ratio between the cheapest and most expensive request types — and both come in through the same API endpoint, both pass the same rate limiter, and both count as "one request" in your access logs.

This cost asymmetry is the foundation of every denial-of-wallet attack. The attacker's goal is to selectively trigger the most expensive operations your system offers, at the highest volume they can sustain, for as long as possible before detection. They don't need to find a vulnerability in your code. They don't need to bypass authentication. They just need to use your product in the most expensive way possible.

## Attack Pattern One: Volume Flooding

The simplest denial-of-wallet attack is brute-force volume. The attacker sends as many requests as possible to an endpoint that triggers model inference. Each request is individually cheap — maybe $0.01 to $0.05 — but the volume makes it expensive.

An unprotected API endpoint handling 1,000 attack requests per minute at $0.03 per request costs $30 per minute, $1,800 per hour, and $43,200 per day. A weekend-long attack — 48 hours before anyone at the company notices on Monday morning — costs $86,400. That is not a theoretical calculation. That is the math of an unprotected endpoint that any attacker with a script and an API key can exploit.

Volume flooding is the least sophisticated attack and the easiest to detect. High request rates from single IP addresses or API keys trigger standard rate limiters. But attackers who distribute their traffic across hundreds of IP addresses, use rotating proxies, or exploit free-tier accounts can bypass simple rate limits. The defense is not just rate limiting by IP — it is rate limiting by cost, which we will cover in the defenses section.

## Attack Pattern Two: Prompt Stuffing

**Prompt stuffing** is more insidious than volume flooding because it maximizes cost per request rather than request count. The attacker sends requests with maximally long input payloads — filling the input field with thousands of words of irrelevant text to inflate the input token count.

Most AI endpoints accept user input without a strict character limit. A text input field designed for a 50-word question can be stuffed with 10,000 words of lorem ipsum, Wikipedia content, or repeated characters. The model processes all of those tokens and you pay for all of them. If your model charges $3.00 per million input tokens, a single request stuffed with 50,000 tokens of junk costs $0.15 in input alone — fifteen times what a normal request costs.

Prompt stuffing is particularly effective against RAG systems where the user query is concatenated with retrieved context before being sent to the model. An attacker who sends a 5,000-token query triggers the retrieval of several context chunks, and the final prompt sent to the model might be 20,000 to 30,000 tokens — the attacker's stuffed query plus the system's own context. The attacker pays for 5,000 tokens of junk input. You pay for 30,000 tokens of model inference. The attacker has leveraged your own retrieval pipeline to amplify their cost attack by a factor of six.

## Attack Pattern Three: Output Manipulation

While prompt stuffing inflates input cost, **output manipulation** targets the more expensive side of the token equation. Output tokens are typically three to five times more expensive than input tokens. An attacker who can cause the model to generate maximally long responses inflicts more financial damage per request than one who stuffs the input.

The technique is straightforward. The attacker crafts prompts that encourage verbose, lengthy responses. "Write a detailed, comprehensive, step-by-step guide to..." or "List every possible consideration for..." or "Explain in exhaustive detail..." — these prompt patterns reliably produce responses of 2,000 to 4,000 tokens or more. Without output length caps, the model happily generates extended responses, and you pay for every token.

A more targeted variant combines output manipulation with instruction following. The attacker asks the model to repeat specific phrases hundreds of times, generate lists with thousands of items, or produce responses in formats that inherently expand output length — such as requesting every answer in a specific verbose template. Some models can be coaxed into generating near-maximum output through careful prompt engineering, turning every request into a maximum-cost event.

At $15.00 per million output tokens, a single response of 4,000 tokens costs $0.06 in output alone. An attacker sending 500 such requests per hour generates $30 per hour in output cost, or $720 per day. Combined with the input cost, the total can exceed $1,000 per day from a single attacker using nothing more than a well-crafted prompt template and a loop.

## Attack Pattern Four: Agent Exploitation

Agent-based systems present the richest target for denial-of-wallet attacks because a single request can trigger a cascade of expensive operations. When an attacker sends a request to an agent system, the agent may reason through multiple steps, invoke several tools, call the model multiple times for planning and execution, and trigger external API calls — each of which costs money.

An agent that is designed to research a topic might perform five web searches, retrieve ten documents, call the model three times for synthesis, and generate a final report. The total cost of this single request could be $0.50 to $2.00. An attacker who triggers this workflow 100 times per hour — well within most rate limits for "legitimate" usage — generates $50 to $200 per hour in cost.

The attack becomes more severe when the agent has access to expensive tools. An agent that can call paid APIs — geocoding services, financial data feeds, premium search engines — multiplies the attacker's damage beyond just model inference cost. A single agent invocation that triggers three paid API calls at $0.10 each adds $0.30 in tool cost on top of the inference cost. The attacker is spending your money on third-party services they would otherwise have to pay for themselves.

The most dangerous variant is the **recursive agent attack**, where the attacker crafts a prompt that causes the agent to enter a reasoning loop — continuously planning, executing, and re-planning without converging on a final answer. If the agent does not have a hard limit on the number of reasoning steps or tool calls per request, a single malicious prompt can trigger dozens of model inferences before timing out. Teams building agent systems in 2025 and 2026 learned this lesson painfully, as OWASP specifically called out resource exhaustion through infinite agent loops as a critical threat vector.

## Attack Pattern Five: Conversation Exploitation

As covered in the previous subchapter, conversation systems where context grows with each turn are vulnerable to cost-per-turn escalation. An attacker can deliberately maintain extremely long conversations, sending short messages at regular intervals to keep the session alive while the accumulated context — and its cost — grows with every turn.

A bot that opens ten simultaneous conversation sessions and sends a message to each every sixty seconds can grow all ten sessions to 100 turns over the course of two hours. If each session's per-turn cost reaches $0.30 by turn 100, those ten sessions cost $3.00 per minute in the later turns. The attacker needs only to sustain the conversations. The cost escalation is built into the architecture.

This attack is uniquely difficult to detect because it looks like legitimate usage. The messages are real text. The conversation topics may be plausible. The pacing is human-like. Only the aggregate cost pattern — ten concurrent sessions with unusually deep conversation threads from a single user or IP range — reveals the attack.

## Detection: Spotting Cost Attacks Before They Drain Your Budget

Denial-of-wallet attacks share a common signature: abnormal cost patterns that deviate from your baseline. The challenge is distinguishing attack traffic from legitimate power users. The solution is multi-dimensional anomaly detection on cost metrics.

Track **per-user cost rate** — the dollars consumed per user per hour. Establish a baseline for normal users. If a user exceeds three times the 95th percentile cost rate, flag the account for review. If they exceed five times, apply temporary rate reduction automatically while alerting the security team.

Track **per-request cost distribution**. Your normal requests follow a cost distribution — perhaps a median of $0.02 with a 95th percentile of $0.08. Requests that cost $0.50 or more are statistical outliers. A sudden increase in high-cost outlier requests from specific users or IP ranges indicates prompt stuffing or output manipulation.

Track **input token distribution per request**. If your average input is 500 tokens and you start seeing requests with 20,000 to 50,000 input tokens, those are likely prompt-stuffed attacks. Normal users do not paste 10,000 words into a question field.

Track **output token distribution per request**. If your average output is 400 tokens and a cohort of requests is consistently generating 3,000 to 4,000 token responses, investigate the prompts. They may be crafted to maximize output length.

Track **agent step count per request**. If your agents typically complete in 3 to 5 steps and you see requests triggering 15 to 20 steps, those requests may be designed to exploit the agent loop.

The most effective detection systems combine multiple signals. A single request with 30,000 input tokens is suspicious. A user sending fifty such requests per hour from rotating IP addresses is an active attack.

## The Critical Distinction: Attackers vs Heavy Users

One of the hardest problems in DoW defense is distinguishing malicious traffic from legitimate heavy usage. Your power users are expensive too. A data analyst who runs fifty complex queries per day through your AI platform is not an attacker — they are exactly the customer your product was built for. A developer who uses your coding assistant for twelve hours straight is not abusing the system — they are getting massive value and will likely renew their subscription.

The distinction matters because the defenses that stop attackers — rate limits, cost caps, throttling — also constrain power users. Aggressive rate limiting frustrates your best customers. Tight cost caps prevent legitimate complex work. The goal is not to minimize cost per user. It is to minimize cost from users who provide zero value while maximizing the headroom available to users who provide the most value.

Several behavioral signals help separate the two populations. Attackers typically show low engagement depth — they send requests but don't meaningfully interact with responses, don't follow up with refinements, don't copy or share results. Power users show high engagement — they read, modify, iterate, and build on previous responses. Attackers often operate from newly created accounts with minimal profile information and no payment history. Power users have established accounts with billing records and usage history. Attackers send requests at machine speed with unnaturally consistent timing — exactly 2.3 seconds between each request, for hours. Power users send requests at human speed with natural variance — bursts of activity followed by pauses for reading.

The practical approach is a risk scoring model that combines these signals into a per-user or per-session risk score. Users with high risk scores — new accounts, machine-speed pacing, low engagement, anomalous cost patterns — get tighter rate limits and cost caps applied automatically. Users with low risk scores — established accounts, natural usage patterns, high engagement, payment history — get generous limits that never interfere with legitimate work. This tiered approach protects your budget from attackers while preserving the experience for the customers who matter most to your revenue.

## Defense Layer One: Per-User Cost Budgets

The most fundamental defense against denial-of-wallet attacks is a **per-user cost budget** — a hard cap on how much any single user can spend per time window. This is different from a rate limit. A rate limit caps requests per minute. A cost budget caps dollars per hour.

The distinction matters because of cost asymmetry. A rate limit of 60 requests per minute allows an attacker to send 60 prompt-stuffed requests at $0.15 each — $9.00 per minute, $540 per hour. A cost budget of $5.00 per hour caps the damage at $5.00 regardless of how the attacker distributes their requests.

Implementing per-user cost budgets requires estimating request cost before execution. For input cost, this is straightforward — count the input tokens before sending the request. For output cost, you need an estimate. Use the model's maximum output length setting as the upper bound, or use historical averages for your specific endpoint. The estimate does not need to be precise — it needs to be conservative enough that the budget is not exceeded.

When a user hits their cost budget, the system should respond gracefully. Return a clear message indicating the limit has been reached and when it will reset. Do not silently degrade service — that confuses legitimate users. Do not reveal the exact budget amount — that helps attackers optimize their approach. Simply indicate that usage limits have been reached for the current period.

## Defense Layer Two: Input and Output Guardrails

**Input length limits** are the simplest and most effective defense against prompt stuffing. Set a maximum input length for each endpoint based on legitimate use cases. If your question-answering system expects queries of 50 to 200 words, set a hard limit of 1,000 tokens. Any input exceeding that limit is rejected before it reaches the model. The rejected request costs you nothing.

**Output length caps** defend against output manipulation. Set the model's maximum output tokens parameter to a reasonable value for each endpoint. A customer support bot does not need to generate 4,000-token responses. Cap output at 800 tokens. A summarization endpoint does not need 10,000-token summaries. Cap at 2,000. The cap prevents the model from generating runaway output regardless of how the prompt is crafted.

**Agent step limits** defend against recursive agent attacks. Set a hard maximum on the number of reasoning steps, tool calls, or model inferences per request. An agent that normally completes in 3 to 7 steps should have a hard limit of 15. If the agent reaches 15 steps without completing, it returns a graceful failure rather than continuing to consume resources. This limit prevents infinite loops and caps the maximum cost of any single agent invocation.

These guardrails are not rate limits — they are per-request cost caps. They ensure that no single request, regardless of how it is crafted, can exceed a known maximum cost. Combined with per-user cost budgets, they create a two-layer defense: individual requests are bounded, and aggregate usage is bounded.

## Defense Layer Three: Pre-Execution Cost Estimation

The most sophisticated defense is **pre-execution cost estimation** — calculating the expected cost of a request before executing it and rejecting requests that would exceed a cost threshold.

The process works as follows. A request arrives. The system counts input tokens and estimates the output token count based on the endpoint type and prompt characteristics. It estimates the total cost using current API pricing. If the estimated cost exceeds the endpoint's per-request threshold — say $0.25 — the request is rejected with a message indicating that the request is too complex to process. If the estimated cost is within the threshold but would push the user's hourly budget over the limit, the request is queued or rejected.

Pre-execution cost estimation is imperfect because output token count cannot be precisely predicted. But it does not need to be precise. It needs to catch the obvious attacks — the 50,000-token stuffed inputs, the prompts designed to elicit maximum-length responses. A conservative estimate that occasionally rejects legitimate complex queries is far better than no estimate that allows every attack through.

For agent systems, pre-execution estimation is harder because the number of steps is unpredictable. The approach here is staged budgeting: allocate a cost budget for the request and consume it as the agent executes. After each step, check whether the remaining budget is sufficient for another step. If not, stop the agent and return the partial result. This prevents runaway agent costs without requiring upfront prediction of agent behavior.

## Defense Layer Four: Authentication and Friction

Unauthenticated endpoints are the easiest targets for denial-of-wallet attacks. An attacker who does not need to create an account can generate unlimited traffic with no accountability. Even free-tier accounts create a barrier — the attacker must create accounts, which can be rate-limited, require email verification, or trigger CAPTCHA challenges.

For endpoints that offer unauthenticated access — demo pages, trial endpoints, public-facing chatbots — add deliberate friction that slows attackers without meaningfully impacting legitimate users. CAPTCHA challenges before the first request. Rate limits of 5 requests per session without authentication. Progressive delays that increase latency after the 10th request in a session. These friction mechanisms are not security measures in the traditional sense. They are economic measures — increasing the attacker's cost of generating each request until the attack becomes unprofitable.

The key principle is that the cost of launching the attack must exceed the damage it inflicts. If an attacker needs to solve a CAPTCHA for every request and can only send 3 requests per minute from each account, the attack generates roughly $5 per hour in damage — annoying but not devastating. Compare that to an unprotected endpoint where 1,000 unauthenticated requests per minute generates $1,800 per hour in damage. The friction reduced the attack's economic effectiveness by 99.7%.

## The Economic Damage Calculation

Every team running AI in production should calculate their maximum exposure to denial-of-wallet attacks. The formula is straightforward. Take the maximum cost per request your system can process — the most expensive possible inference call, including tool calls and agent steps. Multiply by the maximum request rate your system accepts without rate limiting. Multiply by the duration in hours before your monitoring would detect the attack and your team could respond.

For a system with a maximum per-request cost of $0.50, a rate limit of 100 requests per minute, and a detection-to-response time of 4 hours: $0.50 times 100 times 60 times 4 equals $12,000. That is your maximum exposure from a single attack source. If the attacker uses 10 accounts, multiply by 10. If your system has no per-user cost limits, multiply by however many free accounts the attacker can create.

That number — your maximum DoW exposure — should be compared to your monthly AI budget. If a single weekend attack can consume 20% of your monthly budget, your defenses are insufficient. The target is a maximum exposure low enough that an attack is an annoyance, not a financial crisis. For most systems, that means single-attack exposure should be capped at less than 1% of monthly budget, which requires the layered defenses described above.

## Monitoring for Active Attacks

Detection without response is useless. Your monitoring system needs automated responses for cost attacks, not just alerts that page an engineer who may be sleeping.

When per-user cost exceeds the alert threshold, automatically reduce that user's rate limit to the minimum viable level — one request per minute, for example. This buys time for human review without completely blocking a potentially legitimate user. If the user is real, they experience a temporary slowdown. If the user is an attacker, the damage rate drops by 98%.

When aggregate cost rate for the entire system exceeds 150% of projected baseline, trigger an automated audit. Compare current-hour API spend to the same hour on previous days. If the deviation cannot be explained by organic growth, escalate to the security team.

When a single endpoint shows a sudden increase in high-cost requests — defined as requests above the 95th percentile cost — apply temporary input length restrictions and output length caps on that endpoint. This narrows the cost corridor available to the attacker without taking the endpoint offline.

The goal is layered, automated response that contains the financial damage within minutes, not hours. Human review follows automated containment. The engineer investigating at 3 AM should be analyzing an attack that was already contained, not watching the bill climb in real time.

## The Connection to Broader Security

Denial-of-wallet attacks sit at the intersection of cost engineering and security engineering, and neither team typically owns the problem fully. Security teams focus on data exfiltration, injection attacks, and system compromise — threats to confidentiality and integrity. Infrastructure teams focus on uptime and performance — threats to availability. Neither team is typically watching the cost dimension, which is why DoW attacks succeed.

The organizational fix is explicit ownership. Someone — a team, a role, a person — must own cost-layer security. They attend both the security review and the cost review. They ensure that every new endpoint is assessed for DoW exposure before launch. They review per-user cost budgets quarterly. They own the automated containment logic. Without this ownership, DoW defense falls into the gap between teams, and attackers exploit gaps.

For a deeper treatment of the broader AI security landscape, including prompt injection, data poisoning, and model extraction attacks, see Section 16. The cost layer is one attack surface among many, but it is the one most likely to cause immediate financial damage without triggering any of the security alerts your team already has in place.

The next subchapter extends the adversarial lens to token exhaustion and billing abuse — the patterns where attackers or even careless internal users exploit your system's economic structure to consume resources you never intended to offer.
