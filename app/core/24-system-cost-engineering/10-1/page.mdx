# 10.1 — Cost Budgets and Guardrails: Per-Request, Per-Tenant, and Per-Team Limits

A system without cost budgets is a system without cost control. Budgets are not constraints on innovation. They are the infrastructure that makes sustainable innovation possible. Every mature engineering discipline operates within budgets — structural engineers have load budgets, aerospace engineers have weight budgets, electrical engineers have power budgets. AI engineering is no different. Your system consumes dollars the way a circuit consumes watts: continuously, variably, and with catastrophic consequences when consumption exceeds capacity. Yet as of early 2026, the vast majority of AI systems in production run without any cost budget enforcement at all. They track spend after the fact. They review invoices monthly. They discover overruns weeks after they occur. This is not cost engineering. This is cost archaeology — sifting through the wreckage of last month's bill trying to understand what happened.

The alternative is cost budgets: explicit, enforced limits on how much your system can spend at every level. Per-request budgets cap the cost of individual interactions. Per-tenant budgets cap the cost of individual customers. Per-team budgets cap the cost of internal consumers. Together, they form a three-layer defense that makes runaway spend structurally impossible rather than merely unlikely. The organizations that implement all three layers are the ones that can forecast their AI costs within 10 percent month over month. The organizations that implement none are the ones that discover they've burned through their quarterly AI allocation in six weeks.

## Per-Request Budgets: The First Line of Defense

**Per-request budgets** set the maximum cost any single request can incur. They are the finest-grained control in your cost governance architecture, and they are the most important because every higher-level budget is ultimately a function of how much each individual request costs.

A per-request budget is implemented as a composite limit. You set a maximum token ceiling for input — say, 12,000 tokens — and a maximum token ceiling for output — say, 4,000 tokens. You combine this with a model tier restriction: requests in this product category can use models up to the mid-tier price point, not the frontier tier. You add a tool call limit: no single request can trigger more than eight external tool invocations. And you set a total cost cap: regardless of how the tokens and tools combine, the total cost of serving this request cannot exceed $0.35. If any of these limits would be breached, the system takes a predefined action before the cost is incurred, not after.

The typical per-request budget varies dramatically by product tier and use case. A consumer chatbot answering simple questions might set a per-request budget of $0.05 to $0.15. A B2B document analysis tool processing complex contracts might set it at $0.30 to $0.80. An agentic workflow that researches topics, compares sources, and drafts reports might set it at $1.00 to $3.00. The number itself matters less than the fact that it exists and is enforced. A per-request budget of $2.00 that is actively enforced protects your system far better than a vague aspiration to keep costs "reasonable."

What happens when a request would exceed its budget? You have three options, and the right choice depends on your product. The first option is truncation: reduce the input to fit within the token ceiling. If a user submits a 25,000-token document but your per-request input ceiling is 12,000 tokens, the system extracts the most relevant 12,000 tokens and proceeds. The user gets a response based on partial context. This works for RAG systems where the retrieval pipeline can select the most relevant chunks. It does not work for tasks where completeness matters, like legal document review. The second option is degradation: switch to a cheaper model that can serve the request within budget. If the request would cost $0.45 on Claude Opus 4.6 but your budget is $0.35, the system routes to Claude Sonnet 4.5, which serves the request for $0.12. The user gets a slightly less capable response but stays within budget. The third option is rejection: return an error explaining that the request exceeds the system's capacity for this product tier. This sounds harsh, but it is the correct response for requests that would cost $5 or $10 due to massive context or runaway agent behavior. Better to tell the user the request is too large than to silently incur costs that destroy your unit economics.

Most teams implement a tiered approach. Requests that exceed the budget by less than 20 percent get degraded to a cheaper model. Requests that exceed by 20 to 100 percent get truncated and degraded. Requests that exceed by more than 100 percent get rejected with an explanation. This graduated response handles the vast majority of cases without creating a jarring user experience.

## Per-Tenant Budgets: Controlling Customer-Level Spend

**Per-tenant budgets** set the maximum cost any single customer can generate within a time period, typically monthly. They solve a different problem than per-request budgets. Per-request budgets protect you from expensive individual interactions. Per-tenant budgets protect you from customers who send enormous volumes of individually reasonable requests.

Consider a B2B AI platform where each request costs an average of $0.08. A typical customer sends 5,000 requests per month, generating $400 in AI cost. That is comfortable within the $800 monthly subscription price, leaving a 50 percent gross margin. But one customer discovers they can automate their entire research workflow through your API. They send 150,000 requests per month. Their AI cost is $12,000. Their subscription pays $800. You are losing $11,200 per month on a single customer, and that customer alone is consuming more AI budget than your next fifty customers combined. This is not a hypothetical. This pattern played out repeatedly across B2B AI companies in 2024 and 2025. A healthcare analytics company discovered that one hospital system was generating 38 percent of their total AI spend while representing 4 percent of their revenue. Without per-tenant budgets, they had no mechanism to detect or prevent this imbalance until the quarterly financial review.

Per-tenant budgets require three components. First, you need tenant-level cost attribution — the ability to calculate how much each customer costs you per day and per month. This means tagging every model call, every retrieval query, every tool invocation, and every evaluation run with the tenant identifier. Without attribution, you cannot enforce budgets because you cannot measure consumption. Second, you need a budget allocation model. The simplest approach is a flat budget: every customer on the same plan gets the same monthly budget. A more sophisticated approach is proportional budgeting, where the budget scales with the customer's subscription tier. An enterprise customer paying $5,000 per month gets a $2,500 AI cost budget. A startup customer paying $200 per month gets a $100 AI cost budget. The budget is set at 50 percent of revenue to maintain a minimum gross margin. Third, you need enforcement actions. When a tenant reaches 80 percent of their budget, the system sends an alert to your customer success team and to the customer's admin dashboard. At 90 percent, the system begins degrading service — routing to cheaper models, reducing context window size, disabling optional features. At 100 percent, the system either hard-stops AI features or continues serving at the degraded level until the next billing cycle, depending on your business model.

The hardest part of per-tenant budgets is not the technical implementation. It is the business decision about what happens when your best customer hits their limit with two weeks left in the month. The engineering answer is straightforward: enforce the budget. The business answer is more nuanced. You don't want to shut down a customer who is paying you $50,000 per year because they had a busy week. The solution most teams arrive at is a soft overage model: the customer can exceed their budget by up to 25 percent, and the overage is either absorbed or billed as a usage surcharge. Beyond 25 percent, the system degrades. This approach protects your margins while avoiding the customer experience disaster of hard cutoffs.

## Per-Team Budgets: Internal Accountability

**Per-team budgets** bring cost discipline inside your own organization. They answer a question that becomes critical as AI adoption scales across a company: which team is spending how much, and is that spend justified?

Without per-team budgets, AI costs appear as a single line item on the cloud bill. The search team, the chat team, the analytics team, the internal tools team, and the experimental projects team all consume from the same pool. Nobody knows who is spending what. Nobody is accountable for optimizing their share. When the monthly bill spikes by 30 percent, nobody can explain which team caused it without a forensic investigation that takes days. This is the organizational equivalent of a shared credit card with no statement — everyone spends freely because no individual bears the cost.

Per-team budgets fix this by allocating a monthly AI budget to each team or product feature. The search team gets $15,000 per month. The customer support AI gets $25,000 per month. The analytics pipeline gets $8,000 per month. The experimental projects pool gets $5,000 per month. These numbers are not arbitrary — they are derived from current consumption data, projected growth, and the revenue each team's features generate. A team whose features generate $200,000 per month in revenue can justify a $40,000 AI budget. A team whose features are pre-revenue gets a smaller allocation with explicit expectations about when the spend will be justified by revenue.

The enforcement mechanism for per-team budgets is different from per-tenant budgets because you are dealing with internal teams, not customers. The most effective model uses three stages. Stage one: visibility. Every team can see their daily and weekly AI spend in a shared dashboard, broken down by model, by feature, and by cost surface. Visibility alone reduces waste by 10 to 20 percent because teams naturally optimize when they can see the meter running. Stage two: alerts. When a team reaches 70 percent of their monthly budget with more than 40 percent of the month remaining, they get an alert. This gives them time to investigate whether the spend is justified or whether something is wrong — a runaway experiment, a misconfigured pipeline, a prompt regression that is increasing token consumption. Stage three: approval gates. When a team needs to exceed their budget, they submit a request with a justification. This is not bureaucracy — the request is a one-paragraph explanation and the approval is same-day. The point is to create a moment of deliberate decision-making rather than passive overspending.

A logistics company implemented per-team budgets in mid-2025 after discovering that their total AI spend had grown 4x in six months without a corresponding increase in revenue. The investigation revealed that a single data science team had been running large-scale experiments using GPT-5 at frontier pricing for tasks that could have used GPT-5-mini. The experiments were legitimate research, but nobody had asked whether they needed the most expensive model. After implementing per-team budgets, total AI spend dropped 22 percent in the first quarter — not because teams did less, but because teams made deliberate choices about which model to use and how many experiments to run per week.

## Enforcement Mechanisms: Soft, Hard, and Progressive

The effectiveness of any budget depends entirely on its enforcement mechanism. A budget that alerts but never acts is a suggestion, not a budget. A budget that hard-stops without warning is a reliability risk. The right enforcement model uses all three types.

**Soft limits** alert but allow. When spend reaches 80 percent of budget, the system sends a notification to the responsible team or customer. The notification includes current spend, projected end-of-period spend based on current trajectory, the top cost drivers, and suggested actions. Soft limits are appropriate for internal teams, enterprise customers with negotiated terms, and any context where a hard stop would cause more damage than the overage. The danger of soft limits is that they become noise. If the alert fires every month and nobody acts on it, it trains the organization to ignore budget signals. Soft limits work only when someone is accountable for responding to them.

**Hard limits** reject or disable when the budget is reached. The system stops serving requests — or stops serving AI-powered requests and falls back to a non-AI alternative — when the budget hits zero. Hard limits are appropriate for free tiers, trial accounts, internal experimental projects, and any context where the cost of overage exceeds the cost of service disruption. The danger of hard limits is that they can cause outages for legitimate use. If your per-request hard limit is set too low, it will reject valid requests that happen to have large context windows. If your per-tenant hard limit does not account for seasonal spikes, it will cut off customers during their busiest period. Hard limits require careful calibration and regular review.

**Progressive limits** degrade in stages as budget consumption increases. This is the most sophisticated and most practical enforcement model. From zero to 80 percent of budget, the system operates normally — full model capability, full context windows, all features enabled. From 80 to 90 percent, the system begins mild degradation — routing more traffic to mid-tier models, reducing context window by 15 percent, increasing cache aggressiveness. From 90 to 100 percent, the system applies moderate degradation — routing all non-critical traffic to economy models, reducing context window by 30 percent, disabling optional features like per-request evaluation. Beyond 100 percent, the system applies maximum degradation — economy models only, minimal context, cached responses wherever possible. The user experience degrades gradually rather than falling off a cliff. The spend decelerates naturally because cheaper models and smaller contexts cost less per request. A system that was spending $500 per day at full capability might spend $200 per day at maximum degradation — still serving requests, still providing value, but at a fraction of the cost.

Progressive limits are the model that most production teams converge on after trying the alternatives. Pure soft limits let spending run wild. Pure hard limits create outages. Progressive limits find the middle ground: spend is controlled, service continues, and the degradation itself signals to stakeholders that the budget needs attention.

## Setting the Right Budget Numbers

The most common mistake in cost budgeting is setting numbers based on what feels reasonable rather than what the data supports. A budget of $20,000 per month for the search team sounds reasonable until you measure that they are currently spending $28,000 and cannot reduce to $20,000 without eliminating features. A per-request budget of $0.10 sounds tight until you measure that 95 percent of requests cost less than $0.06 and the budget only binds on the expensive 5 percent.

Start with measurement. Before setting any budget, measure current spend at every level for at least four weeks. Capture the daily average, the daily maximum, the 95th percentile, and the distribution by request type. Your per-request budget should be set at the 95th or 99th percentile of current request costs, not the average. Setting it at the average means half your requests would exceed it. Setting it at the 99th percentile means it only triggers for truly anomalous requests — the runaway agents, the massive context windows, the retry storms.

For per-tenant budgets, measure the cost distribution across your customer base. In most B2B AI products, the top 10 percent of customers generate 40 to 60 percent of total AI cost. Your per-tenant budget needs to accommodate legitimate heavy usage while capping the extreme outliers. A budget set at 2x the median customer cost would trigger for about 15 percent of customers. A budget set at 5x the median would trigger for about 3 percent. The right multiplier depends on your pricing model — if heavy users pay more, they deserve a higher budget. If all users pay the same, a tighter budget is necessary to protect margins.

For per-team budgets, start with current spend and add a growth buffer of 10 to 20 percent per quarter. Teams that are growing features or launching new products need more headroom. Teams that are in maintenance mode need less. Review per-team budgets quarterly and adjust based on actual consumption, feature roadmap, and revenue attribution. A team that shipped a new feature generating $50,000 in monthly revenue deserves a budget increase. A team whose feature was deprecated deserves a budget decrease.

## The Budget Dashboard: Making Spend Visible

Budgets without visibility are budgets in name only. Every stakeholder — from the engineer writing prompts to the VP approving the quarterly AI allocation — needs to see where spend stands relative to budget in real time.

The effective cost budget dashboard shows five things. First, current period spend versus budget for every level — per-request histogram, per-tenant scoreboard, per-team breakdown. Second, projected end-of-period spend based on current trajectory. If you have spent 60 percent of your budget with 50 percent of the month gone, the projection shows you will exceed budget by 20 percent. This forward-looking view is more actionable than the backward-looking view of what you've already spent. Third, the top cost drivers at each level. Which requests are the most expensive? Which tenants are consuming the most budget? Which team features have the highest cost per request? Fourth, budget utilization trends over time. Is the search team consistently hitting 95 percent of their budget, suggesting it is too tight? Is the analytics team consistently using only 40 percent, suggesting it is too loose or they are underinvesting? Fifth, alerts and enforcement actions taken. How many requests were degraded due to per-request limits? How many tenants hit their 80 percent warning? How many teams required budget exception approvals?

This dashboard is not a nice-to-have. It is the operational instrument panel for your cost governance system. Without it, budgets are set and forgotten. With it, budgets become a living part of how your organization manages AI spend.

## The Cultural Challenge

The technical implementation of cost budgets is the easy part. The cultural implementation is where teams struggle. Engineers resist per-request budgets because they feel like constraints on building the best possible product. Product managers resist per-tenant budgets because they fear customer churn. Team leads resist per-team budgets because they feel like a lack of trust.

The reframe that works is accountability, not restriction. Per-request budgets don't prevent you from building ambitious features — they force you to design those features to operate within sustainable cost envelopes. A per-request budget of $0.50 doesn't prevent agentic workflows. It requires you to design agentic workflows that solve the problem in five tool calls instead of fifty. Per-tenant budgets don't punish heavy users — they identify the customers whose usage patterns don't match their pricing tier, creating an opportunity for upselling rather than a reason for service disruption. Per-team budgets don't limit innovation — they make the cost of innovation visible, which is the prerequisite for making informed decisions about where to invest.

The organizations that succeed with cost budgets are the ones where leadership frames budgets as financial engineering, not financial policing. The budget is a design constraint, like a latency requirement or a reliability SLA. It shapes the solution without limiting the ambition.

The next subchapter covers what happens when budgets are breached — not with hard stops and error messages, but with graceful cost degradation that automatically reduces spend while keeping the system running.
