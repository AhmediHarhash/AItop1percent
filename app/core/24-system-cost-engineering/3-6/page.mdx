# 3.6 — Model Distillation Economics: Upfront Cost vs Ongoing Savings

**Model distillation** is the practice of using a large, expensive model to generate high-quality outputs, then training a smaller, cheaper model to replicate those outputs. The large model is the teacher. The small model is the student. The student never matches the teacher perfectly, but on a well-scoped task, it gets close enough that the quality gap is invisible to most users — and the cost gap is enormous. Distillation is not a research curiosity. By 2026, it is one of the most commonly deployed cost optimization strategies in production AI, responsible for some of the most dramatic cost reductions in the industry. DeepSeek demonstrated this at scale when they distilled DeepSeek-R1's reasoning capabilities into smaller models — the DeepSeek-R1-Distill-Qwen-7B, trained from Qwen2.5-7B, outperformed the much larger QwQ-32B-Preview on reasoning benchmarks. Microsoft showed similar results distilling Llama 3.1 405B into an 8-billion-parameter student that achieved roughly 21% better accuracy on natural language inference tasks than the undistilled 8B model. The economics are compelling because you pay the teacher's price once, during data generation, and then you pay the student's price forever after, during inference.

## How Distillation Creates Cost Leverage

The mechanism is straightforward. You have a task — say, generating concise product descriptions from raw product data. Your frontier model, Claude Opus 4.5, produces excellent descriptions. Each description costs approximately $0.04 in API charges because the model needs detailed instructions, examples, and the raw product data as input, and it generates 200 to 400 output tokens per description. Your system generates 80,000 descriptions per day. Monthly inference cost: approximately $96,000.

You want a cheaper model to do this work. But the cheaper model — Llama 4 Scout at 17 billion parameters — produces mediocre descriptions when prompted directly. The tone is inconsistent, the formatting drifts, and edge cases like products with unusual attributes get mangled. Prompting alone cannot close the gap because the smaller model lacks the instruction-following fidelity of the frontier model.

Distillation solves this by using the frontier model's outputs as training data. You run 100,000 product descriptions through Claude Opus 4.5, generating a high-quality dataset of input-output pairs. You then fine-tune Llama 4 Scout on those 100,000 pairs. The fine-tuned student model learns to produce descriptions that look like the teacher's — same tone, same structure, same handling of edge cases. The student will not handle every edge case perfectly. On average, it might score 90% as good as the teacher on your quality metrics. But it costs a tenth to run. The 10% quality gap is invisible on 85% of requests and noticeable only on the remaining 15%, which you can route to the teacher model via cascading if needed.

## The Four Cost Components of Distillation

Distillation has four distinct cost components. Each must be calculated independently because they scale differently and hit different budget lines.

**Generation cost** is what you pay the teacher model to produce the training dataset. This is a one-time cost per distillation cycle. If you need 100,000 training examples and each example costs $0.04 in teacher inference, the generation cost is $4,000. If you need 500,000 examples for a more complex task, the generation cost is $20,000. Generation cost scales linearly with dataset size and teacher model pricing. The dataset size you need depends on the task complexity: simple classification might need 20,000 examples, structured generation might need 50,000 to 100,000, and complex multi-step reasoning might need 200,000 or more.

One nuance that teams miss: the generation cost includes not just the raw inference but also the filtering. Not every teacher output is suitable training data. You need quality filters that reject outputs where the teacher hallucinated, produced off-format responses, or generated content that does not meet your standards. Expect to discard 10% to 20% of teacher outputs, which means generating 110,000 to 120,000 examples to get 100,000 clean training pairs. The discarded outputs still cost money. Budget for the overage.

**Training cost** is what you pay to fine-tune the student model on the generated dataset. This depends on the student model's size, the dataset size, the number of training epochs, and the infrastructure you use. Fine-tuning a 7-billion-parameter model on 100,000 examples for three epochs on a managed platform might cost $3,000 to $8,000. Fine-tuning a 13-billion-parameter model on the same data might cost $8,000 to $20,000. If you are using your own GPU infrastructure, the cost is the hourly rate of the GPUs multiplied by the training time. A single A100 running LoRA fine-tuning on a 7B model over 100,000 examples typically completes in 6 to 12 hours, costing $18 to $42 in compute. The full fine-tune without LoRA costs more in both compute and time but can yield slightly better quality on some tasks.

Training cost also includes the engineering time to set up the training pipeline, select hyperparameters, run experiments, and evaluate results. For a first distillation, this might be two to three weeks of an ML engineer's time. For subsequent distillation cycles on the same task, the pipeline is reusable and the engineering time drops to two to five days.

**Evaluation cost** is what you pay to verify the student model meets your quality bar before deployment. You need a held-out evaluation set — typically 1,000 to 5,000 examples — that was not used in training. You run both the teacher and the student on this eval set and compare their outputs across your quality metrics. If the student falls short, you adjust training parameters and retrain. Each evaluation cycle costs the inference price of running both models on the eval set, plus the human review time if you use human evaluators for subjective quality dimensions. Budget $500 to $2,000 per evaluation cycle and expect two to four cycles before the student model is production-ready.

**Ongoing inference cost** is the payoff. This is what the student model costs to run in production, per request, indefinitely. If the teacher model costs $0.04 per request and the student costs $0.003 per request, the ongoing savings are $0.037 per request. At 80,000 requests per day, the student saves $2,960 per day compared to the teacher. Monthly savings: approximately $88,800. That number is the engine that makes the upfront investment worthwhile.

## The ROI Calculation: When Distillation Pays For Itself

The full ROI calculation combines all four cost components. Take the product description example. Generation cost: $4,000 for 100,000 examples. Training cost: $6,000 for compute and $10,000 for engineering time across initial setup and iteration. Evaluation cost: $3,000 across three evaluation cycles. Total upfront investment: $23,000.

Monthly teacher cost at 80,000 requests per day: $96,000. Monthly student cost at 80,000 requests per day: approximately $7,200 in inference plus $2,800 in hosting infrastructure, totaling $10,000. Monthly savings: $86,000. The $23,000 investment pays for itself in approximately eight days. After the first month, the cumulative savings are $63,000.

Over twelve months, the undistilled teacher approach costs $1,152,000. The distilled approach costs $23,000 upfront plus $120,000 in ongoing costs, totaling $143,000. Annual savings: $1,009,000. The ROI is staggering — over 40 times the initial investment returned in the first year.

These numbers are not hypothetical. They represent the actual economics that make distillation one of the highest-ROI infrastructure investments available to AI teams in 2026. The specific numbers vary by task, model, and volume, but the structure of the calculation is always the same: a modest upfront investment unlocks massive ongoing savings because inference cost dwarfs training cost at production volume.

For lower-volume workloads, the math still works but the payback period stretches. At 10,000 requests per day instead of 80,000, monthly savings drop from $86,000 to approximately $10,750. The $23,000 investment pays for itself in 2.1 months instead of eight days. Still an excellent return, but the urgency is lower. At 1,000 requests per day, monthly savings are approximately $1,075 and the payback is 21 months. At that volume, distillation may still make sense but only if the student model's hosting cost is low enough that it does not eat the per-request savings.

## The Quality Gap: 85% to 95% Is Usually Enough

Distilled models are not as good as the teacher. On any given quality metric — accuracy, fluency, format compliance, edge case handling — the student typically achieves 85% to 95% of the teacher's score. The exact gap depends on the task complexity, the dataset size, and the student model's capacity. Simpler tasks with clear patterns (classification, extraction, reformatting) achieve smaller gaps, often 93% to 98%. Complex tasks with nuanced judgment (summarization quality, tone matching, multi-step reasoning) show larger gaps, often 82% to 90%.

The question is not whether the gap exists. It always does. The question is whether the gap matters for your use case. And for most production use cases, it does not.

Consider the product description example. If the teacher model scores 92 out of 100 on a description quality eval and the student scores 87, the five-point gap means that roughly one in fifteen descriptions is noticeably worse than the teacher would have produced. But "noticeably worse" does not mean wrong. It means slightly less polished, slightly less engaging, slightly less precise in its word choices. For an e-commerce catalog generating 80,000 descriptions per day, the difference between 92-quality descriptions and 87-quality descriptions is invisible to the vast majority of shoppers. The savings of $86,000 per month vastly outweigh the marginal quality difference.

The gap matters when quality is binary: the output is either correct or it is not. Medical information, legal analysis, financial calculations — these are domains where a 5% quality gap means 5% of outputs are potentially wrong, and potentially wrong has real consequences. In these domains, distillation still works, but you need a tighter quality bar, a larger training dataset, and a cascading setup where the student handles straightforward cases and the teacher handles anything ambiguous. The economics still favor distillation because the majority of medical or legal queries are routine and well-handled by the student. The minority that require frontier-level precision get escalated.

## The Redistillation Cycle: Maintenance Is Not Optional

A distilled model is a snapshot of the teacher at the time of distillation. When the teacher improves — a new model version, better reasoning capabilities, updated training data — the student does not automatically inherit those improvements. The student continues performing at the level of the teacher that generated its training data. Over time, this creates drift. Users who interact with both the teacher (for complex requests) and the student (for routine requests) may notice the quality gap widening as the teacher gets better but the student stays frozen.

**Redistillation** is the process of regenerating training data from the updated teacher and retraining the student. The cost of redistillation is lower than the initial distillation because you already have the pipeline, the evaluation infrastructure, and the operational processes. You need to regenerate the training data (generation cost), retrain the model (training cost, typically lower because you are warm-starting from the previous student), and re-evaluate (evaluation cost). A typical redistillation cycle costs 40% to 60% of the initial distillation.

How often you redistill depends on how fast the teacher improves and how sensitive your users are to quality drift. For most teams, redistilling once or twice per year is sufficient. Major model releases from providers happen two to four times per year, but not every release materially changes the quality of your specific task's outputs. Monitor the teacher's performance on your eval set after each release. If the teacher's scores improve by more than three to five points, redistillation is worth the investment. If the improvement is marginal, skip the cycle and save the cost.

Budget $10,000 to $20,000 per redistillation cycle for a typical production deployment. Over a year with two redistillation cycles, that adds $20,000 to $40,000 to the total cost of the distilled approach. Even with this maintenance cost included, the economics overwhelmingly favor distillation at production volumes.

## Distillation vs Direct Fine-Tuning: When to Use Each

Distillation and direct fine-tuning are related but not identical. Direct fine-tuning trains the small model on real production data that you have collected and labeled. Distillation trains the small model on synthetic data generated by the teacher model. The choice between them depends on whether you have real data, whether the teacher can generate better data than what you have, and the cost of each data source.

If you already have 50,000 labeled examples from production — customer service conversations with quality ratings, classified support tickets with verified labels, documents with expert annotations — direct fine-tuning on that data is cheaper than distillation because you skip the generation cost entirely. You already have the data. You just train on it.

If you do not have labeled production data, distillation is cheaper than labeling because the teacher generates training data at API prices, which are almost always lower than human labeling prices. Generating 100,000 training examples from a teacher model at $0.04 per example costs $4,000. Labeling 100,000 examples with human annotators at $0.50 per example costs $50,000. The teacher is 12.5 times cheaper as a data source — and it works around the clock without quality drift from annotator fatigue.

There is a third scenario where distillation adds value on top of existing data: augmentation. You fine-tune on your real production data first, then use the teacher model to generate additional examples for edge cases, rare categories, or challenging scenarios that are underrepresented in your production data. This hybrid approach — real data for the common cases, teacher-generated data for the long tail — often produces the strongest student model at a moderate cost.

## Distillation at Scale: The Compounding Advantage

The economics of distillation compound across tasks. If you have ten different AI features in your product, each running on a frontier model, you do not need to distill all ten simultaneously. You start with the highest-volume task — the one where the per-request savings translate to the largest absolute dollar amount. You distill that task first, capture the savings, and use those savings to fund the distillation of the second-highest-volume task. Then the third. Each distillation cycle reduces your total inference cost, and the cumulative savings accelerate with each task migrated.

A mid-size AI product running ten features on Claude Opus 4.5 at a combined cost of $180,000 per month might distill three high-volume features in the first quarter, reducing monthly cost to $110,000. In the second quarter, they distill three more features, dropping to $65,000. By the end of the year, eight of ten features run on distilled models and total monthly cost is $40,000 — a 78% reduction from the starting point. The two remaining features — the ones with complex reasoning requirements or low volume — stay on the frontier model, where the cost is acceptable and the quality bar demands it.

This cascading investment strategy turns distillation from a one-time optimization into a systematic cost reduction program. Each distilled model frees up budget that funds the next distillation. The compound effect over twelve months can transform the economics of an entire AI product line.

## Legal and Contractual Considerations

Distillation introduces a legal dimension that pure prompting does not. When you use a provider's model as a teacher to generate training data for a student model, you are creating derivative work from the teacher's outputs. Model provider terms of service vary on whether this is permitted.

As of early 2026, OpenAI's usage policies permit using model outputs to train other models except for training models that compete with OpenAI's services. Anthropic's policies are similar in structure but differ in specific restrictions. Open-weight models like Llama 4 and Mistral generally permit distillation under their licenses, though some restrict commercial use of the distilled model above certain revenue thresholds. Google's Gemini API terms include specific language about using outputs for model training.

These terms change. Before beginning a distillation project, review the current terms of service for the teacher model you plan to use. If the terms prohibit or restrict distillation, you have three options: use a different teacher model with permissive terms, negotiate a custom agreement with the provider, or use only your own production data (which makes it direct fine-tuning, not distillation). Ignoring the terms and hoping nobody notices is not an option for any serious production deployment.

## The Distillation Decision Framework

Distillation makes economic sense when four conditions hold. First, you have a well-defined task where quality can be measured objectively. If you cannot measure the quality gap between teacher and student, you cannot verify that the student is good enough to deploy. Second, your request volume is high enough that the per-request savings exceed the amortized upfront cost within a reasonable timeframe, typically three to six months. Third, you can tolerate a 5% to 15% quality reduction on the majority of requests, either because the reduction is invisible to users or because you can cascade difficult requests to the teacher. Fourth, you have the engineering capacity to build and maintain the distillation pipeline, including redistillation cycles.

If all four conditions hold, distillation is likely the single highest-ROI investment your AI infrastructure team can make. The combination of modest upfront cost, dramatic per-request savings, and compounding benefits across multiple tasks makes it the closest thing to a guaranteed win in AI cost engineering.

The next subchapter shifts from model-level cost strategies to a different lever entirely: the economics of batch processing versus real-time inference, and when deferring a request by even a few minutes can cut your cost per query in half.
