# 2.6 â€” Reasoning Token Costs: The Hidden Price of Chain-of-Thought and Extended Thinking

In September 2025, a legal technology startup enabled extended thinking on their contract analysis pipeline. The change took one line of configuration. They expected better extraction accuracy on complex multi-party agreements, and they got it: accuracy on nested indemnification clauses rose from 81 percent to 93 percent. What they did not expect was the invoice. Monthly API spend jumped from $14,000 to $43,000, a tripling of cost with no visible change in output length. The responses were the same size. The prompts were the same size. The model was the same model. The team spent two weeks hunting for a billing error before they found the real cause: every single request was now generating hundreds to thousands of reasoning tokens that never appeared in the response but appeared on every line of the invoice. The model was thinking on their dime, and the thinking was expensive.

This is the cost trap of reasoning tokens, and it is catching teams across the industry as extended thinking and chain-of-thought models become the default for complex tasks. The tokens you can see in the response are only part of what you pay for. The tokens you cannot see, the model's internal reasoning, can dwarf the visible output by an order of magnitude.

## What Reasoning Tokens Are and Where They Come From

When you enable extended thinking on Claude, or use OpenAI's o-series reasoning models like o3 or o4-mini, or activate chain-of-thought on any model that supports it, you are asking the model to reason before it responds. The model generates an internal chain of thought: it breaks the problem down, considers alternatives, checks its own logic, and works through the answer step by step. Only after this internal reasoning does it produce the final response that you see.

These internal reasoning steps are not free. They are generated using the same autoregressive process as any other output token. The model runs a forward pass for each reasoning token, just as it does for each response token. The reasoning tokens consume GPU compute, memory, and time. And they are billed as output tokens, the most expensive category on your invoice.

The critical distinction is visibility. Response tokens appear in the API response. You can count them, log them, and see exactly what the model said. **Reasoning tokens** do not appear in the response, or appear only as a summary in some implementations. They exist in the model's internal processing, they are reported in the API usage metadata, and they show up on the bill. But because they are invisible in the response, teams routinely miss them when estimating costs.

Anthropic's extended thinking for Claude models generates thinking tokens that are billed at the standard output token rate for the model being used. Claude Opus 4.5 charges $25 per million output tokens, and every thinking token counts as an output token. Claude Sonnet 4.5 charges $15 per million output tokens for thinking. OpenAI's o3 and o4-mini models similarly bill reasoning tokens at output token rates. The pricing is not hidden. It is documented. But it is easy to overlook because the tokens themselves are hidden from the response.

## The Scale of the Hidden Cost

The ratio of reasoning tokens to visible response tokens is what makes this cost category dangerous. For simple tasks, the ratio is modest. A straightforward classification question might generate 50 visible response tokens and 100 reasoning tokens, a two to one ratio. The reasoning overhead doubles the output cost, which is significant but manageable.

For complex tasks, the ratio explodes. A multi-step legal analysis might generate 200 visible response tokens and 3,000 reasoning tokens, a fifteen to one ratio. A mathematical proof might generate 100 visible tokens and 5,000 reasoning tokens. A code debugging session might generate 300 visible tokens and 8,000 reasoning tokens. In these cases, the visible response is a tiny fraction of what you actually paid for. The invoice reflects the full cost of the model's internal deliberation, not just the polished answer it delivered.

This creates a dangerous mismatch between perceived cost and actual cost. A product manager looking at response lengths sees short, concise answers and assumes the cost per request is low. The engineering team monitoring output token counts in the API response sees modest numbers. But the billing dashboard shows output charges three, five, or ten times higher than the visible output would suggest. The gap is reasoning tokens.

Consider a concrete example. A financial services team uses Claude Opus 4.5 with extended thinking for risk assessment. Each request averages 1,000 input tokens, 300 visible output tokens, and 2,500 reasoning tokens. Without reasoning tokens, the cost per request would be $0.005 for input and $0.0075 for output, totaling $0.0125. With reasoning tokens, the output cost includes the 2,500 thinking tokens billed at the same rate, adding $0.0625. The true cost per request is $0.07, not $0.0125. That is 5.6 times the naive estimate. At 50,000 requests per day, the monthly bill is $105,000 instead of $18,750. The difference is $86,250 per month, all from tokens the team never sees in the response.

## The Thinking Tax

Name this pattern so your team remembers it: **the Thinking Tax**. It is the invisible surcharge you pay whenever you enable reasoning capabilities, and it has three properties that make it particularly treacherous.

First, the Thinking Tax is unpredictable. Unlike response length, which you can control with output constraints and max_tokens, reasoning length varies dramatically based on input complexity. A simple input might trigger 200 reasoning tokens. A complex input hitting the same endpoint might trigger 5,000. Your per-request cost variance explodes when reasoning is enabled, making budgeting harder and cost anomalies harder to detect.

Second, the Thinking Tax is invisible in the response. If your output costs doubled because your responses got longer, you would notice immediately. Log files would show longer responses. Monitoring dashboards would flag the increase. But when your output costs double because reasoning tokens doubled, the responses look exactly the same. The only place the increase is visible is in the API usage metadata and the billing dashboard. If you are not specifically monitoring reasoning token consumption, you will miss the increase until the invoice arrives.

Third, the Thinking Tax is sticky. Once your team enables extended thinking and sees the quality improvement, they are reluctant to disable it. The accuracy gains are real. The legal team that went from 81 to 93 percent accuracy on complex clauses does not want to go back to 81 percent. The Thinking Tax becomes a permanent cost increase that the organization absorbs without ever making an explicit decision about whether the quality gain justifies the price.

## Detecting Reasoning Token Consumption

Every major provider includes reasoning token counts in the API response metadata. The challenge is making sure your team actually captures and monitors this data.

In Anthropic's API, the response object includes a usage field that reports input tokens and output tokens. When extended thinking is enabled, the output token count includes both the visible response tokens and the thinking tokens. Some implementations provide a breakdown, reporting thinking tokens separately. Your logging infrastructure should capture the full usage object on every request, not just the response text.

In OpenAI's API, reasoning models return a usage object that includes completion tokens and a nested reasoning tokens field. The completion tokens number includes reasoning tokens. The separate reasoning tokens field tells you exactly how many tokens were spent on internal reasoning. If you are logging only the response content length as your cost proxy, you are missing the most expensive component.

The first step toward controlling the Thinking Tax is visibility. Add reasoning token counts to your per-request logging. Aggregate them by endpoint, by task type, and by input complexity band. Build a dashboard that shows reasoning tokens as a separate line item alongside input tokens and visible output tokens. When your team can see the three-way split, they can make informed decisions about where reasoning is justified and where it is waste.

The metric to watch is the **reasoning ratio**: reasoning tokens divided by visible output tokens. Track this ratio over time for each endpoint. A ratio of two to one means the model thinks twice as much as it writes. A ratio of ten to one means the model's internal deliberation is the dominant cost driver. When the ratio exceeds a threshold you define, say five to one, flag it for review. Either the task genuinely requires deep reasoning and the cost is justified, or the model is overthinking and you should reduce the reasoning budget or disable extended thinking for that endpoint.

## Controlling Reasoning Costs

Control starts with the question: does this task benefit from reasoning? Not every task does. Classification, simple extraction, routing, and formatting tasks rarely benefit from chain-of-thought reasoning. The model does not need to think deeply about whether a customer message is a complaint or a compliment. It does not need to reason through a multi-step chain to extract a date from a paragraph. For these tasks, enabling reasoning adds cost without adding quality.

The first control lever is selective enablement. Do not enable extended thinking globally across all your endpoints. Enable it only on the endpoints where reasoning demonstrably improves quality. Run an A/B test: process the same eval set with reasoning enabled and disabled. If the quality difference is statistically insignificant, disable reasoning for that endpoint. If the quality difference is significant, keep reasoning enabled and accept the cost as justified.

The second control lever is the reasoning budget. Both Anthropic and OpenAI allow you to set a ceiling on the number of reasoning tokens the model can use. Anthropic's extended thinking accepts a budget parameter with a minimum of 1,024 tokens. OpenAI's reasoning models accept a reasoning effort parameter with values ranging from minimal to high. Setting a lower reasoning budget constrains the model's thinking time, which reduces the Thinking Tax at the potential expense of reasoning quality.

The art is finding the budget sweet spot. Start low. Set the minimum budget, currently 1,024 tokens for Claude, and measure quality. Increase the budget incrementally, maybe doubling each time, and measure quality at each level. For most tasks, quality plateaus well before the maximum budget. A task that achieves 91 percent accuracy with 2,000 reasoning tokens and 92 percent accuracy with 8,000 reasoning tokens is not worth the four times cost increase for one percentage point. Find the knee of the curve, the point where additional reasoning tokens stop producing meaningful quality gains, and set your budget there.

The third control lever is model selection. Not every task needs a reasoning model. OpenAI's o3 is a reasoning model. GPT-5 is not. Claude Opus 4.5 with extended thinking is reasoning-capable. Claude Sonnet 4.5 without extended thinking is a fast, capable model that skips the internal reasoning step. For tasks that do not benefit from extended thinking, use the non-reasoning variant. The cost difference is the entire Thinking Tax, which can be a multiple of the base output cost.

The fourth control lever is task decomposition. Some complex tasks trigger massive reasoning token consumption because they require the model to hold multiple considerations in mind simultaneously. If you break the task into smaller subtasks, each subtask triggers less reasoning. Three focused calls with 500 reasoning tokens each cost less than one complex call with 5,000 reasoning tokens, because the relationship between task complexity and reasoning tokens is often superlinear, not linear.

## The Reasoning Cost Gradient

Different tasks fall on a gradient of reasoning intensity, and understanding where your tasks sit on this gradient is essential for cost planning.

At the low end are tasks that need no reasoning at all. Classification, sentiment analysis, simple extraction, language detection, formatting. These tasks are reflexive for modern models. They do not need to think. Enable reasoning and the model will think anyway, generating reasoning tokens for a conclusion it could have reached in a single forward pass. Every reasoning token on these tasks is pure waste.

In the middle are tasks that benefit from moderate reasoning. Multi-step extraction where fields depend on each other, summarization where the model must identify key themes across a long document, translation of ambiguous phrases where context matters. A reasoning budget of 1,000 to 3,000 tokens captures most of the quality gain for these tasks. The Thinking Tax is present but manageable.

At the high end are tasks that genuinely require deep reasoning. Complex legal analysis, mathematical problem solving, multi-constraint planning, code generation with intricate logic, medical differential diagnosis. These tasks produce reasoning chains of 5,000 to 20,000 tokens or more. The Thinking Tax is substantial, sometimes exceeding the combined cost of input and visible output. For these tasks, the Thinking Tax is the cost of doing business. The alternative is lower quality, which may be more expensive in terms of downstream error correction.

The mistake teams make is treating all their tasks as if they sit at the high end of the gradient. They enable extended thinking globally because some tasks need it, and they pay the Thinking Tax on every task, including the ones that derive no benefit. The discipline is to categorize your tasks on the gradient and apply reasoning selectively.

## Monitoring the Thinking Tax in Production

Production monitoring for reasoning tokens requires dedicated instrumentation. Standard API monitoring that tracks request count, latency, and error rate will not surface reasoning cost problems. You need token-level monitoring that breaks every request into four components: input tokens, cached input tokens, visible output tokens, and reasoning tokens.

Build alerts on reasoning token anomalies. A sudden spike in average reasoning tokens per request often indicates a change in input distribution. Maybe a new customer is sending longer, more complex documents. Maybe a product change is routing harder queries to an endpoint that was tuned for simple ones. Maybe a prompt change inadvertently encouraged the model to reason more deeply. Without reasoning-specific alerts, these cost increases go unnoticed for days or weeks.

Track the cost attribution of reasoning tokens as a percentage of total cost per endpoint. For some endpoints, reasoning tokens will be 10 percent of total cost. For others, they will be 70 percent. The endpoints where reasoning dominates are the ones to focus optimization efforts on. They are also the endpoints where model selection, reasoning budget tuning, and task decomposition yield the highest returns.

Build a reasoning cost dashboard that shows three things. First, total reasoning token spend per day, broken down by endpoint. Second, average reasoning ratio per endpoint, with a trend line. Third, the cost-per-quality-point for reasoning-enabled versus reasoning-disabled variants of each task. This third metric is the decision-making metric. If enabling reasoning costs $50,000 per month and improves accuracy from 88 to 93 percent, that is $10,000 per percentage point. Whether that is worth it depends on your business. But you need the number to make the decision.

## The Extended Thinking Decision Framework

When a team member proposes enabling extended thinking on an endpoint, the answer should never be automatic. Run this framework.

First, establish the baseline. What is the quality of the endpoint without reasoning, using the best prompt you can write? Measure it on your eval set. This is the number you need to beat.

Second, enable reasoning at the minimum budget. Measure quality again. Calculate the cost increase. If quality did not improve meaningfully, stop. Reasoning does not help this task, and the cost increase is pure waste.

Third, if quality improved, increase the reasoning budget incrementally. At each increment, measure quality and cost. Plot the curve. Find the knee, the point where quality gains flatten while costs continue to rise. Set the production budget at or near the knee.

Fourth, calculate the annual cost of the Thinking Tax at the chosen budget level. Present this number alongside the quality gain. Let the team and stakeholders make an explicit decision: is this quality improvement worth this annual cost? If yes, proceed. If no, disable reasoning and optimize the prompt instead.

Fifth, revisit the decision quarterly. Model updates change the baseline. A newer model might achieve the same quality without reasoning. A prompt improvement might close the gap. The Thinking Tax should be re-justified regularly, not accepted indefinitely.

## When the Thinking Tax Is Worth It

The Thinking Tax is worth paying when the cost of wrong answers exceeds the cost of reasoning tokens. In healthcare, a misdiagnosis costs lives. In legal, a missed clause costs lawsuits. In finance, a wrong risk assessment costs millions. For these domains, reasoning tokens are cheap insurance.

The Thinking Tax is also worth paying when it eliminates downstream human review. If enabling reasoning reduces your error rate enough to skip a manual review step, the cost of reasoning tokens is offset by the cost of the human reviewers you no longer need. A team that pays $30,000 per month in reasoning tokens but eliminates $45,000 per month in manual review labor has a net gain of $15,000 per month. This calculation is domain-specific and requires honest accounting of both the AI cost and the human cost it replaces.

The Thinking Tax is not worth paying when the quality gain is marginal, when the task does not require deep reasoning, when the error cost is low, or when the same quality can be achieved through prompt engineering, retrieval, or task decomposition. Most tasks fall into this category. Most tasks do not need extended thinking. The discipline is distinguishing the tasks that do from the tasks that do not, and paying the Thinking Tax only where it earns its keep.

## The Invoice Your Team Does Not Expect

The legal technology startup from the opening of this subchapter eventually got their costs under control. They categorized their contract analysis endpoints into three tiers. Simple extraction tasks, like party names and dates, ran without extended thinking. Moderate complexity tasks, like obligation identification, ran with a 2,000-token reasoning budget. High complexity tasks, like nested indemnification analysis, ran with an 8,000-token reasoning budget. The result was an accuracy profile nearly identical to their original all-reasoning configuration, at a monthly cost of $19,000 instead of $43,000. They saved $24,000 per month, or $288,000 per year, by being selective about where they paid the Thinking Tax.

The lesson is not that extended thinking is too expensive. The lesson is that extended thinking applied indiscriminately is too expensive. The model's reasoning capability is powerful and, for certain tasks, irreplaceable. But every reasoning token is an output token, and output tokens are the most expensive item on your invoice. Apply reasoning where it matters. Measure its value. Set budgets that match the task complexity. And monitor the Thinking Tax with the same rigor you apply to any other major line item in your operating budget.

The next subchapter covers the cost that grows with every conversational turn: multi-turn conversation costs and the compounding context problem that can make a ten-turn chat cost more than the first nine turns combined.
