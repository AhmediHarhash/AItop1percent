# 6.9 — Retry and Error Handling Costs: The Compounding Price of Unreliable Dependencies

**The Infinite Retry Loop** is the anti-pattern where your error handling dutifully retries every failed API call — two times, three times, five times — without any awareness of what those retries cost. It is the most common and most invisible cost amplifier in production AI systems. The pattern looks responsible on the surface. A model call fails with a 500 error. The system retries. The retry succeeds. The user gets their response. The engineering team sees a healthy success rate in their dashboard. Nobody notices that the failed-then-retried request cost three times as much as a clean request, because the retry logic was written by someone thinking about reliability, not economics. Multiply that invisible tax across millions of requests, add a few provider outages that spike error rates to 10% or 15%, and the Infinite Retry Loop quietly inflates your monthly bill by thousands of dollars that never appear in any cost attribution report.

The root problem is that traditional software engineering treats retries as free. When you retry a database query, the cost is a few milliseconds of compute on infrastructure you already pay for. When you retry an AI model API call, the cost is real money. Every retry to OpenAI, Anthropic, Google, or Cohere is a billed request. Every retry through an embedding API generates tokens that appear on your invoice. The economics of retries in AI systems are fundamentally different from the economics of retries in traditional systems, and teams that import their retry logic from pre-AI architectures import a cost structure they did not intend.

## The Math of Retry Costs

The baseline cost of retries is straightforward to calculate, and the numbers are large enough to demand attention. Start with your model API's error rate. In normal operation, major providers deliver error rates between 0.5% and 3%, depending on the model, the endpoint, and the time of day. During degraded performance periods — which happen to every provider multiple times per quarter — error rates can spike to 5%, 10%, or higher for minutes to hours.

Take a system processing 200,000 requests per day at an average cost of $0.02 per request. Under normal conditions with a 2% error rate and a retry policy that retries each failure twice, 4,000 requests fail on the first attempt. Each failure generates up to two retries. If the first retry succeeds 90% of the time, 3,600 requests cost $0.04 (original plus one retry) instead of $0.02. The remaining 400 requests need a second retry, costing $0.06 each. The daily retry overhead is 3,600 times $0.02 extra plus 400 times $0.04 extra, which comes to $88 per day in retry costs. That is $2,640 per month and $31,680 per year in costs that do not appear in any planning spreadsheet because the system was "working correctly."

Now consider what happens during a provider degradation event. The error rate spikes to 12% for four hours. During those four hours, the system processes roughly 33,000 requests. At a 12% error rate, 3,960 requests fail. Each generates up to two retries. The retry cost during that four-hour window alone is approximately $150 — a modest number in isolation. But if degradation events happen twice a month, and if some events last longer or hit harder, the annual cost of degradation-driven retries adds another $3,000 to $5,000 to the baseline retry overhead.

The total: $35,000 to $37,000 per year in retry costs for a system processing 200,000 requests per day. For a system processing a million requests per day, multiply by five. For a system using frontier models at $0.05 to $0.10 per request, multiply by three to five again. A high-volume frontier-model system can easily spend $100,000 or more per year on retries.

## Retry Amplification: The Compounding Effect

The math above assumes a simple architecture: one service calls one API, retries if it fails, and returns the result. Real AI systems are not simple. They are pipelines. A single user request might trigger an embedding call, a vector search, a reranking call, and a model inference call. Each of those steps has its own error handling. Each can retry independently. And that independence creates a compounding effect that can turn a single failure into a cascade of billable requests.

**Retry Amplification** is the name for this compounding pattern, and it is far more dangerous than single-service retries. Consider a three-step pipeline: embedding, retrieval, and generation. Each step has a retry policy of two retries on failure. If the embedding step fails and retries twice before succeeding, the system has made three embedding calls. It then proceeds to retrieval and generation, each of which execute once. Total API calls: five instead of three. The amplification factor is 1.67 for that request.

But the amplification gets worse when failures cluster. If the provider is having a bad day and all three services experience elevated error rates, a single request can trigger retries at each stage. In the worst case — all three steps fail twice before succeeding — the system makes three embedding calls, three retrieval calls, and three generation calls. Nine API calls instead of three. A 3-to-1 amplification factor. If the pipeline has four or five steps, the worst case grows to 12 or 15 API calls for a single user request.

The probability of worst-case amplification is low under normal conditions. But during provider outages, when error rates are elevated across multiple services simultaneously, the probability spikes. And this is precisely the moment when amplification is most expensive — because the failing requests are already costing more (from retries), and the system is processing the same user requests multiple times across multiple services. Teams that have not modeled their retry amplification exposure discover it on the invoice after a bad week.

## The Six Patterns That Inflate Retry Costs

Not all retry policies are equally expensive. Six specific patterns account for the majority of unnecessary retry spending in production AI systems.

The first is **retrying non-transient errors.** A 400 Bad Request error means the request itself is malformed. Retrying it will produce the same 400 error every time, burning money on requests that can never succeed. A 401 Unauthorized error means your API key is invalid or expired. Retrying will not fix the key. A 422 Unprocessable Entity usually means the input exceeds a length limit or contains invalid content. Only 500-series errors and 429 rate-limit errors are worth retrying. Teams that retry on all error codes waste money on every non-transient failure.

The second is **retrying without backoff.** If the provider is rate-limiting you (429) or experiencing capacity issues (503), retrying immediately adds load to an already-stressed system and increases the probability of continued failure. Exponential backoff — waiting 1 second, then 2, then 4, then 8 — gives the provider time to recover and dramatically increases the success rate of each retry attempt. Adding jitter — randomizing the backoff duration by plus or minus 20% to 30% — prevents the thundering herd problem where all your retry attempts hit the provider simultaneously. Teams that retry immediately spend more on retries and have lower retry success rates than teams that back off.

The third is **unlimited retry counts.** A retry policy that says "retry until success" is an open-ended financial commitment to a failing service. If the provider is down for thirty minutes, an unlimited retry policy will burn through hundreds or thousands of billed attempts per request that was in flight when the outage started. Cap your retries. Two or three retries is sufficient for transient errors. If a request has not succeeded after three attempts, the problem is not transient, and further retries waste money.

The fourth is **retrying the entire pipeline on a late-stage failure.** If the generation step fails after embedding and retrieval have succeeded, the system should retry only the generation step. Retrying the entire pipeline from the beginning re-executes the embedding and retrieval calls that already succeeded, doubling or tripling the cost of the retry. Implement stage-level checkpointing so each pipeline stage's result is cached and reusable if a later stage fails. This alone can cut retry costs by 40% to 60% in multi-stage pipelines.

The fifth is **not deduplicating retried requests.** If a request times out rather than returning an error, the original request may still be processing on the provider's side. Retrying creates a duplicate. Both the original and the retry generate output tokens and appear on your bill. Implement idempotency keys or request deduplication to avoid paying twice for the same successful request.

The sixth is **failing to account for retry costs in rate-limit budgets.** If your rate limit is 1,000 requests per minute and your retry policy generates 50 extra requests per minute, your effective capacity is 950 user-facing requests per minute, not 1,000. Under load, the retries themselves can push you into rate limiting, which generates more 429 errors, which generates more retries. This feedback loop — where retries cause rate limiting that causes more retries — is the mechanism behind the Infinite Retry Loop anti-pattern. Breaking the loop requires a retry budget that counts retries against the same rate limit as primary requests.

## Cost-Aware Retry Policies

The alternative to the Infinite Retry Loop is a retry policy that knows what retries cost and makes economically rational decisions about when to retry, how many times, and when to give up.

**Retry budgets** are the most powerful tool for controlling retry costs. A retry budget sets a maximum dollar amount that the system will spend on retries for any single request. If the original request costs $0.03, a retry budget of $0.06 allows up to two retries. If the original request costs $0.10 — a frontier model with a long prompt — the budget might allow only one retry, or it might require explicit escalation before the retry is authorized. The budget is expressed in currency, not in retry count, because a single retry on a frontier model costs more than three retries on a small model. Treating them the same with a flat retry count ignores the economics.

**Circuit breakers** prevent the system from wasting money on a service that is clearly down. The pattern is simple: track the error rate over a sliding window. If the error rate exceeds a threshold — say 20% over the past 60 seconds — the circuit breaker opens and all requests to that service fail immediately without attempting the call. No call means no cost. The circuit stays open for a cooldown period, then allows a small number of probe requests through. If the probes succeed, the circuit closes and normal traffic resumes. If the probes fail, the circuit stays open for another cooldown period. A well-tuned circuit breaker can prevent thousands of dollars in wasted retries during a multi-hour provider outage.

**Fallback routing** turns a cost into a savings opportunity. Instead of retrying a failed frontier model call against the same frontier model, route the retry to a cheaper fallback. If Claude Opus 4.6 returns a 500 error, retry against Claude Sonnet 4.5 or Gemini 3 Flash. The fallback model may produce lower quality output, but it produces output — which is better than a failed request — and it costs a fraction of the original call. For tasks where the quality difference between frontier and mid-tier is small, fallback routing converts retry costs into cost savings. The retry costs less than the original attempt would have.

**Graceful degradation** is the ultimate cost-aware response to persistent failures. Instead of retrying indefinitely or returning an error to the user, the system switches to a degraded mode that provides partial value without the failing service. If the embedding API is down, serve results from a keyword-only index rather than a hybrid vector-and-keyword index. If the generation model is down, return a cached or templated response for common queries. Degraded mode costs nothing in retry spend because there are no retries. It costs something in user experience, but far less than a complete failure — and far less in dollars than an hour of uncontrolled retries.

## Monitoring Retry Economics

You cannot manage retry costs without measuring them. Most observability stacks track retry counts and retry success rates. Almost none track retry costs. The gap means teams know they are retrying but have no idea what those retries cost.

Close the gap by adding three metrics to your monitoring. First, **retry cost per day**: the total dollar value of all retried API calls, calculated as the sum of token costs for every request that was a retry rather than a first attempt. This is your daily retry tax. Second, **retry cost as a percentage of total API spend**: if retries account for 8% of your total inference bill, that is a problem worth solving. If they account for 0.3%, the retry policy is working efficiently. Third, **retry cost per incident**: during a provider degradation event, what was the total cost of retries generated by that event? This metric, tracked over time, tells you whether your circuit breakers and retry budgets are working.

Build dashboards that show these metrics alongside error rates and latency. When the error rate spikes, you should see the retry cost spike in the same chart. If the spikes are proportional, your retry policy is behaving as designed. If the retry cost spike is dramatically larger than the error rate spike, you have retry amplification in your pipeline and need to investigate which stages are retrying independently.

Set alerts on retry cost thresholds, not just error rate thresholds. A 5% error rate that resolves in ten minutes costs relatively little in retries. A 3% error rate that persists for six hours costs far more. Error rate alerts catch the spike. Retry cost alerts catch the sustained bleed. You need both.

## The Retry Budget Calculation

For teams ready to implement cost-aware retries, the calculation starts with four numbers: your average request cost, your average error rate, your maximum acceptable retry overhead, and your pipeline depth.

Average request cost is the total monthly API spend divided by the total monthly request count. If you spend $15,000 on 750,000 requests, your average cost is $0.02 per request. Your average error rate comes from your monitoring — 1% to 3% is typical. Your maximum acceptable retry overhead is a policy decision: most teams land between 3% and 8% of total API spend. Your pipeline depth is the number of independently-retrying stages in your request path.

From these four numbers, you can calculate the maximum retry count per stage that keeps your total retry overhead within budget. If your total retry overhead target is 5% of $15,000 — $750 per month — and your error rate is 2% of 750,000 requests — 15,000 failing requests per month — your budget per failed request is $750 divided by 15,000, which is $0.05. At an average request cost of $0.02, that budget supports two to three retries per failure per stage while staying within the 5% overhead target. If your pipeline has three stages, the per-stage retry budget tightens to one retry per stage to account for amplification.

Write these budget constraints into your retry policies as explicit configuration, not as hardcoded constants buried in error-handling code. When your average request cost changes because you switch models, update the retry budget. When your error rate changes because a provider improves their reliability, adjust the retry count limits. Treat the retry policy as a financial instrument that needs periodic calibration, not as infrastructure that is set once and forgotten.

## What Unreliable Dependencies Actually Cost

Step back from the mechanics and consider the full picture. A dependency with a 2% error rate and a two-retry policy does not just cost you the retries. It costs you the engineering time to build and maintain the retry logic, the monitoring infrastructure to track retry health, the incident response time when retry amplification causes unexpected spending, and the opportunity cost of retry-driven latency that degrades user experience.

Teams that track the fully-loaded cost of unreliable dependencies — not just the retry spend but the operational overhead — often discover that investing in redundancy is cheaper than investing in retries. Maintaining two embedding providers and routing around failures is more cost-effective than retrying against a single provider, because the routing approach avoids retries entirely. The request goes to Provider B instead of retrying Provider A three times. Zero retry cost. Faster recovery. Better user experience.

The decision framework is simple. Calculate your annual retry cost for each dependency. If that cost exceeds the incremental cost of a redundant provider or a self-hosted fallback, invest in redundancy. If it does not, keep the retry policy and manage it with budgets and circuit breakers. Either way, know the number. The teams that get blindsided by retry costs are the ones that never measured them.

The next subchapter covers the opposite end of the efficiency spectrum: how batching external calls across multiple requests amortizes overhead costs and can cut your per-request API spend by 40% or more.