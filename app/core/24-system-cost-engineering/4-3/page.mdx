# 4.3 — Context Pruning: Removing Low-Value Context Before It Reaches the Model

Not everything you could send to the model should be sent to the model. This is the foundational principle of context pruning, and it sounds obvious until you look at how most systems actually work. The typical RAG pipeline retrieves ten chunks of 300 tokens each and stuffs all 3,000 tokens into the prompt because the retrieval system returned them and the engineer assumed more context is better. In reality, two or three of those chunks contain the information needed to answer the query. The other seven are noise — topically adjacent but not answer-bearing. Those seven chunks cost you 2,100 tokens per request. At 100,000 requests per day on GPT-5, that is 210 million wasted input tokens daily, costing $262 per day, $7,875 per month, $94,500 per year. You are paying nearly $95,000 a year for context that does not improve your answers. It may, in fact, degrade them.

The concept deserves a name. **Context ROI** is the quality improvement per dollar spent on context tokens. Every token of context you inject into a prompt has a cost and a quality contribution. Some tokens have high ROI — they contain the exact information the model needs to produce an accurate response. Other tokens have zero or negative ROI — they are irrelevant to the query, they duplicate information already present, or they confuse the model by introducing contradictory or tangential information. Context pruning is the practice of maximizing Context ROI by removing low-value tokens before they ever reach the model.

## The Retrieval Surplus Problem

Most retrieval systems are designed for recall, not precision. They are optimized to ensure that the relevant information is somewhere in the retrieved set, even if that means including a lot of irrelevant information alongside it. This is a reasonable design choice for quality — missing the relevant chunk is worse than including extra irrelevant chunks, from a quality perspective. But from a cost perspective, this design choice is expensive. Every irrelevant chunk that rides along with the relevant ones costs money without contributing to the answer.

The retrieval surplus is the gap between what the retrieval system returns and what the model actually needs. In a well-tuned system, the surplus might be 30% to 40% — the retrieval returns five chunks and the model only needs three. In a poorly tuned system, the surplus can exceed 70% — the retrieval returns ten chunks and the model only needs two. The surplus varies by query type. Specific, factual questions tend to have high surplus because the answer is contained in a single chunk but the retriever returns many similar chunks as candidates. Broad, analytical questions tend to have lower surplus because the answer genuinely requires synthesizing information from multiple sources.

A legal technology company discovered the magnitude of their retrieval surplus in early 2025 when they instrumented their contract analysis system to track which retrieved chunks were actually referenced in the model's output. They were retrieving eight chunks per query, averaging 350 tokens each, for a total of 2,800 context tokens. When they analyzed citation patterns across 50,000 production queries, they found that the model referenced information from an average of 2.4 chunks per response. The remaining 5.6 chunks were retrieved, paid for, and ignored. That surplus was costing the company $130,000 per year in unused context tokens on their Claude Sonnet 4.5 deployment. The retrieval system was doing its job — the relevant chunks were consistently in the retrieved set. But it was doing much more than its job, at the company's expense.

## Relevance Threshold Pruning

The simplest pruning technique is the **relevance threshold** — setting a minimum similarity score below which retrieved chunks are discarded before they enter the prompt. Every vector search returns results with similarity scores, and those scores drop as you move beyond the most relevant results. The top result might have a cosine similarity of 0.92. The second might be 0.87. By the eighth result, the similarity might be 0.61. Setting a threshold at 0.75 would keep the top four results and discard the bottom four.

The challenge is calibrating the threshold. Set it too high and you discard relevant chunks that would have improved the answer. Set it too low and you keep irrelevant chunks that add cost without adding value. The optimal threshold depends on your embedding model, your domain, your chunk design, and your quality requirements. There is no universal "right" threshold. You must empirically determine it by testing different thresholds against your eval suite and measuring both quality impact and token savings.

A practical approach: start with a lenient threshold that keeps most results, then incrementally tighten it while monitoring your quality metrics. Track two numbers at each threshold: the average token count of context per request, and your eval scores. As you tighten the threshold, context tokens drop and eventually quality drops. The optimal threshold is just above the point where quality begins to decline — you are removing the maximum amount of low-value context without touching the high-value context.

The legal technology company set their threshold at 0.72 based on this empirical process. That threshold reduced their average retrieved chunks from eight to 3.8 per query, cutting context tokens by 52%. Their eval suite showed a 1.1% drop in answer accuracy — within their acceptable tolerance. The token savings were $68,000 per year. Adjusting a single threshold parameter, validated by data, eliminated more than half of their context waste.

## Reranking and Top-K Selection

Reranking is a more sophisticated pruning technique. Instead of relying solely on the vector similarity score from the initial retrieval, you pass the retrieved chunks through a **cross-encoder reranker** — a model that scores each chunk's relevance to the query with much higher accuracy than the original embedding similarity. The reranker produces a new relevance ranking that often differs significantly from the original vector search ranking. Chunks that the vector search ranked fifth might be the most relevant according to the reranker, and chunks ranked second might drop to seventh.

After reranking, you select the top-K results, where K is the number of chunks you have determined is sufficient for your task. If your analysis shows that the model rarely needs more than three chunks to produce an accurate answer, you set K to three. The reranker ensures that the three chunks you keep are the three most relevant, not merely the three most similar in embedding space.

The cost of reranking is non-trivial but usually justified. Cross-encoder rerankers like the ones available through Cohere, Jina, or open-source models like BGE-reranker add 30 to 100 milliseconds of latency and $0.001 to $0.005 per query in compute cost. But the savings from keeping three chunks instead of ten dwarf the reranking cost. If reranking reduces context tokens from 3,000 to 900 per query — a savings of 2,100 tokens — the input token savings at 100,000 requests per day on GPT-5 are $262 per day. The reranking cost at $0.003 per query is $300 per day. In this specific example, the reranking cost slightly exceeds the raw token savings on a cheap model like GPT-5. But on Claude Sonnet 4.5 at $3.00 per million input tokens, the token savings jump to $630 per day while the reranking cost stays at $300, netting $330 per day in savings. And on Claude Opus 4.5 at $5.00 per million input tokens, the savings are $1,050 per day against the same $300 reranking cost, netting $750 per day.

The lesson: reranking's cost-effectiveness depends on your model pricing. For inexpensive models, the reranking cost may eat much of the token savings. For mid-tier and premium models, reranking almost always pays for itself through reduced context tokens. Run the numbers for your specific model and volume before deciding.

Research from early 2025 confirmed this pattern. Provence, a unified reranking and context pruning model published by researchers at Yandex and described at ICLR 2025, demonstrated that combining reranking with aggressive context pruning could reduce context window sizes by 80% while maintaining or improving answer quality. The key insight was that pruning without reranking often removes the wrong chunks, while reranking without pruning still passes unnecessary context to the model. The combination — rerank to identify the best chunks, then prune everything else — produces the best cost-quality trade-off.

## Query-Aware Context Filtering

Beyond relevance scores, you can prune context based on the nature of the query itself. **Query-aware filtering** uses the query's characteristics to determine how much and what kind of context the model needs before retrieval even begins.

Some queries need no retrieval at all. "What is our refund policy?" might be fully answered by the system prompt or a pre-loaded knowledge snippet. If your system can detect that the query matches a known FAQ, it can skip retrieval entirely, saving both the retrieval cost and the context token cost. A well-built FAQ detection layer that handles 15% to 20% of queries without retrieval eliminates context costs for those queries completely.

Some queries need minimal context. "What was our Q3 revenue?" requires a single data point from a single source. Retrieving five chunks and sending 1,500 tokens of context is wasteful when a single 200-token chunk contains the answer. Query-aware filtering can classify the query as a simple factual lookup and reduce K to one or two.

Other queries genuinely need extensive context. "Compare our product positioning in EMEA versus APAC over the last three quarters" requires information from multiple documents, multiple time periods, and multiple regions. For these queries, a higher K value and a more lenient relevance threshold is appropriate. The cost is justified because the complexity of the answer demands more context.

The implementation pattern is a lightweight classifier that examines each query and selects a retrieval configuration — how many chunks to retrieve, what relevance threshold to apply, whether to use reranking. This classifier can be a simple rule-based system for well-understood query patterns or a small model that categorizes query complexity. The classifier adds minimal cost — milliseconds of latency and fractions of a cent — but it ensures that simple queries get lean context and complex queries get rich context. You stop paying the complex-query cost for simple queries, which typically represent the majority of your traffic.

## Conversation History Pruning

Context pruning applies to more than just retrieved documents. In multi-turn conversations, the conversation history itself becomes a growing context cost. Every previous turn — the user's message and the model's response — gets included in the next request. A ten-turn conversation might accumulate 4,000 to 8,000 tokens of history. By the fifteenth turn, the history alone can exceed the model's effective attention window, degrading response quality while maximizing cost.

The naive approach is to include the entire history. The cost-optimized approach is selective inclusion. Not every previous turn is relevant to the current query. A user who spent five turns discussing Product A and is now asking about Product B does not need those Product A turns in context. A user who asked a clarifying question in turn three and received a correction in turn four does not need the original incorrect answer cluttering the context.

**Recency-based pruning** is the simplest strategy: keep the last N turns and drop the rest. This works well for conversations that stay on a single topic, but it fails when a user references something from early in the conversation. A sliding window of four to six turns is a common starting point, but the optimal window depends on your conversation patterns.

**Relevance-based pruning** is more sophisticated. Before constructing the prompt, you score each previous turn for relevance to the current query. Turns with high relevance are included. Turns with low relevance are dropped. This approach requires a lightweight relevance scorer — either a simple embedding similarity check or a small classifier — but it ensures that the model always has the context it needs without the context it does not.

**Summarization-based pruning** replaces older turns with a compressed summary. Instead of including turns one through eight verbatim, you summarize them into a two-to-three-sentence recap and include the summary plus the most recent four turns. This preserves the essential context from earlier in the conversation while reducing token count dramatically. We cover summarization as a cost control technique in detail in the next subchapter, but the principle applies directly to conversation history management.

The cost impact is significant. A customer support system handling an average of eight turns per conversation, with an average of 500 tokens per turn, includes 4,000 tokens of history in the final turn's request. If recency-based pruning with a four-turn window reduces that to 2,000 tokens, and 40% of conversations reach eight turns, the savings are substantial. At 100,000 requests per day with 40,000 reaching the eight-turn stage, the pruning saves 80 million tokens per day on those long conversations, or $100 per day at GPT-5 pricing. Annualized, that is $36,500 — from a pruning strategy that takes a day to implement.

## Deduplication: The Hidden Context Waste

A less obvious source of context waste is duplication. Retrieval systems can return chunks that contain the same information in slightly different forms — the same fact stated in a product overview and in a technical specification, the same policy quoted in a training document and in an FAQ. These duplicates double the token cost without doubling the information content. The model does not benefit from seeing the same fact twice. It benefits from seeing the fact once and seeing a different fact with the tokens you saved.

**Semantic deduplication** identifies chunks that are near-duplicates in meaning and keeps only the most relevant version. The simplest implementation computes pairwise cosine similarity between all retrieved chunks and removes any chunk whose similarity to a higher-ranked chunk exceeds a threshold — typically 0.85 to 0.92. This threshold should be set empirically for your domain. Technical documents tend to have more legitimate near-duplicates than conversational content, so technical domains may need a higher threshold to avoid false deduplication.

A document intelligence company found that 18% of their retrieved chunks were near-duplicates of other retrieved chunks, measured at a 0.88 similarity threshold. Deduplicating before sending context to the model removed an average of 540 tokens per request. At their volume of 200,000 requests per day on Claude Sonnet 4.5, that deduplication saved $97,200 per year. The implementation — a few dozen lines of embedding comparison logic running before prompt construction — paid for itself in the first week.

## The Compound Effect: Layering Pruning Techniques

Individual pruning techniques save money. Layered pruning techniques save much more. The most effective context cost optimization combines multiple techniques in sequence: retrieve broadly, rerank for relevance, apply a relevance threshold, deduplicate near-similar chunks, and select top-K. Each layer removes a different type of waste. Reranking removes misranked chunks. Thresholding removes marginally relevant chunks. Deduplication removes redundant chunks. Top-K selection enforces a hard cap on total context tokens.

The legal technology company that started with eight retrieved chunks at 2,800 tokens applied this layered approach. They added a cross-encoder reranker, set a relevance threshold at 0.72, implemented semantic deduplication at 0.88 similarity, and capped at three chunks. Their average context dropped from 2,800 tokens to 840 tokens — a 70% reduction. Their eval suite showed a 2.1% drop in answer completeness on complex multi-document queries and a 0.8% improvement on simple factual queries, where the removal of distracting irrelevant context actually helped the model focus. The net quality impact was negligible. The net cost impact was $180,000 per year in input token savings on their Claude deployment.

But note the quality risk. The 2.1% drop on complex queries was acceptable for this team, but it might not be for yours. Every pruning decision is a cost-quality trade-off. The eval suite is your safety net. Without it, aggressive pruning is a gamble. With it, aggressive pruning is a measured optimization with quantified risk.

## Building Context ROI Into Your Pipeline

Context ROI should not be an afterthought. It should be a design principle embedded in your retrieval pipeline from the start. When you build a RAG system, the default retrieval count — how many chunks to fetch and inject — should be determined by empirical testing against your eval suite, not by an engineer's guess. "Let's start with ten chunks" is a common default that becomes expensive fast. "Let's determine the minimum chunk count that meets our quality bar" is the right approach.

Instrument your pipeline to track context tokens per request as a first-class metric alongside latency, quality scores, and error rates. Monitor it daily. Alert when it drifts upward. A 10% increase in average context tokens per request might not show up in quality metrics or error rates, but it shows up in your monthly bill. If your retrieval system's behavior changes — a new embedding model, a re-indexed corpus, a change in chunk size — context token counts can shift without anyone noticing. The alert catches the shift before it becomes a cost surprise.

Set a context budget for each endpoint or feature. "The contract analysis feature has a context budget of 1,200 tokens per request." That budget forces the retrieval pipeline to be selective. If retrieval returns more tokens than the budget allows, the pipeline must prune, rerank, or summarize to fit within the budget. The budget is not arbitrary — it is based on the empirical relationship between context volume and answer quality for that specific task. You determine it once through testing, set it as a configuration, and enforce it in code.

Review your Context ROI metrics quarterly. As your corpus grows, your retrieval patterns shift, and your models improve, the optimal context strategy changes. A retrieval configuration that was cost-optimal six months ago may be over-retrieving today because your reranker improved, or under-retrieving because your corpus grew and queries now require more diverse source material. The quarterly review ensures your pruning strategy evolves with your system.

## The Pruning-Quality Contract

Every pruning decision carries risk. The risk is that you remove context the model needed, and the answer quality degrades. Managing this risk requires what you might call a **Pruning-Quality Contract** — an explicit, documented agreement between the engineering team and the product team about how much quality degradation is acceptable in exchange for cost savings.

The contract specifies three things. First, the quality metrics that must be maintained and their minimum acceptable thresholds. "Answer accuracy must stay above 91%. Format compliance must stay above 95%. Hallucination rate must stay below 3%." Second, the eval suite and process used to validate pruning changes. "All pruning configuration changes must be validated against the 600-case eval suite before deployment." Third, the rollback criteria. "If any quality metric drops below its threshold for two consecutive days in production, the previous context configuration is restored automatically."

Without this contract, pruning decisions become political. Product managers resist any quality reduction. Engineers resist any cost increase. The contract makes the trade-off explicit and negotiable. "We can save $94,000 per year by reducing from ten chunks to three, with a projected 2% drop in answer completeness on complex queries. Do we accept that trade-off?" That is a real business decision, made with real data, resulting in a deliberate outcome. It is the opposite of the default approach, which is to keep all ten chunks because nobody ever asked whether the cost was justified.

The next subchapter explores a powerful complement to pruning: using summarization to compress the context you do keep, reducing token counts further while preserving the essential information the model needs to generate accurate responses.
