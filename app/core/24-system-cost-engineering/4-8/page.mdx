# 4.8 — Structured Output Efficiency: How Schema Design Affects Token Count

Output tokens are the most expensive tokens in your system. Across every major provider in 2026, the ratio of output-to-input token pricing ranges from three to one on the low end to eight to one on the high end. GPT-5 charges $10.00 per million output tokens versus $1.25 per million input tokens — an eight-to-one ratio. Claude Opus 4.5 charges $25.00 per million output tokens versus $5.00 per million input — five to one. Claude Sonnet 4.5 charges $15.00 per million output versus $3.00 per million input — also five to one. When you ask a model to produce structured output, every unnecessary token in that output costs three to eight times what an unnecessary input token costs. This makes structured output design one of the most underexploited cost levers in production AI systems. The schema you define does not just shape the response — it determines how much you pay for it.

Most teams design their output schemas for developer convenience. Field names are descriptive. Structures are nested for readability. Every possible field is included, even fields that are null or empty for most responses. This is good software engineering practice for internal APIs. It is expensive practice for LLM output. Every field name, every nesting level, every null value, and every repeated structural token costs real money. And unlike input tokens, which you control by editing your prompt, output tokens are generated by the model — you control them only through the instructions and constraints you provide. The schema is your primary tool for controlling output cost.

## The Token Anatomy of Structured Output

When a model generates structured output, it produces every character of that output as tokens. That includes field names, delimiters, quotation marks, colons, commas, brackets, and whitespace for formatting. These structural tokens carry no information value — they are scaffolding, not content. But they cost exactly the same per token as the actual data values.

Consider a simple example. You ask a model to classify a customer support ticket and return the category, confidence score, and a short explanation. A verbose schema might produce output that includes a field called "ticket_classification_category" with a value of "billing_inquiry," a field called "confidence_score" with a value of "0.92," and a field called "classification_explanation" with a value like "The customer is asking about charges on their most recent invoice." That response might consume 45 to 55 tokens depending on the tokenizer.

Now consider the same information with a compact schema: a field called "cat" with a value of "billing," a field called "conf" with a value of "0.92," and a field called "reason" with a value of "Asking about recent invoice charges." The same information, 28 to 35 tokens. The verbose version costs 40% to 60% more in output tokens for identical information content. At a single request, the difference is fractions of a cent. At one million requests per month, the difference is real money.

Run those numbers at scale. If the verbose schema averages 50 output tokens and the compact schema averages 32 output tokens, the difference is 18 tokens per request. At one million requests per month, that is 18 million tokens. At GPT-5's output pricing of $10.00 per million tokens, the verbose schema costs $180 per month more. At Claude Opus 4.5's $25.00 per million output tokens, the difference is $450 per month. At five million requests per month on Claude Opus 4.5, you are spending $2,250 per month on field name verbosity and structural overhead alone. That is $27,000 per year on tokens that carry zero information.

## Field Name Length: The Simplest Win

The single fastest schema optimization is shortening field names. Every character in a field name is tokenized and paid for in the output. Long, descriptive field names are a luxury that scales poorly.

A field called "customer_sentiment_analysis_result" might consume four to five tokens just for the field name. A field called "sentiment" consumes one to two tokens. A field called "sent" consumes one token. Across a schema with ten fields, the difference between verbose and compact field names can be 15 to 30 tokens per response. That sounds trivial until you multiply it by request volume.

A document processing company ran this exact analysis in mid-2025. Their extraction pipeline returned structured output with 14 fields per response. Average field name length was 28 characters. Average output per response was 340 tokens. They renamed every field to the shortest unambiguous abbreviation: "document_classification_type" became "doc_type," "extracted_entity_name" became "ent_name," "confidence_score_percentage" became "conf." Average output dropped from 340 tokens to 285 tokens — a 16% reduction from field names alone. At 800,000 responses per month on GPT-5, that 55-token reduction saved $440 per month. When they combined field name shortening with other schema optimizations, total output token savings reached 48%, saving over $1,300 per month.

The objection is readability. Short field names make the raw output harder for developers to parse at a glance. This is a legitimate concern during debugging, but it has a simple solution: map the short names to descriptive names in your application layer. The model produces compact output. Your code maps "sent" to "sentiment_analysis_result" before passing it to the rest of the system. The mapping costs zero tokens because it happens in your application code, not in the model's generation.

## Flat Structures vs. Nested Structures

Nesting adds structural tokens. Every additional level of nesting introduces brackets, indentation, and repeated structural overhead. If your schema has a top-level field that contains a nested object with three fields, which in turn contains another nested object with two fields, the structural overhead for that nesting can be 10 to 20 tokens — tokens that carry no information, only hierarchy.

Flat structures eliminate this overhead. Instead of nesting an address inside a contact object inside a customer object, use field names that encode the hierarchy: "cust_street," "cust_city," "cust_zip." The information is identical. The token count is lower because you avoid the structural tokens for three levels of nesting.

The savings are proportional to the depth and frequency of nesting. A deeply nested schema with four levels of hierarchy might add 30 to 50 tokens of structural overhead per response. A flat schema with concatenated field names produces the same data in 30 to 50 fewer tokens. At scale, this is meaningful. A system producing two million structured responses per month saves 60 to 100 million tokens by flattening from four levels to one level. At $10.00 per million output tokens, that is $600 to $1,000 per month from structural simplification alone.

Not every schema can be flattened without losing clarity. When the nesting represents genuinely different entities — a list of items within an order, where each item has its own properties — flattening becomes awkward and error-prone. The principle is not "always flatten." The principle is "never nest for readability when flat works for your data model." Reserve nesting for structures that genuinely represent one-to-many or hierarchical relationships. Eliminate nesting that exists only because someone thought the output would look cleaner.

## Return Only What You Need

The most wasteful pattern in structured output is the **full-schema response** — asking the model to return every field in a schema even when most requests only need a subset. If your schema has 20 fields but the downstream consumer for a particular request type only uses five of them, the other 15 fields are pure waste. They consume output tokens, they cost money, and they are discarded the moment they arrive.

This pattern appears most often in systems that use a single schema for multiple purposes. The customer inquiry pipeline and the billing analysis pipeline share a schema because someone decided it was easier to maintain one schema than two. The customer inquiry pipeline needs five fields. The billing analysis pipeline needs eight. The shared schema includes all 20 fields from both pipelines plus extras that were added during development and never removed. Every response from both pipelines includes 20 fields. Both pipelines discard half the output.

The fix is purpose-specific schemas. Define the minimum schema for each use case. The customer inquiry pipeline gets a five-field schema. The billing analysis pipeline gets an eight-field schema. The model generates fewer tokens because the schema constrains the output. You pay only for what you use.

A SaaS analytics company applied this principle in February 2026 and documented the results. Their unified extraction schema had 22 fields. Their three primary use cases needed seven, nine, and twelve fields respectively. They split the schema into three purpose-specific versions. Average output tokens dropped from 410 per response to 230 per response — a 44% reduction. At 1.2 million responses per month on Claude Sonnet 4.5, the output token savings came to approximately $32,400 per year. The engineering effort was three days of schema refactoring and pipeline updates.

## Enums and Codes vs. Verbose String Values

When a field has a finite set of possible values, representing those values as short codes instead of verbose strings saves tokens on every response. A classification field that returns "high_priority_escalation_required" consumes five to six tokens. The same field returning "hp_esc" consumes one to two tokens. A sentiment field returning "strongly_negative" consumes two to three tokens. The same field returning "sn" consumes one token.

This applies to every categorical field in your schema: status codes, classification labels, priority levels, action types, confidence bands. If a field can take one of five to twenty predefined values, define those values as compact codes. Map them to human-readable labels in your application layer. The model generates the compact code. Your code expands it.

The savings compound quickly when you have multiple categorical fields. A schema with eight categorical fields that each save two to three tokens per response saves 16 to 24 tokens per response. At one million requests per month, that is 16 to 24 million saved output tokens. At $10.00 per million output tokens, you save $160 to $240 per month from enum compression alone — and that is on one of the cheaper output rates.

One caveat: extremely abbreviated codes can cause model confusion. If "sn" could mean "strongly_negative" or "status_new" or "serial_number," the model may produce inconsistent output. Keep codes unambiguous within the context of your schema. Two-letter codes that are unique within your field set work well. Single-letter codes risk collision. Test your compact codes with a sample of 500 to 1,000 requests and verify that the model produces the correct code reliably before deploying.

## Provider-Specific Structured Output Features

Major providers have shipped features that constrain structured output generation, reducing wasted tokens by preventing the model from generating anything outside your schema. These features are worth understanding because they affect both quality and cost.

OpenAI's Structured Outputs feature, available on GPT-5 and GPT-5-mini, allows you to define a schema and the model is guaranteed to produce output conforming to that schema. This eliminates the tokens wasted on malformed output — partial fields, extra explanatory text the model adds outside the schema, or repeated fields. When a model generates unstructured output, it sometimes adds conversational preamble like "Here is the result" or follow-up text like "Let me know if you need anything else." These wasted tokens disappear entirely with constrained structured output. The savings from eliminating preamble and conversational filler typically run 10 to 30 tokens per response, depending on the model and the prompt.

Anthropic's tool-use feature with Claude models achieves a similar constraint. When you define output through a tool schema, Claude generates only the fields specified in the schema, with no preamble or conversational wrapper. Google's Gemini models offer a response schema parameter that constrains output format. In each case, the structured output feature acts as a cost guardrail: the model cannot waste tokens on content outside the schema.

The cost benefit of these features is twofold. First, you eliminate the preamble and filler tokens — a fixed savings per response. Second, you eliminate retry costs from malformed output. Without structured output constraints, some percentage of responses will be malformed and require retries. Each retry costs a full inference call. If 3% of responses are malformed and require retries, eliminating those retries saves 3% of your total inference cost. On a $20,000 per month inference bill, that is $600 per month from retry elimination alone.

## Measuring Schema Efficiency

You cannot optimize what you do not measure. For structured output, the key metric is **information density** — the ratio of useful information tokens to total output tokens. If your model generates 200 output tokens and 120 of those tokens are actual data values while 80 are field names, structural delimiters, and null values, your information density is 60%. The other 40% is overhead.

Track information density by sampling output from production. Parse the structured output, separate data values from structural overhead, count the tokens in each category, and calculate the ratio. A well-optimized schema in 2026 achieves 65% to 75% information density. A poorly optimized schema runs at 40% to 55%. The gap between those two ranges represents 20% to 35% potential output token savings.

Track this metric monthly. Compare it across schema versions when you make changes. Set a team target — information density above 65% is a reasonable starting point. When a new schema is proposed, estimate its information density before deploying it. A schema review that catches a 50% information density design before it reaches production saves far more than optimizing the schema after a million responses have already shipped.

Another useful metric is **output tokens per task unit**. If your schema extracts invoice data, track output tokens per invoice. If it classifies support tickets, track output tokens per classification. These metrics let you spot inefficiency independent of request volume. If output tokens per classification creep up from 35 to 48 over two months, something changed — a field was added, values got more verbose, or the model started including extra content. The metric catches the drift before the bill does.

## The Readability Trade-off

There is a genuine tension between schema compactness and developer experience. A schema with field names like "s," "c," "r," and "p" is cheap to generate but nightmarish to debug. When something goes wrong in production and an engineer is reading raw model output at 2 AM, "classification_explanation" is a lot more helpful than "ce."

The resolution is layered readability. The model generates compact output. Your application layer immediately maps it to a human-readable internal format. Logging and debugging tools display the human-readable format. The compact format exists only in the narrow channel between the model and your parsing code. Engineers never see it unless they are debugging the parsing layer itself, which is rare.

Build the mapping as a single configuration file or dictionary that translates compact field names to descriptive names. This adds zero runtime cost — it is a dictionary lookup in your application code. It adds minimal maintenance cost — when you add a field to the schema, you add one entry to the mapping. And it preserves the full developer experience for debugging and monitoring while saving tokens on every single response.

The teams that resist compact schemas usually have not built this mapping layer. They see the trade-off as binary: readable output or cheap output. It is not binary. You get both with a thin translation layer. The engineering effort to build that layer is a few hours. The token savings persist for the life of the pipeline.

## A Worked Example: $9,000 Per Month in Schema Savings

A mid-market insurance technology company ran a claims processing pipeline that extracted structured data from claim descriptions. Their original schema had 18 fields with descriptive names, three levels of nesting, verbose string enums, and every field included in every response even though only 40% of fields had values for any given claim.

Their average output per response was 380 tokens. Monthly volume was 600,000 responses. They were running on Claude Sonnet 4.5 at $15.00 per million output tokens. Monthly output cost for this pipeline: $3,420.

They ran a schema optimization sprint. Step one: they shortened all field names, saving 35 tokens per response on average. Step two: they flattened from three nesting levels to one, saving 22 tokens. Step three: they replaced verbose string enums with two-character codes, saving 18 tokens. Step four: they split the schema into three purpose-specific versions so each use case returned only its needed fields, reducing average output by an additional 75 tokens. Step five: they enabled Claude's structured output tool-use constraint, eliminating 15 tokens of conversational preamble per response.

Total reduction: from 380 tokens to 215 tokens per response — a 43% decrease. New monthly output cost: $1,935. Monthly savings: $1,485, or roughly $17,820 per year. When they applied the same principles to their other two high-volume pipelines, total schema optimization savings reached $9,200 per month — $110,400 per year.

The entire optimization effort took one engineer two weeks. The annual return on that investment was roughly 35 to 1. Schema optimization is not glamorous work. It does not involve new models or new architectures. It is the engineering equivalent of negotiating a better rate on a recurring expense. But at scale, the savings are undeniable.

## When Not to Optimize

Not every schema is worth optimizing. If your pipeline processes 500 requests per day and generates 200 output tokens per response, your total monthly output is 3 million tokens — perhaps $30 to $45 per month on most models. Spending two weeks optimizing that schema to save 40% is a $12 to $18 per month return. The engineering time costs more than the savings.

The threshold for schema optimization ROI depends on your volume and your model's output pricing. As a rough guide, schema optimization pays off when your monthly output token cost for a single pipeline exceeds $500. Below that, the savings are too small to justify the effort. Above $2,000 per month, schema optimization should be a standard practice for every new pipeline. Above $10,000 per month, it should be a quarterly review item with dedicated engineering time.

The other scenario where you should not over-optimize is when output quality depends on verbose explanations. If your users receive the structured output directly — for example, an explanation field shown in the UI — compressing that explanation to save tokens degrades the user experience. Optimize structural overhead aggressively. Optimize user-facing content carefully. The goal is not minimum tokens. The goal is minimum tokens per unit of useful information.

The next subchapter introduces The Prompt Cost Audit — a repeatable, quarterly process that systematically finds and eliminates token waste across your entire prompt and context pipeline.
