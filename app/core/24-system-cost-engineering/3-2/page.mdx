# 3.2 — Model Tier Economics: When a Ten-Dollar Model Does the Job of a Hundred-Dollar Model

In late 2025, a Series B legal technology company ran their entire product — contract analysis, clause extraction, risk scoring, and summary generation — on Claude Opus 4.5. The product worked beautifully. Customers loved the quality. The engineering team was proud. The monthly inference bill was $280,000.

A new VP of Engineering joined in October and asked one question nobody had asked before: which of these tasks actually need Opus? The team had no answer. They had chosen Opus during prototyping because it produced the best results on their hardest examples, and they had never revisited the decision. The VP commissioned a two-week evaluation. The team ran their existing eval suite — 4,200 test cases across all four product features — on Claude Opus 4.5, Claude Sonnet 4.5, Claude Haiku 4.5, and GPT-5-mini. The results were uncomfortable. Clause extraction, which accounted for 35% of total requests, scored 96.2% accuracy on Opus, 95.8% on Sonnet, and 94.1% on Haiku. Risk scoring, another 25% of requests, scored 91.4% on Opus and 90.7% on Sonnet. Summary generation at 30% of requests scored 88.2% on Opus and 86.9% on Sonnet. Only contract analysis, the remaining 10% of requests, showed a meaningful quality drop below Opus — Sonnet scored 79.3% versus Opus's 89.1% on their complex multi-clause reasoning tests.

The team rerouted. Clause extraction went to Haiku. Risk scoring and summary generation went to Sonnet. Only contract analysis stayed on Opus. The monthly inference bill dropped from $280,000 to $74,000 — a 74% reduction. Customer satisfaction scores did not change. Not because customers did not notice quality differences, but because the quality differences on the rerouted tasks were within the noise range of normal variation. The company saved $2.5 million in annualized inference costs by asking a question they should have asked on day one.

## The Default-to-Frontier Trap

Most teams default to the most capable model available and treat that decision as permanent. This is economically irrational, but it happens for understandable reasons. During prototyping, the team wants the best possible output to prove the concept works. They reach for Opus, or GPT-5.2, or Gemini 3 Pro. The prototype succeeds. The team ships. The model choice, which was a prototyping convenience, hardens into a production architecture decision. Nobody revisits it because the product works and there are always more urgent priorities.

The problem is not that frontier models are bad choices. The problem is that they are expensive choices applied uniformly to tasks with wildly different complexity requirements. Sending a simple intent classification request — "is this customer asking about billing, shipping, or returns?" — to Claude Opus 4.6 at $25.00 per million output tokens is like hiring a neurosurgeon to apply a bandage. The neurosurgeon can do it. The neurosurgeon will do it perfectly. But you are paying neurosurgeon rates for bandage work, and the nurse would have achieved the same result at a twentieth of the cost.

The default-to-frontier trap is expensive because of how traffic distributes. In almost every AI product, most requests are simple. The hard cases — the ones that genuinely need frontier reasoning — are the minority. A customer support chatbot might handle thousands of straightforward FAQ-style questions for every one that requires nuanced judgment about a novel complaint. A document processing pipeline might classify and extract fields from hundreds of routine invoices for every one that contains unusual formatting requiring deep contextual understanding. The distribution is not uniform. It is heavily skewed toward simplicity.

## The Query Complexity Distribution

Industry experience across hundreds of production AI systems reveals a consistent pattern in query complexity distribution. The exact percentages vary by domain, but the shape is remarkably similar.

Roughly 60% to 70% of requests in most products are simple. They involve classification, short-form extraction, template-based generation, FAQ matching, or straightforward transformations. These tasks have narrow output requirements, well-defined correctness criteria, and limited ambiguity. Small models handle them well. Mid-tier models handle them excellently. Frontier models handle them perfectly but at ten to fifty times the cost, with no perceptible difference in output quality.

Another 20% to 30% of requests are moderate. They require real reasoning — multi-step analysis, nuanced summarization, context-dependent generation, or synthesis across multiple pieces of information. Mid-tier models handle these tasks at quality levels that meet or exceed production requirements. Frontier models produce marginally better output, but the margin is rarely large enough to justify the cost premium.

Only 5% to 15% of requests genuinely need frontier capability. These are the hard cases: complex reasoning chains that span long contexts, tasks requiring unusual creativity or nuance, edge cases where mid-tier models produce noticeably degraded output, or high-stakes decisions where the marginal quality improvement of a frontier model translates to meaningful business value. These cases justify the premium. The other 85% to 95% do not.

## Task-Model Alignment: Matching Complexity to Cost

**Task-model alignment** is the practice of systematically matching each query type to the cheapest model tier that handles it at or above your quality threshold. It is the single most impactful cost optimization in inference economics, and it requires exactly one thing: data. Specifically, it requires running your eval suite against multiple model tiers and measuring quality per task per tier.

The process is straightforward but requires discipline. First, define your quality threshold for each task type. What is the minimum acceptable accuracy, relevance, or quality score? This is not "as good as possible." It is "good enough for production." For a classification task, you might set the threshold at 93% accuracy. For a summarization task, you might set it at 80% on your rubric. For a complex reasoning task, you might set it at 85%. These thresholds are business decisions, not technical ones. They depend on what your users tolerate, what your product requires, and what your competitors deliver.

Second, run every task type through every candidate model tier. Use your existing eval suite — the same test cases you use for quality assurance — and score each model on each task. Record not just the aggregate score but the distribution. A model that averages 91% but drops below 80% on 12% of test cases has a different risk profile than one that averages 89% but never drops below 85%. The tail matters because the tail is what users complain about.

Third, for each task type, identify the cheapest model that meets the threshold. This is your assignment. Classification goes to Haiku. Summarization goes to Sonnet. Complex reasoning stays on Opus. Simple extraction goes to GPT-5-nano. The assignment is not permanent — you re-evaluate it when new models launch or prices change — but it is the operating configuration that governs your routing.

Fourth, calculate the cost savings. This is where the numbers become compelling and where you build the business case that convinces leadership to invest in routing infrastructure.

## The Cost Arithmetic of Tiered Assignment

The math is simple but the results are dramatic. Consider a product processing 200,000 requests per day with the following distribution: 65% simple tasks, 25% moderate tasks, and 10% complex tasks.

If all 200,000 requests go to Claude Opus 4.5 at an average cost of $0.035 per request — based on roughly 2,000 input tokens and 800 output tokens — the daily cost is $7,000, the monthly cost is $210,000, and the annual cost is $2,520,000.

Now apply tiered assignment. The 130,000 simple requests route to Claude Haiku 4.5 at an average cost of $0.006 per request, totaling $780 per day. The 50,000 moderate requests route to Claude Sonnet 4.5 at an average cost of $0.018 per request, totaling $900 per day. The 20,000 complex requests stay on Claude Opus 4.5 at $0.035 per request, totaling $700 per day. The total daily cost is $2,380. The monthly cost is $71,400. The annual cost is $856,800.

The savings are $1,663,200 per year. That is a 66% reduction. And this is a conservative estimate — it assumes Opus-level pricing for all complex tasks and does not account for further optimization through prompt efficiency or caching.

The blended cost per request drops from $0.035 to $0.012. That changes your unit economics, your pricing flexibility, and your competitive position. A product that was marginally profitable at $0.035 per request becomes comfortably profitable at $0.012. A product that could not afford to offer a free tier at $0.035 per request can afford one at $0.012. The cost savings do not just go to the bottom line. They create strategic options that did not exist before.

## Identifying Which Queries Need Which Tier

The evaluation process described above tells you which task types belong on which tier. But in production, you need to assign individual requests, not task types. A single API endpoint might receive both simple and complex queries. The challenge is classifying each request at runtime so the router can send it to the right model.

The most reliable method is structural classification. If your product has distinct features — a search function, a summarization function, an analysis function — each feature maps to a task type, and each task type maps to a model tier. The routing logic is straightforward: requests to the search endpoint go to Haiku, requests to the summarization endpoint go to Sonnet, requests to the analysis endpoint go to Opus. No runtime classification needed. The architecture encodes the routing.

When a single endpoint handles queries of varying complexity, you need a runtime signal. Input length is a crude but useful proxy. Short inputs — under 500 tokens — are almost always simple tasks. Very long inputs — over 10,000 tokens — often require deeper reasoning to handle the extended context. The presence of specific keywords or patterns can signal complexity. A query that asks "what is the return policy" is simpler than one that asks "compare the warranty terms across these three vendor contracts and identify conflicts with our standard procurement requirements."

Feature flags offer another approach. Let users or their account configuration determine the tier. Enterprise customers on a premium plan get Opus for every request. Standard customers get Sonnet for most requests with Opus reserved for flagged complex tasks. Free-tier users get Haiku. This approach aligns cost with revenue, ensuring that the customers paying the most receive the most expensive inference and the customers paying nothing receive the least expensive inference. The alignment is imperfect — some free-tier users have genuinely complex queries — but it is economically rational.

## The Quality Floor Concept

The critical concept in task-model alignment is the **quality floor** — the minimum quality score at which output is acceptable for production use. Everything above the floor is equivalent from the user's perspective. If your quality floor for classification is 93% accuracy, then a model scoring 93.2% and a model scoring 98.7% are both acceptable. The fact that the more expensive model scores 5.5 points higher does not matter if both models are above the floor. You do not pay a premium for quality the user does not need and, in many cases, cannot perceive.

Setting the quality floor requires product judgment, not just technical measurement. The floor for a medical triage system is different from the floor for a recipe suggestion engine. The floor for a financial report generator used by analysts is different from the floor for a social media caption tool. In high-stakes domains, the floor is high, and frontier models may be justified for a larger share of traffic. In low-stakes domains, the floor is lower, and small models cover more of the distribution.

The mistake teams make is setting the floor implicitly at "whatever the frontier model produces." That is not a quality floor. That is the ceiling. When you define the floor as "whatever Opus does," you have guaranteed that nothing cheaper will ever qualify, because nothing cheaper matches Opus exactly. The floor must be set based on user impact, business requirements, and competitive benchmarks — not based on the output of your most expensive model.

## When Cheaper Models Genuinely Fail

Task-model alignment is not the claim that cheap models work for everything. It is the claim that cheap models work for most things and that you should pay the premium only where it is justified. There are genuine failure modes in the small and mid-tier models that matter for specific tasks.

Complex multi-step reasoning degrades in smaller models. A legal analysis that requires tracing a chain of obligations through five cross-referenced clauses, checking each against a regulatory framework, and synthesizing a conclusion is a task where frontier models genuinely outperform mid-tier models by a meaningful margin. The failure is not that the smaller model produces garbage. The failure is that it misses one link in the chain, and the missing link changes the conclusion.

Long-context coherence is another genuine gap. Smaller models with shorter effective context windows — even when they technically accept long inputs — tend to lose track of information provided early in the prompt when generating output that depends on it. A summarization task over a 50,000-token document may produce adequate summaries from a mid-tier model for the first 30,000 tokens but lose fidelity on the material in the first 10,000. Frontier models with deeper attention mechanisms and larger effective contexts handle this better.

Nuanced tone and style matching also favors frontier models. If your product requires output that matches a specific brand voice, adjusts formality based on context, or maintains a consistent persona across varied inputs, frontier models produce noticeably better results. The gap is not about correctness. It is about quality in the subjective, hard-to-measure sense that users feel but struggle to articulate.

Recognizing these genuine gaps is essential. The goal of task-model alignment is not to eliminate frontier model usage. It is to restrict frontier model usage to the tasks that justify the cost and route everything else to the tier that meets the quality floor at the lowest price.

## Real-World Savings at Scale

The savings from task-model alignment scale linearly with traffic. A startup processing 10,000 requests per day might save $3,000 to $5,000 per month — meaningful for a small team but not transformative. A mid-size SaaS company processing 200,000 requests per day saves $100,000 to $150,000 per month — enough to fund an entire engineering team. An enterprise processing two million requests per day saves $1,000,000 or more per month — enough to change the business model.

The savings also compound over time. A team that implements tiered routing in Q1 does not just save money in Q1. They save money every month after that, at increasing absolute amounts as their traffic grows. A $100,000 monthly saving at 200,000 daily requests becomes a $200,000 monthly saving at 400,000 daily requests because the routing logic scales with the traffic. The upfront investment in evaluation and routing pays for itself many times over.

The financial case for task-model alignment is so strong that the real question is not whether to do it but when. The answer is: as soon as your monthly inference bill exceeds a level where the projected savings justify the engineering effort of building an eval suite and a routing layer. For most teams, that threshold is somewhere between $10,000 and $20,000 per month in inference spend. Below that, the absolute savings may not justify the investment. Above that, every month you delay is money you leave on the table.

The next subchapter covers the architecture and implementation of cost-driven model routing — the system that takes the tier assignments you have defined here and applies them to every request in real time.
