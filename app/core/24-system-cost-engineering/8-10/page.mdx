# 8.10 — Cost Shock Scenarios: Modeling Sudden Price Changes, Traffic Spikes, and Abuse Events

What happens to your margins if your model provider doubles their prices tomorrow? What happens if traffic triples overnight because a blog post goes viral? What happens if both happen in the same week? Most teams cannot answer these questions because they have never modeled the scenarios. They operate on the implicit assumption that next month's costs will look roughly like this month's costs, plus a growth factor. That assumption holds right up until it doesn't — and when it fails, it fails spectacularly. The team that was spending $28,000 per month on inference discovers they are spending $84,000 and has no playbook for responding. Decisions that should take hours take weeks because nobody planned for the contingency. Features get cut, quality degrades, contracts get renegotiated under pressure, and the entire cost structure shifts in ways the finance team did not model and the engineering team did not anticipate.

**Cost shock scenario modeling** is the practice of planning for sudden, unexpected cost increases before they happen. It is the financial equivalent of disaster recovery planning — you hope you never need it, but when you do, the difference between a team that planned and a team that didn't is the difference between a controlled response and a crisis. The teams that survive cost shocks are not the ones with the lowest costs. They are the ones with the fastest response times because they already know what to do.

## The Three Categories of Cost Shock

Cost shocks in AI systems fall into three distinct categories. Each has different triggers, different timelines, and different response playbooks. Understanding the categories is the first step toward modeling them.

**Provider price increases** are the first category. A model provider raises their per-token prices, and your cost per request increases immediately. This has happened multiple times in the AI API economy, though the more common direction since 2024 has been price decreases. The risk of price increases concentrates in two scenarios: niche or specialized model providers who lack the scale to absorb cost pressures, and the end of promotional pricing when a provider moves from adoption-focused to margin-focused pricing. Google's December 2025 changes to the Gemini API free tier, which cut free-tier limits by 50 to 92 percent, demonstrated that favorable pricing conditions can change abruptly. A provider that is subsidizing your usage today to capture market share will eventually need to generate margins from that usage. When they do, your costs jump. The magnitude varies — price increases of 30 to 200 percent have been observed across the API economy, depending on the provider and the tier.

**Traffic spikes** are the second category. Your request volume increases dramatically over a short period, and your total cost scales proportionally — or worse. Traffic spikes come from multiple sources: viral adoption when a media article or social post drives a surge of new users, marketing campaigns that launch successfully and drive above-projected signups, seasonal patterns in domains like retail or tax preparation, and organic growth that exceeds forecasts. In traditional SaaS, traffic spikes increase infrastructure cost modestly because marginal costs are low. In AI systems, traffic spikes increase cost linearly because every additional request incurs the full per-request model cost. A 3x traffic spike creates a 3x cost spike. A 10x spike creates a 10x cost spike. The costs arrive immediately — on the same day the traffic hits — while any revenue from the new users arrives over weeks or months as they convert and onboard.

**Abuse events** are the third category. Coordinated or organic abuse drives consumption far beyond normal patterns, as covered in the billing abuse subchapter. The distinction between an abuse event and a traffic spike is intent and cost profile. A traffic spike is legitimate users generating legitimate requests. An abuse event is one or more actors consuming disproportionate resources through scripted exploitation, free-tier farming, or output maximization. Abuse events are more dangerous than traffic spikes because the incremental usage generates zero revenue — you bear the full cost with no offsetting income. A traffic spike at least brings users who might become paying customers. An abuse event brings only a bill.

## The Modeling Exercise: Running the Numbers Before They Run You

For each cost shock category, the modeling exercise follows a consistent structure. You define the baseline, apply the shock, calculate the impact, and document the response. The entire exercise takes one to two days for a team that has its cost data organized. It should be run quarterly and updated whenever the cost structure changes materially — a model upgrade, a new provider, a major feature launch, or a significant change in traffic patterns.

Start with the baseline. Document your current monthly AI spend broken down by component: production inference, evaluation, retrieval, tool calls, agent costs, and infrastructure. Document your current traffic: average daily requests, peak daily requests, peak hourly requests. Document your per-request cost: average, median, 95th percentile, and 99th percentile. Document your revenue: monthly AI product revenue, gross margin on that revenue, and the cost as a percentage of revenue. This baseline is your reference point for every shock scenario.

**Scenario one: provider price increase.** Take your current monthly inference spend and multiply by the shock factor. Model three levels: a 50 percent increase, a 100 percent increase, and a 200 percent increase. For each level, calculate the new monthly spend, the new gross margin, and the burn rate. A team spending $80,000 per month on inference with $200,000 in monthly revenue has a 60 percent gross margin. At a 100 percent price increase, inference spend jumps to $160,000. With other costs unchanged at $40,000, total cost is $200,000. Gross margin drops to zero. At a 200 percent price increase, inference jumps to $240,000. Total cost is $280,000. The product loses $80,000 per month. How many months of runway does the company have at that burn rate? The answer to that question determines the urgency of the response.

**Scenario two: traffic spike.** Take your current daily request volume and multiply by the spike factor. Model three levels: a 3x spike lasting one week, a 5x spike lasting two days, and a 10x spike lasting four hours. For each level, calculate the total additional cost. A system processing 200,000 requests per day at $0.06 per request spends $12,000 per day. A 3x spike for one week means 600,000 requests per day for seven days: $36,000 per day, or $252,000 for the week — an additional $168,000 over baseline. A 10x spike for four hours means 2 million requests in four hours if the hourly rate is normally 8,333 requests — an additional $100,000 in four hours. Can your auto-scaling handle this volume without infrastructure failure? Can your budget absorb the cost? If the spike is from legitimate users, what is the expected lifetime value of those users, and does it justify the acquisition cost?

**Scenario three: abuse event.** Take the per-user cost of your most expensive abuse pattern — say, an output-maximization attacker consuming $500 per day. Model three levels: a single attacker for one week, ten coordinated attackers for three days, and a hundred bot accounts for 24 hours. The single attacker costs $3,500 over the week. Ten coordinated attackers cost $15,000 over three days. A hundred bot accounts cost $50,000 in 24 hours before detection and blocking. The hundred-bot scenario is the one that tests your detection and response speed. If your monitoring detects the anomaly within one hour instead of 24 hours, the damage drops from $50,000 to $2,083. Detection latency is the critical variable.

**Scenario four: compounding shocks.** The most dangerous scenarios are combinations. A provider price increase of 50 percent coincides with a 3x traffic spike from a successful product launch. The 50 percent price increase alone raises monthly spend from $360,000 to $540,000. The 3x traffic spike alone raises it to $1,080,000. Both together raise it to $1,620,000 — a 4.5x increase from baseline. Or an abuse event hits during a traffic spike, making it harder to distinguish abusive traffic from legitimate traffic and delaying the detection response. Model at least two compound scenarios to stress-test your response capacity.

## The Cost Shock Playbook

The output of scenario modeling is not a spreadsheet. It is a playbook — a documented set of responses for each scenario that the team can execute immediately when the shock occurs. The playbook eliminates the decision-making paralysis that turns a manageable cost shock into a crisis. When the invoice arrives or the alert fires, nobody needs to debate what to do. They open the playbook and execute.

**The provider price increase playbook** has three tiers. Tier one, for increases up to 30 percent: optimize aggressively. Switch to batch API pricing where possible, reduce evaluation sampling rates, implement more aggressive caching, and negotiate a volume commitment with the provider in exchange for a rate lock. These optimizations typically recover 20 to 40 percent of cost without changing the user experience. Tier two, for increases of 30 to 100 percent: switch models. Evaluate alternative providers and model tiers that deliver acceptable quality at lower cost. Claude Sonnet 4.5 instead of Claude Opus 4.6. GPT-5-mini instead of GPT-5. Gemini 3 Flash instead of Gemini 3 Pro. Run regression tests against your eval suite, confirm quality stays above your threshold, and migrate traffic within a week. This requires having alternative providers already benchmarked — not benchmarked for the first time during the crisis. Tier three, for increases above 100 percent: change the architecture. Move high-volume, low-complexity routes to self-hosted open-source models. Implement intelligent routing that sends only the queries requiring frontier capability to the expensive provider. Restructure pricing to pass some of the cost increase to customers. Tier three takes weeks to months to execute, so it must be combined with tier one and tier two measures to bridge the gap.

**The traffic spike playbook** also has three tiers. Tier one, for spikes up to 3x: auto-scale with cost caps. Let the system scale to meet demand, but set a hard daily cost cap that triggers graceful degradation — longer response times, simpler models, reduced features — when the cap is approached. The cap prevents the spike from becoming an unlimited cost event. Tier two, for spikes of 3x to 10x: degrade gracefully and queue. Route excess traffic to a queuing system that processes requests asynchronously instead of in real time. Users see "high demand — your response is being prepared" instead of instant results. The queue smooths the cost spike across a longer time window and allows priority routing for paid users. Tier three, for spikes above 10x: shed load explicitly. Serve a static maintenance page to unauthenticated users. Route only paying customers to the AI system. This is the nuclear option and damages user experience, but it prevents a cost event that could threaten the company's financial position. Define the threshold at which load shedding activates and ensure the infrastructure to implement it is in place before you need it.

**The abuse event playbook** is more operational than the other two. First response: detect and confirm. Automated monitoring flags the anomaly — a cost-per-user spike, a traffic pattern deviation, a sudden increase in output-to-input ratios. An on-call engineer confirms within 30 minutes that the pattern is abusive, not legitimate. Second response: contain. Block the specific users, IP ranges, or account patterns driving the abuse. Implement emergency rate limits on affected endpoints. This should happen within one hour of detection. Third response: quantify and recover. Calculate the total cost of the abuse event. Assess whether the terms of service support cost recovery. Document the attack pattern for future prevention. Fourth response: harden. Update detection rules to catch this specific pattern faster. Adjust rate limits or authentication requirements on the exploited endpoints. Run a postmortem to identify why the existing defenses did not prevent the event.

## Stress Testing: Simulating Shocks Before They Happen

The playbook is only useful if the team knows how to execute it under pressure. Quarterly cost shock drills test the team's ability to detect, respond to, and recover from each scenario. The drill works like a game-day exercise for incident response.

Select one scenario from the playbook. Inject a simulated alert — a dashboard showing a 3x traffic spike starting two hours ago, or a notification that the primary model provider has announced a 75 percent price increase effective in 30 days. The team has two hours to produce a response plan. The response plan must include the specific playbook tier being activated, the estimated cost impact, the actions to be taken, the person responsible for each action, and the timeline for implementation. After the drill, run a retrospective. Was the playbook clear enough to follow under pressure? Were the right people available? Were there dependencies — access credentials, alternative provider accounts, infrastructure switches — that were not in place? Each drill reveals gaps in the playbook that are cheaper to find in simulation than in crisis.

The most valuable drills are the compound scenarios. Simulate a provider price increase that coincides with a product launch. Simulate an abuse event during a holiday traffic spike. These compound scenarios test the team's ability to triage multiple competing priorities and allocate limited engineering time across simultaneous response tracks. Teams that drill these scenarios discover coordination failures — the same engineer is the designated responder for both the pricing playbook and the traffic playbook, creating a bottleneck that would paralyze response in a real event.

## Building Financial Resilience Into the Architecture

Beyond the playbook, the architecture itself can be designed to absorb cost shocks. Three architectural patterns provide structural resilience.

**Multi-provider deployment** means your system can route traffic to at least two model providers at all times. If your primary provider raises prices, you shift traffic to the secondary within hours, not weeks. This requires maintaining evaluation benchmarks for both providers, keeping API integrations current for both, and running a small percentage of traffic through the secondary provider in production to ensure it performs as expected. The cost of maintaining a secondary provider — maybe 5 to 10 percent of traffic routed to the backup — is the insurance premium that buys you pricing leverage and response speed.

**Model tier flexibility** means your system can dynamically shift between model tiers based on cost conditions. In normal operation, complex queries go to a frontier model and simple queries go to a mid-tier model. During a cost shock, the threshold shifts — more queries route to the cheaper tier, with only the most complex or high-stakes queries using the frontier model. This flexibility requires a routing layer that can adjust thresholds without code changes and a quality monitoring system that detects any degradation from the shift in real time.

**Cost-aware autoscaling** means your scaling decisions incorporate cost, not just latency and throughput. Traditional autoscaling adds capacity when latency increases or throughput drops. Cost-aware autoscaling adds a constraint: total cost per hour must stay below a defined ceiling. If scaling to meet demand would exceed the ceiling, the system degrades gracefully instead of scaling indefinitely. This prevents the scenario where auto-scaling faithfully meets a 10x traffic spike and generates a 10x cost spike that nobody authorized.

## The Cost Shock Playbook as an Organizational Artifact

The playbook is not a technical document. It is an organizational artifact that belongs to the intersection of engineering, finance, and product leadership. Engineering owns the technical response mechanisms — model switching, routing changes, rate limits. Finance owns the impact modeling — margin calculations, runway projections, budget reallocations. Product owns the customer communication — explaining degraded service, managing expectations, prioritizing paid tiers during load shedding. All three functions must be represented in the playbook's creation and review.

Store the playbook alongside your incident response documentation. Review it quarterly during cost reviews. Update it whenever the cost structure changes materially. Treat it with the same seriousness as your disaster recovery plan, because for an AI-first company, a cost disaster can be just as existential as a data disaster.

The teams that survive cost shocks are not the ones who predicted the exact scenario. They are the ones who practiced the response muscle. They know who to call, what levers to pull, and how fast they can move — because they rehearsed it before the alert fired. Every hour of planning saves days of crisis. Every drill reveals a gap that would have cost thousands. The cost of preparedness is negligible compared to the cost of surprise, and in the AI economy, surprises arrive on every invoice.

The next chapter shifts from pathology to visibility — building the cost observability and unit economics infrastructure that makes every dollar in your system traceable, attributable, and defensible.
