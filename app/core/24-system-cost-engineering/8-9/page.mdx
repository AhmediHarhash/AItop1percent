# 8.9 — The Eval Cost Spiral: When Your Quality System Costs as Much as Your Product

Your evaluation system is supposed to ensure quality. It is not supposed to cost as much as the product it evaluates. Yet that is exactly what happens in organization after organization, team after team, when well-intentioned quality infrastructure grows unchecked. The team starts with a single LLM-as-judge call on every production response. Then they add a safety check. Then a factuality check. Then a tone check. Then a relevance check. Each one is a separate model call, each one seems individually justified, and nobody adds up the total until the monthly invoice arrives at twice the expected amount. The production system made one model call per request. The evaluation system made five. The evaluation infrastructure cost more than the product it was measuring.

This is **The Eval Cost Spiral** — the pathology where evaluation, monitoring, and quality assurance systems consume a disproportionate and often invisible share of the total AI budget. It is self-inflicted, it is incremental, and it is one of the most common cost surprises in production AI. Unlike billing abuse or agent loops, the Eval Cost Spiral is not caused by adversaries or bugs. It is caused by good engineering instincts — the desire to measure everything, verify everything, monitor everything — applied without economic discipline.

## The Mechanism: How One Eval Becomes Six

The spiral starts innocently. The team launches a production AI system — say, a customer support chatbot that generates responses using a mid-tier model like Claude Sonnet 4.5 or GPT-5-mini. The per-request cost for the production call is $0.02. The team wants to monitor quality, so they add an LLM-as-judge evaluation that runs a model on every production response to score its accuracy. If they use the same mid-tier model as the judge, the eval adds another $0.02 per request. The total cost per request is now $0.04. Cost has doubled, but the team considers this acceptable because they need quality visibility.

Then the team learns that accuracy alone is not sufficient. Users are complaining about tone — responses are technically correct but curt. The team adds a tone evaluation. Another $0.02. Total cost: $0.06. Then a safety incident occurs — the model generates a response that references a competitor's product in a way that could be seen as endorsement. The team adds a safety evaluation. Another $0.02. Total cost: $0.08. Then Product asks for a factuality check because the chatbot occasionally cites policies that have changed. Another $0.02. Total cost: $0.10. Then the compliance team requires a PII detection check to ensure the model is not echoing customer personal data back into responses. Another $0.02. Total cost: $0.12.

The production model call is still $0.02. The evaluation calls now total $0.10. The evaluation infrastructure is 5x the cost of the product itself. The total per-request cost has increased 6x. On a system handling 200,000 requests per day, the production model cost is $4,000 per day. The evaluation cost is $20,000 per day. The team is spending $600,000 per month on evaluation — for a product whose direct inference cost is $120,000 per month.

Each eval dimension was individually justified. No single addition seemed unreasonable. But the cumulative effect is a system where quality assurance consumes 83 percent of the total model spend. This is the Eval Cost Spiral at full maturity, and it happens because nobody tracks the total evaluation cost as a percentage of production cost. Each eval was added by a different team member at a different time in response to a different incident, and nobody drew the line from "we should check tone" to "our evaluation budget exceeds our production budget."

## The Compounding Layers

The spiral does not stop at per-request evaluation. Teams that care about quality — which is to say, all competent teams — build additional layers on top of the per-request checks.

**Meta-evaluation** is the first compounding layer. The team discovers that their LLM-as-judge evaluations are not always reliable. The accuracy judge gives a score of 0.9 to a response that a human reviewer grades as 0.6. So the team builds a meta-evaluation layer that evaluates the evaluations — running a second, more powerful model on a sample of judge outputs to check whether the judges are calibrated correctly. If they sample 5 percent of evaluation outputs and run Claude Opus 4.6 on each, the meta-evaluation adds another layer of cost. The sample is smaller, but the per-call cost is higher because meta-evaluation requires a frontier model to provide a credible check on the mid-tier judge.

**Human review** is the second compounding layer. Even with LLM-as-judge and meta-evaluation, the team needs human review to calibrate the entire system. They route 2 to 5 percent of production responses to human reviewers who score them on the same dimensions the automated judges use. The human review is not free — reviewers cost $25 to $45 per hour, and each review takes 3 to 8 minutes. At 2 percent sampling on 200,000 daily requests, that is 4,000 reviews per day. At 5 minutes per review and $35 per hour, human review costs $11,667 per day or $350,000 per month. Human review is often the most expensive layer in the quality system, and it is rarely included in the "AI cost" budget because it flows through a different cost center — usually labeling or quality assurance rather than infrastructure.

**Storage and infrastructure** is the third compounding layer. Every evaluation generates data: the original response, the judge's score, the judge's reasoning, metadata about the request, and timestamps. At five eval dimensions per request on 200,000 daily requests, the system generates one million evaluation records per day. Each record includes the full response text and the full judge reasoning, averaging 2,000 to 4,000 tokens of stored text. That is 2 to 4 billion tokens of text per day — roughly 8 to 16 gigabytes. Over a month, storage grows by 240 to 480 gigabytes. The storage cost itself is modest, but the infrastructure to query it, aggregate it into dashboards, compute trends, and serve real-time quality alerts is not. Teams routinely spend $5,000 to $15,000 per month on the infrastructure that powers their evaluation dashboards.

**Regression testing** is the fourth compounding layer. Before every model upgrade, prompt change, or system update, the team runs the full evaluation suite against a held-out test set. The test set might contain 5,000 to 20,000 examples. Each example runs through the production pipeline and then through all five eval dimensions. A regression test with 10,000 examples and five eval dimensions generates 60,000 model calls — 10,000 production calls plus 50,000 evaluation calls. At $0.02 per call, each regression run costs $1,200. If the team runs regression tests weekly and additionally before every deployment, they might run four to eight regression tests per month at a total cost of $4,800 to $9,600.

Add it all up. Production inference: $120,000 per month. Per-request evaluation: $600,000 per month. Meta-evaluation: $15,000 per month. Human review: $350,000 per month. Storage and infrastructure: $10,000 per month. Regression testing: $7,000 per month. Total quality system cost: $982,000 per month. Total production system cost: $120,000 per month. The quality system costs 8.2x the production system. The tail is wagging the dog.

## Detecting the Spiral in Your Own System

The Eval Cost Spiral is invisible until you look for it, because evaluation costs are typically scattered across multiple budget line items, multiple teams, and multiple vendors. The production model calls appear on your OpenAI or Anthropic invoice. The human review appears in your labeling vendor's bill or your internal team's payroll. The storage appears in your cloud infrastructure bill. The meta-evaluation might appear as a separate line item under a different project code. Nobody sees the aggregate unless they actively assemble it.

The detection method is straightforward but requires discipline. Calculate the **evaluation cost ratio**: total monthly spend on all evaluation activities divided by total monthly spend on production inference. Include per-request evaluation, meta-evaluation, human review, eval infrastructure, and regression testing. If the ratio is less than 0.5, your evaluation system costs less than half your production cost — a healthy range for most products. If the ratio is between 0.5 and 1.0, your evaluation is approaching the cost of production — worth investigating but not necessarily alarming for high-stakes applications like healthcare or finance. If the ratio exceeds 1.0, your evaluation system costs more than your production system. This is the Eval Cost Spiral, and it requires immediate action.

The second detection signal is **eval dimension growth**. Track how many distinct evaluation dimensions are running per request over time. Plot it on a graph. If the number has increased every quarter and nobody has ever retired an eval dimension, you are on the spiral. Quality dimensions accumulate like barnacles — each one is added in response to a specific incident, and none are ever removed because nobody wants to be the person who turned off the safety check that would have caught the next incident.

## The Fix: Economic Discipline for Quality Systems

Fixing the Eval Cost Spiral does not mean reducing quality. It means applying the same cost engineering discipline to your evaluation system that you apply to your production system. Four strategies, applied together, typically reduce evaluation costs by 70 to 90 percent without meaningfully degrading quality visibility.

**Sampling-based evaluation** is the single most impactful change. Instead of evaluating 100 percent of production responses, evaluate a statistically representative sample. For most quality dimensions, a 5 to 10 percent sample provides sufficient signal to detect trends, identify regressions, and maintain dashboards. The math is simple: moving from 100 percent to 10 percent evaluation on a system doing 200,000 requests per day eliminates 180,000 evaluation calls per day per dimension. Across five dimensions, that is 900,000 fewer model calls per day. At $0.02 per call, the savings are $18,000 per day or $540,000 per month. The quality signal degrades only slightly — a 10 percent sample of 200,000 requests is 20,000 evaluated responses per day, more than enough to compute reliable quality scores with tight confidence intervals.

The key is intelligent sampling, not random sampling. Use stratified sampling that ensures coverage across customer segments, query types, and response categories. Oversample edge cases — long conversations, complex queries, high-stakes domains — that are more likely to reveal quality issues. Undersample routine queries where quality is consistently high. The goal is a sample that catches problems with the same sensitivity as exhaustive evaluation but at a fraction of the cost.

**Cheaper eval models** are the second lever. Not every evaluation dimension requires a frontier model. Simple binary checks — "Does this response contain PII? Yes or no" — can be handled by a small, fast model like GPT-5-nano or Claude Haiku 4.5 at one-tenth the cost of a mid-tier model. Tone evaluation, which involves subjective scoring, may benefit from a mid-tier model. Only factuality and complex accuracy evaluation genuinely require a frontier-class judge. Tiering your eval models by dimension complexity can reduce per-eval costs by 60 to 80 percent. A team that was running Claude Sonnet 4.5 for all five eval dimensions at $0.02 per call can switch PII detection and safety checking to Haiku at $0.002 per call, keep tone and relevance on Sonnet, and use Opus only for factuality — reducing the blended eval cost per request from $0.10 to $0.03.

**Batched asynchronous evaluation** is the third lever. Per-request synchronous evaluation — where the eval runs inline before the response is returned to the user — is the most expensive pattern because it adds latency to every request and cannot be deferred or batched. Switch to asynchronous evaluation where responses are logged and evaluated in bulk every hour or every six hours. Batched evaluation allows you to use batch API pricing, which is typically 50 percent cheaper than real-time pricing. It also allows you to adjust sampling rates dynamically — running higher sampling during known risk periods like after a deployment and lower sampling during stable periods.

**Dimension prioritization and retirement** is the fourth lever. Not every eval dimension needs to run on every request, even within your sample. Prioritize dimensions based on their historical signal value. If your tone evaluation has flagged only 12 issues in the last 90 days, and all 12 were already caught by your accuracy evaluation, the tone dimension is not adding unique signal. Demote it to weekly spot-check status instead of continuous monitoring. If your PII detection has never flagged a true positive because your prompt engineering already prevents PII leakage, consider retiring it entirely and running it only during regression testing. The principle is that every active eval dimension must earn its place through demonstrated signal value. Dimensions that do not catch problems that other dimensions miss are consuming budget without adding insight.

## The Eval Budget as a First-Class Metric

The long-term fix for the Eval Cost Spiral is treating evaluation cost as a first-class metric that is tracked, budgeted, and reviewed alongside production cost. Set an explicit target for the evaluation cost ratio — perhaps 0.3, meaning evaluation should cost no more than 30 percent of production inference. Report the ratio in monthly cost reviews. When the ratio exceeds the target, investigate which dimensions, models, or sampling rates are driving the increase.

Assign ownership of the evaluation budget to a single person or team. Without clear ownership, evaluation costs grow through distributed decision-making — each team adds the eval dimension they need, and nobody is accountable for the total. A cost owner reviews every new eval dimension request through the lens of marginal value: What unique signal does this dimension provide? What would the sampling rate be? What model tier is required? What is the annual cost? Is the expected quality improvement worth that cost? This review process does not prevent necessary eval dimensions from being added. It prevents unnecessary ones from accumulating.

The most sophisticated teams build an **eval cost model** that estimates the total evaluation cost of any proposed quality system change before it is implemented. Adding a new eval dimension? The model calculates the annual cost at current traffic levels. Increasing the sampling rate for an existing dimension? The model shows the cost increase and the expected improvement in statistical confidence. Upgrading the judge model from Haiku to Sonnet? The model shows the per-call cost increase, the total annual impact, and requires a justification for why the cheaper model is insufficient. This proactive modeling prevents the spiral from starting because every cost increase is visible and justified before it is committed.

## When High Eval Spend Is Justified

Not every system with a high evaluation cost ratio has the Eval Cost Spiral. Some systems legitimately require evaluation spend that equals or exceeds production spend. Medical AI systems where a misdiagnosis has life-or-death consequences may justify 100 percent evaluation on every response. Financial AI systems that generate investment advice where errors create legal liability may justify multiple eval dimensions at high sampling rates. Safety-critical systems where a single harmful output generates regulatory consequences may justify frontier-model evaluation on every response.

The distinction between justified high eval spend and the Eval Cost Spiral is intentionality. Justified high eval spend is a deliberate decision documented in the cost model, approved by leadership, and reviewed quarterly. The Eval Cost Spiral is accidental accumulation where nobody decided to spend 5x production cost on evaluation — it just happened through incremental additions without aggregate visibility. If you can explain why your evaluation cost ratio is 3.0 and point to the analysis that justified it, you have a quality-first system with appropriate investment. If your evaluation cost ratio is 3.0 and nobody on the team knows it, you have the Eval Cost Spiral.

The eval system is a product within your product. It deserves the same cost engineering discipline as any revenue-generating feature. Measure it, budget it, optimize it, and question every dollar. The next subchapter addresses the most acute form of cost crisis — what happens when external shocks suddenly multiply your costs beyond anything your budget or architecture anticipated.
