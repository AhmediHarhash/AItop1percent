# Chapter 4 — Prompt and Context Optimization for Cost

The fastest way to cut your AI bill is to send fewer tokens. Not fewer requests, but fewer tokens per request. This chapter covers the complete set of prompt and context optimization techniques that reduce cost without reducing quality: prompt compression, few-shot example economics, context pruning, summarization as cost control, output length constraints, template optimization, and the repeatable prompt cost audit that finds token waste your team did not know existed. Every token you remove from a request is a token you never pay for again.

---

- **4.1** — Prompt Compression: Reducing System Prompt Tokens Without Losing Instruction Quality
- **4.2** — Few-Shot Example Economics: When Examples Cost More Than They Improve
- **4.3** — Context Pruning: Removing Low-Value Context Before It Reaches the Model
- **4.4** — Summarization as Cost Control: Compressing Conversation History and Retrieved Documents
- **4.5** — Output Length Control: Constraining Generation to What Users Actually Need
- **4.6** — Template Optimization: Measuring and Reducing Fixed Token Overhead
- **4.7** — Retrieval Precision as a Cost Lever: Fewer Chunks, Lower Bills
- **4.8** — Structured Output Efficiency: How Schema Design Affects Token Count
- **4.9** — The Prompt Cost Audit: A Repeatable Process for Finding Token Waste

---

*A well-optimized prompt is not shorter. It is exactly as long as it needs to be and not a single token longer.*
