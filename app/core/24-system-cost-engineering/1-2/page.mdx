# 1.2 — The Total Cost Per Request: A Framework for Every Dollar in Your System

In September 2025, a mid-stage health technology startup launched an AI-powered clinical documentation assistant. The product took audio recordings from patient visits, transcribed them, extracted relevant medical information, and generated structured clinical notes for electronic health records. The team priced the product at $1.20 per encounter, based on their estimated model inference cost of $0.28 per encounter. They ran a pilot with three hospital systems. The product worked well. Clinicians loved it. The hospitals signed annual contracts. The startup projected 70 percent gross margins at scale and raised a Series B on those projections.

Four months later, the finance team ran a true cost analysis. The actual cost per encounter was not $0.28. It was $1.47. The company was losing $0.27 on every single encounter. At 18,000 encounters per day across the three hospital systems, that was a loss of $4,860 per day, or roughly $146,000 per month. The Series B runway that was supposed to last eighteen months would last eleven. The board demanded an explanation.

The engineering team had calculated the inference cost correctly. The transcription model cost $0.07 per encounter. The extraction model cost $0.09. The generation model cost $0.12. Total inference: $0.28. What they had not counted was everything else. Embedding the transcript for retrieval against the patient's history cost $0.03. Searching the vector database cost $0.04. Running a medical accuracy check using an LLM judge cost $0.31 — more than the entire generation pipeline. The guardrail model that screened for hallucinated medications cost $0.08. Retry logic for failed extractions added an average of $0.11 per encounter. Monitoring and logging telemetry cost $0.06. Infrastructure for the transcription queue, the embedding pipeline, and the note delivery system cost $0.14 when amortized across requests. Compliance logging for HIPAA audit trails cost $0.05. Human review of flagged notes, which occurred on 8 percent of encounters, cost $0.37 when averaged across all encounters. The true cost was 5.25 times the team's original estimate, and the gap was entirely in costs they never measured.

## The Framework: Total Cost Per Request

**Total Cost Per Request** is the single number that captures every dollar your AI system spends to serve one user interaction. It is the central metric of cost engineering. If you know this number accurately, you can price your product, forecast your margins, identify your optimization targets, and explain your spend to any stakeholder in the company. If you do not know this number, you are operating blind.

The Total Cost Per Request is the sum of all cost components generated when a user sends a request and receives a response. For a complete AI system, that sum includes model inference cost, token usage cost, retrieval cost, tool execution cost, infrastructure cost, evaluation cost, monitoring cost, compliance cost, support cost, retry cost, and allocated overhead. Each of these components is real. Each of them appears on a bill somewhere. The question is whether you are tracking all of them or only the ones that are easy to measure.

Most teams track inference cost because it is the most visible. The API provider sends an invoice. The number is clear. But inference cost is typically 25 to 45 percent of total cost per request in a mature AI system. The other 55 to 75 percent hides in retrieval fees, tool invocations, evaluation runs, infrastructure allocation, retry overhead, and human review. Teams that optimize only inference cost are optimizing less than half their spend. They are leaving the majority of their bill unexamined.

## The Ten Cost Components

The Total Cost Per Request framework breaks every dollar into ten distinct components. Not every system has all ten. A simple prompt-response chatbot might have four. A fully agentic RAG system with human-in-the-loop review might have all ten. The framework is designed to be comprehensive so that you never discover a cost component you did not know existed.

**Model inference cost** is the fee you pay the model provider for each call. For API-based models, this is the per-token charge multiplied by the number of input and output tokens. For self-hosted models, this is the amortized GPU cost divided by the number of requests served. A single request to Claude Opus 4.6 with 3,000 input tokens and 1,500 output tokens costs approximately $0.0525 — $0.015 for input and $0.0375 for output. A single request to GPT-5-mini with the same token counts costs roughly $0.005. The choice of model is the single largest lever on this component, often by 10x or more.

**Token usage cost** is related to but distinct from inference cost. It captures the total token consumption across all model calls in the pipeline, not just the primary generation call. A RAG system might make an embedding call, a reranking call, and a generation call. An agent might make six to fifteen model calls to complete a task. The token usage cost is the aggregate across all calls. This distinction matters because teams often report inference cost for the generation step only and miss the embedding, reranking, classification, and evaluation calls that happen before and after.

**Retrieval cost** covers everything involved in finding and delivering context to the model. Embedding the query into a vector costs money — typically $0.02 to $0.13 per million tokens depending on the embedding model. Searching the vector database costs money — Pinecone charges per query based on pod type, Weaviate charges per search unit, and self-hosted solutions charge in GPU time. Reranking the results costs money — a reranking model call for twenty candidate documents adds $0.002 to $0.01 per request. For a system that processes 500,000 requests per month, retrieval costs alone can reach $5,000 to $15,000 per month depending on the architecture.

**Tool execution cost** covers external APIs and function calls that the model triggers. A customer support agent that looks up order status calls a database API. A research agent that checks current information calls a search API. A coding agent that runs tests calls a compute API. Each call has its own pricing. A web search API might charge $0.005 per search. A database lookup might cost $0.001 in compute. A code execution sandbox might charge $0.02 per run. In agent-heavy systems, tool execution costs can rival or exceed inference costs because a single task might trigger ten to twenty tool calls.

**Infrastructure cost** covers the compute, storage, networking, and orchestration that runs your AI system. This includes the servers that host your API gateway, the queues that buffer requests, the caches that store frequent responses, the load balancers that distribute traffic, and the GPUs that run self-hosted models. For API-first architectures, infrastructure cost is relatively small — maybe $0.005 to $0.02 per request when amortized. For self-hosted architectures, infrastructure cost is the dominant component and can reach $0.10 to $0.50 per request depending on GPU utilization and batch efficiency.

**Evaluation cost** covers the model calls and compute used to assess the quality of your system's outputs. If you run an LLM-as-judge evaluation on 5 percent of production traffic, using a premium model as the judge, the evaluation cost per sampled request can be $0.05 to $0.15. Averaged across all requests, that adds $0.0025 to $0.0075 per request. That sounds small, but at a million requests per month, it is $2,500 to $7,500 per month in evaluation cost alone. Teams that increase their evaluation sample rate during a quality incident can see this cost spike by 10x or 20x temporarily.

**Monitoring cost** covers the observability infrastructure: log storage, trace collection, dashboard hosting, alerting systems. AI systems generate far more telemetry per request than traditional web services because you need to capture prompts, completions, token counts, latency breakdowns, and quality scores. A well-instrumented AI system might generate 5 to 15 kilobytes of telemetry per request. At a million requests per month, that is 5 to 15 gigabytes of log data per month. The storage and query costs for that data range from $500 to $3,000 per month depending on your observability platform.

**Compliance cost** covers the audit trails, data handling, and regulatory requirements that apply to your system. HIPAA-covered systems need encrypted logging of all patient data interactions. EU AI Act compliance requires documentation of model behavior for high-risk applications. SOX compliance requires audit trails for financial decisions informed by AI. These requirements add storage costs, processing costs, and sometimes dedicated infrastructure costs. For regulated industries, compliance cost can add $0.02 to $0.08 per request.

**Support cost** covers the human effort required to operate the system. When an AI system generates a response that a user escalates, a support agent handles the escalation. When a model produces a hallucination that a customer reports, an engineer investigates. When a quality degradation is detected, the team triages and fixes it. These human costs, amortized across all requests, typically add $0.01 to $0.05 per request depending on escalation rate and team size.

**Retry cost** covers the additional model calls, tool calls, and compute generated when a request fails and is retried. A system with a 3 percent failure rate and a two-retry policy generates an extra 6 percent in model calls on average. But that average masks the worst case: during a model provider incident when failure rates spike to 20 or 30 percent, retry costs can temporarily double your total spend. Retry cost is the most volatile component in the framework because it is driven by failure rates that are themselves unpredictable.

## The Four Cost Categories

The ten components do not all behave the same way as your system scales. Some grow linearly with traffic. Some grow in steps. Some stay constant. Some spike unpredictably. Understanding this behavior is essential for forecasting and optimization.

**Variable costs** scale linearly with request volume. If you serve twice as many requests, you pay roughly twice as much. Model inference, token usage, retrieval queries, and tool execution are all variable costs. They are the largest category in most AI systems, typically representing 55 to 70 percent of total cost. Variable costs are the hardest to reduce at scale because every optimization is multiplied by every request. A $0.01 per request reduction on a system serving a million requests per month saves $10,000. On a system serving ten million requests, it saves $100,000. Variable cost optimization has the highest leverage in high-volume systems.

**Semi-variable costs** scale in steps rather than linearly. Infrastructure autoscaling is the clearest example: you add a server when traffic crosses a threshold, and that server serves all traffic until the next threshold. Evaluation sampling is another example — if you evaluate 5 percent of traffic, the evaluation cost scales with traffic, but the sample rate is a policy decision you can adjust. Batch processing is semi-variable: a daily batch job costs the same whether it processes 1,000 or 9,000 items, but jumps in cost when it crosses 10,000 and requires a larger compute instance. Semi-variable costs typically represent 10 to 20 percent of total cost.

**Fixed costs** do not change with request volume. GPU reservations for self-hosted models cost the same whether you serve 100 or 100,000 requests per day. The base subscription for a vector database costs the same regardless of query volume up to the included tier limit. The monthly fee for an observability platform stays constant until you exceed the ingestion cap. Fixed costs typically represent 10 to 20 percent of total cost in API-first architectures and 30 to 50 percent in self-hosted architectures. Fixed costs are your friend at scale — they become a smaller fraction of per-request cost as volume increases. They are your enemy at low volume — they represent a cost floor that you pay regardless of revenue.

**Risk-adjusted costs** are probabilistic. They do not occur on every request. They occur when something goes wrong. Retry storms, abuse spikes, incident response labor, model fallback to more expensive alternatives, emergency scaling — these are all risk-adjusted costs. You cannot predict when they will occur, but you can estimate their frequency and severity based on historical data. A team that experiences one retry storm per month averaging $2,000 in extra cost has a risk-adjusted cost of roughly $0.002 per request on a system serving a million requests per month. Risk-adjusted costs are dangerous precisely because they are invisible in steady state and devastating during incidents. They typically represent 5 to 15 percent of total cost when properly accounted for, and teams that ignore them underestimate their true cost per request by that same margin.

## How to Calculate Your Number

Calculating your Total Cost Per Request is an exercise in measurement, not mathematics. The formula is addition — you sum the ten components. The hard part is measuring each component accurately.

Start with inference cost because it is the easiest. Your API provider gives you per-call cost data. Sum the cost of all model calls in the pipeline for a single request. If your system makes three model calls per request — embedding, generation, and evaluation — sum all three. Do not report just the generation cost.

Move to retrieval cost. Your vector database provider gives you per-query cost data or you can calculate it from your compute allocation. Include embedding cost, search cost, and reranking cost. If you cache embeddings, include the cache hit rate in your calculation — the effective embedding cost is the per-embedding cost multiplied by the cache miss rate.

Estimate tool execution cost by tracking the number and type of tool calls per request. If your agent calls an average of 3.2 tools per request, and each tool call costs between $0.001 and $0.01, the expected tool execution cost is the weighted average across tool types.

Allocate infrastructure cost by dividing your monthly infrastructure bill by the number of requests served. This is a rough allocation, but it gives you a per-request number you can refine. As you get more sophisticated, allocate infrastructure cost by component: the percentage of compute used by inference, retrieval, evaluation, and orchestration.

Measure evaluation cost by tracking the number and cost of evaluation model calls per production request. If you evaluate 5 percent of traffic using Claude Opus 4.6, the evaluation cost per evaluated request might be $0.08, and the amortized cost across all requests is $0.004.

Estimate retry cost from your failure rate and retry policy. If 4 percent of requests fail and each retry costs the same as the original request, the retry overhead is 4 percent of your variable cost.

Estimate support and compliance cost from your operational metrics. Divide your monthly support team cost by the number of requests to get the per-request allocation. Do the same for compliance infrastructure.

Sum everything. That is your Total Cost Per Request. The first time you calculate it, the number will be higher than you expected. That is the point. You are now seeing the true cost of your system for the first time.

## Why This Number Matters More Than Anything Else

The Total Cost Per Request is the number that determines whether your product is viable. If you charge $1.00 per request and your total cost is $0.60, your gross margin is 40 percent. If your total cost is $1.10, you are losing money on every request and losing more money as you grow. No amount of scaling fixes negative unit economics. No amount of fundraising fixes a product that costs more to operate than it charges.

The number also determines your optimization strategy. If inference cost is 60 percent of total cost, switching to a cheaper model is your highest-leverage move. If retrieval cost is 30 percent, optimizing your embedding and search pipeline has more impact than model switching. If retry cost is 15 percent, fixing your reliability issues saves more than any model or pipeline optimization. You cannot optimize what you do not measure, and the Total Cost Per Request tells you where the money goes so you can direct your engineering effort at the largest buckets.

The number enables pricing decisions. When your product team asks whether you can offer a 20 percent discount to win an enterprise deal, you can answer with precision: "Our cost per request is $0.47. At the current price of $0.85, we have a 45 percent gross margin. A 20 percent discount brings the price to $0.68 and the margin to 31 percent. That is sustainable for a high-volume customer but risky for a low-volume one." Without the number, you are guessing, and guessing leads to contracts that are either overpriced — losing the deal — or underpriced — losing money.

The number also enables accountability. When the AI bill spikes by 40 percent in a month, you can decompose the spike by component. Was it inference cost? Then token consumption increased — check for new features, prompt changes, or traffic growth. Was it retrieval cost? Then the vector database is being queried more heavily — check for a crawl storm, a reindexing event, or a change in retrieval depth. Was it retry cost? Then the failure rate increased — check for a model provider incident, a prompt regression, or a downstream service degradation. The Total Cost Per Request framework turns a scary invoice into a diagnostic tool.

## The Baseline Measurement

Every team should know their Total Cost Per Request within the first month of production deployment. Not an estimate. Not a back-of-envelope calculation. A measured number based on actual production data. This means instrumenting every cost-generating operation in your pipeline, collecting cost data for at least two weeks to capture weekday and weekend patterns, and computing the weighted average across all request types.

The first measurement is always surprising. A B2B document analysis company measured their Total Cost Per Request at $0.83 when their engineering team had estimated $0.31. A consumer chatbot company measured theirs at $0.12 when they had estimated $0.04. An e-commerce recommendation company measured theirs at $0.006 when they had estimated $0.002. The ratio of actual to estimated is remarkably consistent: 2x to 3x. Teams underestimate their total cost by 2x to 3x because they account for inference but not for everything else. That is not a mistake of bad estimation. It is a mistake of incomplete measurement. The Total Cost Per Request framework exists to close that gap.

Once you have the baseline, you can set targets. Reducing total cost per request by 15 percent in a quarter is a concrete, measurable objective that your engineering team can plan around. It decomposes into specific initiatives: switch the evaluation model from Opus to Sonnet for a 4 percent reduction, reduce retry rate from 5 percent to 2 percent for a 3 percent reduction, cache frequent embeddings for a 2 percent reduction, negotiate a volume discount with the API provider for a 6 percent reduction. Each initiative has a projected impact, an implementation cost, and a timeline. That is cost engineering, not cost guessing.

## Common Mistakes in Cost Calculation

Three mistakes appear consistently across teams calculating their Total Cost Per Request for the first time.

The first mistake is counting only the primary model call. A team reports their cost as $0.04 per request because that is what the generation step costs. They forget the embedding call at $0.003, the reranking call at $0.005, the evaluation call at $0.02, and the guardrail call at $0.01. The true cost is $0.078 — nearly double the reported number. This mistake is universal. If you have only been tracking your generation model cost, your total cost is almost certainly 1.5x to 3x what you think it is.

The second mistake is ignoring retry cost in the baseline. Teams calculate cost per successful request but forget that 3 to 8 percent of requests require retries. Each retry generates additional model calls and tool calls. The retry cost, amortized across all requests, increases the baseline by the failure rate multiplied by the average retry cost. A 5 percent failure rate with an average retry overhead of 1.2x the original request cost adds 6 percent to your total cost per request. This is invisible in per-call API pricing because retries look like normal calls to the provider.

The third mistake is averaging across request types without understanding the variance. If your system handles both simple queries that cost $0.03 and complex agentic tasks that cost $0.90, the average might be $0.15. But if 10 percent of your customers drive 60 percent of the complex tasks, those customers cost 6x the average while casual users cost 0.5x the average. Pricing based on the average means you lose money on your heaviest users and overcharge your lightest users. The Total Cost Per Request should be calculated by request type and by customer segment, not just as a global average.

The next subchapter maps the Six Cost Surfaces — the six distinct areas of your system where money flows — and reveals why most teams are blind to the surfaces that account for over half their total spend.
