# 7.8 — Networking and Egress Costs: The Expense Nobody Budgets For

In late 2025, a mid-sized legal technology company migrated their document analysis pipeline from an API provider to self-hosted inference on a single cloud provider. Their GPU cost projections were detailed and accurate: two H100 GPUs at $2.80 per hour, reserved for one year, totaling roughly $4,030 per month. They had budgeted for storage, for monitoring, for the engineering time to manage the deployment. What they had not budgeted for — what did not appear in a single line of their cost model — was networking. Their first monthly bill included $4,200 in data transfer charges. The GPU costs matched projections exactly. The networking costs exceeded their entire cloud budget for ancillary services. When the infrastructure lead traced the charges, the culprits were mundane: model responses flowing to a front-end service in a different region, logging data exported to their observability platform, RAG pipeline embeddings shuttled between a vector database in one availability zone and the inference service in another, and daily model telemetry pushed to a dashboard hosted on a separate cloud provider. None of these transfers were individually expensive. Collectively, they nearly doubled the effective cost of the migration.

This story is not unusual. Networking and egress costs are the most consistently underbudgeted line item in AI infrastructure. Teams obsess over GPU pricing, negotiate reserved instances, optimize utilization rates — and then ignore the data transfer charges that can add 20% to 40% to the monthly bill. The reason is simple: GPU costs are visible and predictable. You provision an instance, you know the hourly rate, you can multiply by hours. Networking costs are invisible until the bill arrives. They depend on traffic patterns, service architecture, data volumes, and provider pricing tiers that most engineers never read.

## What Egress Actually Costs

**Data egress** is the charge cloud providers apply when data leaves their network. The word "leaves" is doing heavy lifting in that sentence, because the definition of what counts as leaving varies by provider, by service, and by destination.

The most expensive form of egress is data leaving a cloud provider entirely — flowing from your AWS instance to a user's browser, from your Google Cloud service to a partner's on-premise server, or from your Azure deployment to a different cloud provider. Major cloud providers charge $0.05 to $0.12 per gigabyte for this outbound internet egress, with tiered pricing that decreases slightly at very high volumes. The first terabyte per month might cost $0.09 per gigabyte. The next ten terabytes might cost $0.085. At petabyte scale, you might negotiate down to $0.05. But for most AI systems, the volume sits in the range where the full rate applies.

Inter-region transfer within the same cloud provider is cheaper but not free. Moving data between US East and US West on AWS costs approximately $0.02 per gigabyte. Moving data between US and Europe costs $0.02 to $0.09 depending on the specific regions. Cross-continent transfers to Asia-Pacific or South America cost more.

The one category that is generally free or nearly free is intra-region transfer — data moving between services within the same availability zone and region. This is why service co-location matters so much for AI cost engineering. If your inference service and your vector database sit in the same region and availability zone, the data transfer between them costs effectively nothing. Move the vector database to a different region and you start paying per gigabyte on every retrieval.

## Why AI Systems Generate More Egress Than You Expect

Traditional web applications have a well-understood egress pattern: a user makes a request, the server sends a response, the response is a few kilobytes of HTML or JSON. AI systems break this pattern in several ways that inflate networking costs.

First, model responses are token-heavy. A typical AI-generated response of 500 to 1,000 tokens translates to roughly 2 to 4 kilobytes of text. That is two to ten times larger than a typical API response from a non-AI service. At high request volumes, this difference compounds. A system serving 500,000 requests per day with an average response size of 3 kilobytes generates 1.5 gigabytes of response egress per day, or about 45 gigabytes per month. At $0.09 per gigabyte, that is only $4 per month — trivial. But that is just the response payload to the end user. The real cost is in everything else.

Second, RAG pipelines create internal data transfer that dwarfs the user-facing response. When a user query arrives, your system retrieves context from a vector database — typically 5 to 20 document chunks, each 500 to 2,000 tokens, totaling 5 to 40 kilobytes of retrieved text. That retrieved text flows from the vector database to the inference service. After generation, the response, the retrieved context, and metadata flow to a logging service. If your pipeline includes a reranker, the retrieved documents flow to the reranking service and back. A single user request might generate 50 to 200 kilobytes of internal data transfer when you account for all the services involved.

At 500,000 requests per day with 100 kilobytes of average internal transfer per request, your system moves 50 gigabytes of data internally per day, or 1.5 terabytes per month. If all those services are in the same availability zone, the transfer cost is near zero. If the vector database is in a different region from the inference service — a common situation when teams use a managed vector database in one region and GPU instances in another — that 1.5 terabytes costs $30 to $135 per month in inter-region transfer alone.

Third, logging and observability exports create persistent egress. Production AI systems log prompts, responses, latency metrics, token counts, quality scores, and error details for every request. A robust logging setup for an AI pipeline generates 5 to 20 kilobytes of log data per request. At 500,000 requests per day, that is 2.5 to 10 gigabytes of log data per day, or 75 to 300 gigabytes per month. If your observability platform runs on a different provider — Datadog, New Relic, Grafana Cloud, or a self-hosted instance outside your inference provider — all that log data is outbound internet egress at $0.05 to $0.12 per gigabyte. Three hundred gigabytes of monthly log egress costs $15 to $36 on the cheap end and $27 to $36 on the typical end. Not catastrophic, but also not the zero dollars that most teams budget for logging transfer.

Fourth, multi-model pipelines multiply internal transfer. If your system routes a request through a classifier model, then to a primary generation model, then to a quality-checking model, data flows between three model endpoints. Each hop transfers the prompt, context, and intermediate results. A three-model pipeline with 20 kilobytes of data per hop generates 60 kilobytes of internal transfer per request — three times what a single-model pipeline generates.

## The Real Scenario: Building Up the Bill

Let's walk through the full networking cost for a realistic production AI system. This is a mid-scale RAG application serving 200,000 requests per day on self-hosted infrastructure in a single cloud provider.

User-facing response egress: 200,000 requests at 3 kilobytes average response, totaling 600 megabytes per day, or 18 gigabytes per month. At $0.09 per gigabyte, this costs roughly $1.62 per month. Negligible.

RAG retrieval transfer: 200,000 requests, each retrieving 80 kilobytes of context from a vector database in the same region but a different availability zone. That is 16 gigabytes per day, or 480 gigabytes per month. Intra-region but cross-zone transfer on most providers costs $0.01 per gigabyte, totaling $4.80 per month. Still small.

Logging export to external observability: 200,000 requests at 10 kilobytes of log data per request, totaling 2 gigabytes per day, or 60 gigabytes per month. Exported to Datadog at $0.09 per gigabyte egress, that is $5.40 per month. Manageable.

Now add the complications that real systems have. The team runs a nightly batch job that reprocesses the day's requests through an updated model for quality comparison. That doubles the internal transfer. A compliance requirement sends a copy of all prompts and responses to a data warehouse in a different region for audit purposes — 200,000 requests at 15 kilobytes each, 3 gigabytes per day, 90 gigabytes per month at $0.02 per gigabyte inter-region, adding $1.80 per month. They store embedding vectors in a managed vector database from a third-party provider, requiring cross-provider egress for index updates — 50 gigabytes per month at $0.09, adding $4.50 per month. Model weights are pulled from a model registry in a different region during each deployment, with a 70-billion-parameter 4-bit model weighing 35 gigabytes per pull, deployed weekly across three instances — 420 gigabytes per month of inter-region model weight transfer at $0.02 per gigabyte, adding $8.40 per month.

The total for this moderate-scale system: roughly $30 to $50 per month. That is not the $4,200 from the opening story. So where do the large networking bills come from?

## When Networking Costs Become Material

The opening story's $4,200 monthly bill is real because their architecture hit three multipliers simultaneously.

The first multiplier is **multi-cloud or hybrid deployment**. When your inference runs on one cloud provider and your database runs on another, or your front-end is on Vercel while your inference is on AWS, every data transfer between providers is outbound internet egress at the highest rate. A system that transfers 500 gigabytes per month between two providers pays $45 to $60 per month in egress. Scale that to 5 terabytes — common for a high-volume pipeline with large payloads — and egress alone costs $450 to $600 per month.

The second multiplier is **streaming responses**. When you stream model output token by token to the client, the networking overhead per response increases because each streamed chunk carries its own TCP and HTTP framing overhead. A 2-kilobyte response sent as a single payload uses roughly 2 kilobytes of network transfer. The same response streamed in 50 server-sent events might consume 4 to 6 kilobytes of actual network transfer due to framing overhead. This doubles or triples the egress volume for user-facing responses. At 200,000 requests per day, the difference between streamed and non-streamed responses might be 30 to 60 additional gigabytes of egress per month. At scale — millions of daily requests — the streaming overhead becomes hundreds of gigabytes.

The third multiplier is **high-volume logging at high fidelity**. Teams that log the full prompt, full response, all retrieved context, and all intermediate steps for every request can generate 50 to 100 kilobytes of log data per request. At a million requests per day, that is 50 to 100 gigabytes of logs per day, or 1.5 to 3 terabytes per month. If that logging pipeline exports to an external observability provider, the egress charges hit $135 to $270 per month. If the logs also replicate to a compliance archive in another region, add another $30 to $60. If you run analytics queries against those logs from a different cloud service, the round-trip data transfer adds more.

The legal tech company from the opening hit all three: their front-end was on a different provider from their inference, they streamed responses, and they exported comprehensive logs to an external observability platform. Each multiplier on its own was manageable. Combined, they produced $4,200 per month in networking charges that nobody had modeled.

## Optimization Strategies That Actually Work

The single most effective networking cost optimization is **co-location**. Put every service that communicates with every other service in the same cloud provider, the same region, and ideally the same availability zone. Intra-zone transfer is free or nearly free on every major provider. When your inference service, vector database, logging pipeline, and front-end proxy all sit in the same zone, internal data transfer costs approach zero.

This sounds obvious, but real architectures drift toward distribution for legitimate reasons. The best vector database for your use case might be a managed service hosted on a different provider. Your observability stack might be a SaaS product. Your front-end might be on a platform optimized for web delivery. Each decision makes sense in isolation. Collectively, they create a network topology where data crisscrosses between providers and regions on every request. The cost optimization is to resist this drift — or at least to understand its cost before accepting it.

**Payload compression** reduces egress volume for text-heavy AI payloads. Enabling gzip or zstd compression on API responses and inter-service communication typically reduces payload size by 60% to 80% for text data. A 3-kilobyte response compresses to 600 bytes to 1.2 kilobytes. At 500,000 requests per day, that is the difference between 45 gigabytes per month and 9 to 18 gigabytes per month. Compression is free in compute terms on modern hardware and reduces both egress cost and network latency.

**Log sampling** reduces observability egress without eliminating visibility. Instead of exporting every request's full log to your external observability provider, sample at 10% to 20% and export full details only for errors, slow responses, and flagged quality issues. Sampling reduces log egress by 80% to 90% while preserving your ability to detect and diagnose problems. Store the full unsampled logs within your cloud provider's native logging service — where storage is cheap and no egress is involved — and only export the sample.

**Internal endpoints** for inter-service communication bypass the public internet entirely. Most cloud providers offer private connectivity between services — VPC endpoints on AWS, Private Service Connect on Google Cloud, Private Endpoints on Azure — that route traffic over the provider's internal network instead of the public internet. Internal endpoints are not just cheaper (often free). They are also faster and more secure. If your vector database offers a private endpoint option and you are connecting to it over the public internet, you are paying egress charges for no reason.

**Caching at the response level** reduces egress by serving repeated queries from a cache instead of re-generating them. If 15% of your queries are repeats or near-duplicates, a response cache eliminates 15% of both inference cost and response egress. The cache sits in the same zone as your inference service, so lookups incur no networking cost.

## Embedding Pipeline Egress: The Hidden Multiplier

One networking cost that catches RAG-heavy teams off guard is the embedding pipeline. Every document you ingest into your RAG system must be converted to embedding vectors before it enters the vector database. If you use an external embedding API — OpenAI's embedding endpoint, Cohere's embedding service, or Google's embedding models — every document chunk you embed leaves your infrastructure, hits the external API, and the embedding vector comes back. This is a round trip of egress and ingress for every chunk.

The cost compounds during bulk ingestion. If you are indexing 500,000 documents with an average of 10 chunks per document, that is 5 million embedding API calls. Each chunk averages 500 tokens of text — roughly 2 kilobytes — and the returned embedding vector is typically 6 to 12 kilobytes depending on the model's dimensionality. The round-trip data volume is roughly 8 to 14 kilobytes per chunk. Five million chunks at 10 kilobytes average round-trip generates 50 gigabytes of data transfer. If the embedding API is outside your cloud provider, that is $4.50 in egress at $0.09 per gigabyte — trivial in isolation. But if you re-embed your entire corpus monthly for index freshness, and you also embed every incoming user query at inference time, the embedding egress accumulates.

At inference time, each user query must be embedded before retrieval. If your system processes 300,000 queries per day and each query embedding round trip is 3 kilobytes, that is 900 megabytes per day or 27 gigabytes per month. Combined with the monthly re-indexing egress, you are looking at 80 to 100 gigabytes per month of embedding-related data transfer. At scale — millions of queries per day, weekly re-indexing — the embedding egress can reach 500 gigabytes per month or more.

The optimization is to self-host your embedding model. Embedding models are small — typically 300 million to 1.5 billion parameters — and can run on cheap CPU instances or share a GPU with other lightweight workloads. Running the embedding model in the same availability zone as your vector database eliminates all embedding-related egress. The hosting cost for an embedding model is typically $50 to $200 per month, which pays for itself within the first month if your embedding egress exceeds that amount. And self-hosted embedding removes the per-call API cost as well, which is often the larger saving.

## The Ingress Surprise: Free Does Not Mean Costless

Cloud providers generally do not charge for data ingress — data flowing into their network. This creates an illusion that uploading data, receiving API responses, and pulling model weights are free operations. Technically they are, from a networking charge perspective. But free ingress does not mean free processing. Large ingress volumes trigger storage costs, compute costs for processing the incoming data, and eventual egress costs when the processed results leave.

This matters for AI systems that pull model weights frequently. A 70-billion-parameter model at 4-bit quantization weighs roughly 35 gigabytes. If you pull fresh model weights during every deployment, and you deploy weekly across three instances in two regions, that is 35 gigabytes times 3 instances times 2 regions times 4 weeks, equaling 840 gigabytes per month of model weight transfer. The ingress is free. But if those pulls cross regions, the inter-region transfer is not free. And the storage for maintaining copies of model weights across regions adds $10 to $30 per month depending on the number of versions you retain.

Teams that manage model weight distribution through a central model registry in one region and pull on demand to every serving region are paying inter-region transfer fees that could be eliminated by maintaining regional model caches. Storing a 35 gigabyte model in three regions costs a few dollars per month in storage. Transferring it across regions every week costs more. The optimization is obvious once you see the data flow, but invisible if you only look at the GPU line of your bill.

## Modeling Networking Costs Before You Commit

The mistake that most teams make is not that they overspend on networking. It is that they never model networking costs at all. GPU costs go into the spreadsheet. Storage costs go into the spreadsheet. Networking costs are treated as rounding error until they are not.

Before committing to any self-hosted infrastructure architecture, build a data flow diagram that maps every service-to-service data transfer in your pipeline. For each data flow, estimate the payload size per request, the request volume per day, and the network path: same zone, same region different zone, different region, or different provider. Multiply to get monthly transfer volumes per path. Apply the provider's pricing tier for each path type. Sum the total. Add 30% as a buffer for logging, monitoring, retries, and operational traffic you did not model.

The calculation takes two hours. It prevents the surprise that costs two months of budget negotiation to fix. The legal tech company could have discovered their $4,200 monthly networking bill before they migrated, not after. They would have made the same decision to self-host — the GPU savings still justified the migration — but they would have co-located their services, used internal endpoints, and compressed their payloads, reducing the networking bill to roughly $400 per month instead of $4,200.

## The Monitoring Gap

Even after you model and optimize networking costs, you need to monitor them. Data transfer patterns change as your system evolves. A new feature that adds a reranking step to your RAG pipeline introduces a new inter-service data flow. A logging improvement that captures additional context per request increases export volume. A regional expansion that serves European users from US infrastructure creates cross-region transfer that did not exist before.

Cloud providers break down networking costs in their billing dashboards, but the breakdowns are often too coarse to identify which service or pipeline is responsible. The bill might show "inter-region data transfer: $380" without telling you that $340 of it comes from your nightly batch job replicating data to a compliance archive and only $40 comes from production traffic. Tag your data flows, track transfer volumes per service pair, and set billing alerts at the service level, not just the account level. A $50 per month networking line item that grows to $500 per month over six months is a sign that your architecture is drifting toward distribution — and that drift has a price.

Set up automated cost anomaly detection on your networking line items. Most cloud providers offer budget alerts, but these fire on absolute thresholds — "alert when networking exceeds $500." More useful is a relative alert: "alert when networking costs exceed 120% of the previous month." Relative alerts catch the gradual drift that absolute thresholds miss. A system that grows from $80 to $120 to $180 to $270 over four months never triggers a $500 absolute alert, but a 120% relative alert fires in the second month, giving you three months of lead time to investigate before the cost becomes material.

## The Networking Cost Audit

Run a networking cost audit quarterly, or whenever your architecture changes significantly. The audit has four steps.

First, enumerate every service-to-service data flow in your AI pipeline. Draw the data flow diagram if you do not have one. Include the inference service, the vector database, the reranker, the logging pipeline, the observability exporter, the front-end gateway, the model registry, the embedding service, and any batch processing jobs. For each flow, note the source service, destination service, and the network path between them.

Second, measure actual transfer volumes for each flow over a 30-day period. Use your cloud provider's network flow logs or VPC flow logs to capture actual bytes transferred per service pair. Do not rely on estimates — measure. Teams that estimate networking volumes are consistently off by 30% to 50%, usually in the direction of underestimating.

Third, calculate the cost of each flow using the provider's actual pricing for the network path. Intra-zone flows are free. Cross-zone flows cost $0.01 per gigabyte. Cross-region flows cost $0.02 to $0.09 per gigabyte. Cross-provider flows cost $0.05 to $0.12 per gigabyte. Multiply each flow's monthly volume by its per-gigabyte rate. Rank the flows by monthly cost.

Fourth, identify the top three to five flows by cost and evaluate each one for optimization. Can the services be co-located? Can the payload be compressed? Can the transfer be batched instead of per-request? Can the destination service be replaced with an in-region alternative? Often, one or two flows account for 60% to 80% of the total networking cost, and fixing those flows reduces the bill dramatically while leaving the rest of the architecture unchanged.

An insurance technology company ran this audit in early 2026 and found that 72% of their networking cost came from a single flow: their nightly quality evaluation batch job, which pulled the day's 400,000 production requests from a logging database in one region, sent them to a model endpoint in another region for re-evaluation, and pushed the results to a third region for dashboard visualization. The round-trip data transfer was 3.2 terabytes per month. Moving the batch job to co-locate with the logging database and streaming results to the dashboard eliminated two of the three cross-region transfers, reducing networking cost from $340 per month to $45 per month. The fix took one engineer two days.

## The Provider Landscape Is Shifting

Networking costs are one of the most actively competed-on dimensions in the GPU cloud market. Traditional hyperscalers — AWS, Google Cloud, Azure — have maintained egress pricing that has been a persistent frustration for customers for years. Newer GPU-focused cloud providers are using networking pricing as a competitive weapon. CoreWeave, for instance, has offered zero-egress migration programs specifically targeting AI workloads. Lambda Labs, Vast.ai, and other GPU cloud providers often include more generous egress allowances than the hyperscalers, or charge significantly lower per-gigabyte rates.

When comparing infrastructure options, do not compare GPU pricing alone. Compare the total cost of ownership including networking. A provider that charges $3.00 per GPU-hour with $0.01 per gigabyte egress may be cheaper than a provider that charges $2.50 per GPU-hour with $0.09 per gigabyte egress, depending on your egress volume. At 2 terabytes per month of egress, the cheaper GPU provider costs $180 per month in egress while the pricier GPU provider costs $20. The $0.50 per hour GPU savings is $360 per month on a single GPU, but the egress difference is $160 per month. The GPU-cheaper provider still wins, but the margin is thinner than the GPU price alone would suggest. At 10 terabytes per month of egress, the math may flip entirely.

The next subchapter takes the full picture — GPU costs, networking costs, engineering costs, operational overhead — and puts it into the comprehensive break-even analysis that determines when self-hosting makes financial sense and when it is an expensive vanity project.
