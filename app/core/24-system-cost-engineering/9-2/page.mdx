# 9.2 — Cost Attribution Architecture: Tracking Spend by Feature, User, and Tenant

In late 2025, a B2B SaaS company selling an AI-powered contract analysis platform hit a wall that had nothing to do with technology. They had 340 enterprise customers, a monthly AI inference bill of $218,000, and a pricing model that charged every customer the same flat rate per seat. Their CFO wanted to know which customers were profitable. The VP of Engineering wanted to know which features to optimize first. The head of product wanted to know whether the new clause extraction feature justified its development cost. All three questions required the same data: how much does it cost to serve customer X using feature Y? Nobody could answer. The company's cost tracking stopped at the total monthly bill from Anthropic and OpenAI, split by model name. They could tell you that Claude Sonnet 4.5 cost $134,000 and GPT-5 cost $61,000 last month. They could not tell you whether TenantCorp consumed $800 or $8,000 of that total. They could not tell you whether contract summarization cost three times as much as clause extraction or the reverse. Their billing data existed in a vacuum, disconnected from the product data that would make it meaningful. The company spent four months building a cost attribution system. Within six weeks of deploying it, they discovered that their five largest customers — fourteen percent of their customer base — consumed forty-three percent of their total AI cost. Two of those customers were on legacy pricing plans that made them unprofitable at any volume. One feature, the full-document comparison tool, cost eleven times more per request than any other feature and was used by only seven percent of tenants. Every one of these discoveries was actionable. None of them was visible before attribution existed.

**Cost attribution architecture** is the technical and organizational system that assigns every dollar of AI spend to the dimensions that matter for business decisions: which feature triggered the cost, which user incurred it, which tenant owns it, which workflow produced it, and which team is responsible for it. Without attribution, you have a pile of invoices. With it, you have a cost model that tells you exactly where money flows through your system and why.

## The Five Attribution Dimensions

Cost attribution is only as useful as the dimensions you attribute to. Too few dimensions and you can't answer the questions that matter. Too many and the instrumentation burden overwhelms the engineering team. In practice, five dimensions cover the territory that ninety percent of cost decisions require.

**Per-request attribution** is the atomic unit. Every individual request to your AI system has a cost: the tokens consumed by the model call, the embedding generation for retrieval, the vector database query, the tool calls, and the allocated share of fixed infrastructure. Per-request cost is the building block from which all other dimensions are computed. If you cannot calculate the cost of a single request, you cannot calculate the cost of anything above it.

**Per-feature attribution** groups requests by the product feature that initiated them. Your search feature, your summarization feature, your chat assistant, your document analysis pipeline — each is a distinct cost center. Per-feature attribution answers the question "which parts of the product are expensive?" and is the primary input to engineering prioritization. When you know that search costs $47,000 per month and chat costs $12,000, you know where optimization effort has the highest return.

**Per-user attribution** aggregates costs to individual users. This dimension matters for products with per-user pricing, for identifying power users who consume disproportionate resources, and for detecting abuse patterns where individual users generate anomalous cost through automated or adversarial usage. Per-user cost data is also the input to usage-based billing, which has become the dominant pricing model for AI products — hybrid pricing now leads pure subscription models among AI-first companies.

**Per-tenant attribution** is the multi-tenant version of per-user attribution, aggregated to the organizational level. In B2B products, this is the dimension that determines customer profitability. A tenant that pays you $5,000 per month and costs you $6,200 to serve is destroying value. You cannot know this without per-tenant cost tracking. Per-tenant attribution also reveals consumption patterns that vary dramatically between organizations — some tenants use short, simple queries while others send complex, multi-step workflows that cost ten times more per interaction.

**Per-workflow attribution** tracks cost across multi-step processes. A document processing pipeline that involves extraction, classification, summarization, and quality scoring is a single workflow spanning multiple model calls, multiple retrieval steps, and potentially multiple models. Per-workflow cost tells you the end-to-end cost of a business process, which is the metric that matters for outcome-based pricing models and for comparing the AI system's cost against the human labor it replaces.

## The Cost Context: How Attribution Flows Through Your System

Attribution does not happen at billing time. It happens at request time. Every request that enters your system must carry a **cost context** — a set of identifiers that travel with the request through every service, every model call, every database query, and every tool invocation. When a cost event is emitted — a model call completes, a vector search runs, a tool API responds — the cost context is attached to the event. At aggregation time, the context is what allows you to group, filter, and attribute costs along any dimension.

The cost context contains, at minimum, a unique request identifier, the user identifier, the tenant identifier, the feature name, and the workflow identifier. In more mature systems, it also carries the workflow step, the retry count, the model tier requested, the team that owns the feature, and a session identifier that groups related requests within a user session.

The challenge is propagation. In a simple monolithic application, the cost context is a variable passed through function calls. In a distributed system — which most production AI products are by 2026 — the context must propagate across service boundaries. When your API gateway receives a request and calls a retrieval service, which calls an embedding service, which calls a vector database, which returns results to the retrieval service, which calls a generation service, which calls the model provider — the cost context must survive every hop. This is the same problem that distributed tracing solves for latency and error tracking, and the solution is the same: propagate context through request headers, message metadata, or a shared context store. OpenTelemetry, which has become the standard for observability instrumentation in 2026, supports custom attributes on spans that serve this purpose well. If your system already uses distributed tracing, extending it to carry cost context is a natural and relatively low-effort addition.

The teams that struggle with context propagation are those that have organic, loosely coupled architectures where services communicate through queues, event buses, or async job systems that do not naturally preserve request context. In these architectures, the cost context must be explicitly serialized into the message payload and deserialized by the receiving service. This requires every service that participates in a cost-generating workflow to know about and honor the cost context. It is not technically difficult but it is organizationally difficult — it requires every team that owns a service to add instrumentation, which means cost attribution becomes a cross-team initiative rather than something one team can build in isolation.

## Instrumenting the Cost Event

A cost event is the atomic record that your attribution system produces. Every time your system incurs a billable action, a cost event is emitted. The event contains the cost amount, the cost context, and enough metadata to make the event useful for analysis.

For a model API call, the cost event captures the provider name, the model identifier, the input token count, the output token count, the per-token prices for both input and output, the computed total cost, the latency, the response status, and the full cost context from the request. The computed cost comes from multiplying token counts by the provider's published per-token prices. For providers with tiered pricing, batch discounts, or cached-token discounts, the computation is more involved — but the principle is the same. Every call produces a record with a dollar amount attached.

For a vector database query, the cost event captures the provider, the number of queries in the batch, the number of results returned, the storage consumed, and the cost as calculated from the provider's pricing model. Vector database pricing varies significantly — some charge per query, some per million vectors stored, some per compute unit — so the cost calculation must be specific to your provider.

For external tool calls, the cost event captures the tool name, the API provider, the parameters of the call, the response size, and the cost. Some tool APIs charge per call, some charge per data unit, and some are free but consume compute resources on your infrastructure. All of these need to be represented.

For shared infrastructure — GPU instances, database hosting, platform services — cost events are not emitted per-request. Instead, the cost is allocated periodically. An hourly GPU cost is divided across the requests served during that hour, weighted by the resources each request consumed. A monthly database hosting fee is divided across the queries served during that month. These allocations are approximations, but they capture the infrastructure component that pure API-cost tracking misses. The alternative — ignoring shared costs — leads to a per-request cost number that is artificially low because it excludes the platform the request runs on.

## Building the Cost Data Pipeline

Cost events need to flow from the instrumentation layer to a queryable store where they can be aggregated and analyzed. The pipeline has three stages: collection, enrichment, and storage.

**Collection** gathers cost events from every instrumented service. In a lightweight implementation, this is as simple as each service writing cost events to a shared log stream — a Kafka topic, a cloud event bus, or a structured logging pipeline. The events arrive as structured records with the cost amount, the cost context, and the event metadata. Volume matters here: a system processing 500,000 AI requests per day generates at least 500,000 cost events per day from model calls alone, plus additional events from retrieval, tools, and infrastructure allocation. The collection layer must handle this volume without introducing latency into the request path, which means cost event emission must be asynchronous. The model call completes and the response is returned to the user. The cost event is emitted in the background. If the cost pipeline is down, the request still succeeds — cost events are important but never worth degrading user experience.

**Enrichment** adds metadata that isn't available at emission time. The most common enrichment is mapping internal identifiers to business entities — converting a user ID to a customer name, a feature code to a feature label, a team identifier to a cost center. Enrichment can also add derived fields: whether this request was a retry, the workflow step it belongs to, the pricing tier the tenant is on. Some teams run enrichment inline in the pipeline. Others enrich at query time using joins. Inline enrichment adds processing overhead but makes downstream queries faster. Query-time enrichment is simpler to maintain but slower at scale.

**Storage** is the destination where enriched cost events land for querying. Time-series databases are the natural fit because cost data is inherently temporal and most analytical queries involve time-based aggregation. ClickHouse has emerged as a popular choice for cost analytics because of its columnar storage, fast aggregation, and ability to handle billions of rows with sub-second query times. TimescaleDB, built on PostgreSQL, is another strong option for teams that want SQL compatibility. Some teams use their existing data warehouse — Snowflake, BigQuery, Redshift — which works well for batch analytics but is less suited for real-time dashboards with sub-minute refresh rates.

The storage schema matters. At minimum, you need columns for the timestamp, the cost amount, the cost currency, the provider, the model, the request ID, the user ID, the tenant ID, the feature name, the workflow ID, the workflow step, the token counts, and the event type. Partitioning by time and indexing by tenant and feature are essential for query performance. A well-designed schema makes the difference between a cost dashboard that loads in 200 milliseconds and one that times out.

## Handling Shared Costs: The Allocation Problem

Not all costs can be directly attributed to a single request. Shared costs — the GPU instance that serves multiple models, the vector database cluster that supports multiple features, the platform engineering team that maintains the infrastructure — must be allocated using a model that approximates reality without requiring perfect precision.

The simplest allocation model is **equal distribution**: divide the shared cost by the total number of requests served during the period. A $10,000 monthly GPU hosting bill divided by 3 million requests gives a $0.0033 per-request infrastructure allocation. This is crude but directional. It captures the infrastructure cost that pure API-cost tracking misses, and it is close enough for decisions that don't require cent-level precision.

A more accurate model is **usage-weighted allocation**: divide the shared cost proportionally to the resources each request consumed. If feature A processes 70 percent of the requests but those requests are simple and fast, while feature B processes 30 percent of the requests but they are compute-intensive and long-running, equal distribution overcharges feature A and undercharges feature B. Usage-weighted allocation might use GPU-seconds per request, memory consumption, or inference latency as the weighting factor. This requires more instrumentation — you need to measure the resource consumption of each request, not just the count — but it produces an allocation that reflects actual resource usage.

The most precise model is **direct metering**: instrumenting the infrastructure layer to measure exactly which resources each request consumed. Kubernetes-native environments can use resource quotas, pod-level CPU and memory metrics, and GPU utilization per container to calculate the infrastructure cost of each request with high accuracy. This approach is expensive to implement and maintain, and the precision it provides is rarely necessary for decision-making. The difference between "this feature costs $0.0041 per request in infrastructure" and "this feature costs approximately $0.004 per request in infrastructure" is not meaningful for any real business decision.

Pick the allocation model that matches your decision-making needs. If you are setting per-tenant pricing and need to defend your numbers to enterprise customers, usage-weighted allocation is worth the investment. If you are identifying which features to optimize first, equal distribution is sufficient because the ranking of features by cost is usually the same regardless of how you allocate the shared component.

## The Discovery Phase: What Attribution Reveals

The first month after deploying cost attribution is consistently the most valuable. Not because the system is perfectly calibrated — it won't be — but because even approximate per-feature and per-tenant cost data reveals patterns that were completely invisible at the aggregate level.

The most common discovery is **cost concentration**. In system after system, a small number of features or tenants account for a disproportionate share of total cost. The Pareto pattern holds remarkably well in AI cost data: in many products, roughly twenty percent of features generate eighty percent of cost. Sometimes the concentration is even more extreme. An HR technology company deployed attribution and found that their resume parsing feature — one of nine AI features in their product — consumed sixty-one percent of the total inference budget. The feature sent full resumes, often five to eight pages, as a single prompt to a frontier model for structured extraction. Every other feature used smaller prompts, simpler models, or fewer tokens. Nobody had noticed because nobody could see the per-feature breakdown.

The second common discovery is **tenant cost variance**. In multi-tenant B2B products, the cost to serve different tenants varies by an order of magnitude or more. This variance comes from differences in usage volume, query complexity, document sizes, and feature mix. A legal technology company found that their median tenant cost was $420 per month, but their top decile of tenants cost $4,700 per month — over eleven times the median. Three of their top-cost tenants were on pricing plans that generated less revenue than the AI cost to serve them. Without attribution, these customers appeared profitable because the average cost per tenant, spread across all customers, was well below the average revenue per tenant. The average hid the extremes.

The third common discovery is **retry and redundancy cost**. Attribution reveals how much of your spend comes from retries, fallbacks, and redundant processing that is invisible at the aggregate level. A customer support platform discovered that seventeen percent of their model calls were retries triggered by confidence scoring — the first call returned an answer below the confidence threshold, so the system automatically tried again with a longer prompt or a more capable model. This retry logic was designed for quality, and it worked. But nobody had measured its cost impact because the retries were invisible at the billing level. They showed up only when each retry carried a cost event tagged as a retry in the attribution system. The team optimized by adjusting the confidence threshold upward for low-stakes queries, reducing retries by forty percent and saving $11,000 per month.

## Organizational Prerequisites: Who Owns Attribution

Cost attribution architecture is a technical system, but its success depends on organizational structure. Three prerequisites must be in place.

First, someone must own cost observability as a primary responsibility. Not a side project. Not a quarterly initiative. A named owner — a team, a role, or a charter — with the authority to require instrumentation from other teams and the accountability to deliver accurate, timely cost data. In organizations with a dedicated platform engineering team, cost attribution is a natural addition to their scope. In smaller organizations, it often falls to a senior engineer who also owns infrastructure, but who has explicit time allocated for cost observability rather than squeezing it in between other work.

Second, there must be an agreed-upon tagging schema. What are the feature names? What are the workflow identifiers? How are tenants identified? These seem trivial, but inconsistent tagging is the single most common failure mode in cost attribution. If one service tags a feature as "search" and another tags it as "semantic-search" and a third tags it as "search-v2," your aggregation is garbage. The tagging schema must be defined centrally, documented, and enforced through validation in the instrumentation layer. New features must be registered in the schema before they launch. This is bureaucracy that pays for itself.

Third, cost data must be accessible to the people who make decisions. If cost dashboards are visible only to the infrastructure team, product managers and engineering leads lack the information they need to prioritize optimizations. If finance cannot query cost-per-tenant data, they cannot model pricing changes. The most effective organizations make cost data a shared resource — visible to engineering, product, finance, and leadership — with role-based access where sensitive tenant-level data is restricted to the teams that need it.

## Starting from Zero: The 30-Day Attribution Buildout

If your system has no cost attribution today, here is the sequence that gets you to per-feature and per-tenant visibility in thirty days.

**Week one**: define the tagging schema. List every feature in your product that makes AI calls. List every model provider and every external service with a cost. Agree on a canonical feature name for each. Agree on how user and tenant identifiers will propagate through the system. Document the schema in a shared spec. This is a day of meetings and a day of documentation, and it saves weeks of rework later.

**Week two**: instrument your highest-cost model provider. Wrap every API call to that provider with a cost event emitter that logs the request, the token counts, the computed cost, and the feature and tenant tags from the cost context. Deploy the instrumentation. Verify that events are flowing correctly by sampling.

**Week three**: build the collection and storage pipeline. Set up the event stream, the storage layer, and a basic aggregation query that groups cost by feature and by tenant for the current week. Run the query. Review the results. Fix any tagging inconsistencies that the data reveals.

**Week four**: build the first dashboard. Display cost by feature, cost by tenant, and cost by day. Show the top ten features by cost and the top ten tenants by cost. Share the dashboard with engineering, product, and finance leadership. Let the questions begin.

This is not a complete cost attribution platform. It covers your primary model provider, not all cost sources. It uses equal-distribution allocation for shared costs, not usage-weighted. The dashboard is basic. But it gives you per-feature and per-tenant cost visibility that most teams in 2026 still lack, and it gives your organization the data foundation to make informed cost decisions for the first time.

## What Comes Next

Attribution tells you where every dollar goes. But knowing where money goes is only useful if you can convert that knowledge into business metrics that drive decisions. The next subchapter covers unit economics calculation — how to transform raw cost attribution data into the metrics that matter: cost per request, cost per user, cost per workflow, and the most important metric of all, cost per outcome.
