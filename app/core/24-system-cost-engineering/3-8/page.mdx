# 3.8 â€” Multi-Model Pipeline Costs: Calculating the True Cost of Orchestrated Workflows

In March 2025, a legal technology company built a document processing pipeline that handled contract review for mid-market law firms. The pipeline was elegant: a classification model identified the contract type, an extraction model pulled key clauses, a summarization model generated the executive brief, a compliance model flagged regulatory risks, and a quality-check model verified the outputs for consistency. Five models, one pipeline, one user request. When the team calculated their cost projection for investors, they estimated inference cost based on the summarization model, which was the most expensive step. Their projection came in at $0.08 per document. After three months in production, their actual cost was $0.31 per document. The classification step added $0.02. The extraction step added $0.06. The compliance model added $0.09. The quality-check model added $0.06. Combined with the summarization model's $0.08, the total was nearly four times the original estimate. At 40,000 documents per month, the team was spending $12,400 instead of the projected $3,200. They burned through their runway nine months faster than planned.

The mistake was not building a multi-model pipeline. The pipeline produced excellent results that no single model could match. The mistake was calculating cost as if the pipeline were a single model call. This is one of the most common and most expensive accounting errors in AI product development, and it happens because engineers think about pipeline design as an architecture problem while treating cost as a single-model problem. In 2026, most production AI systems are not single-model products. They are orchestrated workflows where a single user request triggers a cascade of model calls, and each call adds to the bill.

## The Anatomy of Pipeline Cost Multiplication

**Pipeline Cost Multiplication** is the phenomenon where the total inference cost of a multi-model workflow exceeds the cost of any individual model in the pipeline by a factor proportional to the number of steps. The multiplication is not always a clean integer because different steps use different models at different price points, and not every step executes on every request. But the principle holds: every model call in the pipeline has its own token consumption and its own price, and the total is the sum of all steps, not the cost of the most prominent one.

The simplest example is a two-step pipeline: classify then generate. A routing model reads the user's query, classifies it into a category, and selects the appropriate generation model. The classification step uses a small, cheap model, say $0.20 per million input tokens. The generation step uses a mid-tier model at $3.00 per million input tokens and $12.00 per million output tokens. If the classification consumes 200 input tokens and 20 output tokens, and the generation consumes 500 input tokens and 800 output tokens, the per-request cost is the sum of both. Classification costs approximately $0.00004 for input and $0.000004 for output. Generation costs approximately $0.0015 for input and $0.0096 for output. The total is about $0.0111 per request, of which classification is less than one percent. In this case, Pipeline Cost Multiplication is negligible because the cheap router model adds almost nothing.

But consider a more realistic production pipeline. A customer support system receives a user message and runs it through five steps: intent classification using a small model, knowledge retrieval augmentation using an embedding model, response generation using a mid-tier model, tone and policy verification using a second mid-tier model, and a final safety check using a small model. Each step consumes tokens, and the mid-tier models in steps three and four dominate the cost. The total per-request cost is not the generation model's cost. It is the generation model's cost plus the verification model's cost plus the smaller contributions from classification, embedding, and safety. If generation costs $0.015 per request and verification costs $0.012, the pipeline cost is at least $0.027 before you add the smaller steps. That is an eighty percent premium over what you would estimate from the generation model alone.

The multiplication factor depends on how many steps use expensive models and how much token volume each step processes. Pipelines where most steps use small, cheap models and only one step uses a frontier model have multiplication factors close to 1.1 or 1.2. Pipelines where multiple steps use mid-tier or frontier models have multiplication factors of 2 to 4. The legal document pipeline from the opening story had a multiplication factor of 3.9, which is on the high end but not unusual for complex document processing workflows.

## Common Pipeline Patterns and Their Cost Profiles

Different pipeline architectures produce different cost multiplication factors. Understanding the common patterns helps you estimate costs before you build.

The **classify-then-generate** pattern is the simplest and cheapest. A small router model directs traffic to the appropriate generation model. The classification step adds one to three percent to total cost. This pattern is cost-efficient because the routing step is trivial in token volume and uses the cheapest available model. Most model routing architectures follow this pattern, and the cost overhead is negligible.

The **extract-then-summarize** pattern is common in document processing. An extraction model pulls structured data from unstructured text, and a summarization model generates a human-readable summary. Both steps consume significant tokens because the input document flows through both models, often in full. If the document is 3,000 tokens, the extraction model processes 3,000 input tokens and produces 500 output tokens, and the summarization model processes 3,500 input tokens (original document plus extraction results) and produces 400 output tokens. The total token consumption is roughly 7,000 input and 900 output, compared to 3,000 input and 400 output for a single-model approach. The cost multiplication is approximately 2 to 2.5 times, depending on model pricing.

The **generate-then-evaluate** pattern adds a quality gate after generation. A generation model produces the output, and an evaluation model scores it for quality, accuracy, or policy compliance. If the evaluation passes, the output is returned. If it fails, the generation model retries with modified instructions. This pattern has a base multiplication factor of about 1.8 to 2.2, because the evaluation model processes both the input and the generated output. But the retry mechanism adds a variable cost: if ten percent of outputs fail evaluation and require a second generation attempt, the effective multiplication factor increases by another ten to fifteen percent.

The **route-then-process-then-verify** pattern is the most common in production systems with quality requirements. A router selects the model, the selected model processes the request, and a verification model checks the output. The multiplication factor is typically 2 to 3 times, depending on the models used for each step.

The **multi-stage extraction** pattern chains multiple extraction steps where each step refines the output of the previous one. First pass: rough extraction. Second pass: entity resolution. Third pass: relationship mapping. Each pass consumes the original document plus the accumulated outputs of all previous passes, so the token volume grows with each step. A three-stage extraction pipeline on a 3,000-token document might consume 12,000 to 15,000 total input tokens, roughly four to five times what a single extraction pass would cost.

## Calculating Pipeline Cost with Conditional Paths

Not every pipeline step executes on every request. Many production pipelines include conditional branches where certain steps only run when specific conditions are met. A content moderation pipeline might have a fast-path for clearly safe content that skips the detailed analysis step. A customer support pipeline might route simple questions to a single generation model and only invoke the multi-step pipeline for complex questions. These conditional paths reduce average cost because the expensive steps do not execute every time.

To calculate the expected cost of a conditional pipeline, you weight each step's cost by the probability that it executes. If your pipeline has three steps, where step one always runs at $0.003 per request, step two runs seventy percent of the time at $0.012 per request, and step three runs thirty percent of the time at $0.008 per request, the expected cost per request is $0.003 plus 0.7 times $0.012 plus 0.3 times $0.008. That equals $0.003 plus $0.0084 plus $0.0024, totaling $0.0138 per request. Compare this to the worst-case cost where all three steps execute every time: $0.023. The conditional paths save forty percent on average.

The challenge is knowing the execution probabilities accurately. During development, you estimate based on evaluation data. In production, you measure. Many teams discover that their estimated probabilities were wrong. A team might assume that only thirty percent of queries need the expensive analysis step, but in production the number is fifty-five percent because real user queries are more ambiguous than test queries. This mismatch between estimated and actual execution probabilities is a common source of cost overruns in pipeline architectures. Monitor execution rates per step from day one and update your cost model as real data comes in.

Another subtlety is retry probability. When a generation step fails quality checks and retries, it doubles the cost of that step for that request. If ten percent of requests require one retry, that adds ten percent to the cost of that step. If two percent require two retries, that adds four percent more. You need to include retry probabilities in your cost model, especially for pipelines with strict quality gates that trigger frequent retries.

## Optimizing Pipeline Cost Without Sacrificing Quality

Pipeline optimization is about reducing total cost while preserving the quality that the pipeline was built to deliver. There are four primary optimization levers, and the best teams use all of them.

The first lever is **step elimination**. Review each step in the pipeline and ask whether it is still necessary. Teams build pipelines incrementally, adding steps to solve specific problems, and rarely go back to remove steps that are no longer needed. A classification step that was essential when you had five generation models might be unnecessary if you have consolidated to two. A quality-check step that catches errors from a previous model version might be redundant after upgrading to a more capable model. Audit your pipeline quarterly and remove any step that does not measurably improve output quality.

The second lever is **model downgrading**. For each pipeline step, evaluate whether a cheaper model can achieve acceptable quality. The verification step in your pipeline might use the same frontier model as the generation step because that is what was convenient during development, but a model costing one-fifth as much might verify outputs equally well. Classification, routing, and format validation steps almost always work with small, cheap models. Even some generation steps can use mid-tier models when the task is straightforward. Test cheaper models for each step independently, measuring the pipeline's end-to-end quality. If swapping a step from a $12 per million output token model to a $3 model does not degrade pipeline output quality by more than one or two percentage points, make the swap.

The third lever is **input reduction**. Many pipeline steps receive more context than they need. A verification step that checks tone and policy compliance does not need the entire source document, just the generated output. An extraction step that pulls three specific fields does not need the full contract, just the relevant sections. Reducing input tokens to each step directly reduces cost. Some teams build a pre-processing step that trims inputs before passing them to expensive models, and the cost of the trimming step is far less than the savings from reduced input tokens on the expensive model.

The fourth lever is **caching intermediate results**. If the same document flows through multiple pipeline steps, and the first step's output is reusable, cache it. If the classification result for a user determines which pipeline path to follow, and the same user sends similar queries, cache the classification. Caching at the pipeline step level compounds savings because each cached result avoids not just one model call but all downstream model calls that depend on it.

## The Pipeline Cost Spreadsheet

Every team running a multi-model pipeline should maintain a pipeline cost model. This is a simple spreadsheet or database table that lists every step in the pipeline, the model used for that step, the average input tokens per request, the average output tokens per request, the per-token cost, the execution probability, and the resulting expected cost per request for that step. The total pipeline cost is the sum of all steps.

This model should be updated monthly with actual production data. You will find that token consumption drifts as user behavior changes, that execution probabilities shift as your content mix evolves, and that per-token costs change as providers adjust pricing. A pipeline cost model that was accurate in January may be off by twenty percent by June if you do not update it.

The cost model also serves as your optimization map. Sort the steps by cost contribution, and you immediately see where optimization effort yields the most savings. If one step accounts for fifty-five percent of pipeline cost, that is where you focus first. Model downgrading, input reduction, or caching on that single step might save more than optimizing every other step combined.

Teams that maintain and review pipeline cost models quarterly consistently spend fifteen to thirty percent less on inference than teams that estimate pipeline cost once during design and never revisit it. The model is simple to build, simple to maintain, and pays for itself many times over.

## The Multimodal Cost Multiplier

Everything discussed so far assumes text-only pipelines. The moment your pipeline crosses into images, audio, or video, the cost math changes by an order of magnitude. A single multimodal request that processes an image, generates a text description, and synthesizes an audio narration can cost ten to fifty times what a text-only request of similar complexity would cost. Most teams discover this after launch, not before, because they prototype with text and add modalities later without re-running the cost model.

Start with vision. Sending an image to a vision-capable model is not free input, it is token-expensive input. Claude converts images to tokens using a formula based on pixel dimensions, roughly the width times height divided by 750. A standard 1024 by 1024 image consumes approximately 1,400 tokens. At Claude Sonnet 4.5 pricing of $3 per million input tokens, that single image costs about $0.004 to analyze. At Claude Opus 4.6 pricing, it is closer to $0.02 per image. Gemini charges images at roughly 258 tokens per image regardless of resolution, making it cheaper for high-resolution images but more expensive for tiny thumbnails. GPT-5 vision processes images through its multimodal input pipeline at standard input token rates of $1.25 per million tokens, but a single image can easily consume 1,000 to 2,000 tokens depending on detail level. The cost per image analysis seems trivial in isolation, $0.002 to $0.02 per image, but a document processing pipeline that analyzes fifty images per request turns that into $0.10 to $1.00 in vision costs alone.

Image generation is a different cost category entirely. OpenAI's GPT Image 1 charges $0.011 to $0.25 per generated image depending on quality tier and resolution, with medium-quality square images landing around $0.07. The newer GPT Image 1.5 runs $0.009 to $0.20. DALL-E 3 remains available at $0.04 to $0.12 per image, and Midjourney's subscription model works out to roughly $0.05 per image on the Standard plan. Self-hosting Stable Diffusion or FLUX on a single A100 GPU costs approximately $1.50 to $3.00 per GPU hour, which produces roughly 200 to 400 images per hour at standard settings, yielding a per-image cost of $0.004 to $0.015. The self-hosted route is dramatically cheaper at volume but requires infrastructure investment and operational overhead that only makes sense above 50,000 images per month. The critical insight for pipeline cost modeling is that image generation costs do not scale with token volume the way text does. Each generated image is a fixed-cost event regardless of how many tokens of prompt you sent to request it. A one-sentence prompt and a ten-paragraph prompt produce the same per-image charge, which means your optimization lever is generation count, not prompt length.

## Audio Costs: The Per-Minute and Per-Character Trap

Audio pipelines introduce two distinct cost streams: speech-to-text on the input side and text-to-speech on the output side. Teams that budget for one often forget the other.

Speech-to-text pricing has converged around $0.004 to $0.006 per minute across major providers. OpenAI's Whisper API charges a flat $0.006 per minute. Deepgram starts at $0.0043 per minute for base transcription, with enhanced models running $0.007 to $0.009. AssemblyAI advertises $0.0025 per minute but charges on session duration rather than audio length, which adds roughly sixty-five percent overhead on short clips, bringing effective cost closer to $0.004 per minute. For a customer support pipeline processing five-minute calls, speech-to-text adds $0.02 to $0.03 per request. Modest. But a podcast transcription service processing sixty-minute episodes at $0.006 per minute pays $0.36 per episode in transcription alone, which matters when you have thousands of episodes per month.

Text-to-speech is more expensive. OpenAI's standard TTS charges $15 per million characters, which works out to $0.015 per thousand characters. Their HD model doubles that to $30 per million. ElevenLabs, the current quality leader, charges $0.12 to $0.30 per thousand characters depending on plan tier. A typical 500-word response contains roughly 2,500 characters, so generating spoken audio for it costs $0.04 with OpenAI standard TTS or $0.30 to $0.75 with ElevenLabs. For a voice-first application where every response is spoken aloud, this adds a fixed per-response cost that often exceeds the LLM inference cost itself.

The real cost explosion happens with real-time voice. OpenAI's Realtime API, designed for conversational voice agents, prices audio input at $100 per million tokens and audio output at $200 per million tokens. Audio tokens map to time: roughly one token per 100 milliseconds of input and one token per 50 milliseconds of output. A five-minute voice conversation generates approximately 3,000 input audio tokens and 6,000 output audio tokens, costing roughly $0.30 for input and $1.20 for output, totaling $1.50 for a single conversation before you count the text tokens processed alongside the audio. Compare that to a text-based chatbot handling the same exchange for $0.02 to $0.05 in token costs. The real-time voice multiplier is thirty to seventy-five times the text-only cost. Cached audio input tokens are available at a discount, roughly $20 per million instead of $100, which helps for repetitive system prompts, but conversational audio is inherently uncacheable because every utterance is unique.

## Video: The Most Expensive Modality

Video is where multimodal costs become genuinely startling. Video understanding and video generation sit at opposite ends of the same cost spectrum, and both are expensive.

For video understanding, Gemini processes video at roughly 258 tokens per second of footage sampled at one frame per second. A sixty-second video clip consumed as input to Gemini 2.5 Pro at $1.25 per million input tokens costs approximately $0.019 in input tokens alone. That seems manageable until you realize that most video analysis pipelines process minutes or hours of footage. A ten-minute video generates roughly 155,000 input tokens, costing $0.19 with Gemini 2.5 Pro. A surveillance monitoring system processing eight hours of footage per camera per day would consume over 7.4 million input tokens per camera, costing $9.30 per camera per day before any output token costs. Scale that to fifty cameras and the video understanding costs alone exceed $13,900 per month.

Video generation is the single most expensive AI modality in production today. Sora 2 charges $0.10 per second of generated video at standard resolution and $0.30 per second at the Pro tier. Runway Gen-4 operates on a subscription-plus-credits model where effective per-second costs range from $0.08 to $0.20 depending on resolution and plan. Kling AI's credit system translates to roughly $0.05 to $0.15 per second of generated video. The budget option, Vidu, runs approximately $0.04 per second. Google's Veo 3.1 falls in a similar range. At these rates, generating a single thirty-second marketing video clip costs $1.20 to $9.00. Generating a three-minute product demo costs $7.20 to $54.00 in raw generation costs, before any retakes, edits, or quality iterations that typically multiply the final cost by two to four times. Average per-second costs dropped roughly sixty-five percent between early 2024 and late 2025, but even at today's prices, video generation remains orders of magnitude more expensive than any text operation. Budget for video generation the way you budget for cloud compute, not the way you budget for API calls.

## Compound Multimodal Pipelines

The true danger of multimodal costs emerges when pipelines chain modalities together. Consider a real estate listing pipeline that takes property photos, generates text descriptions from the images, translates descriptions into three languages, and produces audio narrations for each language. One listing with twenty photos triggers twenty vision API calls at $0.01 each ($0.20), one text generation call at $0.03, three translation calls at $0.01 each ($0.03), and three audio narrations of roughly 3,000 characters each at $0.015 per thousand characters ($0.135). The total per listing is approximately $0.40. A text-only pipeline that accepted pre-written descriptions and just translated them would cost $0.06. The multimodal pipeline costs nearly seven times as much.

Now scale it. A platform processing 5,000 listings per month at $0.40 each spends $2,000 on multimodal inference. The text-only alternative would cost $300. The $1,700 monthly difference buys the higher quality and richer content that drives more buyer engagement, but only if the team modeled that cost before committing to the architecture.

The compounding gets worse when video enters the pipeline. An e-commerce product video pipeline that generates a fifteen-second video per product ($1.50 to $4.50 per video), adds a voiceover narration ($0.08), and generates subtitles ($0.02) costs $1.60 to $4.60 per product. At 10,000 products, that is $16,000 to $46,000 in a single pipeline run. Teams that prototype this workflow on ten products see a $45 bill and assume the full run will be proportional. It is. And the proportional number is terrifying.

The pipeline cost spreadsheet described earlier in this subchapter becomes non-negotiable for multimodal pipelines. Every modality crossing, where text becomes audio, where images become text, where text becomes video, is a cost multiplication point that must be explicitly modeled. Add a column for modality type to each row in your spreadsheet, because text-token-based cost estimation does not apply to image generation, audio synthesis, or video creation. Those are fixed per-unit costs, not per-token costs, and conflating the two produces fantasy budgets. The team that treats multimodal costs as a rounding error on their text inference bill will burn through budget faster than any other mistake in this chapter.

## When Pipelines Are Worth the Multiplication

Pipeline Cost Multiplication is not a reason to avoid pipelines. It is a reason to account for them honestly. Multi-model pipelines exist because single models cannot handle complex workflows at production quality. A single model asked to simultaneously classify, extract, summarize, and verify will produce worse results than a pipeline where each step uses a model optimized for that specific task. The pipeline costs more per request, but it delivers higher quality, which often means fewer errors, fewer human reviews, and higher customer satisfaction, all of which have their own economic value.

The decision framework is straightforward. Calculate the pipeline's total cost per request using the methods in this subchapter. Calculate the quality improvement the pipeline delivers compared to a single-model approach, measured by your evaluation suite. Calculate the economic value of that quality improvement: fewer support tickets, higher conversion rates, lower error correction costs, reduced compliance risk. If the value exceeds the cost multiplication, the pipeline is worth it. If it does not, simplify.

In most production systems, the answer is that some pipeline steps are worth their cost and others are not. The verification step that catches fifteen percent of generation errors before they reach customers is worth a 1.8 times cost multiplication. The formatting step that slightly improves output presentation but catches errors less than one percent of the time is not. The pipeline cost model tells you exactly which steps to keep and which to cut.

Multi-model pipelines multiply cost in predictable ways that you can model, measure, and optimize. But there is a class of AI system where cost multiplication is not predictable at all, where the number of model calls per request is not fixed but varies wildly based on the task. Those are agent systems, and their cost dynamics are the subject of the next subchapter.
