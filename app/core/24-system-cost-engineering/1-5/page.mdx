# 1.5 â€” Cost as an Engineering Problem, Not a Finance Problem

Most organizations treat AI costs the way they treat electricity bills: someone in Finance reviews the invoice at the end of the month, flags anything unusual, and sends an email asking Engineering to "look into it." This is a catastrophic misunderstanding of where AI costs originate. Your electricity bill is determined by the building's HVAC system and lighting, which were designed once and rarely change. Your AI bill is determined by thousands of engineering decisions made every day in code, in prompts, in model selections, in retry policies, in caching strategies. By the time Finance sees the invoice, the money is already spent. The decisions that determined the bill were made weeks ago by engineers who had no idea they were making cost decisions.

This is not a criticism of Finance. Finance teams are excellent at tracking spend, forecasting based on historical trends, and negotiating contracts. But they cannot control AI costs because AI costs are not determined by contracts or budgets. They are determined by code. An engineer who adds ten examples to a system prompt has just increased input token consumption by 40% for every request that hits that endpoint. An engineer who changes a retry policy from two retries to five retries has just increased the worst-case cost of every failed request by 150%. An engineer who switches from a compact model to a frontier model for a classification task has just tripled the cost per call. These are engineering decisions with financial consequences, and they happen in pull requests, not in budget meetings.

## The Engineering Cost Surface

**The Engineering Cost Surface** is the complete set of technical decisions that collectively determine 80% or more of your AI bill. It is not one decision. It is hundreds of decisions spread across your codebase, your infrastructure configuration, your prompt templates, your model routing logic, and your error handling policies. Each decision individually might be small, but together they form a surface that defines your cost structure as precisely as your database schema defines your data model.

The Engineering Cost Surface includes at least seven categories of decisions. First, prompt design: how many tokens your system prompt consumes, how many few-shot examples you include, how much context you inject from retrieval, how much of the conversation history you carry forward. Second, model selection: which model handles which request type, whether you use a frontier model for everything or route simple tasks to smaller models. Third, output configuration: maximum output tokens, temperature settings that affect retry rates, structured output constraints that affect token efficiency. Fourth, error handling: retry count, backoff policy, timeout thresholds, fallback behavior when the primary model fails. Fifth, caching strategy: which responses are cacheable, cache duration, cache key design, cache hit rate targets. Sixth, evaluation overhead: what percentage of production traffic gets evaluated, which evaluations run inline versus async, how often you re-evaluate the full test suite. Seventh, infrastructure provisioning: instance types, autoscaling thresholds, reserved versus on-demand allocation, multi-region redundancy.

Every one of these decisions lives in your engineering codebase. Every one of them changes when an engineer opens a pull request. And every one of them has a dollar value attached to it, whether the engineer knows it or not.

## Prompt Design as Cost Design

The most powerful cost lever in most AI systems is the prompt. Not the model. Not the infrastructure. The prompt. Because the prompt determines how many tokens flow through the system on every single request, and token volume is the dominant variable cost.

Consider a customer support system handling 100,000 requests per day. The system prompt is 1,200 tokens. Each request includes the last five conversation turns, averaging 600 tokens. The retrieval system injects three relevant knowledge base articles, averaging 800 tokens total. The user's message averages 100 tokens. Total input tokens per request: 2,700. At GPT-5 pricing of $1.25 per million input tokens, that is $0.0034 per request, or $337 per day in input costs alone.

Now an engineer decides to improve response quality by adding eight few-shot examples to the system prompt, adding 1,600 tokens. The system prompt is now 2,800 tokens. Total input per request: 4,300 tokens. Daily input cost: $537. That one pull request increased input costs by $200 per day, $6,000 per month, $72,000 per year. The engineer was solving a quality problem. They had no idea they were making a $72,000 cost decision. No one reviewed the pull request through a cost lens. No one asked whether the quality improvement justified the cost increase. No one even measured the cost impact.

This is what it means for prompt design to be cost design. Every token in the prompt has a price. Every few-shot example, every instruction, every piece of context injected from retrieval carries a per-request cost that multiplies across your entire traffic volume. The engineer who designs the prompt is the engineer who designs the cost.

The reverse is equally powerful. An engineer who compresses the system prompt from 1,200 tokens to 600 tokens by removing redundant instructions saves $168 per day, $5,000 per month. An engineer who reduces retrieval injection from three articles to the single most relevant article saves $267 per day, $8,000 per month. An engineer who implements prompt caching for the system prompt portion, achieving a 90% cache hit rate with a provider that offers 90% discount on cached tokens, saves $273 per day, $8,200 per month. These are not theoretical savings. They are the direct result of engineering decisions made in code.

## Model Selection as Cost Selection

Every model selection decision is a cost decision. When you choose which model handles a request, you are choosing a price point. The price difference between model tiers is not marginal. It is often five to twenty-five times.

The model landscape in 2026 spans a wide cost range. GPT-5-nano and similar compact models cost fractions of a cent per request. GPT-5 and Claude Sonnet 4.5 occupy the mid-tier at a few cents per request for typical workloads. Claude Opus 4.5, GPT-5.2, and Gemini 3 Deep Think sit at the premium tier, costing ten to fifty cents per request for complex tasks. The difference between routing a request to the compact tier versus the premium tier can be a fifty times cost multiplier.

Most AI systems route every request to the same model. This is the simplest architecture and the most expensive. A classification task that could be handled by GPT-5-nano at $0.0001 per request gets routed to GPT-5 at $0.003 per request, a thirty times overspend. A summarization task that needs GPT-5 quality gets routed to Claude Opus 4.5 because the system uses Opus for everything, a ten times overspend. Across 100,000 daily requests, even small per-request overspend compounds into thousands of dollars per month.

The engineering decision to implement model routing, where different request types go to different models based on complexity, is one of the highest-ROI cost optimizations available. Stanford research on LLM cascade routing demonstrated savings of up to 98% on certain workloads by routing simple requests to cheap models and only escalating to expensive models when the cheap model's confidence was low. You do not need to achieve 98% savings to justify the investment. A team that routes 60% of traffic to a model that costs one-tenth as much as their default model reduces total variable costs by more than 50%.

But model routing is an engineering decision. It requires building a classifier or confidence scorer. It requires defining routing rules. It requires monitoring to ensure quality does not degrade on the cheaper model. Finance cannot implement this. Only engineers can. And every day they do not implement it, the organization is overpaying.

## Error Handling as Cost Policy

Your retry policy is a cost policy. This is not a metaphor. When you configure how your system handles failures, you are configuring how much money your system spends when things go wrong. And things go wrong regularly. API providers experience degraded performance. Network connections time out. Rate limits get hit. Models return malformed outputs that need regeneration.

A typical retry policy specifies three parameters: retry count, backoff interval, and timeout. A policy of three retries with exponential backoff starting at one second and a thirty-second timeout means that a failed request can generate up to four total attempts, each consuming tokens and incurring API costs. If the original request costs $0.02, the worst case under this policy is $0.08 for a single user interaction. That is four times the normal cost.

Under normal conditions, retry rates are low, perhaps 1% to 3% of requests. The extra cost is negligible. But during a provider degradation event, retry rates spike. If 20% of requests fail and each triggers three retries, your effective traffic increases by 60%. Your variable costs increase by 60%. And because the retries themselves add load to the degraded provider, the retry rate often increases further, creating the cascading retry storm described in the previous subchapter.

An engineer who changes the retry count from two to five has just increased the worst-case cost of a failed request from $0.06 to $0.12. At 100,000 requests per day with a 20% failure rate during an incident, the difference between two retries and five retries is $24,000 versus $60,000 in extra cost over a three-day incident. That configuration change was made in one line of code, probably with no cost analysis, probably to improve reliability, and it more than doubled the cost exposure during failure scenarios.

The engineering discipline here is to treat every error handling configuration as a cost configuration. Define retry budgets per request. Implement circuit breakers that halt retries when failure rates exceed thresholds. Set cost caps on retry spending. Monitor retry rates as a cost metric, not just a reliability metric. These are engineering decisions that Finance cannot make because Finance does not know what a retry policy is or where it lives in the codebase.

## Caching as Cost Optimization

Caching is the single most underutilized cost optimization in AI systems. Most AI teams do not cache LLM responses because they assume every response is unique. This assumption is wrong for a surprising percentage of workloads.

Consider a product recommendation system. Many users ask similar questions. "What laptop should I buy for college?" and "Best laptop for a student?" are semantically identical. If your system generates a fresh response for each, you pay full token cost twice. If you implement semantic caching, where you check whether a sufficiently similar query has been answered recently, you serve the cached response at zero token cost. The cache hit rate depends on your traffic patterns, but e-commerce and customer support systems routinely achieve 15% to 30% semantic cache hit rates. At 100,000 requests per day, a 20% cache hit rate eliminates 20,000 API calls, saving $400 per day or $12,000 per month at $0.02 per request.

Prompt caching is a different and complementary optimization. Providers like Anthropic and OpenAI offer prompt caching that stores the key-value computations for repeated prompt prefixes. If your system prompt is the same across all requests, the provider caches the computation for that prefix and charges up to 90% less for the cached portion. On a system where the system prompt represents 40% of input tokens, prompt caching at a 90% discount on that portion reduces input costs by 36%. For a system spending $10,000 per month on input tokens, that is $3,600 in monthly savings from a configuration change that takes an afternoon to implement.

Response caching for deterministic queries is even more powerful. If your system handles structured queries with deterministic answers, such as extracting specific fields from a document, the response for a given input does not change between calls. Caching these responses with an appropriate time-to-live eliminates the API call entirely for repeat queries. Systems that process the same documents repeatedly, which is common in contract analysis, compliance checking, and financial reporting, can achieve cache hit rates above 50%.

Every one of these caching strategies is an engineering decision. It requires writing code, designing cache keys, choosing cache durations, monitoring hit rates, invalidating stale entries. Finance cannot implement caching. Finance can only see the bill after the fact. The engineer who implements caching is the one who reduces the bill.

## Why Traditional Cost Controls Fail for AI

Traditional cost controls were designed for a world where infrastructure costs were determined by procurement decisions. You buy servers, you pay for them. You provision cloud instances, you pay for them. The decisions happen at procurement time, and finance controls work because the cost structure is known before the workload runs.

AI costs break this model. The cost structure is determined at runtime, by the workload itself. Two requests to the same endpoint can cost dramatically different amounts depending on prompt length, conversation history, retrieval results, retry behavior, and model routing. A single endpoint can cost $0.005 per request for a short query and $0.50 per request for a complex multi-turn conversation with extensive retrieval. Finance sees the aggregate bill. They cannot see that one endpoint costs one hundred times more per request than another.

Budget alerts, the most common cost control in cloud environments, are reactive. They tell you when spending exceeds a threshold. By the time you receive the alert, the money is spent. In AI systems, where costs accumulate per-request, a budget alert that fires at $50,000 might fire when spending hits $50,000 on Tuesday, but the engineering decisions that caused the overspend were made in code three weeks ago. The alert tells you that you have a problem. It does not tell you which code change caused it, which endpoint is responsible, or which engineering decision to reverse.

Spending limits are another traditional control that fails in AI. You can set a hard cap on API spend, but what happens when you hit the cap? Your system stops serving users. For a production system, a hard spending cap is a self-inflicted outage. No team will accept that. So the cap gets raised, or removed, or set high enough that it never triggers. The control becomes decorative.

The controls that work for AI are engineering controls, built into the system itself. Per-request cost tracking that attributes cost to specific endpoints, specific features, specific customers. Cost budgets per conversation that cap how much a single user session can spend. Model routing rules that automatically downgrade to cheaper models when cost thresholds are approached. Prompt length limits that prevent unbounded context accumulation. Retry budgets that cap the cost of failure handling. Alert systems that detect per-request cost anomalies in real time, not monthly. These controls are code. They live in the repository. They are reviewed in pull requests. They are tested in CI. They are deployed with the application. They are engineering.

## The Organizational Implication

If AI costs are determined by engineering decisions, then cost accountability must live with engineering. This does not mean Finance has no role. Finance still owns budgeting, forecasting, and contract negotiation. But the day-to-day cost management, the decisions that determine whether your system costs $30,000 per month or $90,000 per month, those decisions are engineering decisions and they must be owned by engineers.

This requires a shift in how engineering teams think about their work. Today, most engineers optimize for performance, reliability, and quality. Cost is an afterthought. The pull request review asks: does this change work? Does it pass tests? Does it introduce bugs? It does not ask: does this change increase our monthly API bill by $15,000? Until that question is part of every code review, cost will remain unmanaged.

The shift also requires tooling. Engineers need to see the cost impact of their decisions in real time, not in a monthly report. They need dashboards that show cost per endpoint, cost per feature, cost per customer. They need CI checks that estimate the cost impact of prompt changes. They need staging environments that simulate production cost patterns so they can detect expensive changes before deploying them. These tools do not exist in most organizations because most organizations still think AI cost management is Finance's job.

The teams that get this right build what you might call a **cost-aware engineering culture**. Every engineer understands the cost model. Every pull request considers cost impact. Every architecture decision includes cost as a first-class constraint alongside latency, throughput, and quality. Cost reviews happen at the engineering level, not the finance level. The result is not a team that spends less. It is a team that spends deliberately, where every dollar of AI cost was chosen, not discovered.

## The Gap Between Cost Decisions and Cost Visibility

The fundamental problem is a gap between where cost decisions are made and where cost visibility exists. Decisions are made in code, by engineers, in real time. Visibility exists in invoices, seen by Finance, monthly. The decisions and the visibility are separated by weeks and by organizational boundaries. This gap is where waste lives.

Closing the gap requires moving cost visibility to the point of decision. When an engineer writes a prompt, they should see the estimated cost per request. When an engineer configures a retry policy, they should see the estimated cost under failure scenarios. When an engineer selects a model, they should see the cost comparison against alternatives. When an engineer deploys a change, they should see the projected cost impact based on current traffic patterns.

This is achievable with current technology. Token counting is trivial. Model pricing is public. Traffic patterns are observable. The math is simple multiplication. What is missing is not capability. What is missing is the organizational recognition that cost visibility belongs in the IDE, not in the CFO's monthly report. The next subchapter examines what happens when this gap persists: The Cost Surprise Pattern, where teams discover their true costs only after it is too late to easily fix them.
