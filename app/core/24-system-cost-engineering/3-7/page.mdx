# 3.7 â€” Batch vs Real-Time Inference: The Economics of Asynchronous Processing

Does every request in your system actually need a response in under a second? Most teams answer yes reflexively, then route every single model call through a real-time API endpoint, pay full price for every token, and never question the decision. But when you audit the traffic flowing through a typical AI product, you discover that thirty to fifty percent of model calls have no user waiting on the other side. Nightly report generation. Content moderation backlogs. Data enrichment pipelines. Evaluation runs. Analytics summarization. Weekly digest emails. These workloads do not need millisecond latency. They need correct outputs delivered within hours, and in 2026, processing them asynchronously instead of in real time cuts their inference cost in half.

The distinction between batch and real-time inference is not just an engineering choice. It is a financial strategy. Every model call you move from the real-time path to the batch path immediately reduces its cost by fifty percent with most major providers, and the savings compound as your volume grows. A team processing one million batch-eligible requests per month that switches those requests from real-time to batch saves between $3,000 and $15,000 monthly depending on model tier. Annualized, that is $36,000 to $180,000 in pure savings from a routing change that requires no model swaps, no quality compromises, and no architecture rewrites.

## The Batch Pricing Discount in 2026

**Batch inference** is a processing model where you submit a collection of requests as a single job, the provider processes them on lower-priority compute during off-peak hours, and you receive results within a guaranteed window, typically twenty-four hours. Because the provider can schedule batch jobs to fill idle GPU capacity rather than maintaining always-on endpoints, they pass the efficiency gains back as a pricing discount.

As of early 2026, OpenAI's Batch API offers a flat fifty percent discount on both input and output tokens across all supported models. GPT-5, which costs $15 per million input tokens and $75 per million output tokens at standard real-time pricing, drops to $7.50 input and $37.50 output through the batch endpoint. The smaller models see the same proportional reduction: GPT-5-mini batch pricing is roughly $0.375 per million input tokens and $1.50 per million output tokens. Anthropic offers comparable batch discounts, and Google's Gemini API provides similar asynchronous tiers with 24-hour turnaround windows.

The discount applies identically to the same models, with the same quality, the same context windows, and the same capabilities. You are not getting a worse model. You are trading latency for price. The outputs from a batch request are byte-for-byte identical to what you would get from a real-time request with the same prompt, the same temperature, and the same parameters. The only difference is when you receive the response.

For self-hosted inference, the economics shift but the principle holds. Batch processing on your own GPU infrastructure means higher utilization rates. A real-time endpoint must maintain capacity to handle peak traffic, which means GPUs sit partially idle during off-peak hours. Batch processing lets you fill those idle cycles with queued work, pushing GPU utilization from a typical forty to sixty percent up to eighty-five or ninety percent. Higher utilization means lower cost per request because the fixed cost of the hardware amortizes over more inference jobs. Teams running self-hosted inference typically see a thirty to forty percent reduction in per-request cost for batch workloads compared to real-time, even without an explicit pricing discount.

## Identifying Batch-Eligible Workloads

The first step in capturing batch savings is auditing your system to identify which workloads can tolerate delayed responses. This is not a technical exercise. It is a product exercise. You need to map every model call in your system to a latency requirement, and that requirement comes from the user experience, not from engineering convention.

Start by categorizing every model call into one of three buckets. The first bucket is **user-facing synchronous**, where a person is actively waiting for the response. A chatbot reply, a search result summary, a real-time recommendation. These must stay on the real-time path. Latency matters because a human is staring at a loading spinner.

The second bucket is **user-triggered asynchronous**, where a person initiated the request but does not need the result immediately. A user clicks "generate weekly report" and expects it in their inbox by morning. A user uploads a document for analysis and checks back in an hour. A user requests a bulk export with AI-enriched fields. These are prime batch candidates because the user has already mentally accepted a delay.

The third bucket is **system-initiated background**, where no user is directly involved at all. Nightly content moderation sweeps. Scheduled evaluation runs against your test suite. Data enrichment pipelines that tag incoming records. Analytics summarization that powers dashboards updated daily. Synthetic data generation for training. These are the easiest batch candidates because no human experience is affected by the processing delay.

In a typical AI product, the split looks something like this: forty to fifty percent of model calls are user-facing synchronous, fifteen to twenty-five percent are user-triggered asynchronous, and twenty-five to forty percent are system-initiated background. The second and third buckets together represent thirty-five to sixty percent of total inference volume, and all of it is batch-eligible. At the fifty percent batch discount, moving these workloads to batch reduces total inference spend by seventeen to thirty percent without touching the user-facing experience.

## Calculating Annual Savings from Batch Migration

The savings calculation is straightforward once you know your volume split and your per-request costs. Take a concrete example. Your AI product makes 500,000 model calls per day. Of those, 250,000 are user-facing synchronous, 75,000 are user-triggered asynchronous, and 175,000 are system-initiated background. Your average cost per call on the real-time API is $0.012, combining input and output tokens across your model mix.

At full real-time pricing, your daily inference cost is 500,000 times $0.012, which equals $6,000 per day or $2.19 million per year. Now you move the 250,000 batch-eligible calls to the batch API at fifty percent discount. Those calls now cost $0.006 each instead of $0.012. The remaining 250,000 real-time calls still cost $0.012. Your new daily cost is 250,000 times $0.012 plus 250,000 times $0.006, which equals $3,000 plus $1,500, totaling $4,500 per day or $1.64 million per year. Annual savings: $547,500. That is a twenty-five percent reduction in total inference cost from a routing change.

The calculation gets more compelling at higher volumes and with more expensive models. If your batch-eligible traffic runs through a frontier model at $0.04 per call instead of $0.012, the same fifty percent discount saves $1.825 million annually on the same volume. For companies processing millions of calls per day, the batch discount alone can save seven figures per year.

The calculation also needs to account for the cost of building and maintaining the batch processing infrastructure, but this cost is modest. Most providers' batch APIs require minimal engineering effort: you submit a file of requests, poll for completion, and download results. The incremental engineering cost is measured in days, not months. Even if you build a custom batch orchestration layer with job scheduling, retry logic, and progress tracking, the total engineering investment pays for itself within the first month of batch savings at any meaningful volume.

## The Hybrid Architecture: Real-Time for Users, Batch for Everything Else

The most cost-effective inference architecture in 2026 is not pure real-time or pure batch. It is a hybrid that routes each request to the appropriate processing tier based on its latency requirement. User-facing requests go through the real-time path. Everything else goes through the batch path. This sounds obvious, but most teams do not implement it because they build their inference layer as a single pipeline and never split it.

The hybrid architecture has three components. The first is a **request classifier** that examines each incoming model call and determines whether it is batch-eligible. For API-based calls, this is often as simple as checking the source: calls from your web API handlers are real-time, calls from your cron jobs and background workers are batch. For more complex systems, you might classify based on the calling service, the request metadata, or explicit priority flags set by the application code.

The second component is a **batch queue** that accumulates batch-eligible requests and submits them to the batch API in efficient batches. Instead of sending each background request individually, you accumulate requests over a time window, typically five to sixty minutes, then submit them as a single batch job. This aggregation reduces API overhead and makes it easier to track batch job status, handle failures, and distribute results.

The third component is a **result delivery system** that routes batch results back to the appropriate consumers once processing completes. For nightly report generation, results might go to an email renderer. For data enrichment, results might write back to your database. For evaluation runs, results might feed into your metrics dashboard. The result delivery is specific to each workload, but the pattern is consistent: receive results, validate them, deliver them to the downstream consumer.

The hybrid architecture also enables a powerful optimization: **batch overflow**. During traffic spikes, when your real-time endpoint is under heavy load, you can temporarily reclassify some lower-priority synchronous requests as batch-eligible. Instead of dropping requests or degrading latency, you queue them for batch processing and deliver the result slightly later. The user sees a "processing" state for thirty seconds to a few minutes instead of an instant response, but the system stays stable and you avoid paying surge pricing or provisioning extra real-time capacity.

## Queue-Based Processing: The Middle Ground

Not every workload fits neatly into the real-time or batch categories. Some requests need a response faster than twenty-four hours but do not need it in milliseconds. A customer uploads a ten-page contract for analysis and expects results within fifteen minutes, not tomorrow. A content moderator flags a batch of posts and needs them reviewed within the hour, not within twenty-four hours. For these workloads, **queue-based processing** offers a middle ground.

Queue-based processing sits between real-time and batch. Requests enter a queue and are processed as capacity becomes available, typically within minutes to hours rather than the twenty-four-hour window of batch APIs. You can implement queue-based processing against real-time API endpoints by controlling your concurrency, processing requests sequentially from the queue rather than firing them all simultaneously. This smooths your traffic patterns, avoids rate limit hits, and lets you process at a steady pace without the urgency of synchronous response.

The cost savings from queue-based processing are more modest than full batch pricing, because you are still using the real-time API. But the indirect savings are significant. By smoothing traffic spikes into a steady queue, you avoid the need for over-provisioned infrastructure. You reduce rate limit errors and retries. You can implement priority queues that process high-value requests first and low-value requests when capacity is free. And you can set cost budgets on the queue: if the queue depth exceeds a threshold, you pause processing until the next billing period, preventing runaway costs from unexpected traffic surges.

For self-hosted inference, queue-based processing is even more valuable. Your GPU cluster has a fixed throughput capacity. Without a queue, traffic spikes cause request failures or extreme latency. With a queue, spikes are absorbed and processed at the cluster's natural throughput rate. The trade-off is increased latency for some requests, but for workloads that can tolerate minutes instead of milliseconds, this trade-off is worthwhile. Queue-based processing lets you right-size your GPU cluster for average throughput rather than peak throughput, which can reduce infrastructure costs by thirty to fifty percent.

## Operational Complexity That Batch Processing Adds

Batch processing is not free in terms of engineering effort. Moving workloads from synchronous to asynchronous introduces operational complexity that your team must manage. Ignoring this complexity leads to batch jobs that fail silently, results that arrive corrupted or incomplete, and debugging sessions that take hours because the failure happened in a background process with minimal observability.

The first complexity is **job scheduling and orchestration**. Batch jobs need to run on a schedule or when triggered by upstream events. You need a scheduler that can handle job dependencies, retries, and timeouts. If your nightly report batch depends on the data enrichment batch completing first, you need dependency management. If a batch job fails halfway through, you need the ability to resume from the last successful checkpoint rather than reprocessing the entire batch. Tools like Airflow, Dagster, or Temporal handle this orchestration, but they add infrastructure overhead and require operational expertise.

The second complexity is **failure handling and retries**. In real-time processing, failures are immediately visible: the user gets an error, the application logs the failure, and someone investigates. In batch processing, a failure might go unnoticed for hours. A batch job that fails at three in the morning does not wake anyone up unless you have alerting configured. You need monitoring that tracks batch job completion rates, processing times, and error rates. You need alerting that fires when a batch job fails, takes too long, or produces an unexpected number of results. You need retry logic that handles transient failures without reprocessing successful items.

The third complexity is **result delivery and consistency**. When a batch job completes, results need to reach their consumers reliably. If the batch job processes content moderation decisions but the result delivery pipeline fails, moderation decisions are lost and flagged content remains unmoderated. You need delivery guarantees: at-least-once processing, idempotent writes, and reconciliation checks that verify all expected results were delivered. This is standard distributed systems engineering, but it is engineering you do not need for synchronous processing.

The fourth complexity is **progress tracking and observability**. Stakeholders want to know: is the batch job running? How far along is it? When will it finish? Unlike real-time requests where latency is measured in milliseconds and visible on a dashboard, batch jobs run for hours and their progress is opaque unless you instrument them. You need batch job dashboards that show start time, expected completion time, items processed, items remaining, and error counts. These dashboards are straightforward to build but represent engineering work that does not exist in a purely synchronous architecture.

## When Batch Does Not Make Sense

Batch processing is not always the right answer, even for workloads that can tolerate delay. If your batch-eligible volume is small, less than a few thousand requests per day, the engineering investment in batch infrastructure may exceed the cost savings. At 1,000 requests per day and $0.012 per request, your total daily cost is $12. The batch discount saves $6 per day, or $2,190 per year. If building and maintaining the batch pipeline costs more than $2,190 in annual engineering time, the migration is not worth it.

Batch processing also does not make sense when your workloads require frequent iteration. If you are experimenting with prompts, changing model parameters daily, and testing new approaches, the twenty-four-hour feedback loop of batch processing slows your iteration cycle dramatically. During the experimentation phase, use real-time inference for fast feedback, then switch to batch for the stable, production workloads once your prompts and models are finalized.

Finally, batch processing is risky for workloads where timeliness matters more than cost. A content moderation system that processes posts in twenty-four-hour batches means harmful content stays visible for up to a day. A fraud detection system that processes transactions in batches means fraudulent charges are not caught for hours. For these workloads, the cost of delayed processing far exceeds the cost savings from batch pricing. Keep them on the real-time path and find other ways to optimize their cost, such as model routing, caching, or cascading.

## The Decision Framework

To decide which workloads to move to batch, apply a simple framework. For each workload, answer three questions. First, what is the maximum acceptable latency? If it is under one minute, keep it real-time. If it is one minute to one hour, consider queue-based processing. If it is one hour to twenty-four hours, use the batch API. Second, what is the monthly inference cost of this workload? If it is under $500, the batch discount saves less than $250 per month, and the engineering effort may not be justified. If it is over $2,000, the savings are significant enough to warrant the investment. Third, does this workload have stable, predictable volume? Batch processing works best for predictable workloads with consistent volume. Highly variable workloads with unpredictable spikes are harder to batch efficiently.

The teams that capture the most value from batch processing are the ones that treat it as a first-class infrastructure concern, not an afterthought. They audit their inference traffic quarterly, identify new batch-eligible workloads as the product evolves, and continuously move traffic from real-time to batch as the opportunity arises. Over time, this practice reduces total inference spend by twenty to thirty percent compared to teams that route everything through real-time endpoints.

The batch-versus-real-time decision optimizes the cost of individual model calls. But most modern AI products do not make individual model calls. They chain multiple models together in pipelines, and the cost of those pipelines is the subject of the next subchapter.
