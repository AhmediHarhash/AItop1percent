# 2.10 — Token Budgets: Setting Per-Request and Per-Session Limits That Stick

What is the maximum number of tokens a single request should consume in your system? If you do not have an answer, you do not have a budget. And if you do not have a budget, you do not have cost control — you have cost observation. You watch the bill grow and react after the damage is done. A token budget is the difference between managing your AI costs and being managed by them.

Most engineering teams track total monthly API spend. Some track cost per feature. Very few set explicit limits on how many tokens any individual request, session, or user can consume. The result is predictable: a small number of expensive requests — long conversations, dense context windows, verbose outputs — drive a disproportionate share of costs, and nobody notices until the monthly invoice arrives. **Token budgets** are per-request and per-session caps that prevent runaway consumption before it happens. They are the spending limits on your AI system's credit card.

## The Four Levels of Token Budgets

Token budgets operate at four levels, each controlling a different scope of consumption. Effective cost management requires budgets at all four levels because a gap at any level creates a path for uncontrolled spending.

The first level is the **per-request budget**. This is the maximum total tokens — input plus output — that any single API call can consume. A per-request budget of 8,000 tokens means that the system prompt, user input, retrieved context, and model response must all fit within 8,000 tokens. If the input alone exceeds the budget, the system must truncate context or reject the request. If the model's response approaches the budget, the max_tokens parameter caps the output.

The second level is the **per-session budget**. This is the maximum total tokens consumed across all turns of a single conversation or workflow. A per-session budget of 60,000 tokens means that the cumulative input and output tokens across all requests in that session must stay below 60,000. This controls the compounding conversation cost problem. A user can have a short conversation with large requests or a long conversation with small requests, but the total consumption is bounded.

The third level is the **per-user budget**. This is the maximum tokens a single user can consume within a time period — typically daily or monthly. A per-user budget of 500,000 tokens per day means that no matter how many sessions a user starts or how many requests they make, their total consumption cannot exceed 500,000 tokens in a 24-hour window. This prevents power users from consuming disproportionate resources.

The fourth level is the **per-feature budget**. This is the total token allocation for a product feature across all users. The search feature gets 100 million tokens per day. The recommendation engine gets 50 million. The customer support bot gets 200 million. Per-feature budgets ensure that no single feature can monopolize the overall API budget, and they create accountability for feature teams to optimize their token efficiency.

## Setting the Right Budget: The P95 Plus Headroom Method

The most common question about token budgets is how to set the numbers. Set them too low, and you degrade the user experience by truncating useful context or cutting off responses. Set them too high, and the budgets are meaningless — they never trigger, and costs are uncontrolled.

The **P95 plus headroom method** works reliably. Analyze your current production traffic. For per-request budgets, calculate the 95th percentile of total tokens per request. If 95% of your requests consume 4,200 tokens or fewer, set the per-request budget at 5,000 — the P95 plus roughly 20% headroom. This means only 5% of requests exceed the budget, and the headroom accommodates normal variation without false triggers.

For per-session budgets, calculate the 95th percentile of total tokens per session. If 95% of sessions consume 35,000 tokens or fewer, set the per-session budget at 42,000. For per-user budgets, calculate the 95th percentile of daily user consumption. If 95% of users consume 200,000 tokens or fewer per day, set the per-user budget at 240,000.

The P95 threshold is a starting point, not a religion. For cost-sensitive products with thin margins, you might use P90 — accepting that 10% of requests need intervention. For premium products where user experience is paramount, you might use P99 — ensuring that only 1% of requests are affected. The choice depends on your tolerance for budget enforcement versus your tolerance for cost overruns.

An enterprise SaaS company applied this method to their AI-powered document analysis feature. They analyzed two weeks of production data and found that the P95 for input tokens per request was 6,800 and for output tokens was 1,200. They set the per-request budget at 8,000 input and 1,500 output, with a per-session budget of 80,000 total tokens. In the first month, fewer than 3% of requests triggered budget enforcement. Monthly costs dropped by 18% because the budget eliminated the long-tail expensive requests that had been consuming disproportionate resources.

## Enforcement Mechanisms: Hard Limits, Soft Limits, and Alerts

Once you have set budgets, you need enforcement mechanisms. There are three approaches, each with different trade-offs between cost control and user experience.

**Hard limits** reject or truncate requests that exceed the budget. When a request would consume more input tokens than the budget allows, the system truncates the context — removing older conversation history, reducing retrieved passages, or compressing the system prompt — until the request fits within the budget. When the model's response would exceed the output budget, the max_tokens parameter stops generation. Hard limits guarantee that costs stay within bounds, but they can degrade quality if the truncation removes important context.

**Soft limits** allow the request to proceed but trigger a compensating action. When a session approaches its budget, the system switches to a cheaper model for subsequent turns. When a user approaches their daily budget, the system degrades response quality — using shorter prompts, fewer retrieved passages, or more aggressive summarization — rather than blocking the user entirely. Soft limits preserve the user experience while gradually reducing cost per request.

**Alert-only limits** do not enforce anything. They log the budget violation and notify the engineering team. This is appropriate during the initial rollout of token budgets, when you are learning the distribution and calibrating the thresholds. It is not a long-term strategy because it provides visibility without control. You see the expensive requests after they happen but do not prevent them.

Most production systems use a combination. Alert-only limits at 80% of budget, soft limits at 100%, and hard limits at 120%. When a session reaches 80% of its token budget, the system logs a warning. At 100%, it switches to a cheaper model. At 120%, it stops the session and asks the user to start a new conversation. This layered approach catches most cost overruns with soft intervention and reserves hard limits for extreme cases.

## What Happens When the Budget Is Exceeded

The budget enforcement mechanism determines what happens when a request, session, or user exceeds their allocation. The options form a spectrum from gentle to aggressive, and the right choice depends on the product and the stakes.

**Context truncation** is the gentlest intervention for per-request budgets. The system removes the oldest or least relevant context until the request fits within the budget. The user does not notice unless the removed context was important to the current query. This works well for casual conversations where old context has diminishing value.

**Model downgrade** is the standard intervention for per-session budgets. After a certain number of turns or tokens, the system routes subsequent requests to a cheaper, faster model. The user may notice slightly different response quality, but the conversation continues. Many products already do this — free tiers use budget models while paid tiers use frontier models. Applying the same principle within a session is a natural extension.

**Cached or templated responses** are appropriate when the budget is severely exceeded. Instead of making an API call, the system returns a pre-written response: "I have reached the limit for this conversation. Please start a new conversation to continue." This is a hard stop that prevents any further cost from accruing. It is the equivalent of a circuit breaker.

**Graceful decline with context** is the most user-friendly hard limit. The system tells the user what happened and offers alternatives: "This conversation has used its context allocation. I can continue but may not remember details from earlier in our conversation. For full context, please start a new conversation and summarize the key points." This manages user expectations while controlling costs.

A developer tools company implemented a four-tier enforcement system. At 70% of the per-session budget, the system logged an internal alert. At 85%, it switched from Claude Opus 4.5 to Claude Sonnet 4.5, reducing cost per token by about 40%. At 100%, it activated aggressive context summarization, dropping per-turn input tokens by 50%. At 130%, it returned a cached message suggesting the user start a new session. In practice, 88% of sessions never reached the first threshold. Nine percent triggered the model downgrade. Two percent triggered summarization. Fewer than 1% hit the hard stop. Monthly costs dropped by 22% with minimal impact on user satisfaction scores.

## Per-Feature Budgets and Organizational Accountability

Per-feature budgets create accountability at the team level. Without them, every feature team draws from the same API budget, and nobody owns the cost. With them, each team has a token allocation and responsibility for staying within it.

The allocation process starts with analyzing current consumption by feature. If your product has five AI-powered features, measure the token consumption of each over a representative period. Then allocate budgets proportional to current usage, adjusted for projected growth and strategic priority. A feature that currently consumes 30% of total tokens and is expected to grow 50% in the next quarter gets a budget that accommodates that growth. A feature that is stable gets a budget near its current consumption.

The key organizational benefit is that per-feature budgets force teams to optimize their own token efficiency. When the recommendation team knows they have a budget of 50 million tokens per day, they look for ways to reduce per-request token consumption rather than assuming the overall budget will expand to accommodate their growth. They implement caching, reduce prompt sizes, optimize retrieval, and request only the output they need. Without a feature budget, they have no incentive to do any of this because the cost is invisible to them.

Per-feature budgets also enable trade-off conversations. When Product wants to add a new AI feature, the question becomes: which existing feature gives up tokens to make room, or does the overall budget increase? This forces explicit prioritization rather than the implicit prioritization that happens when every team draws from a shared pool until the monthly invoice shows the damage.

A mid-size technology company implemented per-feature budgets after their monthly API costs grew 300% in six months with no corresponding increase in revenue. They discovered that one feature — an AI-powered search tool — was consuming 55% of total tokens despite generating only 15% of revenue. By assigning the search team a feature budget, they forced the team to optimize their retrieval pipeline, reduce prompt sizes, and implement caching. Within two months, the search feature's token consumption dropped by 40% with no measurable impact on search quality. The freed-up budget was reallocated to a higher-revenue feature.

## The Psychology of Budgets: Why Engineers Resist and Why They Shouldn't

Engineers resist token budgets for the same reason developers resisted performance budgets in the early days of web development. Budgets feel like constraints on creativity. They limit what you can build. They force trade-offs you would rather not make.

This resistance is understandable but misguided. Budgets do not constrain design — they clarify priorities. A token budget forces you to decide what is truly important in your prompt, your context, and your output. Is that 300-token instruction block really necessary, or is it defensive repetition that adds no value? Do you need eight retrieved passages, or would three suffice? Does the model need to explain its reasoning, or does the user just need the answer?

Without a budget, these questions never get asked. With a budget, they get asked on every feature and every pull request. The result is a leaner, more intentional system — one where every token earns its place.

Budgets also create a feedback loop that improves the entire system. When a team hits their budget limit and needs more tokens, they must justify the increase with data: "Adding 200 tokens to the system prompt improves accuracy by 4 percentage points on the eval suite." This is the token marginal value analysis described in the previous subchapter, but driven by organizational pressure rather than voluntary discipline. The budget makes the analysis mandatory.

The most effective way to overcome resistance is to show engineers the data. Pull up the cost distribution. Show that 5% of requests consume 30% of the budget. Show that the system prompt has grown 80% in six months with no corresponding quality improvement. Show the monthly bill broken down by feature. When engineers see the numbers, the budget stops feeling like a constraint and starts feeling like a tool.

## Implementing Budget Infrastructure

Token budget enforcement requires infrastructure. You need a metering layer that counts tokens per request, per session, per user, and per feature. You need a policy engine that compares current consumption against budgets and triggers enforcement actions. You need a logging system that records budget events for analysis and tuning.

The metering layer sits between your application and the API. Before sending a request, it checks the input token count against the per-request budget. After receiving the response, it records the actual input and output tokens consumed and updates the session, user, and feature counters. Most API responses include token usage in the response metadata, making the accounting straightforward.

The policy engine evaluates the current counters against the configured budgets and returns an enforcement decision: proceed, truncate, downgrade, or reject. The policies should be configurable without code changes — stored in a configuration service or database — so that you can adjust budgets without deploying new code.

The logging system records every budget evaluation, every enforcement action, and every budget violation. This data feeds into your cost dashboards and enables you to tune budgets over time. If you see that a particular budget triggers enforcement too often, you may need to raise it. If a budget never triggers, you may be able to lower it.

Several open-source and commercial tools support this infrastructure. AI gateway platforms like Portkey and Kong provide token tracking, rate limiting, and budget enforcement out of the box. If you prefer to build your own, the implementation is modest — a Redis-backed counter system with a thin policy layer handles most cases. The important thing is to have the infrastructure in place before you need it, not scrambling to build it after costs have already spiraled.

## Dynamic Budgets: Adjusting in Real Time

Static budgets — set once and reviewed quarterly — are a good starting point but a poor endpoint. Production traffic patterns change daily. A marketing campaign drives a spike in usage. A new feature launches and changes the consumption distribution. A model migration shifts the cost per token. Static budgets cannot adapt to these changes.

**Dynamic budgets** adjust automatically based on real-time signals. The simplest version is a daily budget that resets at midnight. If the feature budget is 50 million tokens per day and it is 3pm with 40 million consumed, the system knows it is running hot and can throttle or downgrade for the rest of the day. If it is 3pm with 20 million consumed, there is room for higher-quality responses.

A more sophisticated version ties budgets to business metrics. If the daily revenue from AI-powered features exceeds a threshold, the token budget expands to allow higher-quality responses. If revenue falls below a threshold, the budget contracts. This creates a self-regulating system where spending tracks revenue.

The most advanced version uses predictive models. Based on historical patterns, the system predicts how much budget will be needed for the rest of the day and allocates accordingly. If Monday mornings always see a traffic spike, the system pre-allocates more budget for Monday mornings and less for Sunday nights. This smooths the user experience across the day while staying within the overall daily budget.

A large e-commerce platform implemented dynamic per-feature budgets that adjusted hourly based on traffic forecasts. Their AI-powered product search feature experienced ten-times traffic spikes during flash sales. A static budget would either be too tight during sales and degrade the search experience or too loose during normal hours and waste budget. The dynamic system allocated 80% of the daily budget proportionally across expected hourly traffic, with 20% held in reserve for unexpected spikes. During flash sales, the search feature consumed three to four times its normal hourly budget, drawn from the reserve and from lower-priority features that voluntarily shed load. The system kept total daily spend within 5% of the planned budget even during the most extreme traffic events.

## The Relationship Between Budgets and Pricing

Token budgets and product pricing are two sides of the same equation. Your product pricing defines how much revenue each user or feature generates. Your token budgets define how much each user or feature can cost. The gap between revenue and cost is your margin, and token budgets are how you protect that margin.

If you price your product at twenty dollars per user per month, and your per-user token budget translates to fifteen dollars in API costs, you have a five-dollar margin. If a power user consumes tokens that cost thirty dollars, the per-user budget prevents that from happening — or at least ensures you know when it does.

The connection goes deeper. Per-feature budgets help you decide whether a feature is worth building. If a proposed AI feature would require 100 million tokens per day but generate only $2,000 in attributable revenue, and the token cost is $1,800 at current prices, the margin is razor-thin and vulnerable to any usage spike. The per-feature budget makes this visible before launch, not after.

Per-user budgets help you design pricing tiers. If your P50 user costs three dollars per month in tokens, your P90 user costs eight dollars, and your P99 user costs twenty-five dollars, you can price three tiers accordingly. The budget ensures that users in each tier stay within the expected cost range, and any user who consistently exceeds their tier's budget is a candidate for an upgrade conversation, not a silent margin drain.

## Monitoring and Evolving Your Budget Framework

Token budgets are not set-and-forget. They require ongoing monitoring, tuning, and evolution as your product, traffic, and costs change.

Review budgets monthly. Compare actual consumption against budgets at every level. If per-request budgets are triggering enforcement on more than 5% of requests, they may be too tight — or your prompts may need optimization. If per-feature budgets are never triggered, they may be too generous and should be tightened to drive optimization.

Track the **budget utilization rate** — the ratio of actual consumption to budget — at each level. A healthy utilization rate is 60 to 80%. Below 60%, the budget is too generous. Above 80%, the budget is too tight and enforcement actions may be degrading the user experience. Aim for the sweet spot where the budget creates accountability without creating friction.

When you migrate to a new model or adjust pricing tiers, recalibrate your budgets. A model migration that changes the tokens per request — because the new model needs a different prompt or generates differently sized responses — invalidates the old budgets. Recalibrate using the P95 plus headroom method against the new model's traffic patterns.

When you launch new features, assign them budgets before launch, not after. Estimate the expected per-request and daily token consumption during development. Set an initial budget with extra headroom for launch uncertainty. Tighten the budget after two weeks of production data provides a real baseline.

The goal is a living budget framework that evolves with your product. Every feature has a budget. Every team knows their allocation. Every request is metered. Every violation is logged. The monthly bill holds no surprises because you have already accounted for every token before it was consumed.

The next chapter moves from token-level economics to the infrastructure that processes those tokens: model inference cost, including GPU compute, latency trade-offs, and the build-versus-buy decision for inference infrastructure.