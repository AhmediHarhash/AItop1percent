# 9.3 — Unit Economics Calculation: Cost Per Request, Per User, Per Workflow, Per Outcome

Every AI product has a number that determines whether it survives. Not the total monthly bill. Not the annual budget. The number that matters is the cost to produce one unit of value — one request served, one user supported, one workflow completed, one business outcome delivered. This is **unit economics**, and it is the difference between an AI product that scales profitably and one that bleeds money faster as it grows. A team that knows their total AI spend is $95,000 per month has a budget number. A team that knows their cost per resolved support ticket is $1.14 has a business model. The first team can cut costs. The second team can price accurately, forecast reliably, optimize precisely, and prove to their CFO that every dollar of AI spend generates measurable return.

Unit economics for AI products operate at four levels, each more strategically valuable than the last: cost per request, cost per user, cost per workflow, and cost per outcome. Most teams stop at the first level. The teams that build durable businesses calculate all four.

## Level One: Cost Per Request

**Cost per request** is the average total cost to process a single interaction with your AI system. It is the atomic unit of AI economics — the smallest meaningful cost number you can calculate. Every other unit economics metric is built from it.

Calculating cost per request requires summing every cost component that a request touches. For a typical RAG-based application, a single request might involve embedding the user query at a cost of roughly $0.00002 using a model like text-embedding-3-large, querying a vector database at a cost of $0.00001 to $0.0001 depending on the provider and index size, calling a language model for generation at a cost ranging from $0.003 for a small model like GPT-5-mini up to $0.05 for a frontier model like Claude Opus 4.6 with a long context, and allocating a share of infrastructure costs for the compute, networking, and storage that support the pipeline. The generation cost dominates in almost every case — typically eighty to ninety-five percent of the total per-request cost. But the other components matter at scale. An embedding cost of $0.00002 per request is invisible for one request. At ten million requests per month, it is $200 — still modest, but no longer zero.

The practical way to calculate cost per request is to sample, not to model. Take a representative sample of 10,000 requests from your production traffic. For each request, sum every cost event from your attribution system — the model call costs, the retrieval costs, the tool call costs, and the allocated infrastructure costs. Compute the mean, the median, the 90th percentile, and the 99th percentile. The distribution matters as much as the average. If your mean cost per request is $0.018 but your 99th percentile is $0.42, you have a long tail of expensive requests that will dominate your bill and your margin calculations. That long tail — the requests with unusually long inputs, multiple retries, expensive tool calls, or fallbacks to frontier models — is where optimization effort has the highest return.

Typical cost per request ranges in 2026 span from $0.002 for simple classification or routing tasks using small models to $0.15 or more for complex multi-step workflows using frontier models with long contexts. A customer support chatbot using Claude Sonnet 4.5 with moderate context lengths averages around $0.008 to $0.015. A document analysis pipeline using GPT-5 with retrieval and multiple extraction steps averages $0.04 to $0.08. A code generation assistant using a frontier model with large context windows averages $0.03 to $0.12. These ranges matter because they set the floor for what you can charge. If your cost per request is $0.05 and you cannot price above $0.08 per interaction without losing customers, your maximum gross margin on variable costs is thirty-seven percent before accounting for any fixed costs. That is dangerously thin for a software business.

## The Hidden Multiplier: Requests Per Visible Interaction

One trap that catches teams calculating cost per request is conflating visible interactions with actual model calls. When a user asks a question in your chat interface, that single visible interaction might generate three, five, or eight model calls behind the scenes. A RAG pipeline might call the embedding model, then call the generation model, then run a quality check with a smaller model, then format the response with another call. An agent workflow might plan, execute a tool, reflect on the result, and generate a final answer — four model calls for one user-visible turn. A retry-on-low-confidence pattern doubles the calls for every request that falls below the threshold.

The **fan-out ratio** — the number of actual billable calls per user-visible interaction — is one of the most commonly underestimated cost multipliers. When a team says "our cost per request is $0.01," they often mean the cost of a single model call. But the user doesn't experience a single model call. The user experiences a response to their question, and producing that response might require $0.01 in generation, $0.003 in embedding, $0.001 in retrieval, and $0.005 in quality scoring — a true cost of $0.019 per visible interaction, nearly double the number they quoted.

Measure the fan-out ratio for every feature. Count the average number of billable actions per user-visible interaction, across all services in the pipeline. Multiply the per-call cost by the fan-out ratio to get the true cost per interaction. This is the number that connects to user experience, to pricing, and to the unit economics that determine profitability.

## Level Two: Cost Per User

**Cost per user** is the monthly cost to serve a single active user. It is the metric that connects AI cost to per-user pricing models and the metric that reveals which users are profitable and which are not.

The basic calculation is straightforward: take the total AI cost for a given period, divide by the number of active users in that period. If your AI system costs $95,000 in January and you have 12,000 active users, your average cost per user is $7.92 per month. If you charge $29 per user per month, your AI cost represents twenty-seven percent of revenue — leaving seventy-three percent to cover all other costs and contribute to margin.

But the average hides enormous variance. User-level cost distributions in AI products are almost never normal. They are heavy-tailed, meaning a small number of users consume a disproportionate share of resources. In a customer support platform, some users ask two questions a month and cost $0.16. Others use the AI assistant as their primary workflow tool, asking forty to sixty questions per day, and cost $28 per month. The first user is wildly profitable at $29 per seat. The second user is barely breaking even. If your product attracts more heavy users over time — which it will if the product is good — your average cost per user climbs even as your total user count grows.

To calculate cost per user with enough precision for pricing decisions, you need the full distribution, not just the average. Pull per-user cost data from your attribution system for a representative period — at least 30 days, preferably 90 to smooth out seasonal variation. Compute the median, the 75th percentile, the 90th percentile, the 95th percentile, and the 99th percentile. The median tells you what a typical user costs. The 95th percentile tells you what a heavy user costs. The 99th percentile tells you what your most expensive users cost. These percentiles are the inputs to pricing strategy. If your median user costs $4 and your 95th percentile user costs $22, a flat $29 per seat works for most users but leaves almost no margin on the heaviest. You might need usage caps, tiered pricing, or overage charges to make the economics work across the entire distribution.

## Level Three: Cost Per Workflow

**Cost per workflow** is the end-to-end cost of a complete business process — not a single request, but the full sequence of AI interactions required to achieve a defined outcome. This is the metric that matters for B2B products, outcome-based pricing, and any use case where the value delivered is measured in completed processes rather than individual queries.

A document processing workflow illustrates the concept. Ingesting a legal contract for analysis involves several steps: optical character recognition or text extraction at approximately $0.002 per page, document chunking and embedding generation at approximately $0.001 per chunk across an average of 40 chunks per document, classification of the contract type using a small model at approximately $0.003 per classification, extraction of key terms and clauses using a frontier model at approximately $0.04 per extraction call with an average of three calls per document, cross-reference checking against a database of standard terms at approximately $0.01 per check, and summary generation at approximately $0.02. The total cost per document workflow is approximately $0.195. None of the individual steps looks expensive. The workflow total is what matters for pricing.

The calculation for cost per workflow requires workflow-level instrumentation. Your attribution system must tag every cost event with a workflow identifier so that events across multiple services and multiple model calls can be grouped into a single workflow instance. If your attribution system only tracks per-request costs without workflow grouping, you need to add a workflow identifier to the cost context and propagate it through every step.

Once you have workflow-level cost data, the analysis follows the same pattern as per-request and per-user: compute the mean, the distribution, and the percentiles. Workflow costs tend to have even heavier tails than request costs because the number of steps — and therefore the cost multiplier — varies with the complexity of the input. A simple two-page contract might cost $0.08 to process. A complex 90-page agreement with cross-references and amendments might cost $2.40. If your pricing assumes an average of $0.20 per document, you need to know how many documents fall in the long tail.

Workflow costs are also where **cascading cost amplifiers** become visible. A workflow that includes a quality check after the extraction step, and re-runs extraction if the quality score is below threshold, has a variable step count. If twenty percent of documents trigger a re-extraction, the average cost per workflow increases by roughly twenty percent of the extraction cost. If the re-extraction also fails and the workflow escalates to a human reviewer, the cost jumps by the cost of human review time — which is often an order of magnitude higher than the AI cost. Mapping these branching paths and their probabilities is essential for accurate workflow cost modeling.

## Level Four: Cost Per Outcome

**Cost per outcome** is the most strategically important unit economics metric and the one fewest teams calculate. It answers the question that connects AI cost to business value: how much does it cost to produce one successful result?

The distinction between cost per workflow and cost per outcome is the success rate. Not every workflow produces a successful outcome. A customer support AI resolves some tickets autonomously and escalates others to human agents. A document analysis pipeline extracts accurate data from most contracts and produces errors on some that require human correction. A lead qualification model correctly scores most leads and misqualifies some that sales teams waste time pursuing. Cost per outcome accounts for these failures by dividing the total cost of all attempts — successful and failed — by the number of successful outcomes.

Consider a customer support application. The AI handles 10,000 incoming tickets per month. It resolves 6,200 autonomously at an average cost of $0.34 per ticket, including multiple conversation turns, retrieval, and tool calls. The remaining 3,800 tickets are escalated to human agents, but the AI still incurred cost on those tickets before escalation — an average of $0.21 per escalated ticket in model calls that didn't lead to resolution. The total AI cost is 6,200 times $0.34 plus 3,800 times $0.21, which equals $2,108 plus $798, totaling $2,906 per month. The number of successful AI-resolved outcomes is 6,200. The cost per successful resolution is $2,906 divided by 6,200, which equals $0.47 per resolved ticket. Not $0.34 — that is the cost per attempt. The true cost per outcome is $0.47 because it includes the cost of the attempts that failed.

This distinction matters enormously for pricing and for evaluating ROI. If a human agent costs $8 to resolve the same ticket, the AI's cost per outcome of $0.47 represents a 94 percent savings. But if you quoted the cost per attempt of $0.34 and neglected the failed attempts, you would overstate the savings and understate the true cost of AI resolution. Worse, if the resolution rate drops — say from 62 percent to 45 percent because the system is deployed on more complex ticket types — the cost per outcome rises to $0.68 even though the cost per attempt hasn't changed. The attempt cost is stable. The outcome cost is a function of both the attempt cost and the success rate.

## Calculating Cost Per Outcome: The Full Formula

The cost per outcome calculation requires three inputs: the total AI cost for all attempts in a period, the number of successful outcomes, and a clear definition of what constitutes "success."

The total AI cost includes every cost event associated with the workflow, for all instances — both those that produced a successful outcome and those that did not. This is the number from your cost attribution system, aggregated at the workflow level.

The number of successful outcomes comes from your product metrics, not your cost system. It is the count of workflows that achieved the defined result: tickets resolved, documents processed without errors, classifications confirmed as correct, reports generated and accepted by the user. This requires your product to track outcomes — which many products don't do systematically — and to link those outcomes back to the workflow instances in your cost system.

The definition of success is the hardest part and the most important. A resolved ticket might mean the user confirmed the answer was helpful. Or it might mean the user didn't reopen the ticket within 48 hours. Or it might mean a human reviewer validated the resolution. Each definition produces a different success rate and therefore a different cost per outcome. Choose the definition that matches how your business measures value, document it explicitly, and apply it consistently.

Once you have these inputs, the formula is: cost per outcome equals total AI cost divided by successful outcomes. You can also express it as: cost per outcome equals cost per attempt divided by the success rate. This second form makes the relationship between attempt economics and outcome economics explicit. When your success rate is 80 percent, your cost per outcome is 1.25 times your cost per attempt. When your success rate is 50 percent, it is double. When your success rate drops to 30 percent, the cost per outcome is 3.3 times the cost per attempt — and at that point you need to either improve the success rate or acknowledge that the use case is not economically viable with current model capabilities.

## Why Cost Per Outcome Is the Metric That Matters Most

Cost per request tells you what inference costs. Cost per user tells you whether your pricing works. Cost per workflow tells you what a process costs. But cost per outcome is the metric that answers the question your CEO, your investors, and your customers actually care about: is this AI system creating value or destroying it?

The reason cost per outcome is the most important metric is that it is directly comparable to the alternative. Every AI-automated outcome has a counterfactual cost — the cost of producing the same outcome without AI. For customer support, the counterfactual is the fully loaded cost of a human agent resolving the ticket. For document analysis, it is the cost of a paralegal reviewing the contract. For content generation, it is the cost of a copywriter producing the same output. When your AI cost per outcome is lower than the counterfactual cost — and the quality is acceptable — the AI system creates value equal to the difference. When the AI cost per outcome exceeds the counterfactual, the system destroys value regardless of how impressive the technology is.

An insurance company calculated cost per outcome for their claims triage system. The AI classified incoming claims by severity, urgency, and routing destination. The cost per attempt was $0.12 — a quick classification using a medium-tier model. The accuracy rate was 89 percent, meaning 89 percent of claims were correctly triaged on the first pass. The remaining 11 percent required human review and re-triage. The cost per successful triage outcome, including the failed attempts, was $0.14. A human adjuster performing the same triage cost $4.20 per claim in fully loaded labor cost, taking into account salary, benefits, training, and management overhead. The AI system delivered a cost per outcome that was 97 percent cheaper than the human alternative. That number — $0.14 versus $4.20 — was the number that justified the entire AI program, secured the next year's budget, and determined the pricing for their claims-processing product.

But cost per outcome also reveals when AI is not the answer. A recruiting company built an AI resume screening system. The cost per screening attempt was low — $0.08 per resume using a mid-tier model. But the accuracy rate was 61 percent as measured against human recruiter decisions. The cost per accurate screening outcome was $0.13. A human recruiter screened resumes at a cost of $0.85 per resume. The AI was cheaper by 85 percent. But the 39 percent error rate meant that for every 100 resumes, 39 were incorrectly scored — either qualified candidates rejected or unqualified candidates advanced. The cost of those errors — measured in lost candidates and wasted interview time — far exceeded the cost savings. The cost per outcome looked favorable only if you ignored the cost of the errors. When the error costs were included, the AI system was more expensive than human screening. Cost per outcome, properly calculated, revealed the truth that cost per attempt obscured.

## Building the Unit Economics Dashboard

Unit economics are not useful as a quarterly report. They are useful as a daily operational metric — a number your team checks the way an SRE checks latency and error rates.

Build a unit economics dashboard that shows all four levels, updated at least daily. At the top, show cost per request: the mean, the median, and the 95th percentile for the current week, compared to the previous week and the previous month. Below that, show cost per user: the distribution across all active users, the mean, and the 95th percentile, compared to the previous period. Below that, show cost per workflow for each major workflow in your product, with trend lines. At the bottom, show cost per outcome for each workflow that has a defined success metric, with the success rate displayed alongside the cost.

The dashboard should be sliceable by time period, by feature, by tenant, and by model. The ability to slice is what turns a dashboard from a reporting tool into an investigation tool. When cost per outcome for your document analysis workflow spikes from $0.19 to $0.31 in a single week, you need to slice the data to find out why. Did the success rate drop? Did the cost per attempt increase? Did a specific tenant start sending unusually large documents? The dashboard should let you answer these questions in minutes, not days.

Track trends over time. Unit economics that are stable week-over-week indicate a mature, predictable cost structure. Unit economics that are drifting upward indicate cost creep — often from prompt expansion, model upgrades, or changing user behavior that is slowly increasing per-request costs. Unit economics that are volatile indicate systemic instability — perhaps retry rates fluctuating, or traffic patterns shifting unpredictably. Each pattern requires a different response. Stability requires maintenance. Drift requires investigation and correction. Volatility requires architectural attention.

## The Quarterly Unit Economics Review

Beyond the daily dashboard, run a formal unit economics review every quarter. This review brings together engineering, product, and finance to answer five questions.

First, what are our unit economics at each level, and how have they changed since last quarter? If cost per outcome has improved, understand why — was it a model cost reduction, an architectural optimization, or an improvement in success rate? If it has worsened, understand why with equal rigor.

Second, how do our unit economics compare to our pricing? If cost per resolved ticket is $0.47 and you charge $0.99 per resolution, your gross margin on that workflow is 52 percent. Is that sustainable? Does it leave enough room for fixed costs, for margin targets, for the infrastructure investments you need to make next quarter?

Third, which workflows have the best and worst unit economics, and what drives the difference? The workflows with the best economics are candidates for expansion and marketing emphasis. The workflows with the worst economics are candidates for optimization, repricing, or retirement.

Fourth, what would happen to our unit economics if traffic doubled? Would cost per request stay constant, or do you have fixed-cost components that would be spread across more requests, improving economics? Would success rates hold, or do they degrade at higher volume due to latency, rate limiting, or model capacity?

Fifth, what are the specific optimization opportunities that would improve unit economics by five percent or more? Rank them by expected impact and implementation effort. This is the prioritization exercise that turns unit economics data into an engineering roadmap.

## From Unit Economics to Pricing

The ultimate purpose of unit economics is to set prices that are both competitive and sustainable. Cost per outcome is the floor below which you cannot price without losing money. The gap between cost per outcome and the customer's willingness to pay is your margin opportunity. The goal of cost engineering is to push the cost floor down as far as possible while maintaining quality, maximizing the margin you can capture.

When Intercom launched their AI customer support agent Fin, they priced it at $0.99 per resolved conversation — a per-outcome pricing model directly tied to cost per outcome economics. This pricing worked because the cost per resolved conversation was well below $0.99, because $0.99 was dramatically cheaper than the $5 to $15 a human agent costs per conversation, and because the per-outcome model aligned the price with the value delivered. The customer pays only for results. The provider profits only when the AI succeeds. This alignment is only possible when you know your cost per outcome with enough precision to price above it with confidence.

For your own product, cost per outcome at the 95th percentile — not the mean — should be the floor for pricing. The 95th percentile accounts for the expensive edge cases that the mean hides. If your mean cost per outcome is $0.14 but your 95th percentile is $0.89, pricing at $0.50 means you lose money on every outcome in the top five percent of the cost distribution. Price above the 95th percentile, cap the maximum cost per interaction through architectural controls, or implement tiered pricing that charges more for complex workflows.

## What Comes Next

Unit economics tell you the cost to produce value. But cost alone does not determine whether your product is viable — margin does. The next subchapter covers gross margin modeling for AI products: how to calculate contribution margin per feature, how AI-first gross margins compare to traditional SaaS, and how to model the relationship between cost per outcome and long-term business sustainability.
