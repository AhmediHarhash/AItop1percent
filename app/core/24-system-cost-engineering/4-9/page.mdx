# 4.9 — The Prompt Cost Audit: A Repeatable Process for Finding Token Waste

Every prompt has waste. The only question is whether you have looked for it.

Teams build prompts incrementally. A system prompt starts at 200 tokens during prototyping. Someone adds a safety instruction. Someone adds an output format specification. Someone adds three few-shot examples because they helped with a tricky edge case in testing. Someone adds a retrieval context block. Someone adds a "do not do X" instruction after a production incident. Six months later, the system prompt is 1,400 tokens, nobody remembers why half of it is there, and every one of those tokens is charged on every single request. Most of those tokens are doing useful work. Some of them are not. But nobody knows which is which, because nobody has tested.

The **Prompt Cost Audit** is a repeatable process designed to answer exactly that question. It is not a one-time cleanup. It is a recurring practice, run quarterly or after any major prompt change, that systematically identifies every token in your pipeline, measures the value each token delivers, and removes the ones that cost money without adding quality. Teams that run this process consistently find 20% to 40% token waste on their first audit, and 5% to 15% on subsequent quarterly runs. The waste accumulates because prompts grow but rarely shrink. The audit is the mechanism that makes them shrink.

## Step 1: Measure Current State

The audit begins with data. You cannot find waste until you know what you are spending. Pull a representative sample of 10,000 production requests. Not the easiest 10,000. Not the most recent 10,000. A stratified sample that reflects your actual traffic distribution — simple queries, complex queries, short inputs, long inputs, different user segments, different features.

For each request in the sample, log the complete token breakdown. Categorize every token into one of six buckets: system prompt tokens, user input tokens, retrieved context tokens, few-shot example tokens, output tokens, and reasoning tokens if you are using a chain-of-thought or reasoning model. Some requests may not have all categories — a pipeline without RAG has no retrieved context tokens, for example. That is fine. Log what exists.

Aggregate the data. Calculate the mean, median, 90th percentile, and 95th percentile for each token category. The mean tells you the average cost. The 90th and 95th percentile tell you where the expensive outliers live. A pipeline where the mean input is 1,200 tokens but the 95th percentile is 4,800 tokens has a heavy tail problem — 5% of requests are consuming four times the average, and those requests are disproportionately expensive.

Build a simple breakdown that shows each category as a percentage of total tokens. A typical audit might reveal: system prompt 35%, user input 8%, retrieved context 40%, few-shot examples 12%, output 5%. That breakdown immediately highlights where the largest optimization opportunities lie. If retrieved context is 40% of your total tokens, improving retrieval precision will have a bigger cost impact than compressing your system prompt. If few-shot examples are 12%, testing whether those examples are worth their cost is a high-priority experiment.

## Step 2: Identify Fixed vs. Variable

Not all tokens behave the same way across requests. Some tokens are identical in every request — the system prompt, few-shot examples, standing instructions. These are **fixed tokens**. Other tokens change with every request — user input, retrieved context, output. These are **variable tokens**.

The distinction matters because fixed tokens offer the most straightforward optimization opportunities. If you can remove 200 fixed tokens from your system prompt, you save those 200 tokens on every single request. At 100,000 requests per month, removing 200 fixed tokens saves 20 million tokens per month. At $3.00 per million input tokens, that is $60 per month. At $15.00 per million input tokens, that is $300 per month. The savings are guaranteed because fixed tokens appear in every request.

Variable tokens require a different optimization approach. You cannot simply remove them because they change per request. Instead, you optimize the mechanism that produces them — the retrieval system, the conversation history manager, the output schema. Variable token optimization was covered in the preceding subchapters on retrieval precision and structured output efficiency. This audit step separates the two categories so you can apply the right optimization strategy to each.

Create a ledger. On one side, list every fixed token component: system prompt paragraph one, system prompt paragraph two, few-shot example one, few-shot example two, format instructions, safety instructions, and so on. On the other side, list every variable token component: user query, retrieved chunk one through N, conversation history, output. You will test each fixed component individually in the next steps. You will assess each variable component by analyzing its distribution from Step 1.

## Step 3: Test Fixed Token Value

This is where the audit finds its savings. For every section of your system prompt and every fixed component, you run a controlled experiment: remove the section, run your evaluation suite, and measure the quality impact.

Start with the largest fixed token sections first — the biggest potential savings. If your system prompt has a 300-token section of behavioral instructions, remove it entirely and run your eval suite. If quality drops by 0.2% on your primary metric, that section is contributing almost no measurable value. Three hundred tokens times 100,000 requests per month is 30 million tokens. Is 0.2% quality worth $90 per month on a $3 model or $450 per month on a $15 model? For most teams, it is not. Remove the section.

If quality drops by 5% when you remove a section, that section is earning its tokens. Keep it. But test whether you can compress it. Can you convey the same instruction in 200 tokens instead of 300? Rewrite it more concisely, run the eval suite again, and see if the compressed version maintains the same quality. Often, a section that was written in a hurry during development contains redundant sentences, unnecessary qualifiers, or verbose phrasing that can be tightened without losing effectiveness.

Work through every fixed section this way. Remove it. Measure the impact. If impact is negligible, cut it. If impact is significant, try compressing it. Document the result for each section: kept as-is, compressed from X tokens to Y tokens, or removed entirely with zero quality impact. The documentation serves two purposes. First, it justifies the changes to stakeholders who are nervous about modifying a production prompt. Second, it creates a baseline for the next quarterly audit.

A common finding during first audits: instructions that were added to address a specific production incident but that the model now handles correctly without the instruction. Models improve between versions. An instruction that was necessary to prevent a failure mode on Claude Sonnet 3.5 might be unnecessary on Claude Sonnet 4.5. If you never test, you never discover that the instruction is redundant. You just keep paying for it.

## Step 4: Test Context Value

For variable context — primarily retrieved chunks in RAG pipelines — the audit tests the cost-quality tradeoff at different retrieval depths. Run your eval suite at top-K values of two, three, five, eight, and your current production setting. Measure answer quality at each level. Plot the curve.

In most pipelines, the curve has a knee. Quality improves sharply from top-K of one to three, improves modestly from three to five, and plateaus or improves negligibly from five to eight. If your production setting is eight but the knee is at three, you are paying for five extra chunks per request that contribute almost no quality. The audit quantifies exactly how much those extra chunks cost and how little they contribute.

This step often reveals that the optimal top-K differs by query type. Simple factual queries hit the quality ceiling at two chunks. Multi-faceted analytical queries need five or six. If your system treats all queries identically with a fixed top-K, the audit data provides the evidence for implementing adaptive top-K, which was covered in detail in the previous subchapter on retrieval precision.

Document the quality-at-K curve. This is one of the most valuable artifacts the audit produces. It tells you the exact dollar cost of every additional chunk of context and the exact quality benefit. Decisions about retrieval depth stop being guesses and start being calculations.

## Step 5: Test Example Value

Few-shot examples are among the most expensive fixed tokens because they include both input and output demonstrations. A single few-shot example might consume 150 to 400 tokens. Three examples consume 450 to 1,200 tokens. Those tokens appear in every request, making few-shot examples a significant fixed cost.

The audit tests each example individually. Remove example one, keep examples two and three, run the eval suite. Then remove example two, keep one and three. Then remove example three, keep one and two. Then remove all examples and run the eval suite with zero-shot prompting.

The results often surprise teams. In many cases, the model performs identically with one example as it does with three. The second and third examples were added during development as insurance, and they remain in the prompt because nobody tested whether they are still needed. Models in 2026 are significantly better at following instructions from natural language alone than models from 2024 were. Few-shot examples that were essential for GPT-4o might be redundant for GPT-5 or Claude Sonnet 4.5.

When an example is contributing measurably to quality, test whether a shorter example achieves the same effect. Few-shot examples tend to be verbose because they were copied from real data during development. A 300-token example might contain 100 tokens of context that the model does not need to understand the pattern. Trim the example to its essential elements — input, output, and the minimum context needed to illustrate the pattern — and test whether the trimmed version performs equivalently.

A B2B software company audited the few-shot examples in their contract analysis pipeline and found that their three examples consumed 920 tokens combined. Testing showed that one example — the second one — accounted for the entire quality benefit. The other two were redundant. Removing them saved 610 tokens per request. At 350,000 requests per month on Claude Opus 4.5, that was 213 million tokens saved, or $1,067 per month. They further trimmed the remaining example from 310 tokens to 195 tokens without quality impact, saving another $287 per month. Total few-shot savings: $1,354 per month from two hours of testing.

## Step 6: Measure Output Efficiency

The previous subchapter covered structured output schema design in detail. During the audit, you apply those principles diagnostically. Sample 1,000 production outputs and analyze them. How many output tokens are structural overhead versus data values? Are there fields that are null or empty in more than 80% of responses? Are field values longer than they need to be?

Compare the output the model generates to the output the downstream consumer actually uses. If your output has 12 fields and the next system in the pipeline reads only five of them, seven fields are wasted output. If the model generates 50-word explanations but the UI truncates them to 20 words, the extra 30 words are wasted. If the model returns confidence scores to six decimal places but your logic only checks whether confidence is above or below 0.8, five decimal places are waste.

This step surfaces a specific category of waste that accumulates invisibly: fields and formatting that were useful during development and testing but are not needed in production. Debug fields, verbose explanations, intermediate reasoning traces, redundant metadata — all of these appear in production output because nobody removed them after the development phase ended. The audit is the moment to ask: does every token the model generates have a consumer? If not, stop generating it.

## Step 7: Calculate Savings

At this point, the audit has identified specific removable token sources and the quality impact of each removal. Step 7 translates those findings into dollars.

For each optimization identified, calculate the per-request token savings, multiply by monthly request volume, and multiply by the per-token cost for your model. Sum the savings across all optimizations to get the total annual savings.

The calculation is straightforward. Suppose the audit found: 280 removable system prompt tokens with zero quality impact, 450 removable few-shot example tokens with zero quality impact, an optimal retrieval depth of three instead of seven that reduces average retrieved context by 1,200 tokens with less than 1% quality impact, and a schema optimization that reduces average output by 95 tokens with zero quality impact. The total per-request savings is 2,025 tokens.

At 400,000 requests per month, that is 810 million tokens per month. But the savings are not all at the same price. The input token savings — system prompt, few-shot, and retrieval context totaling 1,930 tokens per request — cost differently than the output token savings of 95 tokens per request. On Claude Sonnet 4.5, the input savings are 772 million tokens per month at $3.00 per million, or $2,316 per month. The output savings are 38 million tokens per month at $15.00 per million, or $570 per month. Total monthly savings: $2,886, or $34,632 per year.

Present these numbers in a one-page summary for engineering leadership. Show the current monthly cost, the identified waste, the proposed savings, the quality impact of each change, and the implementation effort. This summary is the business case for the optimization work. Without it, prompt optimization competes with feature development for engineering time and usually loses. With it, the conversation shifts from "should we optimize our prompts" to "when can we start."

## Step 8: Implement and Validate

Implementation follows a strict sequence. Do not change everything at once. Deploy one optimization at a time, run your eval suite after each change, and validate that quality holds. If you batch all changes and quality drops, you cannot identify which change caused the regression.

Start with the changes that have the highest savings and zero quality impact. These are the safe wins — removing instructions the model no longer needs, cutting redundant few-shot examples, eliminating empty output fields. Deploy each one, confirm quality on your eval suite, and confirm in production monitoring that key metrics hold for 48 to 72 hours before moving to the next change.

Then move to changes with small quality trade-offs — reducing retrieval depth, compressing instructions. These require more careful monitoring. Watch your quality metrics closely for the first week after each change. Set alerts for any quality metric that drops below your threshold. Have a rollback plan for each change: if quality drops below threshold, revert the specific change within one hour.

Keep a running log of every audit cycle: what was tested, what was changed, what was the token impact, what was the quality impact. This log becomes the institutional memory for prompt optimization. When a new engineer joins the team and asks "why is the system prompt only 600 tokens," the log explains what was tested and why each remaining token earned its place.

## A Complete Worked Example

A mid-market e-commerce company ran their first Prompt Cost Audit in January 2026 on their product recommendation pipeline. The pipeline received 420,000 requests per month, running on GPT-5 at $1.25 per million input tokens and $10.00 per million output tokens.

Step 1 revealed the following token breakdown for an average request: system prompt 680 tokens, user context 120 tokens, retrieved product descriptions 1,400 tokens at top-K of seven, three few-shot examples at 480 tokens total, and output averaging 210 tokens. Total average input: 2,680 tokens. Total average output: 210 tokens.

Step 2 classified the fixed tokens: system prompt at 680 and few-shot examples at 480, totaling 1,160 fixed tokens per request. Variable tokens: user context at 120, retrieved context at 1,400, and output at 210.

Step 3 tested each system prompt section. The prompt had six sections: role definition at 80 tokens, behavioral constraints at 190 tokens, output format at 120 tokens, product knowledge rules at 160 tokens, safety instructions at 70 tokens, and a legacy section about a discontinued product line at 60 tokens. Removing the legacy section had zero quality impact — it was a leftover from eight months ago. Removing behavioral constraints dropped quality by 0.3%. Compressing behavioral constraints from 190 to 110 tokens maintained the same quality. The format instructions were already tight. Product knowledge rules compressed from 160 to 120 tokens without quality loss. Total system prompt savings: 150 tokens, from 680 down to 530.

Step 4 tested retrieval depth. The quality-at-K curve showed strong improvement from K equals one to K equals three, marginal improvement from three to four, and no measurable improvement from four to seven. They reduced top-K from seven to four. Average retrieved context dropped from 1,400 tokens to 800 tokens. Savings: 600 tokens per request.

Step 5 tested few-shot examples. The three examples consumed 480 tokens total: 140, 170, and 170 tokens respectively. Removing the third example had zero quality impact. Removing the second also had zero quality impact. One example at 140 tokens performed identically to three at 480. Savings: 340 tokens per request.

Step 6 analyzed output. The recommendation output included a field for internal category codes that no downstream system consumed. Removing it saved 25 tokens per response. Product description summaries averaged 45 tokens but the UI displayed only the first 20 words (approximately 25 tokens). Constraining summary length saved another 20 tokens. Output savings: 45 tokens per response.

Step 7 calculated the numbers. Input token savings: 150 plus 600 plus 340 equals 1,090 tokens per request. At 420,000 requests per month, that is 457.8 million input tokens saved. At $1.25 per million, the input savings are $572 per month. Output token savings: 45 tokens per request. At 420,000 requests per month, that is 18.9 million output tokens saved. At $10.00 per million, the output savings are $189 per month. Total monthly savings: $761. Annual savings: $9,132. Quality impact on the eval suite: less than 0.5% on all metrics.

The team noted that $9,132 per year sounded modest. Then they applied the same audit to their customer support pipeline, which ran on Claude Sonnet 4.5 at much higher token prices, and found $47,000 per year in savings. The total across their four main pipelines was $56,000 per year. The audit itself took one engineer one week. The implementation and validation took another week. Two weeks of engineering time for $56,000 in annual savings is a return that justifies running the audit every quarter.

## Making It Recurring

The Prompt Cost Audit is not a one-time project. It is a recurring practice because prompts grow continuously. Every production incident that leads to a new instruction adds tokens. Every model upgrade changes which instructions are necessary. Every feature change modifies the context pipeline. Without a recurring audit, the waste creeps back.

Schedule the audit quarterly. Assign ownership to a specific engineer or a rotating role. Include it in the team's quarterly planning as an expected deliverable. Track cumulative savings across audit cycles so the practice earns its place on the roadmap based on demonstrated ROI, not just good intentions.

Over four quarterly cycles, a typical team's savings curve looks like this: the first audit finds 25% to 35% waste and saves the most. The second audit finds 8% to 15% and catches the drift that accumulated since the first. The third and fourth audits find 5% to 10% each, mostly from model upgrades that made certain instructions redundant and from new features that added tokens nobody budgeted. The savings in later cycles are smaller, but the cost of the audit is also smaller because the process is established and the team knows what to test.

The real value of the recurring audit is cultural. When the team knows that every token will be scrutinized quarterly, they write tighter prompts from the start. Engineers think about token cost when adding a new instruction or a new few-shot example. Product managers ask "how many tokens does this feature add" during planning. The audit changes behavior even when it is not running. That cultural shift — from prompts as free-form text to prompts as cost-engineered artifacts — is worth more than any single audit cycle's savings.

The next chapter moves from per-request optimization to a cost strategy that eliminates entire requests from your bill: caching and deduplication economics.
