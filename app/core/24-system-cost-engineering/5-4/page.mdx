# 5.4 — Prompt Caching and KV-Cache Reuse: Provider-Level Compute Savings

Every time you send a request to a large language model, the model performs the same expensive computation on your system prompt that it performed on the last request, and the request before that, and the ten thousand requests before that. Your system prompt does not change between requests. The model's attention mechanism processes it identically every time. **Prompt caching** — also called KV-cache reuse — is the provider-level optimization that eliminates this redundancy. It is not something you build. It is something the provider offers, and the economics are so favorable that ignoring it is equivalent to paying full price for electricity while your building has free solar panels on the roof.

The mechanism is straightforward but worth understanding precisely, because the details determine whether you save 50% or 90% on your input token costs. When a transformer model processes a prompt, each attention layer computes a set of key and value tensors — the **KV cache** — for every token in the input. These tensors are what the model "remembers" about each token when generating the response. For a 2,000-token system prompt followed by a 500-token user query, the model computes KV tensors for all 2,500 input tokens. The next request has the same 2,000-token system prompt but a different 500-token user query. Without prompt caching, the model recomputes KV tensors for all 2,500 tokens from scratch. With prompt caching, the model reuses the KV tensors for the 2,000 identical system prompt tokens and only computes new tensors for the 500 changed tokens. The compute savings are proportional to the fraction of your input that stays the same across requests.

## How Provider-Level Prompt Caching Works

The providers handle the caching infrastructure. You do not manage a cache server, set TTLs, or monitor hit rates for this optimization. When you send a request, the provider checks whether the prefix of your prompt — the tokens at the beginning that match a previously seen request — has KV tensors already computed and stored in GPU memory. If it does, the provider skips the computation for those tokens and charges you a reduced rate. If it does not, the provider computes the full KV cache, stores it, and charges you the standard rate.

The critical detail is that caching works on the prefix only. The provider matches your prompt from the first token forward and caches the longest matching prefix. This means the order of your prompt content determines what gets cached. If your system prompt comes first and your user query comes last, the system prompt tokens are cacheable because they form the prefix that stays constant across requests. If you put the user query first and the system prompt second, nothing gets cached because the prefix changes with every request. Prompt structure is not just a clarity concern. It is a cost engineering decision.

The first request with a given prefix computes the KV cache from scratch and pays full input token pricing, sometimes with a small surcharge for cache creation. Subsequent requests that share the same prefix reuse the cached KV tensors and pay a dramatically reduced rate on those cached tokens. The cache lives in GPU memory on the provider's infrastructure and persists for a limited window — typically five to ten minutes of inactivity, though some providers offer extended retention.

## Provider-Specific Mechanics: Anthropic

Anthropic's implementation gives you explicit control over what gets cached. You mark specific content blocks in your API request with cache control directives, telling the system exactly which portions of your prompt to cache. The first time the provider sees that content, it creates the KV cache and charges a cache write premium — approximately 1.25 times the base input token price for a five-minute cache, or 2.0 times the base price for a one-hour cache. Every subsequent request that reuses that cached prefix pays only 0.1 times the base input token price on the cached tokens. That is a 90% discount on cached input tokens.

The math deserves a concrete example. Claude Sonnet 4.5 charges $3.00 per million input tokens at the base rate. Cache write tokens cost $3.75 per million for five-minute caching. Cache read tokens cost $0.30 per million. If your system prompt is 2,000 tokens and you send 100,000 requests per day, the first request in every five-minute window pays the write premium, and every subsequent request in that window pays the read discount. With steady traffic of roughly 70 requests per minute, you get perhaps 288 cache writes per day — one every five minutes — and 99,712 cache reads. The daily cost for just the system prompt tokens works out to approximately $2.16 for cache writes and $59.83 for cache reads, totaling roughly $62 per day. Without prompt caching, those same 200 million system prompt tokens would cost $600 per day at the base rate. The savings: $538 per day, or roughly $16,000 per month, from a feature that requires rearranging your prompt structure and adding a cache control directive.

For Claude Opus 4.6 at $5.00 per million input tokens, the same scenario yields even larger absolute savings. Cache reads at $0.50 per million mean your 200 million daily system prompt tokens cost roughly $100 instead of $1,000. That is $900 per day or $27,000 per month saved — purely from the provider reusing computations it already performed.

As of early 2026, Anthropic uses workspace-level isolation for prompt caching. This means that cached KV tensors are shared across all requests within the same workspace but not across different workspaces. If you run separate workspaces for development and production, each workspace maintains its own cache. This matters for testing: your development environment's cache behavior will not match production if the traffic patterns differ significantly.

## Provider-Specific Mechanics: OpenAI

OpenAI's approach is automatic and requires no code changes. The system detects when the prefix of your prompt matches a recently seen prefix and applies the cache discount transparently. There are no cache control directives to set, no cache creation fees, and no explicit cache management. The discount is a flat 50% off input token pricing for cached tokens.

The minimum prefix length for caching is 1,024 tokens, with cache granularity increasing in 128-token increments beyond that. If your system prompt is 800 tokens, it falls below the minimum and gets no caching benefit. If your system prompt is 1,200 tokens, the first 1,024 tokens are eligible for caching, then additional 128-token chunks up to the end of the matching prefix. This minimum length requirement means short system prompts miss the optimization entirely. You need a substantial static prefix to benefit.

Cached prefixes persist for five to ten minutes by default. For GPT-5.1 and GPT-4.1 series models, OpenAI offers extended cache retention of up to 24 hours, where KV tensors are offloaded from GPU VRAM to GPU-local storage when idle and loaded back when a matching prefix arrives. Extended retention is particularly valuable for applications with bursty traffic patterns — if your system sees heavy traffic during business hours and light traffic overnight, standard five-minute caching loses the cache overnight and must rebuild it every morning. Twenty-four-hour retention keeps the cache warm across the quiet period.

The 50% discount is less aggressive than Anthropic's 90% discount on reads, but OpenAI charges nothing extra for cache creation. The total economics depend on your traffic pattern. If you have steady, high-frequency traffic where cache writes are a tiny fraction of total requests, Anthropic's 90% read discount dominates. If you have bursty, lower-frequency traffic where cache writes are a larger fraction, OpenAI's zero-write-premium approach may come out ahead.

## Provider-Specific Mechanics: Google

Google's Gemini models offer both implicit and explicit context caching. Implicit caching, enabled by default since mid-2025 for Gemini 2.5 and newer models, works similarly to OpenAI's automatic approach — the provider detects matching prefixes and applies the discount without code changes. Explicit caching lets you declare specific content for caching with control over retention duration.

Cached token reads cost 10% of the base input price for Gemini models, matching Anthropic's read discount. However, explicit caching adds storage costs: $1.00 to $4.50 per million tokens per hour, depending on the model. This storage cost is unique to Google's explicit caching and creates a breakeven calculation that the other providers do not require. If you cache a 50,000-token context and store it for one hour at $4.50 per million tokens per hour, the storage cost is $0.225 per hour. You need enough cache hits within that hour for the per-read savings to exceed the storage cost. For Gemini 3 Pro at $2.00 per million input tokens, each cache hit on 50,000 tokens saves $0.09 versus the base rate. You need at least three cache hits per hour to justify the storage cost. That is a low bar for any production system, but it is a bar that does not exist with Anthropic's or OpenAI's approach.

## Prompt Architecture for Maximum Cache Savings

The design of your prompt determines what percentage of your input tokens benefit from caching. The principle is simple: put everything that stays constant across requests at the beginning, and put everything that changes at the end. In practice, this means restructuring your prompts into layers ordered by stability.

The first layer — the absolute beginning of your prompt — should be your core system instructions. These change only when you update your product. They might run 500 to 2,000 tokens and include role definition, output format requirements, behavioral constraints, and domain-specific rules. This layer changes perhaps once a week or once a month. It is cached almost permanently in practice because every request starts with it.

The second layer should be relatively stable context that changes less frequently than per-request. If you have few-shot examples, put them here. If you have a company knowledge base snippet that applies to all requests in a category, put it here. This layer might change once per session or once per user segment, but it is stable enough to benefit from caching across multiple requests.

The third layer — the end of your prompt — is where variable content goes. The user's specific query. Retrieved context from RAG. Session history that changes with every turn. These tokens are never cached because they differ per request. The goal is to minimize this layer's size relative to the total prompt, or at least to accept that its tokens always pay full price.

A team that structures their prompt with a 2,000-token system prompt, 1,000 tokens of few-shot examples, and 500 tokens of user query gets 86% of their input tokens cached on every request after the first. A team that intersperses user context throughout their prompt, mixing dynamic content with static instructions, might get only 30% of tokens cached because the prefix breaks early. Same total tokens. Same model. Same quality. Nearly three times the cost difference on input tokens. Prompt architecture is cost architecture.

## The Economics at Scale

The savings from prompt caching compound with three variables: the size of your static prefix, your daily request volume, and your model's base input token price. The relationship is multiplicative. A system with a large system prompt, high traffic, and an expensive model saves enormously. A system with a short prompt, low traffic, and a cheap model saves almost nothing.

Consider four scenarios for a system running 200,000 requests per day with Anthropic's 90% cache read discount. With a 500-token system prompt, the daily cacheable tokens total 100 million, and the savings versus base rate are roughly $270 per day or $8,100 per month on Claude Sonnet 4.5. With a 2,000-token system prompt, cacheable tokens total 400 million, and savings rise to roughly $1,080 per day or $32,400 per month. With a 5,000-token system prompt that includes few-shot examples, cacheable tokens total one billion, and savings reach approximately $2,700 per day or $81,000 per month. With a 10,000-token system prompt typical of complex enterprise applications, cacheable tokens total two billion, and savings approach $5,400 per day or $162,000 per month.

These are input token savings alone. Output tokens are unaffected by prompt caching. For applications where output tokens dominate cost, prompt caching matters less in absolute dollars. For applications where input tokens dominate — summarization of long documents, analysis of detailed context, retrieval-augmented generation with large retrieved passages — prompt caching can be the single largest cost lever available.

The latency benefit is substantial as well. Reusing KV cache tensors eliminates the compute time for the cached prefix. Providers report latency reductions of 50% to 85% on the prefill phase of inference. For a request with a 5,000-token cached prefix, this can shave 500 milliseconds to two seconds off response time depending on the model and hardware. Faster responses improve user experience and reduce the probability of timeout-triggered retries, which themselves have cost implications.

## Limitations and Failure Modes

Prompt caching is not free and it is not magic. Several failure modes catch teams off guard.

Cache invalidation on prefix change is the most common problem. If you modify even a single token in your system prompt, the entire cache is invalidated. Every request after the change pays full input pricing until the new prefix is cached. Teams that deploy system prompt updates multiple times per day — perhaps as part of rapid experimentation — can negate most of the caching benefit. Each update resets the clock. If your cache TTL is five minutes and you update your prompt every thirty minutes, you get five minutes of cache benefit followed by twenty-five minutes of no benefit. The optimization only works when your prefix is stable.

Minimum prefix length requirements mean short prompts get no benefit. If your system prompt is 200 tokens, it falls below OpenAI's 1,024-token minimum. Anthropic requires at least 1,024 tokens for the cacheable prefix as well, with some models requiring 2,048 tokens. A lightweight chatbot with a 300-token system prompt and minimal instructions cannot use prompt caching at all. Teams sometimes artificially pad their system prompts to reach the minimum, but this adds tokens that increase base cost — the savings from caching must exceed the cost of the padding tokens, which only works if request volume is high enough.

Cache TTL means the cache evaporates during quiet periods. If your application sees zero traffic between midnight and 6 AM, the cache expires during that window. The first morning requests pay full price. For applications with strong diurnal patterns, the cache rebuilds every day. The cost of cache rebuilding is usually small relative to total daily savings, but it is not zero, and it means your effective daily savings are slightly less than the theoretical maximum.

Multi-turn conversations complicate caching because the conversation history grows with each turn. The first turn has a clean static prefix: system prompt followed by user query. The second turn has system prompt plus first exchange plus new query. The cached prefix from the first turn covers the system prompt, but the conversation history changes with every turn. The cacheable fraction of total input tokens decreases with each turn as variable conversation context grows relative to the static system prompt.

## When to Prioritize Prompt Caching Optimization

Prompt caching should be one of the first cost optimizations you implement because it requires minimal engineering effort and zero quality risk. You are not changing the model's behavior. You are not reducing tokens. You are not substituting a cheaper model. The model receives exactly the same input and produces exactly the same output. The only difference is that the provider skips redundant computation and charges you less for it. There is no quality-cost trade-off with prompt caching. It is purely upside.

The priority ranking is straightforward. If you are spending more than $5,000 per month on input tokens and your system prompt exceeds 1,024 tokens, restructure your prompts for maximum prefix caching today. If you are using Anthropic's API and have not added cache control directives, you are leaving money on the table with every request. If you are using OpenAI's API, verify that your prompts are structured prefix-first by checking your billing dashboard for cached token counts. If the cached token percentage is low, your prompt structure is the problem.

The one situation where prompt caching does not matter much is when output tokens dominate your cost. If your application generates long responses — multi-page documents, detailed analyses, lengthy code — output tokens might represent 80% or more of your token spend. Saving 90% on input tokens when input is only 20% of the bill saves 18% overall. Still worthwhile, but not transformative. For these applications, output length control and model routing are higher-impact levers.

The next subchapter shifts from provider-level optimization to the question that determines every caching strategy's real-world value: your cache hit rate and the precise dollar amount each percentage point is worth.
