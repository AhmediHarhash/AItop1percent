# 10.7 — Cost Approval Workflows: Who Authorizes Expensive Changes

In March 2025, a backend engineer at a legal tech company updated a single configuration line in the document analysis pipeline. The change swapped the model from Claude Haiku 4.5 to Claude Opus 4.5 for what the engineer described in the commit message as "better quality on complex contracts." The change passed code review. It passed the test suite. It deployed to production on a Tuesday afternoon. Nobody noticed anything unusual until the finance team flagged the AI infrastructure invoice three weeks later.

Monthly inference spend had jumped from $41,000 to $108,000 — a $67,000 increase from a one-line configuration change. The model swap increased per-request cost by roughly 5x on input tokens and 5x on output tokens, and the pipeline processed 2.3 million requests that month. The engineer had not run a cost estimate before deploying. The reviewer had not checked the cost implications. No system in the deployment pipeline flagged the change as cost-impacting. There was no policy requiring approval for model tier upgrades. The change was technically correct — Opus did produce better analysis — but nobody had authorized the $67,000 monthly increase, and the product's unit economics could not absorb it.

This is not a story about a reckless engineer. The engineer made a reasonable technical decision. The problem was that the organization treated cost-impacting changes the same as any other code change: review for correctness, test for functionality, deploy. Nobody asked the question that matters most in cost engineering: what does this change cost?

## The Principle Behind Cost Approval

**Cost approval workflows** are processes that require explicit authorization before cost-impacting changes reach production. They are the cost engineering equivalent of code review. Just as code review exists because even experienced engineers introduce bugs, cost approval exists because even thoughtful engineers underestimate cost impact. The principle is identical: a second pair of eyes catches what the author missed. The difference is that code review catches functional errors, while cost approval catches financial errors.

The analogy to code review is useful because it reframes cost approval from bureaucracy to hygiene. No serious engineering organization deploys untested code. The question is not whether you need testing — it is what level of testing a change requires. Trivial changes get a quick review. Major architectural changes get a thorough design review. Cost approval works the same way. Trivial cost impacts need minimal oversight. Major cost impacts need explicit authorization from someone who owns the budget.

The reason cost approval workflows matter more in AI systems than in traditional software is that AI cost surfaces are uniquely volatile. In traditional software, a code change rarely alters infrastructure costs by more than a few percent. The cost of serving a web page does not change 5x because you refactored a database query. But in AI systems, swapping a model tier can change per-request cost by 3x to 10x. Expanding a system prompt by 2,000 tokens can increase input cost across every request. Enabling a tool integration that calls an external API adds per-request cost that compounds at volume. Disabling a cache layer removes savings that were silently subsidizing the budget.

Each of these changes is small in code — often a single line — but massive in cost.

## Which Changes Need Approval

Not every change needs cost approval. The overhead of reviewing every configuration tweak would slow engineering to a crawl. The key is identifying the changes that create meaningful cost risk and routing only those through the approval process.

**Model tier upgrades** are the highest-impact changes. Moving from a small model to a large model, from a fast model to a frontier model, or from a standard tier to a premium tier can multiply per-request cost by 3x to 20x. Any change that alters which model handles production traffic needs approval. This includes direct swaps, routing configuration changes that shift traffic percentages between model tiers, and changes to fallback logic that determine which model handles overflow traffic.

**System prompt expansions beyond a token threshold** create per-request cost increases that compound across every request. A system prompt that grows from 800 tokens to 2,400 tokens increases the input cost of every single request by the cost of 1,600 additional tokens. On a product handling 100,000 requests per day, that prompt expansion adds roughly $480 per day at typical mid-tier pricing, or $14,400 per month. Set a token threshold — 500 additional tokens is a reasonable starting point — and require approval for any system prompt change that exceeds it.

**New tool integrations that add per-request cost** are easy to miss because the cost does not appear in the model invoice. An agent that calls a search API, a code execution sandbox, or an external data provider adds cost for every invocation. If the tool fires on 40 percent of requests and costs $0.003 per call, that is $0.0012 added to every request on average. On 200,000 daily requests, that tool integration costs $240 per day, or $7,200 per month. New tool integrations require approval with a clear cost estimate.

**Disabling or degrading caching** removes cost savings that the budget depends on. If your cache saves 30 percent of your inference bill, turning it off or changing TTL settings that reduce the hit rate creates an immediate cost spike. Changes to caching configuration need the same approval as changes to the model tier because the financial impact is comparable.

**Increasing retry limits** is the change nobody thinks about. If your system retries failed requests, increasing the retry count from two to five means that worst-case request cost goes from 3x normal to 6x normal. During a provider outage that triggers high retry rates, the cost difference between two retries and five retries can be the difference between a $5,000 day and a $15,000 day.

**Expanding context windows** deserves its own line because the cost is non-obvious. Many teams increase the maximum context window to handle longer documents or conversations without realizing that input token costs scale linearly with context length. Doubling the context window doubles the input cost of every request that uses the expanded window. If 20 percent of your requests hit the maximum context length, and you double that maximum, you have increased the cost of those requests by 2x.

## The Tiered Approval Framework

The approval process should be proportional to the cost risk. A single threshold creates either too much friction on small changes or too little oversight on large ones. A tiered framework solves this by matching the approval requirement to the estimated cost impact.

**Tier one: self-approved.** Changes estimated to increase monthly cost by less than $500 can be approved by the engineer who makes them. The engineer must document the cost estimate in the pull request or change ticket — not just the technical rationale — but no external approval is needed. This tier covers minor prompt adjustments, small configuration tweaks, and experimental changes on low-traffic endpoints.

The documentation requirement is not optional. Even self-approved changes need a cost estimate on record so that the cost engineering owner can spot patterns during weekly reviews. A team that ships five self-approved changes in a month, each adding $400, has accumulated $2,000 in monthly cost that no single change triggered review for. The weekly cost review catches this pattern. The documentation enables it.

**Tier two: team lead approval.** Changes estimated to increase monthly cost between $500 and $5,000 require sign-off from the team lead or the engineer designated as the team's cost owner. The team lead verifies the cost estimate, confirms the change is justified by a product or quality requirement, and ensures the increase fits within the team's budget allocation. This tier covers most model routing changes, moderate prompt expansions, and new tool integrations on medium-traffic endpoints.

**Tier three: director or cost engineering owner approval.** Changes estimated to increase monthly cost above $5,000 require approval from the engineering director, the cost engineering owner, or whoever holds budget authority for AI infrastructure. At this level, the review includes not just the cost estimate but the business justification: what quality improvement or capability does this change deliver, and does the value justify the cost? This tier covers model tier upgrades on high-traffic endpoints, major architecture changes that alter the cost profile, and any change that would push total monthly spend above a budgeted threshold.

The specific dollar thresholds are not sacred. A startup spending $10,000 per month on inference might set the tiers at $100, $1,000, and $5,000. An enterprise spending $500,000 per month might set them at $2,000, $20,000, and $50,000. The principle is that the thresholds match the organization's risk tolerance and budget structure. What matters is that the thresholds exist at all.

## Estimating Cost Impact Before Deployment

An approval workflow is only as good as the cost estimates feeding it. An engineer who submits a change with "I think this might cost a bit more" is not providing a cost estimate. An engineer who submits a change with "this increases per-request cost from $0.0084 to $0.028, and at 150,000 daily requests, monthly cost increases by approximately $8,820" is providing a cost estimate. The second version enables a real approval decision. The first enables nothing.

The estimation process follows a consistent pattern. First, calculate the current per-request cost of the component being changed. If you have per-request cost tracking from your observability pipeline, this number is already available. If not, calculate it from the current model pricing, average token counts, and any tool or API costs per request.

Second, model the cost of the proposed change. If the change is a model tier upgrade, use the new model's pricing against the same average token counts. If the change is a prompt expansion, add the new tokens to the average input count and recalculate. If the change is a new tool integration, estimate the tool's per-call cost and the percentage of requests that will trigger it.

Third, multiply the per-request cost difference by daily request volume and then by 30 to get the monthly cost impact.

This calculation takes five minutes when per-request cost tracking is in place. It takes thirty minutes when the engineer has to calculate current costs from scratch. This is one of the strongest arguments for per-request cost instrumentation as described earlier in this section: it makes cost estimation fast enough that engineers actually do it.

For changes that affect token counts — prompt modifications, context window changes, output limit adjustments — the estimation should include a range rather than a single point. A system prompt expansion adds a fixed number of tokens per request, so the estimate is precise. But a change that increases maximum output length affects cost only for requests that approach the new limit. Estimate the lower bound assuming current output length distributions hold, and the upper bound assuming 20 percent of requests expand to the new limit.

## Integrating Cost Approval Into the Deployment Pipeline

Manual approval processes work at small scale but break as team velocity increases. The sustainable approach is to embed cost estimation into the CI/CD pipeline so that cost-impacting changes are automatically flagged before they reach production.

The implementation is straightforward. The deployment pipeline includes a cost estimation step that runs after tests pass but before deployment. This step compares the incoming configuration — model tier, prompt token counts, tool integrations, retry settings, cache configuration — against the current production configuration. If any parameter has changed, the step calculates the estimated cost impact using current request volumes and pricing data. If the estimated impact exceeds the self-approval threshold, the pipeline blocks deployment and creates an approval request.

Tools like Infracost, originally built for infrastructure-as-code cost estimation, have inspired similar approaches for AI cost estimation. The principle is the same: every change that alters cost gets an automated cost impact assessment before it reaches production. Some teams build this as a custom pre-deployment check that reads model configuration files, looks up pricing from a maintained table, and calculates impact against the past week's request volume. Others integrate it into their observability platform, using historical per-request cost data as the baseline for impact calculation.

The automation handles the easy cases — clearly small changes that self-approve, clearly large changes that escalate. The gray zone in the middle still benefits from human judgment: is this $3,000 increase justified by the quality improvement it enables? That judgment call is what the approval workflow exists to support.

One practical consideration: the cost estimation step must account for staged rollouts. If the change will deploy to 5 percent of traffic initially, the immediate cost impact is 5 percent of the full estimate. But the approval should be based on the full rollout estimate, because that is the cost the organization is committing to if the staged rollout succeeds. Approving a change based on its canary cost and then discovering the full rollout cost is unacceptable defeats the purpose of the workflow.

## How the Pipeline Check Actually Works

The cost estimation check is not a vague "think about cost" reminder. It is a concrete comparison between the current production configuration and the proposed change.

The check maintains a cost configuration file that lists every cost-relevant parameter: the model identifier for each pipeline, the system prompt token count, the maximum output token limit, the list of enabled tools with their per-call costs, the retry limit, and the cache configuration. When a deployment changes any of these parameters, the check fires. It reads the current production values, reads the proposed values, calculates the per-request cost delta, and multiplies by the trailing seven-day average request volume to project the monthly cost impact.

The output is a cost impact report that appears in the pull request or deployment ticket. The report shows three numbers: the estimated monthly cost increase, the estimated monthly cost decrease if the change saves money, and the net monthly impact. It also shows the tier that the change falls into — self-approved, team lead, or director — so the deploying engineer knows immediately whether they need external approval.

If the change is self-approved, the check passes and deployment proceeds. If the change requires team lead or director approval, the check blocks deployment and tags the appropriate approver. The approver reviews the cost impact report and either approves, requests changes, or rejects. Only after approval does the deployment unblock.

The entire process adds less than two minutes to the deployment pipeline for self-approved changes and typically four to eight hours for changes that need external approval, assuming the SLAs described below are met.

## Making the Culture Work

The biggest risk with cost approval workflows is not the process design — it is the cultural adoption. Engineers who see cost approval as bureaucracy will route around it. They will label changes as "no cost impact" without checking. They will break large changes into smaller ones to stay below the self-approval threshold. They will argue that the process slows down velocity, and they will have a point if the process is poorly designed.

The cultural fix starts with framing. Cost approval is not a gate designed to slow engineers down. It is a safety net designed to prevent the kind of surprise that opened this subchapter — a $67,000 monthly increase from a one-line change that nobody costed. Frame the workflow as protection for the team, not oversight of the team. The engineer who submits a cost estimate and gets approval has organizational cover if the cost turns out higher than expected. The engineer who deploys without a cost estimate has no cover at all.

Speed matters too. An approval workflow that takes three days to get sign-off will be circumvented. Set SLAs on approval response times: tier two approvals resolved within four business hours, tier three within one business day. Appoint backup approvers so that a single person's vacation does not block deployments. Use async approval through Slack, Teams, or whatever messaging tool the team already uses — not formal ticketing systems that add unnecessary ceremony.

Transparency reinforces culture. Publish the monthly cost approval summary to the engineering team. Show how many changes were submitted, how many were approved, how many were modified after cost review, and how much total cost was avoided by the review process. When the team sees that the workflow caught a $22,000 monthly increase that the submitting engineer had estimated at $3,000, the value of the process becomes concrete. When they see that 85 percent of submissions were self-approved and cleared in minutes, they see that the process does not slow down routine work.

## The Anti-Patterns That Undermine the Workflow

Three anti-patterns destroy cost approval workflows from the inside.

**The rubber stamp** happens when approvers approve every request without meaningful review. The approval becomes a checkbox, not a decision. This usually happens when the approver does not have cost context — they do not know the current budget, the current spend trajectory, or the cumulative impact of recent approvals. The fix is to give approvers a dashboard view that shows current budget utilization, recent approved changes and their cumulative impact, and the trajectory if this change is approved. An approver who sees "budget is at 87 percent utilization and this change adds another 6 percent" makes a different decision than an approver who sees only the isolated cost estimate.

**The bottleneck** happens when a single approver becomes the gateway for all tier two and tier three changes. When that person is in meetings, on vacation, or simply overwhelmed, deployments queue up and engineers start circumventing the process. The fix is redundancy: at least two approvers for each tier, with clear escalation to the backup if the primary has not responded within the SLA window. Some teams rotate the approver role weekly to distribute the load and build cost awareness across the team.

**The scope creep** happens when the approval process gradually expands to cover changes that do not have meaningful cost impact. Someone adds a rule that all prompt changes require approval, regardless of token count impact. Someone adds a rule that model version updates within the same tier require approval, even though the cost is identical. Each rule addition seems reasonable in isolation, but collectively they turn the process into a bottleneck that covers everything and therefore catches nothing, because approvers stop reading the requests carefully when 90 percent of them are trivial.

Keep the approval scope tight. Only changes that affect cost above the self-approval threshold require external review. Everything else is documented and reviewed in aggregate during the weekly cost review. This separation — individual approval for high-impact changes, aggregate review for low-impact changes — keeps the process both rigorous and fast.

## The Edge Cases: Emergency Changes and Rollbacks

Every approval workflow needs an emergency bypass. When production is degraded and the fix requires a model tier upgrade that exceeds the approval threshold, the on-call engineer cannot wait for a director to review a cost estimate. The emergency bypass allows any engineer to deploy a cost-impacting change during an active incident, with the requirement that the change is reviewed and approved retroactively within 24 hours. If the retroactive review determines that the cost is not justified as a permanent change, the configuration reverts to pre-incident settings.

The emergency bypass must be logged and visible. Every use of the bypass creates an audit trail that includes who triggered it, what incident justified it, what change was made, and what the cost impact was. The cost engineering owner reviews all emergency bypass uses during the weekly review. If the bypass is being used more than once or twice per quarter, either the system is too fragile — requiring frequent emergency changes — or the process is too slow — pushing non-emergency changes through the bypass because the normal approval path is a bottleneck.

Rollbacks are the other edge case. If a previously approved change turns out to cost more than estimated — because the estimate was wrong, because traffic patterns shifted, or because the change had second-order effects on caching or retry behavior — the cost engineering owner should have the authority to roll back the change without requiring a new approval cycle. The principle is that approved changes can be undone unilaterally but new changes require approval. This asymmetry keeps the process from creating a situation where an expensive mistake persists because rolling it back requires the same approval as making it.

## What Good Looks Like

A mature cost approval workflow has five characteristics. First, every engineer on the team knows which changes require cost approval and at what threshold. Second, the deployment pipeline automatically flags cost-impacting changes and calculates estimated impact. Third, approval response times are fast enough that the workflow does not bottleneck deployment velocity. Fourth, monthly summaries show the team how much cost was avoided through the review process. Fifth, the emergency bypass exists, is documented, and has been used at least once without anyone being punished for using it.

The organizations that get this right report two consistent outcomes. Their surprise cost spikes drop to near zero, because every cost-impacting change is estimated before it deploys. And their engineers become more cost-aware over time, because the act of estimating cost impact for every significant change builds a habit of cost thinking that eventually becomes automatic.

The approval workflow does not just prevent bad changes. It teaches the team to think about cost the same way they think about correctness, performance, and security — as a first-class engineering concern that deserves attention before code reaches production.

The cost approval workflow governs how changes are authorized within your own systems. But some of the largest cost decisions are not about configuration changes — they are about which models you run in the first place. The next subchapter examines the full total cost of ownership comparison between open-source and proprietary models, where the calculation is more nuanced than most teams expect.
