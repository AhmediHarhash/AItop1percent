# 1.4 â€” Variable, Semi-Variable, Fixed, and Risk-Adjusted Cost Categories

Every dollar in your AI system falls into one of four categories, and each category behaves differently when traffic changes. If you treat them all the same, your cost model will be wrong, your forecasts will miss, and your unit economics will look healthy in development and catastrophic in production. The four categories are **variable costs** that scale with every request, **semi-variable costs** that scale in steps, **fixed costs** that stay constant regardless of volume, and **risk-adjusted costs** that are probabilistic and hit you hard when they materialize. Mastering these categories is what separates teams that can forecast their AI spend from teams that are surprised by it.

The distinction matters because cost optimization strategies differ by category. You reduce variable costs by reducing per-request consumption. You reduce semi-variable costs by operating efficiently within each tier. You reduce fixed costs by negotiating better commitments or sharing resources. You manage risk-adjusted costs by lowering probability, capping impact, or building insurance mechanisms. A team that optimizes variable costs while ignoring risk-adjusted costs will still get destroyed by a single retry storm. A team that negotiates fixed costs down while ignoring semi-variable step functions will overspend every time they cross a tier boundary. The categories are the lens through which every cost decision becomes clear.

## Variable Costs: The Per-Request Tax

Variable costs are the most intuitive category. Every time a user sends a request, you pay. The cost scales linearly with volume. Double the requests, double the cost. There are no discounts, no tiers, no step functions. One request costs what one request costs, and ten thousand requests cost ten thousand times that amount.

The primary variable cost in most AI systems is token consumption. When you send a prompt to GPT-5 and receive a response, you pay for every input token and every output token. As of early 2026, GPT-5 charges approximately $1.25 per million input tokens and $10.00 per million output tokens. Claude Opus 4.5 charges approximately $5.00 per million input tokens and $25.00 per million output tokens. Claude Sonnet 4.5 charges approximately $3.00 per million input tokens and $15.00 per million output tokens. These prices change frequently, but the structure is consistent: you pay per token, and the cost scales linearly.

For a mid-size SaaS product handling 100,000 requests per day, the math is straightforward. If your average request uses 800 input tokens and generates 400 output tokens, and you are using GPT-5, your daily token cost is 80 million input tokens and 40 million output tokens. That is $100 per day in input costs and $400 per day in output costs, totaling $500 per day or $15,000 per month in pure token spend. Switch to Claude Opus 4.5 for the same workload and the numbers change: $400 per day in input and $1,000 per day in output, totaling $1,400 per day or $42,000 per month. The model selection alone creates a 2.8 times difference in variable cost.

Embedding generation is another variable cost. Every time you process a document for retrieval, you generate embeddings. Every time you process a user query for retrieval, you generate embeddings. Embedding costs are lower per token than generation costs, but they accumulate because retrieval systems process documents in bulk during ingestion and process every query at runtime. A system ingesting 10,000 new documents per day at 2,000 tokens each processes 20 million tokens per day in embeddings alone. At $0.10 per million tokens for a model like OpenAI's text-embedding-3-large, that is $2.00 per day, a negligible cost. But if you are re-embedding your entire corpus weekly for freshness, and your corpus is 5 million documents, you are processing 10 billion tokens per week in embeddings, which becomes $1,000 per week.

API call overhead is a subtler variable cost. Some providers charge per-call fees in addition to per-token fees. Even when the per-call fee is zero, there are infrastructure costs per request: load balancer processing, logging, metrics collection, tracing. These costs are tiny per request but material at scale. At 100,000 requests per day, even a $0.001 per-request overhead adds $100 per day or $3,000 per month.

The key principle with variable costs is that they are the most directly controllable category. Every optimization that reduces tokens, reduces calls, or reduces the model tier for a given request directly reduces variable costs. Prompt compression, response caching, model routing, and output length controls are all variable cost levers. The danger is assuming that because variable costs are linear, they are predictable. They are predictable only if traffic is predictable. A viral product launch, a marketing campaign, or a seasonal spike can double traffic overnight, and variable costs double with it.

## Semi-Variable Costs: The Step Function Trap

Semi-variable costs do not scale linearly. They scale in steps. You pay a flat rate within a tier, and when you exceed the tier threshold, you jump to a higher rate. The cost function looks like a staircase, not a ramp. This creates a deceptive pattern: costs feel fixed when you are in the middle of a tier, and then they spike when you cross a boundary.

The most common semi-variable cost in AI systems is autoscaling infrastructure. If you are self-hosting a model on GPU instances, your compute cost is determined by the number of instances running. One instance handles up to a certain throughput. When traffic exceeds that throughput, autoscaling adds a second instance, and your cost doubles. It does not increase by 1% because traffic increased by 1%. It increases by 100% because you crossed the threshold for a new instance. You pay the full cost of the second instance even if you are only using 10% of its capacity.

For a team running Llama 4 70B on four A100 instances at approximately $3.00 per hour per instance, the base cost is $12.00 per hour or $8,640 per month. If peak traffic pushes autoscaling to eight instances for four hours per day, those peak hours cost $24.00 per hour instead of $12.00. That peak-hour premium adds approximately $1,440 per month, a 17% increase that is not proportional to the traffic increase. If peak traffic only exceeds the four-instance threshold by 5%, you are paying 100% more compute for that 5% marginal traffic during peak hours.

Eval sampling rates create another semi-variable cost. Many teams run automated evaluations on a sample of production requests. At low traffic, you might evaluate 10% of requests. As traffic grows, you adjust the sampling rate to control eval costs. But the adjustment is not continuous. You drop from 10% to 5%, then from 5% to 2%, then from 2% to 1%. Each adjustment is a step that changes your eval cost by a large percentage. And there are minimum thresholds below which your eval results lose statistical significance. You cannot drop to 0.01% sampling and still trust the results. The floor on sampling creates a floor on eval cost that scales with traffic in steps.

Batch processing overhead is a third example. Many AI systems process tasks in batches, whether for document ingestion, report generation, or periodic retraining of retrieval indexes. Batch costs jump when the batch size exceeds what a single compute instance can handle. Processing 10,000 documents in a batch might take one instance for two hours. Processing 11,000 documents might require two instances because the batch does not fit in memory, doubling the compute cost for a 10% increase in work. These step functions are invisible until you hit them, and they make cost forecasting unreliable if you model everything as linear.

The key principle with semi-variable costs is to identify the step boundaries and operate efficiently within each tier. If your autoscaling adds instances at 80% CPU utilization, your cost per request is lowest at 79% utilization and highest at 81%. The marginal cost of the request that crosses the threshold is not the cost of one request. It is the cost of one entire additional instance. Teams that understand this manage their workloads to avoid crossing tier boundaries during peak hours, or they negotiate pricing structures that soften the steps.

## Fixed Costs: The Floor That Never Moves

Fixed costs are the baseline you pay regardless of whether anyone uses your system. Zero users, zero requests, zero tokens, and you still owe the same amount. Fixed costs create a **cost floor** that your revenue must exceed before you can even begin to think about margins.

The largest fixed cost in most AI systems is reserved compute. If you have committed to GPU instances through reserved pricing, such as AWS Reserved Instances or Google Cloud Committed Use Discounts, you pay for those instances whether they are running inference or sitting idle. A one-year commitment on four A100 instances might cost $5,500 per month, a significant discount from on-demand pricing but a cost that exists regardless of traffic. During development, when traffic is zero, you are still paying $5,500 per month. During a slow month, when traffic drops 50%, you are still paying $5,500 per month. The reservation saves you money at high utilization but creates pure waste at low utilization.

Vector database base pricing is another common fixed cost. Managed vector databases like Pinecone, Weaviate Cloud, or Qdrant Cloud charge a base fee for the cluster, regardless of query volume. A production-tier Pinecone pod might cost $70 per month at the smallest tier. A high-availability configuration with replicas might cost $300 to $800 per month. You pay this whether you run one million queries or zero queries. The cost scales with index size and replica count, not with query volume.

Platform licenses and tooling subscriptions are fixed costs that teams often overlook. Experiment tracking platforms like Weights and Biases cost per-seat per-month. Monitoring platforms like Datadog charge per-host per-month with additional costs for log volume. LLM observability platforms like Langfuse or Helicone may charge flat platform fees. Annotation tools for evaluation charge per-seat. These individually small subscriptions compound. A team of ten using five SaaS tools for their AI stack can easily spend $3,000 to $5,000 per month in platform fees alone.

Team salaries allocated to AI operations are the largest fixed cost of all, and the one most teams forget to include in their cost model. If you have three engineers spending 50% of their time on AI system maintenance, monitoring, and optimization, and their loaded cost is $200,000 per year each, you have $300,000 per year in fixed AI labor costs, or $25,000 per month. This cost does not change when traffic doubles. It does not change when traffic drops. It is the single largest line item in most AI cost structures, and it is the one that never appears on the cloud bill.

The key principle with fixed costs is that they determine your minimum viable scale. If your fixed costs are $35,000 per month, you need enough revenue to cover that before variable costs even enter the equation. A team that spends heavily on fixed costs at low traffic will burn cash rapidly. A team that minimizes fixed costs by using on-demand pricing and avoiding commitments will have higher per-unit costs but lower risk. The right balance depends on your confidence in traffic forecasts. If you are confident that traffic will be high and stable, fixed costs through commitments save money. If traffic is uncertain, fixed costs through commitments create risk.

## Risk-Adjusted Costs: The Line Items That Only Appear Sometimes

Risk-adjusted costs are the most dangerous category because they are invisible in normal operations and devastating when they materialize. These are costs that have a probability of occurring and an impact when they do. You cannot see them on your monthly bill most months. But when they hit, they can double or triple your spend for days or weeks.

**Retry storms** are the canonical example. When an API provider experiences degraded performance, requests start timing out. Your system retries those requests. Each retry consumes tokens, incurs API costs, and adds latency. If your retry policy is aggressive, with three retries per failed request and exponential backoff starting at one second, a provider degradation that affects 10% of requests can increase your API costs by 30% for the duration of the incident. A five-hour incident at 100,000 requests per day means roughly 2,000 extra requests per hour, each consuming the same tokens as the original. If a typical request costs $0.02 in tokens, the extra cost is $200 over the five hours. That sounds manageable. But retry storms compound. The retries add load to the already degraded provider, making more requests fail, which triggers more retries. A 10% failure rate can cascade to 40% within minutes if retry policies are not carefully designed. Now you are paying for 40% extra traffic, and the incident lasts longer because the retries are contributing to the overload.

For a system processing 100,000 requests per day at $0.02 per request, normal daily cost is $2,000. A retry storm affecting 30% of requests with three retries each adds $1,800 in a single day, nearly doubling the bill. If the storm lasts three days, the extra cost is $5,400. This happens perhaps two to four times per year, creating an expected annual risk cost of $10,800 to $21,600. That is real money, but it does not appear in any monthly forecast because it is probabilistic.

**Abuse events** are another risk-adjusted cost. A malicious user discovers your AI-powered feature and uses it to generate massive volumes of content, whether for spam, data extraction, or simply to test the limits of your system. A single abuser can generate thousands of requests per hour. If your rate limits are generous, a dedicated abuser can run up $5,000 to $20,000 in API costs over a weekend before anyone notices. Abuse events are probabilistic. They might happen once a year or once a month, depending on your product's visibility and the effectiveness of your rate limiting.

**Provider outages forcing failover** represent a third risk-adjusted cost. If your primary model provider goes down and your system fails over to a secondary provider, the secondary provider might be more expensive. A team using GPT-5-mini as their primary model at $0.40 per million input tokens might fail over to Claude Sonnet 4.5 at $3.00 per million input tokens, a 7.5 times cost increase. If the outage lasts six hours and the system processes 25,000 requests during that window, the cost difference could be $500 to $1,000 for a single incident. These outages happen several times per year. Each one is unpredictable in timing and duration.

The expected value calculation for risk-adjusted costs works like this. Take each risk event, estimate its probability per month, and multiply by the cost impact. If retry storms happen once every three months and cost $5,000 per event, the expected monthly cost is $1,667. If abuse events happen once every six months and cost $15,000 per event, the expected monthly cost is $2,500. If provider failover events happen once every two months and cost $750 per event, the expected monthly cost is $375. Total expected risk-adjusted cost: $4,542 per month. This is real money that must be budgeted, even though it does not appear on any individual month's bill in a predictable way.

## Putting the Categories Together: A Complete Cost Picture

For a mid-size SaaS product doing 100,000 requests per day, the complete monthly cost picture across all four categories might look like this in prose form. Variable costs for token consumption, embedding generation, and API overhead total approximately $18,000 per month. Semi-variable costs for autoscaling tiers, eval sampling, and batch processing add approximately $4,500 per month. Fixed costs for reserved compute, vector database base pricing, platform licenses, and allocated team salary add approximately $35,000 per month. Risk-adjusted costs with an expected value for retry storms, abuse events, and failover add approximately $4,500 per month. The total is approximately $62,000 per month.

Most teams only see the variable costs. They look at their API bill and think they understand their AI costs. They are seeing less than 30% of the picture. The fixed costs alone are larger than the variable costs in this example, and they are the most difficult to reduce because they involve commitments, contracts, and headcount.

The categories also respond differently to growth. If traffic doubles from 100,000 to 200,000 requests per day, variable costs double to $36,000. Semi-variable costs might increase by 60% to $7,200 because you cross a new autoscaling tier. Fixed costs stay at $35,000. Risk-adjusted costs roughly double because more traffic means more exposure to retry storms and abuse, so they rise to $9,000. Total cost at double the traffic is approximately $87,200, a 41% increase for a 100% traffic increase. This is the unit economics leverage that fixed costs provide: the more traffic you serve over your fixed cost base, the lower your cost per request.

But the leverage works in reverse at low traffic. If traffic drops by half to 50,000 requests per day, variable costs drop to $9,000. Semi-variable costs might stay at $4,500 because you are still within the same tier. Fixed costs stay at $35,000. Risk-adjusted costs drop to $2,250. Total cost at half the traffic is approximately $50,750, only an 18% decrease for a 50% traffic decrease. Your cost per request nearly doubles because the fixed cost base spreads over half the volume.

## Why the Categories Demand Different Optimization Strategies

The four categories are not just an accounting exercise. They determine your optimization roadmap. Variable cost optimization focuses on per-request efficiency: shorter prompts, cheaper models for simple tasks, response caching, deduplication. These optimizations have immediate, linear payoffs. Every token you save, you save on every request.

Semi-variable cost optimization focuses on tier management. You monitor utilization relative to tier boundaries. You schedule batch processing to avoid peak hours. You tune autoscaling policies to delay scaling events. You negotiate tier pricing with providers. These optimizations have nonlinear payoffs because avoiding a single tier transition saves the entire cost of the next tier.

Fixed cost optimization focuses on commitment management. You negotiate reserved pricing only when you have confidence in sustained demand. You share fixed infrastructure across multiple teams or workloads. You audit platform subscriptions and eliminate unused tooling. You track the utilization of reserved resources and release underutilized commitments. These optimizations have the largest absolute savings but require organizational discipline.

Risk-adjusted cost optimization focuses on probability reduction and impact capping. You implement circuit breakers to prevent retry storms. You build rate limiters that cap abuse exposure. You configure failover to cost-comparable alternatives instead of premium fallbacks. You build alerting that detects anomalous spend within minutes, not days. These optimizations do not reduce your normal monthly bill, but they prevent the months where your bill is three times normal.

The team that optimizes across all four categories operates a fundamentally different system from the team that only watches the API bill. One is managing their AI costs. The other is watching them happen.

The next subchapter reframes cost management as an engineering discipline, not a finance function, and introduces the concept of the Engineering Cost Surface.
