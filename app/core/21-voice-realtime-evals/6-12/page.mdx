# 6.12 — Trace-Level vs Session-Level Evaluation

In early 2025, a customer support voice AI team optimized their system to 96% trace-level accuracy — every individual response scored as correct or acceptable when evaluated in isolation. They shipped the update. Within two weeks, session-level success rates dropped from 78% to 71%. Users were completing fewer conversations successfully despite every individual turn being higher quality. The team had optimized the wrong metric.

The failure illustrates the gap between trace-level and session-level evaluation. Trace-level evaluation measures individual turns — did the system give the right response to this specific user input? Session-level evaluation measures entire conversations — did the user accomplish their goal by the end of the interaction? Both matter. They measure different things. Teams that optimize only for trace-level accuracy produce systems that sound good turn-by-turn but fail to complete tasks. Teams that optimize only for session-level success miss the specific turn-level failures that destroy conversational quality. You need both layers of evaluation, each serving a different diagnostic purpose.

## What Trace-Level Evaluation Measures

Trace-level evaluation treats each system response as an independent unit. You evaluate turn three without caring what happened in turns one and two. You score whether the response is correct, relevant, and appropriate for the user's input. Trace-level evaluation is precise, granular, and easy to automate.

It measures component-level quality. Did the ASR correctly transcribe the user's speech? Did the intent classifier identify the right intent? Did the dialog manager select the appropriate response? Did the NLG produce a natural, grammatical utterance? Each component can be evaluated independently. A trace with high ASR accuracy and low intent classification accuracy tells you exactly where the failure occurred.

Trace-level evaluation is the foundation for debugging. When a conversation fails, you trace through it turn by turn to find where the system went wrong. Was it turn two, where the ASR misrecognized the city name? Was it turn five, where the dialog manager lost track of the booking request? Was it turn seven, where the response included outdated information? Trace-level evaluation pinpoints the failure point.

It is also the foundation for regression testing. When you change the ASR model, you run trace-level evaluation on 10,000 individual user utterances to see whether accuracy improved or regressed. When you update the intent classifier, you run trace-level evaluation on 5,000 labeled inputs to measure precision and recall. Trace-level metrics tell you whether component changes improve quality before you test them in full conversations.

Trace-level evaluation supports rapid iteration. You can evaluate 10,000 traces in minutes. You cannot evaluate 10,000 full conversations in minutes — each conversation has 5-12 turns and requires maintaining conversational state. Trace-level evaluation gives you fast feedback during development. Session-level evaluation gives you accurate feedback before release.

## What Trace-Level Evaluation Misses

Trace-level evaluation is blind to conversational context. It does not know whether the system remembered information from earlier turns. It does not know whether the current response contradicts something the system said three turns ago. It does not know whether the conversation is making progress toward a goal or spinning in circles.

It misses context-dependent correctness. A response that is correct in isolation can be wrong in context. If the user asks "what about Boston?" and the system responds "flights to Boston start at 320 dollars," that response scores well in trace-level evaluation. But if the conversation context shows that the user already asked about Boston two turns ago and is now asking about a different city, the response is wrong. Trace-level evaluation never sees the context mismatch.

It misses error accumulation. A small error in turn two — the system misrecognized "Houston" as "Austin" — can cascade into larger errors in turns four, six, and eight as the system builds on the wrong information. Trace-level evaluation scores each turn independently and misses the cascading failure. Each turn might score as individually reasonable, but the conversation is fundamentally broken.

It misses conversation efficiency. A system that takes nine turns to accomplish a task that should take three gets high trace-level scores if each individual turn is correct. Trace-level evaluation does not measure whether the conversation is progressing efficiently. It does not penalize repetition, circular questioning, or unnecessary clarifications.

It misses tone and flow problems. A system might give correct information in a tone that feels abrupt or robotic. A system might interrupt the user mid-sentence with a technically correct response that still destroys conversational flow. Trace-level evaluation measures correctness. It does not measure whether the conversation feels natural.

It misses task completion. The most important metric in voice AI is: did the user accomplish their goal? Trace-level evaluation cannot answer this question because task completion is a session-level outcome. A conversation can have 90% trace-level accuracy and still fail to complete the task if the 10% of failed turns were the critical ones.

Teams that rely only on trace-level evaluation ship systems that feel robotic, lose context, and frustrate users despite scoring well on every component metric.

## What Session-Level Evaluation Measures

Session-level evaluation treats the entire conversation as the unit of analysis. You evaluate whether the conversation as a whole succeeded or failed. Did the user accomplish their goal? Did the conversation end naturally or did the user hang up in frustration? How many turns did it take? Did the system maintain consistency across turns?

Session-level evaluation measures task success — the single most important metric. Task success is binary or scored on a 3-point scale: complete success, partial success, failure. Complete success means the user got exactly what they needed. Partial success means the user got some information but not everything they requested. Failure means the user did not accomplish their goal. Task success correlates most strongly with user satisfaction and retention.

It measures conversation efficiency. Efficiency is the ratio of actual turns to expected turns. If a task should take four turns and took four turns, efficiency is 100%. If it took eight turns, efficiency is 50%. Low efficiency indicates the system is asking unnecessary questions, failing to extract information efficiently, or getting stuck in clarification loops.

It measures conversational coherence. Coherence means the conversation feels like a unified interaction, not a series of disconnected exchanges. The system remembers what the user said earlier. It does not contradict itself. It builds on previous context. It acknowledges corrections and adjusts accordingly. Coherence is difficult to measure automatically but obvious to human evaluators.

It measures goal-oriented progress. Did each turn move the conversation closer to task completion or did some turns waste time? A conversation that steadily progresses toward the goal feels purposeful. A conversation that meanders, backtracks, or repeats itself feels broken. Session-level evaluation tracks whether the system is guiding the conversation toward a successful outcome.

It measures terminal outcomes. Did the user complete the task and end the conversation satisfied? Did the user abandon the conversation mid-way? Did the user request escalation to a human agent? Did the user call back within 24 hours to retry the same task? These terminal outcomes are the ultimate measure of system quality.

Session-level evaluation gives you the ground truth: is your system working? Trace-level evaluation tells you why or why not.

## Aggregation Methods: From Traces to Sessions

You cannot evaluate at only one level. You need both. The question is how to aggregate trace-level metrics into session-level insights and how to decompose session-level failures into trace-level root causes.

The simplest aggregation is turn-level accuracy averaged across the session. If a conversation has eight turns and six are correct, session-level accuracy is 75%. This is easy to compute but misleading. It treats all turns as equally important. In reality, some turns are critical — the turn where the system confirms the flight details, the turn where the system processes payment — and some are routine acknowledgments. Averaging ignores this distinction.

Weighted aggregation gives more importance to critical turns. Tag each turn in your evaluation dataset with a criticality score: 1 for routine, 2 for important, 3 for mission-critical. A misrecognition in a routine acknowledgment has weight 1. A misrecognition in the payment confirmation has weight 3. Weighted accuracy reflects which errors actually break the conversation.

Minimum-turn aggregation says the session is only as good as its worst turn. If seven turns are perfect and one turn gives completely wrong information, the session is a failure. This is harsh but realistic for high-stakes domains. In medical triage or financial advice, one wrong answer can cause serious harm. The session-level score must reflect that.

Failure-mode propagation tracks whether trace-level errors compound or get corrected. If the system makes an error in turn two but self-corrects in turn three, the session-level score should reflect recovery. If the error in turn two causes errors in turns four and six, the session-level score should reflect propagation. This requires annotating conversations with error chains, not just individual errors.

Contextual scoring evaluates each trace in the context of previous turns. Instead of asking "is this response correct in isolation?" ask "is this response correct given everything that happened earlier in the conversation?" This requires evaluating traces sequentially, not in parallel. It is slower but produces session-aware scores.

End-state evaluation ignores intermediate turns and focuses on the final outcome. Did the system complete the task correctly? If yes, the session succeeds regardless of how messy the conversation was. If no, the session fails even if most turns were individually correct. This is the ultimate pragmatic metric: users care about outcomes, not process. But it is a lagging indicator. It does not tell you what to fix.

Most teams use a combination. Track trace-level accuracy for debugging and regression testing. Track weighted session-level success for product quality gates. Track end-state task completion for business metrics. Each layer of aggregation reveals different insights.

## When to Evaluate at Each Level

Trace-level evaluation is for development velocity and component testing. Use it when you want fast feedback on a specific change. You updated the intent classifier. Run trace-level evaluation on 5,000 labeled utterances. You tuned the ASR model. Run trace-level evaluation on 10,000 transcriptions. You modified the dialog policy. Run trace-level evaluation on individual dialog turns. Trace-level evaluation is cheap, fast, and precise.

Session-level evaluation is for release gates and product quality checks. Use it before shipping a major update. You redesigned the conversation flow. Run session-level evaluation on 500 full conversations to measure task success and efficiency. You integrated a new backend API. Run session-level evaluation to ensure the changes did not break multi-turn state management. Session-level evaluation is expensive and slow but reflects real user experience.

Use trace-level evaluation continuously in CI/CD. Every pull request runs trace-level regression tests. You catch component-level regressions within hours. Use session-level evaluation weekly or pre-release. You catch conversation-level regressions before they reach production.

Use trace-level evaluation for root cause analysis. A session failed. Which turn caused the failure? Trace through the conversation turn by turn. The trace-level diagnostic tells you whether the failure was ASR, intent classification, dialog management, or NLG. Use session-level evaluation for triage. Which conversations are failing most often? Which task types have the lowest success rates? The session-level diagnostic tells you where to focus improvement effort.

Use trace-level metrics for team-level accountability. The ASR team owns trace-level transcription accuracy. The NLU team owns trace-level intent classification metrics. The dialog team owns trace-level response appropriateness. Each team has a clear, measurable goal. Use session-level metrics for product-level accountability. The product team owns task success rate and user satisfaction. Session-level metrics align teams around user outcomes, not component metrics.

## Bridging the Gap: Trace-Level Metrics That Predict Session Success

Some trace-level metrics correlate more strongly with session-level success than others. If you can only track a handful of trace-level metrics, track the ones that predict session outcomes.

**Critical-turn accuracy** is the strongest predictor. Not all turns matter equally. The turn where the system confirms the booking, the turn where it processes payment, the turn where it provides the final answer — these turns determine session success. Tag critical turns in your evaluation dataset. Measure accuracy specifically on critical turns. Critical-turn accuracy correlates with session success at 0.7 to 0.85, far stronger than average turn-level accuracy.

**Context retention rate** is the second strongest predictor. After each turn, check whether the system still has access to information from earlier turns. If the user provided their account number in turn two, does the system still have it in turn five? If the system asked for a date in turn three, does it remember the user's answer in turn six? High context retention correlates with session success. Low context retention causes the conversation to loop or lose the thread.

**Error recovery success** predicts session outcomes. When the system makes an error — ASR misrecognition, wrong intent classification, incorrect information — does it detect and correct the error within two turns? Error recovery success rate correlates with session success. Systems that recover quickly from errors maintain user trust. Systems that compound errors or ignore them destroy trust.

**Response latency at critical turns** predicts session abandonment. Average latency has weak correlation with session success. Latency specifically at critical turns — the turn where the user is waiting for confirmation, the turn where the user expects an answer — has strong correlation. Users tolerate latency during routine turns. They abandon during latency at critical moments.

**Consistency score** measures whether the system contradicts itself across turns. If the system says the user's balance is 4,200 dollars in turn two and 3,800 dollars in turn five without explaining the discrepancy, consistency is broken. Consistency violations destroy user trust even when each individual turn is correct. Track consistency as a trace-level metric. It predicts session-level satisfaction.

Teams that optimize these five trace-level metrics — critical-turn accuracy, context retention, error recovery, critical-turn latency, and consistency — see session-level success improve even when average trace-level accuracy stays flat. Not all traces are created equal. Optimize the ones that matter.

## Debugging Session Failures with Trace Analysis

Session-level evaluation tells you a conversation failed. Trace-level analysis tells you why. The workflow is: identify failing sessions, decompose them into traces, analyze which traces caused the failure, fix the root cause, re-evaluate.

Start with session-level filtering. Pull all conversations with task success below 70%. Pull all conversations that took more than 150% of expected turns. Pull all conversations where the user abandoned mid-session or escalated to a human. These are your failure cases.

For each failed session, reconstruct the conversation trace. List every user turn and every system turn. Annotate each turn with component metrics: ASR confidence, intent classification confidence, dialog state, response latency. The trace shows the full timeline of what happened.

Identify the failure point. In many cases, a single turn causes the cascade. The system misrecognized a critical piece of information in turn two. Everything after that is wrong. The system lost context in turn four. Turns five through eight are irrelevant or repetitive. Pinpointing the failure point tells you which component to fix.

Classify the failure mode. Was it an ASR problem? An intent classification problem? A dialog management problem? An NLG problem? A backend integration problem? Failure mode classification turns anecdotal debugging into systematic improvement. If 40% of session failures trace back to ASR errors on proper nouns, you know to invest in proper noun recognition.

Aggregate failure patterns across sessions. A single session failure is an anecdote. Fifty session failures with the same trace-level root cause is a pattern worth fixing. Look for common failure points: turn three in appointment booking conversations, turn five in account verification flows, turn seven in troubleshooting sessions. Common failure points indicate systemic problems, not edge cases.

Fix the root cause at the trace level. If ASR is misrecognizing city names, improve city name recognition or add a confirmation step. If intent classification is confusing two similar requests, retrain the classifier or add disambiguation. If the dialog manager is losing context after six turns, fix state management. Trace-level fixes prevent future session-level failures.

Re-run session-level evaluation after the fix. Did task success improve? Did efficiency improve? If the trace-level fix did not improve session-level metrics, either you fixed the wrong trace or the failure mode is more complex than a single turn. Iterate until session-level metrics improve.

## Reporting Metrics at Both Levels

Your evaluation dashboard should show both trace-level and session-level metrics. Leadership cares about session-level success. Engineering cares about trace-level diagnostics. Both need visibility.

Session-level metrics go at the top of the dashboard: task success rate, average turns per conversation, conversation efficiency, user satisfaction score. These are the product health metrics. They tell you whether the system is working for users.

Trace-level metrics go below: ASR accuracy, intent classification precision and recall, dialog policy accuracy, response latency percentiles, error rate. These are the component health metrics. They tell you which parts of the system need improvement.

Show correlations between trace-level and session-level metrics. Which trace-level metrics predict session success? Which have weak correlation? This helps engineering prioritize. If improving ASR accuracy from 94% to 96% would only improve session success from 80% to 81%, but improving error recovery from 60% to 75% would improve session success from 80% to 85%, the prioritization is clear.

Show trace-level metrics broken down by session outcome. Compare ASR accuracy in successful sessions versus failed sessions. Compare response latency in successful sessions versus failed sessions. The gap tells you which component metrics matter. If ASR accuracy is 96% in successful sessions and 95% in failed sessions, ASR is not the limiting factor. If response latency is 300ms in successful sessions and 700ms in failed sessions, latency is driving failures.

Report trends over time at both levels. Session-level success might be stable at 78% while trace-level ASR accuracy improved from 92% to 95%. This tells you that ASR improvements are not translating to session success — something else is the bottleneck. Or session-level success might improve from 75% to 82% while trace-level metrics stay flat. This tells you that conversation design or error recovery improved even though component accuracy did not.

Use trace-level metrics for sprint goals and component team OKRs. Use session-level metrics for quarterly product goals and executive reporting. Both levels matter. They serve different audiences.

## Choosing Evaluation Granularity for Your Use Case

Some systems need trace-level precision. Some need session-level outcomes. Some need both. The choice depends on your domain, your quality bar, and your resources.

High-stakes domains — medical, legal, financial — require both. Trace-level evaluation ensures no individual turn gives dangerous information. Session-level evaluation ensures the conversation as a whole led to the correct outcome. You cannot compromise on either. A medical triage bot that gives correct information turn-by-turn but leads the user to the wrong diagnosis is a failure. A legal information system that completes the session but gave incorrect advice in one trace is a liability.

Transactional domains — booking, ordering, account management — prioritize session-level success. Users care whether they booked the flight, not whether every turn was perfect. Trace-level evaluation is for debugging, not quality gates. If session success is high, trace-level imperfections are acceptable.

Informational domains — customer support, FAQ, help systems — balance both. Session-level success measures whether the user got the information they needed. Trace-level accuracy measures whether individual answers were correct. Both matter because users might ask follow-up questions based on earlier answers. An incorrect answer in turn two can mislead the user even if the session completes.

Companion or entertainment domains — smart home assistants, voice games, ambient AI — prioritize conversational flow and user satisfaction over strict accuracy. Session-level metrics focus on engagement, not task completion. Trace-level metrics focus on naturalness and responsiveness, not correctness.

Resource-constrained teams start with trace-level evaluation because it is cheaper and faster. As the system matures, add session-level evaluation. Resource-rich teams run both from day one. The cost is worthwhile because session-level failures in production are far more expensive than session-level evaluation in development.

Trace-level evaluation is your microscope. Session-level evaluation is your telescope. You need both to see the full picture. The next step is understanding when and how to bring human evaluators into the loop — because some aspects of conversational quality cannot be measured by metrics alone.

