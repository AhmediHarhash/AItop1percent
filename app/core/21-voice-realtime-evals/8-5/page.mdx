# 8.5 — Hallucination-Under-Noise: The HUN Rate Problem

Most teams think hallucination is a text-only problem. They are wrong. In voice systems, hallucination becomes exponentially more dangerous because it couples with acoustic errors. When the automatic speech recognition system mishears "cancel my order" as "I want to order," the language model does not pause or ask for clarification. It generates a confident, coherent, completely wrong response to a request the user never made. The user hears a system that seems to understand perfectly while doing the exact opposite of what they asked. This is the Hallucination-Under-Noise problem, and in 2026 it remains one of the most insidious failure modes in production voice AI.

The phenomenon was first systematically documented in early 2025 when a healthcare appointment system confidently scheduled patients for procedures they had explicitly called to cancel. The ASR layer heard "book" when users said "remove." The LLM layer, receiving what it believed was a booking request, generated warm, reassuring confirmation language. Patients hung up believing their appointments were cancelled. They were not. The system passed every offline evaluation because the test cases used clean audio with perfect transcription. The HUN rate was zero in the lab and twelve percent in production.

## The Coupling Failure Between ASR and LLM

The traditional separation of concerns between speech recognition and language understanding creates a trust boundary that neither component is designed to question. The ASR system outputs text with a confidence score. The LLM receives that text and generates a response. If the ASR confidence is above threshold—typically 0.85 or higher in production systems—the text is treated as ground truth. The LLM has no way to know the user actually said something different. It responds to the phantom input with the same confidence it would apply to perfectly transcribed speech.

This becomes catastrophic when ASR errors are not random noise but systematic misrecognitions that create coherent alternative meanings. "Cancel" and "schedule" are acoustically similar under phone-line compression. "Delete" and "complete" share phonetic patterns. "Stop" and "shop" differ by a single phoneme. When the ASR mishears one of these words, it does not produce garbled text. It produces a different valid sentence. The LLM receives "I want to shop for shoes" when the user said "I want to stop the subscription." The response is fluent, contextually appropriate to the misheard input, and completely wrong for the user's actual intent.

The HUN rate measures exactly this: the percentage of responses that are coherent hallucinations triggered by ASR errors. It is not the ASR error rate. It is not the LLM hallucination rate. It is the joint failure rate where both systems operate within their individual quality thresholds while producing combined behavior that is dangerously incorrect. A system with a three percent ASR error rate and a two percent LLM hallucination rate can have a twelve percent HUN rate if the errors correlate in adversarial ways.

## Acoustic Conditions That Trigger HUN Events

Background noise does not affect all speech equally. Consonants degrade faster than vowels. Fricatives and stops—the sounds that distinguish "cancel" from "cancel"—disappear first in noisy environments. A user calling from a busy street, a car, or a construction site produces audio where these critical phonemes are masked by ambient sound. The ASR system, trained to be robust to noise, fills in the gaps with its language model priors. It guesses the missing consonants based on what words are statistically likely in that context.

This guess is educated but not clairvoyant. In a customer service context, "I want to cancel" and "I want to answer" might both fit the conversational flow. The ASR picks the one with slightly higher language model probability. If the user is calling about a survey, "answer" wins. If they are calling about a subscription, "cancel" should win—but if the acoustic signal is ambiguous, the ASR might guess wrong. The confidence score remains high because the selected word fits the language model. The LLM receives a coherent sentence and responds accordingly.

Accented speech creates similar problems. Non-native speakers often produce phonetic variations that ASR systems, trained predominantly on standard American or British English, misinterpret. A speaker with a strong regional accent saying "I need to halt this payment" might be heard as "I need to help with this payment." The semantic inversion is complete. The user wants to stop something. The system hears a request to assist with it. The response is helpful, polite, and entirely wrong.

Phone line artifacts introduce another layer of distortion. Voice over IP compression, packet loss, and jitter create acoustic glitches that ASR systems must interpret. A brief dropout in the middle of "discontinue" might leave the ASR with "continue." The missing syllable is not silence—it is a gap the system fills with its best prediction. That prediction is based on statistical likelihood, not acoustic evidence. When the statistics mislead, the HUN event occurs.

## Measuring HUN Rate in Production

You cannot measure HUN rate offline. The phenomenon requires real acoustic conditions, real user speech patterns, and real conversational context. Synthetic test cases with clean audio and scripted transcription errors miss the point entirely. You need production traffic with actual ASR uncertainty and actual LLM responses to misheard input.

The measurement approach starts with logging every ASR confidence score alongside the corresponding LLM response. For every turn in every conversation, you capture three things: the audio, the transcription with confidence, and the generated response. You cannot evaluate every conversation manually—volume prohibits it. But you can sample based on risk indicators. Low ASR confidence paired with high-impact actions is the highest priority. A confidence of 0.87 paired with a response that initiates a financial transaction gets reviewed. A confidence of 0.98 paired with a response that provides general information can wait.

Human review of these sampled conversations reveals the HUN events. A reviewer listens to the audio and compares it to the transcription. If they differ in semantically significant ways—not just minor word substitutions but meaning inversions—the conversation is flagged. If the LLM response makes perfect sense given the transcription but is incorrect given the actual audio, that is a HUN event. The rate is the count of flagged events divided by the sample size, extrapolated to the full population with appropriate confidence intervals.

Advanced measurement uses semantic distance as a proxy. For each conversation turn, you compare the user's likely intents based on context—what they have said previously, what actions are common at this conversation stage—with the intent implied by the LLM response. If the user is in a cancellation flow and the response confirms a purchase, the semantic distance is high. You flag these for manual review. The automated metric is not perfect, but it surfaces the right conversations for human inspection.

Another signal is user correction behavior. When a user immediately responds with "No, I said cancel, not continue," that is a HUN event. The system misheard, responded incorrectly, and the user noticed. These corrections are gold for measurement because the user has done the evaluation for you. You log every conversation where the user explicitly corrects the system within two turns of the original statement. The correction rate correlates strongly with the true HUN rate and can be tracked in real time without manual review.

## The Confidence Score Paradox

ASR confidence scores are calibrated to predict transcription accuracy, not semantic correctness. A confidence of 0.92 means the system is ninety-two percent certain it transcribed the audio correctly. But "correctly" means "the most likely word sequence given the acoustic signal." It does not mean "the word sequence the user actually intended." When the acoustic signal is ambiguous, the most likely transcription can still be wrong.

This creates a paradox for threshold-based safety systems. If you set a confidence threshold of 0.90 and only process transcriptions above that threshold, you catch most of the garbled nonsense—the transcriptions where the ASR is genuinely uncertain. But you miss the adversarial errors where the ASR is confidently wrong. The system heard a coherent sentence. It just was not the sentence the user said. The confidence score is high because the selected words fit the language model. The HUN event occurs despite passing the confidence gate.

Lowering the threshold does not solve the problem. At a threshold of 0.80, you catch more uncertain transcriptions, but you also increase the rate of false rejections. Users with clear speech and good audio get blocked because the ASR confidence dipped slightly. The user experience degrades. Meanwhile, the confident-but-wrong errors still pass through because their confidence is genuinely high. You have made the system more cautious without making it more accurate.

The solution is not a single threshold. It is a risk-adjusted threshold based on the action the system is about to take. For low-stakes responses—providing general information, confirming the user's current state—a confidence of 0.85 is acceptable. The cost of an error is a minor user correction. For high-stakes actions—initiating financial transactions, scheduling medical procedures, canceling services—the threshold must be higher. A confidence of 0.95 is the minimum for irreversible actions. Even then, you add a confirmation step. The system asks, "Just to confirm, you want to schedule the appointment, is that correct?" The user has a chance to catch the error before it commits.

## Clarification Triggers as HUN Mitigation

The most effective mitigation is to make the system ask when it is uncertain. This sounds obvious, but it conflicts with the user experience goal of fluid, natural conversation. Every time the system stops to clarify, it breaks the conversational flow. Users perceive the system as less intelligent. Product teams resist adding friction. The result is systems that optimize for perceived fluency at the cost of actual accuracy.

The trick is to clarify selectively based on semantic risk. Not every uncertain transcription deserves a clarification. If the user says something the ASR interprets as "What are your hours?" with a confidence of 0.88, you respond with the hours. The worst case is you provided information they did not ask for. They correct you, you provide the right information, the conversation continues. No harm done.

But if the ASR interprets something as "Cancel my subscription" with a confidence of 0.88, you clarify. The system says, "Just to make sure, you want to cancel your subscription?" The user either confirms or corrects. If they correct—"No, I said I want to change my subscription"—you have caught a HUN event before it caused damage. The clarification adds one extra turn to the conversation. The alternative is executing an action the user did not request. The trade-off is obvious.

The clarification trigger logic is intent-based. You categorize intents by reversibility and impact. Informational queries are low impact. Reversible actions—changing settings that can be changed back—are medium impact. Irreversible actions—canceling services, initiating payments, scheduling appointments—are high impact. High-impact intents trigger clarification if ASR confidence is below 0.95. Medium-impact intents trigger clarification if confidence is below 0.90. Low-impact intents never trigger clarification.

This creates a user experience where the system is confident when the stakes are low and cautious when the stakes are high. Users perceive the system as smart and safe. The perceived intelligence comes from fluent responses to low-stakes queries. The perceived safety comes from confirmation prompts before high-stakes actions. Both perceptions are accurate.

## Acoustic Quality Gates for High-Stakes Actions

Before executing any high-stakes action, you check the acoustic quality of the input that triggered it. This is not the same as ASR confidence. ASR confidence measures how well the transcription fits the audio and the language model. Acoustic quality measures the signal-to-noise ratio, the presence of clipping or distortion, the degree of packet loss in VoIP transmission. You can have high ASR confidence on terrible audio if the language model strongly predicts the likely words.

The acoustic quality check is a gate. If the user says something the system interprets as a high-stakes intent—cancel, purchase, schedule—and the audio quality is poor, the system refuses to proceed without confirmation. It says, "I want to make sure I heard you correctly. The line is a bit unclear. You want to cancel your subscription?" The user confirms or corrects. If the audio quality is good and the ASR confidence is high, the system can proceed without the extra confirmation. The gate adapts to the reliability of the input signal.

Measuring acoustic quality in real time requires features the ASR system already computes but often does not expose. Signal-to-noise ratio, energy in the speech band versus the noise band, spectral flatness, zero-crossing rate—these are all inputs to the ASR model. You extract them and run a lightweight classifier that outputs a quality score. Scores below 0.85 trigger the confirmation gate for high-stakes actions. Scores above 0.95 allow the system to proceed with confidence. Scores in between use the standard confidence threshold logic.

This gate catches a different class of errors than the confidence threshold. It catches the cases where the ASR made a confident guess on ambiguous audio. The confidence score is high because the language model strongly predicted the selected words. But the acoustic quality score is low because the audio was genuinely unclear. The combination of high confidence and low quality is a red flag. It means the ASR is relying on priors rather than signal. That is exactly when HUN events occur.

## Semantic Distance Monitoring

For every response the system generates, you measure its semantic distance from the expected intents given the conversation context. If the user has been in a billing inquiry flow for three turns and the system suddenly responds as if they asked about product features, the semantic distance is high. This might indicate a HUN event—the ASR misheard something and the conversation jumped tracks.

Semantic distance is computed using embedding models. You embed the user's conversation history and the current response. You compare the response embedding to the distribution of embeddings for typical next-turn responses in similar conversation states. If the response is an outlier—more than two standard deviations from the mean—you flag it. The flag does not block the response in real time. It logs the conversation for review. Over time, the flagged conversations reveal patterns in HUN events that you can address systematically.

This monitoring catches HUN events that slip through confidence and quality gates. The ASR was confident. The audio was clean. But the transcription was still wrong because the user said something phonetically ambiguous and the ASR guessed incorrectly. The LLM response makes sense given the transcription but is semantically distant from the conversation flow. A human reviewer listens to the audio and confirms the HUN event. The pattern gets added to the training data for the next ASR model update.

Semantic distance also catches adversarial users trying to exploit the system. If a user deliberately says something ambiguous—phrasing a request in a way designed to confuse the ASR—the semantic distance will be high. The system cannot always prevent these attacks in real time, but it can flag them for fraud review. A user who repeatedly triggers high semantic distance events while executing financial actions is likely probing for exploits. The fraud team investigates.

## The HUN-Aware LLM Response Strategy

The LLM can defend against HUN events if it is designed to distrust the transcription when the stakes are high. Instead of treating the ASR output as ground truth, the LLM considers it a hypothesis. For high-stakes intents, the LLM generates a response that includes a confirmation of the understood intent. "You want to cancel your subscription—is that correct?" This is not just politeness. It is an active defense. If the transcription was wrong, the user will correct it before the action executes.

This strategy requires the LLM to know which intents are high-stakes. You pass that information as metadata alongside the transcription. The input to the LLM is not just the text. It is the text, the ASR confidence, the acoustic quality score, and the intent risk level. The LLM uses all four signals to decide whether to respond directly or to confirm first. The logic is built into the prompt: "If the intent is high-stakes and either confidence or quality is below threshold, generate a confirmation question instead of executing the action."

The LLM can also use semantic plausibility as a check. If the transcription says "I want to cancel my order" but the conversation history shows the user has no active orders, the LLM should question the transcription. It might respond with, "I do not see any active orders on your account. Could you clarify what you would like to cancel?" This response works whether the transcription was correct—the user is confused—or incorrect—the ASR misheard. Either way, the LLM avoids executing a nonsensical action.

This defensive LLM strategy reduces HUN rate without requiring ASR improvements. The ASR can still mishear. But the LLM catches the error before it becomes a user-facing mistake. The cost is slightly longer conversations—one extra confirmation turn per high-stakes action. The benefit is a dramatic reduction in HUN-driven failures. In a 2025 banking voice assistant deployment, adding confirmation prompts for all financial actions reduced the HUN-related complaint rate by eighty-one percent with no change to the ASR system.

## Cross-Checking with Expected User Journeys

Every production voice system has common user journeys. Onboarding flows, support flows, transaction flows. These journeys follow predictable patterns. A user who calls to dispute a charge will typically move through identity verification, issue description, resolution options, and confirmation. If the ASR suddenly interprets something that jumps the user to an unrelated flow—product upsell in the middle of a dispute—you have likely hit a HUN event.

You model these journeys explicitly. Each journey is a state machine. Each state has expected next states and expected intents. You track the user's position in the state machine. If the LLM response moves the user to an unexpected state—one that is not a valid transition from the current state—you flag the conversation. The flag can trigger a real-time intervention. The system says, "I want to make sure I understood. You are calling about a billing dispute, correct?" The user confirms or corrects. The conversation gets back on track.

This cross-checking catches HUN events that semantic distance alone might miss. The response might be semantically coherent with the misheard transcription, but it is still wrong for the user's actual journey. A user in a cancellation flow does not suddenly ask about new features. If the ASR mishears a cancellation-related statement as a features question, the journey model flags it. The system clarifies. The HUN event is prevented.

Modeling user journeys also helps you prioritize which HUN events to fix first. If ninety percent of your HUN events occur in the cancellation flow, you know where to focus. You retrain the ASR on cancellation-related phrases. You add more explicit confirmation steps in that flow. You might even redesign the flow to use simpler, less ambiguous language. The journey model tells you where the system is most vulnerable.

---

The HUN rate is the silent killer in voice AI systems. It does not show up in offline evaluations. It does not trigger confidence alarms. It produces responses that sound perfectly reasonable to anyone who was not on the call. But the user knows. They asked for one thing and got another. They lose trust. They stop using the system. They tell others not to use it. The business impact is slow and corrosive. By the time you notice the pattern, the damage is done. Measuring HUN rate, mitigating it with confirmations and quality gates, and designing LLMs that distrust transcription when stakes are high—these are not optional refinements. They are the difference between a voice system that works and one that quietly fails thousands of users every week.

Next, we address a different kind of deception: when the voice itself is not what it claims to be—deepfake and voice cloning abuse detection.
