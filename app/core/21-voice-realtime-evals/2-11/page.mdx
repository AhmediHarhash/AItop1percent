# 2.11 — Provider Selection Frameworks for Enterprise

Why do most teams spend six weeks evaluating voice AI providers and still choose the wrong one? Because they compare pricing and latency in a spreadsheet while ignoring the constraints that only surface in production — the compliance certifications that Legal requires, the geographic coverage that determines whether your London and Singapore offices can use the same stack, the SLA terms that matter when your voice assistant handles 50,000 calls per day.

Provider selection is not a technical decision. It is a risk management decision with technical inputs. The wrong provider does not just cost more or perform worse. It blocks regulatory approval, creates cascading failures during regional outages, and locks you into pricing that grows faster than your call volume. The right framework evaluates providers across six dimensions simultaneously and weights them according to your specific risk tolerance.

## Latency as a Disqualifier, Not a Differentiator

Every voice AI provider advertises low latency. Most measure it differently, test it under ideal conditions, and report percentiles selectively. The median round-trip latency — the number in the marketing materials — tells you almost nothing about production experience.

What matters is tail latency under load in the regions where your users call from. A provider with 180ms median latency and 950ms p95 latency creates a worse user experience than a provider with 220ms median and 380ms p95. Users tolerate slightly slower average response times. They abandon conversations when responses occasionally take a full second.

Geographic variability often exceeds provider-to-provider variability. A provider with excellent North American latency might route Asia-Pacific traffic through a single point of presence in Singapore, adding 200ms to every call. Another provider might have regional endpoints in Tokyo, Sydney, and Mumbai, keeping latency under 250ms across the region.

Test latency from the locations where your users actually call from. If your contact center serves customers in Brazil, India, and Germany, run latency tests from São Paulo, Mumbai, and Frankfurt — not from your office in San Francisco. Provision test phone numbers in each region, make 100 calls at different times of day, and measure end-to-end latency from speech input to audio output. The p95 and p99 latencies from these real-world tests are the numbers that matter.

One global insurance company evaluated four ASR providers based on published benchmarks showing median latencies between 150ms and 200ms. When they tested from their actual contact center locations in Manila, Warsaw, and Mexico City, p95 latencies ranged from 340ms to 1,100ms. The provider with the lowest published latency had the worst tail latency in two of three regions because they routed all non-US traffic through a single European data center. The company chose the provider with the second-lowest median latency but the most consistent p95 across regions.

## Accuracy Is Context-Dependent

ASR accuracy benchmarks use clean audio recorded in controlled environments. Your users call from cars, kitchens, open-plan offices, and street corners. They speak with accents, use domain-specific vocabulary, and interrupt themselves mid-sentence. Published accuracy numbers are starting points, not predictions.

The only way to know which provider works for your use case is to test on your actual data. Record 500 to 1,000 real customer calls — with permission and appropriate anonymization. Transcribe them through each candidate provider. Manually review the transcripts and calculate word error rates for your data, your audio quality, your vocabulary.

The accuracy ranking often changes. A provider with the best published accuracy on LibriSpeech might rank third on your healthcare vocabulary. A provider optimized for North American English might struggle with the Indian-accented English that represents 40% of your call volume. A provider with excellent accuracy on clean audio might degrade faster than competitors when background noise increases.

Domain-specific vocabulary is the most common accuracy differentiator. If your voice assistant handles insurance claims, pharmaceutical names, or legal terminology, test whether the provider supports custom vocabulary hints or allows you to upload a domain-specific lexicon. Some providers improve accuracy by 8% to 15% when you provide a list of expected terms. Others ignore the hints entirely.

A telehealth company tested three ASR providers on 800 anonymized patient calls. The provider with the highest published accuracy scored 89% word error rate on their data. The provider with the lowest published accuracy scored 91% after the company uploaded a 2,000-term medical vocabulary list. The difference was large enough to change the business case — the higher-accuracy provider reduced misrouted calls by 23% and decreased average call handling time by 18 seconds.

## Compliance Certifications and Regional Data Residency

Healthcare, financial services, and government use cases require specific compliance certifications. HIPAA for healthcare data, SOC 2 Type II for general enterprise, PCI-DSS for payment handling, ISO 27001 for international deployments. Some industries require FedRAMP or regional equivalents.

Not all voice AI providers hold all certifications. Some hold certifications for certain services but not others — their ASR API might be HIPAA-compliant, but their TTS API might not be. Some hold certifications in certain regions but not globally. If your compliance team requires that all voice data remain within the European Union, you must verify that the provider offers EU-only data residency and that the certification covers the specific services you plan to use.

The certification verification process takes weeks. Legal and compliance teams review the provider's audit reports, data processing agreements, and subprocessor lists. They check whether the provider's infrastructure isolates customer data, encrypts it in transit and at rest, and implements access controls that meet your internal policies. If the provider cannot produce documentation, or if the documentation reveals gaps, the evaluation ends regardless of technical performance.

One healthcare system evaluated a voice AI provider with excellent latency and accuracy. The provider held HIPAA certification for their core ASR service but routed TTS synthesis through a third-party subprocessor that was not covered by the certification. The healthcare system's legal team rejected the provider because the architecture created a data flow to a non-compliant component. The voice AI team had to restart the evaluation with providers that offered end-to-end certified stacks.

Data residency requirements often eliminate otherwise strong candidates. A European financial services firm required that all customer voice data remain within EU data centers. Two of the four providers they evaluated could not guarantee EU-only processing — their load balancers and orchestration layers ran in US regions. The firm chose a provider with slightly higher latency but full EU data residency.

## Geographic Coverage and Multi-Region Architecture

If you operate in multiple countries, your voice AI provider must support the regions where you operate. This means more than "we have a data center in Europe." It means local phone numbers, local telephony interconnects, local languages, and local support teams.

Some providers offer global coverage but route all traffic through a single region. Your customers in Australia call a local number, but the audio streams to a US data center for processing, then streams back. The round-trip adds 300ms to 500ms of latency. Other providers operate regional data centers and route traffic to the nearest endpoint, keeping latency low.

Local telephony interconnects matter for cost and reliability. If your provider operates its own telephony infrastructure in a region, calls route directly to their ASR and TTS services. If your provider purchases telephony services from a local reseller, you pay both the provider's markup and the reseller's fees. The cost difference can be 2x to 5x.

Language support is not universal. A provider might offer excellent English, Spanish, and Mandarin ASR but poor or nonexistent support for Hindi, Arabic, or Portuguese. If you plan to expand into new markets, verify that the provider supports the languages you will need — not just today, but over the next two years. Switching providers mid-deployment because your current provider does not support the languages your new market requires is a six-month distraction.

A global logistics company deployed a voice assistant for shipment tracking across 14 countries. They chose a provider with low latency in North America and Europe but discovered that the provider's Asia-Pacific coverage relied on resold telephony services with per-minute costs 3.8x higher than their US costs. When the company expanded into India and Southeast Asia, the cost per call became unsustainable. They migrated to a provider with regional data centers and direct telephony interconnects, reducing per-call costs by 60% in those regions.

## SLAs, Downtime, and Failover Strategy

Voice AI providers advertise uptime SLAs — typically 99.9% or 99.95%. The actual terms matter more than the headline number. What counts as downtime? Does the SLA cover partial degradation, or only complete outages? What is the credit structure if the provider misses the SLA? Do you receive a service credit equal to 10% of your monthly bill, or a prorated refund for the downtime minutes?

Most enterprise voice systems cannot tolerate a single point of failure. If your voice assistant handles customer support, sales, or critical transactions, you need a failover strategy. Some teams architect multi-provider redundancy — if the primary ASR provider fails, traffic automatically routes to a secondary provider. Others rely on the provider's own multi-region redundancy and verify that the provider operates active-active data centers that can survive a regional outage.

Testing failover is not optional. Provision accounts with both your primary and backup providers, configure your orchestration layer to switch between them, and run quarterly failover drills. Measure how long it takes for traffic to reroute, whether call quality degrades during the transition, and what percentage of in-progress calls drop.

One e-commerce company relied on a single voice AI provider for their customer service assistant. When the provider experienced a six-hour outage in their primary US region, the voice assistant went offline. The company's human agent queue grew from 12-minute wait times to 78-minute wait times. Customer satisfaction scores dropped 34 points in a single day. The company spent four months implementing a multi-provider architecture with automatic failover and now tests failover every quarter.

## Pricing Models and Total Cost of Ownership

Voice AI providers charge per minute of audio processed, per API call, per concurrent connection, or a combination of all three. Published pricing is typically based on the lowest tier, the simplest use case, and the most favorable usage patterns. Your actual cost depends on volume discounts, regional pricing differences, and whether you use premium features.

Per-minute pricing is the most common model. You pay $0.005 to $0.02 per minute for ASR, $0.01 to $0.06 per minute for TTS, and $0.004 to $0.015 per minute for telephony. Costs vary by provider, region, and whether you commit to a minimum monthly spend. A provider might charge $0.008 per minute for ASR in North America but $0.018 per minute in Asia-Pacific. If 60% of your call volume comes from Asia-Pacific, your effective cost per minute is far higher than the published rate.

Volume discounts are negotiable but non-linear. A provider might offer 10% discounts at 100,000 minutes per month, 20% at 500,000 minutes, and 30% at 1 million minutes. If your current volume is 80,000 minutes per month but you expect to grow to 200,000 within six months, negotiate the discount tier based on projected volume. Providers want the contract. They will often extend higher-tier pricing if you commit to reaching that volume within a defined period.

Premium features add cost. Real-time transcription, sentiment analysis, speaker diarization, custom vocabulary, and enhanced accuracy models all carry upcharges — typically 20% to 100% of the base per-minute rate. If your use case requires these features, include them in the cost model from the start. A provider with a low base rate and expensive premium features might cost more than a provider with a higher base rate but included features.

One financial services firm compared two ASR providers. Provider A charged $0.006 per minute with a 15% upcharge for real-time transcription and a 25% upcharge for custom financial vocabulary. Provider B charged $0.009 per minute with both features included. At the firm's projected volume of 300,000 minutes per month, Provider A would cost $2,610 per month. Provider B would cost $2,700 per month. The $90 difference was negligible, but Provider B's simpler pricing model reduced billing complexity and eliminated the risk of unexpected costs if the firm added features later.

## The Evaluation Framework

Evaluate providers systematically across six dimensions, weighted by your priorities:

**Latency and performance**: Test p95 and p99 latency in all regions where you operate. Weight this dimension heavily if user experience depends on conversational responsiveness.

**Accuracy and domain fit**: Test word error rates on your actual data, your vocabulary, your audio quality. Weight this dimension heavily if errors create customer dissatisfaction or operational cost.

**Compliance and security**: Verify certifications, data residency, and encryption standards. Weight this dimension heavily if regulatory risk is existential — healthcare, finance, government.

**Geographic coverage**: Verify regional data centers, local telephony interconnects, and language support. Weight this dimension heavily if you operate in multiple countries or plan to expand.

**Reliability and SLAs**: Review SLA terms, test failover, and calculate the cost of downtime. Weight this dimension heavily if voice AI is a critical customer-facing channel.

**Total cost of ownership**: Model costs at current and projected volume, including regional pricing and premium features. Weight this dimension heavily if cost per call directly impacts unit economics.

Score each provider on each dimension. Multiply scores by dimension weights. The highest weighted score identifies the best fit. The process is mechanical, but it forces you to make trade-offs explicit. If you choose a provider with weaker compliance because they have better latency, you document that decision and plan mitigations. If you choose a provider with higher cost because they have better geographic coverage, you justify the cost against the revenue opportunity in new markets.

The framework does not eliminate judgment. It structures judgment so that six months later, when the CFO asks why you chose the more expensive provider, you can show the weighted decision matrix and explain the trade-offs.

The next subchapter explores multi-provider architectures — how to combine best-of-breed components from different providers, the integration complexity, and when single-vendor simplicity beats multi-provider optimization.
