# 2.3 — LLM Integration for Voice: Latency-First Model Selection

Most teams choose their LLM based on benchmark performance. MMLU scores, coding ability, reasoning depth. Then they integrate it into a voice pipeline and discover it is too slow. A model that generates brilliant 400-token responses in text chat becomes unusable in voice when those responses take 4 seconds to synthesize and 30 seconds to speak. The mismatch is not a minor inconvenience. It is a category error. Voice AI is a latency-constrained medium. The smartest model is worthless if it is too slow. The right model is the fastest model that meets your quality threshold.

In mid-2025, a legal services company built a voice AI assistant to help clients check case status. They used Claude Opus 4.5 because it produced the most accurate legal reasoning. The first user test was devastating. Users asked simple questions: "What's the status of my case?" Claude generated thoughtful, comprehensive 250-token responses covering case history, next steps, and legal context. The responses took 3.5 seconds to generate and 22 seconds to speak. Users interrupted after 6 seconds. The system kept speaking. Users hung up. The company switched to GPT-5-mini. Response quality dropped slightly. Latency dropped to 1.2 seconds. User satisfaction jumped 41 percentage points. The lesson was brutal: in voice AI, speed is a feature. Thoroughness is a bug.

## The Latency-Quality Frontier: No Free Lunch

The relationship between model size and inference latency is ironclad. Larger models have more parameters. More parameters require more computation. More computation takes more time. A 175-billion-parameter model running on the same hardware is slower than a 7-billion-parameter model. You can optimize infrastructure. You can use quantization, speculative decoding, and batching. But you cannot escape the fundamental trade-off: larger models are slower.

In 2026, the frontier models are GPT-5, Claude Opus 4.5, and Gemini 3 Pro. These models deliver the highest quality responses. They handle complex reasoning, multi-step tasks, and nuanced language. They are also the slowest. Time to first token on GPT-5 ranges from 400 to 800 milliseconds depending on prompt length and server load. Token generation speed ranges from 25 to 50 tokens per second. A 100-token response takes 2.4 to 4.4 seconds from prompt to completion. For text chat, this is acceptable. For voice, it is not.

The mid-tier models are GPT-5.1, Claude Sonnet 4.5, and Gemini 3 Flash. These models balance quality and speed. They are trained on similar data as frontier models but with fewer parameters or more aggressive optimization. Time to first token ranges from 250 to 500 milliseconds. Token generation speed ranges from 40 to 70 tokens per second. A 100-token response takes 1.7 to 3 seconds. This is borderline acceptable for voice depending on your latency budget and user patience.

The fast models are GPT-5-mini, GPT-5-nano, Claude Haiku 4.5, Gemini 3 Nano, and Llama 4 Scout. These models prioritize speed over capability. Time to first token ranges from 100 to 250 milliseconds. Token generation speed ranges from 60 to 120 tokens per second. A 100-token response takes 0.9 to 1.9 seconds. For voice AI, these models are the default choice. They are fast enough to feel responsive. The quality is lower than frontier models but acceptable for most conversational use cases.

The trade-off is not linear. A model twice as large is not twice as slow. The relationship is superlinear. A 175B model is four to six times slower than a 70B model, not two times slower. A 70B model is three to five times slower than a 7B model. The latency penalty of choosing a larger model compounds quickly. You must decide whether the quality gain justifies the latency cost. Most of the time, it does not.

## Streaming Inference: The First Token Is What Matters

Streaming inference changes the latency calculus. Instead of waiting for the model to generate the complete response, you receive tokens as they are generated. The first token arrives in 100 to 800 milliseconds depending on the model. Subsequent tokens arrive every 12 to 100 milliseconds depending on generation speed. The total time to generate a 100-token response is the same whether you use streaming or not. But the perceived latency is much lower with streaming because you can start TTS synthesis as soon as the first sentence fragment arrives.

The time-to-first-token is the critical metric. It determines how long the user waits before hearing any response. A model with 150 millisecond TTFT feels instant. A model with 600 millisecond TTFT feels sluggish. A model with 1,200 millisecond TTFT feels broken. TTFT depends on prompt length, model size, and server load. A short prompt to a small model on underutilized infrastructure achieves 100 to 150 millisecond TTFT. A long prompt to a large model on overloaded infrastructure achieves 800 to 1,200 millisecond TTFT. You must control prompt length and provision infrastructure appropriately to keep TTFT low.

Token generation speed determines how quickly the response unfolds. A model that generates 80 tokens per second emits a new token every 12.5 milliseconds. A model that generates 20 tokens per second emits a new token every 50 milliseconds. The difference is perceptible. Faster generation feels fluid. Slower generation feels halting. Users tolerate slower generation if TTFT is low. They do not tolerate slow generation if TTFT is also high. The combination of high TTFT and slow generation is conversational death.

Streaming also introduces complexity. You must buffer tokens until you have a complete sentence or sentence fragment to send to TTS. If you send every token individually, you waste API calls and increase cost. If you buffer too many tokens, you delay audio playback and negate the latency benefit of streaming. The optimal buffering strategy depends on response structure, TTS provider capabilities, and acceptable latency variance. Most teams buffer until they see sentence-ending punctuation—period, question mark, exclamation point—then send the fragment to TTS. This balances latency and cost.

## Prompt Engineering for Latency: Say Less, Get More

Prompt length directly impacts TTFT. A 1,000-token prompt processes faster than a 6,000-token prompt. The difference is not marginal. A 6,000-token prompt takes three to six times longer to process than a 1,000-token prompt depending on the model architecture. If your conversation history grows unbounded, your TTFT grows proportionally. You must manage prompt length aggressively.

The simplest strategy is truncation. Keep only the most recent N turns of conversation history. If N is 5, you retain the last 5 user utterances and 5 assistant responses. Older history is discarded. This limits prompt length but loses context. If a user refers to something they said 10 turns ago, the model does not remember. Truncation is fast but crude.

The better strategy is summarization. After every 10 turns, summarize the conversation so far into a condensed prompt. The summary replaces the full history. Prompt length stays bounded. Context is preserved. The cost is summarization latency and potential information loss. If the summary omits a critical detail, the model generates a worse response. You must decide whether the latency gain justifies the quality risk.

The best strategy is semantic filtering. Analyze each turn for relevance to the current query. Retain relevant turns. Discard irrelevant ones. If the user asks about their account balance, retain turns about accounts and balances. Discard turns about refilling prescriptions. Prompt length stays low. Context stays high. The cost is filtering complexity and potential misjudgment. If the filtering logic incorrectly discards a relevant turn, the model loses context. You must test filtering logic rigorously.

Regardless of strategy, you must monitor prompt length. Log the token count of every prompt sent to the LLM. Calculate p50 and p95 token counts. If p95 exceeds 4,000 tokens, investigate why. Long prompts are a latency tax you pay on every request. Reducing prompt length by 50 percent can reduce TTFT by 30 to 50 percent.

## Constraining Output Length: Shorter Is Faster

The LLM generates until it decides the response is complete or until it hits a maximum token limit. If you do not constrain output length, the model generates verbose responses. A user asks "What's my balance?" The model generates "Your current account balance is 1,247 dollars and 32 cents as of today, February 1st, 2026. This includes all pending transactions up to this morning. Would you like me to provide a detailed breakdown of recent activity?" This is 50 tokens. The TTS synthesizes 6 seconds of audio. The user wanted a 2-second answer. They got 6 seconds. They feel the system is over-explaining.

Constrain max tokens. Set the maximum output length to 50, 75, or 100 tokens depending on your use case. The model stops generating when it hits the limit. Responses are shorter. TTS is faster. Latency drops. The risk is truncation. If the model needs 120 tokens to fully answer the question, a 100-token limit cuts off the response mid-sentence. The audio ends abruptly. The user is confused. You must choose a limit high enough to allow complete answers but low enough to prevent verbose rambling.

Use prompt instructions to encourage brevity. Add a system message: "You are a voice assistant. Keep responses under 3 sentences. Be direct and concise." The model learns to generate shorter responses without hitting token limits. The risk is quality degradation. A model trained to be thorough will produce worse answers when forced to be brief. You must test whether brevity instructions reduce quality below your acceptable threshold.

Monitor response length. Log the token count of every LLM response. Calculate p50 and p95 response lengths. If p95 exceeds 150 tokens, your system is generating responses too long for comfortable voice playback. A 150-token response synthesizes to roughly 15 to 20 seconds of audio. Most users interrupt before 10 seconds. You are generating audio that users never hear. You are wasting TTS cost and latency budget on tokens that do not matter.

## Model Selection Decision Tree: Fast, Cheap, or Smart

You cannot have all three. You choose two. Fast and smart is expensive. Fast and cheap is low quality. Smart and cheap is slow. The decision tree is simple.

If your latency budget is less than 1.5 seconds end-to-end, choose a fast model. GPT-5-mini, Claude Haiku 4.5, Gemini 3 Nano, Llama 4 Scout. Optimize for TTFT and token generation speed. Accept lower reasoning capability. Test that response quality meets your minimum threshold. If quality is unacceptable, increase your latency budget. You cannot build sub-1.5-second voice AI with frontier models. The physics do not allow it.

If your use case requires deep reasoning, multi-step tasks, or complex domain knowledge, choose a smart model. GPT-5, Claude Opus 4.5, Gemini 3 Pro. Provision sufficient infrastructure to keep TTFT under 500 milliseconds. Accept 2.5 to 3.5 second end-to-end latency. Test that users tolerate the delay. If users describe the system as slow, simplify the use case or switch to a faster model. You cannot build responsive voice AI with frontier models for complex tasks. The user experience breaks.

If your cost budget is constrained and volume is high, choose a cheap model. GPT-5-nano, Llama 4 Scout, or self-hosted open-source models. Optimize infrastructure for high throughput. Accept lower quality and potentially higher latency variance. Test that quality degradation does not harm user experience. If quality is unacceptable, increase your cost budget. You cannot build high-quality voice AI at massive scale on a tiny budget. The economics do not allow it.

Most teams start with a fast model and upgrade to a smarter model only if quality is insufficient. Starting with a frontier model and trying to make it fast is much harder than starting with a fast model and trying to make it smart. Latency is a hard constraint. Quality is a soft constraint. Optimize for the hard constraint first.

The next subchapter examines the TTS provider landscape in 2026—ElevenLabs, Cartesia, Azure Neural, and others—and the tradeoff between naturalness and speed that defines every TTS decision.
