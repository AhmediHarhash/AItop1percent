# 6.1 — Task Success Rate: The Ultimate Voice Metric

Your ASR accuracy is 97%. Your intent recognition hits 94%. Your TTS naturalness scores average 4.2 out of 5. Your latency p95 sits at 380 milliseconds. Every component metric looks excellent. And yet 38% of users who call your voice AI hang up without accomplishing what they called to do.

This is the measurement gap that destroys voice products. Component metrics tell you whether each piece of your system works. Task success rate tells you whether your system works. The difference between these two questions is the difference between shipping a technically impressive demo and shipping something users actually rely on.

Task success rate is the percentage of conversations where the user accomplished their goal. Everything else is input to that outcome. A user doesn't care that your ASR correctly transcribed their utterance if they still couldn't book the appointment, check their balance, or reset their password. They care whether the conversation delivered what they needed. If it didn't, every other metric is academic.

## The Definition Problem: What Counts as Success

Defining task success is harder in voice than in any other AI domain. In a web form, success is binary — the form submitted or it didn't. In a chatbot, you can track whether the user clicked the "Issue Resolved" button. In voice, the user just hangs up. You have to infer from the conversation whether they got what they needed.

The most rigorous definition comes from the conversation's objective state change. A user calls to book an appointment. Success means an appointment was booked. A user calls to dispute a charge. Success means a dispute case was opened. A user calls to check their balance. Success means the balance was stated and confirmed. The task has a concrete outcome. Either that outcome occurred or it didn't.

This objective definition works for transactional tasks. It fails for informational or exploratory tasks. A user calls to ask about loan options. You can't book a loan in one call. Success might mean the user got enough information to make a decision, or it might mean they learned they don't qualify and saved themselves the application effort, or it might mean they decided your rates aren't competitive and that's valuable information too. The outcome is subjective. You can't measure it by checking a database for a state change.

For subjective tasks, success requires user confirmation. At the end of the call, the system asks: "Did I answer your question?" or "Did you get what you needed?" The user says yes or no. You trust their answer. This introduces measurement noise — some users will say yes to be polite even if they're still confused, some will say no because they didn't like the answer even though the answer was correct — but it's the best signal you have. The alternative is guessing based on conversational signals, which is even noisier.

The third category is partial success. The user called to book an appointment, and they scheduled it, but the time they wanted wasn't available so they settled for a second choice. Is that success? Technically yes — the appointment exists. Experientially, maybe not fully. The user accomplished the task but didn't get their preferred outcome. You need a success taxonomy that distinguishes full success from acceptable-but-compromised outcomes. Otherwise you'll optimize for completion rate while user satisfaction slowly erodes.

## Why Component Metrics Decouple from Task Success

Component metrics measure parts. Task success measures the whole. The whole can fail even when every part succeeds because the failure mode is in the integration, the sequencing, or the user's mental model.

A financial services company in early 2025 launched a voice system for account inquiries. ASR accuracy sat at 96.8%. Intent recognition hit 93.2%. Response accuracy on factual questions reached 97.4%. Latency p95 was 340 milliseconds. Every component exceeded their target. Task success rate, measured by whether users successfully completed their inquiry without escalating to a human agent, came in at 61%. The gap between component performance and outcome performance was 30 percentage points.

The problem was conversational flow. The system asked clarifying questions that were individually sensible but cumulatively exhausting. A user would say "I want to check my balance." The system would ask: "Which account?" The user would say "Checking." The system would ask: "You have two checking accounts. Do you mean the account ending in 4829 or 7391?" The user would say "The one I use for bills." The system couldn't resolve that reference and would repeat the account number options. By the fourth turn, users were frustrated enough to request a human agent. The intent was recognized correctly. The information was accurate. The conversation was unbearable.

This is the fundamental integration failure. Each component optimizes for its own task. ASR optimizes for transcription accuracy. Intent recognition optimizes for classification precision. The dialog manager optimizes for unambiguous slot-filling. Nobody optimizes for the user's experience of moving through the conversation toward their goal. The parts work. The whole doesn't.

The second decoupling happens through context loss. Voice conversations are sequential. The user says something in turn three that references something from turn one. If the system doesn't maintain that context, it will ask the user to repeat information they already provided. This doesn't show up in component metrics — the ASR still transcribed correctly, the intent classifier still labeled the utterance — but it destroys task success because the user loses confidence that the system understands them.

A healthcare voice system in mid-2025 allowed patients to refill prescriptions. A patient would say "I need to refill my blood pressure medication." The system would confirm the medication and ask for the pharmacy. The patient would say "The same pharmacy as last time." The system had no memory of previous calls. It would say "Which pharmacy would you like to use?" The patient, now annoyed, would say "I told you last time, the CVS on Main Street." The system couldn't resolve "last time" because it lived in the current session only. The conversation would spiral. Intent recognition was perfect. Context management was absent. Task success suffered.

## Measuring Task Success in Production

The most direct measurement is outcome tracking. For transactional tasks, you instrument the backend systems. When a conversation ends, you check: did an appointment get booked? Did a payment get processed? Did a case get opened? If the conversation included an intent to perform that action, and the action occurred, that's success. If the intent was present but the action didn't occur, that's failure. The metric is the ratio.

This requires intent logging at the conversation level. You can't just track whether an action happened. You need to track whether the user tried to make it happen through the voice system. A user might call, get frustrated, hang up, and complete the action on the web app five minutes later. That's a voice failure even though the action eventually succeeded. You need to attribute the action to the channel that completed it.

For informational tasks, outcome tracking doesn't work. You measure through post-conversation surveys. At the end of the call, the system asks a single question: "On a scale of one to five, how well did this call meet your needs?" You track the percentage of fours and fives. This is your task success proxy. It's subjective, it's gameable, and it's still better than nothing.

The survey timing matters. If you ask immediately after the call ends, you measure satisfaction with the conversation. If you send a survey 24 hours later, you measure whether the information was useful over time. The two metrics differ. A user might feel satisfied in the moment but realize later that the information was incomplete or incorrect. Delayed surveys catch outcome quality. Immediate surveys catch interaction quality. You want both.

The third method is human review. You sample 200 conversations per week. Trained reviewers listen to the full conversation and code it for task success. They answer: did the user accomplish their stated goal? They use a rubric with clear criteria. This is expensive, slow, and the only way to catch nuanced failures that automated metrics miss. It's your ground truth. You use it to validate your automated measurements. If automated task success says 78% and human review says 64%, your automation is overcounting. You recalibrate.

## The Metric That Predicts Task Success: Turn-Level Progress

Task success is a lagging indicator. By the time you know the conversation failed, the user is gone. You need a leading indicator — something you can measure during the conversation that predicts whether it will end in success.

The best predictor is turn-level progress toward goal completion. Every conversation has an implicit goal state. For a booking task, the goal state is: time slot selected, contact info confirmed, appointment created. You can represent this as a set of required slots. Each turn either fills a slot, confirms a slot, or makes no progress. You track the slot-fill rate per turn. If the conversation is making steady progress — turn one fills two slots, turn two fills one more, turn three confirms and completes — the trajectory points toward success. If five turns pass and no slots have been filled, the conversation is stalled. Stalled conversations rarely end in success.

A logistics company in late 2025 built a voice system for delivery rescheduling. They instrumented slot-fill progress. A successful conversation filled six required slots in an average of 4.2 turns. A failed conversation filled an average of 2.1 slots over 6.8 turns before the user escalated or hung up. The pattern was clear: conversations that filled fewer than three slots in the first three turns had a task success rate of 31%. Conversations that filled four or more slots in the first three turns had a task success rate of 89%.

This metric let them intervene in real time. If a conversation wasn't making progress by turn three, the system would switch strategies — offer to transfer to a human, suggest a simpler alternative flow, or proactively confirm what had been understood so far to reset the context. Real-time intervention lifted overall task success from 68% to 79% over eight weeks.

## The Gap Between Completion and Success

Not every completed conversation is successful. Completion means the conversation reached an end state. Success means the user got what they needed. The two diverge when the system delivers the wrong outcome confidently.

A user calls to report a fraudulent charge. The system misrecognizes the intent as "dispute a charge." It walks the user through the dispute flow, creates a case, and confirms completion. The conversation completes. The user's actual need — flagging fraud, freezing the card, issuing a new card — was never addressed. The system measured completion. The user experienced failure.

This is the confidently-wrong failure mode. The system didn't detect its own error. From the system's perspective, intent was recognized, slots were filled, the transaction succeeded. From the user's perspective, the system did the wrong thing. The only way to catch this is through outcome validation. Did the user call back within 48 hours? Did they escalate to a human agent immediately after? Did they leave a low satisfaction score? These are post-conversation signals that flag completion without success.

The more insidious version is partial completion. The user wanted to book an appointment for two people. The system booked one. The conversation ended. The user didn't realize until later that only one appointment was created. The system counted this as success because an appointment was booked. The user counted it as failure because the task was incomplete. You only catch this through detailed human review or through customer support ticket analysis weeks later.

## Task Success by User Segment

Not all users succeed at the same rate. Task success rate varies by user experience, task complexity, and environmental context. You need to segment the metric to understand where your system works and where it breaks.

First-time users have lower task success than repeat users. They don't know the system's capabilities, they don't know how to phrase requests, they don't know when to interrupt or when to wait. A voice banking system in 2026 measured 54% task success for first-time callers and 81% task success for users on their fifth call or later. The gap was entirely due to learned behavior. Experienced users knew to speak in short, clear phrases. They knew the system couldn't handle complex multi-part requests. They adapted. New users didn't.

Task complexity predicts success rate. Simple single-intent tasks — "What's my balance?" — succeed at 92%. Multi-step tasks — "I want to transfer money from savings to checking and then pay my credit card bill" — succeed at 64%. The more decision points, the more opportunities for failure. The more slots to fill, the more likely one will be misunderstood or forgotten. Complexity is the enemy of success.

Environmental noise drops success rates. A user calling from a quiet room has an 83% task success rate. A user calling from a car on the highway has a 61% task success rate. The ASR struggles with background noise. The user struggles to hear the TTS over road noise. The conversation degrades. You need to measure task success by acoustic environment and decide: do you support noisy environments with degraded success, or do you detect noise and suggest the user call back from a quieter location?

## The Success-Quality Tradeoff

You can maximize task success by narrowing the task scope. If your voice system only handles balance inquiries, success rate will be high because the task is simple. If it handles the full range of banking transactions, success rate will drop because the complexity increases. The tradeoff is between success rate and system utility.

A telecom company in 2025 launched a voice assistant that handled 40 different intents. Task success rate was 59%. They ran an experiment: a restricted version that handled only the top 10 most common intents. Task success rate jumped to 81%. But 30% of callers had needs outside those top 10. They escalated to human agents immediately. The restricted system had higher success on the tasks it handled and lower overall utility.

The optimal strategy is tiered capability with transparent handoff. The system attempts complex tasks but monitors confidence. If confidence drops below a threshold, it hands off to a human before the conversation fails. This maximizes success — simple tasks succeed through automation, complex tasks succeed through escalation — without pretending the system can do everything. The task success metric measures the combined success rate across both automated and human-assisted conversations. The target is 90% or higher for the full set of user needs, not just the easy subset.

## Why Task Success Is the North Star

Task success is the metric that aligns system performance with user value. Every other metric can be gamed. You can optimize ASR accuracy by only supporting a narrow vocabulary. You can optimize latency by caching responses and sacrificing accuracy. You can optimize conversation length by rushing users through flows. None of those optimizations help the user if they don't accomplish their goal.

Task success can't be gamed. Either the user got what they needed or they didn't. You can define it poorly — measuring completion instead of outcome, measuring system state instead of user satisfaction — but if you define it correctly, it forces you to optimize for the right thing. It forces you to care about the user's experience end-to-end, not just the performance of individual components.

The companies that treat task success as the north star build different systems than companies that optimize component metrics. They invest in context management because losing context kills success. They invest in conversational repair because misunderstandings are inevitable and recovery is what separates success from failure. They invest in user research to understand what "success" actually means for their users, which varies by domain and by task. They build systems that work, not systems that score well on benchmarks.

Task success rate is the ultimate voice metric because it measures the only thing that matters: did the system deliver value? Everything else is commentary.

Next, we'll examine first call resolution — the outcome metric that matters most to businesses and the one most tightly coupled to cost savings.
