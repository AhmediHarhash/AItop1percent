# 8.2 — Pre-TTS Content Filtering: Catching Harm Before Speech

Pre-TTS content filtering is the last checkpoint before text becomes irreversible audio. After the LLM generates a response, before the TTS engine speaks it, you have one final opportunity to block harmful content. This is your primary safety control. If this filter fails, the user hears the violation. If this filter is too slow, your time-to-first-audio target breaks. If this filter generates too many false positives, your conversations feel broken and unnatural. The entire voice safety architecture balances on this component.

The challenge is that you are operating under constraints that do not exist in text moderation. You have a latency budget measured in tens of milliseconds, not hundreds. You are often evaluating partial responses because you start TTS before the LLM finishes generating the full answer. You cannot ask the user to wait — every millisecond of delay is perceived as system failure. The moderation decision must be fast, accurate, and made with incomplete information. This subchapter defines how pre-TTS filtering works, where it breaks, and how to build it so it actually protects users without destroying the experience.

## Synchronous vs Asynchronous Moderation and the Latency Tradeoff

The first decision is whether moderation blocks TTS or runs in parallel with it. Synchronous moderation means you wait for the filter's verdict before starting speech. The user hears nothing until you confirm the content is safe. Asynchronous moderation means you start TTS immediately and log violations after the fact. The user hears the response while moderation runs in the background.

Synchronous moderation is safer. If the filter detects a violation, you never speak it. The user never hears it. The harm is prevented. But synchronous moderation adds its full latency to your TTFA. If your moderation API takes 90 milliseconds and your TTFA budget is 250 milliseconds, you just consumed 36% of your total budget on safety. You have 160 milliseconds left for LLM inference, TTS synthesis, and network overhead. That is tight. If your LLM takes 180 milliseconds to generate the first chunk of tokens, you have already blown your target before speech even starts.

Asynchronous moderation is faster. You start TTS as soon as the LLM produces tokens. Your TTFA is not penalized by moderation latency. But if moderation detects a violation after TTS has started, the user hears part of the harmful content before you can stop. You can cut the audio mid-sentence, but the damage is partial rather than prevented. In early 2025, a customer support voice agent used asynchronous moderation and started speaking a response that included a customer's Social Security number. The moderation system flagged the PII violation 340 milliseconds into the response. By then, the agent had spoken six of the nine digits. The company's log showed the violation was detected. The user's recording showed the violation was heard.

The tradeoff is not abstract. It is a choice between two bad options: accept higher latency and risk users perceiving the system as slow or broken, or accept lower latency and risk users hearing content that should have been blocked. Neither option is good. Most production systems use a hybrid approach: synchronous moderation for high-risk content categories where false negatives are unacceptable, asynchronous moderation for lower-risk categories where speed matters more. You define which categories are which based on your risk tolerance and your user base.

A healthcare voice assistant runs synchronous moderation for medical advice, PII, and self-harm content. The system will not speak anything in those categories until moderation confirms it is safe, even if that adds 150 milliseconds to TTFA. For general conversational content, the system uses asynchronous moderation because a rare false negative in small talk is less damaging than consistent multi-second delays. The risk prioritization is explicit. The latency cost is known. The choice is documented.

## Token-Level Streaming Moderation: Checking as the LLM Generates

Modern LLM APIs stream responses token by token rather than waiting for the full completion. This allows you to start TTS earlier, reducing perceived latency. But it also means your content filter must operate on partial responses. You do not have the full sentence. You do not know how the thought will end. You must decide whether the tokens you have seen so far are safe to speak, knowing that the next token might change the meaning entirely.

Token-level streaming moderation evaluates content as it arrives. Every chunk of tokens from the LLM is passed to the moderation API. If the filter flags the chunk as unsafe, you stop TTS and discard the rest of the response. If the chunk is safe, you pass it to TTS and wait for the next chunk. This approach minimizes latency — you are moderating and speaking nearly in parallel — but it creates edge cases where partial content appears safe but the complete content is harmful.

Consider the response "You should not stop taking your medication without consulting your doctor." The first six tokens are "You should not stop taking your." A strict moderation filter might flag this as the beginning of dangerous medical advice because the phrase "you should not stop taking your" sets up a directive about medication. The full sentence is actually safe — it correctly advises consulting a doctor — but the partial sentence looks risky. If you block based on the partial content, you generate a false positive. If you wait for the full sentence, you add latency.

The inverse failure mode is also possible. The partial content looks safe, so you start speaking it, but the full sentence turns unsafe. "It is generally fine to skip a dose if you are experiencing side effects." The first twelve tokens are "It is generally fine to skip a dose if." That might pass moderation because it is a bland, incomplete statement. The remaining tokens "you are experiencing side effects" turn it into medical advice that contradicts prescription instructions. If you started TTS after the first chunk, the user hears "It is generally fine to skip a dose" before your moderation system sees the full context and realizes the advice is unsafe.

Token-level moderation requires buffering strategies. You do not evaluate every single token in isolation. You collect tokens into chunks — typically 10 to 30 tokens, enough to form one or two sentences — and evaluate each chunk. The larger your buffer, the more context your moderation system has, and the fewer false positives or missed violations. But larger buffers also delay TTS. A 30-token buffer might represent 120 milliseconds of LLM generation time. That is 120 milliseconds added to TTFA before you even start moderation.

## Buffering Strategies: How Many Tokens Before You Speak

The buffering decision is one of the most consequential choices in voice safety architecture. You must decide how many tokens to collect from the LLM before passing them to TTS. This is not a technical detail. It is the hinge point between speed and safety.

If you buffer zero tokens, you achieve maximum speed. The instant the LLM produces its first token, you send it to TTS. Your latency is minimized. But your moderation sees one token at a time with no surrounding context. The filter cannot assess semantic meaning. It can only look for specific banned words or phrases. Any violation that depends on context will slip through. A single-token buffer is fast and useless.

If you buffer the entire response, you achieve maximum safety. Your moderation system sees the complete answer. It can evaluate full sentences, check for contradictions, assess tone and intent. But you sacrifice streaming entirely. The user hears nothing until the LLM finishes generating, moderation completes, and TTS processes the full response. Your TTFA might be three or four seconds. The experience feels like early 2010s voice assistants: ask a question, wait in silence, hear a response. That is not conversational. That is interactive voice response.

Production systems use intermediate buffers. A common pattern is a 15-token first-chunk buffer followed by 10-token incremental buffers. The first 15 tokens usually contain at least one full sentence. That gives moderation enough context to make a meaningful safety determination. After the first chunk passes moderation and starts speaking, subsequent chunks are smaller to reduce incremental latency. This pattern balances context and speed.

The risk is that the first chunk can be safe while later chunks are not. You start speaking based on the first 15 tokens, which describe a problem. The next 20 tokens offer a solution that violates policy. Moderation catches it in the second chunk, but the first chunk is already spoken. You cut the audio mid-response. The user hears an incomplete answer, followed by silence or an apology. The experience is broken. You prevented the full violation, but you did not prevent the interruption.

A financial services voice agent used a 20-token first-chunk buffer and 8-token incremental buffers. The system received a query about investment strategies. The LLM's first chunk explained that diversification reduces risk — safe content, passed moderation, started TTS. The second chunk recommended specific stock tickers, which violated the company's policy against providing individualized investment advice. Moderation caught it, stopped TTS, and played a fallback message: "I cannot provide specific stock recommendations." The user heard "Diversification reduces risk by spreading your investments across I cannot provide specific stock recommendations." The transcript shows two coherent parts separated by a non-sequitur. The user complained that the system malfunctioned mid-answer.

## False Positive Costs in Voice: Silence Is Worse Than in Text

False positives in text moderation are annoying. The user types a message, hits send, and sees "Your message cannot be sent because it may violate our content policy." The user edits the message or rephrases the query. The conversation continues. The friction is low. False positives in voice are conversation-breaking.

When a false positive occurs in pre-TTS moderation, one of three things happens. The system generates no audio at all, leaving the user in silence, wondering if the call dropped. The system plays a generic fallback message like "I am sorry, I cannot help with that," which makes no sense if the user's request was benign. Or the system cuts off mid-sentence because a later chunk was incorrectly flagged, producing the broken-response experience described above. All three outcomes feel like system failure.

False positives also compound over a conversation. In text chat, a single false positive is one blocked message in a thread. The user sees context from previous messages and can figure out what happened. In voice, a single false positive is an interruption in the audio flow. The user has no visual context. They do not see a content policy warning. They just hear silence or a non-sequitur. If false positives happen multiple times in one call, users hang up. In 2025, a customer service voice deployment had a 9.4% false positive rate in pre-TTS moderation. The average call length dropped 31% because users ended calls they thought were broken.

This means voice systems require much higher precision than text systems. A text moderation filter with 92% precision might be acceptable if recall is very high. A voice moderation filter with 92% precision is unusable. If one in twelve safe responses is blocked, your user experience is shattered. You need precision above 98% for inline moderation in production voice. Anything below that and you are choosing between shipping an unsafe system or shipping a system users perceive as broken.

The only way to achieve high precision without sacrificing recall is to tune your moderation model on voice-specific data. Text moderation models are trained on text content that users type into forms, chat boxes, and comment sections. Voice moderation models must be trained on text that LLMs generate for speech. The distributions are different. LLMs produce more formal, grammatically complete sentences than human chat messages. They use different vocabulary. They structure reasoning differently. A moderation model trained on human-authored text will have more false positives on LLM-generated content because it is encountering distribution shift.

## Production Architectures for Sub-100ms Filtering

To achieve moderation latencies below 100 milliseconds, you need architectural choices that prioritize speed without gutting safety. The naive approach — call a third-party moderation API for every chunk — will not work. Network round-trip time alone can consume 40 to 80 milliseconds depending on geography. Add API processing time and you are at 120 to 200 milliseconds per chunk. That is too slow.

Production architectures run moderation on-premise or in the same cloud region as the voice infrastructure. You deploy a moderation model inside your own network, eliminating external API latency. The model runs on GPU inference servers co-located with your TTS servers. Network latency drops to single-digit milliseconds. Model inference becomes the dominant cost. If you use a small, fast classifier fine-tuned for your specific content policy, inference can complete in 15 to 40 milliseconds. That is fast enough for inline moderation.

The tradeoff is that you own the model. You must train it, host it, version it, and maintain it. You cannot outsource the problem to a moderation API vendor. You need ML infrastructure and safety ML expertise. For many teams, this is the first time they are running production ML inference that is not the LLM itself. It is a steep operational lift. But it is the only way to hit sub-100ms moderation latencies at scale.

Some teams use tiered moderation. A fast, local rule-based filter runs first and catches obvious violations in under 10 milliseconds. Banned phrases, regex patterns, known bad outputs. If the rule-based filter passes, a lightweight ML classifier runs next, adding another 25 milliseconds. If both pass, the content is cleared for TTS. If either fails, the response is blocked. This approach achieves very low latency for content that clearly passes or clearly fails, and reserves the more expensive ML model for ambiguous cases. The downside is that you are maintaining two moderation systems instead of one, and the rule-based filter has high precision but low recall. It catches only the violations that exactly match known patterns.

Another architectural choice is parallel moderation. Instead of running moderation serially before TTS, you start TTS immediately and run moderation in parallel. TTS begins synthesizing the audio, but you do not stream the audio to the user until moderation completes. If moderation clears the content, you release the buffered audio. If moderation flags a violation, you discard the audio and generate a fallback response. This approach hides moderation latency from TTFA — TTS and moderation happen concurrently — but it wastes TTS compute on responses you might never send. If your moderation filter blocks 3% of responses, you are running TTS on 3% more content than you ultimately deliver. That cost is acceptable if it keeps TTFA low.

## Evaluating Pre-TTS Filters on Production Traffic Patterns

Your moderation filter must be evaluated on the content it will see in production, not on generic benchmark datasets. Generic text moderation benchmarks are built from human-authored text: social media posts, forum comments, chat messages. Your voice agent speaks LLM-generated text. The distribution is different. The vocabulary is different. The sentence structure is different. A model with 96% recall on a benchmark might have 89% recall on your LLM's outputs.

You evaluate by collecting real LLM responses from production or staging traffic, labeling them for policy violations, and measuring your filter's performance on that labeled set. This requires building your own eval dataset. There is no shortcut. You need hundreds or thousands of real responses, annotated by human reviewers who understand your content policy. The annotation task is "Would it be harmful if the voice agent spoke this response aloud to a user?" not "Does this text contain offensive language?" The question is context-specific and modality-specific.

Your eval set must include edge cases: partial sentences, responses cut mid-thought, responses with ambiguous phrasing, responses that are safe in text but would sound inappropriate when spoken. You also need adversarial examples: responses designed to pass moderation while still being harmful. These are rare in organic traffic, so you red-team your own system to generate them. An adversarial example might rephrase a banned medical instruction using euphemisms or indirect phrasing that a keyword filter would miss but a human would recognize as harmful.

You also must evaluate false positive rate, not just recall. Measure how often your filter blocks safe responses. If you are running synchronous inline moderation, every false positive is a conversation interruption. Your acceptable false positive rate depends on your TTFA budget and your user tolerance. A system with a 400-millisecond TTFA target might tolerate a 2% false positive rate if the alternative is 5% of responses containing violations. A system with a 200-millisecond target and a consumer user base might need false positives below 0.5% because users will perceive the system as broken.

## The Impossible Tradeoff and How Production Teams Resolve It

Pre-TTS moderation is an impossible tradeoff. You need high recall to prevent harm. You need high precision to avoid breaking conversations. You need low latency to meet TTFA targets. Optimizing all three simultaneously is not possible with current technology. Every production system makes a choice about which constraint to relax.

Some systems relax recall. They accept that 2-4% of policy violations will be spoken because blocking them would require moderation latencies that destroy TTFA. They rely on post-hoc monitoring and user reporting to catch violations after the fact. This works if the consequences of a spoken violation are low — general conversation, entertainment, non-critical domains. It does not work for healthcare, financial advice, legal guidance, or child-facing applications. For high-stakes domains, recall cannot be sacrificed.

Some systems relax precision. They accept that 5-8% of safe responses will be blocked or interrupted because preventing violations is more important than smooth conversations. They use aggressive moderation models that err on the side of blocking. The user experience suffers. Call completion rates drop. Users complain about interruptions. But policy violations are rare. This works if your users are captive — internal business users, regulated domains where alternatives do not exist — and you can tolerate user frustration in exchange for safety.

Some systems relax latency. They run synchronous moderation and accept that TTFA will be 400 to 600 milliseconds instead of 200 to 250. They are willing to sacrifice conversational naturalness for safety. Users notice the delay, but the system still feels usable. This works for professional use cases, B2B deployments, domains where users expect some latency. It does not work for consumer-facing conversational agents where users compare your experience to Siri, Alexa, or Google Assistant.

The resolution strategy depends on your domain, your users, and your risk tolerance. But the tradeoff is always there. Pre-TTS moderation is the last line of defense before content becomes irreversible audio. The next subchapter covers the audit layer: real-time transcription monitoring that catches violations after they are spoken but before the call ends.

