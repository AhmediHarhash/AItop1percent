# 13.4 — LLM Timeout Recovery: The Filler-Then-Retry Pattern

When the LLM is too slow, silence fills the conversation. In a text interface, the user sees a loading indicator and waits. In a voice interface, the user hears nothing and assumes the system has crashed, disconnected, or stopped listening. Silence in voice is not neutral — it is failure. You have roughly 1.5 seconds of acceptable silence before the user starts to worry and 3 seconds before they interrupt or hang up. LLM timeout recovery is about filling that silence productively while you attempt to get a response.

The challenge is that LLM latency is variable and unpredictable. A request that normally completes in 800 milliseconds might take 3 seconds if the model is under load, if the prompt is unusually complex, or if the generation includes a long output. You cannot know in advance which requests will be slow. You must detect slowness in real-time and recover dynamically.

## Filler Strategies: Buying Time Without Sounding Broken

The filler-then-retry pattern works like this: you send a request to the LLM with a latency budget — typically 1.2 to 1.5 seconds for voice. If the LLM does not respond within that budget, you play a filler phrase to acknowledge the user's request and signal that processing is ongoing. While the filler is playing, you continue waiting for the LLM response. If the response arrives before the filler finishes, you transition smoothly into the response. If the response does not arrive, you play another filler or escalate to a fallback.

Filler phrases must sound natural and buy meaningful time. Generic fillers like "One moment" or "Let me check on that" buy 1.5 to 2 seconds. More elaborate fillers like "Let me look into that for you — this might take a moment" buy 3 to 4 seconds. The longer the filler, the more time you buy, but the more awkward it sounds if the LLM response was actually fast and you did not need the filler.

A customer service voice bot in mid-2025 used a two-tier filler system. If the LLM did not respond within 1.2 seconds, the system played a short filler: "One moment." This bought an additional 1.8 seconds. If the LLM still did not respond, the system played a longer filler: "I'm checking our system for you." This bought another 2.5 seconds. The total timeout before escalation was 5.5 seconds — 1.2 seconds of initial wait, 1.8 seconds for the first filler, 2.5 seconds for the second filler. Ninety-one percent of LLM requests completed within this window. The remaining 9% escalated to a fallback response or human transfer.

Filler phrases should be pre-recorded and pre-rendered as TTS audio so they can be played with zero latency. If you wait until the timeout threshold to generate the filler with TTS, you add 200-400ms of TTS rendering time, which defeats the purpose of the filler. Pre-rendering also ensures consistent quality and pronunciation, which matters when the filler is played frequently.

The tone of the filler affects user perception. Apologetic fillers — "Sorry, this is taking a moment" — acknowledge the delay and preserve goodwill. Neutral fillers — "Let me check that" — treat the delay as routine. Confident fillers — "I'm looking that up for you now" — frame the delay as productive work rather than system slowness. In most cases, neutral or confident fillers perform better than apologetic ones. Apologizing for every slow response trains users to expect failures. Framing delays as normal processing trains users to be patient.

## When to Use Filler vs When to Wait Silently

Fillers are not always the right choice. In some cases, waiting silently is better. The decision depends on the expected delay, the user's mental state, and the conversational context.

If the expected delay is under 1.5 seconds, wait silently. Fillers that play unnecessarily make the system feel slower than it is. If the LLM responds in 1.3 seconds and you play a filler at 1.2 seconds, the user hears "One moment" followed immediately by the response. The filler adds no value and makes the system sound hesitant. Silent waits under 1.5 seconds feel like natural conversational pauses. Fillers during those pauses feel like interruptions.

If the user just asked a complex question that clearly requires processing, wait silently for up to 2 seconds. The user expects a thoughtful response to a complex question and will tolerate a brief pause. If the user asked a simple question that should have an instant answer, use a filler after 1 second. The user expects speed for simple questions, and delays feel wrong.

If the conversation is high-stakes or emotionally charged, use fillers aggressively. A user calling about a billing dispute, a medical issue, or a service outage is already anxious. Silence increases anxiety. Fillers provide reassurance that the system is working and has not abandoned the user. In these contexts, even short delays benefit from acknowledgment.

A mental health support voice assistant in late 2025 used context-aware filler thresholds. For general conversation and check-ins, the filler threshold was 1.5 seconds. For crisis-related queries where the user was expressing distress, the filler threshold was 0.9 seconds. The faster acknowledgment in crisis contexts reduced user anxiety and hang-up rates. Users in distress needed more frequent reassurance that the system was listening and responding, even if responses were still forming.

## The Retry Decision: Same Prompt, Different Model, or Fallback

When the LLM times out, you must decide what to do next. The three options are: retry the same request with the same model, retry with a different faster model, or abandon the LLM and use a fallback response.

Retrying the same request with the same model only makes sense if the timeout was caused by transient load, not by request complexity. If the LLM is under heavy load and requests are queuing, retrying immediately will likely hit the same queue and fail again. If the timeout was caused by a spike in traffic that has since cleared, retrying may succeed. The way to distinguish these cases is by tracking recent timeout rates. If timeout rate in the last 30 seconds is below 5%, retry. If timeout rate is above 20%, do not retry — the system is overloaded and retries will make it worse.

Retrying with a different faster model is the most common recovery path in 2026. Most production systems have access to multiple LLMs with different latency and quality profiles. If your primary model is GPT-5.2 with an average latency of 1.1 seconds and a P95 latency of 2.8 seconds, your fallback model might be GPT-5-mini with an average latency of 480ms and a P95 latency of 900ms. When the primary model times out, you switch to the faster model. The response quality may be slightly lower, but the response is delivered within the acceptable latency window.

A travel booking voice assistant in early 2026 used a three-tier model cascade. Tier 1 was Claude Opus 4.5, optimized for quality and handling complex multi-step booking logic. Tier 2 was Claude Sonnet 4.5, faster but slightly less capable. Tier 3 was a fine-tuned Llama 4 Scout model, very fast but limited to scripted booking flows. The system started with Opus. If Opus did not respond within 1.4 seconds, it played a filler and switched to Sonnet. If Sonnet did not respond within 1.2 seconds, it switched to the fine-tuned Llama model. Ninety-six percent of requests were handled within the acceptable latency window, with 68% served by Opus, 24% served by Sonnet, and 4% served by Llama. The cascade ensured that latency targets were met while maximizing the use of the highest-quality model.

Fallback responses are pre-written responses that do not require LLM generation. They are used when all models time out or when the request is so simple that an LLM is unnecessary. Fallbacks are fast — they can be retrieved and played in under 200ms — but they are inflexible. They work well for common requests that have predictable answers: "What are your hours?" "How do I reset my password?" "What is your return policy?" They do not work well for dynamic requests that require personalization or real-time data.

## Timeout Budget: How Long to Wait Before Recovery

The timeout budget is the maximum time you wait for an LLM response before triggering recovery. Setting this budget requires balancing latency and success rate. A short timeout budget — 1 second — ensures low latency but causes frequent timeouts and frequent fallback to lower-quality models or responses. A long timeout budget — 3 seconds — reduces timeouts but makes the system feel slow.

The optimal timeout budget depends on your P95 LLM latency. If your primary model's P95 latency is 1.8 seconds, setting the timeout budget at 1.5 seconds will cause timeouts on 5% of requests even when the system is healthy. Setting it at 2.0 seconds will allow 95% of requests to complete without timeout. But 2.0 seconds is uncomfortably long in a voice interface. The tradeoff is whether you accept a 5% timeout rate to keep latency under 1.5 seconds, or tolerate 2.0 second latencies to avoid unnecessary timeouts.

Most production systems in 2026 set the timeout budget at the P90 latency — the latency at which 90% of requests complete. This causes timeouts on 10% of requests, which is manageable with a good fallback strategy, while keeping the majority of interactions feeling fast. If your P90 is 1.4 seconds, set the timeout at 1.4 seconds. If your P90 is 2.1 seconds, you have a latency problem that cannot be solved with timeout tuning — you need a faster model.

A healthcare voice assistant in mid-2025 initially set its timeout budget at the P95 latency of 2.3 seconds. User feedback indicated that the system felt "slow" and "unresponsive." The team lowered the timeout budget to the P80 latency of 1.6 seconds. Timeout rate increased from 5% to 20%, but fallback responses were well-designed and users rarely noticed the quality difference. Perceived responsiveness improved significantly, and user satisfaction scores increased by 18 percentage points. The lesson was that speed is more important than avoiding timeouts. Users tolerate occasional fallbacks. They do not tolerate slow responses.

## The User Perception of Filler vs Silence

Users perceive filler and silence differently depending on context. In human conversation, brief pauses are normal and interpreted as thinking time. Longer pauses are awkward and interpreted as confusion or distraction. Fillers like "um," "let me think," or "one moment" signal active processing and maintain conversational flow during long pauses.

Voice AI inherits these same perceptual rules. A 1-second silence after a user's question feels like natural processing time. A 3-second silence feels like the system has frozen. A filler phrase played at the 1.5-second mark transforms the 3-second delay from "frozen system" to "system working on it." The difference is entirely in user perception, not in actual system behavior.

Fillers also serve as implicit feedback that the system heard the user's request. When the user speaks and the system immediately responds with "Let me check that," the user knows their input was received and is being processed. When the user speaks and the system is silent for 2 seconds, the user wonders whether the system heard them or whether they need to repeat themselves. Fillers eliminate this ambiguity.

But fillers have a cost. Overuse trains users to expect delays. If the system plays a filler on every single request, users learn that the system is slow and start mentally allocating extra time for each interaction. If the system plays fillers only on genuinely slow requests, users perceive those cases as exceptions rather than the norm.

A retail voice assistant in late 2025 played fillers on 60% of requests because the timeout threshold was set conservatively at 1.0 seconds and the median LLM latency was 1.1 seconds. Users described the system as "sluggish" and "always thinking" even though the actual response times were comparable to competitors. The team raised the timeout threshold to 1.4 seconds, reducing filler frequency to 15% of requests. Users described the new system as "snappy" and "responsive" even though the average end-to-end latency increased by 120ms. The perception of speed improved because fillers were rare enough to feel exceptional rather than routine.

## Filler Diversity: Avoiding Robotic Repetition

Hearing the same filler phrase repeatedly makes the system sound robotic. If the user has three slow requests in a single conversation and hears "One moment" three times, the system feels scripted and inflexible. Filler diversity — varying the filler phrases across a conversation — makes the system feel more natural.

Most production systems maintain a pool of 8 to 15 filler phrases with similar meanings and durations. When a filler is needed, the system selects one that has not been used recently in the current conversation. This creates variety without requiring dynamic generation. The phrases are pre-rendered as TTS audio, so playback latency is identical regardless of which phrase is selected.

Common filler pools include:
- Short fillers (1.5-2 seconds): "One moment," "Just a second," "Let me check," "Hold on," "Give me a moment"
- Medium fillers (2.5-3.5 seconds): "Let me look that up for you," "I'm checking on that now," "Let me see what I can find," "I'm pulling that information"
- Long fillers (4-5 seconds): "I'm searching our system for that information," "Let me check our records — this might take a moment," "I'm looking into that for you right now"

The system selects from the appropriate length category based on how much time it needs to buy. If the LLM has already been waiting for 1.2 seconds and the expected time to completion is another 1-2 seconds, use a short filler. If the expected time is 3-4 seconds, use a medium or long filler.

A financial services voice assistant in early 2026 used a 12-phrase filler pool with smart selection based on conversation history. The system tracked which fillers had been used in the current call and avoided repeating any phrase unless all 12 had been used. In testing, users exposed to repeated fillers rated the system's naturalness at 6.2 out of 10. Users exposed to diverse fillers rated naturalness at 7.8 out of 10. The same system, with the only difference being filler variety, was perceived as significantly more human-like.

## When Filler Fails: Escalation Paths

If fillers do not buy enough time — the LLM is so slow that even the longest filler expires before the response is ready — you must escalate. The escalation paths depend on the task and the user's tolerance.

The first escalation is to apologize and provide a concrete next step. "I'm sorry, this is taking longer than expected — I'm still working on it." This buys another 2-3 seconds and sets the expectation that the delay is unusual. If the LLM response arrives during this window, the conversation continues. If not, you escalate again.

The second escalation is to offer an alternative. "This is taking a while — would you like me to send this information to your email instead?" or "I can have someone call you back with this information — would that work?" This gives the user control and prevents frustration from building. Many users will accept the alternative rather than continuing to wait.

The third escalation is to fallback to a human agent or to a simplified response that does not require LLM generation. "I'm having trouble retrieving that information right now — let me transfer you to someone who can help," or "I'm unable to access that detail at the moment, but I can tell you the general policy." This is the last resort, reserved for cases where LLM failure is complete rather than just slow.

A government services voice assistant in mid-2025 had a four-tier escalation for complex eligibility questions. Tier 1: wait 1.5 seconds silently. Tier 2: play filler and wait another 2.5 seconds. Tier 3: apologize and offer to send detailed information via mail. Tier 4: transfer to a human case worker. The system reached Tier 3 in 4% of calls and Tier 4 in 1.5% of calls. The escalation strategy prevented user frustration and ensured that no call was abandoned due to LLM latency.

## The Role of Streaming in Timeout Recovery

Streaming LLM responses — where the model returns tokens incrementally as they are generated rather than all at once — dramatically improves timeout recovery. With streaming, you can start playing the first part of the response while the rest is still generating. This reduces perceived latency because the user hears the beginning of the answer within the timeout window, even if the full answer takes longer.

Streaming also allows early detection of slow responses. If the LLM has generated 10 tokens in the first 800ms, you can estimate the total generation time based on token velocity and decide whether to cancel and switch to a fallback. If the LLM has generated zero tokens in the first 1.2 seconds, you know the request is stalled and can switch immediately rather than waiting for a full timeout.

A customer support voice assistant in late 2025 used streaming with token-velocity-based timeout detection. The system started playing the response as soon as the first 8-12 tokens were available, which typically happened within 600-900ms. If token generation slowed to fewer than 15 tokens per second, the system assumed the model was struggling and switched to a fallback mid-generation. This kept 97% of responses within the 1.5-second latency target, compared to 84% without streaming.

Streaming also enables partial fallback. If the LLM generates the first half of a response quickly but stalls on the second half, you can play the first half and then append a generic conclusion rather than regenerating the entire response. "Your account balance is 1,247 dollars and your last payment was received on March 3rd. For more details, you can check your online account or speak with a representative." The first sentence is LLM-generated. The second sentence is a pre-written fallback appended when the LLM times out mid-response.

LLM timeouts are inevitable in production voice systems. Filler-then-retry patterns, timeout budgets, and escalation paths ensure that users experience these timeouts as brief, managed delays rather than system failures. The final recovery challenge is TTS failure, where the audio rendering breaks and you must decide whether to retry, skip, or replace the problematic segment.

