# 6.8 — The Repetition Problem: When the Agent Keeps Asking

"I already told you that." This is the sentence that signals total failure in a voice agent. The user provided information. The system either did not capture it, did not store it, or cannot retrieve it. Now the system is asking again. The user feels unheard. Trust collapses instantly. This is not a transcription problem. This is a memory architecture problem, and it is the most common reason users abandon voice agents in favor of human representatives.

In November 2025, a health insurance voice agent launched with a 78% task completion rate. Within three weeks, it dropped to 61%. The primary driver was repetition. The agent asked for member ID, date of birth, and reason for calling in the first 30 seconds. Then, when transferring context to a specialist agent for prescription coverage questions, the specialist agent asked for member ID and date of birth again. The user had already provided this information. The second request triggered escalation to a human in 44% of cases. The system had the information. It failed to pass it between agents.

The repetition problem manifests in three forms: **asking for information already provided in the current session**, **asking for information the system should know from prior sessions or account data**, and **asking for information the user provided but the system failed to extract**. All three destroy trust. All three are preventable. None of them are acceptable in production.

## Session Memory Architecture and Information Persistence

Session memory is the data structure that persists across turns within a single conversation. When the user says "my account number is 8472," the session memory stores account_number: 8472. When the agent later needs the account number to execute an action, it retrieves it from session memory instead of asking again. If session memory does not persist correctly, the system forgets and re-prompts.

A banking voice agent in late 2025 used stateless turn-by-turn processing. Each user utterance was processed independently. The model received the user's message and the agent's previous response, but no structured memory of extracted information. User: "I want to transfer $200 from checking to savings." Agent: "Okay, how much would you like to transfer?" The user just said $200. The system heard it — the transcription was perfect. The entity extraction identified $200. But the entity was not written to persistent memory. The model generated the next response without access to it.

The fix is structured session state that survives across turns. Every extracted entity is written to a session object: amount: 200, source_account: "checking", destination_account: "savings". Every turn, the agent's prompt includes the current session state as context. When generating the next response, the model sees "session state: amount=200, source_account=checking, destination_account=savings." It does not ask for the amount again. It proceeds to the next required field or confirms the action.

Session state must be append-only for information and update-only for corrections. When the user provides new information, append it to the session state. When the user corrects information, update the relevant field. Never clear the session state unless the user explicitly starts a new task. "Actually, make that $300" updates amount: 300. "Cancel that, I want to pay a bill instead" clears the transfer-related state and initializes bill payment state.

A customer support voice agent in early 2026 rebuilt their session memory architecture after measuring repetition rate at 11%. They implemented a key-value session store that persisted across all turns. Every entity extracted from user speech was written to the store with a timestamp. Every agent response was generated with access to the full session store. When the agent needed information, it checked the store first. If the information existed, it used it. If not, it asked once and stored the response. Repetition rate dropped to 0.8%. Task completion rate increased from 72% to 89%.

Session memory must be scoped correctly. If the user is handling two tasks in one call — paying a bill and checking account balance — the session memory must separate the two contexts. Paying a bill requires amount, biller, and payment date. Checking balance requires account type. Mixing these into one flat session state creates confusion. The user says "checking" in the context of checking their balance, and the system incorrectly stores it as the source account for the bill payment.

The solution is task-scoped session memory. Each task gets its own memory namespace. When the user says "I want to check my balance," the system initializes a balance_check task with its own state object. When the user says "and also pay my electric bill," the system initializes a bill_payment task with a separate state object. The two do not interfere. When the user provides information, the system writes it to the currently active task's state.

## Cross-Session Memory and Account Context

Some information persists across sessions. The user's name, account number, address, phone number, and account history are stored in your database. The agent should not ask for information you already have. If the user is authenticated, you know their account number. Do not ask for it again.

A telecom voice agent in mid-2025 asked every caller for their phone number, even when the caller was calling from that phone number and the system had caller ID. The system had the phone number before the user spoke a single word. Asking for it felt incompetent. Twelve percent of users expressed frustration in their response: "You should already have that," "I'm calling from this number," "Why are you asking me for information you have?"

The fix is account context injection. If the user is authenticated via caller ID, ANI (Automatic Number Identification), or login, retrieve their account data before the first turn and inject it into the session state. The agent's first response should acknowledge the user by name if possible. "Hi Sarah, how can I help you today?" signals that the system knows who they are. It eliminates the need to ask for name, phone number, or account number unless the user is calling about a different account.

Cross-session memory also includes recent interaction history. If the user called yesterday about a billing issue and calls again today, the agent should know. "I see you called yesterday about your bill. Is this about the same issue, or something new?" This acknowledgment proves the system remembers. It reduces repeated explanations and builds trust.

A prescription refill service in late 2025 implemented cross-session context. When a user called, the system retrieved their last three interactions and their active prescriptions. If they called to refill a prescription they had refilled 28 days ago, the agent anticipated the need: "Are you calling to refill your Lisinopril prescription?" If yes, the refill processed in 40 seconds. If no, the user stated the actual reason. The anticipatory question saved an average of 18 seconds per call and reduced repetition of prescription names, dosages, and delivery addresses — all of which were already on file.

Cross-session memory must be accurate and recent. If your account data is stale, you will "remember" incorrect information. A user who moved two months ago and updated their address in the web portal expects the voice agent to know the new address. If the agent confirms the old address, the user must correct it, and the system appears forgetful even though it was trying to be helpful.

Sync account data in real time or near-real time. If a user updates information in one channel, it should be available in the voice channel within seconds, not hours. A retail voice agent in early 2026 synced account data every 60 seconds. A user who updated their email address on the website could call the voice agent 90 seconds later and have the new email confirmed without re-providing it. This level of consistency is table stakes for multi-channel systems.

## Entity Extraction Failures and Implicit Re-Prompting

Sometimes the system asks again because it failed to extract the information the first time. The user said "my account number is 8-4-7-2-3-1-9-0." The ASR transcribed it perfectly. The entity extractor expected a continuous digit string but received a hyphen-separated string. It failed to parse it. The system has the information in the transcript but not in structured form. It asks again.

This is an extraction failure, not a memory failure, but the user experiences it as repetition. They provided the information. The system is asking for it again. The reason does not matter to the user. What matters is that they have to repeat themselves.

A financial services voice agent in late 2025 encountered this with account numbers, routing numbers, and confirmation codes — all of which users spoke in segmented formats. "My account number is 1-2-3-4-5-6-7-8-9-0." The system transcribed "1-2-3-4-5-6-7-8-9-0" but the entity extractor looked for a ten-digit number without hyphens. Extraction failed. The system re-prompted: "I didn't catch that. What's your account number?" The user repeated it, often in the same format, and the system failed again.

The fix is format-tolerant entity extraction. When extracting numeric sequences, strip all non-digit characters before validation. "1-2-3-4-5-6-7-8-9-0" becomes "1234567890". "1 2 3 4 5 6 7 8 9 0" becomes "1234567890". "One two three four five six seven eight nine zero" becomes "1234567890" after digit word normalization. The system accepts all three formats and extracts the same value.

Format tolerance applies to dates, phone numbers, addresses, and names. Dates appear as "March 15," "3/15," "the 15th of March," and "15th." Phone numbers appear as "555-1234," "555 1234," and "5 5 5 1 2 3 4." Addresses appear as "123 Main Street," "123 Main St," and "one twenty-three Main." Names appear with and without middle initials, with and without titles, with and without suffixes. Your entity extractor must normalize all variations into a canonical form.

A healthcare scheduling agent in early 2026 rebuilt their entity extraction with format normalization. Date extraction accepted 14 different input formats. Phone number extraction accepted 8 formats. Address extraction accepted abbreviated and full street types. Name extraction handled titles, middle initials, and suffixes. Repetition due to extraction failure dropped from 9% to 1%. The remaining 1% were cases where the ASR genuinely failed to capture the information due to noise or crosstalk, and re-prompting was appropriate.

## Detecting and Measuring Repetition

Repetition rate is the percentage of clarifications or requests for information that the user already provided earlier in the session. This is different from total clarification rate. A clarification that asks for new information is not repetition. A clarification that asks for information the user gave three turns ago is repetition.

You detect repetition by comparing the current request against the session transcript and session state. If the agent asks "what is your account number?" and the session transcript contains "my account number is 8472" from a previous turn, the request is repetition. If the session state contains account_number: 8472, the request is repetition. If neither the transcript nor the state contains account number information, the request is not repetition.

A customer support voice agent in mid-2025 implemented repetition detection as part of their quality monitoring. Every agent request was checked against session history. If the requested information appeared in the transcript or session state, the system flagged the turn as repetition and logged it. They discovered that 13% of agent requests were repetitions. The breakdowns: 6% were due to session state not persisting correctly, 4% were due to entity extraction failures, 3% were due to the agent asking for information available in account context.

Each root cause required a different fix. Session state persistence was fixed by switching from in-memory state to a Redis-backed session store. Entity extraction failures were fixed by adding format tolerance and phonetic matching. Account context failures were fixed by injecting account data into the session state at conversation start. After all three fixes, repetition rate dropped to 2%, and task completion rate increased by 14 percentage points.

Repetition measurement must distinguish between **explicit repetition** — asking the same question verbatim — and **implicit repetition** — asking for information in a different phrasing. Asking "what's your account number?" followed by "can you provide your account number?" is explicit repetition. Asking "what's your account number?" followed by "which account is this for?" is implicit repetition if the user already specified the account. Both are failures.

You track repetition rate per information type. Account numbers, phone numbers, names, and addresses have different repetition rates because they have different extraction difficulty and storage patterns. If account number repetition rate is 8% but name repetition rate is 2%, focus on improving account number extraction and persistence.

## User Signals of Frustration and Trust Erosion

When the system asks for information the user already provided, the user signals frustration. These signals are detectable and measurable. They include explicit frustration statements, tone changes, response latency increases, and task abandonment.

Explicit frustration statements: "I already told you that," "I just gave you that information," "You asked me this already," "Are you listening?" These appear in 60% to 80% of repetition cases, based on analysis from three large-scale voice agent deployments in 2025. You detect them with keyword matching or sentiment analysis. Any occurrence of "already," "just told," "you asked," or "listening" in proximity to a clarification request is likely frustration.

Tone changes: Users speak more sharply, with higher pitch, faster rate, or increased volume when frustrated by repetition. Acoustic analysis can detect these shifts. A user who was speaking at 140 words per minute and 200 Hz pitch who suddenly jumps to 180 words per minute and 240 Hz pitch after a repetition is signaling frustration. These changes are not always accompanied by explicit frustration statements, but they correlate strongly with negative user experience.

Response latency increases: When the system asks for information the user already provided, the user pauses before responding. They are processing the disconnect between what they said and what the system is asking. A 2-second pause before answering a repeated question is common. This pause is measurable and serves as a signal that the user is confused or frustrated by the request.

Task abandonment: The strongest signal. If the user hangs up, requests a human, or stops responding within 30 seconds of a repetition, the repetition caused abandonment. A telecom voice agent in late 2025 measured abandonment within 30 seconds of repetition at 19%. Compare this to their overall abandonment rate of 6%. Repetition tripled the likelihood of abandonment.

You build a repetition frustration index by combining these signals. Each signal contributes a score. Explicit frustration statement: +3 points. Tone change: +2 points. Response latency over 2 seconds: +1 point. Abandonment: +5 points. A turn with a repetition frustration index above 3 is a critical failure. Above 6 is a user who will not return.

A healthcare voice agent in early 2026 tracked repetition frustration index across all sessions. They found that 11% of sessions included at least one turn with an index above 3. Those sessions had a 34% abandonment rate, compared to 5% for sessions with no high-index turns. They prioritized fixes for the repetition patterns that produced the highest average frustration index. After three months of targeted improvements, high-index turns dropped to 3% of sessions, and abandonment dropped to 7%.

## Preventing Repetition in Multi-Agent Handoffs

Repetition is especially damaging during agent handoffs. The user provides information to Agent A. Agent A determines that Agent B, a specialist, is needed. The conversation transfers to Agent B. Agent B asks for the same information again. The user perceives this as two separate systems that do not communicate. Trust collapses.

A health insurance voice agent in mid-2025 used a multi-agent architecture. A routing agent handled initial triage. Specialist agents handled claims, prescriptions, authorizations, and billing. When the routing agent transferred to a specialist, it passed the conversation transcript but not the structured session state. The specialist agent had access to everything the user said but had to re-extract entities from the transcript. Extraction sometimes failed. The specialist asked for information again. Repetition rate at handoff was 28%. Abandonment rate immediately after handoff was 22%.

The fix is session state handoff, not just transcript handoff. When Agent A transfers to Agent B, it passes the full session state object — all extracted entities, all task context, all user preferences. Agent B initializes with this state pre-populated. It does not ask for information already in the state. The user experiences a seamless transition.

The insurance company rebuilt their handoff protocol. The routing agent extracted member ID, date of birth, reason for call, and any mentioned claim numbers, prescription names, or billing issues. All of this was written to session state. When transferring to a specialist, the routing agent passed the session state as a structured object. The specialist agent received it, acknowledged the context, and proceeded. "I see you're calling about your prescription coverage for Lisinopril. Let me pull up your plan details." Zero repetition. The user heard continuity, not disconnection.

Repetition rate at handoff dropped to 2%. Abandonment immediately after handoff dropped to 6%, close to the baseline abandonment rate. User satisfaction scores for multi-agent interactions increased by 17 points. The change required no new models, no new infrastructure — only a commitment to passing structured context between agents instead of raw transcripts.

## The Zero-Repetition Standard

Repetition is not a "we'll improve this over time" problem. Repetition is a "fix it before launch" problem. Users tolerate many imperfections in voice agents. They tolerate occasional misunderstandings, slow responses, and even task failures if the agent handles them gracefully. They do not tolerate being asked for information they already provided. This is a bright-line failure.

The acceptable repetition rate is zero for information explicitly provided by the user in the current session. If the user said "my account number is 8472," asking for the account number again is unacceptable. The acceptable repetition rate is near-zero for information available in account context. If the user is authenticated and you have their phone number, asking for it is a failure. The only acceptable repetition is re-asking for information that was provided but could not be extracted due to noise, crosstalk, or ASR failure — and even then, the system should acknowledge the failure: "I didn't catch that. Can you repeat your account number?"

A prescription refill service in early 2026 adopted a zero-repetition policy. Any instance of repetition in production triggered an incident review. They analyzed the transcript, the session state, and the extraction logs. They identified the root cause — session state bug, extraction bug, account context bug, or handoff bug — and fixed it within 48 hours. Over six months, they drove repetition rate from 7% to 0.3%. The remaining 0.3% were legitimate re-asks due to ASR failures in noisy environments.

The zero-repetition standard is not idealism. It is respect for the user's time and attention. If you cannot remember what the user told you 30 seconds ago, you are not building an agent. You are building a frustration machine.

The next subchapter covers conversation abandonment — the signals that indicate users are about to hang up, the root causes of abandonment, and how to measure and prevent it.
