# 11.11 — Incident Detection Playbooks for Voice Systems

The voice bot stopped working at 4:17 PM on a Friday. But nobody knew until 6:42 PM when the customer support queue backlog alert fired. For two hours and twenty-five minutes, every caller attempting to use the automated system heard "I'm sorry, I'm having trouble understanding you" and was routed to the hold queue for human agents. The voice recognition API was returning errors, but the error rate monitoring threshold was set for sustained failures, not immediate spikes. The system logged sixteen thousand errors. No alert fired. By the time the on-call engineer investigated, the hold queue had three hundred and twelve waiting callers and the average wait time was forty-seven minutes. The voice bot failure became a customer service disaster.

Voice incidents are different. The conversation is already lost by the time you detect the problem. A user who experiences a failed interaction does not retry immediately. They either wait in the queue for a human agent or hang up frustrated. Every minute of delayed incident detection translates directly to lost conversations, degraded customer satisfaction, and increased support costs. Detection latency for voice systems must be measured in seconds for critical failures and minutes for degradation issues. Detection systems that operate on five-minute or fifteen-minute evaluation windows are too slow. By the time the alert fires, hundreds of conversations have already failed.

## Voice-Specific Incident Categories

Voice system incidents fall into distinct categories that require different detection methods and response procedures. Recognizing which category an incident belongs to determines response urgency and mitigation strategy. The categories are not mutually exclusive. A single root cause can trigger incidents across multiple categories simultaneously, compounding impact and complexity.

Total failure incidents are the easiest to detect and the most urgent to resolve. The voice system stops processing conversations entirely. All calls fail immediately with errors or silence. Detection is straightforward: conversation start rate drops to zero while inbound call attempts remain normal or increase. The gap between call attempts and successful conversation starts is the signal. These incidents require immediate page and all-hands response because every second of downtime is visible to every user.

Partial failure incidents affect a subset of conversations based on some characteristic. One provider fails, affecting only conversations routed to that provider. One intent classification path breaks, affecting only conversations of that type. One region experiences network issues, affecting only callers from that geography. Detection requires segmented monitoring that tracks success rates by provider, by intent, by region, and by device type. Overall success rate might drop from ninety-eight percent to eighty-five percent, which is below threshold but not catastrophic. But success rate for billing intent conversations might drop to twenty percent. The aggregate hides the severity.

Latency degradation incidents occur when the system continues functioning but response times increase beyond acceptable thresholds. Users experience long pauses between speaking and hearing a response. Conversations take twice as long to complete. Users hang up out of frustration even though the system is technically working. Latency degradation is insidious because success rate metrics remain normal. Conversations complete successfully. But user satisfaction plummets. Detection requires latency monitoring with short evaluation windows and thresholds set based on user experience research, not arbitrary technical limits.

Quality degradation incidents are the hardest to detect because the system appears functional on all technical metrics but produces incorrect or inappropriate responses. The voice bot recognizes speech correctly, generates responses confidently, and completes conversations within normal latency, but the responses are wrong. Detection requires semantic quality monitoring and outcome tracking. If resolution rate drops while all technical metrics remain normal, quality degradation is occurring. These incidents often go undetected for hours or days because they are invisible to infrastructure monitoring.

Cost spike incidents occur when operational costs increase dramatically without corresponding increase in conversation volume or quality. Fallback storms, silence billing failures, or retry loops cause costs to multiply. The system functions normally from the user's perspective but bleeds money. Detection requires real-time cost monitoring with thresholds set based on baseline cost per conversation and alerting on per-hour cost anomalies.

Provider outage incidents happen when a third-party speech recognition, synthesis, or telephony provider experiences downtime or degradation. The voice system's own infrastructure is healthy, but its dependencies fail. Detection requires monitoring provider API success rates, latencies, and error types separately from overall system metrics. A provider outage that affects ten percent of traffic might not trigger overall error rate alerts but still represents a significant incident requiring failover to backup providers.

## Detection Patterns: Latency Spikes, Quality Drops, Cost Anomalies

Each incident category has characteristic detection patterns. Effective incident detection uses multiple signals rather than relying on a single metric because no single metric captures all failure modes. The detection system combines real-time metrics, trend analysis, and anomaly detection to catch incidents as early as possible across the full spectrum of failure types.

Latency spike detection monitors percentile latencies at fifteen-second granularity. Median latency is insufficient because it hides tail latencies that affect a minority of users. The detection system tracks ninety-fifth percentile, ninety-ninth percentile, and maximum latency. If ninety-ninth percentile latency exceeds thresholds for more than sixty seconds, an alert fires. The sixty-second window prevents false positives from transient spikes while maintaining fast detection of sustained latency problems. The system also monitors latency by component: recognition latency, LLM inference latency, synthesis latency, and external API latency. Component-specific latency spikes identify which part of the stack is failing.

Latency trend detection catches gradual degradation that percentile thresholds miss. If median latency increases by thirty percent over thirty minutes, an incident is developing even if absolute latency remains below threshold. Trend detection compares current metrics to rolling baselines and alerts when the rate of change exceeds configured limits. A voice system for appointment scheduling detected a memory leak in the synthesis service by monitoring latency trends. Absolute latency was still acceptable, but it was increasing five percent every ten minutes. The trend alert fired thirty minutes before absolute latency crossed the threshold, providing extra time to mitigate.

Quality drop detection combines multiple signals: resolution rate, escalation rate, conversation abandonment rate, and repeat caller rate. If resolution rate drops by more than five percentage points compared to the previous hour, a quality incident is likely. If escalation to human agents increases by more than twenty percent, users are losing confidence in the automated system. If conversation abandonment — users hanging up mid-conversation — increases, the system is failing to meet user needs or is experiencing latency users will not tolerate. No single metric is definitive, but correlated drops across multiple quality metrics confirm an incident.

Semantic quality monitoring uses LLM-based evaluators that sample a percentage of ongoing conversations and score them for coherence, correctness, and helpfulness in near real-time. If the evaluator score drops below threshold for a statistically significant sample, a quality incident alert fires. This detection method catches regressions that outcome metrics miss initially. A customer service voice bot deployed a model update that caused the system to become overly apologetic and verbose. Resolution rates remained stable initially because conversations still completed successfully, but average conversation length increased thirty percent and user satisfaction dropped. Semantic quality monitoring caught the issue within two hours by detecting the verbosity and tone shift.

Cost anomaly detection compares current hourly cost to baseline hourly cost and alerts when cost exceeds threshold multipliers. A threshold of two hundred percent means the system alerts when an hour costs twice the baseline. Thresholds are set based on acceptable cost variance. Systems with stable, predictable costs can use tighter thresholds. Systems with variable conversation types and volumes need looser thresholds to avoid false positives. Cost anomaly detection also monitors cost per conversation. If average cost per conversation increases by fifty percent while conversation volume remains constant, cost efficiency is degrading.

Error rate detection monitors provider API error rates, system error rates, and error type distribution. A sudden spike in four-hundred-series errors from a speech recognition API indicates client-side misconfiguration or malformed requests. A spike in five-hundred-series errors indicates provider-side issues. Error type distribution changes are often more informative than overall error rate. If timeout errors spike while other error types remain stable, latency is the root cause. If authentication errors spike, credentials expired or configuration changed. The detection system alerts on both overall error rate and specific error type anomalies.

## The Initial Triage Checklist for Voice Incidents

When an incident alert fires, the on-call engineer executes a triage checklist that moves from high-level system health to specific component investigation. The checklist is designed for speed and comprehensiveness. Missing a critical check can lead to misdiagnosis and delayed resolution. The checklist is the same regardless of incident category because the category is often unclear initially. Triage reveals the category.

Step one is confirming the alert is valid and not a false positive. Check the monitoring dashboard to verify the metric that triggered the alert is actually anomalous. Compare current values to recent history. If the alert fired due to a monitoring system bug or stale data, acknowledge and move on. If the alert is valid, proceed immediately to step two. This validation step takes ten seconds and prevents wasted investigation of phantom incidents.

Step two is checking overall system health: conversation start rate, conversation completion rate, error rate, and latency. If all metrics are normal except the one that triggered the alert, the incident is localized. If multiple metrics are degraded, the incident is systemic. Systemic incidents indicate infrastructure problems, provider outages, or widespread configuration errors. Localized incidents indicate component-specific failures or bugs in specific conversation flows.

Step three is checking provider health. Review dashboards for all third-party providers: speech recognition, synthesis, telephony, LLM inference. Check provider status pages for announced incidents. Check provider API error rates and latencies from the voice system's perspective. If a provider is degraded or down, the incident is external. Mitigation involves failover to backup providers or graceful degradation. If all providers are healthy, the problem is internal.

Step four is checking recent deployments and configuration changes. The monitoring system maintains a timeline of deployments, feature flag changes, and configuration updates. If a deployment occurred within the last two hours, rollback is the first mitigation to consider. If a feature flag toggled recently, disabling the flag is the immediate test. If no changes occurred, the incident is caused by external factors, organic traffic shifts, or latent bugs triggered by specific conversation patterns.

Step five is sampling recent failed or degraded conversations. Pull transcripts and logs for the ten most recent failed conversations or the ten conversations with highest latency. Read through them to identify common patterns. Do they all fail at the same point in the conversation flow? Do they all involve a specific intent? Do they all come from a specific region or device type? The pattern often points directly to root cause.

Step six is checking for cascading failures. A failure in one component can trigger failures in dependent components. A latency spike in the LLM inference service can cause timeouts in the conversation orchestration layer, which triggers retries, which amplifies load on the LLM service, which increases latency further. The cascade appears as simultaneous degradation across multiple components. Breaking the cascade requires identifying the initiating failure and mitigating it, not addressing each component's symptoms independently.

The triage checklist outputs a preliminary diagnosis: total failure, partial failure, latency degradation, quality degradation, cost spike, or provider outage. The diagnosis determines the next steps. Total failures require immediate rollback or failover. Partial failures require traffic shifting away from the failing segment. Latency degradation requires scaling up infrastructure or reducing per-conversation complexity. Quality degradation requires model rollback or prompt adjustment. Cost spikes require disabling expensive fallback paths or applying cost controls.

## Escalation Criteria and Paths

Not every incident requires waking the entire engineering team. Escalation criteria define when an incident justifies broader involvement and who to involve based on incident severity and duration. Over-escalation fatigues teams and trains them to ignore pages. Under-escalation allows incidents to worsen while a single on-call engineer struggles alone. The criteria must be clear, objective, and calibrated to actual business impact.

Severity one incidents are total failures or partial failures affecting more than fifty percent of traffic. All conversations fail or latency exceeds ten seconds. These incidents justify immediate escalation to the engineering manager, the product manager, and any subject matter experts for affected components. The on-call engineer pages the escalation chain within five minutes of confirming severity one status. The goal is to assemble the team that can resolve the incident fastest, not to make the on-call engineer solely responsible for critical outages.

Severity two incidents are partial failures affecting ten to fifty percent of traffic, quality degradation that drops resolution rates by more than ten percentage points, or latency degradation that doubles response times. These incidents require escalation if they persist for more than thirty minutes without improvement. The on-call engineer attempts initial mitigation solo but escalates if mitigation fails or root cause is unclear. Severity two incidents do not wake people in the middle of the night immediately, but they do require broader team involvement during business hours.

Severity three incidents are minor degradation affecting less than ten percent of traffic, quality issues that reduce resolution rates by five to ten percentage points, or cost spikes below critical thresholds. These incidents are investigated and resolved by the on-call engineer alone. Escalation occurs only if investigation reveals the issue is more severe than initial assessment or if resolution requires expertise the on-call engineer lacks. Most alerts are severity three. Effective triage ensures they stay severity three and do not escalate.

Escalation paths are predefined and documented in the incident response runbook. The on-call engineer does not decide in the moment who to page. The runbook specifies that severity one voice system incidents page the voice platform team lead, the ML engineering lead, and the product manager. Severity two incidents create a Slack incident channel and tag relevant team members for asynchronous collaboration. Severity three incidents are handled via normal on-call processes with postmortem documentation required only if they reveal novel failure modes.

Customer-facing escalation is a separate decision from engineering escalation. If the incident affects external users and will take more than fifteen minutes to resolve, the on-call engineer notifies the customer support team immediately so they can prepare for increased human agent load and update users via status pages. Customer communication does not wait for incident resolution. It begins as soon as impact is confirmed. The support team owns the messaging. Engineering owns the fix. Both happen in parallel.

## Common False Positives and How to Filter Them

False positive alerts erode trust in monitoring. If half of alerts are false positives, engineers begin ignoring alerts or silencing them during off-hours. The result is that real incidents go unaddressed because the signal is buried in noise. Filtering false positives requires understanding the most common causes and implementing detection logic that distinguishes genuine incidents from expected variation.

The most common false positive is alerting on short-duration spikes that resolve before human response is possible. A thirty-second latency spike caused by a transient network issue triggers an alert, but by the time the on-call engineer reads the alert and opens the dashboard, metrics are back to normal. The incident resolved itself. The alert provided no value. Filtering this false positive requires sustained threshold evaluation. The metric must exceed threshold for a minimum duration — sixty seconds or more — before alerting. Transient spikes are logged for trend analysis but do not page humans.

Scheduled maintenance and known traffic patterns generate expected metric changes that should not alert. If a provider schedules maintenance weekly at 3 AM, the monitoring system should suppress alerts for expected failover traffic during that window. If conversation volume spikes every weekday at 9 AM when the contact center opens, cost alerts should account for this pattern. False positive filtering requires teaching the monitoring system about known patterns so it does not treat expected behavior as anomalous.

Low-traffic periods produce high variance in percentage-based metrics. If conversation volume drops to ten conversations per hour overnight, a single failed conversation represents a ten percent error rate. During high-traffic periods, ten percent error rate means hundreds of failures. Alerting on the same percentage threshold for both periods generates false positives during low traffic. Filtering requires combining percentage thresholds with absolute count thresholds. Alert if error rate exceeds ten percent AND absolute error count exceeds twenty. This prevents false positives from small sample sizes.

Monitoring system bugs and stale data cause alerts on metrics that are not actually anomalous. A data pipeline delay causes the monitoring system to process five-minute-old data and compare it to ten-minute-old baseline data, creating apparent drift that does not exist in real time. A provider API reports incorrect latency metrics during a brief API bug. These false positives are harder to filter because they require detecting issues in the monitoring infrastructure itself. The solution is health checks for monitoring systems and cross-validation between multiple data sources. If only one monitoring source shows anomalous behavior and all other sources are normal, the anomaly is likely in the monitoring system, not the voice system.

Alert threshold tuning is the iterative process of adjusting thresholds based on historical false positive rates. The team tracks every alert: was it a true positive that required action, a true positive that self-resolved before action was needed, or a false positive? If a specific alert has a fifty percent false positive rate over four weeks, its threshold is too sensitive. If it has a zero percent false positive rate but missed a known incident, its threshold is too permissive. Optimal thresholds balance false positive rate below five percent with true positive detection rate above ninety-five percent.

## Post-Incident Review for Voice Systems

Every incident that meets severity one or severity two criteria requires a post-incident review. The review is not about blame. It is about learning. Voice systems are complex, multi-component, real-time distributed systems. Failures are inevitable. The goal is ensuring each failure teaches something that prevents similar failures in the future. The post-incident review follows a structured format that focuses on timeline, root cause, impact, and prevention.

The timeline documents when the incident began, when it was detected, when response began, when mitigation was implemented, and when the system returned to normal. The gap between incident start and detection is detection latency. The gap between detection and mitigation is response latency. Both are opportunities for improvement. If an incident started at 4:17 PM but was not detected until 6:42 PM, detection latency was two hours and twenty-five minutes. That is the primary failure. The incident itself is secondary.

Root cause analysis identifies not just what failed but why it failed and why monitoring did not catch it sooner. A voice system incident might have an immediate cause — a provider API returned errors — and a root cause — the monitoring system only checked aggregate error rates, not per-provider error rates, so the provider-specific failure was hidden in the aggregate. Addressing the immediate cause restores service. Addressing the root cause prevents recurrence. Post-incident reviews focus on root cause.

Impact quantification measures the incident in user terms and business terms. How many conversations failed? How many users experienced degradation? What was the customer satisfaction impact? What was the financial impact from lost conversations, increased support costs, or excess provider costs? Impact quantification ensures the team understands the real-world consequences, not just the technical details. It also helps prioritize prevention work. An incident that affected ten users is less urgent to prevent than an incident that affected ten thousand.

Prevention measures are specific, actionable changes implemented as a result of the incident. Vague prevention measures like "improve monitoring" or "test more thoroughly" are not acceptable. Specific prevention measures include "add per-provider error rate monitoring with threshold of five percent" or "implement canary deployment for voice model updates with automatic rollback if resolution rate drops below ninety percent." Each prevention measure is assigned an owner and a completion deadline. The post-incident review includes a follow-up meeting four weeks later to verify prevention measures were implemented.

Blameless culture is critical for effective post-incident reviews. If engineers fear consequences for causing incidents, they will hide problems, avoid risky changes, and provide incomplete information during reviews. The post-incident review focuses on system design, monitoring gaps, and process failures, not individual mistakes. The question is never "who made the mistake" but "how did the system allow this mistake to cause an incident." The individual who deployed a broken change is not at fault. The deployment process that did not catch the breakage before production is at fault.

The post-incident review also documents what went well. If monitoring detected the incident quickly, that success is noted. If escalation procedures worked smoothly, that is documented. If a rollback restored service within minutes, the deployment infrastructure deserves credit. Recognizing what worked reinforces good practices and ensures they are maintained as the system evolves. Post-incident reviews that only document failures miss the opportunity to learn from successes.

Voice system incidents are inevitable, but most are preventable with better monitoring, faster detection, and clearer response procedures. The incident detection playbook transforms chaotic, ad-hoc incident response into systematic, repeatable processes that minimize impact and maximize learning. The next section covers on-call runbooks that provide the detailed procedures engineers need to respond effectively when paged at 2 AM.

