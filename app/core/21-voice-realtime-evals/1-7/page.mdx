# 1.7 — Interruptibility as a Core Requirement

Humans interrupt each other constantly. Mid-sentence, mid-word, mid-thought. You start answering a question, realize the other person misunderstood, and cut yourself off: "Wait, no, I meant—" The other person stops talking. You clarify. The conversation continues. This is not a bug in human communication. It is a feature. Interruption is how we correct mistakes, change direction, and maintain control of the conversation.

Voice systems that cannot handle interruption feel broken. The user says "Set a timer for—wait, actually, make it twenty minutes." The system ignores "wait" and "actually" because it is already processing the first part of the sentence. It sets a timer for an unspecified duration, or worse, for the duration it predicted before the user corrected themselves. The user tries again. The system is still talking. The user raises their voice. The system keeps going. The interaction spirals into frustration. The system is technically functional. The conversation is dead.

Interruptibility is not a feature you add after the core system works. It is a foundational requirement, as essential as speech recognition or synthesis. A voice system without barge-in is not a conversational interface. It is a command-line interface with audio. You issue a command, wait for execution, receive output, issue the next command. That is not how humans talk. Humans talk over each other, interrupt themselves, and expect the conversation to adapt in real time.

## What Barge-In Actually Means

**Barge-in** is the ability to interrupt the system while it is speaking. The user does not wait for the system to finish its response. They hear enough to know the system is going in the wrong direction and cut in: "Stop," "No, not that," "Wait, I meant something else." The system must detect the interruption, stop generating audio, stop playing audio, and start listening to the new input — all within 200-300 milliseconds. Any slower and the system keeps talking while the user is trying to interrupt. The user's speech and the system's speech overlap. The ASR cannot process both simultaneously. The user's correction is lost.

The technical requirements are harsh. The system must continuously monitor for user speech even while generating and playing its own response. This means running ASR in parallel with TTS. The ASR must distinguish between the system's voice and the user's voice. If it cannot, it transcribes its own output as user input, creating a feedback loop. The system hears itself say "Your meeting is scheduled for 3 p.m." and interprets that as the user requesting a 3 p.m. meeting. The dialog state corrupts. The conversation breaks.

Acoustic echo cancellation (AEC) solves this by subtracting the system's audio from the microphone input. The system knows what it is about to play. It plays the audio through the speaker and simultaneously captures the microphone feed. It subtracts the known playback signal from the microphone input. What remains is the user's voice, isolated from the system's voice. In theory. In practice, AEC is fragile. Room acoustics, speaker-microphone placement, and processing delays all introduce errors. If AEC fails, the system either cannot hear the user during playback or hears itself and corrupts the transcription.

The second requirement: stopping TTS instantly. When the system detects a barge-in, it must abort TTS generation and stop audio playback within 100-150 milliseconds. If the system keeps talking for 400 milliseconds after the user starts interrupting, the user has to shout over the system to be heard. The experience is terrible. Fast TTS abort requires streaming synthesis — generating audio incrementally and maintaining the ability to stop mid-sentence. Batch TTS, where the entire response is synthesized before playback begins, cannot be interrupted cleanly. You can stop playback, but the system already committed resources to generating the full response. Latency and compute are wasted.

## The Context Problem: What the System Knows When It Is Interrupted

The user asks: "What restaurants are open near me right now?" The system begins responding: "I found twelve restaurants within two miles. The first is—" The user interrupts: "No, I want Italian." The system stops talking. Now what?

The system needs to know where it was in the conversation when the interruption occurred. Did the user reject the entire query, or just the current result? Should the system start over, or refine the existing search? The answer depends on conversational context, which is ambiguous. "No, I want Italian" could mean "discard the previous results and search for Italian restaurants" or "filter the twelve restaurants you already found to only show Italian ones." The first interpretation requires a new query. The second interpretation requires refining the current result set.

Text-based AI does not have this problem. Each message is independent. The user sends a message, the system responds, the user sends another message. There is no notion of interrupting mid-response because responses are atomic. Voice systems lose this safety. The user can interrupt at any point. The system must decide: do I preserve partial progress, or do I discard everything and start fresh?

The safest default is to discard. When the user interrupts, throw away the current response, clear the playback queue, and treat the interruption as a new turn. This avoids context corruption. The downside: you lose any useful work. If the system spent 380 milliseconds retrieving restaurant data, synthesizing a response, and starting playback, all of that is wasted. The user's interruption might have been refinement, not rejection, but you cannot tell. So you start over. Latency increases. The user waits.

The smarter approach is intent detection on the interruption. When the user says "No, I want Italian," run NLU on that fragment immediately. If the intent is refinement — filtering, narrowing, adjusting — keep the existing search results and apply the filter. If the intent is rejection — "stop," "cancel," "never mind" — discard everything and reset. This requires fast NLU inference during barge-in, which adds latency to an already tight loop. But it avoids wasted work. The system refines instead of restarting.

The failure mode: misjudging intent. The user says "Actually, Thai food," and the system interprets it as refinement. It filters the twelve restaurants for Thai options. But the user meant "discard the previous search and search for Thai restaurants anywhere, not just nearby." The system refined when it should have reset. The results are wrong. The user gets frustrated. Intent detection on sentence fragments is hard. You are running NLU on incomplete, ambiguous input while under time pressure. Errors are common.

## Multi-Turn Interruption and Cascading Failures

The user interrupts once. The system handles it. The user interrupts again. The system handles it. The user interrupts a third time. The system state is now corrupted. Each interruption modified the context slightly. The third interruption is processed based on state that no longer matches the user's mental model. The conversation diverges. The user says things the system cannot interpret because the system's understanding of the conversation is three turns behind.

This is **cascading interruption failure**. The first interruption is handled correctly. The second interruption interacts with the first in unexpected ways. The third interruption compounds the error. By turn four, the system and the user are having different conversations. The user thinks they clarified their request three times. The system thinks it is executing the original request with minor modifications. Neither is right. The conversational state is incoherent.

The root cause is state accumulation. Each interruption modifies dialog state — the system's internal representation of the conversation. If the user interrupts during a multi-step task — booking a flight, ordering food, troubleshooting a problem — the system must decide which parts of the state to keep and which to discard. Keep too much, and stale state poisons future turns. Discard too much, and the user has to repeat themselves. There is no safe middle ground.

The mitigation is explicit confirmation. When the user interrupts, the system stops and confirms: "Got it, you want Italian restaurants. Should I search near you, or somewhere else?" The confirmation forces alignment. The system states its understanding. The user corrects if wrong. The state synchronizes before the conversation continues. The downside: every interruption adds an extra turn. Latency increases. The conversation becomes more transactional and less natural. But the state stays coherent.

The alternative is to reset aggressively. After two interruptions in a row, discard all dialog state and start fresh. The system says: "Let me start over. What are you looking for?" This frustrates users who feel like they already provided information. But it prevents state corruption. The user repeats themselves, but the conversation does not diverge. Teams that try to preserve state across multiple interruptions often build systems that feel "confused" or "like it is not listening." Teams that reset aggressively build systems that feel "forgetful" but not broken. Forgetful is recoverable. Confused is not.

## The UX of Saying No: Teaching Users How to Interrupt

Users are trained by decades of non-interactive voice systems. Automated phone menus, voicemail prompts, GPS navigation — all of these talk at you, not with you. You cannot interrupt. Trying to interrupt accomplishes nothing. The system keeps talking. Users learn helplessness. When they encounter a voice system that does support barge-in, they do not know. They wait for the system to finish. The feature exists. The user never triggers it.

This is the **interruption discoverability problem**. Barge-in only works if users know they can interrupt. If they do not try, the feature is invisible. The system is technically interruptible, but experientially non-interactive. You built the capability. The user never discovers it.

The solution is to teach interruption explicitly. The first time the user interacts with the system, the system says: "You can interrupt me at any time. Just start talking." This sets the expectation. The user knows interruption is allowed. The second technique: respond positively to interruptions. When the user interrupts, the system says "Got it" or "Okay, let me adjust that." The positive acknowledgment reinforces the behavior. The user learns that interrupting works. They do it again next time.

The third technique: create moments that invite interruption. If the system is listing search results — "I found twelve restaurants. The first is Trattoria Roma, the second is Bella Vista, the third is—" — most users will interrupt if they hear the restaurant they want. The list structure creates a natural interruption point. The user does not have to wait for the end. They can cut in as soon as they hear the right answer. This is interruptibility by design. The content structure encourages interruption.

The failure mode is punishing interruptions. If the system reacts to barge-in by stopping, pausing for 600 milliseconds, and then saying "I'm sorry, I didn't catch that," the user learns that interrupting breaks the system. They stop trying. The system is technically interruptible, but it trains users not to interrupt. The UX backfires. The feature exists and makes the experience worse.

## When Interruptibility Breaks: The Edge Cases

A user is on a phone call. Background noise includes other people talking. The system is responding to the user's query. Someone in the background says "stop." The system hears it, interprets it as a barge-in, and stops mid-sentence. The user did not interrupt. The system stopped for no reason. This is a **false positive barge-in** — the system detected speech that was not directed at it and treated it as an interruption.

The mitigation is voice activity detection (VAD) tuned for the user's voice. The system learns the acoustic signature of the user during enrollment or the first few turns. It ignores speech that does not match. This works in quiet environments. It fails in noisy environments where the user's voice is drowned out by background speakers. The system cannot reliably distinguish between the user saying "stop" and someone else saying "stop." False positives are common.

The second edge case: the user interrupts, then stops talking. They say "Wait—" and pause to think. The system stops generating audio. It waits for the user to continue. The user is silent for 900 milliseconds. The system does not know if the user is still formulating their thought or if they expect the system to resume. If the system waits too long, the conversation feels frozen. If the system resumes too early, it talks over the user. There is no right answer. The system must guess.

The heuristic: wait 700-900 milliseconds. If the user has not spoken, assume they want the system to resume or prompt them. The system says: "Did you want to change something?" This breaks the silence without talking over the user. The user either responds or says "no, continue." The state synchronizes. The downside: the 700ms wait feels like latency. The interaction is slower. But it is safer than guessing wrong.

The third edge case: the user interrupts with a non-verbal sound. A cough, a sigh, background noise that sounds like speech. The system detects audio, stops responding, waits for input. The user was not trying to interrupt. The system stopped for nothing. The user is confused. The conversation feels broken. The mitigation: require a minimum duration of detected speech before triggering barge-in. A cough lasts 200 milliseconds. A word lasts 300-500 milliseconds. Set the threshold at 350 milliseconds. Anything shorter is ignored. This filters out most false positives. It also introduces risk: if the user says a short interruption word like "no" or "stop," and it lasts only 280 milliseconds, the system ignores it. The user has to repeat themselves. You traded one failure mode for another.

## The Cost of Interruptibility: Latency, Compute, and Complexity

Barge-in doubles system complexity. You must run ASR continuously, even while playing TTS. You must perform acoustic echo cancellation. You must detect barge-in in under 150 milliseconds. You must abort TTS generation and playback instantly. You must decide whether to discard or refine context. Every one of these steps adds latency, compute cost, and failure modes.

The latency cost is the worst. Running ASR in parallel with TTS means you cannot offload ASR to a background thread. It must run in real time, always listening. ASR latency directly impacts barge-in detection latency. If ASR takes 180 milliseconds to produce a partial transcript, your barge-in detection is at least 180 milliseconds behind the user's speech. Add 50 milliseconds for intent detection, 30 milliseconds for TTS abort signaling, and 40 milliseconds for playback stop. Total barge-in latency: 300 milliseconds. The user interrupts. The system keeps talking for 300 milliseconds. The user talks louder. The overlap degrades ASR accuracy on the interruption. The transcription is wrong. The system misunderstands the correction.

The compute cost is significant. Running full ASR continuously is expensive. Most voice systems use a lightweight VAD model to detect speech, then trigger full ASR only when the user is talking. Barge-in requires always-on ASR because the system must detect speech while generating its own response. You cannot wait for VAD to trigger. By the time VAD detects speech, the user has already started interrupting. The detection is too late. Always-on ASR costs 3-5 times more compute than VAD-gated ASR.

The complexity cost is invisible until something breaks. Barge-in introduces race conditions. The TTS generator is producing audio. The ASR is processing a barge-in. The dialog manager is deciding whether to abort or refine. All three components are running simultaneously, sharing state. If the abort signal arrives after the TTS generator queued the next chunk of audio, the abort fails. The system keeps talking. The user's interruption is lost. The bug is intermittent, timing-dependent, and nearly impossible to reproduce in testing. It only happens in production under load.

Despite all of this, interruptibility is non-negotiable. A voice system without barge-in is not conversational. Users will interrupt. If the system cannot handle it, the conversation fails. The cost is high. The alternative is higher.

The next challenge is not just handling interruptions, but handling the state that accumulates across turns. Voice conversations are continuous. Every turn modifies context. State builds, drifts, and corrupts. Managing that state is what separates functional voice systems from production-ready ones.

