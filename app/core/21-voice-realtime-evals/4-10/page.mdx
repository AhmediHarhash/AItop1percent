# 4.10 — ASR Confidence Scores and Their Calibration

ASR confidence scores are lies. Not intentional lies, but lies nonetheless. The system says it's 94 percent confident in the word "test" when it should have been "chest." Another system says it's 67 percent confident in a transcription that is actually perfect. A third system assigns 0.89 to correct words and 0.87 to incorrect ones — the scores are useless for decision-making. Most ASR providers return confidence scores with every transcription, and most teams ignore them because they've learned the hard way that the scores don't mean what they claim to mean.

A confidence score of 0.9 should mean the transcription is correct 90 percent of the time. That's calibration. A perfectly calibrated system has confidence scores that match empirical accuracy. If you bucket all words with confidence between 0.85 and 0.90 and measure their accuracy, you should see 85 to 90 percent correct. If you see 72 percent or 96 percent, the scores are miscalibrated. Miscalibration makes confidence scores worthless for downstream decisions. You can't set thresholds, you can't route to human review, you can't trust the system's own assessment of uncertainty.

The problem is pervasive. A healthcare voice dictation platform tested three ASR providers in early 2026. All three returned word-level confidence scores. The team bucketed words by confidence range and measured actual accuracy. Provider A: words with confidence 0.9 to 1.0 were correct 96 percent of the time — overconfident. Provider B: words with confidence 0.9 to 1.0 were correct 83 percent of the time — underconfident. Provider C: accuracy was 91 percent across all confidence ranges — the scores were calibrated. Provider C's scores were useful. Providers A and B's scores were decorative.

Confidence scores matter because they enable conditional logic. If the ASR is confident, act immediately. If it's uncertain, wait for confirmation or route to human review. But conditional logic only works if the scores are calibrated. If an overconfident system assigns 0.95 to incorrect transcriptions, you'll act on errors. If an underconfident system assigns 0.60 to correct transcriptions, you'll route perfectly good results to review and waste human time. Calibration is what makes confidence actionable.

## What ASR Confidence Scores Actually Represent

Most ASR systems use neural acoustic models combined with language models to produce transcriptions. The confidence score is usually derived from the model's internal probability estimates — how likely the model thinks this word sequence is given the audio and language context. These probabilities are not inherently calibrated. A model can be highly certain and highly wrong.

Confidence scores are often computed at multiple levels: word-level, phrase-level, and utterance-level. Word-level scores tell you how confident the model is in each individual word. Phrase-level scores aggregate across a phrase or sentence. Utterance-level scores represent confidence in the entire transcription. The three levels can diverge. An utterance can have high overall confidence but contain one low-confidence word. That word is probably wrong.

A navigation voice assistant received utterance-level confidence of 0.91 for "turn left on Main Street." Word-level confidence: "turn" 0.97, "left" 0.88, "on" 0.95, "Main" 0.72, "Street" 0.94. The low confidence on "Main" signaled uncertainty about the street name. The system asked for confirmation: "Turn left on Main Street, is that correct?" The user replied, "No, Maple Street." The utterance-level score was high, but the word-level score caught the error. Always use word-level scores when available — they localize uncertainty.

Some ASR providers compute confidence from multiple signals: acoustic model probability, language model probability, and decoder search scores. The acoustic model evaluates how well the audio matches the hypothesized words. The language model evaluates how plausible the word sequence is. The decoder evaluates how many competing hypotheses were close in probability. Confidence is often a weighted combination of these three. This is better than acoustic probability alone, but it's still not calibrated unless the provider has tuned the weighting on your specific data distribution.

Understand what your ASR provider's confidence score represents. Is it raw model probability? Is it calibrated on a generic dataset? Is it tuned for your domain? Most providers don't document this clearly. Test it empirically. If the documentation says "confidence score represents the model's certainty," assume it's uncalibrated and verify.

## Measuring Confidence Calibration

Calibration is measured by comparing predicted confidence to empirical accuracy. The process is straightforward: bucket your transcriptions by confidence score, measure the accuracy of each bucket, and plot predicted confidence against actual accuracy. A perfectly calibrated system produces a diagonal line — predicted equals actual at every point.

Collect a test set of at least 5,000 utterances with ground truth transcriptions. Run your ASR system on the test set and record the confidence score for each word or utterance. Bucket the results by confidence range: 0.0 to 0.1, 0.1 to 0.2, up to 0.9 to 1.0. For each bucket, calculate the percentage of words or utterances that were transcribed correctly. Plot the results: x-axis is predicted confidence (bucket midpoint), y-axis is empirical accuracy.

A medical transcription service ran this analysis on 8,200 utterances. Confidence range 0.5 to 0.6: empirical accuracy 48 percent — close to calibrated. Confidence range 0.7 to 0.8: empirical accuracy 68 percent — close. Confidence range 0.9 to 1.0: empirical accuracy 97 percent — overconfident by 4 to 7 percentage points. The system was well-calibrated at low to mid confidence but overconfident at high confidence. This meant the team could trust low-confidence scores to identify uncertain transcriptions, but high-confidence scores didn't guarantee correctness as much as the score implied.

Calculate **calibration error** as the mean absolute difference between predicted confidence and empirical accuracy across all buckets. For the medical transcription service, calibration error was 4.2 percentage points. A different ASR provider tested on the same data had calibration error of 11.6 percentage points — much worse. Calibration error below 5 percentage points is good. Below 3 percentage points is excellent. Above 10 percentage points means the scores are nearly meaningless.

Use **reliability diagrams** to visualize calibration. Plot predicted confidence on the x-axis, empirical accuracy on the y-axis, and draw a diagonal line representing perfect calibration. Plot your actual data points. Points above the line indicate underconfidence (the system was more accurate than it thought). Points below the line indicate overconfidence (the system was less accurate than it thought). Most ASR systems are overconfident — they cluster below the line, especially at high confidence values.

## Calibration Across Domains and Conditions

Confidence scores calibrated on one data distribution often miscalibrate on another. An ASR system trained and calibrated on clean studio recordings will be overconfident on noisy street recordings. A system calibrated on American English will miscalibrate on Indian English. Calibration is distribution-dependent.

A customer service ASR system was calibrated on internal test data: quiet office environments, clear speech, standard American accents. Calibration error on the test set: 3.1 percentage points. When deployed to production, the system handled noisy call center environments, fast speech, and diverse accents. The team ran the same calibration analysis on production data. Calibration error: 13.8 percentage points. The system was massively overconfident in production. High-confidence scores in the test environment meant 95 percent accuracy. High-confidence scores in production meant 78 percent accuracy.

Measure calibration separately for each deployment condition. If you serve users in quiet and noisy environments, measure calibration in both. If you support multiple languages, measure calibration per language. If you handle different domains (medical, legal, customer service), measure calibration per domain. Do not assume that calibration holds across conditions. It almost never does.

A multilingual voice assistant measured calibration across English, Spanish, French, and Mandarin. English calibration error: 4.3 percentage points. Spanish: 6.1. French: 8.7. Mandarin: 14.2. The ASR provider had trained primarily on English data and calibrated the confidence scores on English. The scores were usable for English, marginal for Spanish and French, and useless for Mandarin. The team built domain-specific recalibration models for each language — more on that below.

Track calibration over time. Model updates, data drift, and changing user behavior all affect calibration. A dictation app measured calibration monthly. For the first six months, calibration error hovered around 4.5 percentage points. In month seven, it jumped to 9.2. The ASR provider had pushed a model update that improved final WER by 8 percent but destroyed confidence calibration. The provider had retrained the acoustic model but not recalibrated the confidence scoring. The team had to either recalibrate locally or revert to the previous model. They chose recalibration.

## Recalibrating Confidence Scores Locally

If your ASR provider's confidence scores are miscalibrated, you can recalibrate them locally using your own data. Recalibration doesn't change the transcriptions — it adjusts the confidence scores to better match empirical accuracy in your domain.

The standard approach is **Platt scaling** or **isotonic regression**. Both methods learn a mapping from the original confidence scores to calibrated probabilities. Platt scaling fits a logistic regression model. Isotonic regression fits a non-parametric monotonic function. Isotonic regression is more flexible and generally works better for confidence calibration.

Collect a calibration dataset: at least 2,000 utterances with ground truth labels and ASR confidence scores. Label each word or utterance as correct (1) or incorrect (0). Fit an isotonic regression model that maps the original confidence score to the empirical accuracy. For any new transcription, take the ASR's raw confidence score, pass it through the isotonic regression model, and get a calibrated probability.

The multilingual voice assistant used isotonic regression to recalibrate Mandarin confidence scores. The original scores ranged from 0.3 to 0.99 with calibration error of 14.2 percentage points. After recalibration on 3,500 labeled Mandarin utterances, calibration error dropped to 5.8 percentage points. The recalibrated scores were usable. The team set a threshold: recalibrated confidence below 0.75 triggered human review. Before recalibration, they couldn't set a meaningful threshold because the scores were too noisy.

Recalibration requires maintaining a labeled dataset and retraining the calibration model periodically. The isotonic regression model itself is lightweight — it's just a lookup table mapping input scores to output probabilities. But you need fresh labeled data to keep it accurate as your ASR provider updates models or as your user base shifts. A customer service platform retrained their calibration model quarterly using the most recent 5,000 labeled utterances. Calibration error stayed below 6 percentage points across two years of production.

Recalibration works well when the underlying ASR model is good but the confidence scores are miscalibrated. It does not fix a fundamentally inaccurate ASR system. If your WER is 18 percent, recalibration will give you well-calibrated low-confidence scores, but that doesn't make the transcriptions better. Recalibration is a complement to choosing a good ASR provider, not a substitute.

## Using Confidence Scores for Downstream Decisions

Calibrated confidence scores enable conditional workflows. If confidence is high, act immediately. If confidence is low, ask for confirmation, route to human review, or fall back to a safer behavior. The exact thresholds depend on your use case, your calibration quality, and your tolerance for errors.

A voice-controlled smart home system used confidence thresholds to decide whether to execute commands immediately or ask for confirmation. Commands with utterance-level confidence above 0.88: execute immediately. Confidence between 0.65 and 0.88: ask "Did you say [transcription]?" and wait for yes or no. Confidence below 0.65: respond "I didn't catch that, could you repeat?" The thresholds were tuned based on calibration analysis and user testing. At 0.88, the error rate was 2.1 percent — acceptable for low-risk commands like "turn on the living room lights." At 0.65, the error rate was 11 percent — too high to act without confirmation.

Use **risk-weighted thresholds** for different command types. Low-risk commands (play music, set a timer) can use lower confidence thresholds. High-risk commands (unlock the door, authorize a payment) require higher thresholds or always require confirmation regardless of confidence. A voice banking app set thresholds by transaction value. Transactions under 50 dollars: confidence threshold 0.82. Transactions between 50 and 500 dollars: threshold 0.91. Transactions over 500 dollars: always require explicit confirmation even if confidence is 0.99.

Confidence scores also enable **selective human review**. Route low-confidence transcriptions to human annotators for correction. This is common in transcription services where final accuracy matters more than latency. A legal transcription service routed any utterance with average word-level confidence below 0.78 to human review. About 14 percent of utterances met this threshold. Human reviewers corrected errors and returned high-quality transcriptions. The remaining 86 percent went directly to the client with minimal review. Without confidence scores, the team would have had to review everything or accept errors in production.

Measure the **precision and recall of confidence-based filtering**. Precision: of the utterances flagged as low-confidence, what percentage actually contained errors? Recall: of all utterances with errors, what percentage were flagged as low-confidence? A high-precision, high-recall system catches most errors without flagging too many correct transcriptions. A legal transcription service achieved 81 percent precision and 73 percent recall with a confidence threshold of 0.78. Lowering the threshold to 0.72 increased recall to 84 percent but dropped precision to 69 percent — more errors caught, but more false positives sent to review.

Tune thresholds based on your cost structure. If human review is expensive, optimize for precision — only flag utterances that are very likely to have errors. If missing errors is expensive (legal, medical, safety-critical), optimize for recall — flag anything that might be wrong even if it increases false positives. There is no universal threshold. Measure your calibration, understand your costs, and set thresholds accordingly.

## Confidence Scores in Streaming ASR

Streaming ASR produces confidence scores for partial transcriptions as well as finals. Partial confidence scores are even less reliable than final confidence scores because the model has less context. A word that appears with confidence 0.72 in a partial might have confidence 0.94 in the final after more audio context arrives. Partial confidence scores are noisy, but they're still useful if calibrated.

A real-time captioning system used partial confidence to decide whether to display a word immediately or wait for more context. Words with partial confidence above 0.85: display immediately in black text. Words with confidence between 0.60 and 0.85: display in gray text to signal uncertainty. Words with confidence below 0.60: don't display until the next partial or final. This reduced flicker — low-confidence words that were likely to change didn't appear on screen until the system was more certain.

Measure **partial confidence calibration** separately from final confidence calibration. Partial scores are often less calibrated because the model is making predictions with incomplete context. A dictation app found that final confidence scores had calibration error of 4.1 percentage points, but partial confidence scores had calibration error of 9.7 percentage points. The team recalibrated partial scores separately using a dataset of partials and their eventual finals. After recalibration, partial confidence error dropped to 6.3 percentage points — still worse than finals, but usable.

Use **confidence trends** across partials to detect stabilization. If a word appears in three consecutive partials with confidence 0.68, 0.74, 0.81, the rising confidence suggests the model is converging. If a word appears with confidence 0.89, then 0.72, then 0.81, the fluctuating confidence suggests the model is uncertain. A voice assistant tracked confidence trends and only acted on commands when confidence had been stable or increasing for at least two consecutive partials. This reduced false triggers by 28 percent compared to acting on the first partial.

Partial confidence scores are noisier, but they provide early signals about transcription quality. Calibrate them separately, use them cautiously, and combine them with stability metrics (revision rate, consistency across partials) to make better real-time decisions.

## Detecting Confidence Score Degradation in Production

Confidence calibration degrades silently. An ASR provider updates their model. Your user base shifts to noisier environments. A new accent group starts using your product. Suddenly your confidence scores stop matching reality. You need automated detection of calibration drift.

Log confidence scores and ground truth labels for a sample of production utterances. You don't need labels for everything — a random sample of 500 to 1,000 utterances per week is enough. Use human review, user corrections, or downstream feedback (user clicked "wrong transcription") as ground truth signals. Compute weekly calibration error and plot it over time. If calibration error increases by more than 30 percent relative to baseline, investigate.

A customer service transcription platform logged 1,000 random utterances per week and sent them for manual review. Baseline calibration error: 4.8 percentage points. In week 14, calibration error jumped to 8.1 percentage points. The team investigated and found that a new call center location had come online with significantly more background noise than previous locations. The ASR's confidence scores were calibrated for low-noise environments and were overconfident in high-noise conditions. The team recalibrated using data from the new location.

Set up **confidence-accuracy mismatch alerts**. If the percentage of high-confidence (above 0.9) utterances that are actually incorrect exceeds a threshold (say, 8 percent), alert the team. This catches overconfidence. If the percentage of low-confidence (below 0.7) utterances that are actually correct exceeds a threshold (say, 40 percent), that's underconfidence. Both signal calibration drift.

Use **user behavior** as a calibration proxy. If users frequently correct transcriptions that had high confidence scores, the system is overconfident. If users rarely correct transcriptions that had low confidence scores, the system is underconfident. A dictation app tracked correction rate by confidence bucket. High-confidence utterances (above 0.92) had a correction rate of 3.2 percent for three months, then jumped to 7.8 percent in month four. The team recalibrated and the correction rate dropped back to 3.5 percent. User behavior was an early indicator of calibration drift before formal measurement confirmed it.

## When Confidence Scores Aren't Enough

Confidence scores capture model uncertainty but not all sources of error. A perfectly confident model can still be wrong if the audio is ambiguous, if the speaker misspoke, or if the domain vocabulary is unusual. Confidence scores should be one signal among many, not the only signal.

Combine confidence with **domain-specific signals**. A medical transcription system combined ASR confidence with a medical term validator. If the transcription included a term not in the medical dictionary and the confidence was below 0.88, flag for review. If the term was in the dictionary or confidence was above 0.88, accept it. This caught errors that confidence alone missed — like the ASR transcribing "hypertension" as "high pertension" with confidence 0.91 because acoustically it sounded fine, but the validator flagged "pertension" as not a real term.

Use **cross-validation with downstream systems**. If the ASR output feeds into a natural language understanding (NLU) component, check if the NLU is confident in the extracted intent. Low NLU confidence combined with high ASR confidence suggests the transcription might be technically accurate but semantically nonsensical. A voice assistant transcribed "I want to book a fight to Austin" with confidence 0.93. The NLU had low confidence because "book a fight" isn't a known intent. The system asked for clarification and the user corrected to "flight." The ASR was confident but wrong in a way that confidence alone didn't reveal.

Track **error types** that confidence scores fail to catch. Homophones (words that sound identical but have different meanings) often get high confidence scores even when wrong. "There," "their," and "they're" sound the same. The ASR might transcribe "their going to the store" with high confidence when the speaker said "they're going to the store." Confidence doesn't help because acoustically both are equally plausible. You need a language model or grammar checker as a secondary filter.

Confidence scores are a tool, not a solution. They work best when calibrated, when used in combination with other signals, and when tuned to your specific use case. Treat them as one input to your decision logic, not the only input.

Confidence scores are only useful when calibrated — when a score of 0.9 actually means 90 percent correct. Most ASR systems are miscalibrated out of the box. Measure calibration empirically on your data, recalibrate if necessary, and monitor calibration over time. Use calibrated confidence to set thresholds, route to human review, and make real-time decisions. But never rely on confidence alone — combine it with domain knowledge, downstream validation, and user behavior signals. The next subchapter covers building production ASR evaluation datasets that match your real traffic.
