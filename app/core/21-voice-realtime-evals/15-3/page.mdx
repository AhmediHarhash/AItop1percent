# 15.3 — Silence Billing: Paying for Nothing

In early 2026, a mental health voice assistant analyzed their cost breakdown and discovered something absurd. Thirty-two percent of every conversation was silence. Not the user speaking. Not the AI responding. Just dead air. Pauses while patients gathered their thoughts. Gaps while the system processed. Hold time while retrieving session history. Silence. And because they were on per-minute billing, they paid for every second of it. Across 18,000 sessions per month averaging eight minutes each, they were billing 144,000 minutes. But 46,080 of those minutes were silence. At $0.13 per minute, they were spending $5,990 monthly on nothing. Not on bad responses. Not on errors. On the absence of sound. This is the silence billing trap, and it is invisible until you measure it.

## How Silence Billing Works in Per-Minute Pricing

Per-minute billing starts when the connection opens and stops when the connection closes. Everything in between is billable, including silence. The user calls in. The AI plays a greeting. The user pauses to think for four seconds. Those four seconds cost you money. The AI responds. The user says "um" and pauses for three seconds. Those three seconds cost you money. The AI finishes speaking, and there is a two-second gap before the user replies. Those two seconds cost you money.

The provider does not distinguish between productive time and unproductive time. A minute of continuous conversation costs the same as a minute of silence with ten seconds of speech scattered throughout. The meter runs on connection time, not content time. This creates a perverse incentive: the slower your system, the more you pay. If your LLM takes five seconds to generate a response instead of two seconds, you just added three seconds of billable silence to every turn. If your ASR has a two-second latency buffer, you are paying for two seconds of processing delay on every question.

A customer service voice agent in mid-2025 had an average turn processing time of 4.2 seconds. ASR took 0.8 seconds to finalize the transcription after the user stopped speaking. The LLM took 2.6 seconds to generate a response. TTS took 0.8 seconds to synthesize the audio. Total: 4.2 seconds of silence between when the user finished speaking and when the AI started responding. The average conversation had eleven turns. That is 46.2 seconds of processing silence per call. At ninety seconds average call length, thirty-four percent of every call was processing delay. At $0.11 per minute, that silence cost them $0.051 per call. Across 95,000 calls monthly, they were spending $4,845 on processing latency alone.

## Detecting Excessive Silence: The Silence Budget

Not all silence is waste. Some silence is conversational. People pause to think. They take a breath between sentences. They process what the AI said before responding. A completely silence-free conversation would feel unnatural and aggressive. The goal is not to eliminate silence. The goal is to distinguish necessary silence from wasteful silence, then minimize the waste.

Start by measuring your silence distribution. Log every call with timestamps for user speech start, user speech end, AI response start, AI response end. Calculate the gaps. You will see three types of silence: user thinking time, system processing time, and dead air. User thinking time is the pause between when the AI finishes speaking and when the user starts responding. System processing time is the pause between when the user finishes speaking and when the AI starts responding. Dead air is everything else — silence during hold, silence after the conversation has ended but before the user hangs up, silence from connection issues.

A telemedicine voice assistant in late 2025 measured their silence and found that average user thinking time was 2.1 seconds per turn. Acceptable. Average system processing time was 5.8 seconds per turn. Too high. Dead air averaged 9.3 seconds per call, mostly from patients who finished the consultation but waited for the AI to disconnect instead of hanging up themselves. The system processing time was their biggest cost driver. At twelve turns per call, that was 69.6 seconds of processing silence. They set a target: reduce system processing time to under three seconds per turn. That would cut processing silence to 36 seconds per call, saving 33.6 seconds — a $0.073 cost reduction per call at their $0.13 per minute rate.

## Reducing Billable Silence: Filler Audio

One tactic for reducing perceived silence is filler audio. When the system is processing, play something — hold music, a brief acknowledgment sound, a vocal filler like "let me check that." This does not reduce billable time. You are still paying for those seconds. But it changes the user experience from "the system is broken and silent" to "the system is working and I can hear it."

Some teams use filler to reduce abandonment. A pharmacy benefits voice assistant found that users hung up if the silence after their question exceeded six seconds. They assumed the system had failed. The team added a filler phrase: "One moment, I am retrieving your prescription details." That phrase took 2.8 seconds to play. It bought them 2.8 seconds of processing time without the user feeling ignored. Abandonment dropped from eleven percent to four percent. The filler cost them 2.8 seconds of TTS time per call, but it saved them the cost of 7 percent of calls being retried because users thought the system was dead.

Filler audio is a band-aid. It does not fix slow processing. It masks it. The better solution is to reduce the processing time itself. But filler is useful as a temporary measure while you optimize the underlying latency. Just do not treat it as a permanent solution. If you are still playing filler phrases six months after launch, you have not fixed the problem — you have just made it less obvious.

## Reducing Processing Time: Optimizing the Component Chain

System processing time is the sum of ASR finalization time, LLM generation time, and TTS synthesis time. Each component has latency, and the latencies stack. The fastest way to reduce processing silence is to parallelize where possible and optimize the slowest component.

ASR finalization time is usually unavoidable. The ASR model needs to wait for the user to finish speaking, then process the final audio chunk. Most ASR providers have a 0.5 to 1.5 second finalization delay. You cannot eliminate it, but you can reduce it by using streaming ASR with partial transcripts. Send partial transcripts to your LLM before the final transcript is ready. The LLM can start processing while the ASR is still finalizing. When the final transcript arrives, the LLM has already generated part of the response. This does not reduce total processing time, but it reduces perceived latency.

LLM generation time is where most teams have the most room to optimize. A typical voice LLM call in 2026 generates responses in 1.5 to 4 seconds, depending on response length and model size. You can reduce this by using smaller models for simple queries, caching common responses, or streaming the LLM output to TTS before generation completes. Streaming is the most effective. Instead of waiting for the LLM to generate the full response, send the first few tokens to TTS as soon as they are available. TTS can start synthesizing while the LLM is still generating the rest. This cuts the perceived latency from "LLM time plus TTS time" to "max of LLM time or TTS time."

A financial services voice assistant in early 2026 reduced their processing time from 5.1 seconds to 2.3 seconds by implementing streaming LLM-to-TTS. Their LLM generation time was 3.8 seconds. Their TTS synthesis time was 1.3 seconds. Without streaming, total processing time was 5.1 seconds. With streaming, TTS started synthesizing after the first 0.6 seconds of LLM generation. By the time the LLM finished, TTS had already synthesized the first half of the response. Total processing time dropped to 2.3 seconds — LLM start-to-first-token plus TTS synthesis of the remaining tokens. The 2.8-second reduction saved them $0.036 per call at their $0.14 per minute rate. Across 60,000 calls monthly, that was $2,160 in latency savings.

## The User Thinking Time Problem

User thinking time is harder to optimize because you do not control it. Users pause. They think. They hesitate. Some pauses are short — one or two seconds. Some are long — ten or fifteen seconds. You are paying for all of it. The question is whether you should.

One approach is to implement aggressive turn-taking. If the user is silent for more than three seconds after the AI finishes speaking, the AI prompts them: "Are you still there?" or "Do you need more time?" This can reduce unnecessary silence from users who got distracted or stepped away. But it can also frustrate users who are simply thinking. Aggressive prompting works for transactional use cases — order confirmations, appointment scheduling — where long pauses are unusual. It backfires for consultative use cases — therapy, financial planning — where thoughtful pauses are expected.

Another approach is to disconnect after a silence threshold. If the user is silent for more than twenty seconds, the AI says "I will disconnect now. Call back anytime," and hangs up. This prevents runaway billing from users who walked away without hanging up. But it requires careful threshold tuning. Disconnect too early and you frustrate users who are legitimately thinking. Disconnect too late and you waste money on dead connections.

A legal intake voice assistant in mid-2025 set a thirty-second silence threshold for disconnection. If the user was silent for thirty seconds straight, the system disconnected. This cut their average call length from 4.2 minutes to 3.6 minutes. The reduction was entirely from eliminating dead air at the end of calls where users had walked away. The thirty-second threshold was long enough that it never triggered during active conversation. It only triggered when the user had abandoned the call without hanging up. The 0.6-minute reduction per call saved them $0.078 at their $0.13 per minute rate. Across 22,000 calls monthly, that was $1,716 saved by not paying for dead connections.

## The Silence Budget: What Is Acceptable

Every voice AI should have a silence budget — a target percentage of each call that is silence. The budget depends on your use case. A transactional assistant should target fifteen to twenty percent silence. A consultative assistant can tolerate thirty to forty percent. Anything above fifty percent is almost certainly wasteful.

Calculate your silence percentage by dividing total silence seconds by total call seconds. If your average call is 120 seconds and 38 seconds is silence, your silence percentage is thirty-two percent. Compare that to your budget. If your budget is twenty-five percent, you are overspending by seven percentage points. At $0.12 per minute, that is $0.014 per call in excess silence cost. Across 100,000 calls, that is $1,400 monthly in avoidable silence billing.

Breaking down silence by type helps you prioritize fixes. If processing silence is eighteen seconds per call, user thinking time is twelve seconds, and dead air is eight seconds, your biggest opportunity is processing silence. Optimize the component chain first. Then tackle dead air with automatic disconnection. User thinking time is the last thing to address, and only if the other two are already minimized.

## Monitoring Silence Costs in Production

Silence cost should be a first-class metric in your production dashboard. Track silence percentage per call, average silence seconds per call, and total silence cost per day. Set alerts for when silence percentage exceeds your budget by more than five percentage points. This catches regressions early.

A customer support voice agent in late 2025 tracked silence cost daily and noticed a sudden spike. Silence percentage jumped from twenty-four percent to thirty-nine percent over three days. The team investigated and found that a recent LLM deployment had increased average generation time from 2.1 seconds to 4.7 seconds. The new model was more accurate, but slower. The accuracy improvement was intentional. The latency increase was not. They rolled back the deployment, retrained the model with latency constraints, and redeployed. Silence percentage dropped back to twenty-five percent. Without daily silence monitoring, they would not have noticed the regression until the monthly invoice arrived.

Silence is the most invisible cost in voice AI. You cannot see it in conversation logs. You cannot hear it in quality reviews. You only see it when you measure it explicitly. And once you measure it, you realize how much you are paying for nothing — and how much you can save by optimizing what should not have been billable in the first place.

The next cost trap is less predictable: token burst amplification. One complex question can spike your LLM usage by ten times for a single turn, and the cost shows up instantly.

