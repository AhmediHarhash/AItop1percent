# 6.10 — User Satisfaction Correlation: What Predicts Happy Callers

The team had every metric green. Task completion rate at 94%, response latency at 280ms, ASR accuracy at 97%, interruption rate below 2%. The dashboard looked perfect. Then customer satisfaction scores arrived: 3.2 out of 5, down from 3.8 the previous quarter. Users were leaving frustrated despite the system hitting every internal quality bar.

This pattern repeats across voice AI deployments. Teams optimize for metrics they can measure easily — latency, accuracy, uptime — while users care about something harder to quantify: did this conversation feel productive? Did the system understand what I needed? Did I get my problem solved without wanting to scream? The correlation between internal component metrics and actual user satisfaction is weaker than most teams assume. You need to know which metrics predict happy callers and which are measurement theater.

## The Satisfaction Prediction Gap

User satisfaction in voice AI is driven by perceptual factors that do not always correlate with technical metrics. A system can have perfect ASR accuracy but still frustrate users because it interrupts at awkward moments. Response latency can be well within acceptable bounds but feel slow if the system gives no acknowledgment while processing. Task completion can be high but satisfaction low if the conversation required too many turns or felt unnatural.

The gap exists because component metrics measure what the system does, not how it feels. Latency measures milliseconds. Users experience rhythm. ASR accuracy measures word error rate. Users experience whether the system understood their intent. Interruption rate measures overlapping speech. Users experience whether the system respects conversational flow.

Teams that optimize only for component metrics hit a ceiling. You can shave latency from 300ms to 250ms and see no movement in satisfaction scores. You can improve ASR accuracy from 95% to 97% and still have users complaining that the system does not listen. The metrics that predict satisfaction are often second-order effects — combinations of component metrics, contextual factors, and perceptual qualities that emerge from the full conversation.

## Metrics That Actually Predict Satisfaction

Research and production data from voice AI deployments consistently show a small set of metrics that correlate most strongly with user satisfaction. These are not always the ones teams measure first.

**Task success rate** is the strongest predictor. Not task completion — task success. Completion means the conversation ended. Success means the user got what they needed. A customer service call that completes but does not resolve the issue shows high completion, low success. Task success correlates with satisfaction at 0.7 to 0.85 depending on domain. Nothing else comes close. If users accomplish their goal, they forgive latency hiccups, occasional misrecognitions, and minor awkwardness. If they do not accomplish their goal, perfect technical metrics do not matter.

Measuring task success requires knowing what users wanted. For structured tasks — booking appointments, checking account balances, resetting passwords — you can infer intent from conversation outcomes. Did the appointment get booked? Did the user receive the balance? For unstructured tasks — customer support inquiries, technical troubleshooting — you need post-conversation surveys or human review. Teams that skip this measurement optimize for completion and wonder why satisfaction does not improve.

**Conversation efficiency** is the second strongest predictor. Efficiency means how many turns it took to accomplish the task relative to how many turns it should have taken. A password reset that takes three turns feels smooth. A password reset that takes eleven turns because the system kept asking for clarification feels frustrating. Efficiency correlates with satisfaction at 0.5 to 0.7.

Efficiency is not the same as brevity. Some conversations benefit from extra turns — a friendly greeting, a confirmation that the user's request was understood, a summary of what will happen next. The key is whether those turns add value or waste time. Users tolerate longer conversations if each turn moves toward the goal. They abandon shorter conversations if turns feel repetitive or circular.

You measure efficiency by establishing a baseline turn count for each task type. A simple information lookup should take 2-4 turns. A transaction requiring verification should take 5-8 turns. A complex troubleshooting session might need 12-20 turns. When actual conversations exceed baseline by more than 50%, satisfaction drops. When they match or come in below baseline, satisfaction rises.

**Perceived responsiveness** predicts satisfaction more strongly than raw latency. Responsiveness is not just how fast the system replies — it is whether the system gives feedback during processing. A system that stays silent for 800ms while generating a response feels slower than a system that says "let me check that" at 200ms and then delivers the answer at 900ms total. The acknowledgment resets the user's mental clock.

Voice AI teams from customer support platforms report that adding acknowledgment tokens — "one moment," "checking now," "got it" — improved satisfaction scores by 0.3 to 0.5 points on a 5-point scale even when total response time increased slightly. Users do not count milliseconds. They experience whether the system feels engaged or frozen.

You measure perceived responsiveness by tracking both technical latency and conversational acknowledgment coverage. What percentage of responses longer than 600ms include an acknowledgment token? What percentage of user queries receive some feedback within 300ms, even if the full answer takes longer? High coverage correlates with higher satisfaction.

**Error recovery quality** is the fourth strong predictor. Errors happen. ASR misrecognizes a name. Intent classification fails. The backend service times out. What matters for satisfaction is not whether errors occur but how the system handles them. A system that says "I did not catch that, could you repeat it?" after a misrecognition feels cooperative. A system that says "I did not understand" five times in a row feels broken.

Recovery quality has three components: recognition that an error occurred, a clear explanation of what went wrong, and a constructive path forward. "I did not hear the last part of your address" is better than "invalid input." "I could not find that account number, can you confirm the last four digits?" is better than "error." Systems that explicitly acknowledge errors and guide users toward solutions maintain satisfaction even when component accuracy is mediocre. Systems that fail silently or give generic error messages destroy satisfaction even when accuracy is high.

You measure error recovery quality by tagging conversations where errors occurred and then comparing satisfaction scores for good recovery versus poor recovery. Good recovery includes acknowledgment, explanation, and guidance. Poor recovery is repetition, generic messages, or silent failure. In production systems, good recovery salvages 60-80% of otherwise-failed conversations.

## Domain-Specific Satisfaction Drivers

The metrics that predict satisfaction vary by use case. What matters in a banking voice assistant differs from what matters in a healthcare appointment scheduler differs from what matters in an automotive voice interface.

For transactional use cases — banking, booking, ordering — speed and accuracy dominate. Users want to complete tasks quickly and correctly. Task success and conversation efficiency drive satisfaction. Latency matters, but only when it exceeds 500-600ms. Conversational naturalness matters less. Users tolerate robotic phrasing if the system gets the job done fast.

For informational use cases — customer support, technical help, FAQ systems — explanation quality drives satisfaction. Users want to understand the answer, not just receive it. A system that says "your order will arrive Tuesday" rates lower than a system that says "your order shipped yesterday from our Ohio warehouse and will arrive Tuesday via standard ground shipping." Completeness and clarity predict satisfaction more than speed. Users will wait an extra second for a thorough answer.

For high-stakes use cases — medical triage, financial advice, legal information — trust and accuracy are paramount. Users need confidence that the system understood their situation and provided correct guidance. Any hint of misunderstanding or uncertainty destroys satisfaction. These domains require higher ASR accuracy thresholds, more explicit confirmations, and lower tolerance for errors. Satisfaction correlates most strongly with perceived reliability — did the system demonstrate that it understood the stakes?

For ambient or companion use cases — in-car assistants, smart home devices, entertainment systems — interruption cost drives satisfaction. Users multitask while using these systems. An interruption that pulls their attention at the wrong moment — while driving, while cooking, while watching a movie — creates frustration even if the system is technically accurate. Satisfaction correlates with how well the system respects user context and timing. A system that waits for a natural pause before speaking rates higher than one that interrupts mid-sentence.

Teams that apply generic satisfaction metrics across all domains miss these differences. You cannot optimize a healthcare triage bot the same way you optimize a music player voice interface. The component metrics might be identical. The satisfaction drivers are not.

## Building a Satisfaction Correlation Model

Knowing which metrics predict satisfaction in general is useful. Knowing which metrics predict satisfaction for your specific system in your specific domain with your specific users is essential. You build a satisfaction correlation model by collecting both internal metrics and user satisfaction data, then analyzing which metrics move together.

Start by defining satisfaction measurement. Most teams use post-conversation surveys with a single question: "How satisfied were you with this interaction?" on a 5-point scale. Some teams use CSAT — Customer Satisfaction Score — calculated as percentage of users rating 4 or 5. Some use NPS — Net Promoter Score — calculated from "how likely are you to recommend this service?" Both work. CSAT is easier to interpret. NPS correlates better with long-term retention. Pick one and use it consistently.

Collect satisfaction data for every conversation or a statistically significant sample. If you handle 100,000 conversations per week, surveying 2-3% gives you 2,000-3,000 responses — enough to detect correlations. Do not rely on voluntary feedback. Users who volunteer feedback skew negative. Random sampling produces representative data.

Instrument your system to log component metrics for every conversation: latency percentiles, ASR accuracy, intent classification confidence, turn count, error rate, interruption count, response completeness, acknowledgment coverage. Store these alongside satisfaction scores.

Run correlation analysis. For each component metric, calculate Pearson or Spearman correlation with satisfaction scores. Metrics with correlation above 0.4 are meaningful predictors. Metrics below 0.2 are noise. Metrics between 0.2 and 0.4 are weak signals worth monitoring but not optimizing aggressively.

Look for threshold effects. Some metrics do not correlate linearly. Latency below 400ms might have no correlation with satisfaction — all users are equally happy. Latency above 600ms might have strong negative correlation — satisfaction drops fast. Identifying these thresholds tells you where to focus optimization effort. Improving latency from 300ms to 250ms gains nothing. Improving latency from 700ms to 500ms gains a lot.

Build a regression model that predicts satisfaction from component metrics. This model does not need to be complex. A simple linear regression with 5-8 features often explains 50-70% of variance in satisfaction scores. The model tells you which levers to pull. If task success has a coefficient of 0.6 and latency has a coefficient of 0.1, you know that improving task success by 5 percentage points is worth more than cutting latency in half.

Update the model quarterly. User expectations shift. New competitors set new standards. A latency threshold that was acceptable in 2024 might feel slow in 2026. A task success rate that delighted users last year might be table stakes this year. Satisfaction drivers evolve. Your correlation model must evolve with them.

## Survey Design and Bias Mitigation

Satisfaction surveys introduce measurement error if designed poorly. The way you ask the question, when you ask it, and how you sample users all affect the data.

Ask the satisfaction question immediately after the conversation, not hours later. Memory fades. Users who rate a conversation three hours after it ended base their score on whether they ultimately solved their problem, not on whether the voice interaction was good. Users who rate it immediately base their score on the interaction itself. Immediate surveys produce cleaner signal.

Use a single, clear question. "How satisfied were you with this conversation?" works. "Please rate your experience with our voice AI system on a scale of 1 to 5 where 1 is very dissatisfied and 5 is very satisfied" also works. Avoid compound questions like "How satisfied were you with the accuracy and speed of the system?" Users cannot rate two things at once.

Sample randomly, not just from users who completed conversations successfully. If you only survey users who reached the end of the conversation, you miss users who hung up in frustration. Those users have the most valuable feedback. Use a sampling strategy that includes abandoned conversations, repeat callers, and error-heavy sessions.

Beware of survey fatigue. If you ask users to rate every conversation, response rates drop and remaining respondents skew toward extreme opinions — very happy or very angry. A 5-10% random sample sustains response rates and produces representative data.

Account for context outside the conversation. A user who calls to dispute a charge is more likely to rate the conversation poorly than a user who calls to check a balance, regardless of conversational quality. A user who reaches the voice system after waiting on hold for 20 minutes starts frustrated. These contextual factors introduce noise into satisfaction data. You can control for them by segmenting analysis by conversation type, time of day, or customer history.

Some teams track sentiment during the conversation using voice tone analysis or post-call sentiment classification. These signals correlate with satisfaction but introduce their own biases. Sentiment analysis trained on generic speech data often misclassifies domain-specific language. A user saying "I need to cancel my subscription" might sound neutral or positive to a sentiment classifier but reflects dissatisfaction with the product. Use sentiment as a supplementary signal, not a replacement for direct surveys.

## What Component Metrics Do Not Predict Satisfaction

Some metrics that teams obsess over have almost no correlation with user satisfaction. Measuring them is fine. Optimizing for them at the expense of metrics that matter is waste.

ASR word error rate has weak correlation with satisfaction once accuracy exceeds 90-92%. Users do not count misrecognized words. They care whether the system understood their intent. A system that misrecognizes two words but correctly classifies the intent and completes the task rates higher than a system with perfect word-level accuracy that fails to understand what the user wanted. You need ASR accuracy high enough to support intent classification. Beyond that threshold, improving ASR further does not move satisfaction.

Raw response latency below 400ms has negligible correlation with satisfaction. Users do not perceive differences between 200ms and 350ms. They perceive differences between 400ms and 800ms. Teams that chase every millisecond of latency improvement below 400ms are optimizing a metric users do not care about. Use that engineering time on task success or error recovery instead.

Interruption rate has weak correlation unless interruptions occur at highly disruptive moments. A system that occasionally speaks while the user is still finishing a sentence does not bother users much if the interruptions are brief and the system self-corrects. A system that interrupts right before the user provides critical information — the account number, the confirmation code, the address — destroys satisfaction. Interruption count matters less than interruption context.

Conversation length in seconds or turns has almost no correlation with satisfaction in most domains. Users tolerate long conversations if they feel productive. Users abandon short conversations if they feel stuck. What correlates is efficiency — turns relative to baseline — not absolute length. Do not optimize for brevity. Optimize for progress.

Response diversity or creativity scores have no correlation with satisfaction in transactional or informational use cases. Users do not care if the system uses varied phrasing. They care if the system answers their question. Response diversity matters in entertainment or companion use cases where users expect personality. It does not matter in banking or customer support.

System uptime above 99.5% has weak correlation. Users experience availability as binary — the system works or it does not. Improving uptime from 99.5% to 99.9% prevents an additional outage every few months. Most users never notice. Improving task success from 85% to 90% affects every user every day. Prioritize accordingly.

## Using Satisfaction Correlations to Set Roadmap Priorities

A satisfaction correlation model turns abstract improvement ideas into ranked priorities. When product asks whether to optimize latency or improve error recovery, the model provides the answer. When engineering asks whether to invest in better ASR or better intent classification, the model tells you which delivers more satisfaction gain per unit of effort.

Run the analysis quarterly. Identify the top three metrics with the strongest satisfaction correlation and weakest current performance. Those are your high-leverage opportunities. A metric that correlates at 0.6 with satisfaction but currently scores at 70% performance has more improvement headroom than a metric that correlates at 0.4 but already scores at 95%.

Calculate satisfaction elasticity: how much does a 5-percentage-point improvement in metric X improve satisfaction score Y? If improving task success from 80% to 85% raises average satisfaction from 3.5 to 3.8, that is 0.06 satisfaction points per percentage point of task success improvement. If cutting latency from 500ms to 450ms raises satisfaction from 3.5 to 3.55, that is 0.001 satisfaction points per millisecond. The elasticity comparison makes trade-offs explicit.

Communicate priorities in terms of user impact, not technical metrics. Instead of "we are reducing P95 latency by 100ms," say "we are improving perceived responsiveness, which correlates with a 0.2-point satisfaction increase and affects 15% of conversations." Engineering teams respond to technical goals. Leadership responds to user outcomes. Satisfaction correlations bridge the gap.

Track whether improvements in component metrics actually translate to satisfaction gains. Sometimes correlations reverse. A metric that predicted satisfaction in 2025 stops predicting it in 2026 because user expectations changed or because the system hit a performance ceiling where further gains do not matter. If you improve a metric but satisfaction does not move, revisit your correlation model. Either the correlation was spurious, or you hit a threshold effect, or a different factor is now limiting satisfaction.

The goal is not perfect prediction. The goal is better prioritization. A correlation model that explains 60% of satisfaction variance is good enough to make smarter roadmap decisions than intuition alone.

Conversational quality metrics give you diagnostic power. Satisfaction correlation gives you strategic direction. Together they tell you what to measure, what to optimize, and what user outcomes you can expect. The next question is how to evaluate multi-turn conversations at scale — which requires building datasets and protocols designed for conversational depth, not just individual turn accuracy.

