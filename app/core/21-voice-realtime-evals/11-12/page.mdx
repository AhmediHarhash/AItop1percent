# 11.12 — Building Voice-Specific On-Call Runbooks

The page came at 2:47 AM. The alert read "Voice latency P99 exceeded threshold." The on-call engineer, awake for twelve seconds, opened the monitoring dashboard on their phone. Latency was high. That was clear. What was not clear was what to do about it. Was this a provider issue requiring failover? A scaling issue requiring more infrastructure? A deployment issue requiring rollback? A network issue requiring waiting? The engineer spent twenty minutes investigating before determining the LLM inference service was experiencing load-induced latency and needed immediate scaling. By the time mitigation completed, the incident had persisted for thirty-eight minutes. The next morning, the team lead asked why it took twenty minutes to diagnose a known failure pattern that had occurred twice before. The answer was simple: there was no runbook. The engineer had to rediscover the diagnosis procedure under pressure in the middle of the night.

2 AM pages require runbooks. Judgment under pressure is unreliable. Memory under sleep deprivation is unreliable. The on-call engineer at 2 AM is not operating at full cognitive capacity. They need checklists, decision trees, and exact commands to execute. The runbook transforms an ambiguous alert into a structured response procedure that can be followed mechanically without requiring deep system understanding or creative problem-solving. The engineer should be able to read the runbook and execute the steps without needing to understand why each step works, only that it does.

## Runbook Structure for Voice Incidents

Voice incident runbooks follow a consistent structure that makes them scannable under pressure. Each runbook is a single document dedicated to one alert type or one incident category. The document begins with the alert name and description, followed by immediate triage steps, then diagnosis procedures, then mitigation options, then escalation criteria. The structure is the same for every runbook. The on-call engineer learns the structure once and can navigate any runbook efficiently.

The alert identification section at the top of the runbook states exactly what alert triggered the runbook and what the alert means in plain language. "Voice latency P99 exceeded threshold" is the alert name. "Ninety-nine percent of voice responses are taking longer than one point eight seconds, which exceeds the user experience threshold of one point five seconds" is the plain language explanation. This section also states expected false positive rate and common false positive causes. If the alert has a ten percent false positive rate and most false positives occur during scheduled provider maintenance, the runbook says so immediately. The engineer knows to check the maintenance calendar before panicking.

The immediate triage section is a numbered checklist of steps to execute in the first sixty seconds. These steps are purely observational. They do not change anything. They establish situational awareness. For a latency incident, the triage checklist might include: check overall conversation success rate, check per-provider latency breakdown, check recent deployment timeline, check provider status pages, check infrastructure CPU and memory utilization. Each step takes five to fifteen seconds. After completing the checklist, the engineer has a clear picture of system health and likely root cause category.

The diagnosis decision tree follows triage. It uses the observations from triage to narrow down root cause. The tree is structured as yes-or-no questions. "Is latency high only for one provider?" If yes, go to provider-specific diagnosis section. If no, continue. "Did a deployment occur in the last two hours?" If yes, go to deployment rollback section. If no, continue. "Is infrastructure CPU utilization above eighty percent?" If yes, go to scaling section. The decision tree eliminates ambiguity. The engineer does not need to decide what the problem is. They answer a series of simple questions and the tree routes them to the right mitigation procedure.

The mitigation procedures section contains exact commands to execute for each identified root cause. These are not conceptual descriptions of what to do. They are copy-paste-execute commands with placeholders for environment-specific values. "To rollback the voice model deployment, execute: kubectl rollout undo deployment/voice-model-service -n production." The engineer does not need to remember kubectl syntax or deployment names. They copy the command, verify it looks correct, execute it. Each mitigation procedure includes expected impact and expected recovery time. "Rollback will complete in ninety seconds. Latency should return to baseline within three minutes after rollback completes."

The verification section follows mitigation. After executing a mitigation, the engineer must verify it worked. The runbook specifies which metrics to check and what values indicate successful mitigation. "After rollback completes, verify voice latency P99 drops below one point five seconds within five minutes. Verify conversation success rate remains above ninety-five percent. If latency does not improve, proceed to escalation." Verification prevents the engineer from assuming mitigation worked and moving on when the problem persists.

The escalation section specifies when to escalate and who to page. Escalation criteria are objective. "If latency does not improve within ten minutes of rollback, page the voice platform team lead. If latency improves but remains above one second, create an incident channel and tag the ML engineering team for investigation during business hours. If multiple mitigation attempts fail, escalate to severity one and page the engineering manager." The engineer does not decide whether to escalate. The runbook tells them.

## Provider-Specific Troubleshooting Sections

Voice systems integrate multiple third-party providers for speech recognition, synthesis, and telephony. Each provider has unique failure modes, unique API characteristics, and unique troubleshooting procedures. The runbook includes provider-specific sections that address common issues with each provider. These sections are referenced from the main diagnosis decision tree but contain the detailed provider knowledge that on-call engineers cannot be expected to memorize.

The speech recognition provider section covers common issues like elevated error rates, increased latency, degraded accuracy, and unexpected billing. For each issue, the section provides diagnosis steps specific to that provider. If the provider is returning five-hundred-series errors, the diagnosis steps include checking the provider's status page, reviewing recent API quota changes, and verifying authentication credentials have not expired. If the provider's latency is elevated but error rate is normal, the diagnosis steps include checking if the provider is experiencing documented performance issues and whether the issue is regional or global.

Mitigation options for provider issues include failover to backup providers, request throttling to reduce load on struggling providers, and feature degradation to reduce request complexity. The runbook specifies exactly how to trigger failover. "To fail over from Provider A to Provider B, set the feature flag voice-recognition-provider to provider-b using: ff-cli set voice-recognition-provider provider-b --environment production." The runbook also specifies rollback procedures. "To revert to Provider A, execute: ff-cli set voice-recognition-provider provider-a --environment production."

The speech synthesis provider section covers synthesis-specific issues like character limit errors, voice cloning failures, and regional availability outages. Synthesis providers often have more complex failure modes than recognition providers because synthesis involves voice selection, language selection, emotional tone parameters, and streaming versus non-streaming modes. The runbook documents known issues with each mode and how to detect which mode is failing. If streaming synthesis is failing but non-streaming works, the runbook provides the command to disable streaming temporarily.

The telephony provider section is critical because telephony failures prevent users from even reaching the voice AI system. Issues include SIP trunk failures, caller ID delivery problems, DTMF tone recognition issues, and call recording failures. The runbook includes diagnostic commands to check telephony provider status and mitigation steps to route traffic to backup telephony providers if configured. For organizations using multiple telephony providers, the runbook specifies the failover order and any limitations of each provider.

Provider authentication and credential management is a common failure point that affects all providers. The runbook includes a section on checking credential expiration and rotating credentials. If API calls to a provider start returning four-hundred-and-one authentication errors, the immediate mitigation is verifying the credentials stored in the secret management system are current and match the credentials expected by the provider. The runbook provides exact commands to check credential expiration dates and to rotate credentials if needed.

Provider rate limits and quota exhaustion cause failures that appear as five-hundred-and-three or four-hundred-and-twenty-nine errors. The runbook explains how to distinguish quota exhaustion from other errors and provides mitigation steps including throttling traffic, failing over to providers with available quota, and requesting emergency quota increases from the provider's support team. The runbook includes contact information and escalation procedures for each provider's support team so the on-call engineer can request help quickly.

## Rollback Procedures for Voice Deployments

Voice systems require specialized rollback procedures because voice deployments often involve multiple components that must roll back together. A model deployment might include a new LLM inference endpoint, updated prompts, new intent classifiers, and modified conversation flows. Rolling back only the model but not the prompts can cause mismatches that break conversations. The runbook specifies rollback procedures that maintain consistency across components.

The deployment rollback checklist begins by identifying what was deployed. The monitoring system's deployment timeline shows exactly what changed and when. The runbook includes a command to query the deployment timeline: "To view recent deployments, execute: deploy-cli history --service voice-system --hours 24." The output lists deployments with timestamps, component names, and version identifiers. The engineer identifies which deployment correlates with the incident start time.

Single-component rollback procedures apply when only one component changed. If only the LLM inference service was deployed, only that service needs rollback. The runbook provides the exact rollback command: "To rollback the LLM inference service, execute: kubectl rollout undo deployment/llm-inference -n production." The command is idempotent. Running it multiple times is safe. The runbook specifies expected rollback duration: "Rollback completes in sixty to ninety seconds. Traffic automatically shifts to the previous version as new pods become ready."

Multi-component rollback procedures apply when multiple components deployed together. If the deployment included model updates, prompt updates, and conversation flow updates, all three must roll back to maintain consistency. The runbook specifies the rollback order. Models roll back first, then prompts, then conversation flows. The order prevents mismatches where the old model receives new prompts it was not trained with. The runbook provides a script that rolls back all components in the correct order: "To rollback the full voice system deployment, execute: ./scripts/rollback-voice-system.sh --deployment-id 12345." The script handles component ordering and verification automatically.

Rollback verification is mandatory. After rollback executes, the engineer verifies the system returned to healthy state. The runbook specifies verification steps: "After rollback completes, verify latency P99 is below one point five seconds, conversation success rate is above ninety-five percent, and error rate is below two percent. Monitor metrics for five minutes to ensure degradation does not recur." If verification fails, the runbook escalates to severity one because rollback failed to resolve the issue, indicating a more complex root cause.

Partial rollback applies when the deployment cannot fully roll back because the new version introduced database schema changes, external API integrations, or data format changes that are not backward compatible. The runbook identifies deployments that cannot be fully rolled back and provides alternative mitigation strategies. If rollback is not possible, the mitigation might be hotfixing the new version, failing over to a backup system, or gracefully degrading functionality until a fix can be deployed.

Rollback communication is part of the procedure. After initiating rollback, the engineer posts in the incident channel: "Rollback initiated for deployment 12345 at HH:MM. Expected completion in ninety seconds. Monitoring metrics for verification." This communication keeps stakeholders informed and creates a timeline for post-incident review. The runbook includes message templates to copy and paste into incident channels so the engineer does not need to compose messages under pressure.

## Communication Templates for Voice Outages

When voice systems fail, customers and internal stakeholders need clear, timely communication. The on-call engineer is not a communications expert. They are focused on restoring service. The runbook includes pre-written communication templates for common incident scenarios that the engineer can copy, customize with incident-specific details, and send to appropriate channels. The templates balance transparency with avoiding panic.

The initial incident notification template announces the incident and sets expectations: "We are currently investigating elevated error rates in the voice AI system. Users may experience failed calls or longer than normal wait times. Our team is actively working on resolution. We will provide updates every thirty minutes." The template is intentionally vague about root cause because root cause is often unknown initially. It focuses on user impact and response cadence.

The mitigation-in-progress update template informs stakeholders that the issue is understood and mitigation is underway: "We have identified the cause of the voice system degradation as a latency issue with the LLM inference service. We are scaling up infrastructure to resolve the issue. Expected time to resolution is fifteen minutes. Next update in fifteen minutes or when resolved, whichever comes first." This template provides enough detail to build confidence without over-committing to timelines that may not be met.

The resolution notification template announces that the incident is resolved and service is restored: "The voice system incident has been resolved. Latency and error rates have returned to normal. All voice AI functionality is operating as expected. A post-incident review will be conducted to prevent recurrence." This template closes the incident clearly and commits to follow-up investigation without providing details that are not yet available.

The extended outage template applies when mitigation is taking longer than expected: "The voice system incident is ongoing. We are continuing mitigation efforts. Current estimated time to resolution is forty-five minutes. We have escalated to senior engineering leadership. For urgent issues, please contact human support at [contact info]. Next update in thirty minutes." This template acknowledges the situation is more serious than initially expected and provides alternative support channels.

The partial service restoration template applies when mitigation restored some functionality but not all: "We have partially restored voice system functionality. Approximately seventy percent of calls are completing successfully. We are continuing to work on full restoration. Users may experience intermittent issues. Next update in twenty minutes." This template manages expectations by clarifying that service is improved but not fully recovered.

Templates are customized by replacing bracketed placeholders with incident-specific details: "[error type]", "[affected percentage]", "[estimated resolution time]", "[root cause summary]". The runbook includes guidance on how to fill in each placeholder. For estimated resolution time, the guidance says: "Use conservative estimates. It is better to resolve faster than estimated than to miss your estimate. If uncertain, say 'under investigation' rather than providing a specific time."

Internal communication templates differ from customer-facing templates. Internal templates include more technical detail and are more direct about severity. "Severity one incident: voice system total failure. All calls failing. Root cause: provider API outage. Mitigation: failing over to backup provider. ETA: five minutes. Engineering manager paged." Internal templates assume technical literacy and prioritize speed over polish.

## Severity Determination for Voice Incidents

Severity levels determine response urgency, escalation paths, and communication frequency. The runbook provides objective criteria for assigning severity levels so that all on-call engineers apply the same standards. Severity determination happens during initial triage based on observed system behavior and user impact.

Severity one incidents are total failures or failures affecting more than fifty percent of users. The voice system is not processing conversations, or more than half of conversations are failing. All severity one incidents page the engineering manager immediately and require all-hands response. Severity one incidents are the only incidents that justify waking people at night. The runbook is explicit: "If the incident meets severity one criteria, page the engineering manager immediately via PagerDuty. Do not wait to investigate further. Do not attempt solo mitigation before escalating."

Severity two incidents are partial failures affecting ten to fifty percent of users, or quality degradation that significantly impacts user experience. The voice system is partially functional, but many users are experiencing failures or poor quality. Severity two incidents require creating an incident channel and tagging relevant team members. During business hours, severity two incidents escalate to the team lead within fifteen minutes. Overnight, severity two incidents page the team lead if they persist more than thirty minutes without improvement.

Severity three incidents affect less than ten percent of users or represent minor degradation that does not significantly impact user experience. The on-call engineer handles severity three incidents solo and documents the incident in the incident log. No escalation is required unless the incident reveals a novel failure mode or poses risk of escalating to higher severity. Most alerts are severity three. The runbook helps engineers confirm whether an incident can stay severity three or needs escalation.

Severity determination considers not just current impact but trajectory. An incident currently affecting five percent of users but with metrics degrading rapidly is treated as severity two because it is likely to become severity one soon. The runbook includes guidance on trajectory assessment: "If error rate is increasing more than one percentage point per minute, or if latency is increasing more than one hundred milliseconds per minute, escalate immediately even if current impact is below severity two threshold."

Customer impact overrides technical metrics when determining severity. If technical metrics show minor degradation but the customer support team reports overwhelming complaint volume or social media shows users posting about failures, the incident is at least severity two regardless of what the metrics say. The runbook instructs engineers to check the support queue length and social media mentions during triage to catch incidents where user impact exceeds what metrics show.

Regulatory and contractual obligations affect severity determination. If the voice system is subject to a service-level agreement guaranteeing ninety-nine point nine percent uptime, any incident that threatens the monthly uptime commitment is automatically severity two even if current impact is minor. If the voice system handles emergency services, healthcare, or financial transactions, any degradation is at least severity two because user impact includes potential safety or financial harm. The runbook documents these special cases.

## The On-Call Rotation for Voice Systems

Voice systems require specialized on-call rotations because voice incidents are different from typical software incidents. The on-call engineer must understand real-time systems, provider integrations, telephony infrastructure, and voice-specific metrics. Not every engineer on the team has this expertise. The on-call rotation structure balances coverage with expertise requirements and prevents burnout.

The primary on-call rotation includes only engineers who have completed voice systems training and have at least three months of experience with the voice platform. This qualification requirement ensures that the primary on-call engineer can handle common incidents without escalation. New team members shadow the primary on-call for at least two rotations before taking primary themselves. Shadowing means the new engineer is paged for every incident but the experienced engineer leads response.

The secondary on-call rotation includes senior engineers who can handle complex incidents that exceed the primary on-call's expertise. The secondary is only paged for severity one incidents or when the primary on-call explicitly escalates. The secondary rotation uses longer shifts — one week instead of the primary's three-day rotation — because secondary pages are less frequent. The secondary provides backup and mentorship but is not the first responder.

Rotation length affects engineer wellbeing and incident familiarity. Three-day rotations are short enough that engineers do not burn out from extended high-stress periods but long enough that engineers develop familiarity with current system behavior. One-day rotations are too short. Engineers spend the first few hours of their rotation getting oriented. One-week rotations are too long. By day six, engineers are fatigued and response quality declines. Three-day rotations are the tested standard for voice systems in 2026.

Handoff procedures ensure context transfers between rotations. At the end of each rotation, the outgoing on-call engineer writes a brief summary: incidents that occurred, ongoing issues being monitored, upcoming deployments or maintenance, and any runbook gaps or improvements identified during the rotation. The incoming on-call engineer reads the summary before their rotation begins. The handoff document is shared in the team channel and archived. Over time, these handoff documents create a knowledge base of incident patterns and system behavior.

Follow-the-sun rotations are necessary for global voice systems that operate twenty-four hours across multiple regions. A single team in one timezone cannot sustain overnight on-call indefinitely. Follow-the-sun rotations split coverage by region. Asia-Pacific engineers cover their business hours, European engineers cover theirs, and US engineers cover theirs. No one is on-call overnight in their local timezone. This structure requires at least three regional teams and clear handoff procedures when coverage shifts regions.

On-call compensation and time-off policies prevent burnout. Engineers on-call receive additional compensation or compensatory time off. If an engineer is paged overnight, they receive the next day off or can start late the following morning. If an on-call rotation involves more than two severity one incidents, the team reviews the rotation cadence and staffing to ensure the load is sustainable. Burnout leads to engineers leaving on-call rotations, which concentrates load on remaining engineers, which causes more burnout. Preventing the cycle requires treating on-call sustainability as a top priority.

Runbook maintenance is part of the on-call rotation. Each on-call engineer is expected to improve at least one runbook during their rotation based on incidents they handled or gaps they identified. If an incident required investigation that was not covered in the runbook, the engineer adds that investigation procedure before rotating off. This continuous improvement ensures runbooks remain current and comprehensive. A runbook that is never updated becomes obsolete as the system evolves.

Voice-specific on-call runbooks transform middle-of-the-night panic into structured, mechanical response procedures. The engineer who follows the runbook does not need to be the most experienced person on the team. They need to be competent, careful, and capable of following instructions under pressure. The runbook does the thinking. The engineer does the executing. This division of cognitive labor makes voice systems operable twenty-four hours a day without requiring every engineer to achieve deep system expertise. The next chapter covers enterprise voice operations, where voice AI scales from hundreds of conversations to hundreds of thousands.

