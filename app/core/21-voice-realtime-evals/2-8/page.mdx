# 2.8 — End-of-Turn Detection: The Hardest Timing Problem

A user speaks, pauses. Is the pause the end of their sentence or the middle of their thought? Your system has 200 milliseconds to decide. If you wait too long, the interaction feels sluggish — the user finishes speaking, expects a response, and hears silence. If you respond too early, you interrupt them mid-sentence — the user is still formulating their question, and the assistant cuts them off. End-of-turn detection is the timing problem every voice pipeline faces, and it has no perfect solution.

End-of-turn detection determines when the system stops listening and starts responding. The decision is latency-critical. A conversational voice assistant in 2026 targets 300 to 500 milliseconds from end-of-speech to start-of-response. That budget includes end-of-turn detection, ASR finalization, LLM inference, and TTS start. If end-of-turn detection takes 400 milliseconds because the system is waiting to be sure the user is done, the entire budget is consumed before the LLM even runs. The interaction feels broken.

## The Pause-Based Heuristic

The simplest end-of-turn detection method is a silence timer. When VAD stops detecting speech, the system starts a countdown. If silence continues for a threshold duration — 700 milliseconds is common in 2026 — the system treats the pause as the end of the user's turn and proceeds to transcription and response generation. If VAD detects speech again before the threshold, the timer resets and the system keeps listening.

This heuristic works when users speak in complete sentences and pause naturally at the end. It breaks when users pause in the middle of a sentence. A user says, "I need to... uh... transfer money to my savings account." The pause after "to" lasts 800 milliseconds while the user thinks. A 700-millisecond silence threshold triggers end-of-turn detection during the pause. The system transcribes "I need to," the LLM receives incomplete input, and the response is either a clarification request or a guess. Both are wrong.

The threshold duration is a tradeoff. A short threshold — 400 milliseconds — reduces perceived latency. The user finishes speaking and gets a response quickly. But a 400-millisecond threshold interrupts users who pause to breathe or think. A long threshold — 1,200 milliseconds — avoids interruptions. But it makes every interaction feel slow. The user finishes speaking, waits a full second, and then hears the response. That second of silence is dead air. The user perceives the system as unresponsive.

The optimal threshold depends on the user population and the interaction type. Customers calling a support line are often stressed, uncertain, and pausing frequently. They need a longer threshold to avoid interruptions. Users giving short commands to a voice assistant — "Set a timer for 10 minutes" — speak quickly and expect fast responses. They tolerate a shorter threshold. A single threshold does not work for both.

## Semantic End-of-Turn Detection

Pause-based detection ignores content. The system does not know whether the user's sentence is complete or incomplete — it only knows the user is silent. Semantic end-of-turn detection uses ASR transcription to determine whether the user's utterance is linguistically complete. If the transcription ends with a grammatically complete sentence, the system treats it as end-of-turn. If the transcription is a sentence fragment, the system waits for more speech.

Semantic detection requires partial ASR transcription. The ASR model runs on the audio stream in real time, producing intermediate transcriptions as the user speaks. When the user pauses, the system checks whether the current transcription is a complete sentence. If yes, it finalizes transcription and triggers the LLM. If no, it keeps listening. This avoids interrupting users who pause mid-sentence because the system knows the sentence is incomplete.

The challenge is that ASR transcription is not always accurate mid-utterance. Streaming ASR models output partial results that change as more audio arrives. The user says "I need to transfer," and the partial transcription might read "I need two trans fur" before correcting to "I need to transfer" once the full phrase is heard. If the system makes an end-of-turn decision based on partial transcription, it risks deciding on incorrect data.

Semantic detection also struggles with incomplete but grammatically valid sentences. A user says "I want to check my balance." That is a complete sentence. The system treats it as end-of-turn, proceeds to generate a response, and the LLM answers the question. But the user pauses after "balance" and continues: "...for my savings account." The system already started responding based on incomplete information. The response addresses checking balance but does not specify which account. The user has to clarify, adding friction.

Semantic end-of-turn detection in 2026 is used in high-stakes applications where interrupting the user is unacceptable. Medical dictation systems, legal transcription systems, and accessibility tools for users with speech impairments deploy semantic detection because false positives — interrupting mid-sentence — are worse than latency. Consumer voice assistants avoid semantic detection because the latency cost of waiting for grammatically complete sentences makes the interaction feel slow.

## Hybrid Approaches: Combining Pause and Semantics

The hybrid approach uses both pause duration and semantic completeness. The system waits for a pause, then checks whether the partial transcription is semantically complete. If yes, it proceeds. If no, it waits longer or until the pause exceeds a maximum threshold. This reduces false positives from mid-sentence pauses while keeping latency low for grammatically complete utterances.

A typical hybrid configuration in 2026 uses a short initial pause threshold — 500 milliseconds. When the user pauses for 500 milliseconds, the system checks the partial transcription. If the transcription is a complete sentence, the system proceeds immediately. If the transcription is incomplete, the system extends the pause threshold to 1,200 milliseconds and keeps listening. If the user resumes speaking within 1,200 milliseconds, the timer resets. If the user stays silent for the full 1,200 milliseconds, the system treats the pause as end-of-turn even if the transcription is incomplete.

This approach reduces latency for clean interactions and avoids interruptions for hesitant users. The median latency is 500 milliseconds for users who speak in complete sentences. The p95 latency is 1,200 milliseconds for users who pause frequently. The tradeoff is complexity. The system must run partial ASR, evaluate semantic completeness, and adjust the pause threshold dynamically. The orchestration logic is more fragile than a simple silence timer.

Semantic completeness evaluation is not deterministic. The model that evaluates whether a sentence is complete is itself a learned model, typically a small language model fine-tuned to classify sentence completeness. It has false positives and false negatives. A false positive treats an incomplete sentence as complete and triggers premature end-of-turn. A false negative treats a complete sentence as incomplete and adds unnecessary latency. The accuracy of the semantic classifier determines the quality of the hybrid approach.

## User-Initiated End-of-Turn Signals

Some voice systems let the user explicitly signal end-of-turn. The user presses a button, taps the screen, or says a specific phrase like "Done." This eliminates the guessing game. The system knows the user is finished because the user said so. User-initiated end-of-turn detection has zero false positives and zero false negatives for timing — the user controls the timing directly.

The downside is cognitive load. The user must remember to signal end-of-turn. In a hands-free scenario — driving, cooking, walking — pressing a button is not an option. In a conversation-like interaction, saying "Done" after every sentence feels unnatural. User-initiated signals work in dictation applications where the user is focused on the system and the interaction is not conversational. They do not work in ambient voice assistants where the user expects natural turn-taking.

Push-to-talk is the most common user-initiated signal. The user holds a button while speaking and releases it when finished. This pattern is standard in walkie-talkies and military communication systems. It works when latency and accuracy are more important than conversational flow. Some enterprise voice applications in 2026 use push-to-talk for field workers, warehouse staff, and pilots. The user knows the system only listens while the button is pressed, which also addresses privacy concerns.

Voice-activated end-of-turn phrases are less common. Saying "Over" or "Done" after every sentence breaks conversational flow. Some accessibility applications use them because users with speech impairments benefit from explicit control. The user's speech may include long pauses that are part of their natural speech pattern, not end-of-turn signals. Explicit phrases let the user control turn-taking without the system misinterpreting pauses.

## Real-World Variability and Personalization

End-of-turn detection is harder in practice than in controlled tests because users vary. Some users speak quickly with minimal pauses. Some users speak slowly with frequent pauses. Some users pause after every few words while formulating complex thoughts. A fixed threshold optimized for the median user fails for users at the distribution tails.

Adaptive end-of-turn detection personalizes the pause threshold based on observed user behavior. The system tracks how long the user typically pauses between words, between phrases, and between sentences. It builds a user-specific model of pause patterns. When the user pauses, the system compares the pause duration to the user's historical distribution. A pause that is unusually long for this user is more likely to be end-of-turn than a pause that matches their typical mid-sentence pause.

Personalization requires data. The system needs to observe the user across multiple interactions to build a reliable model. The first interaction uses a default threshold. After five interactions, the system has enough data to adjust. After twenty interactions, the personalized model is accurate. This works for returning users in applications where the user logs in — smart speakers, virtual assistants, customer support lines with caller ID. It does not work for one-time interactions with anonymous users.

Privacy constraints limit personalization. Storing user-specific pause patterns requires retaining data tied to the user. In privacy-sensitive applications or jurisdictions with strict data retention laws, this is not allowed. The system must use a population-level default threshold and accept higher error rates for users who do not match the median.

## The Consequences of Getting It Wrong

Interrupting the user mid-sentence is the worst failure mode. The user is speaking, the system cuts them off, and the assistant starts responding to incomplete input. The user stops talking, waits for the assistant to finish, then repeats themselves. The interaction takes twice as long. After three interruptions, the user raises their voice, speaks more slowly, and over-enunciates every word. They are adapting to the system's failures. That adaptation is cognitive load. The system is supposed to make the interaction easier. Interruptions make it harder.

Waiting too long creates dead air. The user finishes speaking and expects a response. They wait. One second. Two seconds. Nothing happens. They say "Hello?" to check if the system is still listening. The system was waiting for end-of-turn confirmation, heard "Hello?" as new input, and restarted the transcription. The original question is lost. The user has to start over. Dead air makes the system feel broken.

The interaction rhythm matters. Humans expect conversational turn-taking with minimal gaps. A response that starts 300 milliseconds after the user stops speaking feels natural. A response that starts 1,500 milliseconds later feels like a system failure. The user's perception of latency is not absolute — it is relative to their expectation of conversational timing. End-of-turn detection errors break that timing.

## Production Strategy

End-of-turn detection is a parameter you tune, not a problem you solve once. The right threshold depends on your user population, your interaction type, and your latency budget. Test with real users in real environments. Measure how often the system interrupts users, how often users perceive dead air, and how often users repeat themselves. Optimize for the failure mode you cannot afford.

Start with a pause-based threshold around 700 milliseconds. That is the 2026 industry default for consumer voice assistants. Measure latency and interruption rates. If users complain about interruptions, increase the threshold to 900 or 1,200 milliseconds. If users complain about slowness, decrease the threshold to 500 milliseconds. The optimal value is the one where user complaints are minimized, not the one that minimizes latency on paper.

If your application tolerates the complexity, deploy hybrid detection. Use a 500-millisecond short threshold with semantic completeness checks. This reduces latency for clean interactions while avoiding interruptions for hesitant users. Monitor the semantic classifier's accuracy. If it misclassifies frequently, fall back to pure pause-based detection.

If your users vary widely, consider adaptive detection. Personalize the pause threshold based on each user's observed behavior. This requires storing user-specific data, so confirm that your privacy and compliance policies allow it. If not, use a population-level default and accept higher error rates.

The next subchapter covers diarization, the problem of identifying who spoke when in multi-speaker audio — a necessity for contact centers, meeting transcription, and any scenario where multiple voices need to be distinguished.

