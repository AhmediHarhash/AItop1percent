# 9.12 — Load Testing Voice Infrastructure at Scale

Your voice system works perfectly with 100 concurrent users. Product launches and traffic grows to 5,000 concurrent users. Latency spikes to 3 seconds. Half the sessions fail with timeout errors. The system was never tested beyond 200 concurrent connections. No one knew where it would break. A healthcare voice assistant went through this exact failure in October 2025. The launch succeeded. The infrastructure did not. The team spent 72 hours firefighting performance issues they could have discovered in two days of proper load testing.

The difference between 100 concurrent sessions and 5,000 concurrent sessions is not just scale. It is behavior. Queueing dynamics change. Resource contention emerges. Race conditions that never appeared in small tests manifest constantly at scale. Network bandwidth limits become visible. Database connection pools exhaust. Memory leaks that were invisible suddenly crash servers. Load testing is not about proving the system works. It is about discovering where and how it fails so you can fix those failure modes before users encounter them.

## Simulating Realistic Voice Traffic

Load testing voice systems requires more than opening 10,000 WebSocket connections and sending random data. You must simulate realistic speech patterns, realistic session durations, realistic silence gaps, and realistic user behavior. Unrealistic load tests produce unrealistic results. A test that sends continuous audio at maximum bitrate will find different bottlenecks than real users who pause between sentences.

The speech pattern simulation should match production distribution. Real users do not speak continuously. They speak in bursts of 3 to 8 seconds, pause for 1 to 3 seconds while the system responds, then speak again. The load test must replicate this pattern. Send audio in bursts, wait for responses, send more audio. Measure latency on each turn, not just on sustained streaming. The system that handles continuous streaming may fail on rapid turn-taking.

The session duration distribution should also match production. Some sessions last 20 seconds. Some last 5 minutes. Some last 30 minutes. The load test should create sessions with realistic duration distribution. Short sessions stress connection setup and teardown logic. Long sessions stress memory management and state accumulation. A load test with only 1-minute sessions will not find memory leaks that manifest after 20 minutes.

The silence simulation is critical. Real users pause mid-sentence, have background interruptions, and sometimes stop speaking entirely while thinking. The load test should inject silence gaps of varying lengths. Some gaps are 200 milliseconds between words. Some are 2 seconds between sentences. Some are 10 seconds when the user is distracted. These gaps test your VAD logic, timeout handling, and session cleanup. Without realistic silence, you are testing a system that does not match production.

The user behavior simulation includes errors and retries. Real users interrupt the system mid-response. They repeat commands when recognition fails. They abandon sessions without properly ending them. The load test should replicate these behaviors. Have some simulated users disconnect abruptly. Have some users send overlapping audio while the system is responding. Have some users retry failed requests immediately. These edge cases expose bugs that clean, orderly tests never find.

## Metrics to Capture During Load Tests

The metrics that matter during load testing are different from the metrics that matter in production monitoring. Production monitoring focuses on user-perceived quality. Load testing focuses on identifying bottlenecks and capacity limits. You need to measure every component independently to determine which one fails first as load increases.

Latency must be measured at every stage of the pipeline. Measure WebSocket connection establishment time. Measure time from audio frame receipt to ASR result. Measure time from ASR result to LLM response start. Measure time from LLM response start to TTS completion. Measure time from TTS completion to audio delivery to client. Each stage has its own latency distribution. As load increases, different stages degrade at different rates.

Percentile distributions matter more than averages. At 100 concurrent users, p95 latency might be 250ms. At 1,000 concurrent users, p95 might be 600ms. At 5,000 concurrent users, p95 might be 4 seconds. The average latency might still look acceptable because most requests complete quickly. But the p95 represents the experience of 5 percent of users. At 5,000 concurrent users, that is 250 users experiencing 4-second latency. Those users are the ones who complain, abandon sessions, and write negative reviews.

Error rates segment by error type. Track connection failures separately from timeout errors, ASR failures, LLM failures, and TTS failures. Connection failures indicate network or load balancer issues. Timeout errors indicate processing is too slow. ASR failures indicate the model cannot keep up. LLM failures indicate API rate limits or capacity exhaustion. TTS failures indicate vocoder overload. Each error type points to a different bottleneck.

Resource utilization metrics identify saturation points. Track CPU utilization, memory utilization, GPU utilization, network bandwidth, and disk I/O on every server type. When CPU reaches 90 percent, you are CPU-bound. When memory reaches 85 percent, you are memory-bound. When GPU utilization reaches 95 percent, you are GPU-bound. When network bandwidth reaches 80 percent of link capacity, you are network-bound. The resource that saturates first is the bottleneck.

Queue depths reveal backlog accumulation. Most production systems have queues between components. There is a queue of audio waiting for ASR processing. A queue of ASR results waiting for LLM processing. A queue of LLM results waiting for TTS processing. At low load, these queues are nearly empty. At high load, they grow. When a queue depth exceeds 100 items, the corresponding component cannot keep up with demand. That component is the bottleneck.

## Finding the Breaking Point

The goal of load testing is not to prove the system can handle expected load. The goal is to find the load level where the system breaks and understand the failure mode. You need to know whether the system degrades gracefully or fails catastrophically. You need to know which component fails first. You need to know how to detect impending failure in production.

The load ramp strategy is to increase load gradually and monitor metrics continuously. Start with 100 concurrent users. Hold for 5 minutes. Increase to 200 concurrent users. Hold for 5 minutes. Continue increasing in steps until something breaks. The holding period is essential. Some failure modes take time to manifest. A memory leak may not crash the server immediately. It requires sustained load to accumulate enough leaked memory to cause failure.

The breaking point is identified by either error rate exceeding 5 percent or p95 latency exceeding 3 times baseline. An error rate above 5 percent means the system is actively failing for a meaningful fraction of users. A p95 latency above 3 times baseline means the user experience has degraded unacceptably. Either condition indicates you have reached capacity. The load level at that point is your maximum sustainable load.

The failure mode analysis determines what broke. If CPU saturated, you are compute-bound. If memory saturated, you are memory-bound. If error rates spiked without resource saturation, you likely hit a concurrency limit, database connection pool exhaustion, or API rate limit. If latency spiked without errors, you likely hit queueing delays from a slow component. Each failure mode has a different fix.

The recovery test is to reduce load after hitting the breaking point and verify the system recovers. If you ramp load to 5,000 concurrent users and the system fails, reduce load to 3,000 and check whether latency and error rates return to normal. If they do, the system degrades gracefully. If they do not, the system has entered a failed state and requires restart. Graceful degradation is critical for production reliability. Failed-state systems cannot self-heal and require manual intervention.

## Chaos Engineering for Voice Systems

Chaos engineering is the practice of injecting failures into a system under load to test resilience. For voice systems, this means deliberately breaking components and observing whether the system recovers. The goal is to find hidden dependencies and failure cascades that only appear when multiple things go wrong simultaneously.

The network partition test simulates connectivity loss between components. Disconnect the edge relay from the central processing cluster while both are under load. Measure how long it takes for edge servers to detect the failure, how many user sessions are affected, and whether sessions recover when connectivity is restored. This test exposes assumptions about network reliability that turn into production incidents.

The process kill test simulates unexpected crashes. While the system is under load, kill a random ASR worker, LLM worker, or TTS worker. Measure how long it takes the system to detect the failure, how traffic redistributes to remaining workers, and whether any user sessions fail permanently. Production systems experience unexpected process crashes. The system must handle them without cascading failures.

The slow dependency test simulates degraded performance of external services. Introduce artificial latency in LLM API calls or TTS generation. Measure how this propagates through the system. Does slow LLM inference cause queues to build up? Does slow TTS cause audio delivery delays? How does the system behave when a dependency is slow but not failing entirely? This test reveals timeout configurations that are too aggressive or too lenient.

The resource exhaustion test simulates running out of critical resources. Fill the database connection pool by holding connections open. Fill the GPU memory by running extra inference jobs. Fill the network bandwidth by sending unrelated traffic. Each resource exhaustion scenario stresses different parts of the system. The system must either reject new requests gracefully or queue them with bounded delays. Silent degradation where latency grows unbounded is not acceptable.

The partial failure test simulates scenarios where some components fail while others continue working. Kill all ASR workers but leave LLM and TTS running. Kill all TTS workers but leave ASR and LLM running. These tests reveal whether the system can degrade gracefully by disabling features or whether it fails completely when any component is unavailable. The ideal behavior is that the system continues serving whatever functionality remains available.

## Capacity Planning from Load Test Results

Load test results provide the data needed for capacity planning. You know the maximum load each server type can handle. You know which resource saturates first. You know the latency distribution at different load levels. From this data, you can calculate how many servers you need to handle expected traffic with acceptable latency and reliability.

The baseline calculation is maximum sustained load per server multiplied by number of servers divided by peak traffic. If each ASR server handles 100 concurrent sessions and you expect 3,000 concurrent sessions at peak, you need 30 ASR servers. Add 50 percent headroom for traffic spikes and instance failures. That brings the total to 45 servers. This calculation assumes even traffic distribution, which never happens in practice.

The traffic pattern adjustment accounts for uneven distribution. Voice traffic has daily peaks and valleys. If peak traffic is 3 times average traffic, you need enough capacity for peak, not average. If traffic is geographically distributed and you run edge infrastructure, you need capacity in each region proportional to that region's traffic. A region with 20 percent of traffic needs 20 percent of total capacity, but you cannot run 9 partial servers. Round up to 10 servers per region.

The failure tolerance calculation adds enough capacity that the system survives instance failures without degrading. If you need 45 ASR servers for peak traffic and you want to survive 3 simultaneous failures, you need 48 servers. The 3 extra servers provide buffer. When a server fails, traffic redistributes to the remaining 47 servers. Each server now handles slightly more load, but not enough to degrade performance. This is N plus 3 redundancy.

The cost optimization is to use autoscaling for components with variable load and fixed capacity for components with stable load. ASR and LLM workers scale with user traffic and should autoscale. Session state databases and monitoring infrastructure have relatively stable load and can use fixed capacity. Autoscaling reduces cost during low-traffic periods. Fixed capacity avoids the complexity of scaling stateful systems.

The validation step is to run the load test again after deploying your calculated capacity. If the calculation was correct, the system should handle peak load with latency and error rates within acceptable bounds. If latency is still too high, you underestimated capacity. If resource utilization is below 50 percent at peak, you overestimated. Iterate until capacity matches actual requirements with appropriate headroom.

## Load Testing as Continuous Practice

Load testing is not a one-time event before launch. It is a continuous practice. As you add features, optimize models, and change infrastructure, the performance characteristics change. A code change that improves ASR accuracy by 2 percent may reduce throughput by 15 percent. You will not discover this without load testing after the change.

The regression testing approach is to run a standard load test after every major change. Define a reference load level, such as 1,000 concurrent users, and a set of target metrics, such as p95 latency below 400ms and error rate below 1 percent. Run this test in a staging environment after every significant code or configuration change. If metrics degrade, investigate before deploying to production.

The capacity monitoring approach is to continuously track production capacity utilization. If production traffic is 60 percent of tested capacity, you have healthy headroom. If production traffic is 85 percent of tested capacity, you are approaching limits. If production traffic is 95 percent of tested capacity, you need to add capacity immediately or risk outages during traffic spikes. Set alerts at 70 percent and 85 percent utilization.

The architecture evolution is that load testing often drives architectural changes. You discover that ASR is the bottleneck. You optimize ASR throughput. Now LLM becomes the bottleneck. You optimize LLM inference. Now TTS becomes the bottleneck. This iterative optimization guided by load testing is how production systems evolve from barely functional to highly efficient. Each optimization unlocks the next bottleneck.

The documentation requirement is to record load test results and the capacity calculations derived from them. When traffic grows or infrastructure changes, you need historical context. Knowing that the last load test in November 2025 showed maximum capacity of 4,000 concurrent users with current infrastructure informs decisions about when to add capacity. Without documentation, you lose institutional knowledge and repeat tests unnecessarily.

## Load Testing as Risk Mitigation

Load testing is how you discover failure modes before users do. It is how you validate capacity planning. It is how you measure the impact of code changes on performance. It is how you build confidence that the system will not collapse under unexpected traffic. A launch without load testing is a launch hoping that nothing breaks. A launch with thorough load testing is a launch knowing where the limits are and how to handle them.

The teams that skip load testing face production incidents that could have been avoided. The teams that invest in load testing face production incidents they already know how to handle. The difference is preparation. Load testing does not prevent all incidents. It prevents the incidents caused by unknown capacity limits, undetected bottlenecks, and unvalidated assumptions.

Voice systems at scale require infrastructure that can handle thousands of concurrent sessions with consistent latency and high reliability. The only way to build that infrastructure is to test it at scale, find where it breaks, fix the breaking points, and test again. This iterative process of load testing, optimization, and validation is the foundation of production-grade voice infrastructure.

---

*Next: Chapter 10 — Multi-Provider Orchestration and Fallbacks*
