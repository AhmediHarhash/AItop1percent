# 6.6 — Slot Filling and Entity Extraction in Voice

In September 2025, a car rental company deployed a voice agent to handle reservations. Their text chatbot had achieved 94% slot-filling accuracy. The voice version, using the same underlying model and logic, achieved 71% accuracy in production. The gap cost them 8,200 failed bookings in the first month. The problem was not the model's ability to understand entities. The problem was that people do not speak entities the way they type them.

A user typing a pickup date writes "March 15." A user speaking a pickup date says "um, let me see, the 15th, no actually the 16th works better, yeah the 16th." The system that extracts "March 15" from text sees "let me see the 15th no actually the 16th works better yeah the 16th" and must determine that the final correct value is March 16. This is not a transcription problem. The ASR produces a perfect transcript. This is an entity extraction problem unique to voice, where users self-correct, hedge, and layer information across multiple clauses within a single utterance.

Slot filling accuracy is the percentage of required fields correctly extracted from user speech. In voice systems, this metric separates working agents from broken ones. If you cannot reliably extract dates, times, account numbers, addresses, and names from natural speech, your agent cannot complete tasks. And users will not repeat themselves three times to help you get it right.

## The Correction Pattern and Final Value Extraction

People correct themselves constantly in speech. "I need a table for four, actually make that six." "My account number is 8-3-2, sorry, 8-2-3." "The appointment should be at 2pm, no wait, 3pm." The system must identify the correction pattern and extract the final stated value, not the first value, not both values.

The car rental company's system extracted the first value. When a user said "pickup on the 15th, actually the 16th," the system wrote "March 15" into the slot and considered the field complete. The user assumed the system heard "the 16th." The confirmation said "your pickup is scheduled for March 15th." Half the users caught the error and corrected it again. Half did not notice until they arrived at the wrong time.

The correction pattern has three forms. **Immediate self-correction**: "the 15th, no, the 16th." **Negation correction**: "not the 15th, the 16th." **Replacement correction**: "actually make that the 16th." Your entity extraction must detect all three. The most reliable signal is recency. When the same slot type appears multiple times in a single utterance, the last instance is usually correct. But not always. "I want to book from March 3rd to March 10th" contains two dates. Both are correct. They fill different slots.

The solution is slot-aware correction detection. When you detect a date entity, check if another date entity appears later in the same utterance or the immediate next utterance within 1.5 seconds. If yes, check if the user used a correction signal word: "no," "wait," "actually," "sorry," "I mean." If yes, the second value replaces the first. If no correction signal, treat them as separate values for separate slots. This logic must run in real time, before the system confirms the value back to the user.

A healthcare scheduling system in late 2025 built correction handling into their slot filling pipeline. When a user said "I need an appointment on the 5th, actually the 6th," the system detected both date entities, identified "actually" as a correction signal, discarded the first value, and filled the slot with "the 6th." When a user said "I'm available on the 5th or the 6th," the system detected both date entities, found no correction signal, and asked "would you prefer the 5th or the 6th?" Slot filling accuracy increased from 73% to 91%. Correction-related re-prompts dropped by 84%.

## Partial Slot Updates and Incremental Filling

People do not always provide all required information in one utterance. "I need to book a flight to New York" provides the destination but not the departure city, date, or time. "Next Tuesday" provides the date but not the time. "In the morning" provides a time range but not a specific time. Your slot filling must handle incremental updates, where the user fills slots across multiple conversational turns.

The challenge is state management. When the user says "next Tuesday," the system fills the date slot. When the user then says "in the morning," the system fills the time slot but must retain the previously filled date slot. When the user then says "actually make that Wednesday," the system must update the date slot without clearing the time slot. Slot state persists across turns until the task completes or the user explicitly clears it.

A travel booking voice agent in 2025 lost slot state between turns. User: "I need a flight to Chicago." Agent: "When would you like to travel?" User: "Next Friday." Agent: "Where are you traveling to?" The system asked for destination again because it cleared all slots after each turn. The user hung up. Thirty-two percent of sessions ended in abandonment due to this single bug.

Incremental slot filling requires a persistent slot state object that survives across turns and allows partial updates. When a new entity is detected, update only the relevant slot. When a correction is detected, update only the corrected slot. When the user provides new information, merge it into the existing state. Confirmation should reflect the full current state, not just the most recent update. "Okay, a flight to Chicago on Friday morning" confirms all three filled slots, even though they were provided across three separate utterances.

A banking voice agent built incremental slot filling for wire transfers, which require six pieces of information: recipient name, account number, routing number, amount, purpose, and confirmation. Users provided these across an average of 4.7 conversational turns. The system maintained slot state, confirmed each addition, and only proceeded to execution when all six slots were filled. Task completion rate was 89%, higher than their text form, because users could provide information in whatever order felt natural and correct mistakes without starting over.

## Confidence Thresholding and Confirmation Strategy

Not every extracted entity is reliable. ASR confidence scores, entity recognition confidence, and contextual plausibility all contribute to extraction confidence. When confidence is low, the system must confirm before acting. When confidence is high, the system can proceed without confirmation. The threshold determines the balance between speed and accuracy.

A prescription refill voice agent in early 2026 used a fixed confidence threshold of 0.85 for all slots. Medication names below 0.85 triggered confirmation. Dosages below 0.85 triggered confirmation. Delivery addresses below 0.85 triggered confirmation. This protected against errors but created friction. Users with clear speech and common medication names were asked to confirm information the system heard perfectly. Average call time increased by 40 seconds per refill request.

The better approach is per-slot risk-based thresholds. For high-risk slots where errors cause real harm, use a high threshold and always confirm. For low-risk slots where errors are easily corrected, use a lower threshold and skip confirmation if confidence exceeds it. Medication names and dosages are high-risk. Delivery time preferences are low-risk. The system can confirm "Lisinopril 10mg" every time while proceeding with "deliver in the morning" without confirmation if confidence is above 0.75.

The pharmacy rebuilt their thresholds by slot type. Medication names required 0.92 confidence or confirmation. Dosages required 0.90 or confirmation. Delivery addresses required 0.85 or confirmation. Delivery time preferences required 0.70 or confirmation. Refill quantity required 0.80 or confirmation. Each threshold was tuned based on observed error rates and error consequences. Average call time dropped back to baseline. Error rates for high-risk slots remained at zero. Error rates for low-risk slots increased slightly but never caused patient harm and were always caught at delivery.

Confidence thresholding must account for ASR uncertainty separately from entity extraction uncertainty. If ASR confidence is 0.65, entity extraction confidence is irrelevant — the transcript itself is unreliable. If ASR confidence is 0.98 but the extracted entity is ambiguous in context, entity confidence may still be low. Both signals matter. The final extraction confidence is the minimum of ASR confidence and entity extraction confidence, not the average. A perfectly extracted entity from a poorly transcribed utterance is still unreliable.

## Homophone and Ambiguity Handling

Voice introduces ambiguities that do not exist in text. "Two," "to," and "too" are homophones. "Eight" and "ate" are homophones. "For" and "four" are homophones. When a user says "I need a table for four," the ASR might produce "I need a table for 4" or "I need a table for four." Both are correct transcriptions. The entity extractor must resolve which meaning applies based on context.

A restaurant reservation system in mid-2025 struggled with party size homophones. When users said "a table for two," the ASR sometimes transcribed "a table for 2," sometimes "a table for to," and sometimes "a table for too." The entity extractor tried to parse all three. It correctly extracted "2" from the first. It failed to extract a number from the second and third, triggering a re-prompt: "How many people will be dining?" The user, who had just stated "two," became frustrated. Sixteen percent of calls included at least one homophone-related re-prompt.

The solution is context-aware entity extraction that resolves homophones based on expected slot type. If the current slot expects a number and the transcript contains "for two," "for to," or "for too," resolve all three to the number 2. If the current slot expects a preposition and the transcript contains "for two," preserve "for" and extract "two" as a separate number entity. The ASR transcript is not the final truth. The entity extractor interprets it.

Ambiguity extends beyond homophones. "Next Friday" is unambiguous if today is Monday — it means the upcoming Friday. It is ambiguous if today is Friday — it might mean the upcoming Friday or the Friday after that. "Morning" might mean 6am to 12pm or 8am to 12pm depending on the user's expectation. "The 15th" is unambiguous if the current month is being discussed. It is ambiguous if the user is booking two months out and did not specify which month.

A travel agent voice system handled date ambiguity by defaulting to the nearest future interpretation and confirming it. User: "I need a flight on the 15th." Agent: "Okay, March 15th, is that correct?" If the user meant April 15th, they corrected immediately. If they meant March 15th, they confirmed. This added one confirmation per ambiguous date, but eliminated the need to re-prompt for the full date specification. Ambiguous dates occurred in 22% of booking requests. Explicit confirmation resolved 98% of them on the first try.

## Noisy Environment and Partial Transcription

Users call from cars, streets, airports, and coffee shops. Background noise degrades ASR accuracy. Partial transcription is common. The user says "I need to book a pickup for 8am on March 10th" but the ASR produces "I need to book a pickup for am on March." The time is partially captured. The date is partially captured. Neither is complete enough to fill a slot.

A logistics company in late 2025 deployed a voice agent for package pickups. In quiet environments, slot filling accuracy was 93%. In noisy environments, it dropped to 64%. The system attempted to extract entities from partial transcripts, failed, and re-prompted. Users repeated themselves. The second attempt also failed due to noise. Users escalated to a human. Twenty-eight percent of calls from noisy environments ended in human handoff, compared to 4% from quiet environments.

The fix is noise-aware slot filling that detects partial transcription and immediately asks for repetition of the specific missing information, not the entire utterance. When the system detects a date pattern but the transcribed date is incomplete, it asks "what was the date again?" not "can you repeat that?" The user repeats only the date, not the full sentence. This reduces the chance of the same noise corrupting the same part of the transcript again.

The logistics company added partial transcription detection. If a date entity was detected but missing day, month, or year components, the system asked "which date?" If a time entity was detected but missing hour or am/pm components, the system asked "what time?" If an address entity was detected but missing street number or zip code, the system asked "what's the full address?" Targeted re-prompts succeeded 81% of the time, compared to 52% success rate for generic "can you repeat that?" re-prompts. Human handoff rate in noisy environments dropped to 11%.

## Multi-Value Slots and List Extraction

Some slots accept multiple values. A grocery order might include ten items. A meeting invite might include five attendees. A support ticket might reference three account numbers. The user provides these as a list, often with conjunctions, pauses, and corrections embedded. "I need apples, oranges, and bananas, oh and also grapes." The system must extract all four items and assign them to the correct slot.

A meal delivery voice agent in 2025 allowed users to order multiple dishes in one utterance. User: "I want the chicken teriyaki, the beef bowl, and two orders of spring rolls." The system needed to extract three menu items and apply a quantity to the third. Their initial slot filler extracted "chicken teriyaki" and stopped, treating the slot as filled. The user expected all three items in their order. When the confirmation came back with only one item, they corrected it, then discovered the quantity was wrong, and corrected again. Average order time was 6 minutes. Thirty-four percent of orders were incomplete or wrong at confirmation.

List extraction requires detecting conjunction patterns and splitting on them. "Apples, oranges, and bananas" splits on commas and "and." "Chicken teriyaki and beef bowl" splits on "and." "Spring rolls times two" or "two spring rolls" applies a quantity modifier to one item. The entity extractor must recognize these patterns and populate a list-type slot with multiple values, preserving order and quantity modifiers.

The meal delivery service rebuilt their entity extraction to handle lists. When the user said "I want X, Y, and Z," the system extracted all three as separate items, added them to the order slot as a list, and confirmed all three: "Okay, chicken teriyaki, beef bowl, and two spring rolls." When the user said "and also add grapes," the system appended grapes to the existing list without clearing the previous items. Order accuracy at confirmation increased to 96%. Average order time dropped to 3.5 minutes.

## Evaluation Metrics for Voice Slot Filling

Slot filling accuracy is the primary metric, but it does not tell the full story. A system with 85% accuracy that fails on high-risk slots is worse than a system with 82% accuracy that fails only on low-risk slots. You need per-slot accuracy, per-slot-type accuracy, correction detection rate, and confirmation overhead rate.

**Per-slot accuracy** measures how often each individual slot is filled correctly on the first attempt. If your system has ten slots, measure accuracy for each. You will discover that some slots are harder than others. Dates are easier than addresses. Times are easier than names. Focus improvement effort on the lowest-performing slots.

**Correction detection rate** measures how often the system correctly identifies and applies user corrections. If the user says "the 15th, no the 16th," did the system extract "16th" or "15th"? Track this separately from overall slot accuracy. A system that ignores corrections will have low slot accuracy and a 0% correction detection rate. A system that handles corrections will have higher slot accuracy and a correction detection rate above 80%.

**Confirmation overhead rate** measures how often the system asks for confirmation when it did not need to. If the system confirms a slot and the user says "yes" or "that's right," the confirmation was necessary or neutral. If the user says "I just told you that" or sounds frustrated, the confirmation was overhead. High confirmation overhead increases call time and frustrates users. Track the percentage of confirmations that result in user frustration signals.

A financial services voice agent in early 2026 tracked all three metrics. Overall slot accuracy was 88%. But account number accuracy was 71%, while amount accuracy was 96%. Correction detection rate was 84%. Confirmation overhead rate was 19% — nearly one in five confirmations annoyed the user. They rebuilt account number extraction with better phonetic matching, increased the confidence threshold to reduce false positives, and reduced confirmation overhead to 7%. Overall slot accuracy increased to 92%, and user satisfaction scores increased by 11 points.

The next subchapter covers clarification request appropriateness — when the agent should ask for more information versus inferring from context, and how to measure whether your clarification strategy helps or hinders task completion.
