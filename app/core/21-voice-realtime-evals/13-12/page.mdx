# 13.12 — Training Teams for Voice Incident Response

Recovery is not just code. It is people making fast decisions under pressure when automated systems are not enough. In November 2025, a voice-enabled insurance claims platform experienced a novel failure mode. A new LLM provider deployed a model update that introduced subtle hallucinations in claim amount extraction. Amounts less than $1,000 were parsed correctly. Amounts greater than $10,000 were sometimes inflated by 10x — a $15,000 claim became $150,000 in the system. Automated monitoring did not catch the issue because eval suites tested typical claim amounts, which were almost all under $5,000. The problem surfaced only when a customer called to report that their claim had been approved for $340,000 instead of the $34,000 they requested. The on-call engineer received the alert at 2 a.m. The engineer had never handled a voice-specific incident before. The runbook covered LLM latency, TTS failures, and ASR degradation — but not LLM hallucination in production conversations. The engineer spent 18 minutes reading code and logs, trying to understand the failure. By the time they identified the root cause and disabled the new LLM provider, 230 claims had been processed with incorrect amounts. The financial exposure reached $4.2 million. The incident was not caused by lack of tools or monitoring. It was caused by lack of training. The on-call engineer did not know how to diagnose voice-specific failure modes, did not know who to escalate to, and did not have authority to make the call that mattered: pulling the new provider immediately.

Voice incident response is a specialized skill. The failure modes are different. The user impact is immediate and public. The time to containment is measured in minutes, not hours. Teams that rely on general software incident response training will fail when voice systems break. You need people who understand voice pipelines, who can distinguish ASR failures from LLM hallucinations from TTS rendering bugs, who know which levers to pull to stop the damage, and who have practiced making those decisions under time pressure.

## The Voice Incident Response Team: Who Is On Call

Not everyone is qualified to respond to voice incidents. The on-call rotation for voice systems should include engineers with specific experience: familiarity with ASR, LLM, and TTS provider failure modes, knowledge of circuit breaker and fallback configuration, access to production dashboards and tracing tools, and authority to disable features or switch providers without waiting for approval. Putting a junior engineer on call for a voice platform without this background is setting them up to fail.

The primary responder is typically a senior backend or infrastructure engineer who has worked on the voice platform for at least six months. They know the architecture, they know where logs live, they understand how traffic flows through the system, and they have seen failures before. They are the first person paged when an alert fires. Their job is triage: Is this a known issue with a known fix? Is this a new issue requiring deeper investigation? Is this a critical issue requiring immediate escalation?

The secondary responder is often a specialist in one of the voice pipeline components: an NLP engineer who understands LLM behavior, a speech engineer who understands ASR failure modes, or a systems engineer who understands TTS rendering and audio codecs. They are paged when the primary responder determines that the failure is specific to one component and needs domain expertise. If ASR accuracy drops by 30 percentage points in five minutes, the speech engineer is paged. If LLM responses start violating content policy at 10x normal rate, the NLP engineer is paged.

The escalation point is a technical lead or principal engineer with decision-making authority. They are paged when the incident requires a significant trade-off: disabling a major feature, switching to a more expensive provider, rolling back a recent deployment, or taking the system offline entirely. The escalation point has the authority to make that call without waiting for VP approval. In a voice system where every minute of downtime affects thousands of users, waiting 20 minutes for executive sign-off is unacceptable. The escalation point makes the decision, executes, and informs leadership afterward.

The cross-functional liaison is often a product manager or operations lead who handles non-technical aspects of the incident: communicating with affected users, coordinating with customer support, drafting public status updates, and managing stakeholder expectations. They are paged when user impact is high or when the incident is likely to generate support tickets, press coverage, or regulatory scrutiny. The liaison does not fix the technical problem. They ensure that the people affected by the problem are informed and supported.

On-call schedule design determines coverage depth. For a voice platform serving global users 24/7, you need true 24/7 on-call coverage with engineers in multiple time zones. A rotation with only US-based engineers means incidents at 3 a.m. Pacific are handled by exhausted engineers who were woken up, not by engineers starting their workday. A well-designed rotation has primary and secondary responders in at least two time zones, so someone is always alert and available. Rotation length should be one week, not 24 hours. One-day rotations cause constant context switching. One-week rotations give the on-call engineer time to learn current system behavior and build context.

## Training for Voice-Specific Failure Modes

Voice incidents are not generic software incidents. The failure modes are different. ASR can fail silently — producing transcripts that are syntactically valid but semantically wrong. LLMs can hallucinate in ways that pass basic validation — generating claim amounts, appointment times, or account balances that are plausible but incorrect. TTS can render audio that is technically correct but unintelligible due to pacing, pronunciation, or codec issues. Traditional monitoring catches crashes and timeouts but misses these subtle failures. Engineers need training to recognize voice-specific degradation.

ASR degradation training covers how to detect transcription accuracy drops, how to isolate the root cause — background noise, codec changes, accent shifts, new vocabulary — and how to mitigate. Engineers learn to check ASR confidence scores, compare transcripts to audio recordings, and correlate accuracy drops with user geography or device type. They practice diagnosing an ASR incident from dashboards and logs without access to the ASR provider's internal metrics. They learn when to switch ASR providers, when to tighten confidence thresholds, and when to escalate to human transcription.

LLM hallucination and policy violation training teaches engineers to recognize when LLM output is technically valid but factually wrong. A hallucinated claim amount passes JSON schema validation. A hallucinated appointment time is formatted correctly. The failure is not detectable through syntax checks. Engineers learn to spot hallucination signals: sudden changes in extracted entity distributions, user complaints about incorrect information, mismatches between LLM output and external data sources. They practice disabling a model, rolling back to a previous version, or switching to a more conservative temperature setting to reduce hallucination risk.

TTS quality degradation training focuses on audio rendering issues that do not cause outright failures. The TTS request succeeds, audio is generated, but the result is garbled, choppy, or robotic. Users complain that the system is hard to understand, but logs show no errors. Engineers learn to sample TTS output, compare across providers, check for codec mismatches, and investigate bitrate or sample rate issues. They practice switching TTS providers mid-incident, testing output quality manually, and making the judgment call: is this audio acceptable or must it be fixed immediately?

Latency spike diagnosis training covers how to identify which component in the voice pipeline is causing slowness. Is ASR taking too long? Is the LLM provider under load? Is TTS rendering slow due to cold starts? Is context retrieval hitting database contention? Engineers learn to read distributed traces, correlate latency spikes with provider status pages, and distinguish transient spikes from sustained degradation. They practice activating circuit breakers, switching to faster fallback models, and pre-warming TTS caches to reduce cold start latency.

Session and context failure training teaches engineers how to diagnose and fix issues where conversations lose state. Users report that the system forgot what they said. Engineers learn to check checkpoint storage, verify session ID propagation, inspect context retrieval logs, and identify whether the failure is in serialization, storage, or deserialization. They practice restoring sessions manually from backups, invalidating corrupted sessions, and rolling back code changes that broke session persistence.

## Simulation Exercises: Practicing Recovery Under Pressure

Knowing what to do is not the same as being able to do it under time pressure with users affected and stakeholders watching. Simulation exercises — also called game days or chaos engineering drills — give teams practice responding to realistic incidents in a controlled environment. The goal is not to test the system. It is to test the people.

The basic simulation structure is simple. A facilitator injects a realistic failure into the production system during a scheduled window — typically during low-traffic hours to minimize real user impact. The on-call team is paged. They do not know in advance what the failure is or when it will occur. They must diagnose the failure, decide on mitigation, execute the fix, and confirm recovery. The facilitator observes, times each step, and takes notes on what went well and what went poorly. After the simulation, the team debriefs: What was hard? What took longer than expected? What information was missing? What would we do differently next time?

Realistic failure scenarios are based on past incidents or plausible near-misses. Simulate an LLM provider outage by configuring circuit breakers to trip for that provider. Simulate ASR accuracy degradation by injecting noise into audio before it reaches ASR. Simulate context loss by randomly dropping checkpoint writes. Simulate cascading failure by introducing latency to one component and observing whether retry storms and queue buildup propagate. The scenarios should be hard enough to stress the team but not so catastrophic that recovery is impossible.

Escalation path testing verifies that engineers know when and how to escalate. During a simulation, introduce a failure that requires escalation — perhaps a decision to disable a high-revenue feature or switch to a 3x more expensive provider. Does the on-call engineer recognize the need to escalate? Do they know who to page? Does the escalation point respond quickly? Does the decision get made within the time budget? If escalation fails during a simulation, it will fail during a real incident.

Cross-functional coordination testing ensures that the technical team and the non-technical team work together smoothly. During a simulation, require the on-call engineer to brief the cross-functional liaison within 10 minutes of identifying the issue. The liaison must draft a user-facing status update, coordinate with customer support, and decide whether to proactively reach out to affected users. This tests whether communication flows quickly and whether the liaison understands the technical details well enough to translate them into customer-appropriate language.

Post-incident review practice turns simulations into learning opportunities. After the simulation, the team reviews metrics: time to detection, time to diagnosis, time to mitigation, time to recovery confirmation. They review communication: Did the team use the incident Slack channel effectively? Did updates flow to stakeholders in real time? They review decision quality: Was the chosen mitigation the best option, or were there better alternatives? Was any action taken that made the problem worse? This review process trains teams to learn from incidents, not just survive them.

Rotating failure types prevents over-specialization. If every simulation is an LLM timeout, the team gets very good at handling LLM timeouts and very bad at handling everything else. Rotate through ASR failures, TTS failures, network issues, authentication outages, session corruption, cascading failures, and novel failure modes the system has never seen. This breadth ensures that the team can handle whatever production throws at them.

## Decision Authority: Who Can Trigger Recovery Actions

In a real incident, speed matters more than consensus. The on-call engineer must know what they are allowed to do without asking permission. Decision authority should be clearly defined in runbooks and communicated during onboarding. Ambiguity causes delay. Delay causes user harm.

Tier-one actions are low-risk mitigations that any on-call engineer can execute immediately without approval. Restarting a crashed service. Clearing a stuck queue. Retrying a failed deployment. Switching to a pre-approved fallback provider. Activating a circuit breaker that is already configured. These actions have been tested, documented, and deemed safe. The engineer executes, logs the action, and informs the team afterward. No approval required.

Tier-two actions are medium-risk mitigations that require secondary confirmation. Disabling a feature that affects less than 10% of users. Rolling back a deployment that went live within the past 24 hours. Switching to a fallback provider that is more expensive but still within budget. Increasing infrastructure capacity by 50%. The on-call engineer can propose these actions, but they must get verbal or written confirmation from the secondary responder or escalation point before executing. This two-person rule prevents mistakes when the on-call engineer is tired or uncertain.

Tier-three actions are high-risk mitigations that require escalation-point approval. Disabling a core feature that affects more than 50% of users. Switching to a provider that costs 3x as much as the current provider. Taking the entire voice platform offline. Rolling back a deployment that is more than one week old. These actions have major business impact or technical risk. The escalation point — a principal engineer or technical director — must approve. But the approval must happen within 10 minutes, not 10 hours. The escalation point is on call precisely so these decisions can be made quickly.

Emergency override authority allows the escalation point to bypass normal processes during catastrophic incidents. If the voice platform is serving hallucinated medical advice to 10,000 users per minute, there is no time for consensus-building or committee review. The escalation point has authority to make the call: pull the provider, shut down the feature, take the system offline. They execute first and explain later. This authority comes with accountability — they will need to justify the decision in the post-incident review — but they are empowered to act decisively when user safety is at risk.

Documentation of decision authority lives in the runbook. Every common mitigation action should be labeled: tier one, tier two, tier three. Every on-call engineer should know their limits. Every escalation point should know they are expected to make high-stakes decisions under time pressure. This clarity eliminates the worst-case scenario: an on-call engineer who knows what action would stop the damage but is afraid to take it without permission, while the escalation point is asleep and unreachable.

## Building a Voice Incident Runbook

The runbook is the on-call engineer's survival guide. It documents known failure modes, diagnostic steps, mitigation actions, escalation paths, and decision authority. A good runbook allows an engineer who has never seen a specific failure to diagnose and mitigate it within 10 minutes. A bad runbook is a wall of text that no one reads.

Failure mode catalog lists known issues with symptoms, causes, and fixes. Each entry includes: failure name, user-visible symptoms, technical symptoms visible in logs or metrics, root cause, mitigation steps, expected recovery time, and past incident links. For example: Failure name is ASR confidence collapse. User symptoms are high repeat rate, users saying "you are not understanding me." Technical symptoms are ASR confidence scores below 0.6 for more than 5% of requests, sustained over two minutes. Root cause is usually provider-side model update or regional outage. Mitigation is switch to secondary ASR provider via circuit breaker, monitor confidence scores for recovery. Expected recovery time is 30 seconds. Past incidents are links to post-mortems from March 2025 and July 2025.

Diagnostic decision tree guides the engineer through triage. Start with: Is the issue affecting all users or a subset? If all users, check provider status pages for ASR, LLM, TTS. If subset, segment by geography, device type, user tier. Is the issue latency, accuracy, or availability? If latency, check p95 and p99 by component. If accuracy, check confidence scores and error rates. If availability, check circuit breaker states and health check results. Each branch of the tree leads to a specific diagnostic action or mitigation.

Mitigation playbooks provide step-by-step instructions for common fixes. Each playbook includes: when to use this mitigation, prerequisites, exact commands or dashboard actions, expected outcome, rollback procedure if mitigation fails, and who to notify. For example, the "Switch LLM provider" playbook says: Use when LLM p95 latency exceeds 2,000 milliseconds for more than three minutes. Prerequisites are secondary LLM provider is healthy, circuit breaker for primary provider is configured. Steps are navigate to circuit breaker dashboard, select primary provider circuit breaker, click "Force Open," verify traffic shifts to secondary provider, monitor p95 latency for improvement. Expected outcome is p95 latency drops below 800 milliseconds within 60 seconds. Rollback is click "Force Closed" to restore primary provider. Notify the engineering lead and post in the incident Slack channel.

Escalation contact list includes names, roles, time zones, and contact methods for everyone who might need to be paged during an incident. Primary responder, secondary responder, escalation point, cross-functional liaison, VP of Engineering, legal counsel, PR lead. For each person, list their Slack handle, phone number, and escalation criteria: when should they be paged? The list should be updated monthly to reflect team changes and role transitions.

Incident communication templates provide pre-written status updates for common scenarios. Internal template for engineering team: "Incident detected at timestamp. Affected component. User impact. Current status. ETA to resolution." External template for users: "We are experiencing issues with our voice service. Some conversations may be slower than usual. Our team is actively working on a fix. We will update you within 30 minutes." These templates allow fast communication without spending time crafting messages from scratch during an incident.

## Post-Incident Learning: Improving Recovery Patterns

Every incident is an opportunity to improve recovery. The post-incident review should answer: What happened? Why did it happen? Why did it take as long as it did to detect, diagnose, and mitigate? What can we change so that next time is faster or so that next time never happens?

The timeline reconstruction documents every event with timestamps: when did the failure start, when did the first alert fire, when did the on-call engineer acknowledge, when was the root cause identified, when was mitigation executed, when was recovery confirmed. The timeline reveals delays. If the failure started at 2:00 a.m. and the first alert fired at 2:08 a.m., monitoring has an eight-minute gap. If the alert fired at 2:00 a.m. and the engineer acknowledged at 2:12 a.m., paging is too slow or the engineer was unreachable. If diagnosis took 25 minutes, either the runbook was inadequate or the engineer lacked training.

Root cause analysis goes beyond "the LLM provider had an outage" to ask why the system was vulnerable to that outage. Why did we not detect the issue sooner? Why did circuit breakers not activate automatically? Why did fallback to the secondary provider not happen? Why did users experience impact instead of graceful degradation? The five-whys technique is useful: ask "why" five times to get from the surface symptom to the underlying systemic issue.

Action items are concrete, assigned, and deadlined. Vague action items like "improve monitoring" are useless. Specific action items like "Add alert for LLM p95 latency exceeding 1,500 milliseconds, owned by Engineer A, due March 15" are actionable. Each incident should generate three to seven action items. Fewer suggests the team did not dig deep enough. More suggests the team is trying to fix too much at once. Prioritize the actions that would have prevented the incident or reduced time to recovery.

Runbook updates incorporate lessons from the incident. If the incident exposed a new failure mode that was not in the runbook, add it. If the diagnostic tree did not guide the engineer to the right diagnosis, update it. If a mitigation playbook was missing a critical step, add the step. The runbook should evolve with every incident, becoming more comprehensive and more accurate over time.

Training updates address knowledge gaps revealed by the incident. If the on-call engineer did not know how to interpret ASR confidence scores, schedule training on ASR metrics. If the escalation point was unsure whether disabling the feature was within their authority, clarify decision authority guidelines and communicate them to the team. If the cross-functional liaison did not know how to draft a user-facing status update, provide templates and examples.

## Building Institutional Knowledge

Voice incident response expertise should not live in one person's head. It should be captured, documented, and shared so that the team's capability grows over time and survives turnover.

Incident retrospective library archives every post-incident review in a searchable repository. Engineers preparing for on-call can read past retrospectives to learn what has gone wrong before, how it was fixed, and what was learned. New hires can study the library to understand the system's failure modes. The library should be tagged by failure type, affected component, and severity so engineers can find relevant incidents quickly.

Shadowing programs pair new on-call engineers with experienced engineers for one or two rotations. The new engineer observes how the experienced engineer handles incidents, asks questions, and learns the informal knowledge that runbooks cannot capture: which dashboard to check first, which metrics are reliable and which are noisy, who to call when the runbook does not have the answer. Shadowing accelerates learning faster than documentation alone.

Voice incident workshops run quarterly, bringing the entire engineering team together to review major incidents from the past three months, discuss emerging failure modes, and practice new mitigation techniques. The workshop is not about blame. It is about collective learning. Engineers who were not on call during a major incident still benefit from understanding what happened and how it was resolved.

Runbook maintenance sprints happen every six months. The team reviews the entire runbook, tests every mitigation playbook, verifies contact lists are current, and removes outdated information. Runbooks decay over time as systems evolve. Regular maintenance keeps the runbook accurate and useful.

---

Training teams for voice incident response transforms on-call from a reactive burden into a proactive capability. Engineers who know how to diagnose voice-specific failures, who have practiced recovery under pressure, who have authority to act decisively, and who learn from every incident are the difference between a platform that survives failures and a platform that collapses under them. Recovery is not just code. It is people making fast, informed decisions when it matters most.

Next: **Chapter 14 — Session State and Memory Integrity**, where we explore how voice systems manage conversational continuity across turns, failures, and reconnections.

