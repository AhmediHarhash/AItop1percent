# 13.6 — Partial Response Salvage: Using What You Have

Half an answer is often better than no answer. This principle separates voice systems that maintain conversational flow from those that collapse under failure. When your language model times out after generating two sentences of a four-sentence response, you have a choice: discard everything and apologize, or speak what you have and recover in the next turn. The instinct is to discard. Partial responses feel incomplete, unprofessional, like admitting failure. But users do not experience partial responses the same way engineers do. A user who hears "Your prescription will be ready tomorrow at your usual pharmacy location" has received useful information, even if the system intended to also say "and we'll send you a text reminder an hour before closing." The partial response answered the core question. The missing detail can come in the next turn, or not at all.

The challenge is knowing when partial is good enough and when it is actively harmful. Salvaging a partial response that cuts off mid-sentence creates confusion. Salvaging a partial response that answers the question but omits context creates a functional turn. The decision must happen in milliseconds, before you commit to speaking. You cannot play two seconds of audio, realize it is incomplete, and then stop. The user has already heard those two seconds. You are committed. Partial response salvage requires detecting truncation, evaluating whether the partial content is usable, and making a binary decision: speak it or discard it.

## Detecting Truncation Versus Natural Completion

The first problem is distinguishing between a response that is complete but shorter than expected, and a response that was cut off mid-generation. Your language model was prompted to provide appointment details. It returned one sentence: "Your appointment is confirmed." Is this a complete response from a model that chose brevity, or a truncated response that was supposed to include date, time, and location? You cannot rely on response length alone. Some valid responses are short. You need structural signals.

The most reliable signal is sentence boundary detection. A response that ends mid-sentence is truncated. A response that ends on a complete sentence might be complete or might be missing subsequent sentences. You parse the generated text for sentence-ending punctuation and grammatical closure. If the response ends with a period, question mark, or exclamation point and the final sentence has a subject and predicate, it is structurally complete at the sentence level. If it ends mid-clause or with a dangling conjunction, it is truncated.

The second signal is semantic completeness relative to the user query. If the user asked "When is my appointment?" and the response is "Your appointment is on," you have semantic incompleteness. The core question was not answered. If the response is "Your appointment is on Thursday at 2pm," you have semantic completeness for the primary question, even if additional context was intended. You evaluate semantic completeness by checking whether the response contains the information type the user requested: a date if they asked when, a location if they asked where, a yes-or-no if they asked a binary question.

The third signal is model metadata. Many language model APIs return stop reason metadata indicating why generation ended. Stop reasons include natural completion, length limit reached, or timeout. If the stop reason is natural completion, the response is as complete as the model intended. If the stop reason is length limit or timeout, the response is truncated. This metadata is definitive when available, but not all APIs expose it, and not all truncation is caused by length limits. Network failures and streaming interruptions can also cause truncation without clear metadata.

You combine these three signals into a truncation confidence score. A response that ends mid-sentence with a timeout stop reason scores 100% truncated. A response that ends on a complete sentence with natural completion metadata scores 0% truncated. A response that ends on a complete sentence but does not answer the user's question scores 60% truncated — structurally complete but functionally incomplete. You set a salvage threshold based on conversation stakes. In a low-stakes conversational AI, you salvage anything above 30% structurally complete. In a high-stakes healthcare system, you salvage only responses above 80% semantically complete.

## When Partial Responses Are Usable

Not all partial responses are salvageable. The decision to use a partial response depends on three factors: whether it answers the user's core question, whether it introduces new ambiguity, and whether the conversation can continue naturally afterward. A partial response that answers the question is usable. A partial response that contradicts itself or leaves the user in an ambiguous state is not.

The clearest usable case is when the partial response contains the critical information and the missing portion is supplementary. The user asked "What's the balance on my account?" The model generated "Your current balance is 2,847 dollars. You have one pending transaction for—" and then timed out. The partial response is usable. The user's question is answered. The missing information about pending transactions is valuable but not necessary for the user to proceed. You speak the partial response, and the conversation continues. If the user wants to know about pending transactions, they will ask in the next turn.

The second usable case is when the partial response completes the primary speech act but omits follow-up offers or transitions. The model generated "I've scheduled your appointment for Tuesday at 10am. Would you—" and timed out. The appointment is scheduled. The confirmation was delivered. The missing portion was probably "Would you like a reminder?" or "Would you like directions?" These are nice-to-haves, not requirements. You speak the partial response and move to the next turn. The user may interpret the abrupt ending as the end of the system's turn and respond naturally.

The third usable case is when the partial response sets up a clarification loop. The model generated "I found three locations near you. The closest is—" and timed out. The partial response is incomplete, but it is not misleading. You speak it, and then immediately follow with a recovery phrase: "I found three locations near you. Let me get those details for you." You have acknowledged the partial information and signaled that more is coming. The user knows the system is working on it. This is usable because it maintains conversational flow even though the response was truncated.

## Unusable Partial Responses That Must Be Discarded

Partial responses become harmful when they mislead, contradict, or create ambiguity that the user cannot resolve. The most dangerous partial response is one that introduces a fact and then negates it. The model generated "Your appointment is confirmed for Thursday at— actually, I see there was a scheduling conflict—" and timed out. The user now believes there is a conflict but does not know whether the appointment is confirmed or not. Speaking this partial response creates confusion worse than silence. You must discard it and either retry for a complete response or use a generic fallback like "Let me check on that appointment."

The second harmful case is partial responses that end on incomplete instructions. The model generated "To reset your password, go to the settings menu and tap—" and timed out. The user now knows they need to go to settings, but they do not know what to do next. They will attempt to follow the partial instruction and fail, creating frustration. Partial instructions are worse than no instructions. You discard this response and either retry or escalate to a human who can provide complete instructions.

The third harmful case is partial responses that contradict the conversation history. The model generated "I don't see any upcoming appointments. Wait, I see—" and timed out. The user was told they have no appointments, and then the statement was reversed. The partial response has poisoned the user's understanding of their state. Even if you retry and get a complete response, the user has lost trust in the system's accuracy. You must discard the partial response and provide a clean response that does not reference the contradiction.

The fourth harmful case is partial responses in high-stakes domains where incompleteness creates liability. In a medical advice system, a partial response like "Based on your symptoms, you should take—" is not salvageable. The user might act on incomplete medical guidance. In a financial trading system, a partial response like "I've placed your order for—" is not salvageable. The user needs to know exactly what was ordered. In these domains, you discard all partial responses and retry until you have complete information or escalate to a human.

## Salvage Patterns: Speak What You Have, Then Recover

When you decide to salvage a partial response, the execution pattern determines whether the user experiences smooth recovery or awkward patchwork. The simplest salvage pattern is direct delivery: you speak the partial response exactly as generated, with no acknowledgment of incompleteness. This works when the partial response is semantically complete and the user is unlikely to notice that more was intended. "Your balance is 2,847 dollars" followed by silence is interpretable as a complete turn. The user gets their answer and moves on.

The second pattern is additive recovery: you speak the partial response, detect that it was incomplete, and then immediately speak an addendum in the same turn. The model generated "Your appointment is on Thursday at" and timed out. You detect truncation. You speak "Your appointment is on Thursday at" and then append "two pm" from a retry or from structured data if available. The user hears "Your appointment is on Thursday at two pm" as a single utterance with a slight pause in the middle. The pause is noticeable but not disruptive. This pattern works only if you can retrieve the missing information quickly enough to append it before the user starts speaking.

The third pattern is bridged recovery: you speak the partial response, acknowledge incompleteness implicitly with a transition phrase, and continue in the next turn. The model generated "I found three locations near you. The closest is" and timed out. You speak that partial response, immediately append "one moment," and then retry for the complete location list. The user hears "I found three locations near you. The closest is one moment" which is awkward grammatically but functionally signals that the system is working on providing the information. You then deliver the full list. This pattern sacrifices elegance for reliability.

The fourth pattern is reset recovery: you detect that the partial response is unusable, discard it, and start over with a fresh generation. You do not speak the partial response at all. You use a transition phrase like "Let me check that for you" and then deliver a complete response. The user never hears the partial response. This is the safest pattern but adds latency because you are effectively retrying from scratch.

## Context for the Next Turn After Partial Delivery

When you salvage a partial response, your dialogue state must reflect what the user actually heard, not what the model intended to generate. This is the same context preservation challenge from TTS failure recovery, but with added complexity: the user heard something, just not everything. Your dialogue state must account for the partial information delivered.

If you spoke a partial response that confirmed an appointment but did not provide a confirmation number, your dialogue state should indicate that the appointment is confirmed but the confirmation number was not delivered. If the user asks "What's my confirmation number?" in the next turn, you recognize that this is a follow-up to the partial response and provide the missing information. If you failed to track that the confirmation number was missing, you might misinterpret the user's request as asking about a different appointment.

The most effective approach is maintaining a partial delivery log that records what information was successfully communicated versus what was intended. When partial response salvage occurs, you log the delivered content and the missing content. The dialogue manager consults this log when interpreting the next user turn. If the user asks about information that was marked as missing, you recognize it as a recovery completion request rather than a new query. If the user moves to a different topic, you clear the log and proceed normally.

The complexity increases when the partial response introduced ambiguity. The model said "I found three locations" but never listed them. The user now expects a list. If they ask a follow-up question like "Which is closest?", your dialogue manager must recognize that the user is asking about the unlisted locations, not about some other set of locations. Your context must preserve the unfulfilled promise. One implementation that handled this well maintained a "pending clarifications" queue: whenever a partial response introduced information that was not fully delivered, the clarification was queued. The next turn either fulfilled the clarification or the user's new query superseded it.

## Metrics for Partial Response Salvage Decisions

You need metrics that tell you whether your salvage decisions are helping or hurting user experience. The first metric is salvage rate: the percentage of truncated responses that you chose to deliver as partial versus discard. A salvage rate of 90% suggests you are over-salvaging and likely delivering confusing partial responses. A salvage rate of 10% suggests you are over-discarding and adding unnecessary latency by retrying responses that would have been usable.

The second metric is user recovery rate after partial delivery. When you salvage a partial response, you track whether the user's next turn is a continuation of the conversation or a confusion signal. Confusion signals include "What?" "I didn't understand that," "Can you repeat that?" or silence followed by timeout. If 40% of partial response deliveries result in user confusion, your salvage criteria are too lenient. If 5% result in confusion, your salvage decisions are well-tuned.

The third metric is question answer completeness: whether the partial response delivered the information the user requested. You evaluate this by checking if the user's follow-up turn asks for the same information again. If the user asked "When is my appointment?" and you delivered a partial response, and then the user asks "What time is it?", the partial response did not answer the question completely. If the user moves to a new topic, the partial response was sufficient.

The fourth metric is time-to-recovery: how long it takes from detecting truncation to delivering either the salvaged partial response or a complete retry response. This captures whether your salvage logic is fast enough to feel seamless or whether the decision-making process introduces noticeable delay. The target is under 800 milliseconds from truncation detection to audio playback. Beyond that, the user perceives a system pause.

A financial services voice system that implemented partial response salvage in 2026 tracked all four metrics and tuned salvage thresholds weekly based on user recovery rate. They started with a 50% salvage rate and 30% user confusion rate. After six weeks of threshold tuning based on semantic completeness scoring, they reached a 35% salvage rate with 8% user confusion rate. They were salvaging fewer responses, but the responses they salvaged were higher quality and users adapted to them naturally. The system also reduced average conversation length by 0.7 turns because users were getting sufficient information in partial responses and did not need to ask follow-up questions for missing details.

Partial response salvage is not about perfectionism. It is about recognizing that users are tolerant of incomplete information as long as it answers their core question and the conversation continues. The teams that salvage effectively are the teams that measure user perception rather than system perfection.

---

When partial salvage is not enough and multiple systems are failing simultaneously, you need a defined hierarchy for how your voice system degrades. Not all failures are equal, and not all degradation strategies preserve the same user experience.

