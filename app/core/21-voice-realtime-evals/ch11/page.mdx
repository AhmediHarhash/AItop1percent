# Chapter 11 — Production Monitoring for Voice Systems

Voice systems fail in ways text systems do not. A latency spike that would go unnoticed in chat destroys a phone conversation. Quality degradation that might be forgiven in a written response becomes unbearable when heard out loud. ASR failures are silent in the logs but catastrophic to the user experience — the system thinks it heard one thing, the user said another, and the conversation derails with no visible error.

The monitoring stack for voice AI must operate at conversation speed. You need real-time alerts for tail latency, automated quality checks on every transcript, drift detection that catches ASR degradation before it ruins a thousand calls. You need to know when TTS quality drops, when conversation success rates decline, when cost-per-minute starts climbing. And you need runbooks that let engineers diagnose and fix problems while users are still on calls — because by the time you review yesterday's logs, the damage is done.

This chapter builds the observability layer that keeps voice systems reliable in production. It covers what to instrument, how to set thresholds, and how to respond when the alerts fire.

---

- 11.1 — The Voice Observability Stack: What to Instrument
- 11.2 — Latency Monitoring: P95, P99, and Tail Latency Alerts
- 11.3 — ASR Quality Monitoring in Production
- 11.4 — TTS Quality Degradation Detection
- 11.5 — Conversation Success Rate Dashboards
- 11.6 — Real-Time vs Batch Quality Evaluation
- 11.7 — Alerting Thresholds for Voice-Specific Metrics
- 11.8 — Call Sampling Strategies for Human Review
- 11.9 — Transcript Analysis for Drift Detection
- 11.10 — Cost Monitoring: Per-Minute and Per-Conversation
- 11.11 — Incident Detection Playbooks for Voice Systems
- 11.12 — Building Voice-Specific On-Call Runbooks

---

*When a voice system degrades, the user experiences it immediately. The question is whether you see it before they hang up.*
