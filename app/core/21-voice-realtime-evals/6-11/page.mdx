# 6.11 — Multi-Turn Evaluation Datasets and Protocols

Most teams measure conversational quality is unnatural. Evaluating conversations requires evaluating conversations — full multi-turn exchanges where context builds, intent shifts, and errors cascade or recover. Single-turn evaluation tells you whether the system can answer one question correctly. Multi-turn evaluation tells you whether it can hold a conversation.

The difference matters. A system that scores 92% accuracy on single-turn queries can fail catastrophically in multi-turn scenarios when it loses context after turn three, misinterprets a follow-up question, or gives contradictory answers across turns. A banking voice assistant that correctly answers "what is my balance?" in isolation but cannot handle "what is my balance... and what were my last three transactions... and can you dispute the second one?" is not production-ready. You discover this through multi-turn evaluation datasets and protocols designed to test conversational coherence, not just individual response quality.

## Why Single-Turn Evaluation Fails for Conversations

Single-turn evaluation measures whether the system produces the correct response to an isolated input. You feed the system a query. It produces an output. You score the output. This works for search engines, classification tasks, and question-answering systems. It does not work for voice AI.

Voice conversations are stateful. Each turn depends on previous turns. The system must remember what the user said three turns ago. It must track whether the user is asking a new question or following up on a previous one. It must maintain consistency — if it said the user's balance was 4,200 dollars in turn two, it cannot say the balance is 3,800 dollars in turn five without explaining the discrepancy.

Single-turn evaluation misses context failures. A user says "I want to book a flight to Boston." The system asks "what date?" The user says "next Tuesday." Evaluated in isolation, "next Tuesday" has no correct response. Evaluated in context, the correct response is "booking a flight to Boston on Tuesday, March 18th — is that correct?" A single-turn eval scores the system based only on turn three. A multi-turn eval scores whether the system correctly integrated information across all three turns.

Single-turn evaluation misses error propagation. The system misrecognizes "Houston" as "Austin" in turn one. The user does not notice immediately. In turn three, the user says "actually, make it a direct flight." The system confirms "direct flight to Austin." Now the user realizes the error. In turn four, the user says "no, I said Houston, not Austin." The system must recognize that the error originated in turn one, not turn four, and correct the entire conversation thread. Single-turn evaluation never tests this recovery path.

Single-turn evaluation misses conversation efficiency. A task that should take three turns but takes nine because the system keeps asking for clarification scores fine on per-turn accuracy but fails on user experience. Multi-turn evaluation measures whether the conversation reaches the goal in a reasonable number of turns, whether turns feel productive, and whether the conversation has natural flow or feels like a series of disconnected exchanges.

Teams that rely only on single-turn evaluation ship systems that pass all tests and fail in production. Users do not have single-turn conversations. Your evaluation dataset should reflect that.

## What a Multi-Turn Evaluation Dataset Contains

A multi-turn evaluation dataset is a collection of full conversations, each annotated with expected behavior at multiple checkpoints. It is not a list of isolated queries. It is a library of realistic conversation scenarios that test how the system handles context, ambiguity, follow-ups, corrections, and goal progression across multiple exchanges.

Each conversation in the dataset includes several components. The **conversation transcript** is the sequence of user turns and system turns from start to finish. User turns include the literal spoken input or ASR transcription. System turns include the full response the system should produce. The transcript captures the conversational flow, including pauses, acknowledgments, confirmations, and error recovery.

The **ground truth annotations** specify what the system should do at key points in the conversation. After turn two, the system should have extracted the user's destination and travel date. After turn four, the system should have confirmed the flight details and asked for payment. After turn six, the system should have completed the booking and provided a confirmation code. Ground truth is not just the final output — it is the intermediate state the system should reach at each step.

The **conversation metadata** includes task type, user intent category, conversation complexity level, expected turn count, success criteria, and any special conditions. A simple information lookup conversation is tagged as low complexity, 2-4 expected turns, success defined as providing the correct answer. A multi-step troubleshooting conversation is tagged as high complexity, 8-15 expected turns, success defined as resolving the issue or escalating appropriately.

The **variation scenarios** test robustness. For each core conversation, the dataset includes variations: the user provides information in a different order, the user interrupts mid-turn to correct themselves, the user gives incomplete information and the system must ask follow-ups, the user expresses frustration and the system must de-escalate. These variations test whether the system handles realistic conversational dynamics or only works for perfectly scripted exchanges.

The **error injection points** specify where you deliberately introduce failures to test recovery. At turn three, simulate an ASR misrecognition. At turn five, simulate a backend timeout. At turn seven, simulate the user providing information that contradicts what they said in turn two. Error injection turns test whether the system recovers gracefully or compounds failures.

A production-grade multi-turn evaluation dataset contains 200-500 unique conversation scenarios, each with 2-5 variations, totaling 500-2,000 full conversations. This is enough to cover the major task types, common error patterns, and edge cases without becoming unmanageable. Smaller datasets miss rare but important scenarios. Larger datasets become expensive to annotate and maintain.

## Building Multi-Turn Scenarios from Real Conversations

The best multi-turn evaluation data comes from real production conversations. Real users follow unexpected paths. They phrase requests in ways you did not anticipate. They make assumptions the system must infer. They recover from errors in creative ways. Real conversations contain all the complexity your system will face. Synthetic conversations, no matter how carefully designed, miss edge cases.

Start by sampling production conversation logs. If your system handles 50,000 conversations per week, sample 1,000 randomly. Prioritize diversity: include short conversations and long conversations, successful conversations and failed conversations, common tasks and rare tasks. Include conversations from different times of day, different user demographics if available, and different entry points into the system.

Filter for quality. Some conversations are too noisy to use as evaluation data — the user was in a car with road noise, the connection dropped mid-conversation, the user abandoned the session without clear reason. Some conversations are too simple — single-turn exchanges that do not test multi-turn dynamics. Keep conversations that have 3-15 turns, clear user intent, and enough structure to define expected behavior.

Anonymize the data. Strip personally identifiable information — names, account numbers, addresses, phone numbers. Replace them with placeholders or synthetic equivalents. A conversation about checking an account balance can be anonymized by replacing the real account number with a fake one while preserving the conversation structure. Anonymization is not just for privacy — it makes the dataset reusable across test environments without leaking production data.

Annotate expected behavior at each turn. For each system turn, specify what the correct response should have been. If the production system made a mistake, annotate what it should have said. If the production system gave an acceptable but suboptimal response, annotate both the actual response and the ideal response. This creates a reference standard you can eval against.

Annotate conversation-level success criteria. Did the user accomplish their goal? Did the conversation end naturally or did the user hang up in frustration? How many turns did it take relative to how many it should have taken? Was the system's final answer correct? These session-level annotations let you evaluate not just individual turns but overall conversation quality.

Create variations by editing real conversations. Take a conversation where the user said "I want to book a flight to Boston on March 18th" and create a variation where the user says "I want to book a flight" and then provides the destination and date in separate turns. Take a conversation where the system correctly understood the user and create a variation where the system misrecognizes a key word and must recover. Real conversations provide the structure. Variations test whether the system handles alternate paths.

Update the dataset quarterly with new production samples. User behavior changes. New task types emerge. Patterns that were rare six months ago become common. A static evaluation dataset becomes less representative over time. Continuously sampling from production keeps the dataset aligned with real usage.

## Annotation Protocols for Multi-Turn Quality

Annotating multi-turn conversations requires more judgment than annotating single-turn data. A single-turn annotation asks "is this response correct?" A multi-turn annotation asks "is this response correct in context, does it advance the conversation toward the goal, does it maintain consistency with previous turns, and does it set up the next turn appropriately?" This complexity demands clear protocols.

Define turn-level quality dimensions. Each turn is scored on several axes: **correctness** — does the response contain accurate information? **contextual relevance** — does the response address what the user just said? **consistency** — does the response align with information provided in earlier turns? **progression** — does the response move the conversation toward task completion? **tone appropriateness** — is the response formal, friendly, empathetic, or neutral as required by the conversation context?

Use a rubric, not free-form judgment. A rubric defines what scores mean. A 5-point correctness scale might define 5 as "completely accurate, fully answers the user's question," 4 as "accurate with minor omissions," 3 as "partially accurate, missing key information," 2 as "mostly inaccurate," 1 as "completely wrong or nonsensical." The rubric removes ambiguity and makes annotations consistent across annotators.

Annotate session-level outcomes separately from turn-level quality. A conversation can have perfect turn-level scores but fail at the session level if it took too many turns or ended without completing the task. Session-level annotations include: **task success** — did the user accomplish their goal? **efficiency** — turn count relative to baseline. **user satisfaction proxy** — did the user express frustration, did they thank the system, did they hang up abruptly? These session-level scores capture what turn-level scores miss.

Train annotators on edge cases. Multi-turn annotation is harder than single-turn annotation because context matters. Two annotators might score the same turn differently based on how they interpreted the user's intent from earlier turns. Resolve these disagreements by establishing canonical interpretations for ambiguous conversations. If a user says "I want to change my flight," does that mean reschedule or cancel? Define it in your annotation guidelines so all annotators score consistently.

Measure inter-rater reliability. Have two annotators independently score the same 50-100 conversations. Calculate agreement rate or Cohen's kappa. Agreement above 80% on turn-level scores and above 85% on session-level scores indicates the protocol is clear enough. Agreement below 70% indicates ambiguity in the rubric or insufficient annotator training. Iterate the protocol until reliability is high.

Use a mix of expert and non-expert annotators depending on the task. For domain-specific conversations — medical triage, legal information, technical support — expert annotators who understand the subject matter produce better annotations. For general conversations — customer service, appointment booking, FAQ — non-expert annotators are sufficient and cheaper. Match annotator expertise to conversation complexity.

Annotate not just correctness but conversation quality patterns. Tag conversations that exhibit common problems: the system lost context after turn three, the system gave contradictory information, the system failed to acknowledge an error, the system interrupted at an inappropriate moment. These tags let you filter the dataset by failure mode and test targeted improvements.

## Dataset Size and Coverage Requirements

How many multi-turn conversations do you need in your evaluation dataset? Enough to cover the task distribution your system handles in production, plus enough variations to test robustness, plus enough error scenarios to validate recovery behavior. For most voice AI systems, that is 500-2,000 conversations.

The minimum viable dataset for early-stage systems is 100-200 conversations covering the top 80% of task types. If your system handles ten task types and three of them account for 80% of production volume, build 30-40 conversations per major task type and 5-10 conversations per minor task type. This gives you enough coverage to catch catastrophic failures without over-investing in annotation.

Production-scale datasets for mature systems contain 1,000-2,000 conversations. This is enough to detect 2-3 percentage point differences in task success rate with statistical confidence. It covers long-tail scenarios that occur once per thousand conversations but still matter. It includes enough error injection tests to validate all major recovery paths.

Dataset coverage is more important than dataset size. A dataset with 2,000 conversations but all of them are simple two-turn information lookups is less valuable than a dataset with 500 conversations spanning a dozen task types and complexity levels. Coverage dimensions include:

**Task type distribution** — your dataset should match production task distribution. If 40% of production conversations are account balance checks, 40% of your dataset should be account balance checks. If 5% are fraud disputes, 5% of your dataset should be fraud disputes. Mismatched distribution produces misleading eval scores.

**Conversation complexity distribution** — some conversations are simple, some are complex. Simple conversations have clear user intent, no ambiguity, no follow-up questions, 2-4 turns. Complex conversations have ambiguous intent, multiple sub-goals, clarifications, corrections, 8-15 turns. Your dataset needs both. If production has 60% simple conversations and 40% complex, your dataset should match.

**Error scenario coverage** — your dataset must include conversations where things go wrong. ASR misrecognitions, intent classification failures, backend timeouts, user corrections, contradictory information, abandoned turns. If your dataset only includes happy-path conversations, you will not detect failure modes until production.

**User variation coverage** — different users phrase requests differently. Some users speak in full sentences. Some speak in fragments. Some provide all information upfront. Some provide information only when asked. Your dataset should include all common phrasing patterns, not just the one your team uses when creating test cases.

**Temporal coverage** — if your system handles time-sensitive tasks like appointment booking or delivery tracking, your dataset must include conversations with relative time references: "tomorrow," "next week," "in three days," "the 15th." These require the system to resolve relative dates correctly.

You measure coverage by comparing dataset task distribution to production task distribution. If production shows 30% of conversations involve task type A and your dataset shows 10%, you are under-covered. If production shows 2% of conversations involve error type B and your dataset shows 15%, you are over-covered. Iterate the dataset toward production distribution.

## Protocol for Running Multi-Turn Evaluations

Running a multi-turn evaluation is more involved than running a single-turn eval. You cannot just feed inputs into the system and score outputs. You must simulate the full conversation flow, maintain context across turns, and evaluate both intermediate state and final outcomes.

Set up a conversation simulation harness. The harness replays a conversation from your dataset turn by turn. It feeds user turn one to the system, captures the system's response, feeds user turn two along with accumulated context, captures the next response, and continues until the conversation completes. The harness tracks conversation state — what information has been provided, what questions have been asked, what confirmations have been given.

Compare system responses to ground truth at each turn. For each turn, the harness checks: does the system response match the expected response exactly, semantically, or not at all? Exact match is rare in natural language. Semantic match means the system said the same thing in different words. No match means the system gave a wrong or irrelevant response. Use a combination of string similarity for structured outputs and LLM-based semantic comparison for natural language responses.

Track context preservation across turns. After each turn, check whether the system retained necessary information from previous turns. If the user provided their account number in turn two, the system should still have access to it in turn five. If the system asked for confirmation in turn three and the user confirmed in turn four, the system should not ask again in turn six. Context loss is a common failure mode. Your eval protocol must detect it.

Evaluate session-level outcomes at conversation end. Did the conversation reach the expected goal? Did it complete in the expected number of turns? Did the system provide all necessary information? Did it make any errors that were not corrected? Session-level evaluation supplements turn-level evaluation. A conversation can have mostly correct turns but still fail if it never reached the goal.

Run the evaluation on a regular cadence. Multi-turn evaluation is more expensive than single-turn evaluation — each conversation requires simulating a full dialog, not just scoring a single response. Most teams run multi-turn eval weekly or before each release. Run it more frequently if your system is changing rapidly. Run it less frequently if your system is stable and you are monitoring production metrics continuously.

Automate as much as possible, but include human review for a sample. Automated comparison catches obvious errors. Human review catches subtle errors — the system gave a technically correct but conversationally awkward response, the system interrupted at a bad moment, the system lost the thread of the conversation. Review 5-10% of evaluated conversations manually to catch what automation misses.

## Balancing Dataset Maintenance Cost and Coverage

Multi-turn datasets are expensive to create and maintain. Each conversation requires 5-20 minutes to annotate depending on length and complexity. A 1,000-conversation dataset costs 80-300 annotation hours. Maintenance adds 10-20% per quarter as you update conversations to reflect system changes and add new scenarios.

Prioritize quality over quantity. A well-annotated dataset of 500 conversations is more valuable than a poorly annotated dataset of 2,000 conversations. Invest in clear annotation protocols, annotator training, and inter-rater reliability testing before scaling up dataset size.

Reuse and extend existing conversations rather than creating new ones from scratch. If you have a conversation about booking a flight, create variations by changing the destination, the date, the number of passengers, or the user's payment method. Variations take 2-5 minutes to create versus 15 minutes for a new conversation from scratch.

Automate ground truth generation for simple tasks. If your system handles structured tasks with deterministic outputs — looking up account balances, checking order status — you can generate ground truth programmatically. Feed the system a query, call the backend directly to get the correct answer, annotate the conversation with that answer. This works for 20-40% of conversations depending on task distribution. The rest require human annotation.

Retire outdated conversations when the system changes. If you redesigned the appointment booking flow, old conversations that test the previous flow are no longer useful. Mark them as deprecated rather than deleting them — they are still valuable for understanding how the system used to work — but do not include them in active evaluations.

Sample production data continuously to keep the dataset representative. Every quarter, sample 50-100 new conversations from production, annotate them, and add them to the dataset. Remove an equivalent number of the oldest or least representative conversations. This keeps dataset size stable while ensuring it reflects current usage patterns.

Multi-turn evaluation datasets are infrastructure, not one-time deliverables. Budget for ongoing maintenance. A dataset that was perfect six months ago is outdated today. A dataset that accurately reflects production today will drift out of alignment in three months unless you actively maintain it.

Multi-turn evaluation datasets give you the ability to test conversational quality before shipping. They tell you whether your system can hold a coherent conversation, not just answer isolated questions. The next step is deciding whether to evaluate at the trace level — individual turns — or the session level — full conversations — and understanding what each level of granularity reveals.

