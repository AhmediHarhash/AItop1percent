# 1.8 — State and Memory in Continuous Conversations

Text-based chat is stateless by default. Each message arrives with no memory of what came before. If context is needed, the application explicitly passes previous messages to the model. The model sees exactly what you give it. Nothing more. This makes state management simple. You control what the model remembers. You log it, version it, and replay it. State is explicit, inspectable, and debuggable.

Voice conversations are continuous by nature. The user does not send discrete messages. They speak in turns, but those turns reference prior turns, assume shared context, and rely on information established earlier in the conversation. "What about Tuesday?" only makes sense if the system remembers that the user is trying to schedule something. "Make it for four people instead" only works if the system knows the user already specified a party size. Voice systems must maintain state across turns. That state is invisible to the user, implicit in the conversation, and vulnerable to corruption at every turn.

## The Difference Between Session Context and Dialog State

**Session context** is everything the system knows about the current interaction. The user's identity, their location, their device, the time of day, their previous requests in this session. Session context is stable. It changes slowly or not at all during the conversation. If the user is calling from New York at 9 a.m. on their mobile phone, that context is true for the entire session. The system can safely rely on it.

**Dialog state** is everything the system knows about the current task. The user wants to book a restaurant. The system has collected: cuisine type (Italian), location (downtown), party size (four people), date (unspecified), time (unspecified). Dialog state is volatile. It changes every turn. The user says "actually, make it six people," and party size updates from four to six. The user says "for tomorrow night," and date updates from unspecified to tomorrow. Dialog state is a constantly mutating data structure. Every turn adds information, refines information, or contradicts information.

The failure mode is confusing the two. Teams treat dialog state like session context — stable, reliable, safe to cache. They store party size in a variable, update it once, and assume it stays correct. Then the user says "wait, just two people," and the system does not update the variable because it already processed party size. The state is stale. The system books a table for four when the user wanted two. The conversation was clear. The state was wrong.

The second failure mode: over-rotating and treating session context like dialog state. The system re-infers the user's location on every turn because it assumes nothing is stable. The user is in New York. The system geolocates them on turn one: New York. Turn two: New York. Turn three: New York. The system wastes 80 milliseconds per turn re-deriving information that has not changed. Latency increases for no reason.

The correct mental model: session context is read-mostly. Dialog state is write-heavy. Session context is established once and referenced many times. Dialog state is updated constantly and must be validated on every turn. If you confuse the two, you either waste compute re-deriving stable facts or you rely on stale dialog state and give wrong answers.

## Slot Filling and the Incremental State Problem

Most task-oriented voice systems use **slot filling** — the system has a template for the task, and each turn fills one or more slots. Booking a restaurant requires: cuisine, location, date, time, party size. Initially, all slots are empty. The user says "I want Italian food in downtown for tomorrow night." Three slots fill: cuisine (Italian), location (downtown), date (tomorrow). Two remain empty: time and party size. The system prompts: "What time, and how many people?" The user responds: "7 p.m., four people." The final two slots fill. The task is complete.

This works perfectly when users provide information in neat, complete chunks. It fails immediately when users correct themselves, provide redundant information, or reference previous slots. The user says "I want Italian food downtown tomorrow night at 7 for four people." All slots fill in one turn. Then the user says: "Actually, make it 7:30." The system must recognize that "7:30" is an update to the time slot, not a new piece of information. It must overwrite the previous value. If the slot-filling logic does not handle updates, the system now has two time values: 7 p.m. and 7:30 p.m. The state is incoherent.

The worse case: partial updates. The user says "Change it to six people at 8." Two slots update simultaneously. The system must parse "six people" as party size and "8" as time. If the NLU model misses the structure, it might interpret "eight" as party size and ignore "six people." The update corrupts the state. The system books for eight people at 7:30 p.m. instead of six people at 8 p.m. The user explicitly corrected both values. The system got both wrong.

The mitigation is explicit slot tracking with confidence scores. Each slot stores not just a value, but also the turn on which it was set and the confidence of the extraction. When the user says "make it 7:30," the system sees that the time slot was previously set on turn two with 92% confidence. The new value "7:30" is extracted on turn four with 89% confidence. The system prioritizes the more recent turn and updates time to 7:30 p.m. The previous value is overwritten. The log preserves both values for debugging.

The edge case: ambiguous updates. The user says "change it to eight." Does "eight" refer to time (8 p.m.) or party size (eight people)? The system must disambiguate. If party size is already set and time is not, "eight" probably means 8 p.m. If time is already set and party size is not, "eight" probably means eight people. If both are set, the system cannot tell. It must ask: "Do you mean 8 p.m. or eight people?" This adds a turn. Latency increases. But the alternative is guessing wrong and corrupting state.

## State Drift Across Multi-Turn Conversations

The user starts a conversation: "Find me a flight to Boston." The system searches, presents options. The user selects one: "The 9 a.m. flight." The system confirms. The user asks: "What is the baggage policy?" The system retrieves baggage information for the selected flight. The user asks: "What about the 11 a.m. flight?" The system searches again, presents the 11 a.m. flight. The user asks: "What is the baggage policy?"

What flight does the system retrieve baggage information for? The 9 a.m. flight, which the user selected, or the 11 a.m. flight, which the user just asked about? The answer depends on whether "the baggage policy" refers to the currently selected flight or the most recently mentioned flight. The user might mean either. The system must guess. If it guesses wrong, it provides irrelevant information. The user gets frustrated.

This is **state drift** — the divergence between what the system thinks the conversation is about and what the user thinks the conversation is about. It happens when references are ambiguous and context is implicit. The user says "that one" or "the earlier option" or "what about the other one." The system must resolve the reference using dialog state. If the state is stale or ambiguous, the resolution is wrong.

The mitigation is explicit grounding. When the user says "what about the 11 a.m. flight," the system does not silently update the selected flight. It explicitly confirms: "Okay, looking at the 11 a.m. flight now." This signals to the user that the context has shifted. The user knows the system is no longer talking about the 9 a.m. flight. Future references are unambiguous because the system just grounded the new context.

The failure mode: over-grounding. If the system confirms every state change explicitly, the conversation becomes robotic and verbose. The user says "what about Boston?" The system says "Okay, searching for flights to Boston instead of New York." The user says "what about tomorrow?" The system says "Okay, changing the date to tomorrow instead of today." Every turn adds a confirmation. The conversation slows to a crawl. Users find it annoying. The system is clear but tedious.

The balance: ground only when ambiguity is high. If the user's reference is unambiguous — "book the 9 a.m. flight" — no confirmation needed. If the reference is ambiguous — "what about that one" — confirm explicitly. The system must detect ambiguity in real time and decide whether grounding is necessary. This requires confidence estimation on coreference resolution, which is hard. Most teams over-ground because it is safer than under-grounding. Users tolerate verbosity better than they tolerate mistakes.

## Context Windows and the Forgetting Problem

LLMs have finite context windows. GPT-5 supports 128,000 tokens. Claude Opus 4.5 supports 200,000 tokens. Gemini 3 Pro supports 1 million tokens. These numbers sound large. They are not infinite. A thirty-minute voice conversation generates 4,000 to 6,000 tokens of transcript. Add system prompts, retrieved context, and intermediate reasoning, and you are at 10,000 to 15,000 tokens per conversation. After ten conversations, you exceed most models' context limits. The system must forget.

The question is what to forget. The naive approach: keep the most recent N turns. When the context window fills, drop the oldest turn. This works for short conversations. It fails catastrophically for long, multi-topic conversations. The user discusses a flight booking, then switches to asking about hotel recommendations, then returns to the flight booking: "Actually, change that flight to the 11 a.m. option." The system has already dropped the flight booking context. It does not know which flight the user is referring to. The context is gone. The system cannot complete the task.

The smarter approach: keep the most task-relevant context. When the context window fills, drop the least relevant turns, not the oldest turns. If the user is currently working on a flight booking, keep all flight-related turns and drop unrelated small talk. This preserves task continuity. The risk: misjudging relevance. If the system drops a turn that later becomes relevant, the context is incomplete. The user references something the system has forgotten. The conversation breaks.

The hardest case: multi-task conversations. The user books a flight, reserves a hotel, schedules a rental car, and asks about restaurant recommendations. All four tasks are active. The system must maintain state for all of them simultaneously. The context window fills quickly. The system must decide which task to prioritize. If it drops hotel state to preserve flight state, the user cannot return to the hotel conversation. If it tries to keep all four, the context window overflows, and the system either crashes or truncates critical information.

The production solution is hierarchical memory. Keep detailed state for the current task in the LLM context window. Summarize completed tasks and store them in external memory — a database, a cache, a key-value store. If the user returns to a completed task, retrieve the summary and inject it back into the context. The summary is lossy — fine-grained details are gone — but the high-level state remains. The user can ask "what hotel did I book?" and the system can answer, even though the full hotel booking conversation is no longer in the LLM context.

The failure mode: summary corruption. The system summarizes a task incorrectly. The user booked a hotel for three nights. The summary says two nights. When the user asks about the booking later, the system retrieves the summary and reports two nights. The user corrects: "No, it was three nights." The system updates the summary. But the original context is gone. The system cannot verify which number is correct. It must trust either the summary or the user. If it trusts the summary, it contradicts the user. If it trusts the user, it propagates the correction even if the user is misremembering. There is no ground truth. The state is ambiguous.

## State Corruption from Failed Turns

The user says "book a table for four at 7 p.m." The ASR mishears "four" as "two." The system extracts party size as two. The system confirms: "Booking a table for two at 7 p.m." The user says "No, four people." The system updates party size to four. The state is corrected. Or is it?

The system's dialog state now contains: party size was initially two (turn one), updated to four (turn two). If the system logs state changes, the log shows two values. If the downstream booking API sees both values, it might use the wrong one. If the state update logic has a bug, it might preserve both values and create a conflict. The correction happened. The state might still be wrong.

The worse case: silent failures. The user says "four people." The ASR transcribes it correctly. The NLU model extracts party size as four. The dialog manager updates the state. The update fails — a database write error, a network timeout, a race condition. The system does not detect the failure. It continues the conversation as if the update succeeded. The user believes the system knows they want a table for four. The system's state still says two. The conversation proceeds. The system books a table for two. The user arrives at the restaurant. The table is wrong.

Silent state corruption is the worst failure mode in voice systems because it is invisible until the task completes. The conversation feels successful. Every turn seems to work. The final outcome is wrong. The user cannot debug it because they do not have access to the system's internal state. They only see the result. From their perspective, the system ignored their correction. From the system's perspective, the correction never happened.

The mitigation is transactional state updates. When the user provides new information, the system writes the update to state, reads it back, and confirms it aloud. "Got it, four people." The confirmation is not just UX polish. It is a validation step. If the write failed, the read will return the old value. The system will confirm "Got it, two people." The user will correct again. The failure is no longer silent. The trade-off: every state update adds latency. The system must write, read, and synthesize a confirmation before continuing. TTFA increases. But state integrity improves.

## The Memory Architecture Voice Systems Actually Need

Text-based chat systems store conversation history as a list of messages. Each message is a discrete unit. The list grows. When it exceeds the context window, the system truncates or summarizes. The architecture is simple because the data structure is simple.

Voice systems need three layers of memory. **Turn-level memory** stores the raw transcript and extracted intents for each turn. This is the atomic unit. Turn-level memory is detailed, complete, and short-lived. It fills quickly and must be pruned aggressively. **Task-level memory** stores the current dialog state — slots, values, confidence scores, and the history of updates. Task-level memory persists for the duration of the task. When the task completes, task-level memory is summarized and moved to long-term storage. **Session-level memory** stores high-level facts about the user and the conversation. The user's identity, their preferences, their history with the system. Session-level memory is sparse, long-lived, and retrieved selectively.

The architecture must support random access. The user asks "what was that flight I looked at earlier?" The system cannot replay the entire conversation transcript. It must retrieve the specific flight search from task-level memory, even if that search happened ten turns ago. Random access requires indexing. Each task gets an identifier. Each slot update is tagged with a task ID and a turn number. The system can query "retrieve all flight searches in this session" or "retrieve the most recent party size update for the restaurant booking task." The query returns the relevant state without scanning the entire history.

The failure mode is memory fragmentation. The user starts a task, switches to a different task, returns to the first task, starts a third task, and never completes any of them. Task-level memory accumulates. Three incomplete tasks are stored, each with partial state. The system does not know which task is active. When the user says "go ahead and book it," the system must decide which task to complete. It guesses wrong. The user meant task two. The system books task one. The outcome is wrong.

The mitigation is explicit task switching. When the user changes topics, the system detects the switch and confirms: "Switching to hotel search. I will save the flight booking for later." The user knows the context has changed. The system knows which task is active. When the user says "go ahead and book it," the system knows which task to complete because the switch was explicit. The conversation is longer. The state is unambiguous.

Voice systems require memory architectures that text systems do not. Turn-level, task-level, and session-level memory must coexist. Updates must be transactional. Context must be retrievable. State must survive corrections, interruptions, and task switches. The architecture is complex because the interaction is continuous. There are no clean message boundaries. There is only flow, interruption, and the constant risk that what the system remembers is not what the user said.

The challenges of voice systems — latency, interruptibility, perception, and state — are foundational. The next step is designing evaluation frameworks that actually measure what matters: not just whether the system is accurate, but whether it is conversationally functional.

