# Chapter 1 — Why Voice Systems Are Categorically Different

Voice AI is not text AI with a microphone attached. The shift from request-response to streaming, from unlimited thinking time to sub-second budgets, from asynchronous interaction to continuous conversation changes every assumption about how AI systems work, how they fail, and how you evaluate them. The constraints that define text AI — batch processing, retry logic, human tolerance for multi-second delays — do not exist in voice. The constraints that define voice AI — perceptual latency budgets, interruptibility, emotional signaling through timing — do not exist in text.

This chapter establishes the mental model for voice AI evaluation. You cannot carry forward your text AI instincts. You need new frameworks, new metrics, and a fundamentally different understanding of what quality means when the system is expected to behave like a human conversation partner, not a chatbot with a voice interface.

The difference is not incremental. It is categorical. And the teams that treat voice as a minor variation on text are the teams whose voice products feel robotic, slow, and frustrating — no matter how good the underlying model is.

---

- 1.1 — The One-Second Wall: Where Conversations Break
- 1.2 — Streaming vs Request-Response: A Paradigm Shift
- 1.3 — Time as a First-Class Constraint
- 1.4 — The Three-Component Pipeline: ASR, LLM, TTS
- 1.5 — Why Traditional AI Metrics Fail Voice Systems
- 1.6 — The Perception Gap: How Users Experience Latency
- 1.7 — Interruptibility as a Core Requirement
- 1.8 — State and Memory in Continuous Conversations
- 1.9 — The Emotional Layer: How Timing Affects Trust
- 1.10 — Voice AI Failure Modes That Do Not Exist in Text
- 1.11 — The Regulatory Landscape for Voice AI in 2026
- 1.12 — Building the Mental Model for This Section

---

*Time is not a performance metric in voice AI. It is the constraint that determines whether your system feels human or feels broken.*
