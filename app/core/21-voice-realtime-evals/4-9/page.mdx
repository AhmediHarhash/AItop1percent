# 4.9 — Real-Time Partial Transcription Quality

The latency target was 200 milliseconds. The team at a voice-driven medical documentation platform had built their entire UX around that number — physicians would speak, see the text appear almost instantly, keep talking. When the ASR provider switched from final-only transcripts to streaming partials in late 2025, the team celebrated. Latency dropped to 180 milliseconds. Users loved the faster response.

Three weeks into production, the complaint pattern emerged. Physicians were correcting the same phrase three times. The transcript would show "patient presents with chest," then update to "patient presents with test," then finally settle on "patient presents with chest pain" when the utterance ended. The words flickered on screen. Physicians lost trust in what they saw. Some stopped looking at the screen entirely and just waited for the final result — defeating the entire point of streaming partials. The problem wasn't accuracy. The final transcripts were fine. The problem was **partial instability** — the intermediate results changed so often that users couldn't act on them.

Streaming ASR produces partial transcriptions before the speaker finishes the utterance. These partials enable low-latency experiences. The user sees text appear in real-time, receives immediate feedback, catches errors as they happen. But partials come with a cost: they are inherently unstable. As the ASR model receives more audio context, it revises its understanding of what was said. The word "test" becomes "chest" when the model hears "pain" afterward. This revision is correct behavior — more context enables better accuracy. But if revisions happen too frequently or too dramatically, the user experience degrades. Evaluating partial transcription quality means measuring not just accuracy but stability, consistency, and the user's ability to trust what they see before the utterance ends.

## The Partial-Final Disconnect

Streaming ASR systems produce two kinds of output: partials and finals. Partials are intermediate transcriptions sent while the user is still speaking. Finals are the completed transcription after the utterance ends or after a sufficiently long pause. Most teams evaluate finals exclusively. They measure WER on completed transcripts, compare final outputs to ground truth, optimize for end-of-utterance accuracy. This creates a dangerous blind spot.

A system can have excellent final accuracy and terrible partial quality. The medical documentation platform measured 4.2 percent WER on final transcripts — best in class for their domain. But when they analyzed partials, they found that 38 percent of words appeared in at least one partial before the final, and 14 percent of words changed at least twice. Users saw "the patient has a history of heart" flicker to "the patient has a history of art" before settling on "the patient has a history of heart disease." The final was correct. The journey to get there was chaotic.

The disconnect happens because final accuracy doesn't penalize instability. WER is calculated on the completed transcript. If the final result is correct, it doesn't matter that the system showed six different intermediate versions along the way. But users experience every intermediate version. Every flicker. Every word that appears and then disappears. If the partial says "I want to book a flight to Boston" and the final says "I want to book a flight to Austin," the WER is low — only one word error. But the user saw "Boston," processed it, maybe even started thinking about their trip, and then had to mentally correct when "Austin" appeared. The cognitive load is real.

You need separate metrics for partials and finals. Final accuracy tells you whether the system ultimately gets it right. Partial stability tells you whether the user can trust what they see in real-time. Both matter. Both need measurement. Optimizing for finals alone produces systems that are accurate on paper and frustrating in practice.

## Partial Accuracy Over Time

Partial transcriptions improve as more audio context arrives. The first partial might be a single word. The second adds a few more. The third revises the first word based on new context. The final partial converges to the final transcription. Measuring partial accuracy means tracking how accuracy evolves over time — not just at the end, but at every intermediate step.

Define time windows based on audio duration. Measure partial accuracy at 500 milliseconds, 1 second, 1.5 seconds, 2 seconds, and so on until the utterance ends. For each window, calculate the WER of the partial transcript at that point compared to the final ground truth. Plot accuracy over time. A good streaming ASR system shows a steep accuracy curve — accuracy improves quickly in the first second and plateaus near-final levels within two seconds. A poor system shows slow convergence — accuracy is still improving at four or five seconds, meaning the user sees unstable text for a long time.

A voice-controlled navigation system measured partial accuracy at fixed intervals. At 500 milliseconds, the average WER was 34 percent. At 1 second, it dropped to 18 percent. At 1.5 seconds, 9 percent. At 2 seconds, 5 percent. Finals averaged 3.8 percent WER. The curve told the story: most of the accuracy improvement happened in the first 1.5 seconds. After that, the partials were close to final quality. This meant users could start trusting what they saw after about 1.5 seconds of speech. The team used this to tune their UX — they dimmed the text for the first 1.5 seconds and made it solid after, signaling to the user when the transcript was stable.

The time-based accuracy metric reveals latency-accuracy trade-offs. If you send partials aggressively early — at 200 milliseconds — the accuracy will be low. If you wait until 2 seconds, accuracy will be higher but the user experiences more delay. The optimal send cadence depends on your use case. For live captions, sending early partials despite lower accuracy is fine — users understand captions are approximate. For voice commands, waiting for higher-confidence partials reduces false triggers. Measure accuracy over time, then decide when to send partials based on the trade-off your users can tolerate.

## Partial Stability: Measuring Revision Frequency

Partial stability is how often a word changes between consecutive partial transcriptions. A stable system produces partials that converge smoothly — each new partial adds words or refines the last word but rarely revises words in the middle of the utterance. An unstable system constantly revises — words appear, disappear, and reappear as the model reinterprets earlier audio based on later context.

Measure **revision rate** by tracking how many words in each partial differ from the corresponding words in the previous partial. Not counting new words added at the end — those are expected as the utterance continues. Count changes to words that already appeared in earlier partials. If partial one says "I want to book a flight" and partial two says "I want to cook a flight," the word "book" was revised to "cook." That's one revision.

Calculate revision rate as revisions per word across all partials for an utterance. A medical documentation system analyzed 10,000 utterances. The average utterance produced 8.4 partial transcriptions before the final. The average revision rate was 0.22 revisions per word — meaning about one in every five words changed at least once before the final. High-instability utterances — those in the 90th percentile — had revision rates above 0.45, nearly one revision per two words. These were the utterances users complained about most.

Revision rate correlates strongly with user dissatisfaction. A voice assistant team ran a study where users rated transcription quality on a five-point scale after each interaction. Utterances with revision rates below 0.15 averaged 4.3 out of 5. Utterances with revision rates above 0.35 averaged 2.8 out of 5. Same final WER — 4.1 percent across both groups. The difference was stability. Users tolerated minor inaccuracies in the final transcript. They could not tolerate seeing the transcript flicker and change constantly before reaching the final.

Track revision rate separately by position in the utterance. Revisions at the end — the last two or three words — are less disruptive than revisions in the middle. If the user sees "I want to book a flight to Boston" and the final word changes from "Boston" to "Austin," they notice but it's not chaotic. If the third word "book" changes to "cook" after six more words have appeared, the user has to mentally re-parse the entire sentence. Weight middle revisions higher than end revisions when calculating instability scores.

## Latency vs. Stability Trade-Offs

Streaming ASR faces a fundamental trade-off: send partials early for low latency, or wait for more context to ensure stability. Early partials have less audio context, leading to lower accuracy and more revisions as context arrives. Delayed partials have more context, leading to higher accuracy and fewer revisions but longer perceived latency. The optimal balance depends on the use case.

For live captions, latency matters more than stability. Captions are expected to be approximate. Users tolerate some flickering because the alternative — waiting three seconds for each caption — breaks the real-time experience. A live captioning system sent partials every 300 milliseconds. Revision rate was 0.31 per word. Users didn't care. They wanted to see the words as close to real-time as possible.

For voice commands, stability matters more than latency. Commands trigger actions. If the partial says "delete email" and then revises to "send email," the system might take the wrong action if it acts on the partial. A voice-controlled email client waited until partials had stabilized for 800 milliseconds before executing commands. This meant users experienced a slight delay — about 1.2 seconds from end of speech to action — but false triggers dropped by 73 percent compared to acting on the first partial.

For real-time dictation, both matter. Users need low latency to maintain flow, but they also need stability to trust what they see. A dictation app used a **two-tier display strategy**: partials appeared in gray text for the first 1 second, then turned black once the revision rate for that segment dropped below 0.1 per word. This gave users a visual signal about stability without sacrificing latency. They saw gray text immediately, knew it might change, and saw it solidify into black once the system was confident.

Measure the latency-stability curve by plotting revision rate against partial send delay. Test sending partials at 200ms, 400ms, 600ms, 800ms, and 1000ms after speech starts. Measure the revision rate for each configuration. You'll see a curve: early sends have high revision rates, later sends have lower rates. The optimal point depends on your user tolerance. Dictation users might tolerate 0.2 revisions per word at 400ms latency. Command users might need 0.05 revisions per word even if it means 1-second latency. Run the curve, find the point that fits your use case.

## When to Act on Partials vs. Wait for Finals

Some systems act on partial transcriptions. Others wait for finals. Acting on partials reduces perceived latency but increases error risk. Waiting for finals increases accuracy but adds delay. The decision depends on the cost of acting incorrectly.

Define **action risk** as the potential damage if the system acts on a partial that changes in the final. For a navigation system, acting on "turn left" when the user said "turn right" is high risk — the user ends up in the wrong place. For live captions, displaying "the meeting starts at three" when the user said "the meeting starts at two" is low risk — the user will see the correction in the next partial. High-risk actions require waiting for finals or high-confidence partials. Low-risk actions can use early partials.

A smart home voice assistant classified commands by risk. Low-risk: play music, set a timer, check the weather. Medium-risk: send a message, make a call, turn off lights. High-risk: unlock doors, disarm security, authorize a payment. Low-risk commands acted on partials as soon as the intent was clear — usually within 600 milliseconds. Medium-risk commands waited for finals or required partials with confidence scores above 0.92. High-risk commands required finals plus explicit user confirmation. The result: most interactions felt instant (low-risk commands dominate usage), but critical actions stayed safe.

Measure **partial-final divergence rate** to understand how often acting on partials would cause errors. For each utterance, compare the action the system would take based on the last partial before the final vs. the action based on the final. If they differ, that's a divergence. A customer service voice bot analyzed 50,000 calls. Divergence rate for intent classification was 8.4 percent — 8.4 percent of the time, the intent extracted from the last partial differed from the intent extracted from the final. This meant acting on partials would cause the wrong action 8.4 percent of the time. That was acceptable for low-stakes intents like "tell me my account balance" but unacceptable for high-stakes intents like "transfer funds."

Set divergence thresholds by action risk. Low-risk actions: tolerate up to 10 percent divergence. Medium-risk: 3 percent or less. High-risk: zero tolerance, always wait for finals. Measure your actual divergence rates in production, then route actions accordingly. If your measured divergence for a specific intent is 2.1 percent and your threshold for that intent is 3 percent, you can safely act on partials. If divergence is 4.3 percent, wait for finals.

## Evaluating Partial Transcription in Multi-Turn Conversations

In multi-turn conversations, partials from the current turn interact with context from previous turns. The system must maintain conversational state while processing unstable partials. This creates unique evaluation challenges.

A customer service bot kept conversation history to handle follow-up questions. User says "What's my balance?" Bot responds with the balance. User says "Transfer half of that to savings." The word "that" refers to the balance from the previous turn. If the partial transcription of the second utterance is unstable — first showing "Transfer half of this," then "Transfer all of this," then "Transfer half of that" — the bot must decide when to lock in the reference and execute the action.

Measure **context-dependent partial instability** by tracking how often revisions in the current partial change the interpretation of context-dependent references. For the transfer example, track how many times the partial changes in a way that would alter which account or amount the bot interprets. A revision from "that" to "this" might not change the interpretation if both refer to the same entity. But a revision from "half" to "all" fundamentally changes the action.

A voice banking system measured context-dependent revisions across 30,000 multi-turn sessions. In 6.2 percent of turns with context references, partial revisions changed the interpretation at least once before the final. In 1.8 percent of turns, the interpretation changed more than once. These were the turns where users either had to repeat themselves or where the bot asked clarifying questions because it acted on an incorrect partial interpretation.

Use **reference stability metrics** for context-heavy interactions. For each turn with a context reference (pronouns, deictic terms like "that" or "there," implicit subjects), measure how many times the partial interpretation of the reference changed before the final. High reference instability — more than one change per turn — signals that the ASR is struggling with context resolution, and the system should wait for finals before acting on context-dependent commands.

## Handling Speaker Overlap and Partial Corrections

Real conversations include overlaps, interruptions, and self-corrections. Streaming ASR must handle cases where the speaker starts a sentence, pauses, and restarts. Or where two speakers talk over each other. Partials in these scenarios are inherently unstable because the audio context is ambiguous.

A transcription service for meetings saw frequent overlap cases: one speaker says "The project deadline is—" and another speaker interrupts with "Actually, we extended it." The ASR must decide whether to finalize the first speaker's partial or wait to see if they continue. If it finalizes too early, the transcript shows "The project deadline is" as a complete sentence when the speaker was about to add more. If it waits too long, latency increases.

Measure **false finalization rate** — the percentage of partials marked as final that are followed by continuation from the same speaker within two seconds. A false finalization happens when the system thinks the speaker is done but they're just pausing. A meeting transcription system had a 12 percent false finalization rate. Twelve percent of finals were followed by the same speaker continuing the sentence. This caused the transcript to show fragmented sentences that should have been joined.

Track **overlap handling accuracy** by measuring how often the system correctly attributes overlapping speech to the right speaker and whether it preserves both speakers' words or drops one. In a 5,000-utterance test set with 18 percent overlap cases, one ASR system correctly transcribed both speakers in 64 percent of overlaps, transcribed only the louder speaker in 29 percent, and completely failed (gibberish or silence) in 7 percent. Another system scored 71 percent, 22 percent, and 7 percent. The first system prioritized clarity (when in doubt, transcribe one speaker well), the second prioritized completeness (try to get both).

Choose your overlap strategy based on use case. For live captions, prioritize the primary speaker — drop the overlap if necessary to keep the main thread clear. For legal or medical transcription, prioritize completeness — mark overlaps explicitly even if it means "Speaker 1 and Speaker 2 both speaking" in the transcript. Measure your handling strategy against your use case requirements. If your use case demands completeness and your overlap handling only preserves both speakers 64 percent of the time, you have a gap to close.

## Partial Transcription Metrics in Production Dashboards

Partial quality degrades silently. Unlike final transcripts, which users can report as incorrect, partials are ephemeral — the user sees them, they change, and the user has no way to report "that partial flickered too much." You must measure partial quality in production through instrumentation, not user reports.

Log every partial sent to the client. For each utterance, record the sequence of partials, the timestamps, and the final. Calculate revision rate, partial-final divergence, and time-to-stability for every utterance. Aggregate these metrics in your production dashboard. Track the 50th, 90th, and 99th percentile values. A 50th percentile revision rate of 0.12 is fine. A 99th percentile of 0.68 means one percent of your utterances are chaotic, and those users are having a terrible experience.

A dictation app tracked partial metrics alongside final WER. Final WER stayed stable at 3.9 percent for three months. But in week eight, the 90th percentile revision rate jumped from 0.23 to 0.41. Investigation revealed that a backend change had reduced the ASR's context window to improve latency — the system was now making decisions with less audio context, leading to more revisions. Final accuracy was unaffected because the finals still had full context. But partials were unstable. The team reverted the change. Without partial metrics in the dashboard, they would never have caught the regression.

Set alerts on partial stability metrics. If 90th percentile revision rate exceeds 0.35 for more than one hour, alert the on-call team. If partial-final divergence for high-risk intents exceeds 5 percent, alert immediately. These thresholds depend on your baseline — measure your normal ranges for a week, then set alerts at 1.5x normal for warnings and 2x normal for critical. Partial degradation is often a leading indicator of deeper ASR issues. Catch it early.

Evaluating real-time partial transcriptions requires measuring accuracy, stability, revision frequency, and context-handling across time — not just the final result. Users experience every partial, every flicker, every revision. Your metrics must capture that experience. Measure partials with the same rigor you measure finals, set thresholds based on user tolerance and action risk, and monitor partial quality in production continuously. The next subchapter covers ASR confidence scores and their calibration — whether the system's internal certainty matches reality.
