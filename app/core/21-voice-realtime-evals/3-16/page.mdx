# 3.16 — Acoustic Framing: How Audio Cues Shape Time Perception

The first sound a user hears after they stop speaking determines how fast the system feels. The last sound before the system begins its response shapes whether the transition feels natural or mechanical. These sounds — the acoustic frame around the response — have disproportionate influence on perceived latency. A 350-millisecond response that begins with a soft breath sound and follows a gentle trailing tone feels faster and more natural than a 280-millisecond response that starts abruptly after dead silence.

Acoustic framing is the deliberate use of sound to signal intent, mark transitions, and create perceptual continuity in conversation. Humans do this naturally. Before speaking, we inhale. Our lips part. We make a soft "mm" or "ah" sound. After speaking, our voice trails off. We exhale. These sounds are not part of the message, but they're part of the communication. They signal "I'm about to speak" and "I'm finished speaking." When voice systems omit these acoustic frames, they feel robotic and slow. When they include them, they feel responsive and human.

## The Onset Frame: Signaling Intent Before Speech

The moment between the user's speech ending and the system's response beginning is the most critical window in the entire interaction. If that window is silent, the user's brain enters a state of uncertainty. Did the system hear me? Is it processing? Is the connection alive? Each millisecond of silence amplifies the uncertainty. By 300 milliseconds, the user is wondering if the system failed. By 500 milliseconds, they're repeating themselves.

**Breath sounds** are the most natural onset frame. Before humans speak, they inhale. The inhale is audible — a soft intake of air lasting 40 to 80 milliseconds. If a TTS system generates a breath sound before the first word, the user perceives the response as starting earlier. The technical latency is unchanged — the system still takes 350ms to generate the first word. But the perceptual latency drops by 60 to 100 milliseconds because the breath signals "I'm preparing to respond."

In mid-2025, a conversational AI company tested breath-augmented TTS. The baseline system took 340ms from speech end to first word. The breath-augmented system took 360ms — 40ms for the breath, 320ms for the word. Users rated the breath-augmented system as faster and more natural. The 40ms breath sound bought 80ms of perceptual speed because the user heard the system engaging.

**Lip parting sounds** are another natural onset cue. Before producing the first consonant or vowel, human lips move into position. This movement produces a subtle sound — a soft "p" or "m" onset, a gentle click, a faint rustling. These sounds are barely audible but perceptually significant. They mark the transition from silence to speech. TTS systems that generate realistic lip parting sounds feel more human because they reproduce the full acoustic onset, not just the phonemes.

**Soft verbal fillers** — "mm," "ah," "um" — are human hesitation markers that also function as acoustic frames. When a person is formulating a response, they often emit a soft filler sound while they think. If a voice assistant uses a filler sound at the start of a response — especially for complex queries — it signals "I'm thinking about this" rather than "I'm slow." The filler adds 60 to 100 milliseconds of technical latency but improves perceptual naturalness.

The trade-off is subtle. Overusing fillers makes the system sound uncertain or unintelligent. Using them sparingly on complex queries makes the system sound thoughtful. The healthcare assistant from earlier examples added a soft "mm" filler to the onset of responses that required multi-step reasoning. Users described the system as "more natural" and "less robotic." Technical latency increased by 70ms. Perceived responsiveness improved.

**Acknowledgment tones** are non-speech acoustic frames that signal "I heard you, processing now." A gentle tone, a soft chime, a brief pulse — these sounds are not trying to be human, but they fill the silence and reduce uncertainty. For systems where naturalness is less important than transparency — enterprise voice interfaces, voice-controlled industrial systems — acknowledgment tones are effective. For systems aiming for conversational naturalness, breath sounds and fillers are better because they blend into speech.

## The Trailing Frame: Marking the End of Speech

Just as the onset frame signals the start of a response, the trailing frame signals its end. Human speech doesn't stop abruptly. The voice fades. The breath releases. The lips close. These acoustic markers tell the listener "I'm finished." If a TTS system cuts off abruptly — the last phoneme ends and the audio stops — the transition feels mechanical. If the system includes a natural trailing frame, the end feels complete.

**Exhale sounds** are the natural counterpart to breath onset. After finishing a sentence, humans exhale. The exhale is softer than the inhale — 30 to 60 milliseconds of gentle airflow. If a TTS system adds a subtle exhale after the last word, the response feels closed. The user knows the system is done. The absence of an exhale leaves the response feeling incomplete, like a sentence ending mid—

**Voice fade and trailing phonemes** involve extending the final phoneme slightly longer than standard TTS duration and adding a natural amplitude decay. Instead of cutting the last vowel at its target duration, the system extends it by 20 to 40 milliseconds and reduces volume gradually. The result is a voice that trails off rather than stopping. Listeners perceive the trailing voice as more natural and more complete.

**Trailing silence with acoustic context** means leaving a brief pause — 100 to 150 milliseconds — after the final phoneme before the system returns to a ready state. This pause is not dead silence. It includes the ambient acoustic environment — room tone, background hum, the faint noise floor of the microphone. The pause with context feels like the end of a conversational turn. Dead silence feels like a dropped connection.

## How Acoustic Framing Changes Perceived Latency

Acoustic framing doesn't reduce technical latency. It changes how users perceive time. The mechanism is perceptual anchoring — the user's brain uses the first sound it hears as the anchor for "when the response started." If the first sound is a breath at 200ms and the first word arrives at 240ms, the brain anchors the response start at 200ms. If the first sound is the word at 240ms, the brain anchors at 240ms.

A financial services voice assistant in late 2025 measured 380ms from speech end to first word. Users described it as "slow" and "laggy." The team added a 50ms breath sound at 200ms — before the word was ready. Users now heard something at 200ms and the word at 250ms. Technical latency was unchanged — 380ms from speech end to word completion. But the user's brain anchored the response at 200ms. User complaints about lag dropped by 55%.

**The gap between acoustic onset and semantic content** determines how much latency the framing buys. If the breath sound arrives at 180ms and the word at 200ms, the perceptual gain is small — the word follows the breath immediately. If the breath arrives at 150ms and the word at 350ms, the perceptual gain is large — the user hears the system engaging 200ms before content arrives. But if the gap is too large — breath at 100ms, word at 500ms — the framing feels disconnected. The breath doesn't predict the word. The system feels broken.

The optimal gap is 50 to 150 milliseconds. Breath sound arrives. Within 50 to 150ms, the first word follows. The brain perceives continuity. The response feels fast.

## Implementing Acoustic Framing in TTS Pipelines

Acoustic framing requires modifying the TTS generation pipeline to include non-phonemic audio before and after the synthesized speech.

**Onset injection** means prepending a breath or filler sound to the TTS output before the first phoneme. If the TTS system generates the word "Hello," the final audio includes a 40ms breath, then "Hello." The breath is not part of the linguistic content, but it's part of the acoustic signal. Implementation options include pre-generating a library of breath sounds and randomly selecting one to prepend, or using a breath synthesis model that generates realistic inhales conditioned on the speaker's voice characteristics.

Pre-generated breath libraries are simpler but less natural. Every breath sounds identical. Synthesized breaths are more natural but require additional model inference. For systems where naturalness is critical, synthesized breaths are worth the cost. For systems where speed matters more, pre-generated libraries are sufficient.

**Trailing injection** means appending an exhale or trailing silence to the TTS output after the last phoneme. The TTS system generates "Goodbye," then appends a 40ms exhale and 100ms of ambient room tone. The response ends with acoustic completeness. The user perceives the system as finishing naturally rather than cutting off.

**Dynamic framing based on response type** means adjusting the framing based on what the system is saying. Short confirmations — "Yes," "Got it," "Done" — don't need breath onsets. They're quick and definitive. Adding a breath makes them feel slow. Long responses — multi-sentence explanations, complex instructions — benefit from breath onsets and trailing exhales. They signal the start and end of a substantial turn.

The healthcare assistant from earlier examples used dynamic framing. Responses under 10 words had no breath onset. Responses over 20 words included a 50ms breath before the first word and a 40ms exhale after the last. Users rated the system as "more natural" without being able to articulate why. The framing was subtle enough to feel natural, not obvious enough to feel designed.

## When Acoustic Framing Backfires

Acoustic framing is not universally beneficial. There are contexts where it damages the user experience.

**Over-framing creates artificial delay.** If every response — even single-word confirmations — includes a breath onset, the system feels slow. Users don't need to hear the system "preparing" to say "Yes." The framing adds 50ms of unnecessary latency. The rule is: use framing for substantial responses, omit it for brief ones.

**Inconsistent framing breaks immersion.** If some responses have breath onsets and others don't, and the pattern is random, users notice the inconsistency. The system feels like it's switching between modes. Framing must be applied consistently based on clear rules — response length, complexity, turn position.

**Unnatural framing feels robotic.** If the breath sound is identical every time, or if the exhale is perfectly uniform, the framing draws attention to itself. Users hear the pattern and recognize it as artificial. Natural breath sounds vary in duration, intensity, and pitch. Synthesized framing must include variation to feel human.

**Framing that doesn't match the voice creates dissonance.** If the TTS voice is a low-pitched male speaker but the breath sound is high-pitched, the framing clashes with the voice. Breath sounds must be matched to the voice characteristics — pitch, timbre, intensity. Mismatched framing is more distracting than no framing at all.

A customer support assistant in late 2025 added breath framing using a single pre-recorded breath sample. The TTS voice was female. The breath sample was male. Users described the system as "weird" and "uncanny." The team replaced the breath sample with a female-voiced breath matched to the TTS speaker. The uncanny valley effect disappeared.

## Acoustic Framing and Cultural Expectations

Acoustic framing that feels natural in one language or culture may feel strange in another. The acceptable duration of silence before a response varies across cultures. The use of verbal fillers varies. The expectation of breath sounds varies.

**North American English speakers** expect responses within 200 to 350 milliseconds. Longer delays feel slow. Breath onsets and soft fillers are perceived as natural. A 60ms "mm" filler before a complex response feels thoughtful.

**Japanese speakers** tolerate longer pauses before responding. A 500ms pause is not considered slow — it's considered polite, giving space for the previous speaker to continue if they wish. Breath sounds are less prominent in Japanese conversational rhythm. Adding American-style breath framing to a Japanese voice assistant can feel overly eager or interruptive.

**Spanish speakers** have faster conversational turn-taking. Responses are expected within 150 to 250 milliseconds. Longer pauses feel distant. Breath onsets are subtle and quick. A 40ms breath is natural. A 100ms breath feels like the system is hesitating.

**British English speakers** expect slightly more formal and deliberate pacing than American English. A 350ms pause before a response feels appropriate for thoughtful questions. Verbal fillers like "um" and "ah" are perceived as less professional. Breath sounds are acceptable but should be subtle.

Acoustic framing must be tuned to the language and culture of the user. A universal framing strategy — the same breath duration, the same filler sounds, the same trailing silence — will feel unnatural to significant portions of your user base. Measure user perception across regions and adjust framing parameters accordingly.

## Measuring the Impact of Acoustic Framing

The impact of acoustic framing is perceptual, not technical. Measurement requires user studies, not latency profilers.

**A/B testing framed vs unframed responses** means serving 50% of users a TTS system with acoustic framing and 50% a system without. Measure user satisfaction, perceived responsiveness, and turn abandonment rate. If the framed group reports higher satisfaction and lower abandonment, framing works. If both groups rate the system identically, framing has no perceptual effect.

The financial services assistant that added 50ms breath framing ran a two-week A/B test. The framed group rated the system 18% higher on "feels responsive" and had a 12% lower turn abandonment rate. Technical latency was identical between groups. Perceptual latency improved.

**Qualitative feedback on naturalness** involves asking users to describe the system in their own words. If users describe the framed system as "more human," "smoother," or "more natural," framing is working. If users describe it as "mechanical" or "weird," framing is poorly implemented.

**Latency perception surveys** ask users to estimate how long the system took to respond. If users estimate 250ms for a system that actually took 350ms with framing, framing reduced perceptual latency. If users estimate 400ms for a system that took 350ms without framing, the lack of framing made the system feel slower.

Acoustic framing is invisible when it works. Users don't notice the breath sounds or trailing exhales. They just feel the system is fast, natural, and responsive. When framing fails, users notice. The system feels off, robotic, or slow. The difference is in the subtlety of implementation and the match between framing and user expectations.

Next, we examine conversational rhythm — how to match the natural cadence of human turn-taking to make interactions feel effortless.
