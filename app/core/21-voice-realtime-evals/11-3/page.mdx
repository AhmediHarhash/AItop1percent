# 11.3 — ASR Quality Monitoring in Production

In November 2025, a healthcare voice assistant launched with 96% word error rate accuracy in testing. Three weeks into production, patient complaints spiked. The system was misunderstanding medication names, botching dosages, and transcribing symptoms incorrectly. The engineering team reviewed their ASR metrics. Word error rate in production: still 96%. The model had not regressed. The benchmark had not changed. But the real-world accuracy had collapsed to 80% on the conversations that mattered most — elderly patients with accents, users in noisy environments, medical terminology the benchmark never covered. The team had been monitoring WER on test data while production quality silently degraded on real users.

This is the central problem of ASR quality monitoring: the metrics you can measure easily do not correlate with the quality users experience. Benchmarks test clean audio, native accents, and common vocabulary. Production audio is noisy, accented, domain-specific, and full of edge cases. Confidence scores tell you when the ASR model is uncertain but not whether its output is correct. Offline evaluation gives you a number but not the distribution of errors across user cohorts, conversation types, or vocabulary domains. You cannot manage what you do not measure, and most teams are not measuring ASR quality in the way that predicts user satisfaction.

## Confidence Score Tracking Over Time

Every ASR system returns a confidence score with each transcription. The score represents the model's certainty that the transcription is correct. A confidence of 0.95 means the model is highly confident. A confidence of 0.60 means the model is uncertain. Confidence scores are imperfect — a confident model can still be wrong — but they are the only real-time signal you get for every transcription without human review.

The mistake teams make is logging confidence scores but not analyzing them. The score for a single transcription tells you almost nothing. The distribution of confidence scores over time tells you everything. If your median confidence score is 0.92 and it drops to 0.85 over two weeks, something changed. The audio quality degraded, the user population shifted, the ASR model was updated and performs worse on your domain, or a bug in audio preprocessing is corrupting inputs. The median confidence drop is an early warning that accuracy is declining even if you have not measured WER directly.

You need dashboards that show confidence score distributions by day, by user cohort, by conversation type. Plot the median, the 25th percentile, and the 10th percentile. The median tells you the typical case. The 25th percentile tells you how often the model is uncertain. The 10th percentile tells you how often the model is very uncertain. If the 10th percentile confidence drops from 0.70 to 0.50, you have a growing tail of low-quality transcriptions. These are the cases most likely to confuse the LLM and frustrate users.

Confidence thresholds let you detect when a conversation is likely to fail. You set a minimum acceptable confidence — say, 0.75 — and track the rate of transcriptions that fall below it. If 5% of transcriptions fall below 0.75, that is normal. If 15% fall below, something is wrong. You can alert on this rate just like you alert on error rates. A spike in low-confidence transcriptions is a leading indicator of user-reported issues.

The pattern that works in production: tag every transcription with its confidence score, the user ID, the conversation ID, the audio duration, and any metadata like device type or network quality. Log this in a time-series database. Build alerts that fire when the percentage of low-confidence transcriptions exceeds a threshold, when median confidence drops by more than 10% week-over-week, or when confidence scores for a specific user cohort degrade. These alerts catch ASR degradation days or weeks before users complain loudly enough for the problem to reach leadership.

## Sampling Strategies for Transcription Verification

Confidence scores tell you when the ASR model is uncertain. They do not tell you when it is confidently wrong. The only way to measure true accuracy in production is to compare ASR output to human-verified transcriptions. You cannot verify every transcription — that does not scale. You can verify a sample, and if you sample correctly, that sample tells you what is happening across the full population.

The simplest sampling strategy: random sampling at a fixed rate. You pick 0.5% of all transcriptions, send them to human reviewers, and measure WER on the sample. This gives you an unbiased estimate of overall accuracy. If your sample WER is 8%, your population WER is approximately 8%. Random sampling works when you care about average accuracy across all users. It does not work when you care about accuracy for specific cohorts, edge cases, or high-stakes conversations.

The smarter sampling strategy: stratified sampling by confidence score. You sample 50% of transcriptions with confidence below 0.60, 10% of transcriptions with confidence between 0.60 and 0.75, 2% of transcriptions with confidence between 0.75 and 0.90, and 0.2% of transcriptions with confidence above 0.90. This oversamples the cases where the model is uncertain, which are the cases most likely to be wrong. You still get full population coverage, but you spend your review budget where it matters most. After human review, you can reweight the samples to estimate population-level WER.

The highest-value sampling strategy: targeted sampling on critical conversation types. In a healthcare voice assistant, you oversample conversations about medications, allergies, and dosages. In a legal assistant, you oversample discussions of dates, names, and financial figures. In a customer support assistant, you oversample conversations where users expressed frustration or confusion. You define the high-stakes subsets where accuracy matters most, and you verify transcription quality on those subsets at much higher rates than general conversations. This ensures that even if your overall WER is acceptable, you know whether it is acceptable on the conversations that carry legal, financial, or safety risk.

The instrumentation pattern: flag conversations for sampling based on confidence, conversation type, user feedback, or random selection. Queue flagged transcriptions for human review. Reviewers listen to the audio and either confirm the transcription is correct or provide the correct transcription. You calculate WER on the reviewed sample and log it alongside the ASR confidence score. Over time, you build a dataset of ASR confidence scores paired with ground-truth accuracy. This lets you calibrate confidence thresholds. Maybe confidence above 0.85 corresponds to 98% accuracy, but confidence between 0.70 and 0.85 corresponds to 88% accuracy. You use this calibration to set smarter thresholds for flagging low-quality transcriptions.

The mistake teams make: they sample transcriptions, measure WER, publish a number, and never act on it. Sampling is not a reporting exercise. It is a feedback loop. If you discover that WER on medical terminology is 15% worse than WER on general conversation, you need to either fine-tune your ASR model on medical vocabulary, add a post-processing step to correct common medical term errors, or escalate low-confidence medical transcriptions to human review before passing them to the LLM. The sample tells you where the model is weak. The action is what improves the system.

## Detecting ASR Drift: When Accuracy Degrades Slowly

ASR drift is what happens when transcription accuracy declines gradually over weeks or months. The model does not change. The benchmark stays the same. But real-world performance degrades because the input distribution shifts. Your users start using the system in noisier environments. A new user demographic with different accents adopts the product. A feature update changes the conversation patterns and introduces vocabulary the ASR was never trained on. You do not notice the drift until user complaints accumulate, and by then you have already lost trust.

Drift detection requires continuous measurement. You cannot compare this month's accuracy to last month's if you only measure accuracy once per quarter. You need a standing process that measures accuracy on a sampled subset of transcriptions every week. You plot WER over time. A flat line means accuracy is stable. An upward trend means accuracy is degrading. A sudden jump means something broke. The trend line is the signal that justifies investigation before the problem becomes visible to users.

The challenge: production WER is noisy. A single week's sample might show 9% WER, the next week 7%, the next 10%, all due to random variation. You need statistical rigor to distinguish real drift from noise. The pattern that works: calculate a rolling average WER over four weeks. Alert when the rolling average increases by more than two percentage points compared to the previous four-week period. This smooths out weekly noise while catching meaningful trends.

Cohort-level drift detection is more sensitive than population-level detection. Maybe overall WER is stable, but WER for users on Android devices increased from 8% to 13%. Maybe WER for users in customer support conversations increased while WER for general inquiries stayed flat. You need to break down WER by user cohort, device type, conversation type, and audio quality indicators. This breakdown surfaces drift in specific subsets before it is large enough to move the population-level metric.

The instrumentation that catches drift: log every sampled transcription's WER alongside metadata about the user, the conversation, the device, and the audio characteristics. Build dashboards that show WER trends by cohort. Set alerts that fire when WER for any cohort increases by more than a threshold over a rolling window. The alert does not tell you why accuracy degraded, but it tells you which subset to investigate. You pull transcriptions from the affected cohort, listen to the audio, and identify patterns. Maybe background noise increased. Maybe a new feature is generating conversation patterns the ASR struggles with. Maybe a recent OS update on Android changed how audio is captured and degraded quality.

The response to drift depends on the cause. If user behavior changed, you might need to fine-tune the ASR model on new data that matches the current distribution. If audio quality degraded, you might need to improve noise suppression in preprocessing. If a specific vocabulary domain is causing errors, you might need to add a custom vocabulary or post-processing rules. Drift detection gives you lead time to address these issues before they escalate into full-blown user satisfaction crises.

## Domain-Specific Quality Tracking: Medical, Legal, Technical

Generic WER measures how often the ASR gets words wrong, but it does not weight errors by severity. In general conversation, mistranscribing "cat" as "hat" is a minor annoyance. In medical conversation, mistranscribing "milligrams" as "grams" is dangerous. In legal conversation, mistranscribing a date or a name can invalidate a contract. In technical support, mistranscribing a product model number breaks the entire interaction. Domain-specific quality tracking measures accuracy on the vocabulary that matters most.

You start by identifying the critical vocabulary for your domain. In healthcare, that is medication names, dosages, anatomical terms, and symptoms. In legal, that is party names, dates, financial figures, and legal terms of art. In technical support, that is product names, model numbers, error codes, and technical jargon. You build a list of terms that appear in your conversations and that carry high stakes. This list does not need to be exhaustive — a few hundred high-frequency, high-stakes terms cover most critical errors.

You instrument a domain-specific WER metric that measures accuracy only on these terms. When a conversation contains a medication name, you sample it for review and measure whether the ASR got the medication name right, ignoring errors on other words. You aggregate these measurements into a domain-specific WER: the percentage of critical terms that are transcribed incorrectly. This number is often much worse than overall WER. A system with 5% general WER might have 15% WER on medication names because those names are rare in training data, phonetically ambiguous, and spoken quickly.

Domain-specific WER gives you a focused quality metric that aligns with user harm. If your general WER improves from 8% to 6% but your medication WER stays at 15%, you have not reduced the risk that matters. If your medication WER drops from 15% to 8%, you made the system safer even if general WER did not change. You set thresholds and alerts on domain-specific WER just like you do for overall WER. An alert fires when medication WER exceeds 10%, even if overall WER is fine.

The pattern that works in production: use named entity recognition or keyword matching to detect when a transcription contains domain-critical terms. Flag those transcriptions for higher sampling rates. Measure WER on the critical terms separately from general WER. Build dashboards that show both metrics over time. Report domain-specific WER to stakeholders — product, legal, compliance — because this is the number that correlates with real-world risk. Use domain-specific WER to prioritize ASR improvements. If medication names are the biggest source of errors, you fine-tune the ASR on medical vocabulary or add a post-processing step that corrects common medication transcription errors using a medical dictionary.

The mistake teams make: they know their domain has critical vocabulary, but they do not instrument it separately. They measure WER on everything and assume that if overall WER is good, domain-specific WER is also good. This assumption fails in practice. ASR models are trained on general data and perform worse on domain-specific terminology. If you do not measure domain accuracy explicitly, you will not know where your system is weak, and you will not prioritize the fixes that reduce real harm.

## Correlating ASR Quality with Downstream Conversation Success

ASR errors do not matter in isolation. They matter when they break the conversation. A transcription error that the LLM can recover from is low-impact. A transcription error that causes the LLM to misunderstand the user's intent, provide the wrong answer, or take the wrong action is high-impact. ASR quality monitoring must connect transcription accuracy to conversation outcomes.

The correlation you need to measure: does WER predict conversation success? You define success metrics — task completion, user satisfaction, session duration, abandonment rate — and you correlate them with ASR confidence scores and sampled WER. If conversations with median ASR confidence above 0.90 have an 85% success rate, and conversations with median confidence below 0.70 have a 50% success rate, you have a clear signal that ASR quality drives outcomes. This justifies investment in ASR improvements and sets the threshold for escalating low-confidence transcriptions to human review.

The instrumentation pattern: tag every conversation with its ASR quality metrics — median confidence, percentage of low-confidence transcriptions, sampled WER if available. Tag every conversation with its outcome — completed, abandoned, user corrected the system, user expressed frustration. Join these datasets and calculate outcome rates by ASR quality quartile. The top quartile of ASR quality should have significantly better outcomes than the bottom quartile. If it does not, either your outcome metrics are wrong, your ASR quality proxies do not correlate with real accuracy, or conversation success is driven by LLM quality rather than ASR quality.

The surprising finding many teams discover: ASR errors are less damaging than expected if the LLM is robust. A transcription that says "I need to book a fight" instead of "I need to book a flight" does not break the conversation if the LLM infers the correct intent from context. A transcription that says "my son is taking 50 grams of ibuprofen" instead of "50 milligrams" does break the conversation because the LLM accepts the transcription literally and provides dangerous guidance. The correlation analysis tells you which types of ASR errors are recoverable and which are catastrophic. You prioritize fixing the catastrophic errors first.

The pattern that justifies ASR investment: show that improving ASR quality from the bottom quartile to the median would increase conversation success by X percent. Translate that into business impact — more revenue, fewer support tickets, higher user retention. Leadership understands business metrics. They do not understand WER. The correlation gives you the translation layer. If reducing WER from 12% to 8% would increase task completion from 65% to 75%, and task completion drives revenue, you can justify the cost of fine-tuning ASR, adding human review, or switching to a better ASR provider.

The mistake teams make: they treat ASR as an isolated component with its own quality metrics, disconnected from the product metrics that leadership cares about. ASR is not the product. The conversation is the product. ASR quality matters only to the extent that it affects conversation quality. If you cannot show that connection, you cannot prioritize ASR improvements rationally. You will either over-invest in ASR because engineers care about WER for its own sake, or under-invest because leadership does not see the link to outcomes. The correlation gives you the evidence to invest correctly.

ASR quality in production is not a number. It is a system of continuous measurement, cohort analysis, drift detection, domain-specific tracking, and outcome correlation. The benchmark WER you measured in testing tells you almost nothing about the accuracy your users experience. The confidence scores you log every day tell you when the model is uncertain but not whether it is wrong. The samples you review tell you ground truth but only for a fraction of conversations. The downstream success metrics tell you whether ASR errors are breaking conversations. Together, these signals form a monitoring system that surfaces ASR problems early, quantifies their impact, and guides you toward the fixes that matter most.

---

*Next: 11.4 — TTS Quality Degradation Detection*
