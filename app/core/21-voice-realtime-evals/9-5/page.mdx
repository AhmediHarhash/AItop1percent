# 9.5 — Audio Codec Selection: Quality vs Bandwidth

The codec you choose determines what the user hears, what your ASR system receives, and how much network bandwidth you consume. In early 2025, a telehealth platform discovered this the hard way when they selected a codec optimized for music streaming to transmit doctor-patient conversations. The codec produced pristine audio quality for human listening but introduced frequency transformations that confused their medical speech recognition system. Transcription accuracy on medication names dropped from 94% to 81%. The codec was technically excellent — it was just the wrong codec for the job. The team spent six weeks rebuilding their entire audio pipeline around a codec designed for voice, not music. The lesson was expensive: codec selection is not about picking the highest quality option. It is about matching the codec's characteristics to your pipeline's requirements.

## The Codec Determines Three Critical Constraints

Every codec makes tradeoffs between audio fidelity, bandwidth consumption, and computational complexity. A codec optimized for high-fidelity music reproduction might preserve frequencies above 8 kHz that are irrelevant for speech recognition but consume three times the bandwidth of a voice-optimized codec. A codec with aggressive compression might reduce bandwidth to 16 kbps but introduce artifacts that degrade ASR accuracy. A codec with minimal latency might sacrifice some quality to avoid buffering delays.

Your choice of codec ripples through every component in your voice pipeline. The ASR provider you selected might not support the codec you prefer. The TTS system might output audio in a format that requires transcoding before streaming to clients. The network conditions your users experience might make a low-bandwidth codec essential even if it sacrifices some quality. The codec is not an isolated technical decision — it is the interface contract between every component in your system.

The most common mistake is selecting a codec based on audio quality alone, measured by listening tests or frequency response charts, without considering how that codec performs when fed into your specific ASR model or when transmitted over your users' actual network conditions. Codec selection requires testing the entire pipeline, not just the codec in isolation.

## Opus: The Dominant Voice Codec and Why

By 2026, Opus has become the default codec for real-time voice applications. It is an open standard designed specifically for interactive audio, optimized for low latency, high quality, and network adaptability. Opus can operate at bitrates from 6 kbps to 510 kbps, adapting in real time based on network conditions. It handles both speech and music well, though its sweet spot is conversational voice. It introduces minimal latency — typically 20 to 40 milliseconds of algorithmic delay — which makes it suitable for real-time interaction.

Opus is widely supported. All major browsers support Opus over WebRTC. Most ASR providers accept Opus-encoded audio, either natively or with automatic transcoding. Most TTS providers can output Opus-encoded streams. The infrastructure ecosystem has standardized around it, which means you spend less time debugging codec compatibility issues and more time building features.

The quality-to-bandwidth ratio is exceptional. At 24 kbps, Opus produces voice quality that most listeners perceive as indistinguishable from uncompressed audio in typical conversational contexts. At 16 kbps, it still delivers acceptable quality for telephony-grade applications. At 12 kbps, it degrades gracefully, prioritizing intelligibility over fidelity. This adaptability makes Opus suitable for users on strong WiFi connections and users on congested mobile networks within the same application.

Opus also includes forward error correction. When packet loss occurs, Opus can use redundant data encoded in subsequent packets to reconstruct lost frames, reducing the perceptual impact of network issues. This resilience is critical for voice applications where a single dropped packet can create audible gaps or pops.

The main alternative to Opus in 2026 is legacy codec support. Some enterprise telephony systems still use G.711 or G.729 for compatibility with older infrastructure. Some mobile carriers use AMR-WB for cellular voice. If your system must interoperate with these environments, you need transcoding infrastructure. But for new systems built in 2026, starting with Opus is the correct default.

## PCM vs Compressed: Quality vs Bandwidth Tradeoffs

Pulse Code Modulation, or PCM, is uncompressed audio. It samples the audio waveform at a fixed rate — typically 16,000 or 48,000 times per second — and encodes each sample as a 16-bit integer. PCM is simple, introduces no algorithmic latency, and preserves the original signal exactly. It is also bandwidth-intensive. Uncompressed 16 kHz mono audio at 16 bits per sample consumes 256 kbps. At 48 kHz, it consumes 768 kbps.

For local processing or offline batch transcription, PCM is often the right choice. There is no decoding overhead, no compression artifacts, and no compatibility issues — every audio library can read PCM. For real-time streaming over networks, PCM is rarely practical. The bandwidth requirement is too high for most mobile and residential internet connections, especially when transmitting bidirectional audio.

Compressed codecs like Opus reduce bandwidth by exploiting redundancy and perceptual irrelevance in the audio signal. They discard frequencies humans cannot hear, use predictive coding to avoid retransmitting stable signal components, and apply lossy quantization to reduce bit depth where precision is not perceptually important. The result is audio that sounds nearly identical to the original but consumes one-tenth to one-twentieth the bandwidth.

The cost of compression is complexity and latency. Encoding and decoding Opus audio requires CPU cycles — not many on modern processors, but enough to matter at scale when you are processing thousands of concurrent streams. Compression also introduces algorithmic latency as the codec buffers frames for analysis before encoding. For Opus, this latency is minimal. For older codecs like MP3, designed for file storage rather than real-time interaction, the latency can be hundreds of milliseconds.

The decision between PCM and compressed audio comes down to where bandwidth is scarce and where it is plentiful. If you are transmitting audio between microservices within the same data center, PCM is simpler. If you are transmitting audio from a mobile device over a cellular connection, compression is essential. If you are storing audio for offline analysis, PCM avoids generation loss from repeated encoding and decoding. If you are streaming audio to end users, compression reduces infrastructure costs and improves user experience on constrained networks.

## Sample Rate Implications: 8 kHz, 16 kHz, 48 kHz

The sample rate determines the highest frequency the codec can represent. By the Nyquist theorem, a sample rate of N Hz can represent frequencies up to N divided by 2 Hz. An 8 kHz sample rate captures frequencies up to 4 kHz. A 16 kHz sample rate captures up to 8 kHz. A 48 kHz sample rate captures up to 24 kHz.

Human speech contains most of its intelligibility in frequencies below 4 kHz. Vowels and voiced consonants sit between 100 Hz and 2 kHz. Unvoiced consonants like S and F extend up to 8 kHz. Frequencies above 8 kHz contribute to naturalness and fidelity but are not essential for understanding what someone said. This is why telephony systems have historically used 8 kHz sampling — it is the minimum rate that preserves intelligibility for conversation.

Modern ASR systems are trained on 16 kHz audio. They expect to see frequencies up to 8 kHz, which improves accuracy on sibilants and fricatives that carry phonetic information. If you feed an ASR system 8 kHz audio when it was trained on 16 kHz, accuracy degrades by 5% to 15% depending on the vocabulary and acoustic conditions. If you feed it 48 kHz audio, most ASR systems downsample to 16 kHz internally, wasting bandwidth and CPU cycles on frequencies the model ignores.

For voice applications in 2026, 16 kHz is the standard sample rate. It balances ASR accuracy, perceptual quality, and bandwidth efficiency. At 16 kHz with Opus encoding at 24 kbps, you get high-quality voice that ASR systems handle well and that sounds natural to human listeners.

There are exceptions. If you are building a music application or a high-fidelity podcast recording tool, 48 kHz is appropriate. The extra bandwidth preserves audio quality that listeners care about. If you are integrating with legacy telephony infrastructure that only supports 8 kHz, you accept the quality tradeoff and build your ASR pipeline around narrowband models or upsample the audio before transcription, accepting the accuracy penalty.

The sample rate also affects latency indirectly. Higher sample rates mean larger frame sizes for the same time duration, which can increase buffering delays if not handled carefully. At 16 kHz with 20-millisecond frames, each frame contains 320 samples. At 48 kHz with 20-millisecond frames, each frame contains 960 samples. The encoding and decoding time scales with frame size, though the difference is typically small — a few milliseconds at most.

## Codec Switching During Calls Based on Network Conditions

Network conditions change during a call. A user starts on WiFi, then walks outside and switches to LTE. A user joins a call from a café with strong signal, then moves to a subway platform where bandwidth drops and latency spikes. A user shares a residential internet connection with someone who starts a video download mid-call. Your codec must adapt to these changes without dropping the call or forcing the user to reconnect.

Opus supports bitrate adaptation in real time. The encoder can switch from 64 kbps high-quality mode to 16 kbps narrowband mode within a single stream, adjusting frame by frame based on network feedback. This adaptation happens automatically when the codec receives bandwidth estimates from the transport layer. If the network reports increasing packet loss or rising latency, Opus reduces bitrate to fit within available bandwidth. If conditions improve, it scales back up.

The challenge is detecting network degradation fast enough to adapt before the user experiences glitches. WebRTC tracks metrics like round-trip time, packet loss rate, and jitter. When these metrics cross thresholds — round-trip time above 300 milliseconds, packet loss above 5%, jitter variance exceeding 30 milliseconds — the codec should reduce bitrate. The adaptation must be gradual to avoid perceptual artifacts. Dropping from 64 kbps to 16 kbps in a single frame creates an audible quality shift that sounds like a glitch. Stepping down over five frames — 64, 48, 32, 24, 16 — produces a smoother degradation.

Codec switching also requires coordination with the ASR and TTS components. If the codec switches from wideband to narrowband mode, the ASR system might need to switch models or adjust confidence thresholds. If the TTS output was generated at 48 kHz and the network can only sustain 16 kbps, the server must transcode the output in real time. This transcoding adds latency and CPU cost, which is why some systems pre-generate TTS at multiple bitrates and switch between them without transcoding.

In 2025, a customer service platform built adaptive codec switching but forgot to test the transition edge cases. When network conditions oscillated — degrading, then recovering, then degrading again — the codec switched bitrates every few seconds, creating a rollercoaster of audio quality that users described as worse than staying at low quality the entire time. The fix was to add hysteresis: once the codec drops to a lower bitrate, it requires stable good conditions for at least ten seconds before switching back up. The perceptual result was much smoother.

## ASR Compatibility: Not All Codecs Work with All Providers

Most ASR providers accept a limited set of codecs. The major providers in 2026 — OpenAI, Anthropic, Google, Deepgram, AssemblyAI — support Opus, FLAC, WAV, and sometimes MP3 or AAC. They typically do not support legacy telephony codecs like G.729 or AMR-NB without explicit transcoding on your end. If your audio pipeline delivers a codec the ASR provider does not recognize, the API returns an error or, worse, accepts the audio and produces garbage transcriptions without warning.

The codec must also match the ASR model's training distribution. If the model was trained on 16 kHz audio and you send 8 kHz narrowband audio, accuracy degrades. If the model expects Opus and you send MP3, the compression artifacts differ in ways that increase the word error rate. If the model was trained on raw PCM and you send heavily compressed Opus at 12 kbps, sibilants and fricatives degrade, increasing errors on words like "six," "seventy," and "prescription."

You discover codec compatibility issues during integration testing, ideally. You discover them in production when a user on a specific network or device encodes audio in a format your ASR provider does not handle, and the transcriptions fail silently. The mitigation is to test every codec and sample rate combination your users might send. If your web client uses Opus at 24 kbps and your mobile app uses AAC at 32 kbps because iOS defaults to AAC, you must verify that both work with your ASR provider. If one does not, you add server-side transcoding, which adds latency and cost but prevents silent failures.

Codec negotiation happens during call setup. The client advertises the codecs it supports. The server responds with the codec it prefers from that list. If there is no overlap, the call fails to establish. The negotiation logic must prioritize codecs based on ASR compatibility, bandwidth efficiency, and quality, in that order. If the client supports Opus and G.711, choose Opus. If the client only supports G.711, accept it but flag the call for lower ASR confidence thresholds.

Some systems run into trouble when they upgrade their ASR provider and the new provider supports a different codec set than the old one. A 2025 fintech company switched from a legacy ASR vendor to a modern API-based provider and discovered that their phone-based voice flows, which used G.729 encoding, were no longer supported. They had to build a transcoding layer to convert G.729 to Opus on the server before sending to the new ASR provider. The transcoding added 35 milliseconds of latency and required deploying new infrastructure, all because they did not verify codec compatibility before migration.

## Measuring Codec Impact on ASR Accuracy

Codec choice affects ASR accuracy, but the impact is not always obvious. A codec that sounds fine to human ears might degrade transcription quality in subtle ways. The only reliable way to measure codec impact is to run your evaluation dataset through your ASR pipeline with different codec configurations and compare word error rates.

Start with your baseline: uncompressed 16 kHz PCM audio. Measure ASR accuracy across your test set. Then encode the same audio with Opus at 64 kbps, 32 kbps, 24 kbps, 16 kbps, and 12 kbps. Measure ASR accuracy at each bitrate. The difference between PCM and Opus at 64 kbps should be negligible — typically under 0.5% relative WER increase. At 24 kbps, the increase should be under 2%. At 12 kbps, expect 5% to 10% degradation, concentrated on low-volume speech and noisy environments.

Now repeat the test with other codecs: AAC, MP3, G.711. Compare the results. In most cases, Opus at 24 kbps outperforms MP3 at 64 kbps for ASR accuracy, even though MP3 consumes more bandwidth. The reason is that Opus is optimized for speech, preserving the frequency bands and temporal characteristics that ASR models rely on, while MP3 applies perceptual masking that can obscure phonetic cues.

The test must include realistic noise conditions. Record or synthesize audio with background noise at different signal-to-noise ratios — 20 dB, 10 dB, 5 dB. Codec behavior under noise is different than in clean conditions. Some codecs suppress background noise aggressively, which helps perceptual quality but removes acoustic context that ASR models use. Other codecs preserve noise, which hurts perceptual quality but sometimes improves ASR accuracy by maintaining the full acoustic scene.

You also need to test edge cases: very quiet speech, very loud speech, speech with accents, speech with technical jargon, speech with overlapping speakers. Codec artifacts manifest differently across these conditions. A codec that handles normal conversational volume well might clip loud speech or suppress quiet speech below the noise floor.

The output of this testing is a codec compatibility matrix: which codecs work well with your ASR provider, at what bitrates, under what conditions. This matrix informs your codec negotiation logic and your quality degradation strategy when network conditions force you to lower bitrates.

## Bandwidth Planning for Concurrent Streams

Every concurrent voice stream consumes bandwidth on your server infrastructure. If you are running a relay server that bridges WebRTC clients to WebSocket-based ASR and TTS providers, each connected user generates bidirectional audio streams. If the average call uses Opus at 32 kbps for incoming audio and 32 kbps for outgoing audio, each user consumes 64 kbps of sustained bandwidth.

At 1,000 concurrent users, that is 64 Mbps. At 10,000 users, 640 Mbps. At 100,000 users, 6.4 Gbps. The bandwidth scales linearly with concurrency, and it is sustained, not bursty — voice streams run continuously for the duration of the call.

Cloud providers charge for egress bandwidth. AWS charges $0.09 per GB for data transferred out of us-east-1 to the internet. A single user on a ten-minute call at 64 kbps uploads and downloads roughly 4.8 MB of audio. If your relay server is in AWS and sends that audio to an external ASR provider, you pay egress charges on the upload. If the TTS provider sends audio back through your relay to the client, you pay egress charges again. At scale, bandwidth costs can exceed compute costs.

The mitigation is to colocate your relay servers with your ASR and TTS providers when possible, or to use providers that peer with your cloud network to avoid egress charges. Some ASR providers offer regional endpoints that sit within the same cloud region as your infrastructure, reducing both latency and bandwidth costs. Some TTS providers support WebSocket connections that stay within the cloud provider's internal network.

You also optimize codec bitrate. If your ASR provider achieves acceptable accuracy at Opus 24 kbps instead of 32 kbps, switching saves 25% bandwidth. At 100,000 concurrent users, that is a 1.6 Gbps reduction. If egress costs $0.09 per GB, the monthly savings at 50% duty cycle and 30 days is $21,000. Codec tuning is not just a quality decision — it is a cost decision.

Bandwidth planning also requires headroom for peak load. If your average concurrency is 10,000 users but your peak is 50,000 during product launches or outages, your infrastructure must sustain 3.2 Gbps during those peaks. If your network links are provisioned for 2 Gbps, calls fail. The headroom requirement depends on your traffic patterns and your tolerance for degraded quality during peaks. Some systems reduce codec bitrates automatically when approaching bandwidth limits, sacrificing quality to maintain availability.

---

The codec you choose is not a one-time decision made during initial development. It is a dynamic configuration that adapts to network conditions, user devices, ASR provider capabilities, and cost constraints. Teams that treat codec selection as a fixed architectural choice discover limitations when they try to scale, when they switch providers, or when their users operate in environments the original codec was not designed for.

Next, we examine the infrastructure pattern that bridges browser-based WebRTC clients to server-side ASR and TTS pipelines: the relay server.
