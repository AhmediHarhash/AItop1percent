# 12.14 â€” Building the Enterprise Voice AI Team

Most AI engineering teams are structured for text: machine learning engineers who fine-tune models, backend engineers who build APIs, product managers who define features, and data scientists who analyze performance. This team can build a competent text-based AI system. It cannot build a production-grade voice system. Voice introduces requirements that text-based teams are not equipped to handle: real-time audio processing, conversational flow design, speech-specific failure modes, telephony integration, and acoustic optimization. A financial services company learned this the hard way in mid-2025. They assigned their existing AI team to build a voice agent for customer support. The team had built successful text-based chatbots for the same use case. They assumed voice was a straightforward extension: connect a speech-to-text API to the existing chatbot logic and add a text-to-speech layer for responses. The first prototype worked in the demo. It failed in production. Users complained that the agent talked over them, paused in unnatural places, misunderstood common phrases, and sounded robotic. The team spent four months debugging issues they did not have vocabulary to describe. The turning point came when they hired a conversational designer with a background in interactive voice response systems. Within two weeks, she identified problems the engineering team had been calling "model issues" but were actually "conversation design issues." The lesson was not that the engineers were incompetent. It was that voice AI requires specialists that text AI does not.

## The Roles: What Voice AI Needs

The voice engineer is responsible for the audio pipeline: WebSocket connections, audio format handling, acoustic echo cancellation, noise suppression, and latency optimization. This is not a role that a backend engineer can absorb as a side responsibility. Voice engineering requires expertise in signal processing, real-time systems, and audio codecs. The voice engineer chooses between Opus and AMR-WB for audio compression, tunes jitter buffers to balance latency and audio quality, and debugs why certain network conditions cause audio dropouts. They work at the boundary between the application layer and the physical layer, which most software engineers never touch.

The conversational designer owns the dialogue flow, turn-taking logic, acknowledgment strategies, and personality design. Conversational design is not UX design adapted for voice. It is a distinct discipline with its own principles. A conversational designer thinks in terms of conversational repair, grounding, and mutual understanding. They design how the agent acknowledges user input, how it handles interruptions, how it signals that it is thinking, and how it recovers from misunderstandings. They write sample dialogues for every user intent and test them with real users to identify where the flow breaks. Without a conversational designer, voice agents sound like text chatbots read aloud: grammatically correct but conversationally unnatural.

The speech scientist brings expertise in phonetics, prosody, and acoustic modeling. They are responsible for understanding why the agent fails to recognize certain accents, why it generates synthetic speech with unnatural stress patterns, and why certain words are consistently misrecognized. Speech scientists analyze phoneme-level transcription errors, tune language models for domain-specific vocabulary, and evaluate trade-offs between different speech recognition engines. This role is critical for voice systems that serve global audiences with diverse accents or specialized vocabularies that general-purpose speech models handle poorly.

The compliance specialist for voice AI is not the same as the compliance specialist for text AI. Voice introduces regulatory requirements around call recording, consent, biometric data, and telecommunications regulations. The compliance specialist ensures that the system complies with laws like the Telephone Consumer Protection Act in the US, GDPR's rules on voice data processing in the EU, and industry-specific regulations like HIPAA for healthcare or PCI-DSS for payment card data. They define data retention policies for call recordings, consent workflows for biometric voice authentication, and disclosure requirements for AI-generated voices. They are the person who prevents the company from accidentally violating wiretapping laws by recording calls without proper consent notifications.

The platform engineer for voice systems builds the infrastructure that the voice engineer, conversational designer, and speech scientist depend on. They manage the Kubernetes clusters that run real-time inference, the CDN that delivers audio with low latency, the monitoring systems that track session quality, and the disaster recovery infrastructure that fails over when a region goes down. This role overlaps with traditional DevOps or SRE, but it requires specialization in real-time systems. A platform engineer who has only worked with request-response APIs will struggle with the operational challenges of stateful, long-lived WebSocket connections.

## Team Structure for Different Company Sizes

A startup building its first voice agent cannot hire five specialists. The minimum viable team is three people: one engineer who handles both voice engineering and platform infrastructure, one person who combines conversational design with product management, and one compliance-aware advisor who can be a contractor rather than a full-time hire. This team can build a voice agent that works for a narrow use case with a defined user base. It cannot build a platform-grade voice system that serves diverse use cases and scales to millions of sessions.

A mid-sized company with voice as a core product feature should build a dedicated voice team of six to ten people. This includes two voice engineers who split responsibilities between real-time audio and telephony integration, two conversational designers who own different parts of the user experience, one speech scientist who focuses on recognition and synthesis quality, one compliance specialist who works across legal and product, and two to three platform engineers who manage infrastructure. At this scale, the voice team is distinct from the text-based AI team, but they share resources like model training infrastructure, evaluation tooling, and data pipelines.

An enterprise with voice AI as a strategic platform investment needs a team of twenty to fifty people organized into specialized sub-teams. One sub-team focuses on audio engineering and real-time infrastructure. Another focuses on conversation design and user experience. A third focuses on speech quality and model adaptation. A fourth focuses on compliance, governance, and security. A fifth focuses on platform operations and reliability. Each sub-team has a technical lead and reports into a director or VP responsible for the voice platform as a whole. At this scale, voice AI is not a feature. It is a product line with its own budget, roadmap, and engineering organization.

The mistake mid-sized companies make is trying to operate like a startup for too long. They assign voice responsibilities to existing team members who treat voice as a part-time project. This works for a pilot but collapses under production load. The first major incident reveals that no one has deep expertise in any of the critical domains, and the team spends weeks learning skills they should have hired for. The correction is often overcorrection: hiring a dozen specialists at once without clear role definitions or integration into the existing team. The right approach is gradual specialization. Hire the voice engineer first, then the conversational designer, then the speech scientist, growing the team as the product matures from pilot to platform.

## Internal vs. Outsourced Capabilities

Voice AI teams face build-versus-buy decisions at every layer: build a custom voice pipeline or use a managed platform, build conversational design in-house or hire an agency, train custom speech models or use vendor APIs. The decision depends on differentiation requirements and control needs. If voice is a core product differentiator, building in-house provides control over quality, latency, and cost. If voice is a supporting feature that must work reliably but does not define the product, outsourcing to a managed platform reduces operational burden.

The most commonly outsourced capability is speech recognition and synthesis. Vendor APIs from Google, Microsoft, Amazon, and specialized providers like Deepgram and AssemblyAI provide production-grade quality without requiring in-house speech science expertise. The trade-off is vendor lock-in and limited control over model behavior. A team that uses a vendor's speech API cannot tune the model for their specific accent distribution, vocabulary, or acoustic environment. They depend on the vendor's roadmap for improvements and must tolerate any quality regressions the vendor introduces.

Conversational design is harder to outsource because it requires deep product knowledge. An external agency can provide frameworks, best practices, and sample dialogues, but they cannot design the conversation flow for a product they do not intimately understand. The most successful outsourcing pattern is hybrid: hire an agency to train the internal team in conversational design principles, provide initial dialogue templates, and conduct user testing, but keep ongoing design and iteration in-house. This builds internal capability while leveraging external expertise to accelerate the learning curve.

Compliance is almost always a combination of internal and external. The internal compliance team defines requirements based on the company's regulatory obligations and risk tolerance. External legal counsel provides guidance on interpretation of telecommunications law, data privacy regulations, and industry-specific rules. External auditors validate that the implementation meets the requirements. The mistake is assuming that compliance can be fully outsourced. External advisors can tell you what the law requires. They cannot tell you how to implement it in your specific architecture. That requires an internal compliance specialist who understands both the regulations and the technical system.

## Career Paths in Voice AI

Voice AI is still a young field, and career paths are not as well-defined as in mature engineering disciplines. A software engineer who wants to specialize in voice can follow multiple trajectories. One path is voice platform engineering: deep expertise in real-time systems, audio processing, and infrastructure for stateful workloads. This path leads to senior and principal engineer roles focused on building and scaling the core voice platform. Another path is applied speech science: understanding acoustic models, phonetic transcription, and prosody generation. This path leads to research scientist or machine learning specialist roles focused on improving recognition and synthesis quality.

Conversational designers often come from backgrounds in UX design, technical writing, or linguistics. They transition into voice by learning how conversation differs from visual interfaces and how to design for the constraints of audio-only interaction. The career path leads to senior conversational designer roles and eventually to leadership positions overseeing the entire user experience for voice products. Conversational design is one of the fastest-growing specializations in AI because every company building voice agents discovers they need this expertise.

The challenge for individuals building a career in voice AI is that many companies do not yet have defined voice roles. A conversational designer applying to a company that has never heard the term "conversational design" must explain the role and convince the hiring manager that it is distinct from UX design or copywriting. A voice engineer applying to a company that treats voice as a feature rather than a platform must advocate for the technical depth required to do the job well. Early-career professionals entering voice AI often need to educate their employers about what the role requires.

For companies, the career path challenge is retention. Voice specialists are in high demand and short supply. A conversational designer with three years of experience can command significant salary premiums because so few people have that expertise. Companies that invest in training someone in voice AI must create career progression that keeps them engaged. This means defining senior and principal levels for voice roles, providing opportunities to work on increasingly complex problems, and ensuring that voice specialists are not trapped in junior roles with no path to leadership.

## Cross-Functional Collaboration: Product, Engineering, Compliance, and Support

Voice AI teams cannot operate in isolation. They must collaborate with product teams who define use cases, engineering teams who maintain upstream and downstream systems, compliance teams who define regulatory boundaries, and support teams who handle escalations. The collaboration model determines whether the voice team is a bottleneck or a force multiplier.

The most common collaboration failure is treating the voice team as a service organization that executes requests from product. Product defines a new use case, specifies the requirements in a document, and hands it to the voice team to implement. The voice team builds what was specified, launches it, and discovers that the requirements missed critical details about conversational flow, error handling, or user expectations. The back-and-forth consumes weeks. The better model is embedded collaboration: the voice team participates in product planning from the start. When a new use case is being considered, the conversational designer attends the planning meetings, sketches dialogue flows, and identifies technical constraints before the requirements are finalized.

Engineering collaboration requires clear interface boundaries. The voice team owns the audio pipeline, speech processing, and conversation management. The backend engineering team owns business logic, data access, and integrations with other systems. The boundary is the API between the voice agent and the backend services. This boundary must be designed collaboratively. If the voice team designs the API without input from backend, it will include requirements that are expensive or impossible for backend to implement. If backend designs the API without input from voice, it will produce responses that are difficult to convert into natural speech or require multiple follow-up turns that degrade the conversational experience.

Compliance collaboration is continuous, not one-time. The compliance team does not hand the voice team a checklist of requirements and walk away. They participate in architecture reviews, test new features before launch, and review incident post-mortems to ensure compliance obligations were met during failures. The voice team, in turn, keeps compliance informed of technical decisions that affect regulatory posture: changes to data retention, new third-party vendors, or features that process new types of user data.

Support collaboration closes the feedback loop. When users escalate issues from the voice agent to human support, the support team sees failure modes that the engineering team does not. A support ticket that says "the voice agent hung up on me" is a signal that timeout logic is too aggressive or that error handling is non-existent. The voice team must review support tickets regularly, classify failure patterns, and prioritize fixes based on user impact. Without this feedback loop, the team optimizes for metrics like latency and accuracy while missing the user experience failures that drive dissatisfaction.

## Hiring for Voice AI: What to Look For

Hiring for voice AI roles is difficult because the candidate pool is small and the required skills are not taught in standard computer science or UX programs. A job posting for a "voice engineer" receives fewer applicants than a posting for a "backend engineer," and many applicants do not have direct voice experience. The hiring strategy must balance finding candidates with exact experience and finding candidates with transferable skills who can learn voice-specific expertise.

For voice engineers, the transferable skills are real-time systems experience and low-level programming. A candidate who has worked on video streaming, online gaming, or financial trading systems understands the constraints of real-time processing even if they have not worked with audio. They can learn audio codecs and speech APIs. A candidate whose only experience is building REST APIs will struggle with the conceptual shift to stateful, latency-sensitive systems. The interview should assess understanding of concurrency, network protocols, and performance optimization, not just familiarity with specific voice technologies.

For conversational designers, the transferable skills are dialogue writing and user research. A candidate who has written interactive fiction, designed chatbot conversations, or conducted user interviews understands how people communicate and how to design for interaction flows. They can learn the constraints of voice. A candidate whose only experience is visual UI design will struggle to think in terms of turn-taking, interruption, and ambiguity resolution. The interview should include a design exercise: given a use case, design a sample conversation and explain the reasoning behind each turn.

For speech scientists, there is less room for transferable skills because the domain knowledge is specific. A candidate needs formal training in phonetics, acoustics, or speech processing. The practical experience may come from academia, industry, or personal projects, but the theoretical foundation is not something someone can learn on the job in a few months. The interview should assess both theoretical knowledge and practical problem-solving: given a speech recognition error pattern, how would you diagnose the root cause and propose a fix?

## Building the Voice AI Team Before You Need It

The most expensive hiring mistake is waiting until the voice product is in crisis to build the team. A company launches a voice agent with a generalist team, discovers quality issues in production, and then scrambles to hire a conversational designer or speech scientist. By this point, users have experienced the poor quality, executives are questioning the investment, and the new hire inherits a system that needs major redesign under time pressure. The team should be built before the product reaches production, not after.

The right hiring sequence is: voice engineer first, conversational designer second, platform engineer third, speech scientist fourth, compliance specialist fifth. The voice engineer builds the prototype and proves the technical feasibility. The conversational designer shapes the user experience before it becomes locked into code. The platform engineer ensures the system can scale beyond the prototype. The speech scientist tunes quality for production traffic. The compliance specialist ensures the system meets regulatory requirements before launch. Hiring in this order builds capability in sync with product maturity.

For companies where voice is strategic, the team-building timeline should start twelve to eighteen months before the planned production launch. This allows time to hire specialists, train them on the company's domain and user base, design and iterate on the conversational experience, and run pilots that surface issues before production. The teams that try to compress this timeline into three to six months ship voice agents that work but do not delight users. They spend the next year fixing problems that could have been avoided with earlier investment in the team.

---

Building the enterprise voice AI team is not about hiring more engineers. It is about hiring different engineers with different expertise. The teams that recognize voice as a distinct discipline with its own roles, career paths, and collaboration models ship voice systems that meet production quality and reliability standards. The teams that treat voice as a feature that any AI engineer can build ship systems that work in demos and fail in production.

The next subchapter addresses the voice AI maturity model, the progression from pilot to platform and the capabilities required at each level.

