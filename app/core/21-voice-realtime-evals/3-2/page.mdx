# 3.2 — Time to First Audio: The Metric That Matters Most

Time to first audio is the only latency metric your users directly perceive. They do not know how long your ASR took. They do not care whether your LLM generated the first token in 150ms or 300ms. They do not measure your TTS synthesis speed. What they feel — viscerally, emotionally, conversationally — is the gap between when they stop speaking and when they hear the first syllable of your response. That gap is time to first audio. It is the metric that determines whether your voice AI feels like a conversation or feels like waiting for a machine.

Every other latency metric is a proxy. Component-level measurements matter only insofar as they contribute to TTFA. End-to-end latency matters only because it correlates with TTFA. When teams optimize the wrong metrics — ASR latency, LLM time-to-first-token, TTS synthesis speed — they often make TTFA worse because they ignore the gaps between components, the network hops, the buffering strategies that delay audio playback even after synthesis completes. Optimizing TTFA requires thinking end-to-end, user-to-user, from the moment the user's voice stops to the moment sound reaches their ears.

## What TTFA Includes and What It Does Not

Time to first audio starts when the user finishes speaking. Not when they start speaking — when they finish. The clock begins the moment the ASR system detects end-of-speech, closes the audio stream, and begins processing the final utterance. It ends when the first chunk of synthesized audio reaches the user's device and begins playback. Everything in between counts: ASR transcription, LLM prompt construction, LLM inference to first token, text-to-speech synthesis, audio transmission, and client-side buffering before playback.

TTFA does not include the time the user spent speaking. If the user takes four seconds to ask a question, those four seconds are not part of TTFA. TTFA does not include the time the system takes to generate the rest of the response after the first audio chunk plays. If the system starts playing audio at 400ms but takes another three seconds to finish synthesizing the full response, only the 400ms counts toward TTFA. The remaining synthesis time affects other metrics — like audio continuity or time-to-full-response — but not TTFA.

This distinction matters because teams sometimes conflate TTFA with total response time. A system can have excellent TTFA and poor total response time if it starts playing audio quickly but synthesizes slowly, leading to choppy playback or long pauses mid-response. Conversely, a system can have poor TTFA but smooth total response time if it delays starting playback but then streams audio continuously. Both matter. But TTFA is the metric that determines first impression, conversational rhythm, and whether the user feels heard.

## Why TTFA Matters More Than Component Latencies

A team can have the fastest ASR in the industry and the worst TTFA in production. This happens when the system waits to accumulate multiple ASR results before finalizing the transcript, adding 100 to 200 milliseconds of buffering delay that the ASR provider never reports. It happens when the orchestration layer waits for a database write to complete before sending the transcript to the LLM, adding 80ms of unnecessary serialization. It happens when the TTS system synthesizes the full response before sending any audio, instead of streaming chunks as they are generated.

Component latency tells you how fast individual pieces run in isolation. TTFA tells you how fast the system feels to a human. The gap between these two measures is where most latency problems hide. A 90ms ASR paired with a 180ms LLM paired with a 70ms TTS should theoretically deliver 340ms TTFA. In practice, teams often measure 550 to 700ms TTFA because of inter-component delays they never instrumented. Network round trips add 40ms here, 30ms there. Serialization and deserialization add 15ms per hop. Buffering logic adds another 50ms. Client-side audio initialization adds 60ms. These invisible gaps compound into 200+ milliseconds of latency that does not show up in any component dashboard.

Optimizing TTFA requires tracing the full path, not just the major components. You must measure the time between when the ASR emits a transcript and when the LLM receives it. You must measure the time between when the LLM generates the first token and when the TTS begins synthesis. You must measure the time between when the TTS produces the first audio chunk and when that chunk reaches the client. These gaps are often larger than the component processing times themselves.

## Measuring TTFA in Production

Accurate TTFA measurement requires end-to-end instrumentation. Generate a unique trace ID for every conversational turn. Emit a timestamp when the user stops speaking and the ASR stream closes. Emit a timestamp when the first audio byte is delivered to the client and playback begins. The difference is your TTFA for that turn. Aggregate these measurements across all users, all sessions, all geographies, and all times of day to understand your distribution.

Do not rely on server-side estimates. Server-side measurement can tell you when audio was sent to the client, but it cannot tell you when playback actually began. Client-side buffering, network jitter, and device playback latency add 30 to 100 milliseconds of delay that server logs never capture. If you measure TTFA purely server-side, you are underestimating by at least 50 milliseconds and sometimes by 150 milliseconds or more.

Client-side measurement requires cooperation from your client application. The client must log the timestamp when the user's speech ends, send that timestamp to the server with the audio, and log the timestamp when audio playback begins. The server joins these events using the trace ID and calculates TTFA. This is more complex than server-only logging, but it is the only way to measure what the user actually experiences.

Segment TTFA by context. A question that requires complex reasoning will have higher TTFA than a simple command. A user in a low-bandwidth region will have higher TTFA than one on fiber. A session during peak traffic will have higher TTFA than one during off-peak hours. Measuring aggregate TTFA without segmentation hides these patterns. You need to know not just that your P95 TTFA is 780ms, but that it is 620ms for simple queries and 1.1 seconds for complex ones, or that it is 510ms in North America and 950ms in Southeast Asia.

## Where TTFA Delays Hide

The most common TTFA bottleneck is not the LLM. It is waiting for the ASR to decide the user is done speaking. End-of-speech detection is probabilistic. Most ASR systems wait for a silence threshold — typically 400 to 800 milliseconds of quiet audio — before finalizing the transcript. If your threshold is 700ms, you are adding 700ms to TTFA before any processing begins. Shortening the threshold to 300ms cuts TTFA dramatically but increases the risk of cutting off users mid-sentence. This tradeoff is fundamental and unresolvable through infrastructure optimization alone.

The second most common bottleneck is LLM time-to-first-token. Even if your LLM generates the full response in 250ms, the time to produce the first token might be 180ms. TTS cannot begin until it has text to synthesize. If the LLM batches multiple tokens before emitting anything, you wait for that batching delay. Streaming LLMs that emit tokens immediately as they are generated cut TTFA by 50 to 150 milliseconds compared to batched output, even when total generation time is identical.

The third bottleneck is TTS buffering. Many TTS systems accumulate a minimum amount of text — one sentence, or 50 characters — before beginning synthesis. This reduces the number of synthesis requests and improves audio quality by giving the prosody model more context, but it delays the first audio chunk. A TTS system that waits for a full sentence before synthesizing might add 200 to 400 milliseconds to TTFA compared to one that begins synthesis immediately upon receiving the first few words.

The fourth bottleneck is network topology. If your ASR runs in US-East, your LLM runs in US-West, and your TTS runs in EU-Central, every request crosses multiple regions, adding 80 to 150 milliseconds per hop. Co-locating all components in the same region or same availability zone cuts these hops to single-digit milliseconds. For latency-sensitive applications, regional co-location is not optional.

## Optimizing TTFA Without Sacrificing Quality

The fastest way to improve TTFA is to reduce end-of-speech detection latency. If your ASR waits 700ms of silence before finalizing, drop it to 400ms. You will cut 300ms from TTFA. The risk: users who pause mid-sentence will be cut off, and the ASR will finalize prematurely. Mitigation: implement phrase-level detection that finalizes at natural boundaries — commas, conjunctions, sentence-ending intonation — rather than pure silence thresholds. This allows shorter silence windows without cutting users off mid-thought.

The second fastest improvement is streaming LLM output. If your LLM generates tokens sequentially but your system waits for the full response before sending anything to TTS, you are wasting 100 to 300 milliseconds. Stream the first token immediately. Send it to TTS as soon as it arrives. Begin synthesizing and transmitting audio while the LLM continues generating the rest of the response. This overlaps LLM and TTS latency, cutting TTFA by the duration of LLM inference beyond the first token.

The third improvement is minimizing TTS buffering. Configure your TTS provider to begin synthesis after the first five to ten words rather than waiting for a full sentence. Some degradation in prosody is acceptable if it cuts TTFA by 150 milliseconds. Users tolerate slightly less natural intonation far better than they tolerate long pauses. If your TTS provider does not support low-latency streaming, switch providers. This is not a feature — it is a requirement for conversational voice AI.

The fourth improvement is geographic optimization. Deploy ASR, LLM, and TTS in the same region. Route users to the region closest to them. Use edge computing to handle orchestration logic as close to the user as possible. Every network hop you eliminate saves 20 to 60 milliseconds. Three hops eliminated equals 150ms of TTFA improvement with zero quality cost.

## TTFA as a Quality Gate

TTFA should be a release gate. If a model update, infrastructure change, or feature addition increases TTFA beyond your threshold, the change does not ship. This sounds obvious, but many teams treat TTFA as a performance metric to monitor rather than a quality requirement to enforce. They ship changes that degrade TTFA by 100ms because the feature is valuable and they assume users will tolerate the latency. Users do not tolerate it. They stop using the product.

Set a TTFA target based on conversational research and user testing. For most voice AI products, a P95 TTFA below 600ms is acceptable. A P95 above 800ms is unacceptable. The range between 600ms and 800ms is a judgment call based on use case, user tolerance, and competitive landscape. A voice assistant used while driving may tolerate 750ms. A customer service bot competing with human agents must hit 500ms or users will prefer humans.

Measure TTFA in every deployment stage. Run synthetic load tests that simulate real user conversations and measure TTFA under load. If your TTFA is 450ms in staging with ten concurrent users but 720ms in production with 500 concurrent users, your system does not scale. Fix the scaling problem before it reaches users. Track TTFA in A/B tests. If a new TTS provider improves naturalness but increases TTFA by 80ms, run a controlled experiment. Measure user engagement, session length, and satisfaction. If users prefer the faster system despite lower naturalness, choose speed. If they prefer naturalness despite higher latency, choose quality. Data decides.

## The TTFA-User Satisfaction Curve

User satisfaction does not degrade linearly with TTFA. It follows a step function. Below 400ms, most users perceive the system as instantaneous. Between 400ms and 600ms, the system feels responsive. Between 600ms and 800ms, the system feels slightly slow but acceptable. Above 800ms, satisfaction drops sharply. Above 1000ms, users begin to believe the system is broken or their connection has failed.

This step function means that small TTFA improvements below 400ms have minimal user impact. Dropping from 380ms to 320ms is imperceptible to most users. But small improvements near the 800ms threshold have enormous impact. Dropping from 850ms to 750ms moves the user experience from "frustrating" to "acceptable." The same 100ms reduction has vastly different value depending on where you start.

Use this curve to prioritize optimization work. If your P95 TTFA is 950ms, cutting it to 850ms is critical. If your P95 is 450ms, cutting it to 350ms is a nice-to-have. Focus your engineering effort where the user impact is largest. This often means focusing on tail latency rather than median latency. A system with 400ms P50 and 900ms P95 should optimize P95 first. The users experiencing 900ms are the ones abandoning your product.

## TTFA and Conversational Flow

Voice AI is not a series of isolated requests. It is a conversation. TTFA affects not just individual turns but conversational rhythm. A system with 650ms TTFA feels fine on the first turn. On the fifth turn, it feels exhausting. The user begins to dread asking follow-up questions because every turn requires waiting nearly a second for a response. Conversation slows to a crawl. Engagement drops.

Conversational flow requires consistency as much as speed. If TTFA varies wildly — 350ms on one turn, 780ms on the next, 420ms on the third — the user cannot build a rhythm. They do not know when to expect a response. They hesitate before speaking. They interrupt the system or wait too long. The conversation becomes awkward. A system with consistent 550ms TTFA feels better than one with 400ms median and 850ms P95 because the user can predict and adapt to the consistent delay.

Measure multi-turn TTFA. Track not just individual turn latency but the aggregate latency across a five-turn conversation. If your per-turn TTFA is 500ms but your five-turn conversation takes 3.2 seconds of cumulative wait time, the user is spending 40 percent of the conversation waiting. That is unacceptable for any use case that aspires to feel conversational.

## TTFA in the Age of Faster Models

Model improvements in 2025 and 2026 dramatically reduced LLM inference latency. GPT-5-nano, Gemini 3 Flash, and Claude Sonnet 4.5 all deliver inference latency 30 to 50 percent lower than their 2024 predecessors. This should have cut TTFA proportionally. In many systems, it did not. Why? Because LLM latency was never the dominant contributor to TTFA. Network hops, ASR buffering, TTS configuration, and orchestration overhead remained constant. Faster models improved one component but left the system-level bottlenecks untouched.

Teams that focused only on LLM speed discovered that upgrading from GPT-5-mini to GPT-5-nano cut LLM latency from 220ms to 140ms but reduced TTFA from 680ms to 640ms — a 40ms improvement when they expected 80ms. The missing 40ms was absorbed by other components whose latency did not change. This is the tax of sequential pipelines. When one component speeds up, the others become proportionally more important.

The lesson: faster models are necessary but not sufficient. You must optimize the entire pipeline. Upgrading to GPT-5-nano buys you 80ms of latency budget. Spend that budget wisely — on better ASR accuracy, richer TTS expressiveness, or geographic expansion. Or spend it on cutting TTFA further by optimizing the components that were already fast. The budget is yours. But do not assume the model improvement alone will deliver the TTFA you need.

---

Next, we examine how to measure latency at the component level, the instrumentation required to identify bottlenecks, and the production infrastructure that makes continuous latency optimization possible.
