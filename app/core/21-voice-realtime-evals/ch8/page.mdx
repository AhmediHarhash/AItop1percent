# Chapter 8 — Real-Time Safety for Voice Systems

A harmful response that is spoken cannot be unspoken. In text-based AI, you can filter after generation, redact before display, or show a warning message. In voice, once the TTS engine starts speaking, the damage is done. The user has heard it. If that user is a journalist testing your system, the recording is already online. If the system hallucinated medical advice, someone might follow it. If it disclosed private information, the breach has occurred.

Real-time safety for voice requires detecting unsafe content before it reaches TTS, monitoring transcriptions for policy violations as the conversation unfolds, and making split-second decisions about whether to continue or terminate a call. The latency constraints are brutal. You have 200 milliseconds to check content, make a safety determination, and either proceed or block. This chapter defines the unique safety challenges of voice AI and the evaluation methods that tell you whether your safety controls actually work under production speed and noise.

---

- 8.1 — Why Voice Safety Is Different from Text Safety
- 8.2 — Pre-TTS Content Filtering: Catching Harm Before Speech
- 8.3 — Real-Time Transcription Monitoring for Policy Violations
- 8.4 — The Latency Cost of Safety: Budgeting for Moderation
- 8.5 — Hallucination-Under-Noise: The HUN Rate Problem
- 8.6 — Deepfake and Voice Cloning Abuse Detection
- 8.7 — Caller Emotional Escalation Detection
- 8.8 — Disclosure Requirements: When the Agent Must Identify Itself
- 8.9 — The Call Drop Decision: When to End a Dangerous Conversation
- 8.10 — Adversarial Audio Attacks on Voice Systems
- 8.11 — Building Voice-Specific Red Team Protocols
- 8.12 — Safety Monitoring in Production Voice Deployments

---

*In text, you can edit. In voice, you must prevent. These twelve subchapters define how.*
