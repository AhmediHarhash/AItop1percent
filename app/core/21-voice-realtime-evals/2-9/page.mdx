# 2.9 — Diarization: Who Spoke When in Multi-Speaker Audio

When two people speak in the same audio stream, your transcription system produces a wall of text with no indication of who said what. The customer said "I never authorized this charge." The agent said "Let me look into that for you." The transcript reads: "I never authorized this charge let me look into that for you." Without speaker labels, the transcript is unusable. Diarization is the process of segmenting audio by speaker — identifying when each speaker starts and stops talking. It is mandatory for contact centers, meeting transcription, and any application where multiple people share the same microphone.

Diarization is not speaker identification. Speaker identification matches a voice to a known identity — "this is Alice." Diarization only distinguishes speakers — "Speaker 1 said this, Speaker 2 said that." You do not need to know who the speakers are. You only need to know that they are different people. Diarization outputs a timeline: Speaker 1 from 0 to 3 seconds, Speaker 2 from 3 to 7 seconds, Speaker 1 from 7 to 9 seconds. ASR then transcribes each segment separately, and the final transcript is labeled by speaker.

## How Diarization Works

Diarization models analyze audio to identify speaker changes. The model extracts speaker embeddings — numerical representations of each speaker's voice characteristics. Voices with similar embeddings belong to the same speaker. Voices with different embeddings belong to different speakers. The model clusters the embeddings and assigns a speaker label to each cluster. Each audio segment is then tagged with its cluster's speaker ID.

The process is not perfect. If two speakers have similar voices — two men with the same accent, two women with similar pitch — their embeddings overlap. The clustering algorithm may assign both speakers to the same cluster, merging them into one. The transcript will attribute both speakers' words to a single speaker ID. If the speakers interrupt each other frequently, the segmentation boundaries become blurry. The model may assign part of Speaker 1's sentence to Speaker 2 or miss a brief interjection entirely.

Modern diarization systems in 2026 use deep learning models trained on large multi-speaker audio datasets. Pyannote.audio, AssemblyAI, and Deepgram provide diarization as part of their ASR services. The models achieve 90 to 95 percent accuracy on clean, two-speaker audio with minimal overlap. Accuracy degrades to 70 to 80 percent on noisy audio with four or more speakers and frequent interruptions. The error rate doubles when speakers talk over each other.

Diarization latency depends on whether the system processes audio in real time or batch mode. Batch diarization processes the entire audio file after the conversation ends. It has access to the full audio context and can refine speaker boundaries by analyzing forward and backward in time. Real-time diarization processes audio as it arrives, with no access to future audio. It must make speaker change decisions based only on past and current audio. Real-time diarization is less accurate but necessary for live transcription.

## When Diarization Matters

Diarization is critical in contact centers. A customer calls, speaks with an agent, and the interaction is recorded and transcribed. The company needs to know what the customer said versus what the agent said. Analytics systems measure agent compliance, customer sentiment, and call resolution quality. Without diarization, the transcript is a single block of text. You cannot measure whether the agent followed the script. You cannot detect when the customer became frustrated. You cannot analyze the interaction structure.

Meeting transcription tools require diarization for usability. Five people join a Zoom call. They discuss a project, interrupt each other, overlap, and talk simultaneously. The raw ASR transcription is a stream of text with no indication of who said what. Diarization segments the transcript by speaker. The output is readable: Speaker 1 said X, Speaker 2 said Y, Speaker 3 agreed with Speaker 1. The user can follow the conversation. Without diarization, the transcript is noise.

Voice assistants in shared spaces benefit from diarization. A smart speaker in a living room hears multiple family members. One person asks a question, another person answers unrelated to the speaker. Without diarization, the system conflates the two voices. It might interpret background conversation as a command or respond to the wrong person. Diarization identifies that two different people spoke and only responds to the person who addressed the assistant.

Podcast and video transcription services use diarization to identify hosts, guests, and speakers. The user uploads a two-hour podcast with three hosts. The transcription service returns a speaker-labeled transcript: Host 1, Host 2, Guest. The user can edit the labels to assign real names, but the segmentation is done automatically. Without diarization, the user has to manually listen to the audio and insert speaker labels — a task that takes as long as the audio itself.

## Diarization Accuracy in Real-World Noise

Diarization accuracy in controlled lab conditions is high. Two speakers, quiet room, no background noise, no overlap. The model identifies speaker changes with 95 percent accuracy. Real-world audio is messier. Background noise, music, phone line distortion, overlapping speech, and speaker interruptions all degrade accuracy.

Background noise affects speaker embeddings. If one speaker is close to the microphone and another is far away, the distant speaker's voice is mixed with room noise. The embedding for the distant speaker includes noise characteristics, which differ from segment to segment. The clustering algorithm treats the same speaker as multiple speakers because the embeddings vary. The result is over-segmentation — one speaker is split into multiple speaker IDs. The transcript reads: Speaker 1, Speaker 3, Speaker 1 again. Speaker 3 is Speaker 1 speaking from farther away.

Overlapping speech is the hardest problem. Two speakers talk at the same time. The audio contains both voices mixed. The diarization model must detect that overlap exists, separate the voices, and assign each to the correct speaker. Some diarization systems in 2026 detect overlap and mark the segment as "multiple speakers." They do not attempt to separate the voices. The ASR transcribes the overlapping audio as a single garbled sentence. Other systems attempt separation but introduce errors. They assign overlapping speech to one speaker and drop the other speaker's words entirely.

The error rate for overlapping speech in production diarization systems is 40 to 60 percent in 2026. If 10 percent of a meeting involves overlapping speech, diarization will misattribute 4 to 6 percent of the entire transcript. In a 30-minute meeting, that is 70 to 100 seconds of incorrect speaker labels. For post-hoc transcription, this is tolerable. For real-time compliance monitoring in a contact center, this is a risk. If an agent says something policy-violating during overlap, the system may attribute it to the customer instead.

## The Latency Cost of Diarization

Batch diarization adds processing time but not user-facing latency. The system receives the full audio file, runs diarization, and returns a speaker-labeled transcript. The user waits for the result. The wait time is acceptable because the alternative is manual labeling. A two-minute phone call processed in 15 seconds with diarization is faster than manual work.

Real-time diarization adds latency to the live transcription pipeline. The system receives audio, runs diarization, segments by speaker, runs ASR on each segment, and outputs labeled transcription. Diarization models in 2026 add 50 to 150 milliseconds of processing time per audio segment. If the system processes audio in 1-second chunks, diarization adds 50 to 150 milliseconds per second of audio. The cumulative delay grows. After 10 seconds of conversation, the live transcript is 500 to 1,500 milliseconds behind real time.

The latency cost is higher when diarization runs on the cloud. The client device streams audio to the cloud, the cloud runs diarization and ASR, and the cloud streams the labeled transcript back to the client. Each network hop adds round-trip time. If the client's connection has 40 milliseconds RTT, the total latency is VAD detection time plus diarization time plus ASR time plus network time. The sum can exceed 2 seconds for the first word of transcription to appear.

Some systems reduce latency by skipping real-time diarization for speaker-labeled live transcription and adding speaker labels post-hoc. The live transcript is unlabeled. After the conversation ends, the system runs batch diarization on the recorded audio and updates the stored transcript with speaker labels. This works when the user does not need live speaker labels — meeting notes, recorded calls, archived transcriptions. It does not work when the user needs real-time speaker attribution — live compliance monitoring, accessibility captions for multi-speaker panels.

## Speaker Enrollment and Identification

Diarization assigns anonymous speaker IDs: Speaker 1, Speaker 2. It does not know who the speakers are. Speaker identification goes one step further — it matches each speaker ID to a known identity. This requires speaker enrollment. Each speaker provides a voice sample, the system extracts an embedding, and stores the embedding with the speaker's name. During diarization, the system compares each segment's embedding to the enrolled speaker embeddings and assigns a name if there is a match.

Speaker identification works when the set of possible speakers is small and known in advance. A household with four people enrolls all four voices. When someone speaks to the smart speaker, the system identifies which household member is speaking and personalizes the response. A corporate meeting room enrolls the ten people who regularly use the room. When a meeting is transcribed, the system labels the speakers by name instead of by number.

Accuracy depends on the quality of the enrollment samples and the similarity of the speakers' voices. If two enrolled speakers sound similar, the system may confuse them. If the enrollment sample is noisy or brief, the embedding is unreliable. Best practice in 2026 is to collect 30 to 60 seconds of clear speech per speaker during enrollment. Shorter samples reduce accuracy. Longer samples improve accuracy but increase enrollment friction.

Privacy concerns limit speaker identification in many applications. Storing voice embeddings tied to user identities is biometric data. In jurisdictions with strict biometric privacy laws — Illinois, the EU under GDPR — storing and processing voice embeddings requires explicit consent and specific data handling procedures. Diarization without identification avoids this. The system labels speakers as 1, 2, 3 without storing biometric data tied to real identities.

## When Diarization Fails

The most common diarization failure is merging two speakers into one. Two speakers with similar voices are assigned the same speaker ID. The transcript attributes both speakers' words to one speaker. In a customer support call, the customer and the agent are both labeled as Speaker 1. The transcript is useless for compliance analysis. You cannot distinguish who made which statement.

The second most common failure is over-segmentation. One speaker is split into multiple speaker IDs. The speaker moves closer to the microphone, moves farther away, or changes vocal tone. The embedding shifts, and the clustering algorithm assigns a new speaker ID. The transcript reads: Speaker 1, Speaker 2, Speaker 1 again. Speaker 2 is the same person as Speaker 1, just with a different acoustic profile. This makes the transcript confusing but does not lose information — all the words are still there, just mis-labeled.

Overlapping speech is lost or misattributed. Two people talk at the same time. The diarization model either merges the overlap into one speaker or drops one speaker's words entirely. In a heated argument captured on a customer service line, both the customer and the agent are shouting over each other. The transcript shows only one side of the argument. The other side is missing. This is a compliance risk. If the agent violated policy during the overlap, and the transcript only captured the customer's words, the violation is not documented.

Background voices are sometimes treated as speakers. A customer calls from home. Their child is playing in the background, saying unrelated words. Diarization assigns the child a speaker ID. The transcript includes the child's words as Speaker 2, interspersed with the customer's words as Speaker 1. The transcript is noisy. Analytics systems measuring sentiment or intent are confused by the irrelevant speech. The solution is filtering — mark short-duration, low-energy speakers as background noise and exclude them from the transcript. This requires tuning. If the threshold is too aggressive, it excludes soft-spoken speakers.

## Production Strategy

Use diarization when you need to distinguish speakers. Do not use it when you only care about the content and the number of speakers is always one. Diarization adds latency and cost. If the ROI is knowing who said what, deploy it. If the ROI is just knowing what was said, skip it.

If deploying real-time diarization, measure the latency impact. Run A/B tests with and without diarization and measure user-perceived latency. If users complain about delay, consider post-hoc diarization instead. Label the transcript after the conversation ends. This works for recorded calls, meeting notes, and archived content. It does not work for live captions or real-time compliance alerts.

Tune diarization for your speaker population. If your users are mostly two-speaker phone calls, optimize for two speakers. If your users are meetings with five to ten participants, optimize for multi-speaker scenarios. Some diarization models let you specify the expected number of speakers. If you know in advance that a call has exactly two speakers — customer and agent — tell the model. It will not try to find a third speaker in background noise.

Monitor diarization accuracy in production. Log the number of speakers detected, the duration of each speaker's segments, and the frequency of speaker changes. Compare this to ground truth when available. In contact centers, you know there are exactly two speakers. If diarization reports three or four speakers on most calls, the model is over-segmenting. Tune the clustering threshold to merge similar embeddings more aggressively.

If accuracy is unacceptable, fall back to unlabeled transcription. Some use cases can tolerate unlabeled transcripts. A user uploading a podcast for transcription can manually insert speaker labels if the automated diarization is wrong. A user transcribing a lecture with one speaker does not need diarization at all. Do not force diarization on use cases where the failure cost exceeds the benefit.

The next subchapter will explore how to evaluate voice systems end-to-end — not just individual components, but the entire pipeline from user speech to system response.

