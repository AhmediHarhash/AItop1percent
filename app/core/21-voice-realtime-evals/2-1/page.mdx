# 2.1 — Anatomy of a Voice AI Pipeline: End-to-End Flow

In early 2025, a telehealth startup spent nine months building a voice AI assistant for patient intake. The demo was flawless. Every component worked individually. The ASR transcribed accurately. The LLM generated helpful responses. The TTS sounded natural. But when users spoke to the actual system, they described it as "broken" and "unresponsive." The engineering team was confused. Every benchmark passed. Every unit test succeeded. The problem became clear only when they measured end-to-end latency: the system took four to six seconds from user speech to audio response. Individual components were fast. The pipeline as a whole was not.

They had optimized each piece in isolation. They had never measured the complete journey from microphone to speaker. By the time they understood the problem, they had burned through their Series A runway and shut down the product. The lesson was brutal but clear: you cannot build a voice AI system by assembling fast components. You must understand and optimize the entire pipeline as a single latency-critical flow.

## The Five Stages of Voice AI

Every voice AI interaction moves through five distinct stages. User speaks. System captures and transcribes. System generates response. System synthesizes speech. User hears audio. The stages seem simple. The reality is not. Each stage has latency, failure modes, and dependencies on the stages before and after it. Understanding the complete flow is not optional background knowledge. It is the foundation of everything else you will build.

The first stage is audio capture. The user speaks into a microphone. Their voice becomes a stream of audio samples, typically captured at 16 kHz or 48 kHz depending on the platform. On a phone call, the telephony provider digitizes the audio. On a mobile app, the device's microphone captures raw PCM data. On a web app, the browser's Web Audio API streams audio frames to your backend. This stage feels instantaneous but it is not. Phone networks introduce 50 to 150 milliseconds of latency just capturing and transmitting audio. Mobile apps buffer audio to reduce packet loss, adding 100 to 200 milliseconds. Web apps wait for getUserMedia permissions and buffer initialization, adding another 50 to 100 milliseconds. Before your ASR provider sees a single audio frame, 150 to 400 milliseconds have already elapsed.

The second stage is automatic speech recognition. The audio stream flows to an ASR provider. Deepgram, AssemblyAI, OpenAI Whisper, Azure Speech, Google Speech-to-Text. The provider processes audio in chunks or streams, depending on the API you chose. Streaming ASR emits partial transcripts as the user speaks, with latency ranging from 200 to 600 milliseconds from speech to first partial result. Batch ASR waits for complete utterances, with latency from 800 milliseconds to 2 seconds. The ASR provider returns text. This text is rarely perfect. It contains disfluencies, filler words, punctuation errors, and occasional hallucinations. The text is also not instantaneous. Even the fastest streaming ASR cannot return a complete sentence until the user finishes speaking. If the user pauses mid-sentence, the ASR may interpret it as an utterance boundary and send incomplete text to the next stage. If the user speaks quickly without pauses, the ASR may wait too long, delaying the entire pipeline.

The third stage is language model inference. The transcribed text becomes a prompt to an LLM. GPT-5, Claude Opus 4.5, Gemini 3, Llama 4 Maverick. The model generates a response. In 2026, most voice systems use streaming inference, where the model emits tokens as soon as they are generated rather than waiting for the complete response. Token-by-token streaming reduces perceived latency but does not eliminate it. The first token still requires the full prompt processing time, which ranges from 100 to 800 milliseconds depending on model size, prompt length, and infrastructure. A 4,000-token conversation history on a large model can take 600 milliseconds just to process the prompt before the first token appears. After the first token, generation speed depends on tokens per second. Fast models on optimized infrastructure generate 40 to 80 tokens per second. Slower models or under-provisioned deployments generate 10 to 20 tokens per second. A 50-token response on a fast model takes 600 milliseconds. On a slow model, 2.5 seconds. This is the stage where latency variance explodes. The same model can respond in 800 milliseconds or 3 seconds depending on load, prompt complexity, and token count.

The fourth stage is text-to-speech synthesis. The LLM's text response flows to a TTS provider. ElevenLabs, Cartesia Sonic, Azure Neural, Google WaveNet, Amazon Polly. The provider synthesizes audio. In 2026, most TTS providers support streaming synthesis, where they begin generating audio as soon as the first sentence fragment arrives rather than waiting for the complete text. Streaming TTS emits audio chunks with latency from 150 to 500 milliseconds for the first chunk. Non-streaming TTS waits for complete text and synthesizes the full response, with latency from 800 milliseconds to 2 seconds. The quality-speed tradeoff is stark. The most natural voices—ElevenLabs Multilingual v2, Cartesia Sonic—require more processing time. Faster voices sound robotic or flat. You choose naturalness or speed, rarely both. The TTS stage also depends on text length. A short response synthesizes quickly. A long response, even with streaming, can take 2 to 4 seconds to generate completely. If your LLM generates verbose responses, your TTS stage becomes the bottleneck.

The fifth stage is audio playback. The synthesized audio streams to the user's device. On a phone call, the telephony provider plays audio back over the line. On a mobile app, the audio player buffers and renders PCM samples. On a web app, the browser's audio context plays back the stream. This stage also has latency. Phone networks introduce 50 to 150 milliseconds of playback delay. Mobile apps buffer audio to prevent crackling, adding 50 to 100 milliseconds. Web apps wait for audio context initialization and playback scheduling, adding another 50 to 100 milliseconds. The user does not hear audio until this stage completes. Even if your TTS is fast, playback delay can make the system feel sluggish.

## The Latency Budget: Where Time Disappears

When you add the stages together, the total latency becomes clear. Audio capture: 150 to 400 milliseconds. ASR: 200 to 2,000 milliseconds depending on streaming vs batch. LLM: 600 to 3,000 milliseconds depending on model and response length. TTS: 150 to 2,000 milliseconds depending on streaming vs batch. Audio playback: 50 to 150 milliseconds. In the best case, with streaming ASR, fast LLM, and streaming TTS, you achieve 1,150 milliseconds total latency. In the worst case, with batch ASR, slow LLM, and batch TTS, you hit 7,550 milliseconds. The difference between a responsive system and a broken one is not a single slow component. It is the accumulation of delays across every stage.

The latency budget forces trade-offs at every stage. If your ASR adds 600 milliseconds, you have 1,400 milliseconds left for a 2-second target. If your LLM takes 1,000 milliseconds, you have 400 milliseconds left for TTS. If your TTS needs 500 milliseconds for natural voice, you must shave 100 milliseconds somewhere else or accept a 2.1-second response time. The budget is unforgiving. Every millisecond you spend in one stage is a millisecond you cannot spend in another. There is no reserve. There is no buffer. You have 2 seconds total, and the stages consume it completely.

Teams that succeed measure latency at every stage. They instrument the pipeline end-to-end. They log timestamps for audio capture start, ASR first partial, ASR final result, LLM first token, LLM last token, TTS first chunk, TTS last chunk, audio playback start. They calculate p50, p95, and p99 latencies for each stage. They identify where variance comes from and where bottlenecks form. They do not guess. They measure. A team that measures discovers that their ASR is fast but their LLM prompt is 6,000 tokens due to verbose conversation history. A team that guesses blames the TTS provider and wastes two weeks testing alternatives that make no difference.

## The Handoff Problem: Where Integration Adds Latency

The pipeline is not just five stages. It is also four handoffs. ASR to LLM. LLM to TTS. Audio capture to ASR. TTS to playback. Each handoff introduces latency. The latency is not in the components. It is in the integration. Network round trips. API call overhead. Serialization and deserialization. Buffering and batching. Handoffs are where fast components become slow systems.

Consider the ASR to LLM handoff. Your ASR provider returns a transcript via webhook or streaming API. Your backend receives the text. It constructs a prompt. It calls the LLM API. This seems instantaneous but it is not. The webhook delivery takes 10 to 50 milliseconds depending on network latency. Your backend processes the webhook, loads conversation history from a database, formats the prompt, and sends it to the LLM. This takes 20 to 100 milliseconds depending on your infrastructure. The LLM API call itself has overhead—TLS handshake, request serialization, response deserialization—adding another 10 to 30 milliseconds. The handoff consumes 40 to 180 milliseconds. For a 2-second latency budget, this is 2 to 9 percent of your total time. And you have four handoffs.

The LLM to TTS handoff is worse. Your LLM streams tokens. Your TTS provider expects complete sentences or sentence fragments. You must buffer tokens until you have enough text to send to TTS. Buffering adds latency. If you wait for a complete sentence, you add 200 to 800 milliseconds depending on response length. If you send fragments too early, you waste TTS API calls and increase cost. If you send fragments too late, you delay audio playback. The optimal buffering strategy depends on response structure, TTS provider capabilities, and acceptable latency variance. There is no universal answer. Every team finds their own balance.

The audio capture to ASR handoff introduces format conversion. Your telephony provider sends audio in mu-law or G.711 format. Your ASR provider expects linear PCM at 16 kHz. You must decode, resample, and re-encode. This takes 5 to 20 milliseconds per audio chunk. If you process 100 chunks per conversation, you spend 500 to 2,000 milliseconds on format conversion alone. Teams that ignore format conversion discover it only when they profile their pipeline and see CPU spikes during audio processing. By then, they have deployed to production and users are complaining about lag.

The TTS to playback handoff introduces buffering. Your TTS provider streams audio chunks. Your playback system buffers chunks to prevent underruns. If the buffer is too small, audio crackles. If the buffer is too large, playback latency increases. Phone systems buffer 100 to 200 milliseconds by default. Mobile apps buffer 50 to 100 milliseconds. Web apps buffer 20 to 50 milliseconds. The buffer is necessary but it costs time. Every millisecond buffered is a millisecond the user waits.

## Failure Modes: Where the Pipeline Breaks

The pipeline fails in predictable ways. ASR returns incorrect transcripts. The user said "I need to refill my prescription" but the ASR returned "I need two real my subscription." Your LLM generates a response about subscription management instead of prescription refills. The user repeats themselves. The conversation derails. ASR errors cascade through the pipeline. A single transcription mistake ruins the entire interaction.

The LLM generates text that the TTS cannot pronounce. Medical terms, abbreviations, URLs, code snippets. Your TTS provider tries to synthesize "NSAID" and produces "en-es-ay-eye-dee" instead of "en-sed." Your TTS tries to synthesize "bit.ly/report" and produces "bit dot lee why slash report." The audio sounds robotic and confusing. The user asks for clarification. The conversation slows. TTS pronunciation errors are invisible in text but obvious in audio. Teams that test with written transcripts miss them completely.

The network fails mid-conversation. The user's phone loses signal. The websocket disconnects. The audio stream stops. Your ASR provider waits for more audio. Your backend waits for the ASR result. Your LLM waits for the prompt. The user hears silence. After 10 seconds, they hang up. The conversation is lost. Network failures are rare but devastating. A system that does not handle disconnections gracefully loses every user who hits them.

The LLM generates a response too long for real-time playback. The user asks a simple question. Your LLM generates 400 tokens of explanation. Your TTS synthesizes 30 seconds of audio. The user interrupts after 5 seconds but your system keeps playing the remaining 25 seconds. The user repeats their question. Your system is still finishing the previous response. The conversation becomes overlapping chaos. Long responses are fine in text chat. In voice, they are a failure mode. You must constrain LLM output length or accept that users will interrupt and abandon.

The ASR detects speech when there is none. Background noise. The user coughs. A car horn. Your ASR returns a partial transcript. Your pipeline interprets it as user input. Your LLM generates a response. Your TTS synthesizes audio. The system speaks when the user did not. This is the "phantom utterance" problem. It happens more often than teams expect, especially in noisy environments like cars, streets, or kitchens. You need voice activity detection or confidence thresholding to filter false positives. Without it, your system becomes chatty and annoying.

## Visibility: Observability in a Five-Stage Pipeline

You cannot optimize what you cannot see. The voice pipeline is a black box unless you instrument it. Every stage must emit metrics. Timestamps, latencies, success rates, error codes. You must log every handoff. You must trace every request end-to-end. Without observability, you are debugging blind. With observability, you see exactly where latency accumulates and where failures occur.

Instrument audio capture. Log when the audio stream starts. Log the sample rate, encoding format, and buffer size. Log when the first audio chunk arrives and when the stream ends. Log dropped packets and underruns. If your audio capture is unstable, the rest of the pipeline inherits that instability. You must see it to fix it.

Instrument ASR. Log when audio is sent to the provider. Log when the first partial transcript arrives. Log when the final transcript arrives. Log the transcript text, confidence score, and any detected language or disfluencies. Log ASR errors—timeouts, rate limits, invalid audio format. If your ASR is slow or inaccurate, you need the logs to prove it. Without logs, you guess.

Instrument the LLM. Log when the prompt is sent. Log the prompt length in tokens. Log when the first token is generated. Log when the last token is generated. Log the total token count, the model used, and the inference time. Log LLM errors—rate limits, content policy violations, timeouts. If your LLM is the bottleneck, the logs show it immediately. If the bottleneck is elsewhere, the logs exonerate the LLM and redirect your attention.

Instrument TTS. Log when text is sent to the provider. Log the text length. Log when the first audio chunk is generated. Log when the last chunk is generated. Log the total audio duration and synthesis time. Log TTS errors—rate limits, invalid text, unsupported characters. If your TTS is slow, the logs tell you whether the problem is the provider or the text you are sending.

Instrument playback. Log when audio playback starts. Log when playback ends. Log buffer underruns, playback stalls, and user interruptions. If users complain about choppy audio, the logs reveal whether the problem is your TTS, your network, or the user's device.

Trace end-to-end. Assign a unique request ID to every conversation. Log the request ID at every stage. When a conversation fails, search for the request ID and see the complete timeline. You discover that the ASR took 3 seconds, the LLM took 400 milliseconds, and the TTS took 200 milliseconds. The ASR was the bottleneck. Without tracing, you would have blamed the LLM because the user experience felt like "the AI is thinking too long." With tracing, you fix the actual problem.

## The Baseline Measurement: Know Your Current State

Before you optimize, measure your current state. Deploy your pipeline. Run 1,000 test conversations. Measure p50, p95, and p99 latency at every stage. Measure end-to-end latency from user speech to audio playback. Measure success rate—what percentage of conversations complete without errors. Measure user-perceived quality—what percentage of transcripts are accurate, what percentage of responses are relevant, what percentage of audio is natural. This is your baseline.

The baseline tells you where you are. If your p50 end-to-end latency is 3.2 seconds, you know you have work to do. If your p99 latency is 8 seconds, you know your system is unreliable. If your ASR accuracy is 82 percent, you know transcription errors are common. If your LLM generates off-topic responses 6 percent of the time, you know prompt engineering is needed. The baseline is not a goal. It is a starting point. You cannot improve without knowing where you started.

Teams that skip baseline measurement guess at improvements. They switch ASR providers hoping for better latency. They measure again and discover latency increased because the new provider has slower streaming. They switch back. They wasted two weeks. Teams that measure baseline before changing anything make informed decisions. They see that ASR is fast but LLM is slow. They optimize the LLM. Latency drops. They measure again. The improvement is real and quantified.

The next subchapter examines the ASR provider landscape in 2026—Deepgram, AssemblyAI, Whisper, and beyond—and how to choose the right provider for your latency and accuracy requirements.
