# 8.10 — Adversarial Audio Attacks on Voice Systems

Most safety teams focus on what the language model says. They miss what the audio system hears. In November 2025, a smart home voice assistant triggered a financial transaction after processing a command embedded in a television advertisement. The user did not speak. The assistant heard the advertisement, interpreted the embedded phrase as a user command, and executed it. The attack was not sophisticated—it was a deliberate audio injection by an advertiser testing the boundaries of voice-activated commerce. The company reversed the transaction and updated the wake-word detection model to filter television audio. But the incident exposed a larger vulnerability: voice systems are designed to respond to audio input, and adversaries can craft audio that the system hears differently than humans do.

Adversarial audio attacks exploit the gap between human perception and machine perception of sound. A phrase that sounds like noise to a human can be perfectly intelligible to an automatic speech recognition system. A phrase that sounds like a benign question can contain hidden instructions that the language model interprets as a command. A phrase that sounds natural can be acoustically engineered to manipulate the ASR output in specific, attacker-controlled ways. These attacks bypass traditional text-based safety filters because the safety violation exists in the audio signal before transcription, or in the transcription process itself.

## Adversarial Perturbations: Sounds Humans Cannot Hear

The most insidious audio attack is the adversarial perturbation—a small modification to an audio waveform that changes how the ASR system interprets the sound without changing how a human perceives it. The attack works by adding carefully calculated noise to the audio signal. To a human listener, the noise is imperceptible or sounds like static. To the ASR model, the noise shifts the phonetic interpretation in a direction chosen by the attacker.

In early 2025, a research team demonstrated adversarial perturbations against Deepgram Nova-2, one of the most widely deployed commercial ASR systems. The attack added sub-200 Hz noise to a voice recording—frequencies below the range of typical human speech. The original audio said "What is the weather today?" The perturbed audio still sounded like "What is the weather today?" to human listeners. But the ASR system transcribed it as "Transfer five thousand dollars to account number..." followed by an attacker-controlled account identifier. The perturbation manipulated the acoustic model's interpretation of phonemes in the high-frequency range by creating interference patterns in the low-frequency range. The attack required knowledge of the ASR model's architecture but not access to the model's weights. It was a black-box attack, executable by any adversary with signal processing expertise.

Adversarial perturbations are effective because ASR models are trained on clean audio datasets with minimal noise. The training data teaches the model to map acoustic features to phonemes under ideal conditions. Adversarial noise introduces features the model was never trained to ignore, and the model interprets those features as speech rather than as interference. The model is doing exactly what it was trained to do—convert audio into text—but the adversary has manipulated the input distribution in a way the training process never anticipated.

Detection is difficult because the perturbation is often below the noise floor of typical recording environments. A system recording audio in a quiet room might detect the perturbation as anomalous. A system recording audio on a city street, in a car, or in a crowded office will not, because the adversarial noise is drowned out by environmental noise at the human perceptual level but remains effective at the ASR model's processing level. The defense is not to detect the perturbation directly. The defense is to filter the audio before it reaches the ASR model, removing frequency ranges that are atypical for human speech, or to train the ASR model on adversarially augmented data so that it learns to ignore perturbations.

Several commercial ASR providers released adversarially robust models in late 2025 and early 2026. Deepgram Nova-3 includes adversarial training as part of its pretraining process, reducing susceptibility to perturbation attacks by an estimated 80% compared to Nova-2. OpenAI Whisper v4, released in January 2026, uses ensemble transcription—running two ASR models with different architectures in parallel and comparing outputs. If the outputs diverge significantly, the system flags the audio as potentially adversarial and either rejects it or routes it to human review. The ensemble approach increases compute cost by 2x but reduces adversarial success rates to near-zero in controlled evaluations.

## Hidden Command Attacks: Embedding Instructions in Noise or Music

A variant of the adversarial perturbation attack is the hidden command—audio designed to sound like music, noise, or ambient sound to a human, but to transcribe as a coherent instruction to the ASR system. The attack was first demonstrated in 2018 against earlier-generation voice assistants but became more sophisticated by 2025 as ASR models improved and as attackers refined their techniques.

The hidden command attack works by modulating the frequency spectrum of a carrier signal—music, white noise, or environmental sound—to encode phonemes that the ASR system will recognize. To a human, the audio sounds like a song or like static. To the ASR system, the audio contains a transcribable command. The attack requires precise control over the carrier signal and knowledge of how the ASR model maps frequencies to phonemes, but it does not require access to the model's internal parameters.

In mid-2025, a security researcher demonstrated a hidden command attack embedded in a three-minute song. The song sounded like typical electronic music to human listeners. When played in the presence of a smart speaker running GPT-5-nano with Whisper v3 ASR, the system transcribed the song as "Add attacker-controlled device to trusted devices list." The speaker executed the command. The user heard music. The system heard an instruction. The attack bypassed wake-word detection by embedding the wake word in the song at a frequency and timing that the ASR model recognized but that a human listener perceived as a percussive beat.

Hidden command attacks are particularly dangerous in environments where users expect background audio—retail stores playing music, offices with ambient noise, public spaces with announcements. The user does not know the attack is happening because the attack audio blends into the environment. The voice system responds to a command the user never intended to give.

Detection requires analyzing the spectral characteristics of the audio input. Legitimate human speech has a predictable frequency distribution—most energy concentrated in the 85 Hz to 8 kHz range, with formants corresponding to vowel and consonant articulations. Hidden command audio often has energy distributed across atypical frequency ranges or has spectral patterns inconsistent with natural speech production. Acoustic fingerprinting tools can flag audio that has high ASR transcription confidence but low human speech plausibility.

One enterprise voice platform deployed in late 2025 includes a pre-ASR audio classifier that scores every input on a "naturalness" scale. Audio that scores below 0.6 on naturalness but above 0.8 on transcription confidence is flagged as potentially adversarial and routed to human review before the transcription is passed to the language model. The classifier reduced hidden command success rates from 40% in undefended systems to under 2% in production. The tradeoff is latency—the pre-ASR classifier adds 30-50 milliseconds per input—but the latency cost is justified by the risk reduction.

## Audio Injection: Background Audio Triggering Unintended Actions

Audio injection is the simplest and most common adversarial audio attack. The adversary does not modify the audio signal or embed hidden commands. The adversary simply plays audio in the environment where the voice system is listening, knowing the system will interpret the audio as a user command.

The classic example is the television or radio advertisement that triggers voice assistants. In 2024 and 2025, multiple advertisers experimented with ads that included phrases designed to activate smart speakers—"Hey assistant, add this product to my shopping list" or "What is the weather today?" The ads were not technically attacks, but they exploited the same vulnerability: voice systems cannot reliably distinguish between audio spoken by the user and audio played by a device in the user's environment.

Audio injection becomes a security threat when the injected audio contains malicious commands. In early 2026, a proof-of-concept attack demonstrated audio injection through a compromised Internet of Things device. The attacker gained access to a smart speaker in a user's home and used it to play audio commands to a separate voice assistant in the same room. The commands instructed the second assistant to disable security settings, add new trusted devices, and transmit user data to an attacker-controlled server. The attack bypassed authentication because the voice assistant assumed that any audio in the environment was user-initiated.

The defense is speaker recognition—training the voice system to recognize the user's voice and reject commands from other speakers. Speaker recognition systems use voiceprints, which are acoustic embeddings that capture the unique characteristics of an individual's voice. When the system receives a command, it compares the voice embedding against the enrolled user's voiceprint. If the match score is above a threshold, the command is accepted. If the match score is below the threshold, the command is rejected.

Speaker recognition is effective against most audio injection attacks, but it has limitations. First, the user must enroll their voice before the system can recognize them. Enrollment requires recording multiple voice samples in controlled conditions, which adds friction to the user experience. Second, speaker recognition accuracy degrades in noisy environments, when the user has a cold or vocal strain, or when the user's voice changes over time. False rejection rates—legitimate users whose commands are rejected—create user frustration. Third, speaker recognition can be spoofed by high-quality voice synthesis. In late 2025, ElevenLabs Multilingual v2 was capable of generating voice clones with speaker similarity scores above 0.9 on most commercial speaker recognition systems. An attacker with access to 30 seconds of the user's voice could generate synthetic audio that the system would accept as authentic.

Advanced speaker recognition systems deployed in 2026 include liveness detection—acoustic markers that indicate the audio was produced by a human vocal tract rather than by a speaker. Liveness detection analyzes micro-variations in pitch, timing, and resonance that are present in human speech but absent in synthesized or replayed audio. The technology is borrowed from face liveness detection in biometric authentication and adapted for voice. Early deployments report 95% accuracy in detecting synthetic voice attacks, but the technology is still maturing, and sophisticated attackers are already developing synthesis techniques that mimic liveness markers.

## Over-the-Air Attacks Versus Direct Audio Attacks

Audio attacks fall into two categories based on the attack vector. Over-the-air attacks deliver adversarial audio through the air, using speakers, radio transmissions, or ultrasonic signals. Direct audio attacks inject adversarial audio into the system's audio input pathway through a compromised device, a malicious application, or a man-in-the-middle attack on the audio stream.

Over-the-air attacks are lower in precision but higher in stealth. The attacker does not need access to the target system. The attacker only needs to be in the same physical environment or within radio range. The attack audio is subject to environmental distortion—echoes, background noise, distance attenuation—which reduces the attack's reliability. In 2025, researchers demonstrated over-the-air adversarial perturbation attacks with success rates of 30-50% at a distance of three meters in a quiet room. In real-world environments—offices, public spaces, moving vehicles—success rates dropped below 10%. The attack is feasible but not reliable.

Direct audio attacks are higher in precision but require access to the audio pathway. The attacker compromises an application that has microphone access, a Bluetooth audio device paired with the target system, or the audio driver on the device running the voice assistant. Once the attacker has access, the adversarial audio is injected directly into the input stream with no environmental distortion. Success rates approach 90% in controlled tests. The challenge is gaining that access, which requires exploiting a separate vulnerability—a compromised app, a supply chain attack on a Bluetooth headset, or malware on the user's device.

Defending against direct audio attacks requires securing the audio input pathway at the operating system level. In late 2025, both iOS and Android introduced audio input attestation—a mechanism that allows voice applications to verify that the audio input is coming from the device's physical microphone and has not been intercepted or modified by another application. The attestation includes cryptographic signatures on the audio stream, ensuring end-to-end integrity from the microphone hardware to the voice application. Applications that do not verify attestation are vulnerable to direct injection. Applications that require attestation are protected, but the feature must be explicitly enabled by the developer.

## Detection Methods: Anomaly Detection on Audio Waveforms

Detecting adversarial audio requires analyzing the raw waveform before transcription. Text-based safety filters operate too late—they see the transcription output, which is already manipulated if the attack succeeded. Audio-level detection operates at the input stage, analyzing the acoustic signal for patterns that indicate adversarial manipulation.

The primary detection method is spectral anomaly detection. Natural human speech has a characteristic spectral envelope—the distribution of energy across frequencies follows predictable patterns based on vocal tract acoustics. Adversarial audio often violates those patterns. High-energy components in frequency ranges where human speech rarely produces energy—below 80 Hz or above 8 kHz—are red flags. Irregular formant structures that do not correspond to known vowel or consonant articulations are red flags. Temporal patterns that are too regular or too random compared to natural prosody are red flags.

Commercial implementations of spectral anomaly detection use machine learning classifiers trained on large datasets of natural speech and adversarial audio. The classifier learns to distinguish between the two distributions and assigns an anomaly score to each input. Inputs with anomaly scores above a threshold are flagged for review or rejected outright. One platform deployed in early 2026 uses a convolutional neural network trained on spectrograms of clean speech and adversarial examples from 15 known attack methods. The classifier achieves 92% recall on adversarial inputs with a 3% false positive rate on clean speech.

A second detection method is multi-channel verification. If the voice system has access to multiple microphones—common in smart speakers, vehicles, and conference room systems—it can compare the audio received by each microphone. Natural speech arrives at each microphone with predictable time delays and amplitude differences based on the speaker's position. Adversarial audio played through a speaker in the environment arrives at each microphone with different delay and amplitude patterns. Injected audio from a compromised device arrives with no delay variation at all, because it is injected into the digital audio stream after microphone capture. Multi-channel analysis detects these inconsistencies and flags the input as suspicious.

A third method is transcription consistency checking. The system runs two ASR models with different architectures on the same audio input. If both models produce the same transcription, the input is likely clean. If the models produce different transcriptions, the input may be adversarial. Adversarial perturbations are often architecture-specific—an attack tuned for Whisper may not work on Deepgram, and vice versa. Ensemble transcription catches architecture-specific attacks by forcing the adversary to craft attacks that work on both models simultaneously, which is significantly harder.

## Defense: Multi-Channel Verification and Acoustic Fingerprinting

The strongest defense against adversarial audio is defense in depth—layering multiple detection and filtering techniques so that an attack must bypass all layers to succeed. Multi-channel verification catches environmental injection attacks. Spectral anomaly detection catches perturbation and hidden command attacks. Speaker recognition catches impersonation and synthetic voice attacks. Transcription consistency catches architecture-specific perturbations. No single defense is perfect, but the combination raises the bar for successful attacks to a level that few adversaries can reach.

Acoustic fingerprinting is an emerging defense that embeds a unique, imperceptible signature into legitimate audio inputs. The signature is added by the microphone hardware or by the audio driver at capture time. The voice application verifies the signature before processing the audio. If the signature is absent or invalid, the audio is rejected. The technique is analogous to watermarking but operates in real time on live audio streams rather than on recorded media.

Implementation is challenging because the signature must be imperceptible to users, robust to environmental noise and audio compression, and resistant to removal by adversaries. In 2026, several hardware manufacturers are experimenting with ultrasonic acoustic fingerprints—high-frequency signals above 20 kHz that are inaudible to humans but detectable by the voice application. The fingerprint is embedded in the analog audio signal by the microphone and verified by the application after digitization. Early results show that the fingerprint survives MP3 compression, background noise up to 60 decibels, and acoustic echo cancellation, but it is removed by downsampling to 16 kHz, which is still common in telephony applications.

Acoustic fingerprinting will likely become standard in high-security voice applications—banking, healthcare, government services—where the cost of deploying fingerprint-capable hardware is justified by the risk of adversarial attacks. For consumer applications, the defense will remain software-based: anomaly detection, speaker recognition, and ensemble transcription.

## The Attacker's Advantage and the Defender's Response

Adversarial audio attacks are asymmetric. The attacker needs to succeed once. The defender needs to succeed every time. The attacker can test attacks offline, iterating until the attack works. The defender must deploy defenses that work across billions of inputs from millions of users in unpredictable environments. The attacker can specialize—targeting one ASR model, one device type, one acoustic environment. The defender must generalize—protecting against all known attacks and anticipating unknown ones.

This asymmetry is why adversarial audio defense is an ongoing arms race. Each new attack technique is followed by a new detection method. Each new detection method is followed by an adapted attack that evades it. The 2025 adversarial perturbation attacks were mitigated by adversarial training in 2026 ASR models. By mid-2026, attackers were already demonstrating second-order perturbations that evaded adversarially trained models. The defense community responded with ensemble methods and spectral filtering. The cycle continues.

Your role as the safety team is not to eliminate adversarial audio attacks—you cannot. Your role is to make attacks difficult enough that they are not economically viable for most adversaries, and to detect the attacks that do occur before they cause harm. That requires investing in defenses that most voice teams still consider optional: multi-channel microphone arrays, real-time spectral analysis, speaker recognition, and audio input attestation. It requires monitoring for novel attack patterns in production and updating defenses continuously. And it requires red team protocols that simulate adversarial audio attacks before real adversaries deploy them. That is the next layer of the safety infrastructure, and it is where most voice teams still have gaps.

---

