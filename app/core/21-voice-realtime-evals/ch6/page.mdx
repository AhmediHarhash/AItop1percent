# Chapter 6 — Conversational Quality: Beyond Component Metrics

A voice agent can have excellent ASR accuracy, low latency, and natural TTS — and still fail completely because the conversation does not flow. The user tries to book an appointment but the agent keeps asking for information already provided. The user asks a question but the agent answers something adjacent, forcing multiple clarifications. The user gives up after seven turns when the task should have taken two.

Component metrics tell you whether each piece works. Conversational quality metrics tell you whether those pieces combine into something a human would tolerate using. This is the layer where most voice AI systems fail. They measure latency and WER religiously while ignoring the fact that 60% of callers abandon before completing their task. This chapter defines the metrics that actually predict whether your voice agent succeeds or drives users away.

---

- 6.1 — Task Success Rate: The Ultimate Voice Metric
- 6.2 — First Call Resolution: Measuring Outcome, Not Output
- 6.3 — Turns to Completion: Efficiency in Dialogue
- 6.4 — Topic Coherence: Maintaining Context Across Turns
- 6.5 — Intent Recognition Accuracy in Spoken Conversations
- 6.6 — Slot Filling and Entity Extraction in Voice
- 6.7 — Clarification Request Appropriateness
- 6.8 — The Repetition Problem: When the Agent Keeps Asking
- 6.9 — Conversation Abandonment: Exit Signal Analysis
- 6.10 — User Satisfaction Correlation: What Predicts Happy Callers
- 6.11 — Multi-Turn Evaluation Datasets and Protocols
- 6.12 — Trace-Level vs Session-Level Evaluation
- 6.13 — Human Evaluation Protocols for Conversational Quality

---

*Perfect ASR does not guarantee a good conversation. The next thirteen subchapters define what does.*
