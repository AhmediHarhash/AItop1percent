# 13.11 — Recovery Metrics: Measuring What Matters

If you cannot measure recovery, you cannot improve it. The metrics that matter are not the ones that make dashboards look good. They are the ones that reveal whether users are protected when failures happen, whether recovery is fast enough to preserve conversation flow, and whether the cost of recovery is sustainable at scale. In April 2025, a financial services voice platform proudly reported 99.7% availability. The executive team celebrated. The engineering team knew the number was misleading. Availability was measured as "percentage of time the service responded to health checks." What it did not measure: how many conversations failed mid-turn, how many recoveries lost user context, how long users waited during recovery attempts, or how many users gave up and hung up after recovery felt too slow. The platform was "available" in the technical sense while delivering a poor user experience during the 0.3% of time it was recovering from failures. The metrics measured uptime. They did not measure recovery quality.

Recovery metrics must capture four dimensions: success rate, latency, user perception, and cost. A recovery mechanism that succeeds 95% of the time but takes eight seconds to recover has failed from the user's perspective. A recovery mechanism that succeeds instantly but costs $4 per recovery is unsustainable at 10,000 recoveries per day. A recovery mechanism that preserves technical state but loses conversational context has succeeded technically but failed experientially. All four dimensions matter. Measuring only one gives a distorted picture.

## Recovery Success Rate by Failure Type

Not all failures are equally recoverable. ASR failures caused by background noise might be unrecoverable — the audio is genuinely unintelligible, and retrying will produce the same null result. LLM timeouts caused by transient network issues are highly recoverable — a retry after 200 milliseconds will likely succeed. TTS failures caused by invalid input are unrecoverable without fixing the input. Measuring overall recovery success rate obscures these differences. Measuring recovery success rate per failure type reveals which failures the system handles well and which failures expose gaps.

ASR recovery success rate should be segmented by failure cause. If ASR fails due to low audio volume, recovery via volume normalization should succeed at high rates — ideally above 90%. If ASR fails due to unsupported language, recovery via language detection and routing to a multilingual model should succeed above 80%. If ASR fails due to crosstalk or multiple speakers, recovery is harder — success rates of 50% might be realistic. Tracking these segments shows where to invest. If unsupported language failures have 40% recovery success, the language detection model needs improvement.

LLM recovery success rate should distinguish timeout failures from content policy violations. Timeout failures are usually recoverable through retry, provider switch, or model downgrade. Content policy violations — the user's input triggered a safety filter — are not recoverable through retry. The input itself is the problem. If your LLM recovery success rate is 60%, and you discover that 50% of failures are content policy violations, the recovery logic is not the issue. The input validation and user guidance are the issue. Measuring the split allows correct prioritization.

TTS recovery success rate should track fallback effectiveness. If the primary TTS provider fails and the system falls back to a secondary provider, what percentage of fallbacks succeed? If the answer is 95%, the fallback is working. If the answer is 40%, the secondary provider is unreliable and should be replaced. If TTS failures are caused by malformed input — perhaps the LLM generated text with invalid SSML tags — recovery requires input sanitization, not provider retry. Segmenting by root cause shows whether the recovery mechanism matches the failure type.

Session recovery success rate measures how often disconnected users successfully reconnect and resume their conversation. If 10,000 users experience network disconnections per day and 9,500 successfully reconnect with full context, session recovery success rate is 95%. The 5% who fail to reconnect either could not establish a new connection, encountered an authentication error, or found that their session had expired. Drilling into the 5% failure cases reveals whether the issue is network reliability, authentication system capacity, or session timeout policies.

Recovery success trends over time show whether the system is improving or degrading. A recovery success rate that starts at 85% in January and climbs to 92% by June indicates that improvements — better retry logic, more reliable fallback providers, refined error categorization — are working. A success rate that starts at 90% and declines to 78% indicates that new failure modes are emerging faster than recovery mechanisms are adapting. Perhaps a new LLM provider was introduced with different failure characteristics. Perhaps user behavior shifted toward longer conversations where session recovery is harder. Trend analysis identifies whether recovery quality is on the right trajectory.

## Recovery Latency: How Long Did It Take

Recovery latency is the time between failure detection and successful recovery. For voice systems, this duration directly determines user experience. A recovery that completes in 300 milliseconds feels like a brief pause. A recovery that takes three seconds feels like the system hung. A recovery that takes 10 seconds prompts the user to hang up. Recovery latency must be measured at percentiles, not averages, because the tail determines user perception.

P50 recovery latency shows typical recovery performance. If median recovery latency is 400 milliseconds, most users experience recovery as a short pause. If median recovery latency is 2,000 milliseconds, most users notice something went wrong. P50 is a useful baseline, but it is insufficient alone because it hides the worst cases.

P95 and P99 recovery latency reveal the user experience for the unlucky minority. If p95 recovery latency is 5,000 milliseconds, 5% of users who experience failures wait five seconds or more for recovery. That 5% will remember the experience, complain to support, and possibly churn. If p99 recovery latency is 12,000 milliseconds, 1% of users wait over 12 seconds, which is likely longer than they are willing to wait. They hang up. P99 latency sets the floor for the worst acceptable user experience.

Recovery latency by failure type shows which recovery paths are fast and which are slow. ASR retry recovery might average 300 milliseconds because it only requires resending the audio to the ASR provider. LLM provider failover might average 1,200 milliseconds because it requires establishing new connections, re-sending context, and waiting for a cold inference run on the new provider. TTS fallback might average 600 milliseconds because cached phoneme data reduces synthesis time on the fallback provider. If TTS fallback is consistently slower than ASR retry, and TTS failures are more common, you know where to optimize.

Recovery latency decomposition breaks total recovery time into stages: detection time, decision time, execution time, validation time. Detection time is how long it takes to recognize that a failure occurred. Decision time is how long it takes to choose a recovery strategy. Execution time is how long the recovery action itself takes — retrying a request, switching providers, retrieving context from a checkpoint. Validation time is how long it takes to confirm recovery succeeded. If total recovery latency is 2,000 milliseconds and 1,500 milliseconds of that is execution time, the bottleneck is the recovery action. If 1,200 milliseconds is detection time, the monitoring and alerting logic is too slow.

Recovery latency under load reveals whether recovery performance degrades when the system is stressed. In normal conditions, recovery latency might be 400 milliseconds at p95. Under 3x normal load, p95 recovery latency might climb to 2,000 milliseconds because retry queues are full, fallback providers are slower due to load, and checkpoint retrieval contends with other database operations. Load testing recovery paths ensures that recovery remains fast enough even when the system is already under pressure — which is exactly when failures are most likely to occur.

## User Perception of Recovery: Did They Notice

Technical recovery success does not guarantee perceived recovery success. A system can recover perfectly from a technical perspective — state preserved, conversation resumed, no data lost — while still creating a jarring user experience. User perception metrics measure whether recovery felt seamless, noticeable, or broken from the user's point of view.

Explicit user feedback is the most direct signal. After a conversation that included a recovery event, ask the user: "Did you experience any issues during this call?" If the user says no, recovery was imperceptible. If the user says yes, recovery was noticeable. If the user describes the issue accurately — "the system paused for a moment" — recovery was smooth but visible. If the user describes confusion or frustration — "I had to repeat myself three times" — recovery failed from a UX perspective. Explicit feedback can be collected via post-call surveys, star ratings, or follow-up prompts.

Implicit behavioral signals reveal user perception without asking directly. If the user hangs up within five seconds of a recovery event, recovery likely failed. If the user repeats their last utterance immediately after recovery, they perceived that the system lost context. If the user says "Are you still there?" or "Did you hear me?" after recovery, they interpreted the pause as a connection loss. These phrases can be detected via transcript analysis. High rates of confirmation-seeking behavior after recovery indicate that recovery is not smooth enough.

Conversation abandonment rate after recovery shows how often users give up following a failure. If 1,000 conversations experience a failure and recovery event, and 100 of those conversations end within 30 seconds of recovery, the abandonment rate is 10%. Compare this to the baseline abandonment rate for conversations that did not experience failures. If baseline abandonment is 2%, recovery is adding 8 percentage points of abandonment. That gap is the cost of noticeable recovery.

Repeat rate after recovery measures how often users must repeat information post-recovery. If the system successfully recovers context, the user should not need to repeat what they said before the failure. If transcript analysis shows that 30% of users re-state their request after recovery, context preservation is failing. Either the checkpoint did not restore correctly, or the system did not signal to the user that context was preserved. Either way, the user perceived recovery as incomplete.

NPS and CSAT scores segmented by recovery events compare satisfaction for users who experienced recovery versus users who did not. If users who experienced recovery have an NPS of 40 and users who did not have an NPS of 65, recovery is damaging satisfaction by 25 points. If the gap is only five points, recovery is reasonably smooth. If users who experienced recovery have higher NPS than users who did not — unlikely but possible — the recovery messaging and reassurance are so effective that they build trust.

## Recovery Cost: Retries, Fallbacks, Escalations

Every recovery action has a cost: API calls, compute resources, human escalation time, degraded user experience. Recovery cost must be measured to ensure that the recovery strategy is sustainable at scale. A recovery mechanism that costs $2 per event is fine if failures happen 100 times per day. It is financially ruinous if failures happen 50,000 times per day.

Retry cost is the most direct. Every retry consumes an API call, compute time, and network bandwidth. If ASR retry costs $0.02 per minute of audio and the system retries 10,000 ASR failures per day, retry cost is $200 per day. If LLM retry costs $0.10 per request and the system retries 5,000 LLM timeouts per day, retry cost is $500 per day. These costs are incremental — they add to the baseline cost of successful requests. Tracking retry cost separately from base cost shows how much the system is spending to recover from failures versus serving successful requests.

Fallback provider cost often differs from primary provider cost. The primary LLM provider might charge $0.08 per request. The fallback provider might charge $0.15 per request. Every failover from primary to fallback adds $0.07 in cost. If 8,000 requests per day fail over, fallback cost is $560 per day. If the fallback provider is cheaper — perhaps $0.05 per request — failover actually reduces cost by $0.03 per request. Some teams deliberately use a cheaper fallback provider, accepting slightly lower quality during recovery in exchange for cost savings. Measuring the cost difference allows informed trade-offs.

Model downgrade cost is measured in quality degradation rather than dollars. If the primary LLM is Claude Opus 4.5 and the fallback is Claude Haiku 4.5, the cost is the quality gap between Opus and Haiku. If downgrade happens for 3% of requests, and Haiku's task performance is 8 percentage points lower than Opus, the overall quality degradation is 0.24 percentage points. If quality SLAs require 92% task success and the system delivers 91.76% due to downgrades, recovery is pushing the system below SLA. The cost is contractual risk, not just user experience.

Human escalation cost is the highest. If recovery fails and the conversation is escalated to a human agent, the cost includes agent time, tooling overhead, and context transfer inefficiency. A human agent costs $25 per hour. If escalated calls average eight minutes, each escalation costs $3.33 in labor. If 500 conversations per day escalate, human escalation cost is $1,665 per day. Reducing escalation rate by improving automated recovery directly reduces labor cost.

Total cost of recovery as a percentage of base operational cost shows whether recovery spending is proportional. If the system spends $10,000 per day on base operations — ASR, LLM, TTS for successful requests — and $1,200 per day on recovery costs, recovery is 12% of base cost. If recovery cost climbs to $4,000 per day while base cost remains steady, recovery is 40% of base cost. At that point, reducing failure rate or improving recovery efficiency becomes a top financial priority, not just a technical goal.

## Building Recovery Dashboards

Recovery metrics must be visible in real time. Dashboards should show recovery success rate, latency, user perception signals, and cost, segmented by failure type, time of day, and system component. Engineers need to see when recovery degrades before users complain. Product teams need to see how recovery affects satisfaction and retention. Finance teams need to see how recovery affects margins.

The recovery overview panel shows aggregate metrics: total failures in the last hour, recovery success rate, median recovery latency, and total recovery cost. These four numbers give a quick sense of whether recovery is healthy. If recovery success rate drops from 90% to 70%, something changed. If median recovery latency jumps from 400 milliseconds to 1,800 milliseconds, a bottleneck appeared. If recovery cost doubles in one day, either failure rate spiked or a more expensive fallback provider is being used.

The failure type breakdown panel shows recovery metrics per failure category: ASR failures, LLM timeouts, TTS errors, network disconnections, authentication failures. Each category displays success rate, latency at p50 and p95, and cost per recovery. This segmentation reveals which failure types are well-handled and which need improvement. If ASR recovery success is 95% but authentication recovery success is 50%, focus recovery improvement efforts on authentication.

The time-series trend panel shows recovery metrics over the past 24 hours, seven days, and 30 days. Trends reveal whether recovery is improving, stable, or degrading. A gradual decline in recovery success rate over three weeks suggests that a slow-moving issue — perhaps increasing user complexity, changing traffic patterns, or provider reliability degradation — is affecting recovery. A sudden spike in recovery latency on a specific day suggests an incident that should be investigated.

The user impact panel shows metrics tied to user experience: conversation abandonment rate after recovery, repeat rate after recovery, NPS/CSAT segmented by recovery events, and percentage of users who experienced recovery in the past 24 hours. This panel keeps the team focused on outcomes, not just technical metrics. High recovery success rate is meaningless if users are still abandoning conversations after recovery.

The cost panel shows retry costs, fallback costs, downgrade costs, and escalation costs, broken down by component and provider. If retry costs are climbing due to one specific LLM provider, the team can investigate whether that provider's reliability is declining or whether retry logic is misbehaving. If escalation costs are spiking, the team knows that automated recovery is failing more often and human intervention is increasing.

## Alerting on Recovery Degradation

Metrics are only useful if they trigger action. Alerts should fire when recovery degrades beyond acceptable thresholds, giving teams time to investigate and fix before user impact becomes severe.

Recovery success rate alerts fire when success rate for any failure type drops below a threshold — say, 85% — sustained over five minutes. This indicates that a recovery mechanism that usually works is now failing. The alert should include the failure type, current success rate, historical baseline, and affected request count. Engineers can investigate whether the failure type changed — perhaps ASR is encountering a new audio codec — or whether the recovery mechanism broke — perhaps the fallback provider is down.

Recovery latency alerts fire when p95 or p99 latency exceeds user-acceptable thresholds. If p95 recovery latency exceeds 2,000 milliseconds, users are experiencing multi-second pauses. The alert should include the failure type, current latency percentiles, and a link to traces showing slow recovery examples. Engineers can identify bottlenecks — slow checkpoint retrieval, slow provider failover, slow context re-establishment.

Recovery cost alerts fire when cost per recovery exceeds budget or when total daily recovery cost exceeds a threshold. If retry cost suddenly doubles, either failure rate doubled or the retry logic is firing more aggressively than intended. If fallback cost spikes, the primary provider might be failing more often, triggering expensive failovers. Cost alerts ensure that recovery does not silently consume budget that was allocated for feature development.

User impact alerts fire when conversation abandonment rate after recovery exceeds baseline by a significant margin — say, 5 percentage points. This indicates that users are noticing recovery and choosing to end the conversation rather than continue. The alert should trigger investigation into what changed. Did recovery latency increase? Did context preservation break? Did the recovery messaging become unclear?

---

Recovery metrics transform recovery from a best-effort engineering task into a measurable, improvable capability. Success rate shows whether recovery works. Latency shows whether it is fast enough. User perception shows whether it is smooth enough. Cost shows whether it is sustainable. Together, these metrics allow teams to prioritize recovery improvements with the same rigor they apply to feature development.

Next: **13.12 — Training Teams for Voice Incident Response**, where recovery is not just code — it is people making fast decisions.

