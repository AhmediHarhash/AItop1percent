# 8.12 — Safety Monitoring in Production Voice Deployments

A voice safety system that works perfectly in testing can fail silently in production. In March 2025, a customer service voice agent deployed with a comprehensive safety filter that had passed red team evaluation and adversarial testing. For the first two weeks, safety incident rates were within expected bounds—less than 0.1% of calls triggered safety flags. In week three, the incident rate began climbing. By week five, it had reached 0.8%. The safety team did not notice until week seven, when a user complaint escalated to executive leadership. The system had been producing progressively more unsafe responses for a month, and no one had been watching the right metrics.

The failure was not in the safety filter. The filter was working exactly as designed. The failure was in the data distribution. The user population in production was different from the user population in testing. Production users had stronger accents, used more colloquial language, and asked questions in domains the test set did not cover. The ASR transcription error rate in production was 40% higher than in testing. Transcription errors caused the safety filter to miss queries it would have caught if the transcription had been accurate. The system degraded gradually, and without real-time monitoring, the degradation was invisible until it caused user harm.

## Real-Time Safety Dashboards: What Metrics to Show

Production safety monitoring for voice systems requires a dashboard that surfaces the right signals in real time. The metrics that matter are not the same as the metrics that matter for text systems, because voice systems have additional failure modes—ASR errors, speaker recognition errors, emotional tone misclassification, timing failures—that text systems do not.

The first metric is safety incident rate: the percentage of calls that triggered any safety flag. This is the headline number. If the baseline rate is 0.1% and the current rate is 0.5%, something changed. The dashboard shows this metric with a 24-hour moving average and a seven-day trend line. A sudden spike indicates an acute issue—a new attack, a bad model deployment, a configuration error. A gradual upward trend indicates a chronic issue—data drift, model degradation, or an emerging attack pattern that is spreading.

The second metric is safety incident rate by category. Not all safety incidents are equal. A call flagged for potential fraud is different from a call flagged for harmful content. A call flagged for self-harm risk is different from a call flagged for abusive language directed at the agent. The dashboard breaks down incident rate by category—fraud, harmful content, self-harm, abuse, privacy violations, policy violations—and shows the trend for each. A spike in fraud flags suggests an attacker campaign. A spike in self-harm flags suggests a change in user population or a news event driving crisis calls. A spike in abuse flags suggests a product change that is frustrating users.

The third metric is ASR transcription confidence. Low ASR confidence correlates with safety filter failures. If the ASR transcribes a harmful query incorrectly, the safety filter never sees the harmful content. The dashboard shows the distribution of ASR confidence scores across all calls and flags calls where confidence is below 0.7 on high-risk queries. These calls are routed to human review, because low confidence transcription is a leading indicator of missed safety incidents.

The fourth metric is safety filter latency. The safety filter must run in real time, but real time has a budget. If the filter takes 300 milliseconds to classify a query, and the total response budget is 500 milliseconds, the filter is consuming 60% of the latency budget. If filter latency increases—due to model drift, increased query complexity, or infrastructure issues—the system either exceeds latency targets or skips the filter to stay within budget. Both outcomes are safety failures. The dashboard tracks P50, P95, and P99 filter latency and alerts if P95 exceeds the target.

The fifth metric is call termination rate by reason. Every time the system terminates a call, the reason is logged—safety policy violation, fraud detection, user request, technical failure. The dashboard shows termination rate by reason with a 24-hour and seven-day trend. A spike in safety-driven terminations indicates that the system is encountering more high-risk interactions. A spike in technical failure terminations indicates an infrastructure issue. A spike in user-requested terminations indicates a product experience issue that is causing users to hang up.

The sixth metric is false positive rate on safety flags. Not every flagged call is a genuine safety incident. Some are false positives—benign queries that triggered the filter incorrectly. False positives are measured by sampling flagged calls and reviewing them manually. The dashboard shows false positive rate as a percentage of all flagged calls, updated weekly. A rising false positive rate indicates that the filter is becoming too sensitive, which degrades user experience and wastes human reviewer time.

## Alerting Hierarchies: P1, P2, and P3 Thresholds

Not every safety incident requires immediate human attention. The alerting system is tiered to match incident severity with response urgency. P1 alerts require immediate action—someone must respond within 15 minutes. P2 alerts require action within one hour. P3 alerts are reviewed in the next business day.

P1 alerts are triggered by safety incidents that pose immediate risk of harm or immediate risk of regulatory violation. A call where the agent provided advice that could cause physical injury. A call where the agent disclosed personally identifiable information without authorization. A call where the agent executed a financial transaction without proper authentication. A call where the system detected a credible threat of violence or self-harm and failed to escalate. These are never-events—they should not happen, and when they do, the response is urgent.

The P1 alerting threshold is calibrated to fire on high-confidence, high-severity incidents. The system does not fire P1 alerts on ambiguous cases, because false P1 alerts erode trust and cause alert fatigue. One enterprise voice team initially set the P1 threshold too low and generated 15 P1 alerts per week, most of which were false positives. The on-call team began ignoring P1 alerts. The threshold was recalibrated to fire only on incidents where automated review confidence exceeded 0.95 and severity was rated critical by the safety taxonomy. P1 alert volume dropped to fewer than two per week, and response time improved because the on-call team trusted that P1 alerts were genuine.

P2 alerts are triggered by safety incidents that are serious but not immediately harmful, or by patterns that suggest an emerging threat. A call where the agent gave incorrect information on a safety-critical topic but the user did not act on it. A call where the safety filter flagged a query but the system generated a borderline response. A series of calls from the same user or the same phone number that show escalating risk behavior. A spike in safety incident rate that exceeds baseline by more than 2 standard deviations. These require investigation but not immediate response.

P2 alerts are reviewed by the safety on-call within one hour. The reviewer listens to the call recording, reads the transcript, examines the ASR confidence scores and safety filter outputs, and determines whether the incident requires escalation to P1, whether it requires a policy or model update, or whether it is a known edge case that does not require action. P2 alerts generate the majority of the safety team's reactive work.

P3 alerts are batch notifications of trends or low-severity incidents. A weekly report of all calls flagged for minor policy violations. A daily digest of calls with ASR confidence below 0.6. A monthly summary of false positive safety flags. These are reviewed during business hours by the safety team as part of continuous improvement work. P3 alerts do not require immediate action, but they inform long-term decisions about model retraining, policy updates, and infrastructure investment.

## Call Sampling Strategies for Safety Review

You cannot review every call. At scale, a voice system handles thousands or millions of calls per day. Human review must be selective. The sampling strategy determines which calls are reviewed and which are trusted to automated monitoring.

The first sampling tier is automated flagging: every call that triggers a safety flag is reviewed. This is non-negotiable. If the system thought the call was risky, a human verifies whether the system was correct. Automated flagging typically captures 0.1% to 1% of total calls, depending on how sensitive the safety filter is calibrated.

The second sampling tier is random sampling of unflagged calls. Even if the safety filter did not flag a call, the call may still contain safety issues the filter missed. Random sampling catches these blind spots. One enterprise voice team samples 0.5% of all unflagged calls daily—roughly 500 calls per day at their scale. Reviewers listen to the calls and mark any that should have been flagged but were not. The false negative rate—safety incidents the filter missed—is calculated from this sample. If the false negative rate exceeds 2%, the filter is recalibrated or retrained.

The third sampling tier is stratified sampling by risk category. Some call types are higher risk than others. Calls involving financial transactions, healthcare advice, or access to sensitive data are sampled at higher rates than calls involving general customer service questions. Calls from new users are sampled at higher rates than calls from established users with clean histories. Calls with low ASR confidence are sampled at higher rates than calls with high confidence. Stratified sampling ensures that the highest-risk segments of the call population receive disproportionate review attention.

The fourth sampling tier is anomaly-driven sampling. If a call has unusual characteristics—extremely long duration, high emotional arousal, repeated safety filter triggers that did not result in escalation, unusual speaker recognition patterns—it is flagged for review even if it did not trigger a safety policy. Anomalies are often leading indicators of novel attacks or edge cases the system was not designed to handle.

The sampling rate is calibrated to the available reviewer capacity. If the safety team has five full-time reviewers and each reviewer can evaluate 40 calls per hour, total capacity is 1,600 calls per eight-hour shift. If automated flagging produces 800 calls per day, the team has capacity for 800 additional sampled calls. The sampling strategy allocates that capacity across random, stratified, and anomaly-driven sampling based on where the team expects to find the most valuable signal.

## Trend Detection: Rising Safety Incident Rates

The dashboard shows incident rate trends, but detecting a meaningful trend requires distinguishing signal from noise. Safety incident rates vary naturally due to time of day, day of week, seasonality, and news events. A spike on Monday morning may be normal. A spike that persists for three days is not.

Trend detection uses statistical process control—comparing current incident rates against historical baselines and flagging deviations that exceed expected variance. The baseline is calculated from the previous 30 days of data, excluding known anomalies like product launches, major news events, or planned system changes. The system calculates the mean incident rate and the standard deviation. If the current 24-hour incident rate exceeds the baseline mean by more than 2 standard deviations, an alert is triggered.

The challenge is distinguishing between a genuine trend and a temporary spike caused by a single high-volume attacker or a news event. A single attacker making 200 calls in one hour can spike the incident rate for that hour, but the spike disappears when the attacker stops. A news event—a celebrity scandal, a natural disaster, a political crisis—can drive a wave of calls that include more emotional distress, more crisis language, and more safety-sensitive topics than normal. The incident rate spikes, but the spike is not a system failure—it is a legitimate change in user behavior.

To distinguish, the system tracks not just incident rate but also incident diversity. If the spike is driven by 200 calls from the same phone number all asking the same query, it is an attacker. If the spike is driven by 200 calls from 200 different phone numbers asking 50 different queries, it is a population shift or a news event. The response is different. An attacker spike triggers fraud mitigation—blocking the phone number, throttling call volume from suspicious sources. A population spike triggers capacity planning—scaling up human reviewer capacity, adjusting safety filter sensitivity, preparing crisis escalation pathways.

## Post-Incident Analysis for Voice Safety Failures

When a safety incident reaches production and causes harm, the post-incident process begins. The goal is not to assign blame. The goal is to understand how the system failed and to ensure the same failure does not happen again.

The post-incident analysis team includes representatives from the safety team, the product team, the infrastructure team, and legal. The analysis follows a structured format. First, the timeline: when did the incident occur, when was it detected, when was it escalated, when was it resolved. Second, the root cause: what specific failure in the system allowed the unsafe interaction to happen. Third, the contributing factors: what environmental, architectural, or process conditions made the root cause possible. Fourth, the corrective actions: what changes to models, policies, infrastructure, or processes will prevent recurrence. Fifth, the follow-up: who is responsible for each corrective action and what is the deadline.

In April 2025, a healthcare voice agent provided incorrect medication dosage information to a user. The incident was detected three hours later during routine call sampling. The post-incident analysis found that the root cause was an ASR transcription error. The user asked about "50 milligrams" but the ASR transcribed it as "15 milligrams." The language model responded based on the incorrect transcription. The safety filter did not catch the error because the response was within safe bounds for 15 milligrams—the error was in the dosage, not in the safety of the advice given for that dosage.

The contributing factors were: the ASR model had higher error rates on numeric speech than on general speech, the safety filter did not verify that dosage information matched the user's query, and the system had no mechanism to ask the user to confirm numeric inputs before generating medical advice. The corrective actions were: retrain the ASR model on a dataset augmented with numeric speech examples, add a safety policy requiring confirmation of numeric inputs in medical contexts, and implement a post-response check that compared the response's numeric content against the query's numeric content and flagged mismatches for review. All three corrective actions were deployed within two weeks. The incident type has not recurred in the six months since.

Post-incident reports are shared with the entire engineering and product organization, not just the safety team. Transparency about failures builds a safety culture. It signals that finding and fixing failures is valued more than hiding them. It ensures that knowledge about edge cases and failure modes spreads across teams, so that future designs incorporate lessons from past incidents.

## Building the Safety On-Call Rotation

Production voice safety requires 24/7 coverage. Safety incidents do not wait for business hours. The on-call rotation is staffed by members of the safety team, the trust and safety team, and senior engineers who understand the system architecture and can debug incidents in real time.

On-call responsibilities include responding to P1 and P2 alerts, triaging safety incidents, making real-time decisions about whether to escalate incidents to legal or executive leadership, and coordinating with infrastructure on-call if the incident requires system changes. The on-call engineer has access to all call recordings, transcripts, logs, and dashboards. They have authority to disable features, throttle traffic, or escalate incidents without waiting for approval.

The on-call rotation is typically one week per person. Handoff happens at a scheduled time with a written summary of ongoing incidents, trends observed during the previous week, and any known issues that require monitoring. The outgoing on-call engineer briefs the incoming engineer in a 15-minute call. The written summary is stored in a shared document so that the full on-call history is searchable.

On-call engineers are trained on the safety incident taxonomy, the escalation protocols, the legal and regulatory constraints, and the technical architecture of the safety monitoring system. Training includes simulated incidents where the engineer must respond to a P1 alert, investigate the root cause, decide whether to escalate, and document the incident. The simulation is timed. If the engineer cannot complete the response within 15 minutes, they receive additional training before joining the rotation.

On-call is compensated. Engineers receive additional pay for on-call weeks and additional pay for each incident they respond to outside business hours. The compensation signals that on-call work is valued and that the organization takes safety seriously enough to invest in 24/7 coverage.

## Continuous Improvement: Feeding Production Incidents Back Into Red Team Protocols

Every production safety incident is a red team finding that was missed during pre-deployment testing. The incident reveals an attack vector, an edge case, or a failure mode that the red team did not anticipate. The continuous improvement loop takes that finding and feeds it back into the red team protocol so that future systems are tested for the same vulnerability before they reach production.

In mid-2025, a production voice agent experienced a barge-in exploit where a user interrupted a legal disclaimer at a precise moment and forced the system to proceed with a high-risk action without user confirmation. The incident was detected, analyzed, and mitigated within 24 hours. The safety team documented the attack in the red team protocol as a new test scenario: "Interrupt legal disclaimers at multiple points and verify the system does not proceed without explicit confirmation." Every subsequent voice system deployed by that organization was tested against that scenario before launch.

The feedback loop is formalized. Every production incident generates a red team update ticket. The ticket includes the attack transcript, the failure mode, and the recommended test scenario. The red team reviews the ticket weekly and incorporates new scenarios into the protocol. The protocol is versioned. Each version includes a changelog documenting which scenarios were added based on production incidents.

This loop ensures that the red team protocol evolves in response to real-world adversarial behavior, not just theoretical threats. The attackers are constantly probing for weaknesses. The red team protocol must learn from those probes and ensure that the next system is harder to exploit than the last.

## The Observability Stack for Voice Safety at Scale

Production safety monitoring at scale requires infrastructure. The observability stack includes real-time logging, metrics aggregation, anomaly detection, alerting, dashboards, and long-term storage for compliance and post-incident analysis.

Real-time logging captures every call's metadata: user ID, call duration, ASR transcription, ASR confidence scores, safety filter outputs, emotional tone scores, speaker recognition outputs, and any escalation or termination actions. Logs are structured in JSON format and streamed to a centralized logging service. The logging service is indexed for fast search—an engineer investigating an incident can query by user ID, phone number, time range, safety flag type, or keyword in the transcript.

Metrics aggregation computes statistics across all calls in real time. Incident rate, false positive rate, ASR confidence distribution, filter latency, termination rate—all are calculated continuously and exposed via a metrics API. The dashboard queries this API and renders the data with 5-second refresh intervals.

Anomaly detection runs statistical models on the metrics stream and flags deviations from baseline. The models are tuned to minimize false positives—an anomaly alert should be actionable, not noise. One enterprise system uses a seasonal ARIMA model to predict expected incident rate based on historical patterns, then alerts if the observed rate deviates from the prediction by more than 10%.

Alerting routes notifications based on severity. P1 alerts go to PagerDuty and trigger an immediate page to the on-call engineer. P2 alerts go to Slack and email. P3 alerts are aggregated into daily digests. The alerting system includes deduplication—if the same issue triggers multiple alerts, only the first alert is sent, and subsequent alerts are batched.

Long-term storage retains all call recordings and transcripts for the period required by regulatory compliance and litigation hold policies. In healthcare, that may be seven years. In financial services, it may be five years. The storage is encrypted at rest and in transit. Access is logged and requires multi-factor authentication. Retention policies are enforced automatically—data older than the retention period is deleted.

The observability stack is tested during pre-deployment chaos engineering exercises. The team simulates incidents—an ASR model failure, a spike in safety incidents, a P1 alert—and verifies that the stack captures the signal, triggers the correct alerts, and provides the data needed for investigation. If the stack fails during the exercise, it is fixed before production deployment.

## The Safety Monitoring Maturity Model

Organizations building voice safety monitoring progress through predictable stages. Stage one is reactive monitoring: the team reviews calls after user complaints. There is no real-time dashboard, no automated alerting, no structured sampling. Safety incidents are discovered days or weeks after they occur. This is where most teams start, and it is insufficient for production deployment at scale.

Stage two is basic real-time monitoring: the team has a dashboard showing incident rate and a basic alerting system. Calls flagged by the safety filter are reviewed within 24 hours. Sampling is random and infrequent. Incidents are detected within hours, not weeks, but the team has limited visibility into trends or emerging threats.

Stage three is comprehensive monitoring: the team has real-time dashboards with multiple metrics, tiered alerting, stratified sampling, trend detection, and post-incident analysis processes. Incidents are detected within minutes. The team can distinguish between signal and noise. Red team findings feed back into the protocol. This is the minimum bar for high-stakes production deployments.

Stage four is predictive monitoring: the team uses machine learning to predict safety incidents before they occur, based on leading indicators like ASR confidence degradation, user behavior changes, or environmental trends. The system flags calls for review not just because they triggered a rule, but because they match patterns associated with past incidents. Predictive monitoring is rare in 2026 but is the direction the most mature organizations are moving.

Your goal is to reach stage three before production launch and to build toward stage four as the system scales. The monitoring infrastructure you build in the first six months will determine whether you can detect and respond to safety failures before they become crises. The next challenge is not just monitoring individual calls—it is scaling the entire voice system to handle real-time audio streaming at global scale without sacrificing latency, quality, or safety. That is the architecture challenge of the next chapter.

---

