# 4.8 — Disfluency Processing: Ums, Ahs, and Restarts

In June 2025, a legal transcription service deployed an ASR system that achieved 96% WER on their test set — professional court reporters reading scripted depositions. Within the first week of production, attorneys complained that transcripts were "filled with garbage." The team reviewed sample recordings and found the issue: real deposition speech contained constant disfluencies. Witnesses said "um" and "uh" between phrases, restarted sentences mid-word, corrected themselves, repeated words for emphasis, and trailed off without finishing thoughts. The test set, built from clean scripted speech, contained almost none of this. The ASR system had never seen disfluency patterns and handled them inconsistently — sometimes transcribing "um" as "um," sometimes as "and," sometimes omitting it entirely, sometimes hallucinating words to smooth over hesitations.

Real human speech is not clean. People hesitate, search for words, restart sentences, correct themselves, and fill pauses with vocalizations like "uh," "um," "like," "you know," and "so." These are called disfluencies, and they occur in 6 to 12% of spontaneous speech, higher under stress or when discussing complex topics. If your ASR system is trained only on clean, fluent speech, it will fail to handle the way people actually talk.

## Disfluency Taxonomy

Disfluencies fall into several categories, each with different implications for ASR and downstream processing. **Filled pauses** are vocalizations like "um," "uh," "er," "ah" that occupy silence. They signal hesitation or cognitive load but carry no semantic content. **Repetitions** occur when a speaker repeats a word or phrase, often unintentionally: "I need to, I need to schedule an appointment." **Restarts** happen when a speaker begins a sentence, stops, and starts over: "Can you transfer me to, uh, actually, can you help me with billing?" **Corrections** are self-repairs where the speaker catches and fixes an error: "I want a flight to Boston, no, sorry, to Baltimore." **Prolongations** are stretched-out words: "I neeeed to speak to a supervisor."

Each type poses different challenges. Filled pauses are acoustically distinct — short vocalizations with characteristic pitch and duration — but must be distinguished from actual words that sound similar. "Um" sounds like the beginning of "umbrella." "Uh" sounds like "a" or the start of "under." Repetitions and restarts create ambiguity about which version of the phrase is the speaker's intent. Corrections require the system to recognize that the first statement is being overridden.

A customer service platform analyzed 10,000 call transcripts for disfluency distribution. Filled pauses appeared in 68% of calls, averaging 4.2 occurrences per minute. Repetitions appeared in 41% of calls, averaging 1.8 per minute. Restarts appeared in 29% of calls, averaging 1.1 per minute. Corrections appeared in 19% of calls, averaging 0.6 per minute. The cumulative effect was that the average call had a disfluency every 8 to 12 seconds. This is normal human speech, not an anomaly.

## How ASR Models Handle Disfluencies

ASR models trained on clean, read speech have limited exposure to disfluencies. The model's language model assigns low probability to sequences like "um uh I need" because it rarely appears in training data. When the acoustic model hears "um," the language model might prefer "and" or "I'm" because those are more common. The result is inconsistent handling: sometimes disfluencies are transcribed, sometimes deleted, sometimes replaced with similar-sounding words.

A medical dictation system measured how often "um" and "uh" were correctly transcribed versus misrecognized. "Um" was transcribed correctly 62% of the time, transcribed as "and" 18% of the time, transcribed as "I'm" 9% of the time, and omitted 11% of the time. "Uh" was transcribed correctly 48% of the time, transcribed as "a" 31% of the time, and omitted 21% of the time. The inconsistency meant that downstream processing could not reliably detect or remove disfluencies.

Restarts and corrections create a different problem. When a speaker says "I want to book a fl— no, a hotel," the ASR system transcribes "I want to book a fl no a hotel" or "I want to book a flight no a hotel." The language model tries to make sense of the fragment "fl" and might expand it to "flight," creating a transcript that says the opposite of what the user intended. The speaker corrected themselves, but the ASR system missed the correction signal and included both the error and the correction.

## Pass-Through Versus Filtering

There are two approaches to disfluencies in transcripts: pass-through and filtering. **Pass-through** transcribes disfluencies exactly as spoken, producing transcripts like "Um I need to uh schedule an appointment." **Filtering** removes disfluencies, producing "I need to schedule an appointment." The right choice depends on the use case.

Pass-through is appropriate when the transcript is a verbatim record — legal depositions, medical dictation, meeting minutes. In these contexts, hesitations and corrections are meaningful. An attorney might use a witness's hesitation as evidence of uncertainty. A physician's self-correction from one diagnosis to another is clinically significant. Removing disfluencies would alter the record.

Filtering is appropriate when the transcript feeds downstream natural language understanding. A voice assistant does not need to process "um" and "uh" — they add no information and can confuse intent detection. A chatbot transcript reader benefits from clean text. A search query parser works better with "coffee shops near me" than "uh coffee shops uh near me."

A healthcare voice assistant tested both approaches. Pass-through transcripts had 14% more words than filtered transcripts. Intent classification accuracy on pass-through transcripts was 79%. Intent classification accuracy on filtered transcripts was 88%. The disfluencies confused the NLU model, which had been trained on clean written text. They deployed filtering for real-time interactions and pass-through for audit logs.

## Training ASR on Disfluent Speech

The most robust solution is to train the ASR model on spontaneous speech that includes disfluencies. This teaches the model to recognize "um" and "uh" as distinct tokens and to handle restarts and corrections naturally. The challenge is data: large-scale ASR training corpora like LibriSpeech are mostly read speech. Spontaneous speech corpora exist — Switchboard, Fisher, CallHome — but are smaller and harder to obtain.

A contact center provider fine-tuned their ASR model on 5,000 hours of real customer service calls, which contained high rates of disfluency. After fine-tuning, the model transcribed "um" correctly 91% of the time, up from 62%. It transcribed "uh" correctly 84% of the time, up from 48%. It also learned to recognize restart patterns — when a speaker abruptly stopped mid-word and started a new phrase, the model produced a transcript with a clear break rather than hallucinating a smooth continuation.

Training on disfluent speech improves pass-through accuracy but does not solve the filtering problem. You still need a separate component to detect and remove disfluencies if your use case requires clean transcripts.

## Disfluency Detection and Removal

If your ASR model passes through disfluencies, you need a post-processing step to detect and remove them. This is a sequence tagging problem: label each word as fluent or disfluent, then delete disfluent words. The challenge is distinguishing genuine disfluencies from similar-sounding fluent speech.

A disfluency detection model must recognize filled pauses ("um," "uh," "er," "ah"), false starts (partial words or abandoned phrases), and repetitions (identical consecutive words or phrases). It also must recognize corrections, where the speaker explicitly negates a previous statement ("no," "actually," "I mean," "sorry").

A legal transcription service built a disfluency detector using a fine-tuned BERT model trained on 20,000 annotated legal depositions. The model tagged each word as fluent or disfluent. It achieved 89% precision and 84% recall on filled pauses, 76% precision and 71% recall on repetitions, and 68% precision and 62% recall on corrections. The lower performance on corrections reflected the difficulty of distinguishing "no" as a correction signal from "no" as an answer.

After deploying disfluency removal, transcript readability improved significantly. Attorneys reported that filtered transcripts were easier to review and search. But the 11% false positive rate — fluent words incorrectly removed — created occasional errors. A witness who said "I did not see the car" sometimes had the transcript read "I did see the car" because "not" was misclassified as a disfluency. The team tuned the detector to minimize false positives on negations, accepting slightly higher false negatives on other disfluency types.

## Handling Restarts and Corrections in Real Time

Restarts and corrections create ambiguity for real-time systems. When a user says "I want to book a fl— no, a hotel," the system hears "flight" in the first phrase, begins processing a flight booking intent, then hears "hotel" in the correction. Should it abandon the flight booking and switch to hotel booking? Should it ask for clarification? Should it wait for the user to finish before acting?

Most real-time voice assistants wait for a complete utterance before processing. The user speaks, the system transcribes, the system detects intent, the system responds. If the user self-corrects mid-utterance, the correction is included in the transcript and the final intent reflects the corrected version. This works if the system does not act until the utterance is complete.

Problems arise in systems that stream partial results and act incrementally. A voice assistant that begins searching for flights as soon as it hears "I want to book a flight" will execute the wrong action if the user corrects to "hotel" mid-sentence. Incremental systems must either delay action until the utterance is confirmed complete or implement correction detection to abort in-progress actions when a correction signal is detected.

A travel booking voice bot implemented correction detection by monitoring for negation words ("no," "not," "actually," "wait," "sorry") after a slot was filled. If a user said "I want to fly to Boston, actually, Baltimore," the system detected "actually" as a correction signal, discarded "Boston," and replaced it with "Baltimore." This reduced booking errors from 9% to 3% in cases where users self-corrected.

## The Impact on Word Error Rate Measurement

Disfluencies complicate WER calculation. If the ground truth transcript includes "um" and "uh" and the ASR transcript omits them, are those errors? If the ground truth is "I want to book a flight no a hotel" and the ASR transcript is "I want to book a hotel," is that an error or an improvement?

The answer depends on whether you are measuring verbatim accuracy or semantic accuracy. For verbatim applications — legal transcription, medical dictation — omitting "um" is an error. For semantic applications — voice assistants, chatbots — omitting "um" is correct. You must define WER relative to the desired output.

A customer service platform created two test sets: a verbatim set where ground truth included all disfluencies, and a semantic set where ground truth had disfluencies removed. They measured WER separately. Verbatim WER was 11%, reflecting the system's imperfect handling of disfluencies. Semantic WER was 6%, reflecting the system's accuracy on fluent content. Both metrics were meaningful, but for different purposes.

Some teams normalize ground truth before WER calculation: remove disfluencies from both ground truth and hypothesis, then measure WER. This isolates ASR performance on fluent content from performance on disfluency handling. It is useful for comparing models but does not reflect real-world accuracy if disfluencies are passed through in production.

## Disfluency Patterns Across Speakers and Contexts

Disfluency rates vary by speaker and context. Speakers under stress, discussing complex topics, or thinking aloud produce more disfluencies. Scripted or rehearsed speech has fewer. Native speakers have different disfluency patterns than non-native speakers, who might use filled pauses from their first language or hesitate more often due to lower fluency.

A healthcare ASR system measured disfluency rates across patient demographics. Patients aged 18-35 had disfluency rates of 7%. Patients aged 65+ had disfluency rates of 11%. Non-native English speakers had disfluency rates of 14%. Patients calling about acute conditions (chest pain, injury) had disfluency rates of 9%. Patients calling about chronic conditions (medication refills, routine questions) had disfluency rates of 6%. The variation was large enough to affect overall WER across demographics.

If your user base has high disfluency rates, your test set must reflect that. Testing on clean speech and deploying to users who hesitate frequently will underestimate real-world WER.

## Prosodic Cues for Disfluency Detection

Disfluencies are not just lexical — they have prosodic markers. Filled pauses are often followed by a brief silence. Restarts often feature an abrupt pitch change or energy drop before the speaker starts over. Corrections are frequently preceded by a lengthening of the previous word, as the speaker realizes their mistake mid-utterance.

Advanced disfluency detectors use prosodic features — pitch, duration, energy, pause length — in addition to lexical features. A model that sees only the words "I want to book a flight no a hotel" might struggle to detect the correction. A model that also sees the pitch drop and pause before "no" recognizes the correction signal more reliably.

A legal transcription provider integrated prosodic features into their disfluency detector. Correction detection recall improved from 62% to 74%. Restart detection recall improved from 58% to 69%. The improvement came from better recognition of hesitation and revision signals in the audio, which are invisible in text-only models.

Prosodic features require access to the audio signal or to acoustic features from the ASR model. If you only have the final transcript, you cannot use prosody. But if you have access to the ASR decoder's internal state or to the original waveform, prosodic cues are a valuable signal.

## Disfluency Handling in Multi-Turn Dialogues

Disfluencies occur not just within utterances but across turns. A user might start a question, stop, and continue in the next turn: "I want to book a—" (pause) "Sorry, can you help me with hotels?" The restart spans two user turns. A system that treats each turn independently might process the first turn as an incomplete request and fail.

Multi-turn disfluency handling requires the system to recognize that the second turn is a restart or correction of the first. Cue words like "sorry," "wait," "actually," "no" at the start of a turn are strong signals. If the second turn begins with a negation or restart cue, the system should discard the incomplete previous turn and treat the new turn as the user's intent.

A customer service chatbot implemented cross-turn correction detection. If the user said "I need to—" and then "Actually, can you transfer me to billing?" the system recognized "Actually" as a correction cue and discarded the incomplete first turn. This reduced confusion when users changed their mind mid-request.

## User Perception of Disfluency Handling

Users do not notice when their disfluencies are handled correctly. They notice when they are handled incorrectly. If a user says "I want to book a flight, no, a hotel" and the system books a flight, the user blames the system for not listening, even though the error was in disfluency handling, not ASR.

Explicitly confirming ambiguous requests helps. If the system detects a correction or restart, it can ask: "I heard you say hotel. Is that correct?" This surfaces the ambiguity and lets the user resolve it, rather than the system guessing wrong.

A travel voice assistant added confirmation prompts for any request containing negation or restart cues. User error recovery improved — when the system misunderstood a correction, users caught it during confirmation rather than after booking. Completion rates for complex requests increased by 14%.

## Test Set Construction for Disfluency Evaluation

Your test set must include disfluent speech if you want to measure disfluency handling. Scripted test sets have almost no disfluencies. Spontaneous test sets — real user recordings — have realistic disfluency rates but require careful annotation to mark which words are disfluent.

A contact center built a disfluency test set by sampling 500 real customer calls, transcribing them verbatim, and annotating each word as fluent or disfluent. The test set included 6,400 disfluent words out of 52,000 total words, a 12% disfluency rate. They measured ASR accuracy separately on fluent and disfluent words. Fluent word WER was 6%. Disfluent word WER was 19%. The system was three times more likely to misrecognize a disfluent word than a fluent word.

This test set allowed them to optimize disfluency handling without degrading performance on fluent speech. They fine-tuned the ASR model to improve disfluent word accuracy from 81% to 87%, reducing overall WER by 1.4 percentage points.

## The Future of Disfluency Modeling

Newer ASR models trained on large-scale spontaneous speech corpora handle disfluencies more robustly. GPT-4o-mini's Whisper-based ASR in 2025, trained on 600,000 hours of diverse audio, transcribed disfluencies more consistently than earlier models. Claude Opus 4.5 and Gemini 3 Pro's ASR components, introduced in late 2025, included explicit disfluency tokens in their vocabularies, allowing them to represent "um" and "uh" as distinct units rather than approximate phonetic matches.

But even the best models require post-processing for applications that need clean transcripts. Disfluency detection remains a separate task. The difference is that newer models provide more reliable disfluency labels, making downstream filtering more accurate.

Real human speech is messy. People hesitate, correct themselves, and restart sentences. ASR systems that expect clean, fluent input will fail. The systems that succeed are those that model disfluency as a normal part of speech, handle it gracefully, and remove it when necessary without removing semantic content. This completes the ASR evaluation stack: accuracy on fluent speech, robustness to noise, domain vocabulary handling, crosstalk suppression, and disfluency processing. Together, these dimensions determine whether your voice system works in the real world. The next chapter shifts from what the system hears to how fast it responds — latency measurement and optimization in real-time voice systems.

