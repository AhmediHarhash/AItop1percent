# 4.12 — ASR Provider Comparison Methodologies

Most teams choose an ASR provider based on vendor benchmarks, marketing claims, and pricing. They see that Provider A claims 95 percent accuracy, Provider B claims 96 percent, and Provider C is cheapest. They pick Provider B. Six months into production, they discover that Provider B performs worse than Provider A on their specific traffic — accents, noise, domain vocabulary. The decision was made on the wrong data. Vendor benchmarks are optimized for the vendor's strengths, not your reality.

The only way to choose an ASR provider is to test them all on your data, under your conditions, with your metrics. This is a **provider bake-off**: a controlled experiment where you run multiple ASR systems on the same audio and compare results using objective measurements. A fair bake-off requires identical test conditions, representative data, comprehensive metrics, statistical rigor, and cost-normalized analysis. Without this, you're guessing. With it, you have evidence.

A legal transcription company ran a four-provider bake-off in late 2025. They tested Google Cloud Speech-to-Text, AWS Transcribe, Azure Speech, and AssemblyAI on 5,000 utterances of real courtroom audio. Vendor benchmarks had all four providers within 1 percentage point of each other on WER. The bake-off revealed a 6.8 percentage point spread: best provider at 4.2 percent WER, worst at 11.0 percent. The best provider on the vendor benchmark scored third in the bake-off. The winner was a provider they almost didn't test because its marketing was less aggressive. The bake-off saved them from a bad decision that would have cost tens of thousands of dollars per month in rework and corrections.

## Defining the Test Conditions

A fair ASR bake-off starts with identical test conditions for all providers. Every provider must process the same audio files, in the same format, with the same configuration options, at the same time. Any variation introduces confounds that make comparison invalid.

Use your production evaluation dataset as the test set — the same dataset you built following the guidelines in the previous subchapter. This ensures the test is representative of real usage. If your dataset is 5,000 utterances covering your accent distribution, noise profiles, and domain vocabulary, all providers are tested on those 5,000 utterances. No exceptions. Do not let a provider argue "our system works better on different data." The data is your production reality.

Standardize **audio format** across all tests. Convert all audio to the same sample rate (typically 16 kHz), bit depth (16-bit), and codec (WAV or FLAC, uncompressed or lossless). Some ASR providers accept MP3 or other lossy formats, but lossy compression can degrade accuracy. Use lossless formats to eliminate this variable. A customer service platform tested one provider on MP3 audio and another on WAV. The MP3 version had 0.4 percentage points higher WER due to compression artifacts. They reran the test with WAV for both and the difference disappeared.

Configure each provider with **comparable settings**. Most ASR APIs offer configuration options: enable punctuation, enable speaker diarization, specify language or dialect, provide custom vocabulary or language models. Set these options as consistently as possible across providers. If all providers support custom vocabulary and you use domain-specific terms, provide the same vocabulary list to all. If only some providers support it, test both with and without custom vocabulary to understand the impact.

A medical transcription service provided a 5,000-word custom medical vocabulary to all four providers in their bake-off. Three providers supported custom vocabulary and showed 1.2 to 2.1 percentage point WER improvement with it enabled. One provider didn't support custom vocabulary and had 3.8 percentage points higher WER on medical terms. The team noted this as a feature gap, but they measured it explicitly rather than letting it be a hidden confound.

Run tests **at the same time of day** if you're using real-time ASR APIs that might have variable latency or performance based on load. A voice assistant tested three providers over a weekend when API load was low and saw latency 30 to 40 percent lower than weekday production traffic. They reran the test on a Tuesday afternoon and got realistic latency numbers. If you're testing batch transcription (uploading files for offline processing), time of day matters less, but calendar time still matters — don't compare a test from January 2026 to a test from March 2026 because providers update models frequently.

Document **API versions and model identifiers** for every provider. ASR providers frequently release new models. Google's Chirp model, AWS Transcribe's latest version, Azure's neural voices — all improve over time. Record which specific model version you tested. If you rerun the bake-off six months later, you'll want to know whether improvements are due to model updates or changes in your data. A dictation app tested AssemblyAI's "best" model in Q1 2025 and got 6.1 percent WER. They retested in Q3 2025 on the same audio and got 4.9 percent WER. AssemblyAI had shipped a model update in between. The improvement was real, but it wasn't a fair comparison to the Q1 results from other providers unless those were also retested.

## Metrics Beyond WER: Comprehensive Evaluation

WER is the primary metric, but it's not sufficient. A provider with 5 percent WER might have terrible performance on domain-specific terms, or high latency, or poor confidence calibration. You need a suite of metrics.

Measure **overall WER** on the full test set. This is your headline number. But also measure WER on key segments: by accent, by noise level, by utterance length, by domain vocabulary density. A navigation assistant measured overall WER and segmented WER. Provider A: 5.2 percent overall, 4.1 percent on American accents, 8.9 percent on British accents, 12.1 percent on Indian accents. Provider B: 5.8 percent overall, 5.3 percent American, 6.1 percent British, 6.4 percent Indian. Provider A was better overall, but Provider B was more consistent across accents. The team served a global user base and chose Provider B because consistent quality mattered more than slightly lower overall WER.

Measure **entity-level accuracy** if your application depends on specific entities. Names, places, product names, technical terms. Extract entities from ground truth and ASR output, then calculate entity precision and recall. A voice commerce app measured product name accuracy separately. Provider A: 4.8 percent overall WER, 88 percent product name recall. Provider B: 5.1 percent overall WER, 94 percent product name recall. Product name accuracy was critical for order correctness, so the team chose Provider B despite slightly higher overall WER.

Track **latency** for real-time ASR. Measure time from end of speech to receiving the final transcription. Run each provider on the same network connection and instance type to control for infrastructure differences. A voice assistant tested three providers. Provider A: 380 milliseconds average latency. Provider B: 520 milliseconds. Provider C: 290 milliseconds. All three had similar WER (within 0.5 percentage points). Provider C won on latency. For real-time interactions, latency below 400 milliseconds was a hard requirement, so Provider B was eliminated despite competitive WER.

Measure **confidence calibration** as discussed in the previous subchapter. Some providers return well-calibrated confidence scores, others don't. If you plan to use confidence for routing or thresholds, calibration matters. A customer service transcription platform tested confidence calibration across four providers. Provider A: calibration error 3.2 percentage points. Provider B: 8.7 percentage points. Provider C: 11.4 percentage points. Provider D: 4.1 percentage points. Providers A and D had usable confidence scores. Providers B and C's confidence scores were decorative. The team eliminated B and C because their workflow depended on confidence-based routing to human review.

Test **streaming vs. batch performance** if your application uses both. Some providers optimize for batch (offline transcription) and have worse real-time streaming performance, or vice versa. A meeting transcription service tested both modes. Provider A: 4.2 percent WER batch, 5.8 percent WER streaming. Provider B: 5.1 percent WER batch, 5.3 percent WER streaming. Provider A was better for batch but worse for streaming. The team used both modes in production (batch for recorded meetings, streaming for live captions), so they needed a provider that performed well in both. Provider B won.

## Controlled Testing: Avoiding Confounds

Provider bake-offs are experiments. Like any experiment, they require control over confounding variables. If you change multiple things at once, you can't attribute differences to the ASR provider vs. other factors.

Use **the same audio files** for all providers. Do not re-record or resample. Feed the exact same WAV or FLAC files to every provider's API. A healthcare voice app tested three providers but accidentally used different audio preprocessing for one provider — they applied noise reduction to the audio before sending it. That provider showed 2.1 percentage points lower WER. They reran the test with identical preprocessing (none) and the advantage disappeared. The apparent improvement was due to preprocessing, not the ASR.

Run tests from **the same network location**. Network latency and packet loss vary by region. If you test Provider A from a US-East data center and Provider B from a Europe-West data center, latency comparisons are invalid. A voice assistant team tested all providers from the same AWS region (us-east-1) using the same instance types (c5.2xlarge) to ensure network conditions were identical.

Control for **API quota and throttling**. Some providers throttle requests if you exceed free-tier limits or if you send too many concurrent requests. Make sure you're on a paid plan with sufficient quota for all providers during testing. A dictation app tested four providers but hit API rate limits on one provider during testing. That provider showed higher latency and some failed requests. They upgraded to a higher quota tier and reran the test. Latency dropped to competitive levels.

Blind the evaluation if possible. Don't let the team member analyzing results know which provider produced which transcription until after metrics are calculated. This prevents unconscious bias — the tendency to interpret ambiguous results in favor of a preferred provider. A legal transcription service used a two-stage process: one engineer ran the ASR tests and saved results with anonymous labels (Provider X, Provider Y, Provider Z). A different engineer calculated metrics on the anonymous results. Only after metrics were finalized did they reveal which label corresponded to which provider.

## Statistical Significance Testing

WER differences must be statistically significant to be meaningful. A 0.3 percentage point difference might be noise. A 1.5 percentage point difference is probably real. You need statistical tests to know.

Use **McNemar's test** or **paired t-tests** to compare WER between providers. These tests account for the fact that you're testing on the same audio, so the results are paired (not independent). A customer service bot compared two providers on 5,000 utterances. Provider A: 5.2 percent WER. Provider B: 4.8 percent WER. The difference was 0.4 percentage points. McNemar's test p-value: 0.14 — not statistically significant at the 0.05 threshold. The apparent difference could be random chance. The team couldn't confidently say Provider B was better.

In a different comparison, Provider A: 6.1 percent WER. Provider C: 4.3 percent WER. Difference: 1.8 percentage points. McNemar's test p-value: 0.002 — highly significant. Provider C was genuinely better. The team chose Provider C with confidence.

Calculate **confidence intervals** on WER estimates. A 5 percent WER with a 95 percent confidence interval of 4.2 to 5.8 percent is more certain than 5 percent WER with an interval of 3.1 to 6.9 percent. Wider intervals mean you need more data to narrow the estimate. A voice assistant tested providers on 1,200 utterances and got WER estimates with plus or minus 1.2 percentage point confidence intervals. They increased the test set to 4,000 utterances and the intervals narrowed to plus or minus 0.6 percentage points. The larger test set gave them more confidence in the ranking.

Use **bootstrapping** if you want to estimate confidence intervals without strong distributional assumptions. Resample your test set with replacement 1,000 times, calculate WER on each resample, and use the distribution of WER values to construct a confidence interval. A legal transcription service used bootstrapping to estimate confidence intervals for each provider. Provider A: 4.2 percent WER, 95 percent CI 3.8 to 4.7 percent. Provider B: 5.1 percent WER, 95 percent CI 4.6 to 5.6 percent. The intervals didn't overlap, confirming that Provider A was better with high confidence.

Don't declare a winner unless the difference is **both statistically significant and practically meaningful**. A 0.2 percentage point WER difference might be statistically significant on a huge test set but not worth switching providers over if it comes with worse latency, higher cost, or worse confidence calibration. Define your **minimum detectable difference** — the smallest WER difference you care about. If your threshold is 0.5 percentage points and the measured difference is 0.3, treat the providers as equivalent even if the difference is statistically significant.

## Cost-Normalized Comparisons

ASR accuracy matters, but so does cost. A provider with 0.5 percentage points lower WER but double the price might not be worth it. You need cost-normalized comparisons: accuracy per dollar.

Collect **pricing information** for each provider. ASR pricing is usually based on audio duration: dollars per hour of audio processed. As of 2026, prices range from 0.006 dollars per minute (Google Cloud Speech-to-Text standard) to 0.048 dollars per minute (premium models from specialized providers). Prices vary by tier (standard vs. enhanced models), features (real-time vs. batch, with or without speaker diarization), and volume commitments.

A customer service platform collected pricing for five providers. Provider A: 0.012 dollars per minute, 5.2 percent WER. Provider B: 0.009 dollars per minute, 6.1 percent WER. Provider C: 0.024 dollars per minute, 4.1 percent WER. Provider D: 0.036 dollars per minute, 3.8 percent WER. Provider E: 0.015 dollars per minute, 5.8 percent WER.

Calculate **cost per correct word** or **cost per percentage point WER reduction**. The customer service platform processed 1.2 million minutes of audio per month. Monthly cost: Provider A 14,400 dollars, Provider B 10,800 dollars, Provider C 28,800 dollars, Provider D 43,200 dollars, Provider E 18,000 dollars. Cost per percentage point of WER reduction relative to the worst provider (B at 6.1 percent): Provider A costs 14,400 dollars for 0.9 percentage point improvement (16,000 dollars per point). Provider C costs 28,800 dollars for 2.0 percentage point improvement (14,400 dollars per point). Provider D costs 43,200 dollars for 2.3 percentage point improvement (18,783 dollars per point).

The team's decision: Provider C. It was the second most expensive in absolute terms, but it had the best cost-efficiency for WER improvement. Provider D was more accurate but the marginal improvement (0.3 percentage points) wasn't worth the extra 14,400 dollars per month.

Factor in **downstream costs** of errors. If ASR errors require human correction, factor in the cost of correction. A legal transcription service calculated that each WER percentage point cost them 8,000 dollars per month in human review and correction time. Provider A: 4.2 percent WER, 14,400 dollars API cost, 33,600 dollars correction cost (4.2 percent * 8,000), total 48,000 dollars. Provider B: 5.1 percent WER, 10,800 dollars API cost, 40,800 dollars correction cost, total 51,600 dollars. Provider A was more expensive for API calls but cheaper overall because lower WER reduced correction costs. They chose Provider A.

Consider **tiered pricing and volume discounts**. Many providers offer lower per-unit prices at higher volumes. A voice assistant processed 200,000 minutes per month. Provider A offered 0.012 dollars per minute up to 100,000 minutes, then 0.009 dollars per minute above that. Effective price: 0.0105 dollars per minute. Provider B was flat 0.010 dollars per minute. At 200,000 minutes, Provider A was cheaper despite the higher base rate. Check pricing tiers carefully and calculate effective cost at your actual volume.

## Testing Real-Time vs. Batch Workflows

Real-time ASR and batch ASR have different performance characteristics. Real-time systems prioritize low latency and stream partial results. Batch systems prioritize accuracy and process complete audio files. Test the workflow you'll actually use in production.

For **real-time workflows**, test streaming APIs. Send audio in chunks (typically 100 to 500 milliseconds per chunk) and measure partial transcription quality, latency, and stability. A voice assistant tested three providers in streaming mode. Provider A: partials appeared every 200 milliseconds, revision rate 0.28 per word, final WER 5.1 percent. Provider B: partials every 400 milliseconds, revision rate 0.15 per word, final WER 5.3 percent. Provider C: partials every 150 milliseconds, revision rate 0.41 per word, final WER 4.9 percent.

Provider C had the best final WER but the worst partial stability. Provider B had slightly higher WER but much better partial stability. For real-time interactions, stability mattered more than a 0.4 percentage point WER difference. The team chose Provider B.

For **batch workflows**, test file upload APIs. Upload complete audio files and measure turnaround time and WER. A meeting transcription service tested batch processing on 1-hour audio files. Provider A: 8 minutes turnaround, 4.1 percent WER. Provider B: 15 minutes turnaround, 3.9 percent WER. Provider C: 4 minutes turnaround, 4.8 percent WER. For asynchronous transcription where results were needed within an hour, all three were fast enough. The team chose Provider B for the best accuracy.

If you use both workflows, test both. A podcast transcription platform offered both: real-time captions during live recording (streaming) and polished transcripts afterward (batch). They tested all providers in both modes and chose a provider that performed well in both, even though it wasn't the absolute best in either mode. Consistency across workflows was more valuable than specialization.

## Re-Testing Cadence and Provider Tracking

ASR providers improve over time. Google, AWS, Azure, AssemblyAI, Deepgram, and others ship model updates quarterly or even monthly. A provider that ranked third in your bake-off in January might rank first by June. You need a re-testing cadence.

**Re-run the bake-off every six to twelve months** on your current production eval dataset. A customer service platform tested four providers in Q1 2025 and chose Provider A (5.2 percent WER). They retested in Q3 2025. Provider A had improved to 4.9 percent WER (model update in Q2). Provider B, which had been second at 5.8 percent WER, had improved to 4.6 percent WER (major model update). Provider B was now better. The team switched providers and saw immediate WER improvement in production.

Track **provider release notes and model updates**. Subscribe to provider changelogs, blog posts, and release announcements. If a provider announces a major model update, add it to your re-testing queue. A dictation app tracked AssemblyAI, Deepgram, and Google Cloud Speech-to-Text release notes. When Deepgram announced their Nova-2 model in late 2025, the team ran a quick test on their standard eval dataset. Nova-2 WER: 4.1 percent, a 1.2 percentage point improvement over the previous Deepgram model and 0.7 percentage points better than their current provider. They ran a full bake-off and switched.

Maintain a **provider scorecard** with historical results. Track WER, latency, cost, and confidence calibration for each provider over time. This shows trends. A voice assistant maintained a scorecard spanning 18 months. They saw that Provider A's WER improved steadily (6.1 percent to 5.4 percent to 4.9 percent) while Provider B's stagnated (5.8 percent in every test). Provider A's investment in model improvements was visible in the data. The team prioritized Provider A for future tests.

Set up **automated testing infrastructure** if you re-test frequently. A script that uploads audio to each provider's API, collects transcriptions, calculates WER, and generates a comparison report. A legal transcription service ran automated monthly tests on a 500-utterance smoke test set (a subset of their full eval set). If any provider showed a WER change of more than 0.5 percentage points up or down, they triggered a full bake-off on the 5,000-utterance set. This caught regressions early — in one case, a provider's WER jumped from 4.3 percent to 6.1 percent due to a bad model deployment. The team reported it, the provider rolled back, and WER returned to normal.

## Handling Multi-Language and Multi-Dialect Scenarios

If your application supports multiple languages or dialects, test each language separately. ASR quality varies dramatically by language. A provider that's excellent for English might be mediocre for Spanish or terrible for Hindi.

Build **language-specific eval datasets** following the same principles as the English dataset: representative sampling, realistic noise, domain vocabulary, sufficient size. A global customer service platform supported English, Spanish, French, Mandarin, and Hindi. They built separate 3,000-utterance eval datasets for each language, sampled from production traffic in each language.

Test all providers on all languages. A multilingual voice assistant tested four providers across five languages. English WER: Provider A 4.2 percent, Provider B 4.8 percent, Provider C 5.1 percent, Provider D 5.9 percent. Spanish WER: Provider A 6.1 percent, Provider B 5.3 percent, Provider C 6.8 percent, Provider D 7.2 percent. French WER: Provider A 7.8 percent, Provider B 6.9 percent, Provider C 7.1 percent, Provider D 8.9 percent. Mandarin WER: Provider A 9.2 percent, Provider B 8.1 percent, Provider C 7.4 percent, Provider D 9.8 percent. Hindi WER: Provider A 11.3 percent, Provider B 9.7 percent, Provider C 10.2 percent, Provider D 12.1 percent.

No single provider was best across all languages. Provider A was best for English. Provider B was best for Spanish and Hindi. Provider C was best for Mandarin. The team had to choose: use different providers for different languages (multi-provider strategy) or use a single provider with acceptable quality across all languages (single-provider strategy).

They chose a **multi-provider strategy**: Provider A for English, Provider B for Spanish and Hindi, Provider C for Mandarin and French. This maximized WER across all languages but added operational complexity (multiple API integrations, multiple billing relationships, multiple re-testing cadences). An alternative team in a similar situation chose a **single-provider strategy**: Provider B across all languages. It wasn't the best for any single language, but it was top-three for all languages, and the operational simplicity was worth the slight accuracy trade-off.

Test **dialect variation** within a language. British English, American English, Indian English, Australian English, South African English — all have different acoustic characteristics. A provider optimized for American English might struggle with Indian English. A global voice assistant tested English providers on American, British, Indian, and Australian accents separately. Provider A: 4.1 percent American, 6.8 percent British, 9.2 percent Indian, 7.1 percent Australian. Provider B: 5.3 percent American, 5.9 percent British, 6.1 percent Indian, 6.4 percent Australian. Provider A was better for American accents but worse for all others. Provider B was more balanced. The team's user base was 40 percent American, 60 percent other. They chose Provider B for consistent multi-dialect quality.

## Documenting and Communicating Results

A provider bake-off generates a lot of data. Metrics, cost calculations, latency measurements, confidence calibration scores, segment-level WER breakdowns. Document everything clearly so the decision is transparent and reproducible.

Create a **bake-off report** that includes: test conditions (dataset size, audio format, configuration settings), providers tested (names, models, versions, API configurations), metrics measured (WER overall and by segment, latency, cost, confidence calibration), statistical significance tests, cost-normalized comparisons, and final recommendation with rationale. A legal transcription service's bake-off report was 18 pages. It included summary tables, statistical test results, cost breakdowns, and detailed rationale for the chosen provider. The report was shared with Engineering, Product, Finance, and Legal teams. Everyone understood the decision and the trade-offs.

Share **raw data** with the team. Provide access to the test audio, ground truth transcriptions, ASR outputs, and calculated metrics. This enables others to validate your analysis or explore different trade-offs. A voice assistant team shared their bake-off data in a shared Google Drive folder. A product manager noticed that one provider had much better accuracy on product name entities even though overall WER was slightly higher. This insight influenced the final decision in favor of that provider.

Present results with **visualizations**: WER by provider (bar chart), WER by segment (grouped bar chart for each provider across segments), latency distribution (box plots), cost vs. WER scatter plot, confidence calibration curves (reliability diagrams for each provider). A customer service platform used Tableau dashboards to present bake-off results. Stakeholders could filter by language, noise level, or accent and see how each provider performed. The interactive dashboard made the decision process transparent and data-driven.

Archive the bake-off results. A voice assistant team kept a repository of bake-off reports dating back two years. When they re-ran tests, they compared to historical baselines to track provider improvement over time. When new team members joined, they read past bake-off reports to understand why the current provider was chosen and what alternatives existed.

Choosing an ASR provider based on vendor benchmarks is gambling. Running a rigorous bake-off on your own data, with controlled conditions, comprehensive metrics, statistical validation, and cost normalization, is evidence-based decision-making. Invest the time to test properly. The decision you make will affect accuracy, cost, latency, and user satisfaction for months or years. A week spent on a thorough bake-off saves months of production problems and expensive provider switches later. The next subchapter moves from ASR evaluation to the broader challenge of evaluating complete voice interactions end-to-end — from speech input to system response.
