# 5.6 — Pronunciation Accuracy for Names, Numbers, and Acronyms

The hardest part of TTS is not synthesizing vowels or consonants. It is pronouncing things that do not follow standard phonetic rules. Names like "Nguyen" or "Siobhan." Phone numbers where "555-1000" should be "five five five, one thousand" not "five hundred fifty-five, one thousand." Acronyms like "API" that should be spelled out versus "NASA" that should be pronounced as a word. Medical terms where "atorvastatin" has a specific stress pattern that pharmacists recognize instantly but TTS systems get wrong. Financial codes where "AAPL" means "Apple stock" not "A-A-P-L." Every time your TTS system encounters one of these edge cases, it must choose: apply standard grapheme-to-phoneme rules, or apply special-case logic. Choose wrong and your system sounds incompetent. Choose right consistently and users stop noticing pronunciation at all, which is the highest compliment TTS can receive.

In mid-2025, a healthcare voice assistant deployed to help patients manage prescription refills. The system handled routine medications flawlessly. When a patient asked to refill "lisinopril" or "metformin," the TTS pronounced the drug names correctly and confirmed the order without issue. Then a patient with a more complex medication regimen asked to refill "atorvastatin" and "montelukast." The system mangled both pronunciations, stressing the wrong syllables and producing phonetic outputs that the patient did not recognize as the medications they took daily. The patient assumed the system had pulled the wrong records and hung up. The technical team later discovered that the TTS model had been trained primarily on general conversational text. It had seen common drug names like "aspirin" and "ibuprofen" thousands of times. It had seen "atorvastatin" rarely enough that it reverted to generic phonetic rules, producing pronunciations that were technically valid English but pharmacologically unrecognizable.

The failure illustrates the core challenge of pronunciation accuracy. TTS models learn phonetic mappings from training data. For common words, they see enough examples to learn correct pronunciations. For rare words, proper nouns, technical terms, and domain-specific vocabulary, they fall back on statistical patterns that often produce plausible-sounding but incorrect outputs. A name like "Catherine" gets pronounced correctly because the model has seen it thousands of times. A name like "Saoirse" gets mangled because the model has never encountered Irish Gaelic phonetics and guesses based on English orthography. The result is not random noise. It is confidently wrong pronunciation that sounds almost right, which is worse than obvious failure because users cannot tell whether the error is in the TTS or in the underlying data.

## Why Pronunciation Edge Cases Matter

Pronunciation errors on names and domain-specific terms directly damage trust and usability. When a voice banking assistant mispronounces a customer's name, it signals that the system does not know who it is talking to. The customer hears their name mangled and immediately questions whether the system has pulled the correct account data. When a healthcare voice assistant mispronounces a medication name, the patient cannot confirm that the system is referring to the right drug. When a customer service assistant mispronounces a product name or technical term, the user loses confidence that the system understands their issue.

The problem compounds in professional and high-stakes contexts where pronunciation accuracy carries social and functional weight. Healthcare professionals recognize drug names by their phonetic patterns. Mispronounce "atorvastatin" and a pharmacist immediately knows you are not fluent in pharmaceutical terminology. Mispronounce a patient's name and the patient knows you have not taken the time to learn how to address them correctly. In human conversation, these mispronunciations are forgivable if the speaker attempts to learn and correct. In TTS, they are permanent system characteristics. Every time your voice assistant says "SAOR-ees" instead of "SEER-sha," it broadcasts that it has not been trained on the linguistic diversity of the population it serves.

Numbers present a different category of pronunciation challenge. Unlike names, numbers have strict conventions that vary by context. A phone number "555-1000" should be read as "five five five, one thousand" with a pause after the area code. A dollar amount "1,234.56" should be read as "one thousand two hundred thirty-four dollars and fifty-six cents." A model number "XZ-450" might be "X-Z four fifty" or "X-Z four five zero" depending on domain conventions. A year "2025" should be "two thousand twenty-five" in most contexts but "twenty twenty-five" in casual conversation. TTS systems must infer these conventions from context or risk producing technically correct but pragmatically wrong pronunciations. A voice assistant that reads a phone number as "five hundred fifty-five thousand" has applied mathematical pronunciation rules to a context where digit-by-digit reading is expected. The user hears a nonsensical string of numbers and assumes the system malfunctioned.

Acronyms occupy the worst of both worlds. Some acronyms are pronounced as words: "NASA," "UNICEF," "radar." Others are spelled out letter by letter: "FBI," "API," "HTML." Still others have mixed conventions: "SQL" can be "sequel" or "S-Q-L" depending on who is speaking. TTS systems must learn these conventions from training data or domain-specific rules. When they guess wrong, they produce outputs that are technically parsable but culturally incorrect. A developer who hears "H-T-M-L" pronounced as "HTML" (a single word rhyming with "caramel") knows the TTS system was not trained on technical discourse. A businessperson who hears "CEO" pronounced as "see-oh" instead of "C-E-O" knows the system lacks basic professional vocabulary.

## Measuring Pronunciation Accuracy on Edge Cases

Evaluating pronunciation accuracy requires building test sets that cover the specific edge cases your system will encounter. Generic pronunciation benchmarks measure how well a TTS system handles common English words. They do not measure how well it handles the Vietnamese names, pharmaceutical terms, and product codes your users will actually ask about.

Start by extracting high-frequency entities from your production data. If you are building a healthcare voice assistant, identify the top five hundred medication names, the top two hundred procedure names, and the most common physician names in your user base. If you are building a voice banking assistant, identify the most common transaction types, account types, and merchant names. If you are building a customer service assistant, identify the most frequent product names, model numbers, and technical terms. These entities define your pronunciation accuracy requirement. Your TTS system must pronounce them correctly or users will immediately notice failures.

For each entity, generate a reference pronunciation either from domain expert recording or from phonetic transcription databases. Medical terminology databases often include phonetic guides. Name pronunciation databases exist for many languages and cultural backgrounds. For entities without existing references, record domain experts pronouncing the terms and use those recordings as ground truth. Then generate TTS outputs for each entity and compare them to the reference pronunciations using both automated and human evaluation.

Automated pronunciation accuracy evaluation uses phonetic alignment algorithms. You convert both the reference pronunciation and the TTS output into phonetic transcriptions, then compute edit distance at the phoneme level. If the reference pronunciation for "Nguyen" is /wiŋ/ and your TTS output is /ŋuːjɛn/, the phoneme-level edit distance is high, indicating a significant mispronunciation. If the reference for "atorvastatin" stresses the second syllable and your TTS stresses the third, you can detect stress misalignment through prosodic analysis. These automated metrics provide scalable measurement but miss subtle errors that humans perceive as wrong even when phoneme-level alignment is close.

Human evaluation remains the gold standard for pronunciation accuracy. Play TTS outputs to domain experts or native speakers and ask them to classify each pronunciation as correct, understandable but incorrect, or incomprehensible. A correct pronunciation is indistinguishable from how a fluent speaker would say the word. An understandable but incorrect pronunciation is phonetically wrong but still recognizable. An incomprehensible pronunciation is so far from correct that listeners cannot identify what word the system attempted to say. Your incomprehensible rate must be zero. Your understandable-but-incorrect rate should be low single digits. Your correct rate should exceed ninety-five percent for high-frequency entities in your domain.

Cross-linguistic and cross-cultural validation is critical for names. A TTS system trained primarily on American English will mispronounce names from other linguistic traditions. "Nguyen" is a common Vietnamese surname. "Saoirse" is Irish. "Xiomara" is Spanish. "Zbigniew" is Polish. If your user base includes speakers from these backgrounds, your TTS system must pronounce their names correctly according to the phonetic rules of their native languages, not according to English orthographic patterns. Validate pronunciation accuracy with native speakers from each linguistic community you serve. A pronunciation that sounds acceptable to an English speaker may be offensively wrong to a native speaker of the name's origin language.

## Technical Approaches to Pronunciation Customization

Modern TTS systems offer multiple mechanisms for controlling pronunciation: pronunciation dictionaries, SSML phoneme tags, and model fine-tuning on domain-specific corpora. Each approach trades off between precision, scalability, and engineering effort.

Pronunciation dictionaries allow you to specify custom phonetic mappings for specific words. When your TTS system encounters "Nguyen," it looks up the entry in the dictionary, finds the phonetic transcription /wiŋ/, and synthesizes audio from that phonetic target instead of applying default grapheme-to-phoneme rules. Dictionaries work well for finite sets of known entities. If you are building a voice assistant for a hospital system, you can populate the dictionary with the names of every physician on staff, every common medication, and every procedure code. When your TTS encounters these entities, it uses the dictionary pronunciation. For everything else, it falls back to default behavior.

The limitation of pronunciation dictionaries is that they require manual curation and do not scale to open-ended entity sets. If your system operates across hundreds of hospitals, maintaining physician name dictionaries becomes impractical. If your system handles customer names from a global user base, you cannot pre-populate a dictionary with every possible name. Dictionaries work for closed-domain applications with stable entity sets. They fail for open-domain systems where entities are dynamic and unbounded.

SSML phoneme tags embed pronunciation instructions directly into the text your TTS system processes. Instead of sending "Nguyen" as plain text, you send it wrapped in a phoneme tag that specifies the exact phoneme sequence the TTS should synthesize. This gives you per-utterance pronunciation control without maintaining a global dictionary. The downside is that someone must generate those phoneme tags for every entity in every utterance. If your text is statically scripted, you can annotate it once. If your text is dynamically generated by an LLM, you need a real-time entity recognition and phoneme generation pipeline that runs on every LLM output before passing it to TTS. That pipeline adds latency, complexity, and potential failure modes.

Model fine-tuning on domain-specific corpora is the most scalable long-term solution. You collect or generate a training dataset of text-audio pairs from your target domain, then fine-tune your TTS model to learn domain-specific pronunciation patterns. If you are building a medical TTS system, you fine-tune on medical textbooks, clinical notes, and recorded physician narration. The model learns that "atorvastatin" has a specific stress pattern, that drug names ending in "-mab" are monoclonal antibodies with predictable phonetics, and that medical acronyms like "EKG" should be spelled out. If you are building a financial TTS system, you fine-tune on earnings call transcripts, financial news, and ticker symbol pronunciations. The model learns that "AAPL" is "Apple," that dollar amounts have specific formatting rules, and that financial terms like "amortization" carry domain-specific stress patterns.

Fine-tuning requires significant data collection and compute resources, but it produces a TTS system that handles domain-specific pronunciation natively without manual annotation or lookup tables. The model's internal representations learn the phonetic regularities of your domain, and it generalizes those patterns to entities it has never seen. A TTS model fine-tuned on pharmaceutical terminology will pronounce "atorvastatin" correctly even if that specific drug was not in the training set, because it has learned the stress and phonetic patterns common to statin-class drugs. Fine-tuning does not eliminate the need for pronunciation dictionaries for high-value entities, but it reduces the dictionary size from thousands of entries to dozens.

## Handling Numbers, Codes, and Structured Data

Numbers present unique pronunciation challenges because their correct pronunciation depends on semantic context, not just orthography. The string "2025" might be a year, a quantity, or a model number, and each context requires different pronunciation. TTS systems must infer context from surrounding text and apply the appropriate normalization.

Phone numbers require digit-by-digit reading with pauses that signal structure. "555-1234" should be pronounced "five five five, one two three four" with a pause after the area code. The pause is functional information. It tells the listener where the area code ends and the local number begins. A TTS system that reads phone numbers without pauses or with pauses in the wrong places forces the listener to parse digit sequences mentally, which is error-prone. Worse, a TTS system that applies numerical reading rules and pronounces "555-1234" as "five hundred fifty-five thousand, one thousand two hundred thirty-four" produces nonsensical output that the listener cannot use.

Currency amounts have different conventions. "1,234.56" should be pronounced "one thousand two hundred thirty-four dollars and fifty-six cents," not "one comma two three four point five six." The TTS system must recognize the pattern, parse it as currency, and apply the correct normalization. Some systems allow you to tag currency amounts explicitly in the input text to bypass inference. Others rely on heuristics, detecting strings that match currency patterns and applying the appropriate rules. Heuristics work most of the time but fail on edge cases. If your system says "The total is 50," the TTS might pronounce it as "fifty" (correct) or "fifty dollars" (wrong if the user already knows the context is currency). Explicit tagging eliminates ambiguity but requires upstream processing.

Alphanumeric codes like model numbers, license plates, and confirmation codes require character-level reading unless domain conventions specify otherwise. "XZ-450" might be "X-Z four five zero" or "X-Z four fifty" depending on whether the number component is a sequence or a quantity. License plates are always character-level: "ABC-1234" is "A-B-C, one two three four." Confirmation codes are character-level, often with pauses or prosodic breaks to chunk the string into memorable segments: "A7F9B2" becomes "A seven F nine, B two."

The solution is normalization rules that classify structured data by pattern and apply domain-appropriate pronunciation. You build regex patterns or classifiers that detect phone numbers, currency, dates, times, codes, and other structured entities. For each category, you apply the normalization that matches user expectations. You test these rules against production data to find edge cases where the pattern matches but the normalization is wrong. A string like "1-800-FLOWERS" looks like a phone number but should be pronounced "one eight hundred, flowers," not "one eight zero zero, F-L-O-W-E-R-S." Handling these cases requires layering heuristics, maintaining exception lists, and validating against human pronunciation norms.

## Acronym Pronunciation Conventions

Acronyms are unpredictable. Some are pronounced as words, some as letter sequences, and some vary by community or context. TTS systems must learn these conventions or default to letter-by-letter pronunciation, which is safe but often wrong.

Common acronyms with established pronunciations can be handled with lookup tables. "NASA" is always a word. "FBI" is always letters. "SQL" is "sequel" in some communities and "S-Q-L" in others, but you can choose one convention and apply it consistently. The lookup table approach works for high-frequency acronyms where pronunciation conventions are stable and widely agreed upon. It fails for rare acronyms, new acronyms, and acronyms where pronunciation is contested.

For acronyms not in the lookup table, TTS systems must infer pronunciation from orthographic structure. Acronyms with vowel-consonant patterns that resemble pronounceable words are more likely to be pronounced as words. "UNICEF" has vowels in positions that allow syllable formation, so pronouncing it as a word is reasonable. "XML" has no vowels and is difficult to syllabify, so letter-by-letter pronunciation is safer. Some systems use machine learning classifiers trained on acronym-pronunciation pairs to predict whether a new acronym should be pronounced as a word or spelled out. These classifiers work better than pure heuristics but still fail on edge cases where community conventions override phonetic patterns.

The safest default is letter-by-letter pronunciation. If your TTS system encounters an unfamiliar acronym, spelling it out is almost never wrong. It might sound overly formal or pedantic, but it is comprehensible. Pronouncing an unfamiliar acronym as a word risks producing a nonsensical sound that confuses listeners. A user who hears "API" pronounced "appy" will not recognize it as "A-P-I." A user who hears "API" spelled out will understand immediately. Letter-by-letter pronunciation sacrifices naturalness for accuracy, which is the right trade-off when you do not have high-confidence pronunciation data.

For systems serving technical or professional domains, maintain domain-specific acronym pronunciation guides. A developer-facing voice assistant should know that "API" is "A-P-I," "HTML" is "H-T-M-L," and "JSON" is "J-S-O-N" or "Jason" depending on convention. A healthcare assistant should know that "EKG" is "E-K-G," "MRI" is "M-R-I," and "HIPAA" is "hippa." These conventions are not universal, but they are predictable within domains. Learn them, encode them, and validate that your TTS applies them correctly.

## Continuous Learning from Mispronunciation Reports

Even with dictionaries, fine-tuning, and normalization rules, your TTS system will mispronounce entities. The question is how quickly you detect and fix those errors. The best teams build feedback loops that capture mispronunciation reports from users and incorporate corrections into the system.

Explicit feedback mechanisms allow users to report mispronunciations directly. If your voice assistant has a text interface, users can flag words that were pronounced incorrectly and optionally provide corrections. If your voice assistant is voice-only, you can offer a "report pronunciation error" command that logs the utterance and the timestamp for review. These reports give you direct signal about which entities your system struggles with. You aggregate the reports, prioritize the entities with the highest error frequency, and either add dictionary entries, update normalization rules, or include the entities in the next fine-tuning dataset.

Implicit feedback comes from user behavior. If users consistently ask for clarification after your TTS says a particular name or term, that is a signal of potential mispronunciation. If users repeat themselves multiple times when trying to confirm a medication name, the mispronunciation may be causing confusion. You instrument your conversation logs to detect these patterns and flag entities that correlate with clarification requests or repeated user utterances. Not every flagged entity will be a mispronunciation, but the correlation is strong enough to prioritize them for manual review.

Crowdsourced pronunciation validation accelerates correction cycles. When your system flags an entity as potentially mispronounced, you generate TTS outputs and send them to a panel of domain experts or native speakers for validation. They classify the pronunciation as correct, incorrect, or ambiguous. If multiple evaluators agree it is incorrect, you prioritize a fix. If evaluators disagree, you investigate whether the entity has multiple valid pronunciations or whether your TTS output is producing an ambiguous phonetic form that some listeners interpret as correct and others as wrong.

Over time, this continuous learning loop narrows the gap between your TTS system's pronunciation capabilities and the full diversity of entities your users encounter. You will never reach one hundred percent accuracy on every possible name, term, and code. But you can reach ninety-nine percent on the entities that matter most, which is the threshold where pronunciation accuracy stops being a user-facing issue and becomes a background quality characteristic that users trust implicitly. Reaching that threshold requires treating pronunciation as an ongoing operational responsibility, not a one-time training task.

Speech rate controls whether your TTS output feels rushed, draggy, or perfectly paced for the listener's comprehension and context.
