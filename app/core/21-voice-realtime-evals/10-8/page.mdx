# 10.8 â€” A/B Testing Across Providers in Production Traffic

Your team believes Claude Opus 4.5 delivers better voice conversation quality than GPT-5.2 based on eval suite results from last month. You route 95 percent of production traffic to Claude and reserve GPT as a backup. Three weeks later, you analyze production data and discover that GPT conversations have 12 percent higher task completion rates, 8 percent lower escalation to human agents, and statistically indistinguishable CSAT scores despite costing 20 percent less per conversation. Your eval suite missed what production traffic revealed: GPT is better for your use case. You have been overpaying for inferior outcomes for three weeks because you trusted static evals over production measurement.

A/B testing across providers in production is the only way to know which provider actually performs best for your specific users, your specific domain, and your specific conversation patterns. Eval suites measure general capability on representative test cases. Production traffic measures real performance on real users with all the complexity, edge cases, and unexpected behaviors that eval suites cannot capture. The teams that run continuous A/B tests across providers optimize cost and quality simultaneously. The teams that pick a provider based on benchmark results and never test alternatives leave performance and cost improvements on the table indefinitely.

## Traffic Splitting Strategies for Voice

The simplest traffic split assigns each new conversation to a provider randomly with predefined probabilities. Fifty percent of conversations go to Claude, 50 percent to GPT. Randomization eliminates selection bias and ensures balanced comparison. The risk is that random assignment creates inconsistent user experiences if a user has multiple conversations over time. They might talk to Claude on Monday, GPT on Wednesday, and Claude again on Friday, experiencing subtle quality differences that make your system feel inconsistent.

User-level assignment solves the consistency problem by hashing user ID to determine provider assignment. Every conversation for User A goes to Claude. Every conversation for User B goes to GPT. This creates consistent per-user experiences and enables longitudinal analysis: you can measure whether users assigned to Claude have higher retention or lifetime value than users assigned to GPT. The trade-off is that user-level assignment takes longer to reach statistical significance because you need enough users, not just enough conversations. If you have 10,000 conversations per day but only 2,000 unique users per day, user-level assignment collects data five times slower than conversation-level assignment.

Stratified sampling ensures balanced distribution across important user segments. You assign 50 percent of enterprise customers to Claude and 50 percent to GPT. You do the same for free-tier users. This prevents scenarios where randomization accidentally assigns 70 percent of enterprise customers to one provider, confounding your analysis with segment effects. Stratified sampling adds complexity to your routing logic because you must classify users into segments before assigning providers, adding 10 to 30 milliseconds of latency. The latency cost is justified when user segments have different usage patterns or quality expectations.

Adaptive allocation shifts traffic toward better-performing providers as data accumulates. You start with a 50-50 split. After 5,000 conversations, Provider A shows five percentage points higher task completion. You shift to 60-40 in favor of Provider A. After 10,000 more conversations, the gap persists. You shift to 70-30. Adaptive allocation minimizes the time spent sending traffic to inferior providers but requires careful statistical guardrails to avoid premature optimization based on noise. The teams that use Bayesian bandit algorithms for adaptive allocation converge to optimal provider selection faster than teams that use fixed allocation. The teams that implement adaptive allocation naively overreact to random fluctuations and thrash between providers.

## Metrics to Compare: Latency, Quality, Cost, User Satisfaction

Latency is the easiest metric to measure and the most dangerous to optimize in isolation. Provider A responds 100 milliseconds faster than Provider B on average. You declare Provider A the winner and route all traffic to it. Two months later, you discover that Provider B had eight percent higher task completion rates, meaning users were more likely to solve their problems despite the slightly longer wait. You optimized for the wrong metric and made your product worse.

Time to first token measures how quickly the provider starts responding. Time to last token measures total response generation time. For voice, time to first token is more important than time to last token because TTS can start playing audio before the full response is generated. Provider A generates the first token in 120 milliseconds but takes 800 milliseconds to generate the full response. Provider B generates the first token in 200 milliseconds but completes the full response in 600 milliseconds. Provider A feels faster to users even though it is slower overall because users hear audio 80 milliseconds sooner. The teams that measure only end-to-end latency miss this nuance. The teams that measure first-token latency separately optimize for perceived responsiveness.

Task completion rate measures whether the conversation successfully resolved the user's issue without escalation or repeat contact. This is the metric that correlates most strongly with business outcomes. A provider that completes 78 percent of tasks is objectively better than a provider that completes 71 percent of tasks, regardless of latency or cost, because task completion directly drives user satisfaction and reduces operational costs. The challenge is measuring task completion accurately. Explicit user confirmation is gold standard but happens in only 30 to 50 percent of conversations. Implicit signals like conversation length, sentiment trajectory, and whether the user returns with the same issue within 48 hours provide proxy measurements.

Cost per successful resolution combines cost and quality into a single metric. You divide total provider cost by the number of successfully completed tasks. Provider A costs $0.10 per conversation and completes 75 percent of tasks, yielding $0.133 per resolution. Provider B costs $0.14 per conversation and completes 88 percent of tasks, yielding $0.159 per resolution. Provider A is more cost-efficient on a per-resolution basis despite Provider B having higher per-conversation quality. The decision depends on whether you prioritize cost efficiency or absolute quality. The teams that measure cost per resolution make economically rational decisions. The teams that measure only cost per conversation or only completion rate optimize the wrong dimension.

## Statistical Significance with Voice-Specific Sample Sizes

Voice conversations are not independent observations. Turns within a conversation are correlated. Users within a session are the same person across multiple turns. Conversations from the same user across days might be correlated based on persistent user satisfaction or dissatisfaction. The teams that ignore these correlations underestimate variance and overestimate statistical significance, leading to false positives where they conclude Provider A is better when the difference is actually noise.

Conversation-level clustering is the minimum correction. You treat each conversation as one observation, not each turn. If you want to detect a five percentage-point difference in task completion rate between two providers with 80 percent power and alpha of 0.05, you need approximately 1,200 conversations per provider assuming baseline completion rate of 75 percent. If conversations average 10 turns each, that is 12,000 turns per provider, but the statistical power comes from the 1,200 conversation-level observations, not the 12,000 turn-level observations.

User-level clustering is necessary when using user-level assignment. If the same 1,000 users have multiple conversations over the test period, your effective sample size is 1,000, not the 5,000 total conversations those users had. Ignoring user-level clustering causes you to declare significance prematurely. The teams that use mixed-effects models or cluster-robust standard errors correctly account for user-level correlation. The teams that use naive t-tests on conversation counts overestimate significance by 2x to 5x depending on how many repeat users exist in the data.

Early stopping rules prevent running tests longer than necessary. You set a maximum sample size of 10,000 conversations per provider and check for significance every 1,000 conversations. If you detect a statistically significant difference at 4,000 conversations with effect size large enough to matter, you stop the test and make a decision. If you reach 10,000 conversations without detecting significance, you conclude there is no meaningful difference and default to the cheaper provider. The teams that use early stopping make decisions weeks faster than teams that commit to fixed-duration tests. The teams that check significance after every 100 conversations inflate their false positive rate due to multiple testing and need to apply Bonferroni correction or similar adjustments.

## Controlling for Confounders: Time of Day, User Segment, Query Type

Time of day affects both user patience and provider performance. Users calling at 2 AM might be in crisis mode with lower patience for latency. Provider latency might be lower at 2 AM due to reduced global load. If you assign all daytime traffic to Provider A and all nighttime traffic to Provider B by random chance, you confound provider differences with time-of-day differences. The solution is either stratifying by time of day or running tests long enough that both providers see balanced time-of-day distribution. The teams that check for time-of-day balance in their A/B tests avoid confounded results. The teams that assume randomization is sufficient discover spurious differences.

User segment matters because enterprise users and free-tier users have different tolerance for quality versus latency. Enterprise users care more about quality and less about latency. Free-tier users care more about speed. If randomization assigns 60 percent of enterprise users to Provider A and 40 percent to Provider B, and Provider A has higher quality but higher latency, you might conclude Provider A is better when the real story is that enterprise users prefer quality over speed. Stratifying by user segment or including segment as a covariate in your analysis isolates provider effects from segment effects.

Query type complexity influences which provider performs better. Provider A excels at simple procedural queries but struggles with complex reasoning. Provider B is slower for simple queries but handles complex queries better. If your test period happens to include 70 percent simple queries and 30 percent complex queries, Provider A looks better overall. A month later, your query mix shifts to 40 percent simple and 60 percent complex, and Provider A suddenly performs worse. The teams that track query type distribution and either stratify by query type or analyze provider performance separately for each query type discover these conditional effects. The teams that analyze only aggregate metrics miss them.

Traffic volume spikes can confound results if they affect providers differently. Provider A might handle normal load gracefully but degrade under heavy load. If a traffic spike occurs during the test and affects Provider A more than Provider B, you might conclude Provider B is better when the real issue is Provider A's scalability. The teams that monitor provider load and either exclude spike periods from analysis or include load as a covariate avoid this confounder. The teams that ignore load assume providers behave identically across load levels and draw wrong conclusions when spikes occur.

## Rollout Patterns: Shadow Mode, Gradual Percentage Increase, Full Switch

Shadow mode runs both providers in parallel but only shows users the primary provider's responses. You log both providers' responses and compare them offline. Shadow mode eliminates user-facing risk: no user ever sees the secondary provider's responses, so quality issues do not affect satisfaction. The cost is double the inference spend because you generate responses from both providers for every conversation. Shadow mode makes sense for initial validation before any production exposure but becomes too expensive for long-term testing.

Gradual percentage rollout starts at one percent of traffic to the new provider, monitors metrics for 24 hours, and increases to five percent if metrics hold. You continue doubling the percentage every 24 to 48 hours until you reach 50 percent, then run a full A/B test. Gradual rollout limits blast radius: if the new provider has a catastrophic quality issue, only one percent of users experience it before you catch it and roll back. The trade-off is slower data collection. Reaching statistical significance takes two to three times longer with gradual rollout compared to jumping straight to 50-50 splits.

Cohort-based rollout assigns specific user cohorts to the new provider rather than using random percentages. You start with internal employees, then beta users who opted into experimental features, then a small segment of tolerant users like free-tier or low-engagement users. If metrics hold, you expand to general population. Cohort-based rollout protects your highest-value users from exposure to potential regressions. The risk is that your test cohorts are not representative of your general population, leading you to conclude the new provider is better when it only performs better for early adopters.

Full switch with monitored rollback is the fastest approach. You switch 100 percent of traffic to the new provider, monitor metrics in real time, and roll back if metrics degrade beyond predefined thresholds. This approach minimizes testing time but maximizes risk. If the new provider has a subtle quality issue that does not trigger your rollback thresholds but degrades user experience by five percent, you might not detect it until CSAT data arrives 48 hours later. The teams that use full switch have extremely robust real-time monitoring and automated rollback systems. The teams that lack those capabilities should use gradual rollout instead.

## Automated Decision-Making and Continuous Testing

Manual A/B test analysis works when you run one test per month. It breaks down when you run continuous testing across multiple providers, multiple prompt versions, and multiple configuration changes simultaneously. Automated decision-making uses predefined statistical thresholds and business rules to make rollout decisions without human intervention. If Provider A shows statistically significant improvement in task completion rate with effect size above five percentage points and no significant degradation in latency or cost, the system automatically increases Provider A's traffic allocation. If metrics degrade, the system automatically rolls back.

The risk of automated decision-making is optimizing for local maxima or overfitting to noise. Your automated system detects that Provider A has two percentage points higher task completion during weekday mornings and shifts 80 percent of morning traffic to Provider A. The improvement was actually random noise, and the shift has no real effect. The teams that use automated decision-making include minimum effect size thresholds, require sustained improvements over multiple days, and periodically re-test decisions to prevent drift. The teams that automate naively thrash between providers based on noise.

Continuous testing means you are always running A/B tests. You never declare a permanent winner. You continuously split traffic 50-50 or 60-40 between your top two providers, measure outcomes, and adjust allocations every week. This catches provider performance drift: Provider A might be better this month but Provider B might improve next month when they launch a new model version. Continuous testing costs more in inference spend because you are always sending meaningful traffic to the second-best provider, but it catches opportunities and regressions faster than periodic testing.

Multi-armed bandit algorithms optimize the exploration-exploitation trade-off in continuous testing. You want to exploit the best-performing provider most of the time but still explore other providers to detect when they improve. Thompson sampling and upper confidence bound algorithms allocate traffic proportionally to each provider's probability of being optimal, automatically shifting traffic toward better providers while maintaining enough exploration to catch changes. The teams that use bandit algorithms converge to optimal provider selection faster than teams using fixed allocation. The teams that implement bandits without proper Bayesian priors or confidence bounds overexploit early leaders and miss late improvements.

Managing provider contracts, pricing negotiations, and procurement complexity requires a different skill set from technical evaluation. That is the subject of the next subchapter.

