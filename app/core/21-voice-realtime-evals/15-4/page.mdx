# 15.4 — Token Burst Amplification: When LLM Costs Spike Mid-Call

Most voice AI conversations cost between $0.08 and $0.25. But every few hundred calls, you see one that cost $1.80. Not because it lasted longer. Not because it failed and retried. Because one question near the end of the conversation triggered a token burst. The user asked something complex. The LLM needed the full conversation history to answer. The context window exploded from 2,400 tokens to 18,000 tokens. The response itself was 1,200 tokens — detailed, multi-step, with examples. That single turn consumed more tokens than the previous nine turns combined. Your per-minute costs stayed flat. Your token costs spiked by twelve times. This is token burst amplification, and it is the most unpredictable cost driver in voice AI.

## How Context Accumulates in Voice Conversations

Text chatbots can reset context. The user closes the browser, the session ends, the next conversation starts fresh. Voice calls do not work that way. The conversation is continuous. Every turn adds to the context. The LLM sees everything that has been said so far — the full conversation history — on every subsequent turn.

A customer support call starts simple. The user says "I need help with my order." The system asks for the order number. The user provides it. The system retrieves the order details and summarizes them. The user asks when it will ship. The system checks the shipping status and responds. Five turns. The conversation history is now roughly 1,800 tokens — the initial question, the order number, the order details, the shipping question, and all the responses. That is manageable.

Then the user asks a follow-up: "Can you compare this order to my last three orders and tell me if the shipping time is typical, and if not, explain why?" This question requires the system to retrieve three additional orders, compare shipping times, identify outliers, and generate an explanation. The LLM now needs the full conversation history plus the details of four orders plus the comparison logic. The context window jumps to 9,200 tokens. The response is 850 tokens because it has to explain the comparison. Total tokens for this turn: 10,050. The previous five turns averaged 720 tokens per turn. This turn is fourteen times larger.

## Token Burst Patterns: The Three Triggers

Token bursts happen for three reasons: long conversation history, complex queries that require extended context, and tool calls that inject large amounts of external data into the context.

Long conversation history is cumulative. Every turn adds 200 to 800 tokens to the context. Most voice conversations are short — three to seven turns. Context stays under 4,000 tokens. But some conversations stretch to twenty or thirty turns. A therapy session. A financial planning consultation. A technical support troubleshooting call. By turn twenty, the context is 12,000 to 20,000 tokens. Every subsequent turn processes that entire history. Even a simple question like "What did we decide about the budgeting approach?" requires the LLM to scan the full 18,000-token history to find the relevant section and respond.

Complex queries spike context even without long history. A user asks a single question that requires multi-step reasoning. "What are the tax implications of selling my house in 2026 versus 2027, assuming I reinvest the proceeds in a rental property?" The LLM needs tax rules, capital gains calculations, depreciation schedules, state-specific regulations, and hypothetical scenario modeling. The response is detailed and long. A 1,400-token response instead of the usual 200-token response. The input context was small, but the output context is massive, and that output becomes part of the input for the next turn.

Tool calls inject external data. The user asks "Summarize my account activity for the last ninety days." The system calls an API that returns ninety days of transactions — 340 line items. That transaction data gets injected into the LLM context so the LLM can summarize it. The transaction data alone is 6,800 tokens. Add the conversation history and the summarization prompt, and the total context is 9,500 tokens. The LLM processes all of it, generates a 600-token summary, and now the context for the next turn includes that summary plus the raw transaction data unless you explicitly prune it.

## Detecting Token Bursts Before They Hit Your Bill

Token bursts are invisible in real time unless you track token usage per turn. Most teams track total tokens per conversation. That hides bursts. A conversation with nine normal turns and one burst turn shows an average of 1,800 tokens per turn. It looks fine. But the burst turn consumed 11,000 tokens. If you are not logging per-turn token counts, you will not see it until you analyze your invoice and wonder why some conversations cost eight times more than others.

Log token usage at turn granularity. Record input tokens, output tokens, and total tokens for every turn. Set an alert threshold. If any turn exceeds 6,000 tokens, log it as a burst event. Review burst events weekly. You will see patterns. Certain questions always trigger bursts. Certain tool calls always inject massive context. Certain user behaviors — asking for detailed comparisons, requesting summaries of long histories — always spike tokens.

A wealth management voice assistant in early 2026 logged per-turn token usage and found that four percent of turns were burst events — over 6,000 tokens. Those four percent of turns accounted for thirty-one percent of total LLM costs. The median turn cost $0.004 in LLM tokens. The median burst turn cost $0.048. Twelve times higher. The bursts were not evenly distributed. Sixty-eight percent of burst turns happened in conversations longer than fifteen turns. The longer the conversation, the higher the likelihood of a burst.

## Mitigating Bursts: Context Pruning

Context pruning removes old or irrelevant turns from the conversation history before sending it to the LLM. Instead of sending the full twenty-turn history, you send the most recent five turns plus a summary of the earlier turns. This caps the context size and prevents runaway growth.

The challenge is deciding what to prune. Aggressive pruning saves tokens but risks losing information the LLM needs to answer the current question. Conservative pruning retains too much context and does not prevent bursts. The balance depends on your use case.

Transactional voice assistants can prune aggressively. If the user is booking an appointment, the LLM does not need to remember the small talk from the beginning of the call. Prune everything except the current task context. Keep the appointment date, time, and patient details. Drop the greeting, the pleasantries, and the earlier questions that have already been answered.

Consultative voice assistants need conservative pruning. If the user is in a therapy session, the LLM might need to reference something said fifteen minutes ago. Pruning too aggressively breaks continuity. The solution is semantic pruning. Summarize earlier turns into a compressed representation, then include the summary plus the recent turns. A twenty-turn conversation gets compressed to a 400-token summary plus the most recent four turns. Total context: 2,000 tokens instead of 14,000 tokens.

A financial planning voice assistant in mid-2025 implemented rolling summarization. Every five turns, the system sent the conversation history to a separate summarization model, generated a 300-token summary, and replaced the first five turns with that summary. The context window stayed capped at roughly 4,500 tokens regardless of conversation length. Burst events dropped by seventy-two percent. Average token cost per conversation dropped by thirty-eight percent. The summarization itself cost tokens — 300 tokens of output per five turns — but that cost was far lower than the cost of processing the full unsummarized history on every subsequent turn.

## Mitigating Bursts: Response Length Limits

Long outputs are expensive. A 1,200-token response costs six times more than a 200-token response. If your use case does not require long responses, cap them. Add a max tokens parameter to your LLM call. Set it to 400 tokens for transactional assistants. Set it to 800 tokens for consultative assistants. This prevents runaway generation.

The risk is truncation. If the LLM needs 600 tokens to answer the question and you capped it at 400 tokens, the response gets cut off mid-sentence. The user hears an incomplete answer. You paid for 400 tokens and delivered a broken experience. The solution is to set the cap slightly above your typical response length. If your median response is 220 tokens, set the cap at 500 tokens. This allows for natural variation without enabling extreme outliers.

A healthcare voice assistant in late 2025 set a 600-token response cap. Their median response was 310 tokens. The cap prevented responses from exceeding 600 tokens, which eliminated the longest tail of their token distribution. Before the cap, two percent of responses exceeded 1,000 tokens. After the cap, zero responses exceeded 600 tokens. Average response length dropped from 340 tokens to 295 tokens. The cost saving was modest — thirteen percent reduction in output tokens — but it eliminated the worst outliers. The most expensive turns became predictable.

## Mitigating Bursts: Tool Call Result Pruning

When a tool call returns data, that data gets injected into the LLM context. If the tool returns a small result — a single order record, a brief status message — the context increase is negligible. If the tool returns a large result — ninety days of transactions, a full customer interaction history, a detailed account summary — the context increase is massive.

Prune tool results before injecting them into the LLM. If the tool returns 340 line items, do you need to send all 340 to the LLM? Or can you filter it first? If the user asked about transactions over $500, filter the tool result to only include those transactions before sending it to the LLM. The LLM sees twenty-three line items instead of 340. The context shrinks from 6,800 tokens to 920 tokens.

If filtering is not possible, summarize the tool result. Send the raw data to a lightweight summarization model, generate a 400-token summary, and send the summary to the main LLM instead of the raw data. The LLM gets the information it needs without processing thousands of tokens of raw data.

A banking voice assistant in early 2026 implemented tool result summarization for account activity queries. When a user asked to review recent transactions, the system retrieved the transactions, sent them to a summarization model, and injected a 350-token summary into the LLM context instead of the raw 4,200-token transaction list. The LLM could still answer questions about the transactions based on the summary. Token usage per tool call dropped by eighty-three percent. Burst events triggered by account activity queries dropped by ninety-one percent.

## The Token Budget Per Conversation

Every conversation should have a token budget — a maximum number of tokens you are willing to spend. The budget depends on your business model. If you are charging $5 per consultation and your LLM costs $0.015 per thousand tokens, you can afford roughly 80,000 tokens before the LLM cost exceeds thirty percent of revenue. That is a generous budget. Most conversations will use far less.

Set a budget based on your margin target. If your target LLM cost per conversation is $0.10, and you are paying $0.015 per thousand tokens, your budget is 6,667 tokens per conversation. Track actual usage against budget. If fifty percent of conversations exceed budget, you have a cost problem. Either increase prices, reduce token usage, or accept lower margins.

Enforce the budget with hard caps. If a conversation reaches the token budget, the system warns the user: "We are reaching the end of our session. Do you have one more question?" This gives the user a chance to ask a final question, then the conversation ends. You do not cut them off abruptly, but you do prevent runaway token costs from conversations that stretch to forty turns.

A legal intake voice assistant in mid-2025 set a 12,000-token budget per conversation. Their average conversation used 5,400 tokens. Ninety-two percent of conversations stayed under budget. The remaining eight percent hit the budget cap and were warned. Of those, four percent asked a final question and ended. Four percent ignored the warning and continued. For those, the system enforced a hard disconnect at 15,000 tokens. This prevented the most extreme outliers — conversations that would have consumed 30,000 or 40,000 tokens if left unchecked. The budget enforcement cost them nothing in user satisfaction. The users who hit the cap were already in exceptionally long conversations and expected an endpoint.

## Token Burst Amplification Is a Design Problem

Token bursts are not random. They are the result of product decisions. You decided to maintain full conversation history. You decided to allow unlimited conversation length. You decided to inject raw tool results into context. You decided not to cap response length. Each decision made sense in isolation. Together, they created a cost structure where one complex question can spike your per-conversation cost by ten times.

The solution is to design for cost, not just for capability. Ask: what is the longest conversation we expect? What is the largest tool result we will inject? What is the maximum response length we need? Set limits based on those answers. Then build the system to enforce those limits through pruning, summarization, and budgets.

You will still see token bursts. Complex questions will still spike usage. But the spikes will be bounded. A burst that would have cost $0.80 in a system with no limits might cost $0.22 in a system with smart pruning and caps. The capability is nearly identical. The cost is four times lower. That difference compounds across thousands of conversations. It is the difference between sustainable margins and a cost structure that breaks your business model the moment you scale.

The next cost trap is less visible but equally destructive: fallback storms, where cascading retries multiply your costs without the user ever noticing.

