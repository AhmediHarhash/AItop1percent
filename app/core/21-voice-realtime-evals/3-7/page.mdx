# 3.7 — Network Latency and Geographic Distribution

In September 2025, a telehealth company launched a voice-enabled symptom checker to users across Asia-Pacific. The system was built and tested in their San Francisco headquarters, where median latency sat comfortably at 380 milliseconds. When Australian users started calling in, they experienced consistent 720-millisecond delays. The system felt sluggish and unresponsive. Users abandoned calls within 40 seconds. The failure was not in the AI stack. The failure was physics.

A voice packet traveling from Sydney to a data center in Northern Virginia crosses 16,000 kilometers of fiber optic cable. At the speed of light in glass, that distance introduces roughly 80 milliseconds of one-way latency before accounting for routing overhead, congestion, and protocol handshakes. Round-trip latency for a single request-response cycle often exceeds 180 milliseconds from network transport alone. When you stack ASR processing, LLM inference, and TTS synthesis on top of that baseline, you cross perceptual thresholds where conversation feels broken.

Network latency is not a variable you can optimize away. It is a constraint imposed by the speed of light and the physical distance between your users and your infrastructure. If your users are global and your servers are centralized, your voice system will fail for a meaningful portion of your user base. This subchapter explains how network latency compounds with processing latency, how geographic distribution affects voice UX, and when you must deploy closer to users to preserve conversational quality.

## The Physics of Network Latency

Light travels through fiber optic cable at approximately 200,000 kilometers per second — about two-thirds the speed of light in a vacuum. This physical limit means that a signal traveling from Los Angeles to London, roughly 8,800 kilometers, requires at least 44 milliseconds one-way, or 88 milliseconds round-trip, before accounting for any routing overhead.

Real-world network latency is always higher than the theoretical minimum. Packets traverse multiple routers, each introducing microseconds to milliseconds of queuing delay. Internet traffic does not travel in straight lines — routing decisions send packets through geographically suboptimal paths based on peering agreements, congestion, and network topology. A packet from São Paulo to a data center in Frankfurt might route through Miami and New York, adding hundreds of kilometers to the physical path.

TCP connection establishment requires a three-way handshake: SYN, SYN-ACK, ACK. For a user in Mumbai connecting to a server in Oregon, this handshake alone can consume 200-300 milliseconds before any application data is transmitted. If your voice system uses HTTPS, the TLS handshake adds additional round-trips. HTTP/2 and HTTP/3 reduce some overhead, but the initial connection cost remains dominated by round-trip time.

Voice systems send multiple requests per conversation. ASR sends audio chunks for transcription. The LLM receives the transcript and generates a response. TTS converts the response to audio. If each of these components is hosted in a single region far from the user, each step pays the full round-trip latency tax. A user in Tokyo speaking to a system hosted exclusively in Northern Virginia might experience 150 milliseconds of network latency per request. Across three sequential requests — ASR, LLM, TTS — that adds 450 milliseconds of latency that has nothing to do with model performance.

## Geographic Distribution of Real Users

If your user base is concentrated in a single region and your infrastructure is in that same region, network latency is negligible. A system serving only North American users from AWS us-east-1 will see median network latencies under 40 milliseconds for most users. But the moment your users span continents, centralized infrastructure becomes a voice UX liability.

A global SaaS company serving enterprise customers in Europe, North America, and Asia-Pacific from a single US-based data center will see wildly inconsistent latency profiles. Users in Frankfurt might experience 90-millisecond round-trip times. Users in Singapore face 220 milliseconds. Users in Sydney see 250 milliseconds or more. The same voice system that feels snappy for users in New York feels sluggish and frustrating for users in Melbourne.

User distribution patterns often emerge unevenly. A company might launch in the United States, expand to Western Europe, then add customers in India, Australia, and Japan. Early infrastructure decisions — hosting everything in us-east-1 because that is where the team is located — become compounding technical debt as the user base globalizes. By the time latency complaints surface from international users, the architecture is deeply coupled to a single-region deployment model, and the cost of refactoring is significant.

Some products have inherently global user bases from day one. Customer support voice bots for multinational corporations, voice-enabled mobile apps with international downloads, telehealth platforms serving expatriates and travelers — these systems cannot afford to assume users are geographically clustered. If you build for a global audience and deploy centrally, you are choosing to deliver a degraded experience to a large fraction of your users.

## Edge Deployment Strategies

The standard solution to geographic latency is edge deployment: running inference infrastructure in multiple regions close to where users are located. Instead of forcing a user in Sydney to send every request to Virginia, you deploy ASR, LLM, and TTS endpoints in AWS ap-southeast-2 or Google Cloud australia-southeast1, reducing network latency to under 20 milliseconds.

Multi-region deployment introduces operational complexity. You must manage infrastructure in multiple clouds or regions, monitor latency and availability independently for each region, and handle failover when a regional endpoint becomes unavailable. You must also decide which components to deploy regionally and which to keep centralized. Some teams deploy ASR and TTS at the edge but route LLM requests to a single central region where the most powerful GPUs are available. This hybrid approach reduces network latency for audio processing while centralizing the most expensive inference workload.

Another approach is content delivery network-style routing, where a global load balancer directs each user to the nearest available region based on IP geolocation or latency-based DNS resolution. This works well for stateless requests but introduces challenges for voice systems that maintain conversation state. If a user's requests are routed to different regions mid-conversation, you must replicate session state across regions or accept that the user's conversation will reset.

Some platforms offer managed edge inference. OpenAI's API routes requests to the closest available region automatically. Google Cloud's Vertex AI and AWS Bedrock support multi-region deployments with automatic failover. Azure AI services provide global endpoints with latency-based routing. These managed solutions reduce operational burden but may not support all models or configurations, and you lose control over exactly where your data is processed — a concern for teams with strict data residency requirements.

For models you host yourself, edge deployment means maintaining model replicas in each target region. If you fine-tuned a Llama 4 Scout model for customer support, you must deploy that model to us-east-1, eu-west-1, ap-southeast-1, and any other region you want to serve. Model synchronization becomes critical: when you release a new model version, all regions must update within a narrow time window to avoid user experience inconsistencies where some users get the new behavior and others still see the old version.

## When You Must Deploy Closer to Users

Not every voice system requires multi-region deployment. If your user base is geographically concentrated, the operational overhead of edge deployment outweighs the latency benefits. But several signals indicate when you must deploy closer to users.

If your P95 or P99 latency metrics are dominated by users in specific geographies, and those users represent a meaningful fraction of your traffic, you need regional infrastructure. A system where 30% of users are in Europe and 25% are in Asia-Pacific cannot deliver acceptable voice UX from a single US data center. The median user might be happy, but 55% of your users are experiencing degraded performance.

If you operate under regulatory or data residency requirements that prohibit transferring user data across borders, you must deploy regionally. GDPR allows data transfers under specific mechanisms, but many enterprises prefer to keep European user data within EU regions to simplify compliance. Healthcare systems in countries with strict health data laws may require that voice data never leaves national borders. In these cases, regional deployment is not a performance optimization — it is a legal requirement.

If your latency budget is tight enough that 100 milliseconds of additional network latency breaks the user experience, you must deploy at the edge. A voice system targeting 400-millisecond end-to-end latency cannot afford to spend 200 milliseconds on network transport. Reducing network latency from 200 milliseconds to 20 milliseconds by deploying regionally frees up 180 milliseconds for model inference and audio processing, making an otherwise impossible latency target achievable.

If your users complain about delays, ask where those users are located. If complaints cluster in specific geographies far from your data centers, network latency is likely the root cause. This pattern is especially common in B2B systems where enterprise customers in Europe or Asia-Pacific expect parity with the performance their North American colleagues experience. Ignoring geographic latency disparities leads to churn in international markets.

## The Cost-Latency Tradeoff in Multi-Region Deployment

Edge deployment reduces latency but increases cost. Running inference in four regions instead of one quadruples your fixed infrastructure costs if each region runs continuously. Most voice systems do not have uniform global traffic — usage peaks during business hours in each region, creating an opportunity to scale regional infrastructure up and down based on local demand.

Auto-scaling helps, but it introduces cold-start latency. If your Sydney region scales down to zero instances overnight when Australian users are asleep, the first user in the morning triggers a cold start, waiting 30-60 seconds for an instance to spin up. Some teams maintain a small always-on footprint in each region to avoid cold starts, accepting higher baseline costs to preserve user experience.

GPU availability varies by region. The latest H100 GPUs might be readily available in us-east-1 but scarce in ap-southeast-2 or eu-central-1. This availability skew can force you to deploy older or less powerful hardware in some regions, creating performance inconsistencies. A user in Sydney might experience slower LLM inference not because of network latency but because the Sydney region is running on A100s while the Virginia region has H100s.

Data transfer costs compound with multi-region deployment. Sending audio from a user in Tokyo to ASR in ap-northeast-1, then sending the transcript to an LLM in us-east-1, then sending the LLM response back to TTS in ap-northeast-1 incurs multiple cross-region data transfer charges. Cloud providers charge for egress traffic between regions — often 2-8 cents per gigabyte. For high-volume voice systems processing thousands of hours of audio daily, these transfer costs can exceed compute costs.

Some teams compromise by deploying latency-sensitive components at the edge and keeping expensive components centralized. ASR and TTS run in each region to minimize audio transport latency. The LLM runs in a single region with the best GPU availability and lowest cost. This hybrid approach reduces network latency for the perceptually critical components — the user hears their transcription quickly and gets audio output quickly — while centralizing the most expensive inference workload. The tradeoff is that LLM inference still pays cross-region latency, but if LLM latency is 200 milliseconds and network latency is 100 milliseconds, centralizing the LLM adds 100 milliseconds while saving substantial infrastructure costs.

## Measuring Network Latency in Production

You cannot optimize what you do not measure. Instrumenting network latency separately from processing latency is essential for diagnosing performance issues. If total latency degrades from 400 milliseconds to 600 milliseconds, you need to know whether the problem is slower model inference, network congestion, or a configuration change that routed traffic through a different region.

Log the timestamp when the user's request arrives at your edge or load balancer. Log the timestamp when the request reaches your ASR service. The difference is network latency from the user to your infrastructure. Log the timestamp when ASR completes, when the LLM receives the transcript, when the LLM completes generation, when TTS starts, and when TTS completes. Subtracting sequential timestamps isolates latency contributed by each component and by network hops between components.

Track latency by geographic region or by the user's source IP prefix. If users in Japan see P95 latency 200 milliseconds higher than users in California, and both groups route to the same data center, you have identified a geographic disparity. If users in Japan route to a Tokyo data center and still see high latency, the problem is not network distance — it is component performance in that region.

Monitor latency for cross-region communication between services. If your ASR service in eu-west-1 sends transcripts to an LLM in us-east-1, measure the round-trip time for that request. Cloud providers publish expected inter-region latency benchmarks — AWS publishes median latencies between all region pairs. If your observed latency significantly exceeds published benchmarks, investigate routing issues, misconfigured DNS, or network path inefficiencies.

## When Network Latency Is Acceptable

Not every voice system requires sub-50-millisecond network latency. Some use cases tolerate higher latency because the user's expectation is shaped by the context. A voice-enabled document search system where the user speaks a query and waits for the system to retrieve and summarize a policy document can tolerate 800-millisecond latency because the user understands the system is performing a complex task. The pause feels purposeful, not broken.

Asynchronous voice systems — voicemail transcription, recorded meeting summarization, voice memos converted to text — have no real-time latency requirements. These systems can process audio in batch, route requests to the cheapest available region, and take seconds or minutes to complete without harming user experience.

If your users are geographically concentrated and your infrastructure is colocated with them, network latency is negligible. A voice system serving only Japanese customers from a Tokyo data center will see network latencies under 30 milliseconds for the vast majority of requests. Optimizing network topology within a single region has diminishing returns once latency drops below 20 milliseconds.

But for real-time conversational systems serving a geographically distributed user base, network latency is a first-order variable in the latency equation. Ignoring it means accepting that a large fraction of your users will experience degraded performance, no matter how well you optimize model inference. The next subchapter explores how streaming chunking strategies allow you to trade latency for quality within the ASR and TTS components themselves.
