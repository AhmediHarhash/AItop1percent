# 7.2 — Barge-In Detection Latency: The Race to Stop Speaking

In early 2025, a healthcare scheduling assistant launched with voice support. The system could book appointments, check availability, and confirm patient information. Users loved it for simple requests. But when a user tried to interrupt — "no, not Tuesday, Wednesday" — the system kept talking for another half-second, listing Tuesday appointment slots the user had already rejected. By the time the system stopped and asked "I'm sorry, could you repeat that?" the user had lost trust. Interruptions did not work. Users learned to wait. The voice interface became slower than the touch interface it was supposed to replace. The root cause was not the voice activity detection accuracy. The VAD correctly identified that the user had started speaking. The problem was the time between detection and silence. The system took 180 milliseconds to stop. In conversation, that is an eternity.

## The 100-Millisecond Threshold

Human conversation operates on precise timing. When you interrupt someone, you expect them to stop talking within 100 to 150 milliseconds. If they keep talking longer than that, you perceive them as ignoring you or as not having heard you. You either raise your voice and interrupt more forcefully, or you stop trying and wait for them to finish. This timing expectation transfers directly to voice systems. When a user interrupts a voice assistant, they expect immediate silence. If the system keeps talking for 200 milliseconds or more, the user experiences frustration. They spoke, and the system did not respond. The interaction feels broken.

This is not a soft preference. It is a hard perceptual boundary. Research on turn-taking in human conversation consistently shows that gaps longer than 200 milliseconds feel unnatural. The listener perceives the speaker as hesitating, confused, or unresponsive. In voice systems, the same perceptual boundary applies. If the system takes longer than 100 milliseconds to stop speaking after the user starts speaking, the user perceives the system as slow. If the system takes longer than 200 milliseconds, the user perceives the system as broken. The difference between 80 milliseconds and 120 milliseconds is the difference between "responsive" and "sluggish." The difference between 120 milliseconds and 250 milliseconds is the difference between "sluggish" and "ignoring me."

The implication is that you have approximately 100 milliseconds of total latency budget from the moment the user's first phoneme reaches the microphone to the moment the speaker stops emitting sound. This budget must cover voice activity detection, the decision to trigger barge-in, the command to stop TTS, the time for the TTS to halt synthesis, the time to flush audio buffers, and the time for the speaker to go silent. Every millisecond in this chain adds to the total. If your VAD takes 50 milliseconds to detect speech, your TTS stop command takes 20 milliseconds to execute, your audio buffer takes 30 milliseconds to flush, and your speaker has 10 milliseconds of hardware latency, you are already at 110 milliseconds. That is the bare minimum for acceptable performance, and it assumes perfect conditions with no network delays, no processing hiccups, and no contention for system resources.

## The VAD-to-Stop Pipeline

The barge-in detection pipeline begins with the voice activity detector. The VAD processes incoming audio in short frames, typically 10 to 20 milliseconds each. For each frame, the VAD produces a probability that the frame contains speech. This probability is compared against a threshold. If the probability exceeds the threshold for a sufficient number of consecutive frames, the VAD triggers a speech detection event. The number of frames required to trigger depends on the tuning. A single frame is too noisy — you will get false positives from transient sounds. Two frames — 20 to 40 milliseconds of sustained speech — is more reliable but introduces latency. Three frames — 30 to 60 milliseconds — is even more reliable but pushes the latency budget.

Most production VADs use a two-frame or three-frame trigger with adaptive thresholding. The threshold adjusts based on ambient noise levels. In a quiet room, the threshold is low. In a noisy environment, the threshold is higher to avoid false positives from background sounds. This adaptive behavior improves accuracy but adds complexity to latency tuning. The VAD must estimate the noise floor, adjust the threshold, and then apply the threshold to incoming frames. This estimation can add 10 to 20 milliseconds of additional latency if the noise floor changes rapidly — for example, if a door slams or someone nearby starts talking just before the user interrupts the system.

Once the VAD triggers, it sends a barge-in event to the conversation controller. The controller is responsible for managing the dialogue state, including when the system is speaking, when it is listening, and when it is processing. When the controller receives a barge-in event, it must immediately issue a stop command to the TTS subsystem. This command may be a function call if the TTS is local, or an API request if the TTS is cloud-based. For cloud TTS, the stop command incurs network latency. A round-trip to a cloud TTS service can add 20 to 50 milliseconds depending on geographic distance and network conditions. This is why low-latency barge-in almost always requires local TTS. Streaming cloud TTS can work if the stop command is sent over the same persistent connection used for audio streaming, avoiding the overhead of establishing a new connection. But even then, the latency is higher than local TTS.

## Stopping TTS Mid-Sentence

The TTS subsystem receives the stop command and must halt synthesis immediately. This is harder than it sounds. Most TTS systems generate audio in chunks. A typical streaming TTS might produce 100 or 200 milliseconds of audio at a time and buffer it for playback. When a stop command arrives, the TTS must stop generating new chunks, but it cannot retroactively erase chunks that have already been sent to the audio playback system. Those chunks are in a buffer, waiting to be played. The TTS can only prevent future chunks from being generated. The buffered audio will still play unless the playback system itself is commanded to flush its buffer.

This creates a two-stage stop. First, the TTS halts synthesis. Second, the audio playback system flushes its buffer. The playback system is typically managed by the operating system or by an audio framework like Core Audio on iOS, AAudio on Android, or ALSA on Linux. These frameworks have their own buffering to ensure smooth playback and avoid audio glitches. The buffer size varies by platform and configuration, but 50 to 100 milliseconds is common. When a flush command is issued, the framework stops playback and discards the buffered audio. But the flush command itself takes time to execute. The framework must interrupt the audio thread, clear the buffer, and signal that playback has stopped. This can take 10 to 30 milliseconds depending on the platform and the current state of the audio thread.

In the worst case, the system generates a chunk of audio, the chunk is buffered, the stop command arrives, the TTS halts, the buffer flush command is issued, and the buffered audio finishes playing before the flush completes. The user hears an extra 50 to 100 milliseconds of speech after they started interrupting. From their perspective, the system ignored them. They spoke, and the system kept talking. The total latency is the sum of VAD detection time, stop command transmission time, TTS halt time, buffer flush command time, and residual playback time. If each of these stages takes 40 to 50 milliseconds, the total is 200 milliseconds or more. That is too slow.

## Optimization Strategies for Sub-100ms Stop Latency

The most effective optimization is to use a low-latency local TTS with minimal buffering. A local TTS eliminates network latency from the stop command. The TTS runs on the same device or server as the conversation controller, so the stop command is a function call or inter-process message, not a network request. Execution time is typically under 5 milliseconds. The TTS should be designed to generate audio in small chunks — 20 to 40 milliseconds at a time — and should support immediate halt on command. When the halt command is received, the TTS stops mid-chunk if necessary, discarding any partially synthesized audio that has not yet been sent to the playback system.

The playback buffer should be kept as small as possible without causing audio glitches. A 20-millisecond buffer is ideal for barge-in but may cause underruns if the TTS cannot sustain the required throughput. A 50-millisecond buffer is a safer default. The buffer size should be configurable and tuned based on the TTS performance characteristics and the platform's scheduling latency. On a high-priority real-time thread, a 20-millisecond buffer is achievable. On a standard thread competing with other processes, 50 milliseconds is more realistic. The playback system should support a fast flush operation that preempts the current buffer and stops playback within one scheduling quantum — typically 5 to 10 milliseconds on modern operating systems.

The VAD should use a two-frame trigger in quiet environments and a three-frame trigger in noisy environments. The frame size should be 10 milliseconds to minimize latency. The threshold should be set aggressively enough to trigger on the first clear speech signal but conservatively enough to avoid false positives from transient noise. This is a tuning exercise. You cannot optimize for both simultaneously. The right trade-off depends on your use case. A medical assistant that handles sensitive information in a quiet clinic can use an aggressive threshold. A customer service bot that operates in noisy call center environments needs a conservative threshold. Test both false positive and false negative rates during tuning. A false positive rate of 2 to 5 percent is acceptable if it enables a faster trigger. A false negative rate above 5 percent means users will perceive barge-in as unreliable.

## The Role of Pre-Stopping Prediction

Some advanced systems use predictive barge-in, where the VAD begins analyzing audio before the user speaks and prepares the TTS to stop based on early acoustic signals. This technique is speculative. The VAD looks for acoustic patterns that precede speech — a sharp increase in energy, a shift in frequency spectrum, or a change in the noise floor that suggests someone is about to speak. When these patterns are detected, the system pre-arms the stop mechanism. The TTS is warned that a barge-in may be imminent. The playback buffer is reduced. The audio thread is given higher priority. The system is poised to stop at the first confirmed speech signal.

This approach can reduce stop latency by 20 to 40 milliseconds because some of the preparation happens before the user's first phoneme. But it also increases the risk of false positives. A shift in background noise, a cough, or a rustle of clothing can trigger the pre-arming state. If the system overreacts and stops unnecessarily, the user experiences an interruption in the system's speech that was not caused by their own voice. The system appears glitchy. The user loses trust. Predictive barge-in works best in controlled environments where acoustic conditions are stable and predictable. It is risky in noisy environments or in multi-user scenarios where other people's voices can trigger false positives.

The other risk with predictive barge-in is that it can introduce gender bias, accent bias, or age bias. If the pre-stopping model is trained primarily on adult male voices in quiet environments, it may react more slowly to women's voices, children's voices, or voices with non-native accents. These voices may have different frequency profiles or different energy patterns. The model may not recognize their pre-speech acoustic signatures as reliably. The result is that some users experience faster barge-in responses than others. This is not just a performance issue. It is a fairness issue. You must evaluate pre-stopping accuracy across demographic groups and acoustic conditions to ensure it does not degrade the experience for specific populations.

## Measuring Stop Latency in Evaluation

You measure barge-in stop latency by injecting a user utterance into the audio stream at a precise timestamp while the system is speaking and measuring the time from the start of the user's utterance to the moment the system's audio stops. This requires test instrumentation that can control both playback and capture with millisecond precision. The test setup plays a pre-recorded system utterance through the speaker. At a predetermined point in the utterance, a simulated user speech signal is injected into the microphone input. The timestamp of the injection is recorded. The timestamp when the system's audio output goes silent is also recorded. The difference is the stop latency.

Repeat this test across at least 100 interruptions at different points in the system's utterance, with different user speech patterns, and under different acoustic conditions. Measure the median latency, the 95th percentile latency, and the maximum latency. The median tells you typical performance. The 95th percentile tells you the worst performance most users will encounter. The maximum tells you the pathological case. For production quality, the median should be under 80 milliseconds, the 95th percentile should be under 120 milliseconds, and the maximum should be under 200 milliseconds.

Also measure the variance. If the median is 70 milliseconds but the 95th percentile is 180 milliseconds, you have a consistency problem. Most interruptions work well, but one in twenty feels broken. Users will notice the inconsistency and will become hesitant to interrupt, unsure whether the system will respond quickly or slowly. Consistent latency is almost as important as low latency. A system that always stops in 100 milliseconds feels more responsive than a system that stops in 60 milliseconds most of the time but takes 200 milliseconds occasionally.

## What Happens When Stop Latency Is Too High

When stop latency exceeds 150 milliseconds, users perceive the system as unresponsive. They interrupt, the system keeps talking, and they conclude that interruptions do not work. This changes their behavior. They stop trying to interrupt and instead wait for the system to finish before speaking. This defeats the purpose of having barge-in. The conversation becomes turn-based instead of fluid. The user asks a question, the system responds, the user waits for the response to finish, and only then do they ask a follow-up or issue a correction. The interaction slows down. What should take 15 seconds takes 30 seconds because the user is adding padding to every turn.

In high-stakes domains like healthcare or customer service, slow barge-in has a second consequence. Users become frustrated and abandon the voice interface entirely. They switch to touch, to chat, or to talking to a human. A voice scheduling assistant that cannot handle interruptions loses to a web form. A voice support bot that makes users wait loses to live chat. The abandonment rate correlates directly with barge-in latency. In one 2025 study of customer service voice bots, systems with sub-100ms barge-in had an abandonment rate of 8 percent. Systems with 150 to 200ms barge-in had an abandonment rate of 18 percent. Systems with greater than 200ms barge-in had an abandonment rate of 32 percent. Users gave up. They hung up. They found another way to solve their problem.

The third consequence is user frustration that transfers to the brand. A bad voice UX is not just a bad feature. It is a bad impression. The user associates the sluggish, unresponsive voice system with the company that deployed it. They tell friends. They leave reviews. They rate the app poorly. A healthcare provider that deploys a scheduling assistant with 200-millisecond barge-in latency does not just lose appointments. They lose patient trust. A bank that deploys a voice assistant with slow barge-in does not just lose transactions. They lose the perception of being modern and customer-focused. The cost of bad barge-in is not just operational. It is reputational.

The next subchapter covers false positive barge-ins — when noise, coughs, or background voices trigger interruption and the system stops speaking unnecessarily.
