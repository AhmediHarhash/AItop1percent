# 3.13 — The Cold Start Problem in Voice Pipelines

In August 2025, a healthcare voice assistant shipped with impressive response times in testing — averaging 280ms from speech end to TTS start. In production, 40% of users experienced their first interaction taking over two seconds. The team had optimized the hot path beautifully. They had forgotten that most users don't keep voice apps running continuously. Every conversation began with a cold start.

Cold start is the latency penalty paid when a system must initialize before it can serve a request. In web services, cold start adds tens to hundreds of milliseconds. In voice systems with large language models, STT engines, and TTS synthesizers, cold start can add 500 to 2000 milliseconds. That's the difference between a system that feels instant and one that feels broken. The first response sets the user's expectations for the entire conversation. A two-second delay on turn one makes users assume every turn will be slow, even if subsequent responses arrive in 300ms.

The cold start problem is not optional to solve. It is the first thing users experience. It determines whether they trust the system enough to continue.

## The Sources of Cold Start Latency

Cold start latency comes from multiple layers of initialization, each adding delay before the system can process speech.

**Model loading** is the most obvious source. When a voice pipeline starts, it must load the STT model, the LLM, and the TTS model into memory. For small models, this takes 200-500ms. For large models — GPT-5, Claude Opus 4.5, Gemini 3 Pro — loading can take 1000-1500ms. If models are stored on disk, loading time includes disk read latency. If models are stored remotely and fetched on demand, loading time includes network transfer. The healthcare assistant was loading a 70-billion-parameter LLM from disk on every session start. The model file was 140GB. Even with SSD storage, the first inference took 1.8 seconds.

**Connection establishment** adds delay when the system must establish network connections to remote services. If STT runs as a separate service, the first request includes TCP handshake, TLS negotiation, and service discovery. If the LLM is accessed via API, the first call includes authentication, connection pooling setup, and routing. If TTS is remote, the same applies. Each connection adds 50-150ms. When all three components are remote and connections are established serially, you can add 300-600ms before the first byte of data flows.

**Cache warming** adds delay when caches start empty. Prompt caching systems store recent prompts and their KV caches to avoid recomputation. On cold start, these caches are empty. The first inference computes everything from scratch. If your system relies on prompt caching to achieve 150ms LLM latency, the cold start might take 600ms because the cache hasn't built yet. The system is technically working correctly — it's just working slowly until it warms up.

**Resource allocation** adds delay in containerized and serverless environments. When a voice service is deployed on Kubernetes or as a Lambda function, the first request may trigger container instantiation. The platform must allocate CPU and memory, pull the container image, start the runtime, and initialize the application. In serverless environments, this can add 500-2000ms. In Kubernetes with autoscaling, it can add 200-800ms. The second request hits a warm container and sees none of this delay. The first user pays the full cost.

**Dependency initialization** adds delay when the system must initialize libraries, load configuration, and establish internal state. Even if the model is preloaded and connections are established, the application code itself has initialization overhead. Python runtimes load modules. Frameworks initialize routers and middleware. Configuration files are parsed. Logging systems connect. For a complex voice pipeline with multiple services, this initialization can add 100-300ms. It's invisible in benchmarks where the system runs for hours. It's painfully visible to the first user.

The healthcare assistant had all five sources active simultaneously. Every conversation started with container creation, model loading, connection establishment, cache warming, and dependency initialization. The cumulative delay was 2.1 seconds. After that, responses averaged 280ms. But users never gave the system a second chance. They assumed it was always slow and switched to keyboard input.

## Warmup Strategies That Hide Cold Start

The goal of warmup is to pay the initialization cost before the user arrives, so the first real interaction sees hot-path latency.

**Preloading models** at service start eliminates model load time from the user-facing path. When the voice service container starts, it immediately loads the STT model, LLM, and TTS model into memory. This happens during container initialization, before the service is marked ready to receive traffic. Kubernetes health checks ensure the service doesn't receive requests until models are loaded. The user's first request hits a system with models already in memory. The 1.8-second LLM load happens before any user is connected. The cost doesn't disappear — it's moved to service startup. But startup happens once per container, not once per conversation.

The healthcare team implemented model preloading in October 2025. Container startup time increased from 2 seconds to 8 seconds, but the first user request dropped from 2.1 seconds to 620ms. The 620ms was cache warming and connection establishment. Users noticed. The perceived responsiveness of the system changed overnight.

**Connection pooling** eliminates connection establishment time from repeated requests. Instead of creating a new connection for each inference, the service maintains a pool of persistent connections to STT, LLM, and TTS services. The first request uses a connection from the pool — which was established during service initialization. Subsequent requests reuse the same connections. If a connection dies, the pool replaces it asynchronously. The user never sees connection setup latency.

Connection pooling is standard practice in web services but often neglected in voice pipelines. Teams assume that because they're calling an API, connection management is handled automatically. It's not. HTTP clients default to creating new connections unless explicitly configured for pooling. The healthcare assistant was creating a new HTTPS connection to the LLM API for every turn. Each connection added 120ms. Enabling connection pooling with a pool size of 10 persistent connections dropped per-turn latency by 120ms and eliminated connection setup from the user-facing path entirely.

**Cache priming** eliminates cache-miss latency from the first real request. When the service starts, it sends a dummy request through the entire pipeline — STT to LLM to TTS. The dummy request has no user attached. Its only purpose is to populate caches. The LLM processes the dummy prompt and stores the KV cache. The TTS synthesizes the dummy response and caches phoneme embeddings. When the first real user arrives, the caches are warm. The first real request benefits from cache hits just like the hundredth request.

Cache priming works best when the dummy request is similar to real user requests. If your system has a standard greeting or opening phrase, use that as the priming request. If your system has a shared system prompt that appears in every inference, send that prompt through the LLM during warmup. The closer the dummy request is to real traffic, the more cache hits you get on the first real turn.

**Lazy connection establishment with pre-warm** is a hybrid strategy for systems that can't afford to hold persistent connections idle but still want to avoid first-request connection latency. The service establishes connections during startup but sends a lightweight ping every 30 seconds to keep them alive. If a connection idles for too long and the remote service closes it, the ping fails and the connection is re-established immediately — before the next user request arrives. The user-facing path always sees a live connection.

**Predictive warmup** for high-traffic systems involves starting containers and warming models before traffic arrives, based on traffic forecasting. If your voice system sees a surge of users every morning at 8am, you can start warming containers at 7:50am. By the time users arrive, all containers are hot. This requires traffic prediction and proactive scaling, but it eliminates cold start for the majority of users. The cost is running warm containers slightly ahead of demand.

## Connection Pooling Implementation

Connection pooling is not automatic. It requires explicit configuration in every HTTP client and service SDK you use.

**For LLM API clients**, most SDKs default to creating new connections per request unless you configure a session or connection pool. The OpenAI Python SDK, for example, creates a new HTTP session for each API call unless you instantiate the client with a persistent session. The Anthropic SDK behaves the same way. The fix is to create the client once at service startup and reuse the same client instance for all requests. The client manages the connection pool internally.

The healthcare assistant was creating a new OpenAI client for every turn. The fix was a four-line change: instantiate the client once in the service initialization code, store it as a module-level variable, and call it from request handlers. First-turn latency dropped by 110ms.

**For STT and TTS services**, the same principle applies. If you're calling a remote STT service, use a persistent HTTP client with connection pooling enabled. If the STT service supports WebSocket connections, establish the WebSocket during service startup and keep it alive with periodic pings. WebSocket eliminates per-request connection overhead entirely. The first audio chunk and the hundredth audio chunk use the same connection.

**Connection pool sizing** matters. A pool that's too small creates contention — requests wait for a free connection. A pool that's too large wastes memory and file descriptors. For most voice services handling 10-100 concurrent conversations, a pool size of 10-20 connections per downstream service is sufficient. If your service handles 1000 concurrent conversations, scale the pool accordingly. Monitor connection pool exhaustion metrics — if requests are frequently waiting for available connections, increase the pool size.

**Connection health monitoring** prevents using stale connections. Even with pooling, connections can die silently — the remote service restarts, the network hiccups, the idle timeout expires. If your client reuses a dead connection, the request fails and retries, adding latency. Modern connection pools support health checks: before reusing a connection, send a lightweight ping. If the ping fails, discard the connection and establish a new one. The health check adds 5-10ms but avoids the 500ms penalty of retrying a failed request.

## Hiding Cold Start from Users

Even with warmup, some cold start latency is unavoidable. The question is whether the user sees it.

**Progressive disclosure** means showing the user something immediately, even if the full response isn't ready. When the user presses the voice button, play a soft acknowledgment sound — a gentle tone or a brief "listening" animation. The user knows the system heard them. While the system processes speech, show a visual indicator that work is happening. When the LLM is generating a response, display partial results as they arrive. The user sees progress. A 600ms delay with visible progress feels faster than a 400ms delay with no feedback.

The healthcare assistant added a 50ms acknowledgment tone when the user stopped speaking. It signaled "I heard you, processing now." User-perceived latency dropped even though technical latency was unchanged. The acknowledgment tone cost 50ms but made the subsequent 600ms feel like active processing rather than dead time.

**Speculative execution** means starting work before you're certain you need the result. If your voice system has a common opening phrase — "How can I help you today?" — you can start synthesizing the TTS for that phrase during service startup, before any user arrives. When the first user connects and the system decides to use the greeting, the audio is already generated. Playback starts immediately. Speculative execution trades compute cost for latency. You generate responses you might not use. The cost is acceptable when the speculative work is cheap and the latency win is large.

**Background model loading** in multi-model systems means loading secondary models asynchronously while serving the first request with a fast fallback. If your system uses a large LLM for complex queries but a small LLM for simple ones, you can start with the small model loaded and load the large model in the background. The first user gets a fast response from the small model. By the time a complex query arrives, the large model is ready. The user never sees model load time.

**Session persistence** means keeping the user's session warm between turns. After the user's first request, don't tear down the conversation state. Keep the LLM context, the connection pool, and the cached embeddings alive for 30-60 seconds. If the user speaks again within that window, the second turn sees no cold start at all. If the user doesn't speak again, the session times out and resources are released. Session persistence assumes users engage in multi-turn conversations. For systems where 70% of interactions are multi-turn, session persistence eliminates cold start from 70% of turns at the cost of holding resources idle for 30 seconds.

The healthcare assistant implemented session persistence with a 45-second timeout. Average per-session latency dropped by 35% because second and third turns saw no initialization overhead. Memory usage increased by 12% because sessions stayed alive longer. The trade-off was worth it.

## Cold Start in Serverless and Edge Environments

Serverless environments amplify cold start because they optimize for cost over latency. A Lambda function that hasn't been invoked in five minutes is shut down. The next invocation must start a new container, load the runtime, initialize dependencies, and load models. This can add 1-3 seconds. For voice systems, this is unacceptable.

**Provisioned concurrency** in AWS Lambda keeps a minimum number of warm instances running at all times. These instances are pre-initialized and ready to serve requests immediately. The first user hits a warm instance. Cold start is eliminated for traffic within provisioned capacity. The cost is paying for idle instances. For a voice system with unpredictable traffic, you provision enough warm instances to cover the 90th percentile of traffic. The 10% of requests that exceed provisioned capacity hit cold start. The 90% see hot-path latency.

**Edge deployment** moves models closer to users, reducing network latency and enabling persistent local state. If your voice system runs on Cloudflare Workers or AWS Lambda at Edge, the service is deployed in 200+ locations worldwide. The user connects to the nearest edge location. The edge maintains warm containers and persistent connections. Cold start is localized to the edge node, not a central region. If the edge node is idle, cold start affects only users in that geography. Most users hit warm nodes.

**Hybrid architectures** keep lightweight models at the edge and heavy models in the cloud. The edge handles STT and TTS locally. The LLM runs in the cloud but is kept warm by periodic health checks from all edge nodes. The user's audio is processed at the edge, sent to a warm cloud LLM, and synthesized back at the edge. Cold start affects only the edge components, which are small and fast to initialize. The large LLM never goes cold because it receives traffic from all edges.

## Measuring Cold Start in Production

Cold start is not a single metric. It's a distribution. Some users see it. Some don't. The question is: what percentage of users experience cold start, and how bad is it?

**First-request latency vs steady-state latency** is the primary measurement. Tag every request with a session ID. Measure the latency of the first request in each session separately from subsequent requests. If first-request latency is 800ms and steady-state is 300ms, you have a 500ms cold start penalty. If first-request latency is 320ms and steady-state is 300ms, your warmup strategies are working.

The healthcare assistant tracked first-request latency as a separate metric. Before optimization, first-request p50 was 2100ms and steady-state p50 was 280ms. After warmup strategies, first-request p50 was 340ms and steady-state p50 was 280ms. The cold start penalty dropped from 1820ms to 60ms. The 60ms residual was cache warming that couldn't be avoided.

**Cold start rate** is the percentage of requests that experience cold start. If 40% of sessions see elevated first-request latency, your cold start rate is 40%. If warmup strategies reduce that to 5%, you've eliminated cold start for 95% of users. The goal is not zero cold start — that's impossible in auto-scaling environments. The goal is reducing cold start rate to single digits.

**Time-to-ready** measures how long the service takes to become ready after startup. If your container takes 8 seconds to load models and establish connections, time-to-ready is 8 seconds. If auto-scaling starts new containers when traffic spikes, every new container has an 8-second window where it can't serve traffic. If traffic spikes faster than 8 seconds, some users hit cold containers. Reducing time-to-ready reduces the window of vulnerability.

The healthcare assistant reduced time-to-ready from 8 seconds to 3.5 seconds by loading only the most commonly used models at startup and loading secondary models lazily. Cold start rate during traffic spikes dropped from 18% to 4%.

Cold start is the hidden tax on every voice interaction. Users don't see the engineering behind it. They only see whether the system responds instantly or makes them wait. The systems that feel instant have eliminated cold start through preloading, connection pooling, cache priming, and strategic resource management. The systems that feel slow have left cold start for users to experience. The difference is not in the model or the infrastructure. It's in whether the team treated the first 600ms as seriously as the steady-state 300ms.

Next, we examine the latency optimization patterns that reliably shave 50-200ms from voice pipelines — and the patterns that waste time chasing diminishing returns.
