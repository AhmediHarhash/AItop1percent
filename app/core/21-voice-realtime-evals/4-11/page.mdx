# 4.11 — Building Production ASR Evaluation Datasets

In early 2025, a voice-controlled medical records platform launched with an ASR system that achieved 2.1 percent word error rate on the vendor's benchmark. The vendor's benchmark consisted of carefully curated audio from audiobook narrators, podcast hosts, and studio recordings — professional speakers in quiet environments with standard American accents. The company's actual users were emergency room physicians speaking rapidly in noisy hospital environments, using medical terminology, often with non-American accents. The production WER was 14.7 percent. The benchmark was meaningless.

The team spent eight weeks building a custom evaluation dataset: 6,400 real utterances from their production environment, professionally transcribed, covering their actual accent distribution, noise conditions, and domain vocabulary. They tested five ASR providers on this dataset. The vendor whose benchmark showed 2.1 percent WER scored 11.3 percent on the custom dataset — still the worst of the five. The provider they ultimately chose had scored 4.8 percent WER on the vendor benchmark but 7.2 percent on the custom dataset. The custom dataset revealed the truth: what works in general doesn't necessarily work in your specific environment.

ASR evaluation datasets must match your production traffic. Your accents, your noise profiles, your speaking styles, your domain-specific vocabulary, your real-world conditions. Generic benchmarks are useful for comparing model families, but they tell you almost nothing about performance in your application. Building a production ASR eval dataset is not optional if you care about accuracy. It's the only way to measure what actually matters: how well the system works for your users, in your environment, on your tasks.

## Representative Sampling: Matching Production Distribution

The first rule of building an ASR evaluation dataset is representation. The dataset must reflect the distribution of audio conditions in production. If 40 percent of your production audio comes from mobile phones in noisy environments, 40 percent of your eval dataset should too. If 18 percent of your users have Indian accents, 18 percent of your eval dataset should include Indian accents. If your system handles both scripted prompts (like menu navigation) and free-form dictation, your dataset should include both in the same proportions as production.

A customer service voice bot analyzed one month of production traffic: 62 percent mobile phone calls, 28 percent landline calls, 10 percent VoIP calls. Noise levels: 35 percent quiet (background noise below 40 dB), 48 percent moderate (40 to 60 dB), 17 percent loud (above 60 dB). Speaker accents: 51 percent American English, 22 percent British English, 14 percent Indian English, 8 percent Spanish-accented English, 5 percent other. The team built a 5,000-utterance eval dataset that matched these distributions. They stratified sampling: randomly selected utterances from each combination of phone type, noise level, and accent until the dataset matched the production distribution.

Track your production distribution along every relevant dimension: device type, noise level, accent, speaking rate, utterance length, domain vocabulary density, and any other factor that affects ASR accuracy. Use production logs to measure these distributions. Then sample your eval dataset to match. If you can't perfectly match — maybe you don't have enough samples of a rare condition — oversample the rare condition slightly to ensure you have enough examples to measure it, but document the deviation and weight the final metrics to reflect true production proportions.

A navigation voice assistant had 94 percent of users speaking in cars. The team's eval dataset initially included 60 percent car audio and 40 percent other environments because they wanted to test generalization. This was a mistake. The dataset should have been 94 percent car audio to match production. They rebuilt it with correct proportions. The WER on the rebuilt dataset was 1.8 percentage points worse than the original, but it was accurate — the original dataset had underweighted the car environment and made performance look better than it was.

Sampling must also cover **temporal variation**. Production conditions change over time: new user cohorts, new devices, seasonal noise patterns (air conditioning in summer, heaters in winter). Sample audio from multiple time periods. A dictation app sampled 1,000 utterances per month for six months to build a 6,000-utterance dataset. This captured seasonal variation in background noise and user behavior. An eval dataset sampled from a single week would have missed these shifts.

## Minimum Dataset Sizes for Reliable Measurement

ASR evaluation datasets must be large enough to produce statistically reliable WER estimates. A dataset of 100 utterances might show 5 percent WER with a confidence interval of plus or minus 3 percentage points. That's not useful — you can't tell if a system with 5 percent WER is actually better than a system with 6 percent WER. You need enough utterances to narrow the confidence interval.

A general rule: **1,000 utterances minimum for basic WER measurement, 3,000+ for comparing systems, 5,000+ for fine-grained analysis**. At 1,000 utterances, the 95 percent confidence interval on WER is roughly plus or minus 1.5 percentage points (assuming WER near 5 percent). At 3,000 utterances, it's plus or minus 0.8 percentage points. At 5,000, plus or minus 0.6 percentage points. The confidence interval narrows with the square root of sample size — doubling the dataset size reduces the interval by about 30 percent.

A voice assistant team compared two ASR providers on a 500-utterance test set. Provider A: 6.2 percent WER. Provider B: 5.8 percent WER. They selected Provider B. Six months later, they ran a larger comparison on 4,000 utterances. Provider A: 5.9 percent WER. Provider B: 6.3 percent WER. The initial result was noise. The small dataset led to the wrong decision. The larger dataset revealed the truth.

For **segment-specific analysis**, you need even more data. If you want to measure WER separately for each accent group, each noise level, each device type, you need enough samples in each segment to get reliable estimates. A legal transcription service wanted to measure WER for quiet, moderate, and noisy conditions separately. They needed at least 1,000 utterances per condition, so the total dataset was 3,500 utterances (not perfectly balanced, but at least 1,000 in each bucket). With fewer samples per segment, the segment-level WER estimates would have been too noisy to act on.

Track **word count, not just utterance count**. WER is computed at the word level, so total word count matters more than utterance count for statistical power. A dataset of 3,000 short utterances (average 4 words each, 12,000 total words) has less statistical power than a dataset of 1,500 longer utterances (average 12 words each, 18,000 total words). Aim for at least 50,000 total words in your dataset for basic WER measurement, 100,000+ for robust comparisons.

## Annotation Protocols: Ensuring Ground Truth Quality

An ASR evaluation dataset is only as good as its ground truth labels. If the human transcriptions contain errors, your WER measurements are garbage. Professional transcription requires clear guidelines, quality control, and validation.

Define a **transcription style guide** that specifies how to handle ambiguities. Should annotators transcribe exactly what was said, including false starts and filler words ("um," "uh"), or clean it up? Should they expand contractions ("don't" vs "do not")? Should they transcribe numbers as digits ("23") or words ("twenty-three")? Should they include punctuation? The ASR output will include or exclude these elements based on how the model was trained. Your ground truth must match the ASR's output format, or WER comparisons will be meaningless.

A medical dictation platform defined strict rules: transcribe verbatim including false starts, preserve all medical terminology exactly as spoken, expand abbreviations only if the speaker clearly said the full term, use sentence-case punctuation, transcribe numbers as they were spoken (if the physician said "twenty-three," write "twenty-three," not "23"). The ASR system they used also transcribed verbatim with punctuation. Ground truth and ASR output matched in format, making WER calculations valid.

Use **multiple annotators per utterance** for high-stakes datasets. Have two or three annotators independently transcribe the same audio, then measure inter-annotator agreement. If agreement is high (above 95 percent word-level match), the transcription is reliable. If agreement is low, the audio is ambiguous or the guidelines are unclear. Resolve disagreements through discussion or a third annotator, then update the guidelines to clarify the ambiguous case.

A customer service voice bot had three annotators transcribe 500 utterances independently. Inter-annotator agreement: 91 percent. The 9 percent disagreement was concentrated in a few categories: transcribing background speech (one annotator included it, two ignored it), handling overlapping speech (different strategies for attribution), and spelling of proper names (no consistent rule). The team updated their guidelines to clarify these cases, then re-annotated the dataset. Agreement improved to 96 percent.

Track **annotator accuracy** by having a quality control reviewer spot-check 10 percent of transcriptions. If the reviewer finds errors, send the utterance back for re-annotation. If a specific annotator consistently produces low-quality work, remove them from the pool or provide additional training. A legal transcription service maintained a quality score for each annotator: percentage of utterances that passed QC review on the first pass. Annotators with scores below 92 percent were retrained. Annotators with scores below 85 percent after retraining were removed.

Use **professional transcription services** if you lack in-house capacity. Services like Rev, GoTranscript, or Scribie provide human transcription with accuracy guarantees (typically 98 to 99 percent accuracy). Costs range from one dollar to three dollars per audio minute as of 2026. For a 5,000-utterance dataset with average utterance length of 8 seconds (about 667 minutes total), professional transcription costs 700 to 2,000 dollars. That's cheap compared to the cost of choosing the wrong ASR provider or deploying an inaccurate system.

## Handling Domain-Specific Vocabulary and Entities

Generic ASR systems struggle with domain-specific terms: medical vocabulary, legal jargon, technical product names, company-specific acronyms. Your eval dataset must include these terms in realistic proportions to measure how well the ASR handles them.

A pharmaceutical voice assistant dealt with drug names, chemical compounds, and medical conditions. The team's first eval dataset included 3 percent domain-specific terms — much lower than the 18 percent rate in production. The dataset underestimated WER on domain terms. They rebuilt the dataset with realistic domain term density: 18 percent of words were medical vocabulary. WER on the new dataset was 3.2 percentage points higher. The original dataset had hidden a major accuracy problem.

Sample utterances that include **rare but critical terms**. A legal transcription service handled case law citations, statutory references, and Latin legal terms. These appeared in only 6 percent of utterances but were critical to get right. The team oversampled utterances with legal terms to ensure the eval dataset included at least 800 examples. This allowed them to measure legal term accuracy separately: overall WER 4.1 percent, legal term WER 11.3 percent. The ASR struggled with legal terms, and oversampling made this visible.

Label **entity types** in your ground truth if entity accuracy matters. For each proper name, product name, location, or domain term, tag it in the transcription. Then measure WER separately on entities vs. non-entities. A navigation assistant measured overall WER at 5.8 percent but place name WER at 9.2 percent. Place names were the failure mode. The team tuned the ASR's language model with a place name vocabulary list, and place name WER dropped to 6.1 percent. Without entity-level labeling, they wouldn't have diagnosed the problem.

Include **homophones and near-homophones** that matter in your domain. Medical terms have many: "hyper-" vs "hypo-", "ileum" vs "ilium", "cord" vs "chord." If your ASR confuses these, the transcription might look plausible but mean the opposite. A medical eval dataset specifically included 200 utterances with high-risk homophone pairs. The team measured homophone error rate: 12.8 percent. One in eight homophones was transcribed as the wrong variant. This was unacceptable for clinical use, so the team switched to a medical-specialized ASR provider with homophone error rate of 3.1 percent.

## Noise Profiles and Environmental Realism

Real-world audio is noisy. Background conversations, traffic, wind, music, machinery, electronic interference. Your eval dataset must include realistic noise at realistic levels, or your WER estimates will be too optimistic.

Record audio in **actual production environments**, not studios. A delivery logistics voice app was used by drivers in trucks. The team initially recorded eval audio in an office with simulated truck noise played through speakers. WER on this dataset: 6.4 percent. WER in production: 11.2 percent. The simulated noise didn't match real truck noise — real trucks have engine vibration, road noise, irregular bumps, and wind patterns that studio simulations miss. The team re-recorded the eval dataset in actual trucks. WER on the new dataset: 10.8 percent — much closer to production.

If you can't record in production environments, use **noise augmentation** on clean recordings. Add background noise samples (cafes, streets, offices, vehicles) at realistic signal-to-noise ratios (SNR). A customer service bot augmented clean recordings with noise samples from the MUSAN dataset (a public collection of music, speech, and noise). They measured SNR in production: 10 to 20 dB for most calls. They added noise to clean recordings to match this SNR range. WER on augmented data: 8.1 percent. WER in production: 8.4 percent. Close enough to be useful.

Vary noise levels across the dataset. Don't make every sample equally noisy. Production traffic includes quiet calls, noisy calls, and everything in between. A voice assistant team stratified their dataset into three SNR buckets: high SNR (above 20 dB, quiet), medium SNR (10 to 20 dB, moderate), low SNR (below 10 dB, noisy). They matched the production distribution: 40 percent high SNR, 45 percent medium, 15 percent low. Then they measured WER separately for each bucket. High SNR: 3.2 percent WER. Medium: 7.1 percent. Low: 18.6 percent. This revealed that their ASR degraded sharply in low SNR conditions, and they prioritized noise robustness improvements.

Include **domain-specific noise types**. Medical environments have beeping monitors, paging systems, and alarms. Factories have machinery. Call centers have keyboard typing and other agents talking. Retail has music and customer chatter. Use noise samples that match your actual environment. Generic office noise is not a substitute for hospital noise.

## Privacy and Data Handling for Real User Audio

Using real production audio for evaluation creates privacy risks. Audio contains personally identifiable information (PII), sensitive content, and in some domains (healthcare, finance) regulated data. You must handle evaluation datasets with the same rigor as production data.

Obtain **explicit user consent** to use their audio for evaluation. Many jurisdictions (EU under GDPR, California under CCPA) require consent before using personal data for purposes beyond the original service. A voice banking app added a consent flow during onboarding: "We may use anonymized recordings of your interactions to improve our service. Do you consent?" Users who declined were excluded from eval dataset sampling. About 73 percent of users consented.

**Anonymize or redact PII** from audio and transcriptions. Remove names, account numbers, addresses, phone numbers, social security numbers, dates of birth, and any other identifiers. A healthcare voice app used automatic PII detection on transcriptions and flagged utterances with detected PII for manual review. Reviewers either redacted the PII (replaced names with "[NAME]", numbers with "[NUMBER]") or excluded the utterance from the dataset if PII was too pervasive. About 22 percent of utterances were excluded due to PII.

Store eval datasets with **appropriate access controls**. Use encryption at rest and in transit. Limit access to team members who need it for model evaluation. Log access for audit purposes. A financial services voice assistant stored their eval dataset in a secure S3 bucket with encryption enabled, access restricted to the ASR and ML teams, and access logs sent to the security team for monthly review. This met their compliance requirements under SOX and PCI-DSS.

Consider **synthetic or simulated data** for highly sensitive domains. Generate utterances using text-to-speech systems with realistic variation in speaker characteristics, speaking rate, and prosody. Add noise and environmental effects. Synthetic data avoids privacy risks but may not fully represent production conditions. A healthcare startup used synthetic data for initial ASR evaluations, then supplemented with a small set of real, heavily anonymized data for final validation. Synthetic WER: 7.2 percent. Real anonymized WER: 8.9 percent. The gap was acceptable given the privacy constraints.

Check **regulatory requirements** for your domain and jurisdiction. HIPAA in the US requires business associate agreements (BAAs) with any vendor handling health data, including ASR providers. GDPR in the EU requires data processing agreements (DPAs) and compliance with data transfer rules. Ensure your eval dataset creation and storage complies with all applicable regulations, or you risk fines and legal liability.

## Versioning and Maintaining Eval Datasets Over Time

Production conditions change. User behavior evolves. New accents, new devices, new vocabulary. An eval dataset that was representative in January 2026 might be outdated by January 2027. You need versioning and refresh strategies.

**Version your eval datasets** with timestamps and metadata. A voice assistant team maintained quarterly dataset versions: v1.0 (Q1 2025), v1.1 (Q2 2025), v2.0 (Q3 2025 — major distribution shift when they launched in new regions), v2.1 (Q4 2025). Each version was frozen once created — they didn't modify past datasets. New ASR models were tested on the most recent version and sometimes retested on older versions to track performance over time.

Track **distribution drift** between eval datasets and production. Compare production traffic in the current quarter to your eval dataset. If the accent distribution has shifted by more than 10 percentage points, or if noise levels have changed significantly, or if new domain vocabulary has emerged, refresh the dataset. A customer service bot detected that production accent distribution had shifted from 60 percent American / 25 percent British / 15 percent other in Q1 to 52 percent American / 28 percent British / 20 percent other in Q3. They sampled a new eval dataset to match the Q3 distribution.

Add **new samples** to capture emerging edge cases. If production logs reveal new failure modes — a new device type, a new accent, a new kind of background noise — add samples to the eval dataset that cover these cases. A navigation voice assistant added 300 utterances from users with cochlear implants after several users reported poor accuracy. Cochlear implant speech has unique acoustic properties. The eval dataset hadn't included it. After adding these samples, the team measured WER for cochlear implant users at 16.3 percent vs 5.1 percent for non-implant users. This led to a specialized ASR model for accessibility.

Balance **stability vs. freshness**. You want the eval dataset to be stable enough that improvements in WER are due to ASR changes, not dataset changes. But you also want it to stay current with production. A common pattern: maintain a **core frozen set** of 3,000 utterances that never changes (for tracking long-term trends) and a **current set** of 2,000 utterances that refreshes quarterly (for measuring current production performance). Test every ASR model on both sets.

## Building Multi-Condition and Challenge Sets

Beyond a general eval dataset, build **challenge sets** that focus on specific difficult conditions. These are smaller datasets (500 to 1,000 utterances) that isolate hard cases: heavy accents, extreme noise, fast speech, overlapping speakers, children's voices, elderly speakers, whispered speech.

A voice assistant built six challenge sets: heavy accents (300 utterances, all non-native speakers), high noise (200 utterances, SNR below 5 dB), fast speech (250 utterances, speaking rate above 200 words per minute), children (150 utterances, ages 6 to 12), elderly (200 utterances, ages 70+), whispered (100 utterances). They measured WER on each challenge set separately. General eval WER: 5.4 percent. Heavy accent WER: 14.1 percent. High noise: 22.7 percent. Fast speech: 8.9 percent. Children: 19.2 percent. Elderly: 11.3 percent. Whispered: 31.4 percent. These challenge sets revealed performance gaps that the general dataset obscured.

Use challenge sets to **prioritize improvements**. If your product serves elderly users and elderly WER is 11.3 percent while general WER is 5.4 percent, you have a demographic fairness problem. Prioritize improving elderly speech recognition. If your product is used in noisy environments and high-noise WER is 22.7 percent, noise robustness is your biggest opportunity. Challenge sets make these gaps visible and actionable.

Share challenge sets across teams. If multiple teams work on voice features, they can all test on the same challenge sets and compare results. A company with three voice products (assistant, dictation, transcription) built shared challenge sets. Each product team tested their ASR on all challenge sets quarterly. This created healthy competition and shared learning — if the dictation team found a technique that improved elderly WER, the assistant team adopted it.

Building production ASR evaluation datasets is not a one-time task. It requires representative sampling, sufficient size for statistical power, rigorous annotation, domain-specific coverage, realistic noise, privacy compliance, versioning, and challenge sets. The dataset is infrastructure — as critical as the ASR system itself. Invest in it early, maintain it over time, and use it to make every ASR decision based on evidence from your actual production environment. The next subchapter covers ASR provider comparison methodologies — how to run a fair bake-off using the datasets you've built.
