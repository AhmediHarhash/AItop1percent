# 4.2 — Character Error Rate: When WER Does Not Apply

What is a word? In English, the answer seems obvious. Words are separated by spaces. "The quick brown fox" is four words. Count the spaces, add one. But in Mandarin Chinese, Japanese, Thai, Khmer, Lao, and Burmese, there are no spaces between words. Written text flows continuously. "我要订一个会议室" — "I want to book a meeting room" — is seven characters with no spaces. You cannot count words because words are not orthographically marked. Word Error Rate becomes undefined.

Even in languages with spaces, WER breaks down for very short utterances. A user says "yes" and the system transcribes "yeah." One substitution, one word total. WER is 100 percent. The meaning is identical. The metric is useless. A user says "okay" and the system transcribes "OK." Depending on how you tokenize, this might be one substitution or a case variation. Either way, WER exaggerates the error. When the entire utterance is one or two words, a single mistake dominates the metric in ways that do not reflect user experience.

Character Error Rate solves both problems. Instead of counting word-level errors, CER counts character-level errors — substitutions, deletions, and insertions at the character level. For languages without word boundaries, CER is the only viable transcription metric. For short utterances in any language, CER provides finer granularity and more stable measurements. But CER introduces its own tradeoffs. It is sensitive to spelling variation, punctuation, and formatting in ways that WER is not. Knowing when to use CER and how to interpret it is essential for building voice systems that serve global users.

## The CER Calculation: Edit Distance at the Character Level

Character Error Rate is defined the same way as Word Error Rate, but the units are characters instead of words. The formula is the sum of character substitutions, deletions, and insertions divided by the total number of characters in the reference transcript.

A substitution happens when the ASR system replaces one character with another. The reference is "hello" and the hypothesis is "hallo." One substitution — the second "e" became an "a." A deletion happens when the ASR system drops a character. The reference is "meeting" and the hypothesis is "meting." One deletion — the second "e" is missing. An insertion happens when the ASR system adds a character. The reference is "book" and the hypothesis is "boook." Two insertions — two extra "o" characters.

The calculation uses the Levenshtein distance algorithm at the character level. If the reference is "schedule" — eight characters — and the hypothesis is "schedual" — eight characters, but with one substitution — the CER is one divided by eight, or 12.5 percent. If the reference is "我要订一个会议室" — seven characters — and the hypothesis is "我要定一个会议室" — also seven characters, but with one substitution — the CER is one divided by seven, or 14.3 percent.

The denominator is always the number of characters in the reference, just as with WER. If the reference is twenty characters and the hypothesis is forty characters because the system hallucinated extra output, CER is measured against the original twenty. This means CER can exceed 100 percent in pathological cases.

Spaces are characters. In languages that use spaces to separate words, the space is counted in the character total. If the reference is "book now" — eight characters including the space — and the hypothesis is "booknow" — seven characters, no space — there is one deletion. CER is one divided by eight, or 12.5 percent. This makes CER sensitive to word boundary errors even in English.

## When to Use CER: Languages Without Word Boundaries

For Mandarin Chinese, Japanese, Thai, and other languages without orthographic word boundaries, CER is the standard metric. You cannot compute WER because you cannot define where one word ends and another begins without linguistic analysis. Some teams preprocess the transcripts by segmenting them into words using a tokenizer, then compute WER on the segmented output. This introduces a dependency on the tokenizer's accuracy and makes results non-comparable across different segmentation approaches. CER avoids this problem by working directly on the character sequence.

In Mandarin, each character typically represents a morpheme — a unit of meaning — but words are often multi-character. "会议室" means "meeting room" and consists of three characters. If the ASR system transcribes this as "会一室," two of the three characters are correct, but the middle character is wrong. The CER is one divided by three — 33 percent. The meaning is destroyed. A WER calculation would require you to first decide whether "会议室" is one word or two, and the answer is ambiguous depending on the lexicon you use.

Japanese mixes three scripts — kanji, hiragana, and katakana — often within a single sentence. Word boundaries are implied by script transitions and grammatical particles, but not by spaces. "会議室を予約する" — "to book a meeting room" — is eight characters with no spaces. If the system transcribes this as "会議質を予約する," one character is wrong — "室" became "質." CER is one divided by eight — 12.5 percent. The meaning is corrupted. Word segmentation would require a morphological analyzer, and different analyzers produce different segmentations.

Thai is even more complex. It uses spaces to separate phrases or clauses, not words. Within a phrase, words run together. The sentence "ฉันต้องการจองห้องประชุม" — "I want to book a meeting room" — is 25 characters with no spaces. If the ASR system drops one character or substitutes one, CER measures the error at the character level without needing to solve the word segmentation problem.

For these languages, CER is not a choice — it is the only practical transcription accuracy metric. The alternative is to introduce a segmentation preprocessing step, which adds complexity, introduces new error modes, and makes your evaluation non-reproducible across teams using different segmenters.

## When to Use CER: Short Utterances in Any Language

Even in English and other space-delimited languages, CER is more informative than WER for very short utterances. Voice assistants, voice commands, and confirmation prompts often involve one- to three-word responses. "Yes." "No." "Cancel." "Confirm." "Help." When the entire utterance is one word, a single transcription error produces 100 percent WER regardless of how close the hypothesis is to the reference.

The reference is "yes" and the hypothesis is "yeah." WER is 100 percent — one error, one word. CER is two divided by three — 67 percent. The hypothesis is three characters, the reference is three characters, and two characters are substituted. The error is still significant, but CER gives you finer granularity. If you are tracking progress on single-word utterance accuracy, CER will show smaller, more measurable improvements than WER.

The reference is "okay" and the hypothesis is "OK." In WER terms, this might count as one substitution if you treat them as different words, or zero errors if you normalize case and treat them as equivalent. In CER terms, this is either four substitutions — "okay" has four letters, "OK" has two, requiring deletions and case changes — or zero errors if you normalize. Either way, CER forces you to make an explicit decision about case sensitivity and abbreviation handling.

Short utterances are common in interactive voice systems. Users answer yes-no questions. They issue single-word commands — "stop," "pause," "resume," "next." They provide confirmation — "correct," "right," "that's it." A system optimized for WER on long-form transcription may perform poorly on these short utterances because the training data and evaluation focus on multi-sentence speech. CER allows you to measure and optimize short utterance accuracy as a distinct capability.

## The CER-to-WER Relationship: What the Numbers Mean

CER is always lower than WER for the same transcript pair, assuming the hypothesis is reasonably close to the reference. This is because characters are smaller units than words. A single word error typically involves multiple character errors, but a single character error is a smaller fraction of the total character count than a word error is of the total word count.

If the reference is "schedule a meeting" — 18 characters including spaces — and the hypothesis is "schedule the meeting" — 19 characters — there is one word substitution ("a" became "the") and one character insertion ("e" added). WER is one divided by three words — 33 percent. CER is one divided by 18 characters — 5.6 percent. The CER is much lower because the error affected only one character out of 18, while it affected one word out of three.

This means you cannot directly compare CER and WER. A system with 10 percent WER might have 2 percent CER, or 5 percent CER, or 8 percent CER, depending on the nature of the errors. Whole-word substitutions with very different spellings produce high CER relative to WER. Single-character typos produce low CER relative to WER.

In practice, CER is useful for showing incremental progress when WER is already low. If your WER is 5 percent, a 0.5 percentage point improvement to 4.5 percent is significant but hard to measure reliably with small test sets. The corresponding CER drop — say, from 1.2 percent to 0.9 percent — might be easier to measure and track. CER also makes it easier to see progress on error types that WER treats coarsely, like spelling variations and punctuation.

## CER Limitations: Sensitivity to Formatting and Capitalization

CER is extremely sensitive to formatting decisions that do not affect meaning. If the reference uses "3 PM" and the hypothesis uses "3pm," there is one character difference — the space. CER registers an error. If the reference spells out "three PM" and the hypothesis uses "3 PM," there are four character differences. CER increases even though the meaning is identical.

Capitalization is another source of CER noise. If the reference is "Apple announced a new product" and the hypothesis is "apple announced a new product," there is one character substitution — uppercase "A" became lowercase "a." The meaning is identical. CER treats this as an error. If you are transcribing proper nouns and the model struggles with capitalization, CER will be higher than WER even if the word-level content is correct.

Punctuation affects CER in ways that do not affect WER unless you explicitly include punctuation as separate tokens in the WER calculation. If the reference is "Let's go, now!" and the hypothesis is "Let's go now," there are two character deletions — the comma and the exclamation mark. CER increases. WER is unaffected if you strip punctuation before calculating it, which is common practice.

These sensitivities mean CER requires more careful preprocessing and normalization than WER. You need to decide: Do you normalize case before computing CER? Do you strip punctuation? Do you expand abbreviations? Do you convert numbers to words or words to numbers? Every decision affects the metric. If you are comparing CER across systems or over time, you must use identical preprocessing. Otherwise, the numbers are not comparable.

## Using CER for Non-English ASR Evaluation

For teams building voice products in Chinese, Japanese, Korean, Thai, Vietnamese, or other languages without consistent word boundaries, CER becomes the primary transcription metric. You should still supplement it with domain-specific evaluation — named entity accuracy, numerical accuracy, semantic similarity — but CER is your baseline.

The challenge is that CER does not have the same intuitive interpretation as WER. A native English speaker has a rough sense of what 10 percent WER feels like — one word in ten is wrong. A 2 percent CER is harder to interpret. Is that good? Is the transcript usable? You need to calibrate CER against human perception of transcription quality in your target language.

Build test sets that include native speakers from your target user population. Measure CER on this data. Then have human evaluators rate the transcripts on a scale — unusable, barely usable, acceptable, good, excellent. Find the CER threshold where transcripts transition from unacceptable to acceptable. This threshold becomes your quality bar. In some languages, 2 percent CER might be excellent. In others, especially those with rich character sets or complex morphology, 5 percent CER might be acceptable.

Track CER by character type. In Japanese, measure CER separately for kanji, hiragana, and katakana. In Chinese, measure CER separately for common characters versus rare characters. This helps you identify whether errors are concentrated in specific script types or character frequency bands. If your model performs well on high-frequency characters but fails on rare ones, that is a signal to improve your language model or expand your training data.

Use CER alongside semantic metrics. Just as with WER, CER cannot tell you whether the errors matter. A 3 percent CER that concentrates errors in filler words and particles is very different from a 3 percent CER that concentrates errors in domain-specific terminology. Measure whether the system correctly transcribes the twenty most important terms in your domain. Measure whether users can complete their tasks based on the transcripts. CER is a component of your evaluation framework, not the entire framework.

## Combining WER and CER: Multi-Metric Evaluation

Some teams measure both WER and CER, even for languages with word boundaries. This provides two perspectives on the same transcription errors. WER shows word-level accuracy. CER shows character-level accuracy. Comparing the two reveals the nature of the errors.

If WER is 10 percent and CER is 2 percent, most errors are minor spelling variations, capitalization issues, or punctuation differences. The word-level content is mostly wrong, but the character-level content is close. This suggests the model is getting phonetic transcription right but struggling with spelling and formatting. The fix might be better language modeling or post-processing.

If WER is 10 percent and CER is 8 percent, most errors are whole-word substitutions with very different spellings. The model is producing words that are phonetically or semantically unrelated to the reference. This suggests acoustic modeling problems or vocabulary gaps. The fix might be better audio preprocessing, more training data, or domain-specific fine-tuning.

Tracking both metrics over time shows whether improvements are meaningful. If CER drops from 5 percent to 3 percent but WER stays at 12 percent, you improved spelling and formatting but did not fix the core transcription errors. If WER drops from 12 percent to 8 percent and CER drops from 5 percent to 1 percent, you improved both word-level and character-level accuracy — a real quality gain.

For voice command systems, WER is often more important because the command parser works at the word level. For transcription systems where users read the output, CER may be more important because character-level errors are visually distracting even if word-level content is correct. For real-time systems where users see streaming updates, both matter — WER affects semantic understanding, CER affects visual stability.

## Practical Recommendations: When to Choose CER Over WER

Use CER as your primary metric when you are building ASR for languages without word boundaries. Use it when your primary use case involves short utterances where WER is too coarse. Use it when you are optimizing for visual transcription quality and character-level errors matter as much as word-level errors.

Do not use CER as your sole metric for English or other space-delimited languages unless you have a specific reason. WER is more interpretable and aligns better with how most users perceive transcription quality in these languages. Use CER as a supplementary metric to track specific error types or to measure progress when WER is already low.

Normalize your transcripts consistently before computing CER. Decide on case sensitivity, punctuation handling, number formatting, and abbreviation expansion, then apply the same preprocessing to all reference and hypothesis transcripts. Document these decisions. If you switch preprocessing, your CER will change even if model quality does not.

Calibrate CER thresholds using human evaluation. Do not assume that a CER of 2 percent is acceptable just because it sounds low. The acceptability of a given CER depends on your language, your domain, and your users' tolerance for errors. Measure user satisfaction at different CER levels. Find the threshold where satisfaction drops. That is your quality bar.

The next subchapter examines the benchmark-to-production gap — why ASR models that score 3 to 5 percent WER on LibriSpeech routinely produce 15 to 25 percent WER in real-world deployments, and what you can do about it.

