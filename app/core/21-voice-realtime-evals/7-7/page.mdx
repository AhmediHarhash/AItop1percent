# 7.7 — End-of-Utterance Detection: Semantic vs Pause-Based

Is the user done speaking? This is the single most important question in voice interface design, and it has no clean answer. A pause might mean the user is finished. It might mean they are thinking. It might mean they lost their train of thought and are about to continue. If you respond too soon, you interrupt them. If you wait too long, the conversation feels sluggish and broken. The difference between a pause that means "I'm done" and a pause that means "I'm thinking" is often less than 300 milliseconds and contains no acoustic markers that reliably distinguish the two.

End-of-utterance detection is the mechanism that determines when the system stops listening and starts responding. Every voice interaction depends on it. Get it right and the conversation flows naturally. Get it wrong and every turn feels mistimed, stilted, or adversarial. The two primary detection strategies — pause-based and semantic — each solve different problems and create different failure modes. Most production systems use a hybrid approach, but that hybrid is harder to calibrate than it appears.

## Pause-Based Detection: The Silence Timer

The simplest end-of-utterance detection strategy is pure pause duration. If the user stops speaking for X milliseconds, assume they are done. The system starts a timer the moment it detects silence. If the timer reaches the threshold before the user speaks again, the utterance is considered complete and the response pipeline begins. If the user starts speaking again before the threshold, the timer resets and the system continues listening.

In mid-2025, a voice-controlled smart home system used a fixed 800-millisecond pause threshold. If the user said "turn on the lights in the kitchen" and paused for 800 milliseconds, the system responded. This worked perfectly for users who spoke in complete sentences without internal pauses. It failed for users who paused between clauses, users who spoke more slowly, users who were thinking while speaking, and users who were interrupted by external events (a dog barking, a child asking a question) mid-command.

A user would say "turn on the lights in..." pause 900 milliseconds to remember which room "...the bedroom," and the system would interpret "turn on the lights in" as a complete utterance and respond with "which lights?" The user would then have to say "the bedroom" in response to the prompt, even though they had originally intended to provide that information in one continuous thought. The system had forced a multi-turn interaction where the user expected a single turn.

The failure rate correlated directly with pause duration. At 800 milliseconds, the system interrupted users mid-thought 11% of the time. Extending the threshold to 1,200 milliseconds reduced interruptions to 4% but increased perceived response latency by 390 milliseconds on average, making the system feel slower. Shortening the threshold to 600 milliseconds reduced latency to 510 milliseconds but increased interruptions to 18%. There was no threshold value that optimized both metrics simultaneously for all users.

## The Cognitive Pause Problem

Humans pause while speaking for cognitive reasons. We pause to retrieve a word from memory, to structure a complex thought, to translate between languages, to calculate a number, to decide how much information to share. These pauses are part of the utterance, not the end of it. A pause-based detection system has no way to distinguish a cognitive pause from a completion pause. Both are silence. The only difference is what happens after the silence ends.

A healthcare voice assistant used by nurses during patient intake in late 2025 encountered this constantly. A nurse would say "patient reports chest pain that started..." pause 1,100 milliseconds to check the patient's chart "...four hours ago," and the system would cut in after the pause with "can you describe the pain?" The nurse had not finished providing the initial information. The system's impatience forced the nurse to repeat information or answer out of sequence, which slowed down the intake process and increased cognitive load during an already high-stress task.

The team analyzed 3,800 nurse utterances to find patterns that distinguished cognitive pauses from completion pauses. They found that cognitive pauses were typically shorter (600 to 1,400 milliseconds), occurred mid-sentence at syntactically incomplete points (after "that started" but before the time phrase), and were often followed by more speech within three seconds. Completion pauses were typically longer (1,200 to 2,500 milliseconds), occurred at syntactically complete points (end of a full sentence or clause), and were rarely followed by additional speech.

The system implemented a **syntactic completeness check**. After detecting a pause, it analyzed the transcript so far. If the sentence was syntactically incomplete — an open clause, an unfinished prepositional phrase, a dependent clause without its main clause — the system extended the detection window by 600 milliseconds regardless of pause duration. If the sentence was syntactically complete, the system used the standard pause threshold. This reduced mid-utterance interruptions by 68% while adding only 140 milliseconds to average response latency.

## Semantic Detection: Is the Sentence Complete?

Semantic end-of-utterance detection does not wait for a long pause. Instead, it analyzes the content and structure of what the user has said so far. If the utterance forms a semantically complete thought — a full sentence with subject, verb, and object, or a grammatically valid fragment that answers the current question — the system concludes the user is done, even if they have only paused for 200 milliseconds.

This is dramatically faster than pause-based detection. A user says "schedule a meeting for tomorrow at two p.m." The sentence is semantically complete. The system does not need to wait 1,000 milliseconds to confirm the user is done. It can respond within 400 milliseconds of the last word, making the interaction feel nearly instantaneous. The latency reduction is so significant that some voice systems use semantic detection as the primary strategy and pause-based detection only as a fallback.

The failure mode is premature response when the user intended to continue. A user says "I need to book a flight to Seattle." Semantically complete sentence. The system responds with "what dates are you traveling?" But the user had intended to say "I need to book a flight to Seattle on March fifth returning March tenth in economy class." They paused after "Seattle" for only 300 milliseconds to recall the dates, which was not long enough to signal incompleteness to a semantic detector. The system interrupted before they could finish, forcing a multi-turn interaction where the user expected to provide all information upfront.

A corporate travel voice assistant in early 2026 used semantic detection as its primary strategy. It achieved an average response latency of 470 milliseconds, which users described as "impressively fast." It also interrupted users mid-thought 14% of the time, which users described as "annoying" and "pushy." The system was fast but not patient. The team added a **continuation likelihood model** that predicted whether the user was likely to keep speaking based on the semantic content and conversational context. If the user's utterance was semantically complete but the current task required more information (booking a flight requires at least departure date, return date, and class), the system extended the detection window even though the sentence was complete. This reduced premature interruptions to 6% while increasing average latency only to 520 milliseconds.

## Intonation and Prosody as Completion Signals

Spoken language carries information in prosody — pitch, stress, rhythm, and intonation patterns. In most languages, a falling pitch at the end of a sentence signals completion. A rising pitch signals a question or incompleteness. A speaker who ends a clause with rising intonation is likely to continue. A speaker who ends with falling intonation is likely finished.

Voice systems can use prosodic features to improve end-of-utterance detection. If the user's pitch drops at the end of the last word and the pause exceeds 400 milliseconds, confidence that the utterance is complete is high. If the pitch rises or remains flat and the pause is only 300 milliseconds, confidence is lower and the system should wait longer.

A language learning voice assistant in mid-2025 incorporated prosodic analysis into its detection model. After detecting a pause, the system analyzed the pitch contour of the last 500 milliseconds of speech. If pitch dropped by more than 20% and the pause exceeded 500 milliseconds, the system treated the utterance as complete. If pitch remained stable or rose, the system extended the detection window to 1,400 milliseconds. This hybrid approach reduced both premature interruptions and excessive delays, bringing the system's turn-taking quality from user-rated 6.2 out of 10 to 8.1 out of 10.

The limitation of prosodic detection is that prosody varies by language, dialect, individual speaker style, and emotional state. A speaker under stress may use flatter intonation. A speaker from a culture with different prosodic norms may use rising intonation at the end of statements. A speaker with hearing loss may have atypical prosody. Prosody is a valuable signal, but it cannot be the only signal. The system must combine it with pause duration and semantic completeness to handle diverse users.

## The Hybrid Model: Pause, Syntax, Semantics, Prosody

The best production systems do not choose one detection strategy. They use all of them in combination, weighted by confidence. After the user stops speaking, the system starts a timer. Simultaneously, it analyzes the transcript for syntactic completeness, semantic completeness, and prosodic cues. Each analysis produces a confidence score that the utterance is complete. The system combines these scores into a single **completion confidence** value.

If completion confidence exceeds a high threshold (typically 0.90), the system responds immediately, even if the pause duration is short. This allows fast responses to clearly complete utterances. If completion confidence is moderate (0.60 to 0.90), the system waits until the pause exceeds a medium threshold (700 to 1,000 milliseconds). If completion confidence is low (below 0.60), the system waits until the pause exceeds a long threshold (1,400 to 2,000 milliseconds) or until the user explicitly signals they are done (by saying "that's it" or "done").

A financial services voice assistant deployed in January 2026 used this hybrid model. For an utterance like "transfer five hundred dollars to checking," all signals aligned: semantically complete, syntactically complete, falling intonation, 600-millisecond pause. Completion confidence was 0.94. The system responded in 520 milliseconds. For an utterance like "I want to transfer..." with a 500-millisecond pause, the signals diverged: semantically incomplete, syntactically incomplete, flat intonation. Completion confidence was 0.22. The system extended the detection window to 1,800 milliseconds, giving the user time to continue with "...five hundred dollars to my savings account."

The hybrid model reduced premature interruptions to 2.8%, reduced excessive delays (over 1,200 milliseconds) to 5.1%, and brought average response latency to 680 milliseconds. More importantly, it handled edge cases gracefully. Users with slower speech patterns were not interrupted because the system waited longer when syntactic and semantic signals indicated incompleteness. Users with fast, complete speech got fast responses because the system did not wait unnecessarily when all signals indicated completion.

## Handling Disfluencies and False Starts

Real human speech is full of disfluencies — hesitations, false starts, filler words, self-corrections. A user says "I need to... uh... actually can you... let me start over, I want to schedule a meeting." Each pause, each "uh," each "actually" is a potential end-of-utterance signal. If the system responds to the first pause, it interrupts a disfluent but ongoing utterance. If it waits too long, the user sits in silence.

Disfluency detection helps the system distinguish pauses that are part of the utterance from pauses that signal completion. If the user's speech contains filler words ("uh," "um," "like"), hesitation markers ("let me think," "hold on"), or self-correction phrases ("actually," "I mean," "wait"), the system extends the detection window even if the pause duration or semantic completeness would normally trigger a response. The user has signaled that they are still formulating their thought.

A legal document voice assistant used by attorneys in late 2025 dealt with highly disfluent speech. Attorneys think out loud. They revise sentences mid-utterance. They pause frequently to choose precise wording. A typical utterance: "add a clause that... no wait, make it a section... titled Indemnification and... let me think... and Limitation of Liability." The system that responded after the first pause ("that") would interrupt constantly. The system that waited until a two-second pause would feel unresponsive.

The team implemented **disfluency-aware detection**. If the transcript contained hesitation markers or self-corrections, the system required both a long pause (1,600 milliseconds) and semantic completeness before responding. If the transcript was fluent, the system used standard thresholds. This reduced interruptions during disfluent speech by 74% while maintaining fast responses to fluent speech. The key insight was that disfluencies are not errors — they are signals that the user is engaged in active cognitive processing and needs more time.

## The Tradeoff Between Responsiveness and Accuracy

Every millisecond you shorten the detection window, you increase responsiveness and you increase the risk of premature interruption. Every millisecond you extend the window, you increase accuracy and you decrease perceived responsiveness. There is no configuration that maximizes both. You must choose a point on the curve and accept the tradeoff.

The optimal point depends on your use case. For high-stakes tasks where interrupting the user is worse than a slow response (medical intake, legal documentation, financial transactions), bias toward accuracy. Use longer detection windows, require higher completion confidence, tolerate 900 to 1,200 milliseconds of average latency. For low-stakes tasks where speed matters more than perfect accuracy (setting timers, playing music, casual conversation), bias toward responsiveness. Use shorter windows, accept 5 to 8% premature interruption rate, target 400 to 600 milliseconds of average latency.

A customer support voice assistant in January 2026 ran an A/B test to find the optimal tradeoff for their use case. Group A used aggressive detection: 600-millisecond pause threshold, semantic-first strategy, 480-millisecond average latency, 9% premature interruption rate. Group B used conservative detection: 1,400-millisecond pause threshold, pause-first strategy, 1,120-millisecond average latency, 2% premature interruption rate. User satisfaction was nearly identical — 7.8 for Group A, 7.9 for Group B. Task completion rate was identical. Time to resolution was 8% faster in Group A.

The team chose Group A's configuration. The marginal improvement in interruption rate in Group B did not justify the latency cost. Users tolerated occasional premature responses better than they tolerated slow responses. This is not universal — a different use case with higher-stakes interactions might have chosen Group B. The point is that the tradeoff is real, measurable, and must be optimized for your specific user population and task types.

## Evaluating End-of-Utterance Detection Quality

You measure detection quality by tracking four metrics. First, **premature response rate** — the percentage of responses where the system began speaking before the user was actually done. You detect this by identifying cases where the user interrupted the agent within two seconds of the agent starting to speak. If the user interrupted that quickly, the agent likely responded too soon.

Second, **excessive delay rate** — the percentage of responses where the system waited longer than 1,200 milliseconds after the user's last word before responding. You measure this by instrumenting the silence duration between the user's last detected speech and the agent's first word. Delays over 1,200 milliseconds feel unnatural to most users.

Third, **false continuation rate** — the percentage of pauses where the system continued listening but the user had actually finished and was waiting for a response. You detect this by identifying cases where the user, after a long pause, said something like "are you there?" or "hello?" or simply repeated their original utterance. These are signals that the user expected a response but the system kept listening.

Fourth, **average response initiation latency** — the median time between the user's last word and the agent's first word across all turns. This is the single most important metric for perceived responsiveness. Target 600 to 800 milliseconds for most use cases. Under 500 milliseconds is exceptional. Over 1,000 milliseconds is problematic.

A voice-controlled automotive assistant tracked these metrics across 32,000 drive sessions in late 2025. Initial results: premature response rate 7%, excessive delay rate 11%, false continuation rate 6%, average latency 940 milliseconds. The team implemented hybrid detection with prosodic analysis and adaptive thresholds. Updated results: premature response rate 3%, excessive delay rate 4%, false continuation rate 2%, average latency 710 milliseconds. The improvement in user experience was dramatic — users rated the updated system 8.6 out of 10 compared to 6.9 for the original.

## When Detection Cannot Be Perfect

Some utterances are genuinely ambiguous. The user pauses. The pause could mean they are done or they are thinking. The sentence is semantically complete but the user might have more to add. Prosody is neutral. Syntax is complete. There are no disfluency markers. The system must guess, and sometimes it will guess wrong.

When detection uncertainty is high, the best strategy is to **make the uncertainty visible**. After waiting the maximum reasonable detection window (1,400 to 1,800 milliseconds) without high completion confidence, the system can prompt gently: "did you have more to add?" or "go ahead" or simply "mm-hmm" as a backchannel to signal it is listening. This gives the user an opportunity to continue if they were not done or to confirm completion if they were. It adds a half-turn to the conversation, but it prevents the worse failure of responding to incomplete input or waiting indefinitely in silence.

A therapy chatbot voice interface in January 2026 used this strategy. If the user paused for more than 1,600 milliseconds and completion confidence was below 0.65, the system said "take your time" or "I'm listening." This cue reassured the user that the system was still engaged and gave them permission to continue without pressure. It also allowed the user to explicitly signal completion by saying "that's all" or "I'm done." The explicit signal was far more reliable than any algorithmic detection.

End-of-utterance detection is the invisible infrastructure of voice interaction. When it works, users never think about it. When it fails, it is all they notice. You cannot build a great voice interface without solving this problem deeply, and you cannot solve it with a single fixed threshold. You solve it with hybrid models, adaptive windows, prosodic analysis, and relentless measurement of the millisecond-level timing that separates natural conversation from awkward turn-taking.

Next: the overlap window — what happens when both parties speak at the same time.

