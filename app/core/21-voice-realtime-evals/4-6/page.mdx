# 4.6 — Domain Vocabulary Accuracy: Medical, Legal, and Technical Speech

Generic ASR models fail catastrophically on specialized vocabulary. A model trained on everyday speech transcribes "hypertension" as "hyper tension," "atrial fibrillation" as "a trial fibrillation," "Kubernetes" as "Cooper Nettie's," "habeas corpus" as "have a corpse," and "atorvastatin" as "a tour of a statin." These are not phonetic errors. The model hears the sounds correctly but lacks the vocabulary to map those sounds to the correct words. When your users speak the language of medicine, law, finance, or engineering, a general-purpose ASR system transcribes gibberish.

Domain vocabulary accuracy is the most overlooked dimension of ASR evaluation. Teams measure WER on their test set, see 95% accuracy, and assume the system works. Then they deploy to radiologists who dictate "pneumothorax" and see "new motor axe." To oncologists who say "chemotherapy regimen" and see "came with therapy regime and." To attorneys who say "summary judgment" and see "summery judge meant." The WER on general speech is irrelevant. The WER on domain-specific terms is what determines whether the system is usable.

## The Domain Vocabulary Gap

ASR models are trained on massive corpora of transcribed speech — mostly conversational English, broadcast news, podcasts, and YouTube. These corpora contain very few instances of "myocardial infarction," "force majeure," "CIDR block," or "amortization schedule." The model's language model assigns low probability to these terms because it has rarely seen them. When the acoustic model produces phonemes that could map to a rare medical term or a common phrase, the language model biases toward the common phrase.

A radiology dictation system in early 2025 measured ASR accuracy on 500 chest X-ray reports. Overall WER was 8%, which seemed acceptable. But when they isolated the 200 unique medical terms in those reports — anatomical structures, pathologies, measurements — WER was 34%. The model transcribed "pleural effusion" as "plural of fusion" in 60% of cases. It transcribed "atelectasis" as "at a lecture says this" in 40% of cases. These errors were not random. They were systematic replacements of rare domain terms with common conversational phrases.

The legal domain faces identical problems. "Voir dire" becomes "far deer." "Estoppel" becomes "stop all." "Indemnification" becomes "in them if I cation." "Certiorari" becomes "search for rarities." These errors do not just degrade WER. They destroy semantic meaning. A radiologist who dictates "no evidence of pneumothorax" and sees a transcript that says "no evidence of new motor axe" cannot use that transcript. The error rate does not matter. A single error in a critical term makes the entire transcript unusable.

## Measuring Domain-Specific WER

You cannot evaluate domain vocabulary accuracy on general test sets. You need a test set that reflects the actual vocabulary distribution of your domain. For a medical dictation system, this means real clinical language: diagnoses, medications, anatomical terms, lab values, procedure names. For a legal assistant, it means real legal terminology: case citations, procedural terms, Latin phrases, contract language. For a DevOps voice tool, it means real technical vocabulary: service names, infrastructure terms, command syntax, configuration parameters.

Build a domain-specific test set by sampling real user utterances and manually transcribing them. You need at least 500 utterances covering the full vocabulary range. Stratify by term frequency — include common terms like "appointment" or "contract" and rare terms like "thrombocytopenia" or "subpoena duces tecum." Weight the test set toward the rare terms because those are where errors concentrate.

A financial services voice bot built a test set of 800 utterances containing 300 unique financial terms: "amortization," "escrow," "collateral," "underwriting," "refinance," "APR," "FICO score," "forbearance," "points," "PMI." They measured WER separately on domain terms versus non-domain terms. WER on general vocabulary was 6%. WER on financial terms was 22%. The system failed where it mattered most.

You also need to measure term-level accuracy, not just phoneme or word accuracy. If the user says "metoprolol" and the system transcribes "met a troll all," that is five word errors but one term error. The semantic failure is at the term level. Track how often the system correctly transcribes complete multi-word domain terms. "Atrial fibrillation" is two words but one term. If the transcript says "a trial fibrillation," the term is wrong even though half the phonemes are correct.

## Hot-Word Boosting and Custom Vocabulary

The primary technique for improving domain accuracy is hot-word boosting. You provide the ASR system with a list of domain-specific terms that should be recognized with higher priority. When the acoustic model produces phonemes that could match either a hot word or a common phrase, the language model biases toward the hot word. This shifts the probability distribution in favor of rare domain terms.

Hot-word lists are typically limited to 100 to 5,000 terms, depending on the ASR provider. You must choose which terms to boost. The wrong terms waste capacity and can introduce new errors by over-boosting words that sound like common phrases. The right terms are high-value, frequently used, and phonetically confusable with common words.

A healthcare dictation platform created a hot-word list of 1,200 medical terms. They prioritized terms that appeared in at least 2% of dictations and had historical WER above 15%. "Hypertension," "diabetes," "atorvastatin," "metformin," "CT scan," "ultrasound," "biopsy," "prognosis," "prescription," "diagnosis" all made the list. Obscure terms like "rhabdomyolysis" or "pneumonoultramicroscopicsilicovolcanoconiosis" did not, because they appeared rarely and boosting them would not materially improve overall accuracy.

After deploying hot-word boosting, WER on the 1,200 boosted terms dropped from 26% to 9%. WER on non-boosted domain terms remained at 28%. WER on general vocabulary stayed at 6%. The improvement was isolated to the terms they explicitly boosted. This confirmed that hot-word boosting is not a general solution — it is a targeted patch for known high-value terms.

## Custom Language Models and Domain Adaptation

Hot-word boosting is a runtime override. Custom language models are a training-time solution. You fine-tune the ASR system's language model on domain-specific text corpora — medical literature, legal documents, technical manuals — so the model learns the probability distribution of your domain. This requires access to the ASR provider's training pipeline, which not all providers offer, and requires large domain corpora.

A legal tech company fine-tuned a language model on 50,000 court transcripts, 10,000 legal briefs, and 5,000 contracts. The fine-tuned model reduced WER on legal terms from 31% to 11%. But the process took six weeks and required re-deployment of the entire ASR stack. Hot-word boosting, by contrast, took two hours to configure and required no retraining. The trade-off is clear: custom language models deliver better accuracy but require more investment. Hot-word boosting delivers partial improvement with minimal effort.

Some ASR providers offer domain-specific pre-trained models — medical, legal, finance — that have already been fine-tuned on relevant corpora. These models work better than general models for domain vocabulary but worse than fully custom models trained on your specific data. A radiology group tested a medical-specific ASR model and found WER on medical terms was 14%, compared to 34% for the general model and 8% for a custom model trained on their own dictation logs. The medical-specific model was a middle ground: better than general, not as good as custom, and available immediately.

## Pronunciation Dictionaries and Phonetic Overrides

Some domain terms are transcribed incorrectly because the ASR system does not know how to pronounce them. Drug names, especially, follow non-standard pronunciation rules. "Omeprazole" is pronounced "oh-meh-PRAY-zole," not "oh-MEP-rah-zole." If the acoustic model hears the correct pronunciation but the language model has never seen the word, it produces a phonetic guess that is wrong.

You can provide a pronunciation dictionary that maps spellings to phonetic representations. When the acoustic model hears phonemes matching the dictionary entry, the system transcribes the correct spelling. This works for terms with consistent pronunciation but fails for terms with regional or speaker variation.

A pharmacy voice assistant added pronunciation entries for the 200 most commonly misprescribed drugs. "Atorvastatin," "omeprazole," "metformin," "lisinopril," "amlodipine," "simvastatin," "losartan," "gabapentin," "sertraline," "citalopram." After adding the dictionary, WER on these drugs dropped from 19% to 7%. But the dictionary did not help with drugs that had multiple acceptable pronunciations or drugs spoken with heavy accents.

Pronunciation dictionaries are brittle. They work well in controlled environments with trained speakers — radiologists dictating in quiet rooms — but fail in uncontrolled environments with diverse accents and background noise. They are a supplementary technique, not a primary solution.

## Measuring Rare Term Accuracy

High-frequency domain terms appear in your test set often enough to measure accurately. Rare terms — those that appear in fewer than 1% of utterances — do not. If "thrombocytopenia" appears twice in your 500-utterance test set, measuring its accuracy is statistically meaningless. But rare terms are often the highest-stakes terms. A radiologist dictating "no evidence of malignancy" cannot tolerate errors on "malignancy."

You need a separate rare-term test set. Compile 100 to 200 utterances that specifically contain rare, high-stakes vocabulary. This is not a representative sample of production traffic. It is a stress test for the terms that matter most. A medical ASR team built a rare-term test set with 150 utterances containing oncology terms, rare medication names, uncommon pathologies, and complex anatomical structures. WER on this set was 41%, compared to 12% on the general medical test set. This revealed that the system was fragile on exactly the terms where fragility was unacceptable.

Track rare term accuracy separately from overall WER. Even if overall WER is 8%, if rare term WER is 40%, your system is not safe for clinical use. The metric that matters is the one that measures the failure mode with the highest consequence.

## Context-Aware Vocabulary Expansion

Some systems detect the domain context dynamically and adjust the vocabulary in real time. If the user says "I need to schedule a doctor appointment," the system infers a healthcare context and boosts medical terms. If the user says "I need to review the contract," the system infers a legal context and boosts legal terms. This allows a single ASR system to handle multiple domains without maintaining separate models.

A virtual assistant for insurance agents used context-aware boosting. When the conversation involved auto insurance, it boosted terms like "collision," "comprehensive," "deductible," "premium," "liability." When the conversation involved health insurance, it boosted "copay," "deductible," "premium," "out-of-pocket," "network." The system tracked conversation history and adjusted the hot-word list every 30 seconds based on detected topics.

This reduced WER on domain terms from 18% to 11% without requiring the agent to manually specify the domain. But it introduced latency — the system needed time to detect context before applying the correct vocabulary. In the first 10 seconds of a conversation, before context was established, WER on domain terms was still 18%. The system improved as the conversation progressed, but early utterances had higher error rates.

## The False Positive Risk of Over-Boosting

Hot-word boosting increases the probability of boosted terms, which means it also increases the probability of false positives. If you boost "hypertension" to fix transcription of the medical term, the system might now transcribe casual speech like "hyper tension in the room" as "hypertension in the room." Over-boosting turns a recognition problem into a hallucination problem.

A legal transcription service boosted 800 legal terms and immediately saw new errors. Conversational phrases like "stop all the noise" were transcribed as "estoppel the noise." "I'm done if I cation works" became "I'm done indemnification works." The system was now too eager to recognize legal terms, even when the speaker was using casual language. They reduced the boost strength from 3x to 1.5x and the false positives disappeared, but accuracy on real legal terms declined slightly.

The optimal boost strength depends on the phonetic uniqueness of the term. Terms that sound nothing like common phrases — "thrombocytopenia," "pneumothorax" — can be heavily boosted without false positive risk. Terms that sound similar to common phrases — "estoppel," "voir dire" — must be boosted cautiously.

## Domain-Specific Confidence Calibration

Confidence scores on domain terms should reflect the uncertainty introduced by rare vocabulary. A system that transcribes "thrombocytopenia" with 95% confidence when it has rarely seen that word is overconfident. A well-calibrated system should report lower confidence on rare terms, signaling to downstream logic that verification is needed.

A radiology dictation system tracked confidence scores and actual accuracy for domain terms versus general vocabulary. On general vocabulary, 95% confidence correlated with 97% accuracy. On domain terms, 95% confidence correlated with only 78% accuracy. The confidence scores were not calibrated for rare vocabulary. The system was equally confident about terms it knew well and terms it had barely seen.

They recalibrated confidence scoring by penalizing scores for words outside the training distribution. After recalibration, domain terms transcribed with 95% confidence had 94% accuracy, and terms with lower confidence were flagged for human review. This allowed radiologists to trust high-confidence transcriptions and double-check low-confidence ones.

Confidence calibration is especially important in high-stakes domains. A physician who trusts a 95% confidence score on a medication name and prescribes the wrong drug because the transcription was incorrect has caused patient harm. Calibrated confidence prevents this by surfacing uncertainty appropriately.

## Continuous Vocabulary Expansion

Your domain vocabulary changes over time. New drugs are approved, new medical procedures are developed, new legal precedents are set, new technologies are named. If your ASR system's vocabulary is static, accuracy degrades as users adopt new terms.

A healthcare ASR provider monitored transcription logs for terms that appeared frequently but had low confidence scores. These were strong signals of new vocabulary. In Q3 2025, they detected a surge in the term "semaglutide," a new weight-loss medication. The ASR system transcribed it as "semi glute tide" with 40% confidence. They added "semaglutide" to the hot-word list within two days, and accuracy on that term improved immediately.

This continuous monitoring requires infrastructure to detect and respond to vocabulary drift. You need a process to review flagged terms, validate that they are legitimate domain vocabulary and not transcription noise, and update the hot-word list or retrain the language model. Teams that treat vocabulary as a one-time configuration find that accuracy degrades within months as language evolves.

## The Ground Truth Problem for Domain Terms

Annotating domain-specific transcripts requires domain expertise. A general transcriptionist listening to a radiologist dictate "subsegmental atelectasis in the right lower lobe" might transcribe "sub segmental at a lecture says in the right lower lobe." Your ground truth is wrong, which makes your WER measurement wrong, which makes your optimization useless.

Use domain experts for annotation. A medical transcription team should include licensed medical transcriptionists who know the vocabulary. A legal transcription team should include paralegals or court reporters who know legal terminology. The cost is higher, but the ground truth is accurate.

A radiology ASR provider initially used general transcriptionists to label test data. They measured WER at 11% and considered the system production-ready. Then they had radiologists review the same transcripts and found that 18% of the "ground truth" labels were wrong — the transcriptionists had mislabeled medical terms they did not recognize. When they corrected the ground truth, the actual WER was 23%, not 11%. They had been optimizing against incorrect labels for six months.

Domain expertise in annotation is not optional. If your annotators do not know the vocabulary, your ground truth is noise.

## When to Reject Domain Term Transcriptions

If the ASR system produces a domain term transcription with low confidence, the downstream system must decide whether to accept it, ask for confirmation, or reject it outright. The decision depends on the consequence of error. A wrong drug name in a prescription is catastrophic. A wrong technical term in a DevOps command might fail safely. The rejection threshold must be calibrated to risk.

A pharmacy voice assistant set different confidence thresholds for different term categories. Drug names required 90% confidence. Dosages required 95% confidence. General instructions like "take with food" required 70% confidence. If a drug name was transcribed with 85% confidence, the system asked the pharmacist to confirm: "I heard atorvastatin. Is that correct?" This added one extra interaction but prevented dispensing errors.

The user experience of rejection matters. If the system constantly asks for confirmation on every domain term, users abandon it. If the system never asks for confirmation and makes high-stakes errors, users lose trust. The right balance is to ask for confirmation on terms where confidence is borderline and consequences are high, and to accept transcriptions where confidence is high or consequences are low.

Domain vocabulary accuracy determines whether your ASR system is usable in specialized fields. Generic models fail. Targeted techniques — hot-word boosting, custom language models, pronunciation dictionaries — close the gap. The next challenge arises when two people speak at once, and the system must decide which voice to follow.

