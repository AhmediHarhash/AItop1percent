# 7.6 — Turn-Taking Accuracy: Speaking and Listening in Rhythm

Most teams think barge-in is the hard part. They are wrong. Barge-in handles the exception case — when the user needs to interrupt. Turn-taking handles the default case — the 90% of the conversation where both parties follow natural conversational rhythm. Who speaks when? How long should the agent wait before responding? How do you avoid talking over the user during their natural pauses? Bad turn-taking destroys voice UX faster than any other single failure mode. It does not announce itself with error messages. It just makes every interaction feel awkward, stilted, and fundamentally wrong.

Turn-taking is the rhythm of conversation. In human dialogue, speakers trade turns with millisecond precision, using acoustic cues, linguistic patterns, and contextual understanding to know when it is their turn to speak. Voice systems must replicate this rhythm artificially. The system must detect when the user has finished speaking, wait an appropriate amount of time, then respond without delay but also without interrupting. Get the timing right and the conversation flows. Get it wrong and the user either sits in uncomfortable silence waiting for the system to respond or gets cut off mid-sentence by an agent that could not wait.

## The Turn-Taking Timing Window

Turn-taking accuracy depends on three timing thresholds. First, the **end-of-utterance detection window** — how long the system waits after the user stops speaking before it concludes they are done. Too short and the system interrupts the user during natural pauses between clauses or while they are thinking. Too long and the conversation feels sluggish. Second, the **response initiation delay** — how much time passes between detecting end-of-utterance and the agent beginning to speak. This should be as short as possible, ideally under 300 milliseconds, but any delay over 500 milliseconds makes the system feel slow. Third, the **overlap tolerance window** — if the user starts speaking again before the agent finishes responding, how quickly does the system detect the overlap and yield the floor?

In early 2025, an insurance voice assistant set its end-of-utterance window to 600 milliseconds. If the user paused for 600 milliseconds, the system assumed they were done and started responding. This worked well for users who spoke in complete sentences without pauses. It failed catastrophically for users who paused to think, users with slower speech patterns, and users who naturally paused between dependent clauses. A user would say "I need to file a claim for..." pause 700 milliseconds to recall the date "...the accident on March third," and the system would interrupt after the pause with "I can help you file a claim, what type of claim is it?" The user would then have to interrupt the interruption to finish their original sentence. The system recorded this as a barge-in event, but the root cause was not the user's behavior — it was the system's impatience.

The team extended the window to 1,200 milliseconds. Now the system waited longer before assuming the user was done. This eliminated the false interruptions but introduced a new problem: the conversation felt slow. Users who did finish speaking in one sentence had to wait over a second for the agent to respond. The average turn-taking delay (time from user finishing to agent starting) increased from 780 milliseconds to 1,490 milliseconds. Users described the system as "laggy" and "unresponsive" even though it was technically more accurate.

The solution was not a single fixed window. It was **adaptive end-of-utterance detection** that adjusted the window based on the user's observed speech patterns. If the user had paused mid-utterance earlier in the conversation, the system extended the window. If the user consistently spoke in complete bursts without internal pauses, the system shortened the window. This brought the average delay down to 620 milliseconds while reducing false interruptions by 81% compared to the original fixed window.

## Premature Response: The Agent That Cannot Wait

Premature response is when the system starts speaking before the user is actually done. This happens in two scenarios. First, the end-of-utterance window is too short and the user is still gathering their thoughts. Second, the system uses semantic cues to detect completion (the user's sentence sounds complete) but the user had more to say.

A job search voice assistant in mid-2025 used semantic analysis to detect end-of-utterance. If the user's utterance formed a syntactically complete sentence, the system assumed they were done, even if they had only paused for 200 milliseconds. A user would say "I'm looking for software engineering jobs." The system would detect a complete sentence and immediately respond with "what location are you interested in?" But the user had intended to continue: "I'm looking for software engineering jobs in Seattle with salary above one hundred twenty thousand." The system interrupted before they could finish. The user then had to provide the location and salary information separately in response to explicit prompts, making the interaction feel interrogative rather than conversational.

Semantic completion detection is powerful for reducing latency — you do not have to wait a full second to know the user is done if their sentence is grammatically complete and ends with falling intonation. But it breaks when users naturally deliver information in multiple sentences or when they pause between phrases for cognitive reasons (thinking, translating from another language, dealing with a distraction). The fix is to combine semantic cues with **pause duration** and **intonation**. If the sentence is semantically complete and the pause exceeds 500 milliseconds and the intonation pattern suggests finality, the system can proceed. If any of those signals is ambiguous, wait longer.

Premature response is particularly damaging because it trains the user to speak in unnatural patterns. Users learn that pausing mid-thought will cause the system to interrupt, so they speak in rapid, unbroken bursts to avoid being cut off. This increases cognitive load, increases speech errors (the user stumbles over words because they are rushing), and makes the interaction exhausting. A voice interface that forces users to change their natural speech patterns has failed at the UX level, regardless of task completion rate.

## Delayed Response: The Agent That Waits Too Long

The opposite failure is delayed response — the system waits so long to respond that the user thinks it did not hear them, did not understand, or has crashed. Silence beyond 800 milliseconds is uncomfortable. Silence beyond 1,500 milliseconds is alarming. If the user finishes speaking and the agent does not respond within two seconds, the user will assume something is wrong and either repeat themselves or give up.

A municipal services voice assistant in late 2025 used a conservative end-of-utterance window of 1,800 milliseconds to avoid cutting users off. The system never interrupted anyone mid-utterance. It also made every conversation feel broken. Users would finish speaking, wait in silence, assume the system had not registered their input, and say "hello? are you there?" The system, still in its detection window, would then receive two utterances — the original request and "hello are you there" — and attempt to process both as a single turn, generating a confused response.

The metric that matters is **perceived responsiveness** — the time between the user finishing their thought and the agent acknowledging it. Perceived responsiveness should be under 700 milliseconds for high-quality voice UX. Under 500 milliseconds feels instant. Under 300 milliseconds feels like magic. Above 1,000 milliseconds feels sluggish. Above 1,500 milliseconds feels broken. The difference between 600 milliseconds and 900 milliseconds is the difference between a system users trust and a system users tolerate.

Reducing perceived responsiveness is not just about shortening the end-of-utterance window. You can also reduce the **response initiation delay** — the time it takes to generate and begin speaking the response once you have decided the user is done. If STT takes 250 milliseconds, LLM inference takes 180 milliseconds, and TTS takes 220 milliseconds, your minimum response initiation delay is 650 milliseconds before the first word is spoken. You cannot make that zero, but you can pipeline it. Start TTS on the first tokens while the LLM is still generating the rest of the response. Stream audio as it is synthesized rather than waiting for the full sentence. These optimizations can cut 150 to 300 milliseconds from the user's perceived wait time.

## Overlapping Speech: Who Yields?

In human conversation, overlapping speech happens constantly. Both speakers start talking at the same moment. One speaker yields and the other continues. This happens in 8 to 12% of turn transitions in natural dialogue. Voice systems must handle overlap without collapsing into chaos.

When overlap occurs — the agent is speaking and the user starts speaking simultaneously — the system has three options. First, **agent yields immediately** — the moment the system detects the user's voice, it stops speaking and listens. This is the standard barge-in behavior. Second, **agent continues briefly then yields** — the system finishes the current word or phrase, then stops. This reduces the jarring effect of cutting mid-word but risks talking over the user for an extra 200 to 400 milliseconds. Third, **both pause and retry** — both the agent and the user stop speaking, wait a beat, then one of them starts again.

The correct choice depends on the nature of the overlap. If the user clearly intended to interrupt — their speech begins with high energy, they use an interruption phrase like "wait" or "stop" — the agent should yield immediately. If the overlap seems incidental — both parties happened to start speaking at the exact same moment, neither was trying to interrupt the other — the agent can finish its current phrase then pause to see if the user continues.

A customer support voice assistant in January 2026 analyzed 4,200 overlap events to determine optimal yielding behavior. In 71% of cases, the user intended to interrupt and expected the agent to stop. In 22% of cases, the overlap was incidental and the user expected the agent to continue. In 7% of cases, it was ambiguous. The system implemented a **yielding classifier** that analyzed the first 300 milliseconds of the user's overlapping speech — if it contained interruption markers ("wait," "no," "actually") or high acoustic energy, the agent yielded immediately. If it sounded like the user was just starting to speak without urgency, the agent finished its current sentence then paused. This reduced awkward overlap-related confusion by 64% compared to always yielding immediately.

The hardest case is when the user and agent start speaking at the exact same instant. Neither has priority. The user has not yet said anything that signals interruption intent. The agent has not yet said anything that establishes its turn. The system must make a split-second decision: continue or yield. Most production systems default to yielding, because the user's intent to speak is a stronger signal than the agent's generated response. The agent can always regenerate. The user's thought, once lost, is harder to recover.

## Turn-Taking Accuracy Metrics

You measure turn-taking accuracy by tracking three rates across all conversation turns. First, **premature interruption rate** — the percentage of agent responses that began speaking before the user was actually finished. You detect this by looking for barge-in events that occurred within two seconds of the agent starting to speak. If the user interrupted the agent's response very quickly, the agent likely started speaking too soon. A premature interruption rate above 5% indicates the end-of-utterance window is too short.

Second, **delayed response rate** — the percentage of turns where the agent took longer than 1,000 milliseconds to respond after the user finished speaking. You measure this by instrumenting the end-of-utterance detection timestamp and the response initiation timestamp. If the gap exceeds one second, the response is delayed. A delayed response rate above 8% indicates the system is too conservative or has latency bottlenecks in the response pipeline.

Third, **overlap collision rate** — the percentage of turn transitions where both the user and agent started speaking within 200 milliseconds of each other, creating overlap. Overlap collision rate in well-designed systems is typically 3 to 6%. Higher rates indicate the end-of-utterance detection is unreliable. Lower rates are possible but often indicate the system is waiting so long that it never risks overlap, which likely means high delayed response rate.

A telehealth voice assistant tracked these metrics across 21,000 patient interactions in late 2025. Initial results: premature interruption rate 9%, delayed response rate 14%, overlap collision rate 7%. The team identified the root cause — they were using a fixed 1,400-millisecond end-of-utterance window. For fast speakers, this was too long (delayed response). For slow speakers or those pausing mid-utterance, this was too short (premature interruption). They implemented adaptive windows based on per-user speech rate, which brought premature interruption rate to 3%, delayed response rate to 6%, and overlap collision rate to 4%. Users described the updated system as "much more natural" even though task completion rate only improved by 2% — the functional outcome was nearly the same, but the feel of the interaction transformed.

## Turn-Taking in Multi-Party Conversations

Turn-taking becomes exponentially harder when more than two parties are present. If three people are on a voice call with a voice assistant, the system must track who is speaking, who the utterance is directed at (another human or the agent), and when it is appropriate for the agent to respond. If two humans are talking to each other, the agent should remain silent. If one human addresses the agent, the agent should respond even if other humans are also present.

A conference room voice assistant used in enterprise meetings in early 2026 struggled with multi-party turn-taking. The system would respond to rhetorical questions one participant asked another. It would interrupt side conversations. It would fail to respond when multiple people asked it questions simultaneously. The root cause was that the system treated all detected speech as directed at the agent and used the same turn-taking rules regardless of conversational context.

The team implemented **addressee detection** — the system analyzed each utterance to determine whether it was directed at the agent or at another human. Cues included wake word presence (if the utterance started with "hey assistant," it was clearly directed at the agent), eye gaze direction (if the speaker was looking at the microphone or screen, more likely directed at the agent), and linguistic markers (second-person pronouns referring to the agent). If an utterance was classified as human-to-human, the agent did not respond and did not start its turn-taking detection window. If it was classified as human-to-agent, the normal turn-taking rules applied.

Addressee detection reduced inappropriate agent responses by 78% but introduced a new failure mode: the agent sometimes failed to respond to legitimate questions because it misclassified them as human-to-human. A participant would say "what's the status of the roadmap" without using the wake word, the system would assume they were asking another participant, and the agent would stay silent. The participant would then have to repeat the question with the wake word, which felt unnatural. The solution was to use addressee detection as a confidence signal, not a binary gate. If addressee confidence was high (above 0.88 that the utterance was directed at the agent), respond normally. If confidence was low, wait an extra 600 milliseconds before responding. If another human answered during that window, the agent stayed silent. If no one answered, the agent responded. This balanced precision and recall without requiring users to always use the wake word.

## Turn-Taking and User Impatience

Users are not patient. If they finish speaking and the agent does not respond within a second, they assume the system is broken and take action — they repeat themselves, they say "hello?", they tap the screen, they hang up. These impatience behaviors corrupt the turn-taking model because they introduce unexpected utterances during the detection window.

A retail voice assistant in late 2025 used a 1,200-millisecond end-of-utterance window. When the system was under high load, response initiation delay sometimes stretched to 900 milliseconds, making the total perceived delay over two seconds. Users would finish speaking, wait a second, assume the system had not heard them, and say "are you there?" The system, which had already committed to responding to the original utterance, would now receive a second utterance mid-response-generation. It would cancel the in-progress response, process the new utterance ("are you there?"), and reply "yes, I'm here, how can I help?" This abandoned the original request entirely. The user would then have to repeat their original request, creating a frustrating loop.

The fix was **impatience utterance detection**. If the system received a short utterance (under eight words) within two seconds of the user's previous utterance, and that utterance matched a set of impatience patterns ("hello," "are you there," "did you hear me," repeating the previous utterance verbatim), the system ignored it and continued processing the original request. This eliminated the loop in 89% of cases. The system would respond to the original request even if the user had expressed impatience, which felt more natural than responding to the impatience utterance and losing the original intent.

## Silence as a Turn-Taking Signal

Silence is not the absence of information. It is information. In conversation, silence communicates uncertainty, disagreement, consideration, or emotional reaction. Voice systems that treat all silence as "the user is done talking" miss these cues.

If a system asks "would you like to proceed?" and the user pauses for four seconds before saying "yes," the pause means something. It means the user was hesitant, uncertain, or weighing options. A turn-taking model that responds the moment the user says "yes" without acknowledging the pause misses the emotional context. A better response would be "you hesitated — do you have questions before we proceed?" This requires the system to track not just whether the user spoke but how long they paused before speaking.

A mental health support voice assistant in January 2026 implemented **pause-aware turn-taking**. If the user paused for longer than three seconds before responding to a direct question, the system flagged the response as hesitant and adjusted its follow-up. Instead of moving to the next question immediately, it offered space: "take your time" or "is there something I can clarify?" This change had no impact on task completion rate — the user completed the same assessment either way. But user satisfaction scores increased by 19%, and users described the system as "more understanding" and "less robotic." The turn-taking rhythm stayed the same, but the acknowledgment of meaningful silence changed the perceived quality of the interaction.

Turn-taking is invisible when it works and glaring when it fails. Every awkward pause, every premature interruption, every overlap collision chips away at the user's trust that the system understands conversation. You cannot fix turn-taking with better prompts or smarter models. You fix it with latency optimization, adaptive detection windows, and relentless measurement of the millisecond-level timing that makes dialogue feel human or feel wrong.

Next: end-of-utterance detection — how to tell the difference between a pause and the end of a thought.

