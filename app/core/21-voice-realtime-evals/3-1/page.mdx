# 3.1 — The Latency Budget: Allocating Milliseconds Across Components

In November 2025, a healthcare assistant product launched with what the team considered excellent component performance. Their ASR latency averaged 85ms. Their LLM response generation hit 180ms. Their TTS synthesized audio in 110ms. Each component met its individual target. Yet user complaints flooded in within the first week — conversations felt sluggish, unnatural, broken. When the team measured end-to-end latency, they discovered the problem: 85 plus 180 plus 110 equals 375ms for processing alone. Add network round trips, queueing delays, audio buffering, and inter-service communication, and actual time-to-first-audio hit 920 milliseconds. They had optimized each piece without understanding that voice AI operates under a total latency budget, and they had spent far more than they had.

The team spent three months re-architecting. They cut TTS to 65ms by switching providers. They shaved LLM latency to 120ms through prompt optimization and speculative decoding. They eliminated two network hops by co-locating services. They reduced buffering from 80ms to 35ms. The final system delivered 420ms time-to-first-audio at median — still not perfect, but under the critical threshold where conversations started to feel natural. The lesson was clear: voice AI does not reward component-level excellence. It rewards ruthless allocation of a fixed budget.

## The 800-Millisecond Constraint

Human conversational expectations impose a hard ceiling. Research from psycholinguistics and production voice systems converges on the same number: responses that take longer than 800 milliseconds start to feel wrong. Not unacceptable — wrong. The user begins to wonder if the system heard them. They start repeating themselves. They lose conversational flow. By 1000 milliseconds, the interaction no longer feels like dialogue — it feels like operating a slow machine.

This is not a soft guideline. It is a perceptual threshold rooted in how human conversation works. In natural dialogue, response latency averages 200 to 300 milliseconds. Delays beyond 500 milliseconds trigger social repair mechanisms — clarifications, repetitions, discomfort. Voice AI systems must operate within this constraint or accept that they will never feel conversational, regardless of how accurate or helpful their responses are.

The practical implication: you have approximately 800 milliseconds from the moment the user stops speaking until the moment they hear the first word of your response. Every component in your pipeline — automatic speech recognition, language model inference, text-to-speech synthesis, network transmission, orchestration overhead — must fit within that budget. If your components sum to 900 milliseconds, you do not have a slow system. You have a broken system.

## Component Allocation: Where the Milliseconds Go

A typical voice AI pipeline has five major contributors to end-to-end latency. **ASR latency** is the time from when the user finishes speaking to when you have a complete transcription. **LLM inference latency** is the time from receiving the transcript to generating the first token of the response. **TTS latency** is the time from receiving response text to producing the first chunk of synthesized audio. **Network latency** includes round trips to each service, both upstream and downstream. **Orchestration overhead** includes queueing, buffering, serialization, inter-service communication, and any processing your platform adds.

A reasonable starting allocation for an 800ms total budget might look like this: ASR consumes 100 to 150 milliseconds. LLM inference consumes 200 to 300 milliseconds. TTS consumes 80 to 120 milliseconds. Network latency consumes 50 to 100 milliseconds depending on geography and provider proximity. Orchestration overhead consumes 50 to 100 milliseconds if your infrastructure is well-designed, or 150 to 300 milliseconds if it is not. The sum of these ranges spans 480 to 770 milliseconds under favorable conditions — leaving little room for variance, retries, or unexpected delays.

This is why latency engineering for voice AI is fundamentally different from latency engineering for web applications or batch processing. You are not optimizing for speed. You are optimizing for fit. A component that runs in 50 milliseconds is not twice as good as one that runs in 100 milliseconds if the 100ms option provides better quality and you have budget to spare. Conversely, a component that runs in 200 milliseconds when your allocation is 150 milliseconds is not slightly worse — it breaks the entire system, regardless of how good its output is.

## Measuring Your Current Spend

Before you can allocate a latency budget, you must know where your milliseconds currently go. Most teams discover that their intuition about component latency is wrong. They assume the LLM is the bottleneck because it is the most computationally expensive component. In reality, the bottleneck is often network round trips, serialization overhead, or poorly configured buffering in the ASR or TTS pipeline.

Instrumentation must capture every hop. Tag each request with a unique trace ID. Emit timestamps at every component boundary: when the audio stream closes, when the ASR transcript is ready, when the LLM receives the prompt, when the first LLM token is generated, when the TTS request is sent, when the first audio chunk is synthesized, when the audio reaches the client. Do not rely on component-reported latency alone. Components often report processing time but exclude queueing, network transmission, or internal buffering delays.

The healthcare assistant team that opened this subchapter believed their ASR latency was 85 milliseconds because that is what their provider's dashboard reported. When they instrumented end-to-end, they discovered the actual latency was 140 milliseconds. The provider was measuring time from when they began processing audio to when they emitted a transcript. They were not counting the 40 milliseconds it took for the audio stream to fully arrive, or the 15 milliseconds it took to serialize and transmit the transcript back. Those 55 hidden milliseconds were the difference between meeting their allocation and exceeding it.

Measure in production, not in isolated benchmarks. Component latency in a synthetic test often differs from latency under real traffic. Network conditions vary by geography. TTS latency increases when the system is under load. LLM inference slows when request queues grow. You need percentile distributions across real user sessions, not point measurements from a controlled test.

## Making Tradeoffs: Quality vs Speed vs Cost

Once you know your current latency spend, the hard part begins: deciding where to cut and what to preserve. Every component offers a quality-speed tradeoff. Faster ASR models sacrifice accuracy on accented speech or noisy audio. Faster LLMs produce shallower reasoning and more generic responses. Faster TTS systems sound more robotic or less expressive. You cannot simply choose the fastest option for every component and expect acceptable output quality.

The framework for making these tradeoffs starts with identifying your quality floor — the minimum acceptable performance for each dimension. For ASR, the floor might be 92% word error rate on your target accent distribution. For the LLM, it might be passing 85% of your task-specific eval cases. For TTS, it might be a mean opinion score above 4.0 on a 5-point naturalness scale. Once you define these floors, the question becomes: what is the fastest option that stays above the floor?

Some teams discover they can sacrifice LLM sophistication without user impact. A medical assistant that answers scheduling questions does not need a frontier reasoning model — a fine-tuned smaller model running at 120ms might produce indistinguishable responses compared to a 300ms frontier model for that narrow task. Other teams discover they cannot compromise on TTS quality because robotic-sounding voices destroy trust in healthcare contexts, even if the content is perfect. The tradeoff is domain-specific and use-case-specific. There is no universal answer.

Cost enters the tradeoff because faster components are often more expensive. Dedicated low-latency ASR endpoints cost more than shared batch-optimized ones. Running LLMs on high-memory GPUs with large batch sizes reduces per-token cost but increases latency. Running them on smaller batches with faster prompt processing increases cost but cuts latency. TTS providers charge premium rates for low-latency streaming synthesis compared to batch generation. You are often choosing between three constraints: quality, latency, and cost. You can optimize for two. Rarely all three.

## Reallocation Strategies When Components Exceed Budget

When a component exceeds its latency allocation, you have four options: optimize the component, replace the component, reduce its scope, or steal budget from another component. Optimization means tuning configuration, improving infrastructure, or eliminating internal inefficiencies. Replacement means switching to a faster provider or model. Scope reduction means narrowing what the component does — simplifying prompts, reducing TTS expressiveness, or constraining ASR vocabulary. Budget theft means accepting higher latency elsewhere to accommodate the overrun.

Optimization should always come first. Many latency problems are self-inflicted. A team running LLM inference on a cold endpoint that spins up on every request will see 800ms added latency compared to a warm, always-on endpoint. A team sending full conversation history to the LLM on every turn when only the last two turns matter will double inference time for no quality gain. A team using a TTS provider in a distant region will pay 60 to 100 milliseconds in unnecessary network latency. These are configuration errors, not fundamental limits.

Replacement makes sense when optimization hits diminishing returns. If your ASR provider cannot deliver below 180ms no matter how you tune it, and a competitor offers 90ms at similar accuracy, switching is the right move. If your LLM provider's fastest model is still too slow, moving to a smaller open-source model you host yourself might cut latency in half while maintaining acceptable quality for your task. Replacement is disruptive — it requires new integrations, re-tuning, and re-validation — but sometimes it is the only path to meeting your budget.

Scope reduction is underused. Many teams assume they must preserve every feature of a component when latency forces hard choices. In reality, users often do not notice scope reductions if the core functionality remains strong. A customer service bot that uses a 150ms LLM with a simplified prompt might perform indistinguishably from one using a 300ms LLM with a complex Chain-of-Thought prompt if the task is straightforward. The complex prompt added latency but no perceptible quality. Cutting it saves 150 milliseconds at no user cost.

Budget theft is the last resort. If your LLM genuinely needs 280ms and you allocated 200ms, and optimization cannot close the gap, you must find 80ms somewhere else. Can ASR drop from 120ms to 90ms by switching providers? Can TTS drop from 100ms to 70ms by reducing expressiveness? Can you cut orchestration overhead from 80ms to 50ms by eliminating a service hop? If the answer is yes, reallocation works. If the answer is no, you either accept that your total latency exceeds 800ms or you re-scope the product to fit within physical constraints.

## The Compounding Effect of Sequential Latency

Voice AI pipelines are sequential. ASR must finish before the LLM can start. The LLM must generate tokens before TTS can synthesize them. Each delay compounds. A 50ms overrun in ASR plus a 50ms overrun in the LLM plus a 50ms overrun in TTS equals 150ms of total overage, not 50ms. This is why small inefficiencies cascade into system failure.

The compounding effect makes variance particularly dangerous. If ASR latency varies between 80ms and 200ms, and LLM latency varies between 150ms and 350ms, and TTS latency varies between 60ms and 140ms, your total latency in the worst case is not 290ms — it is 690ms just for component processing, before network or overhead. Add 150ms of network and orchestration, and your P99 latency reaches 840 milliseconds. Your median might be 450ms. Your P99 breaks the conversational threshold. Users experience this as inconsistency — sometimes the system feels fast, sometimes it feels broken.

Reducing variance is as important as reducing median latency. A system with consistent 500ms latency feels better than one with 350ms median and 900ms P95. Consistency allows the user to build a mental model of how the system behaves. Variance destroys that model. The user cannot predict when the system will respond, so they treat every interaction as uncertain. This increases cognitive load and reduces satisfaction even when median latency is acceptable.

## Latency Budget as a Living Document

Your latency budget is not a one-time calculation. It evolves as your system matures, as models improve, as user expectations shift, and as you add features. A budget that worked in beta with 500 daily users may not work at scale with 50,000 concurrent sessions. A budget that assumed GPT-5-mini in January 2026 may need revision when GPT-5-nano launches with 40% lower latency at equivalent quality.

Track your budget against actuals weekly. If your LLM allocation is 220ms and actuals are consistently hitting 180ms, you have 40ms of slack. You can spend that slack on more complex prompts, richer TTS expressiveness, or additional features that were previously latency-prohibitive. If actuals are consistently exceeding allocation, treat it as a production incident — not a minor issue. Latency overruns that persist for weeks indicate a structural problem that will only worsen under load.

Communicate the budget to every team that touches the voice pipeline. Product teams need to understand that adding a feature requiring an extra LLM call costs 200ms of latency, which might exceed the budget. Engineering teams need to understand that migrating to a new TTS provider must not increase latency even if it improves quality, unless you can cut latency elsewhere. Leadership needs to understand that improving conversational naturalness is not free — it may require spending latency budget that forces tradeoffs in other areas.

The latency budget is the forcing function that keeps your voice AI system honest. Without it, every team optimizes locally and the system degrades globally. With it, you have a shared constraint that aligns decision-making. Every millisecond is accounted for. Every trade-off is explicit. Every feature competes for the same scarce resource. This is how you build a voice system that feels fast even when the underlying components are not — by never spending more than you have.

---

Next, we examine the single most important latency metric in voice AI: time to first audio, the measure that defines whether your system feels conversational or broken.
