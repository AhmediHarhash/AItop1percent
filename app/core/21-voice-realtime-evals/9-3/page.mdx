# 9.3 — Handling Network Jitter and Packet Loss

The network between your user and your server is not a pipe. It is a series of routers, switches, wireless hops, and congested links that introduce variable delay and occasionally drop packets entirely. Your audio pipeline must tolerate this chaos without forcing the user to repeat themselves or hear distorted responses.

In late 2025, a voice AI therapy platform discovered that 14% of their sessions had at least one moment where the system failed to hear the user correctly. The team assumed speech recognition errors. Logs showed something else: the system received only 94% of audio packets during affected sessions. The missing 6% were not transcription failures—they were packets the server never saw. The network dropped them. The therapy sessions recorded partial user utterances with silent gaps where dropped packets should have been, and the transcription model did its best with incomplete audio.

The network will fail. Your system must degrade gracefully.

## Jitter: Why Packets Arrive at Unpredictable Times

Jitter is variation in packet arrival time. You send audio packets every 20 milliseconds. The first packet arrives 20 milliseconds after you send it. The second arrives 23 milliseconds after you send it. The third arrives 17 milliseconds after you send it. The interval between sending packets is constant. The interval between receiving them is not.

Jitter occurs because packets traverse different network paths, wait in router queues of varying depth, and compete with other traffic for bandwidth. A packet delayed by 10 milliseconds at one router might skip ahead of the next packet in the sequence if that packet was delayed 15 milliseconds at a different router.

Small jitter is invisible if you buffer adequately. If your buffer holds 60 milliseconds of audio and jitter varies between minus 5 and plus 10 milliseconds, you never underrun. Large jitter forces you to choose between larger buffers that add latency or smaller buffers that risk underruns.

The magnitude of jitter depends on network path. Users on wired connections with uncongested routes see median jitter below 5 milliseconds. Users on cellular networks see median jitter between 10 and 30 milliseconds. Users on congested WiFi or cellular networks with poor signal see jitter exceeding 100 milliseconds regularly.

A healthcare voice AI measured jitter distribution across 50,000 sessions in mid-2025. The median session had median jitter of 12 milliseconds and 95th percentile jitter of 38 milliseconds. The 95th percentile session had median jitter of 45 milliseconds and 95th percentile jitter of 220 milliseconds. A buffer sized for typical sessions would underrun constantly during high-jitter sessions. A buffer sized for high-jitter sessions would add 200 milliseconds of latency to typical sessions unnecessarily.

## Measuring Jitter in Real Time

You cannot manage jitter without measuring it. The measurement is straightforward: track expected arrival time for each packet and compare to actual arrival time. If you expect packets every 20 milliseconds and packet N arrives 23 milliseconds after packet N-1, you observed 3 milliseconds of jitter.

Store recent jitter measurements in a sliding window. Calculate percentiles continuously. The 50th percentile tells you typical jitter. The 95th percentile tells you how much buffer you need to prevent underruns in 95% of cases. The difference between 50th and 95th percentile tells you whether jitter is consistent or spiky.

A real-time voice translation service monitored jitter every 5 seconds with a 60-second sliding window. When 95th percentile jitter exceeded 80 milliseconds, the system logged a network quality warning and increased buffer size by 20 milliseconds. When 95th percentile jitter fell below 40 milliseconds for 30 consecutive seconds, the system decreased buffer size by 10 milliseconds. Buffer size adapted to current conditions with hysteresis to prevent rapid oscillation.

Jitter measurement also reveals network path changes. If a user is on a cellular connection and switches towers, jitter characteristics change abruptly. A sudden increase in median jitter indicates a network transition. You might increase buffering temporarily until the new network path stabilizes.

## Packet Loss: When Audio Data Never Arrives

Packet loss is worse than jitter. A delayed packet arrives late but intact. A lost packet never arrives. The audio it contained is gone. If you wait for it, you wait forever. If you proceed without it, you have a gap in your audio stream.

Real-world packet loss rates depend on network quality. Wired networks with uncongested paths lose less than 0.1% of packets. WiFi networks lose 0.5% to 2% depending on signal strength and interference. Cellular networks lose 1% to 5% in good conditions and 10% to 20% in poor conditions. A user walking through a building while on a voice call experiences packet loss every time they pass through an area with degraded signal.

Lost packets create three problems. First, the speech recognition model receives incomplete audio and produces incorrect transcriptions. Second, playback buffers underrun because expected audio never arrives. Third, you cannot distinguish between a lost packet and a delayed packet until you decide the packet is not coming.

You detect packet loss by examining packet sequence numbers. If you receive packet 100 followed by packet 102, packet 101 is missing. You wait briefly in case packet 101 arrives out of order. If it does not arrive within your jitter tolerance window—typically 50 to 150 milliseconds—you declare it lost.

A voice assistant for warehouse workers experienced 8% packet loss during peak shift hours when hundreds of workers used the system simultaneously on the warehouse WiFi network. The system waited 80 milliseconds for delayed packets before declaring them lost. During packet loss events, transcription latency spiked because the system spent 80 milliseconds waiting for packets that never came. The team reduced the wait threshold to 40 milliseconds during detected packet loss conditions. Latency improved. Transcription accuracy dropped slightly because some delayed packets were declared lost prematurely. The tradeoff was correct—responsiveness mattered more than perfect accuracy in a warehouse environment.

## Packet Loss Concealment: Playing Something Instead of Silence

When a packet is lost, you must decide what to play during the time that packet would have occupied. The naive approach: play silence. The user hears a brief gap. For single lost packets, the gap is 20 to 40 milliseconds. For multiple consecutive lost packets, the gap is longer.

Silence is noticeable. A 40-millisecond gap sounds like a glitch. A 100-millisecond gap sounds like the connection dropped. Users find sustained silence more disruptive than mild distortion.

The standard alternative is packet loss concealment. The system estimates what the lost audio should have sounded like based on surrounding packets and plays that estimate. The estimation algorithm ranges from simple (repeat the last packet) to sophisticated (predict the next packet based on audio signal characteristics).

Repeating the last packet works well for short losses. If you lose one 20-millisecond packet, you play the previous 20-millisecond packet again. The audio sounds slightly stuttered but remains intelligible. If you lose three consecutive packets, repeating the last packet makes the audio sound robotic.

Prediction-based concealment uses the audio signal's frequency content and amplitude envelope to extrapolate what the next packet should contain. If the lost packet occurred during a vowel sound, the algorithm continues the vowel with gradually decreasing amplitude. If it occurred during a consonant, the algorithm fills the gap with noise that matches the spectral characteristics of the consonant.

A voice AI phone system implemented prediction-based packet loss concealment in early 2026. During testing on networks with 5% packet loss, evaluators could not reliably distinguish between concealed audio and real audio for single-packet losses. For losses of three or more consecutive packets, concealment quality degraded but remained preferable to silence. Transcription accuracy with concealment was 91.2% compared to 87.4% with silence filling.

## Forward Error Correction: Sending Redundancy to Tolerate Loss

Packet loss concealment reconstructs lost audio after the fact. Forward error correction prevents loss from causing gaps by sending redundant data preemptively. The simplest form: send each audio packet twice. If one copy is lost, the other arrives. You tolerate 50% packet loss without gaps.

Sending everything twice doubles bandwidth consumption. More sophisticated forward error correction encodes redundancy efficiently. You send packet N along with a small amount of information that lets you reconstruct packet N-1 if it was lost. If packet N-1 arrives successfully, you discard the redundancy. If packet N-1 is lost, you reconstruct it from the redundancy in packet N.

The bandwidth overhead depends on how much loss you want to tolerate. To reconstruct one lost packet out of every ten, you add roughly 10% overhead. To reconstruct one lost packet out of every three, you add roughly 30% overhead. You tune overhead based on observed loss rates.

Forward error correction works best for voice where high-fidelity reconstruction is not required. If you lose an audio packet and reconstruct it from redundancy, the reconstructed audio is lower quality than the original but still intelligible. For applications where every bit matters—like medical dictation—you need stronger error correction or retransmission.

A telehealth platform deployed forward error correction in regions with poor cellular network quality. In urban areas with 1% packet loss, the overhead was 10% and loss concealment handled the occasional missing packet. In rural areas with 8% packet loss, the overhead was 30% and forward error correction prevented nearly all audio gaps. The team configured overhead dynamically based on measured packet loss during the first 10 seconds of each connection.

## The Quality Cliff: When Degradation Becomes Abandonment

Users tolerate mild quality degradation. They abandon systems with severe degradation. The transition between acceptable and unacceptable is sharp. You do not see gradual abandonment as quality declines—you see a cliff.

Research on voice quality perception consistently shows that Mean Opinion Score (MOS) above 4.0 on a 5-point scale is considered good, MOS between 3.5 and 4.0 is acceptable, and MOS below 3.5 triggers abandonment. The difference between acceptable and unacceptable is half a point on a subjective scale.

Packet loss and jitter degrade MOS. Clean audio with no loss or jitter scores above 4.0. Audio with 1% packet loss and moderate jitter scores around 3.8. Audio with 3% packet loss scores around 3.4. At 5% packet loss, MOS drops below 3.0 and users describe the experience as poor.

The cliff occurs because users expect voice systems to work reliably. When they encounter choppy audio or missing words, they assume the system is broken, not that their network is degraded. They do not retry on a better network—they switch to a competitor.

A voice AI startup for restaurant ordering lost 40% of orders initiated over cellular networks. The team investigated and found that sessions with packet loss above 4% had abandonment rates of 68%. Sessions with packet loss below 2% had abandonment rates of 8%. The difference between 2% and 4% packet loss was the difference between a viable product and a failed product. The team deployed aggressive buffering and forward error correction to keep effective packet loss below 2% even when actual network loss reached 6%. Cellular abandonment rates dropped to 12%.

## Geographic Distribution to Minimize Network Hops

The best way to handle jitter and packet loss is to avoid it. The fewer network hops between user and server, the less jitter you see and the lower your packet loss rate. Geographic distribution puts servers physically close to users.

A voice AI platform with servers only in US-East saw median latency of 180 milliseconds and 95th percentile packet loss of 2.8% for users in Asia. After deploying servers in Singapore, Tokyo, and Mumbai, Asian users saw median latency of 65 milliseconds and 95th percentile packet loss of 0.9%. Network conditions did not change. The number of hops between user and server changed.

Edge deployment is the extreme version. Instead of running servers in a few large regions, you run small servers in dozens or hundreds of edge locations. Users connect to the closest edge server. Latency and packet loss both drop dramatically. Infrastructure complexity increases proportionally.

The tradeoff is cost versus quality. Running servers in three regions is manageable. Running servers in thirty regions requires sophisticated deployment automation and monitoring. Running servers at the edge in hundreds of locations requires infrastructure most teams do not have. Choose based on whether network quality is your constraint. If your users tolerate current network conditions, centralize infrastructure and reduce costs. If network quality drives abandonment, distribute infrastructure and accept complexity.

## Adaptive Quality: Degrading Gracefully Under Poor Conditions

When network conditions degrade beyond what buffering and error correction can handle, the alternative to connection failure is adaptive quality reduction. The system detects that it cannot deliver full-quality audio reliably and switches to a lower bitrate codec or reduces model output verbosity.

Audio codecs support multiple quality levels. Opus, the most common codec for real-time voice, encodes at bitrates from 6 kbps to 510 kbps. At 64 kbps, voice quality is excellent. At 24 kbps, quality is acceptable for conversation. At 12 kbps, quality is poor but intelligible. Switching from 64 kbps to 24 kbps reduces bandwidth consumption by 60% and makes the same network conditions tolerable.

The quality reduction is audible. Users notice the difference between 64 kbps and 24 kbps. But they notice connection failure more. A system that stays connected with reduced quality is better than a system that fails entirely.

A voice assistant for delivery drivers adapted bitrate based on measured packet loss. At packet loss below 2%, the system used 64 kbps encoding. At packet loss between 2% and 5%, it switched to 32 kbps. At packet loss above 5%, it switched to 16 kbps. The quality reduction was noticeable but drivers completed 95% of attempted voice interactions even in poor coverage areas. The previous version with fixed 64 kbps encoding had a 73% completion rate.

Adaptive quality requires client cooperation. The server cannot unilaterally change codec settings—the client must support the lower-quality codec and switch when requested. This means negotiating codec capabilities during connection setup and implementing signaling to trigger quality changes mid-session.

## Monitoring Network Quality in Production

You cannot fix network problems you do not see. Production systems need instrumentation for jitter distribution, packet loss rate, buffer underruns, and quality degradation events. The metrics must be per-session so you can identify patterns. Are certain users always experiencing poor quality? Are certain times of day worse? Are certain regions showing systematic degradation?

Aggregate metrics hide problems. If 95% of sessions have excellent network quality and 5% have terrible quality, your median metrics look fine but 5% of users are abandoning. You need to track the distribution, not just the average.

A voice platform analyzed per-session network metrics and discovered that users on a specific mobile carrier in a specific city experienced 10% packet loss during evening hours. The platform operators contacted the carrier, who identified a congested cell tower and upgraded capacity. Packet loss for those users dropped to 2%. The issue was invisible in aggregate metrics because it affected only 0.3% of total sessions, but it affected 40% of sessions in that city during peak hours.

The metrics you track should include: median jitter, 95th percentile jitter, packet loss rate, buffer underrun frequency, forward error correction activation rate, adaptive quality downgrade frequency, and time spent at each quality level. Tag each metric with session metadata: user location, network type (WiFi vs cellular), carrier, time of day, and session duration. The patterns emerge when you can slice the data.

---

Your system now handles variable packet arrival times and tolerates occasional packet loss. But connections themselves have lifecycle. They establish, run for minutes or hours, and eventually close. Managing that lifecycle correctly prevents resource leaks, zombie connections, and user frustration when sessions drop unexpectedly.

