# 8.8 — Disclosure Requirements: When the Agent Must Identify Itself

In November 2025, a healthcare appointment scheduling system was fined $280,000 under California law for failing to disclose its automated nature. The system used ElevenLabs Multilingual v2 to generate a voice that sounded indistinguishable from a human receptionist. Callers believed they were speaking to a person. They provided medical information, discussed symptoms, and scheduled procedures without knowing the system was fully automated. The violation was discovered when a patient filed a complaint after learning—weeks later—that the "helpful receptionist" who had scheduled her surgery was a voice synthesis model. The fine was small compared to the reputational damage. Patient trust dropped. Media coverage was brutal. The health system spent six months rebuilding credibility.

The technical quality of the voice was not the issue. The system worked. It scheduled appointments correctly. It answered questions accurately. The problem was deception. Patients had a right to know they were speaking to a machine. The system denied them that knowledge. This is the disclosure problem. As voice AI becomes perceptually indistinguishable from human speech, the obligation to disclose its automated nature becomes both more important and more difficult. Disclosure protects user autonomy. It also protects the deploying organization from regulatory penalties and trust collapse. But poorly executed disclosure destroys user experience. The challenge is to meet the legal obligation without making the system unusable.

## The Regulatory Landscape in 2026

The EU AI Act, which entered full enforcement in mid-2025, classifies most customer-facing voice AI systems as general-purpose AI with systemic risk obligations. Article 52 explicitly requires disclosure when an AI system interacts with natural persons unless it is obvious from the context that the system is automated. "Obvious from the context" is narrowly defined. A chatbot on a website with a robot avatar is obvious. A voice on a phone call is not. The voice could be human or synthetic. The user cannot tell. Therefore, disclosure is mandatory.

The disclosure must occur before the user provides any personal data or consents to any action. If the system asks for the caller's name, date of birth, or account number before disclosing its automated nature, that is a violation. The user must know they are speaking to an AI before they share information. This creates a design constraint: the disclosure must be the first thing the system says, or it must happen within the first few seconds before any sensitive interaction.

California's SB-1001, originally passed in 2019 and significantly expanded in 2024, imposes similar requirements with higher penalties. It applies to any automated voice system that simulates human conversation with the intent to deceive. Intent is inferred if the system does not proactively disclose. The penalty for non-disclosure ranges from $2,500 to $10,000 per violation. A violation is defined per interaction. A system that handles ten thousand calls per month without disclosure accrues millions in potential penalties.

Other US states have adopted similar laws. By 2026, any voice AI system serving US customers must assume disclosure is legally required. The variation is in enforcement intensity. California aggressively enforces. Other states rely on complaints. But the risk is uniform: undisclosed voice AI is a liability. The question is not whether to disclose but how to disclose without destroying the user experience.

International jurisdictions follow similar trends. Canada's PIPEDA amendments in 2025 added disclosure requirements for automated decision-making systems, including voice AI. Australia's AI Ethics Framework, while not legally binding, establishes industry norms that de facto require disclosure. Japan's guidelines for AI transparency apply to voice systems used in customer service. If you operate globally, you must design for the strictest requirement, which is the EU AI Act. Compliance with the EU standard ensures compliance almost everywhere else.

## When Disclosure Must Happen

The legal obligation is "before the user provides personal data or consents to any action." In practice, this means the disclosure must occur within the first ten seconds of the call. The system answers. The caller hears a voice. Before they say anything substantive, the voice identifies itself as automated. This is the cleanest approach. It eliminates ambiguity. The user knows from the start.

Some systems defer disclosure until the user is about to provide sensitive information. The call starts without disclosure. The system greets the caller and asks how it can help. The caller states their intent. Only when the system is about to request an account number or personal details does it disclose: "Before we continue, I should let you know I am an automated assistant. I can help you with this, or I can transfer you to a person." This approach delays disclosure to avoid disrupting the opening flow. It is legally risky. If the user has already stated their name or reason for calling, they have arguably provided personal data. The disclosure came too late.

Another trigger for disclosure is when the user explicitly asks. If at any point the caller says "Am I talking to a person?" or "Is this a bot?" the system must answer truthfully and immediately. Evasive answers—"I am here to help you"—do not satisfy the obligation. The correct response is "I am an automated assistant." The user has asked a direct question. The answer must be direct.

Disclosure must also occur before certain high-stakes actions. If the system is about to schedule a medical appointment, execute a financial transaction, or cancel a service, it must confirm its automated nature before executing. Even if the system disclosed at the start of the call, it should re-disclose before irreversible actions: "Just to confirm, I am an automated system. I am about to schedule your appointment for Thursday at 3 PM. Is that correct?" This gives the user a chance to object or request human oversight.

Some systems disclose continuously through design cues. The voice includes subtle markers that signal automation—a consistent lack of filler words like "um" or "uh," perfectly timed responses with zero latency variation, or an explicitly synthetic vocal quality. These cues are not legally sufficient on their own. The user might not notice them. But they complement explicit disclosure. A user who hears "I am an automated assistant" and also notices the voice never pauses or stumbles will have no doubt about the system's nature.

## How to Disclose Without Destroying UX

The most common disclosure phrasing in 2026 is "This is an automated assistant" or "You are speaking with an automated assistant." These phrases are clear, direct, and short. They do not apologize for being automated. They do not oversell capabilities. They state the fact. The user knows what they are dealing with. The call continues.

The phrasing matters. "I am a bot" sounds dismissive and low-quality. Users associate "bot" with bad experiences. "I am an AI assistant" is better but slightly ambiguous. Some users do not know what AI means. "I am an automated assistant" is clearest. It uses plain language. It does not assume technical knowledge. It works for all demographics.

Tone and placement also matter. The disclosure should sound natural, not robotic or apologetic. The system should not say "Unfortunately, I am just an automated system." The word "just" diminishes the system's capabilities and sets a negative expectation. The word "unfortunately" implies the user is stuck with second-tier service. A better phrasing is "Hi, this is an automated assistant. I can help you with appointments, billing, and general questions. How can I help you today?" The disclosure is embedded in a confident, helpful greeting. The user hears competence, not limitation.

Some systems use humor to soften the disclosure. "Hi, this is an automated assistant—I promise I am here to help, even if I do not need coffee breaks." This works for some brand voices. It risks sounding flippant for serious contexts. A healthcare or financial services voice should not joke. A retail or hospitality voice might. The decision depends on brand tone and user expectations.

The disclosure should not dominate the opening. If the first thirty seconds of the call are spent explaining the system's limitations, the user will hang up. The disclosure is one sentence. It is followed immediately by value: "How can I help you today?" The user knows the system is automated. They also know it is ready to help. The balance is critical.

## Recording Consent and Two-Party Notification

Many jurisdictions require consent before recording calls. In the United States, some states are one-party consent—only one party to the conversation must consent—and others are two-party or all-party consent—all parties must consent. California, Florida, Pennsylvania, and several others are two-party states. If your system records calls involving users in these states, you must notify them and obtain consent before recording.

The standard notification is "This call may be recorded for quality and training purposes." The phrasing is familiar. Users expect it. Most accept it without objection. But the notification is legally required to occur before recording starts. If your system begins recording before the notification completes, you are in violation. The recording must start only after the notification is delivered.

This creates a technical design constraint. The call connects. The system delivers the disclosure and recording notification. Only then does recording begin. You log the timestamp of the notification and the start of recording. If the user objects to recording, the system must either stop recording or terminate the call, depending on your legal requirements and operational needs. Most systems allow opt-out: "If you prefer not to be recorded, I can connect you with a representative who will not record the call."

The recording notification and the AI disclosure can be combined. "Hi, this is an automated assistant. This call may be recorded for quality and training purposes. How can I help you today?" The user receives both pieces of information in the opening seconds. They know they are speaking to an AI. They know the call might be recorded. They can proceed with informed consent or opt out.

In some jurisdictions, recording notification alone is not sufficient. The user must actively consent. This is rare in the US but more common in the EU under GDPR. The system must say "Do you consent to this call being recorded?" The user must say yes. If they say no, recording is prohibited. This adds friction. Most users consent, but the extra step increases call handling time and creates opportunities for user confusion. You use active consent only when legally required.

## The Transparency Paradox: Disclosure That Damages Trust

There is a paradox in disclosure. Users have a right to know they are speaking to an AI. But revealing that fact can damage trust and reduce task completion rates. Studies from 2024 and 2025 consistently show that users rate identical interactions more negatively when they know the system is automated. The same conversation that receives a satisfaction score of 4.2 out of 5 when users believe they are speaking to a human receives a 3.6 when they know it is AI. The system's performance does not change. The user's perception does.

This is not irrational bias. It reflects learned experience. Most users have had bad experiences with automated systems—rigid phone trees, unhelpful chatbots, systems that could not understand their requests. They approach disclosed AI with lowered expectations and heightened skepticism. Even if the system performs well, they are primed to notice failures and discount successes.

The transparency paradox creates a tension. You are legally required to disclose. Disclosure reduces user satisfaction and trust. The solution is not to avoid disclosure—that is illegal and unethical. The solution is to build systems good enough that disclosure does not matter. If the system consistently solves the user's problem quickly and correctly, they will not care that it is automated. Disclosure becomes a neutral fact, not a red flag.

This requires excellence in execution. The system must have low latency, high accuracy, and robust error handling. It must recognize when it cannot help and escalate gracefully. It must never trap the user in loops or force them to repeat information. If the system is excellent, users will say things like "I did not realize I was talking to a bot until you mentioned it—it was actually helpful." If the system is mediocre, disclosure confirms their worst expectations.

Some companies try to mitigate the trust problem by emphasizing the benefits of automation. "I am an automated assistant, which means I am available 24/7 and can help you immediately without wait times." This reframes automation as a feature, not a limitation. It works for some users. Others remain skeptical. The effectiveness depends on delivery and context.

## Disclosure in Multi-Turn Conversations

A single call might involve multiple turns, context switches, and handoffs. The disclosure at the start of the call covers the initial interaction. But if the conversation shifts topics or if the system transfers the user to a different automated module, should it disclose again?

The conservative answer is yes. If the user is transferred from a billing module to a technical support module, and both are automated, the transfer should include a reminder: "I am transferring you to our technical support system, which is also automated." This ensures the user does not mistakenly believe they have been escalated to a human. It prevents the frustration of expecting human help and receiving another bot.

The pragmatic answer depends on context. If the transfer is seamless—the conversation continues without interruption—repeated disclosure is jarring. The user already knows they are speaking to an automated system. Telling them again feels redundant. A middle-ground approach is to disclose when the user's expectation might change. If the system is about to perform an action the user might assume requires human judgment—approving a refund, scheduling a medical procedure—it confirms: "I can handle this for you as an automated system, or I can transfer you to a person. Which would you prefer?" This gives the user agency without being repetitive.

Handoffs from automated to human agents should be explicit. "Let me connect you with a specialist who can help." The user understands they are moving from automation to a person. The human agent, when they join, should confirm: "Hi, this is Jordan, I am a person, and I am here to help you." This sounds obvious, but some users, after interacting with a high-quality AI, are not sure whether the next agent is human or another bot. Explicit confirmation prevents confusion.

## Ethical Disclosure Beyond Legal Minimums

Legal compliance is the floor, not the ceiling. The law requires disclosure before collecting personal data. Ethics might require disclosure earlier or more comprehensively. If your system is designed to influence user decisions—recommending products, advising on health choices, guiding financial actions—the user should know the recommendations come from an automated system, not a human expert.

This is especially true in domains where users expect human judgment. A legal advice chatbot, even if it discloses its automated nature, might mislead users who do not fully understand what that means. They might believe "automated" means the system is executing a human expert's logic. They might not realize the system is a statistical pattern matcher without true legal reasoning. Fuller disclosure would explain not just that the system is automated but what that implies about its limitations: "I am an automated assistant. I provide general information based on common patterns, but I am not a lawyer and my responses are not legal advice."

Medical triage systems face similar obligations. Disclosing "I am an automated assistant" is legally sufficient. But ethically, the system should also explain that it cannot diagnose, that it provides guidance based on symptom patterns, and that users with urgent symptoms should seek human medical care immediately. The user should understand both the system's nature and its limits.

Some companies go further and disclose the underlying technology. "I am powered by GPT-5" or "I use Claude Opus 4.5." This is not legally required, but it provides additional transparency. Users who understand these models can calibrate their trust accordingly. Users who do not understand can ignore the detail. The disclosure does no harm and might build credibility with technically informed users.

The ethical standard is informed consent. The user should have enough information to decide whether to engage with the system, what information to share, and how much weight to give the system's responses. Legal disclosure meets the first criterion—knowing the system is automated. Ethical disclosure meets all three. It explains what automation means in this context and what the user should and should not expect.

## Practical Implementation: Disclosure in Production Systems

Implementing disclosure in a production voice system is straightforward. The system's opening prompt includes the disclosure statement. The dialogue manager logs that the disclosure was delivered. If the user asks whether they are speaking to a person, the intent recognition triggers a response that confirms the automated nature. If the user requests a human, the system escalates.

The challenge is ensuring disclosure happens reliably. Systems fail. Network glitches cut off the opening seconds of a call. The user might not hear the disclosure. You cannot assume the user heard something just because the system said it. You need confirmation. One approach is to treat the disclosure as a required acknowledgment. The system says "This is an automated assistant. I can help you with your account. Does that sound good?" The user's response—whether explicit yes or implicit agreement by stating their intent—confirms they engaged after disclosure. If the user does not respond, the system repeats the disclosure.

Another approach is to log disclosure delivery per session and include it in audit trails. If a user later claims they were not informed the system was automated, you can review the session log and confirm the disclosure was delivered. This does not prove the user heard it, but it proves the system attempted disclosure. That is often sufficient for legal defense.

You also need to handle edge cases. If the user interrupts the disclosure—speaking over it—does the system continue or stop? Best practice is to let the disclosure complete, then respond to the user's interruption. The disclosure is legally required. The user's impatience does not override the obligation. The system finishes the disclosure sentence, then addresses the user's input.

If the system uses barge-in—allowing the user to interrupt at any time—the disclosure must be marked non-interruptible. The first sentence completes before barge-in is enabled. The user can then interrupt subsequent sentences. This ensures the disclosure is always delivered in full. After that, the system can be as responsive as the design allows.

## When Disclosure Alone Is Not Enough

Disclosure tells the user the system is automated. It does not tell them whether the system is competent. A user who knows they are speaking to an AI might still overestimate its capabilities. They might ask questions the system cannot answer. They might expect reasoning it cannot perform. Disclosure should be accompanied by capability signaling.

Capability signaling is the practice of telling users what the system can and cannot do. "I can help you with account balances, recent transactions, and bill payments. For disputes or complex issues, I will connect you with a specialist." The user now knows both that the system is automated and what tasks it handles. They can decide whether to proceed or ask for a human immediately.

This reduces frustration. Users do not waste time trying to get the automated system to do things it cannot do. They also have realistic expectations. If they ask for something outside the system's scope, they are not surprised when it escalates. The signaling sets the frame. The interaction proceeds within that frame. Both sides understand the boundaries.

Capability signaling is especially important in high-stakes domains. A healthcare triage bot should say "I can help you assess your symptoms and determine whether you need immediate care. I cannot diagnose conditions or prescribe treatments." A financial advice bot should say "I can provide general information about investment options. I cannot give personalized financial advice or recommend specific investments." The user knows what the system is, what it can do, and what it cannot. They make informed decisions.

---

Disclosure is not a user experience problem to be minimized. It is a legal obligation and an ethical responsibility. The goal is not to hide the system's automated nature but to reveal it clearly while still delivering value. A well-designed disclosure is brief, natural, and confident. It informs without apologizing. It sets expectations without limiting ambition. The system says "I am an automated assistant" and then proceeds to demonstrate that being automated does not mean being unhelpful. Over time, as voice AI quality improves, disclosure will become unremarkable. Users will expect automation. They will judge systems not on whether they are human but on whether they work. Until then, you disclose. You do it early. You do it clearly. You do it every time. That is the standard.

Next, we address the most consequential decision a voice system makes dozens of times per second: when to allow the call to continue and when to drop it—the call drop decision and abort criteria.
