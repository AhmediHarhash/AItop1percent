# 12.19 — Synthetic Voice Detection: When Callers Are Not Human

AI can now call AI. In mid-2025, a customer service platform for a retail chain began receiving calls that followed normal conversation patterns, asked coherent questions, and responded appropriately to agent prompts — but the callers were not human. They were voice agents powered by text-to-speech synthesis and large language models, designed to automate returns fraud. Each synthetic caller requested a return authorization for a high-value item, provided plausible purchase details scraped from real order confirmation emails obtained through phishing, and successfully navigated the multi-step return verification process. Over eight weeks, the fraud ring processed three hundred seventy-four fraudulent returns worth one point two million dollars. The customer service agents never suspected the callers were synthetic. The fraud was detected only when warehouse staff noticed that returned items were not arriving and initiated an investigation. By the time the pattern was identified, the damage was done.

Synthetic voice detection for caller authentication is a different problem than synthetic voice detection for general fraud prevention. In authentication scenarios, you are verifying that a known user is who they claim to be — synthetic voice is one of several spoofing techniques an attacker might use to impersonate a legitimate user. In caller verification scenarios, you are determining whether the caller is human at all, regardless of identity. The threat model is broader. The attacker is not trying to clone a specific user's voice — they are trying to create a plausible human caller using synthetic speech. The detection challenge is distinguishing high-quality synthetic voices from real human voices in real-time conversations.

## The Synthetic Voice Threat Model

Synthetic voice technology in 2026 operates at commercial quality. Text-to-speech models like OpenAI's Codex Voice, Google's Lyra 3, and ElevenLabs' multilingual TTS produce speech that most listeners cannot distinguish from human voices in blind tests. Voice cloning models can replicate a specific speaker from as little as fifteen seconds of sample audio. These tools are openly available, either as paid APIs or open-source implementations. The barrier to entry for synthetic voice fraud is low.

The attacker's goal determines the attack pattern. In impersonation fraud, the attacker wants to sound like a specific person — usually a high-value target like a CEO or account holder. They obtain a voice sample through publicly available recordings, phishing, or social engineering, then use a voice cloning model to generate speech that matches the target's voice characteristics. In mass fraud, the attacker does not care about sounding like a specific person — they just need to sound plausibly human. They use a high-quality text-to-speech model to generate speech for automated calls, often cycling through different synthetic voices to avoid pattern detection.

The interaction model varies. In fully automated attacks, the synthetic voice is part of a voice bot that handles the entire conversation autonomously using a large language model for dialogue management and text-to-speech for speech output. The bot can hold multi-turn conversations, respond to clarifying questions, and adapt its strategy based on agent responses. In hybrid attacks, a human operator drives the conversation while using real-time voice conversion to mask their identity or text-to-speech to read pre-scripted responses. A financial services company detected a hybrid attack in 2025 where the caller was human, but every time the caller needed to provide account numbers or authorization codes, they switched to a text-to-speech voice reading the digits. The pattern was subtle — most of the conversation was human, only specific high-value utterances were synthetic — but it was detectable.

The scale of the threat is increasing. In 2024, synthetic voice attacks were rare enough that most customer service platforms did not deploy detection systems. By 2026, synthetic voice attacks are common enough that deployment without detection is negligent. A telecommunications company reported that 2.3% of inbound calls in Q1 2026 were flagged as synthetic by their detection system. Not all flagged calls were fraudulent — some legitimate users employed text-to-speech tools for accessibility reasons — but the volume of synthetic calls was significant enough to require systematic detection and triage.

## Detection Methods for Synthetic Speech

Synthetic voice detection relies on identifying artifacts that distinguish synthetic speech from human speech. These artifacts fall into three categories: spectral artifacts introduced by neural vocoders, prosodic artifacts from unnatural intonation and rhythm patterns, and temporal artifacts from unnatural phoneme transitions and timing.

Spectral artifacts are the most reliable detection signal. Neural vocoders — the components of text-to-speech systems that convert linguistic features into audio waveforms — introduce subtle distortions in the frequency spectrum that differ from natural speech production. A common artifact is over-smoothed harmonic structure. Human vocal cords produce complex harmonic patterns with natural variability. Neural vocoders produce harmonics that are too regular, too clean. A detection model trained to identify these patterns can classify speech as synthetic with high accuracy.

A European insurance company deployed a spectral artifact detector in 2025 using a convolutional neural network trained on mel-spectrograms. The model analyzed the frequency content of speech in overlapping time windows, looking for harmonic regularities, phase coherence patterns, and high-frequency noise characteristics that distinguished synthetic from human speech. During validation testing, the model achieved 94% detection accuracy on synthetic speech generated by the ten most common text-to-speech models, with a false positive rate of 1.8% on human speech. The model ran on GPU inference infrastructure, processing each audio chunk in real time as it arrived from the telephony system.

Prosodic artifacts provide complementary detection signals. Human speech exhibits natural variability in pitch, timing, and stress patterns. Synthetic speech, even from advanced models, tends to exhibit prosody that is too consistent or too predictable. Pauses are too evenly timed. Pitch contours are too smooth. Stress patterns are too regular. A detection model can learn these patterns by comparing speech to expected distributions of prosodic features in natural conversation.

A financial services company implemented prosodic analysis using a recurrent neural network that analyzed pitch contour, energy envelope, and pause distribution across entire utterances. The model was trained on ten thousand hours of authentic customer service calls, learning the natural variability of human speech in phone conversations. During deployment, the model flagged calls where prosody was statistically unusual — too regular, too predictable, or inconsistent with emotional content. The false positive rate was higher than spectral detection — 4.2% — because human speakers also exhibit unusual prosody in certain contexts, such as reading scripted information or speaking in a second language. But the model caught synthetic speech that evaded spectral detection, particularly newer models with improved vocoder quality.

Temporal artifacts focus on phoneme transitions and speech timing. Human speech production involves physical constraints — the tongue, lips, and vocal cords can only move so fast. Phoneme transitions take time and exhibit coarticulation effects where adjacent sounds influence each other. Synthetic speech can violate these constraints, producing transitions that are too fast or lack natural coarticulation. A detection model trained on phoneme-level features can identify these violations.

A telecommunications company implemented temporal detection using a transformer-based model that analyzed phoneme sequences and transition timings. The model was trained on phonetically annotated speech data, learning the expected duration and acoustic properties of phoneme transitions in natural speech. During deployment, the model flagged speech where transitions violated natural constraints — phonemes that appeared too quickly, transitions that lacked expected coarticulation effects, or timing patterns inconsistent with human speech production. The detection accuracy was 89%, lower than spectral methods, but the model caught specific synthesis artifacts that other methods missed.

## Real-Time vs Post-Call Detection

Synthetic voice detection can operate in real time during a call or post-call during fraud investigation. Real-time detection enables immediate intervention — flagging the call for agent review, triggering additional authentication steps, or terminating the call if fraud confidence is high. Post-call detection enables forensic analysis — identifying patterns across multiple calls, attributing fraud campaigns, and gathering evidence for law enforcement. Both approaches are necessary. Real-time detection prevents fraud. Post-call detection informs defense improvements.

Real-time detection operates under strict latency constraints. The detection model must analyze speech and return a classification within milliseconds, fast enough that the result is available before the conversation proceeds. A customer service platform implemented real-time detection with a latency budget of three hundred milliseconds per audio chunk. Audio was streamed to the detection service in one-second chunks. Each chunk was analyzed for synthetic artifacts using a lightweight convolutional model optimized for inference speed. If two consecutive chunks were flagged as synthetic, the system alerted the agent with an on-screen warning: "Possible synthetic caller — verify identity using additional authentication." The agent could then ask clarifying questions, request additional verification, or escalate to a supervisor.

The challenge with real-time detection is balancing accuracy and latency. Complex models that analyze full conversational context achieve higher accuracy but require more computation and introduce more latency. Lightweight models that analyze short audio chunks run faster but have lower accuracy. A financial institution tested three real-time detection architectures: a lightweight CNN analyzing one-second chunks with two hundred millisecond latency and 87% accuracy, a medium-weight RNN analyzing five-second windows with six hundred millisecond latency and 92% accuracy, and a heavyweight transformer analyzing fifteen-second windows with one point eight second latency and 95% accuracy. They deployed the medium-weight model, accepting slightly lower accuracy than the heavyweight model to avoid the user experience impact of two-second detection delays.

Post-call detection operates without latency constraints. The full call audio is analyzed after the call completes, using the most accurate models available. A retail company implemented post-call detection using an ensemble of four models: spectral artifact detection, prosodic analysis, temporal analysis, and conversational coherence analysis. Each model analyzed the full call. Results were combined using a weighted voting scheme. Calls flagged by three or more models were marked as high-confidence synthetic. Calls flagged by two models were marked as medium-confidence. Calls flagged by one model were marked as low-confidence. High-confidence synthetic calls triggered automatic fraud investigation. Medium-confidence calls were reviewed by a fraud analyst. Low-confidence calls were logged but not investigated unless other fraud signals appeared.

Post-call detection also enables pattern analysis across calls. A telecommunications company used post-call detection to identify fraud campaigns by clustering synthetic calls based on voice characteristics, conversation patterns, and behavioral signals. One cluster of forty-seven calls over three weeks all used the same synthetic voice model, targeted the same account type, and followed nearly identical conversation scripts. The cluster analysis enabled the company to attribute the calls to a single fraud operation, identify the accounts targeted, and implement targeted defenses before the campaign expanded.

## False Positive Management: Legitimate Voice Cloning Uses

Not every synthetic voice is fraudulent. Legitimate use cases for synthetic and cloned voices exist, and detection systems must account for them. Users with speech disabilities may use text-to-speech tools to communicate. Users who have lost their voice due to medical conditions may use voice banking and cloning services to restore a synthetic version of their original voice. Users in accessibility scenarios may employ voice agents to make calls on their behalf. Flagging these legitimate uses as fraud creates harm.

A healthcare company faced this issue in 2025 when their synthetic voice detection system flagged calls from patients using assistive speech technology. The patients had degenerative neurological conditions that impaired speech, and they used tablet-based text-to-speech apps to communicate with customer service. The detection system correctly identified the speech as synthetic and flagged the calls for additional authentication. The additional authentication process was inaccessible to some patients, who could not complete voice-based verification because they could not produce natural speech. The company had deployed synthetic voice detection without considering accessibility implications. The fix required implementing an alternative authentication path for users who self-identified as using assistive technology, allowing them to authenticate using knowledge-based questions or caregiver verification instead of voice biometrics.

The challenge is distinguishing legitimate assistive technology use from fraud. Both produce synthetic speech. The difference is context and intent. A fraud caller using synthetic voice typically exhibits other suspicious signals — unusual account activity, high-value transactions, requests that deviate from normal behavior. A legitimate assistive technology user exhibits normal account behavior, consistent interaction patterns, and willingness to use alternative authentication when voice fails.

A financial institution addressed this by implementing fraud scoring that combined synthetic voice detection with behavioral signals. Synthetic voice alone did not trigger fraud classification. Synthetic voice plus high-risk transaction type plus unusual account activity triggered fraud classification. Synthetic voice plus normal transaction type plus consistent account history triggered an accessibility flag, and the user was offered alternative authentication without fraud investigation. The model reduced false positives on legitimate assistive technology users by 91% while maintaining fraud detection accuracy above 94%.

User disclosure is another approach. Some platforms ask users to declare if they are using synthetic voice or assistive technology at the start of a call. A telecommunications company implemented this with an IVR prompt: "If you are using a speech assistance device or text-to-speech tool, press one. Otherwise, please state your request." Users who disclosed assistive technology use were routed to an accessibility-optimized authentication flow. Users who did not disclose and were later detected using synthetic voice were flagged for fraud review. The disclosure approach reduced false positives, but it relied on user honesty — a fraudulent caller could simply declare assistive technology use to bypass detection. The company mitigated this by applying additional fraud signals to users who declared assistive technology, including transaction limits and heightened monitoring.

## Integrating Synthetic Detection into Call Flows

Synthetic voice detection must be integrated into call handling workflows, not bolted on as an afterthought. The detection result must inform authentication decisions, agent behavior, and fraud investigation processes. Integration requires careful design of escalation paths, agent tooling, and user communication.

Agent alerting is the primary integration point for real-time detection. When synthetic voice is detected during a call, the agent must be notified in a way that does not disrupt the conversation or alert the caller. A retail company implemented this using a visual alert in the agent desktop interface. A yellow banner appeared at the top of the screen with the message "Synthetic voice detected — verify identity." The agent could acknowledge the alert with a single click, which logged acknowledgment in the call record. The alert did not interrupt the call audio or generate an audible notification that the caller might hear. The agent was trained to respond to the alert by asking additional verification questions — "Can you confirm the email address on file?" or "What was the last transaction on your account?" — without explicitly stating that synthetic voice had been detected.

Escalation paths define what happens when synthetic voice is detected and additional verification fails. A financial services company implemented a three-tier escalation policy. Tier one: synthetic voice detected, but the caller successfully answers additional verification questions — call proceeds normally, but the interaction is logged for post-call fraud review. Tier two: synthetic voice detected, and the caller fails one verification question — agent transfers the call to a specialized fraud verification team trained in handling potential synthetic voice fraud. Tier three: synthetic voice detected, and the caller fails multiple verification questions or refuses verification — call is terminated, account is flagged for fraud investigation, and the customer is contacted through a verified channel to confirm whether they initiated the call.

Post-call workflows handle synthetic voice detections that occur during post-call analysis. A telecommunications company implemented a workflow where high-confidence synthetic voice detections triggered automatic fraud case creation. The case included the call recording, detection model outputs, transaction details, and account history. A fraud analyst reviewed the case within twenty-four hours. If fraud was confirmed, the account was locked and the customer was contacted. If fraud was not confirmed — for example, the caller was a legitimate user employing assistive technology — the account was annotated to prevent future false positives on that user.

User communication must be handled carefully when synthetic voice is detected and fraud is suspected. You cannot tell a fraudulent caller "we detected synthetic voice" without revealing your detection capabilities and enabling them to evade detection in future attacks. A retail bank addressed this by using neutral language during call termination: "We are unable to verify your identity at this time. For your security, please visit a branch with government-issued identification to complete this transaction." The message did not reveal that synthetic voice was the reason for rejection. It provided a plausible alternative explanation — identity verification failure — that did not educate the attacker.

## Synthetic Detection as Adaptive Defense

Synthetic voice technology improves continuously. The models that produce detectable artifacts today will be replaced by models with fewer artifacts tomorrow. Detection systems must evolve at the same pace. This requires continuous model retraining on adversarial examples, monitoring for detection degradation, and rapid deployment of updated models when new synthesis techniques emerge.

A financial institution implemented a detection model retraining pipeline that ran monthly. The pipeline collected audio samples flagged as synthetic during the previous month, obtained ground truth labels through fraud investigation outcomes, and retrained the detection model on the updated dataset. The retrained model was validated on a held-out test set, deployed to a canary environment for two weeks of production testing, then rolled out globally if performance metrics were stable. The pipeline ensured that detection models adapted to evolving synthesis techniques without manual intervention.

Detection degradation monitoring is critical. If your detection model was trained on synthesis artifacts from 2025 models, it may fail on synthesis artifacts from 2026 models. A telecommunications company implemented monitoring that tracked detection model confidence distributions over time. When the average confidence score on flagged samples dropped below a threshold — indicating that the model was less certain in its classifications — the monitoring system alerted the machine learning team to investigate whether new synthesis techniques were evading detection. In early 2026, this alert caught a degradation event where detection accuracy on a new voice cloning model dropped from 92% to 78% within two weeks. The team retrained the model on adversarial samples from the new cloning model and restored detection accuracy to 94% within ten days.

The arms race between synthesis and detection is permanent. There is no final solution. The best you can achieve is continuous adaptation — monitoring for new threats, collecting adversarial examples, retraining models, deploying updates, and monitoring for the next threat. The operational infrastructure to support this cycle is as important as the detection models themselves.

Synthetic voice detection is one dimension of biometric security. Consent frameworks are another. We turn there next.
