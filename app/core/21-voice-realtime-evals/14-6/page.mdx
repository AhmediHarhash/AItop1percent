# 14.6 — Memory Drift: When Context Degrades Across Turns

Your support system handles complex technical troubleshooting conversations that run twenty to thirty turns. The model maintains context beautifully through turn fifteen. It remembers that the user is running version 4.2 of the software, that they already tried restarting, that their system is a Mac with 16GB of RAM, and that the error message mentions a network timeout. Then on turn sixteen, the user says "and I'm still seeing that timeout error" and the model responds "what error are you seeing?" The context has degraded. The model no longer has access to the error details mentioned in turn four. Memory drift has occurred, and the conversation loses coherence.

Memory drift is the gradual loss of contextual information across conversation turns. It is different from state drift. State drift is when stored facts become incorrect. Memory drift is when stored facts disappear. The system once knew the user's appointment preference was mornings, but that information is no longer accessible in context. The system once knew the user mentioned a family member in turn six, but when the user refers to "my daughter" in turn eighteen, the system has no idea who that is.

## How Memory Drifts: Context Window Limits

Memory drift in modern voice systems is primarily caused by context window exhaustion. Every conversation turn consumes tokens. User speech is transcribed to text, system responses add more text, internal reasoning traces add more tokens, and structured state adds metadata. A twenty-turn conversation can easily consume 8,000 to 12,000 tokens depending on verbosity and system architecture.

Most production voice systems in 2026 run on models with context windows ranging from 32,000 to 200,000 tokens. That sounds like plenty. But context windows are not infinite, and long conversations eventually exceed capacity. When the window fills, the system must choose what to discard. The naive approach is first-in-first-out: drop the oldest turns and keep the most recent. This preserves short-term context but destroys long-term memory. The user mentioned their medication allergy in turn two. By turn twenty, that information is gone, and the system suggests the medication they explicitly said they cannot take.

Even with large context windows, memory drift occurs due to attention dilution. Models with 128,000-token windows can technically hold hundreds of conversation turns, but performance degrades as context length increases. The model's ability to retrieve specific facts from early in the conversation weakens. The information is still in the window, but the model cannot reliably access it. This is the "needle in a haystack" problem. The user's email address is in turn three. In turn twenty-five, when the system needs to confirm the email, it might hallucinate a similar-sounding address or ask the user to repeat it because retrieving it from turn three is unreliable.

Summarization is a common mitigation strategy, and it introduces a second source of memory drift. Instead of keeping full conversation history, the system periodically summarizes older turns and discards the originals. Turn one through turn ten are summarized into a 200-token summary, freeing up space for new turns. This works until the summary loses critical details. The user mentioned that their preferred appointment time is mornings except Tuesdays. The summary records "prefers morning appointments." The Tuesday exception is lost. Later, when the system offers a Tuesday morning slot, the user is frustrated because they already said Tuesdays do not work.

A third cause is priority decay. The system uses a weighted retention strategy where high-importance information is kept longer and low-importance information is discarded sooner. Importance is determined by heuristics: entity names are high-importance, conversational filler is low-importance. But heuristics miss context-dependent importance. "I have a flight at noon" seems low-importance when mentioned casually in turn seven. By turn twenty, when the user is trying to schedule something before their flight, that detail is critical, but it has already been discarded as unimportant small talk.

## Detecting Memory Drift in Production

Memory drift is harder to detect than state drift because it manifests as absence, not error. The system does not store incorrect information; it simply lacks information it once had. Detection requires comparing current conversational behavior against expected behavior if full context were available.

The most reliable signal is reference resolution failure. The user says "can you send that to my daughter?" and the system responds "who should I send it to?" or "I don't have information about your daughter." If the user mentioned their daughter in earlier turns, this is a memory drift event. The system lost the referent. Tracking pronoun and reference resolution failures across turns reveals drift patterns. A conversation with zero reference failures in the first ten turns and three failures in turns fifteen through twenty indicates memory degradation is happening around turn twelve.

Another signal is repeated questions. The user says their email is john at example dot com in turn three. The system asks for their email again in turn eighteen. If the email was stored in structured state, this is a state management bug. If the email was mentioned conversationally but not extracted to state, this is memory drift. The system once had access to that information in context but no longer does. Logging all questions asked and comparing against information previously provided identifies repeat-question drift.

Self-contradiction is a third signal. The system says "I'll send this to your work email" in turn eight based on context from turn three. In turn twenty, the system says "I'll need your email address to send this." The system contradicted its own earlier statement because it no longer has access to the turn three context where the user provided their email. Detecting these contradictions requires the system to maintain a belief log — a record of what the system has claimed to know at each turn. When a new claim contradicts a previous claim, investigate whether memory drift caused the contradiction.

User frustration signals are also diagnostic. When users say "I already told you that" or "we just talked about this" or "I mentioned that earlier," they are signaling that the system is asking for information it should already have. Logging these phrases and correlating them with turn number reveals when memory drift becomes noticeable to users. If frustration signals spike after turn fifteen, the system is losing critical context around turn twelve to fourteen.

Quantitative detection requires context retention metrics. At each turn, the system evaluates whether it can still access key facts from earlier turns. If the user mentioned their name in turn one, test at turn ten whether the system can retrieve the name from context. If retrieval fails, memory drift has occurred. This can be automated with synthetic probes: inject a known fact at turn five and test retrieval at turns ten, fifteen, and twenty. If retrieval accuracy drops below 90 percent, memory drift is affecting usability.

## Preventing Memory Drift: Structured Context Retention

The best prevention is extracting critical information from conversational context and storing it in structured state. If the user mentions their email in turn three, extract it immediately and store it in a dedicated state field. Structured state persists across the entire conversation regardless of context window limits. The email is available in turn thirty even if turn three has been summarized or discarded.

Structured extraction requires identifying what counts as critical. Not everything the user says needs to be stored. Conversational filler, acknowledgments, and low-stakes preferences do not need persistence. But identifiers, contact information, constraints, and user-stated requirements must be extracted. The system uses an extraction policy that defines which entity types and fact types are always extracted and which are left in conversational context.

The extraction policy is domain-specific. A medical appointment scheduler extracts patient identifiers, appointment preferences, medication allergies, and insurance information. A technical support bot extracts product version, error messages, steps already attempted, and system specifications. A shopping assistant extracts product preferences, budget constraints, and delivery requirements. The policy is built from failure analysis: every time memory drift causes a user-facing error, ask what information should have been extracted and was not.

Priority-based context retention is the second prevention strategy. Instead of discarding turns purely by age, the system scores each turn by importance and retains high-importance turns longer. Importance is determined by multiple factors: does the turn contain an entity extraction, does it contain a user constraint, did the user explicitly emphasize the information, does it resolve ambiguity from earlier turns. High-scoring turns are kept in full. Low-scoring turns are summarized or discarded.

The priority algorithm is tuned to avoid over-retention. If every turn is marked high-importance, the system effectively retains everything and gains no benefit. The target is that 20 to 30 percent of turns are high-importance, 40 to 50 percent are medium-importance, and 20 to 30 percent are low-importance. Low-importance turns are summarized first. Medium-importance turns are summarized second. High-importance turns are summarized only when the context window is nearly full.

Periodic context refresh is a third prevention strategy. Every five to ten turns, the system regenerates a summary of the conversation so far, prioritizing the most relevant facts. This summary is injected into context, replacing older summaries. The refresh prevents gradual detail loss by re-extracting important facts from full history before that history is discarded. If the user mentioned a Tuesday exception for morning appointments in turn six, the turn ten refresh re-surfaces that detail even if turn six itself is no longer in context.

## Recovering from Memory Drift

Prevention fails. Memory drift will occur in long conversations despite structured extraction and priority retention. Recovery mechanisms minimize user-facing impact when drift happens. The first recovery strategy is ask-and-reextract. When the system detects that it needs information it no longer has, it asks the user to repeat it and immediately extracts the information to structured state this time. "Can you remind me of your email address?" retrieves the information and ensures it will not drift again.

Ask-and-reextract is safe but increases conversation length. Users tolerate one or two instances in a thirty-turn conversation, but if the system is constantly asking users to repeat themselves, the conversation feels broken. The threshold for acceptable reextraction is one instance per twenty turns. More than that, and users perceive the system as forgetful.

The second recovery strategy is inference-based reconstruction. The system uses remaining context and structured state to infer the drifted information. The user says "send it to my daughter" but the daughter's name has drifted out of context. The system checks structured state and finds a contact labeled "daughter — Sarah." The system infers the referent without asking the user. Inference works when structured state contains enough information to resolve the ambiguity. It fails when the state is incomplete or the inference is ambiguous.

The third recovery strategy is graceful degradation. Instead of failing outright when drifted information is needed, the system acknowledges the gap and continues with partial information. "I don't have your daughter's name in front of me — can I send this to you and you can forward it?" The system admits the limitation and offers an alternative path that does not require the drifted information. This is slower than direct retrieval but avoids conversation breakdown.

The fourth recovery strategy is context replay. The system maintains a full conversation log outside the model's context window. When memory drift is detected, the system retrieves specific turns from the log and reinjects them into context temporarily. The user mentions "my daughter" and the system cannot resolve the reference. The system searches the log for turns mentioning "daughter," finds turn six, and reinjects that turn into context. The model now has access to the information and can resolve the reference. After the resolution, the reinjected turn is removed to free context space.

Context replay is expensive. Searching the log, reinjecting turns, and re-running inference adds 200 to 500 milliseconds of latency per recovery event. This is acceptable for occasional drift but not sustainable if drift is frequent. The replay strategy is used as a last resort when ask-and-reextract would frustrate the user and inference-based reconstruction is not confident enough.

## The Long Conversation Problem

Voice systems designed for short interactions — three to five turns — rarely experience memory drift. The entire conversation fits comfortably in context. But as systems handle more complex tasks, conversation length increases. Technical support troubleshooting runs twenty to forty turns. Complex transaction workflows like booking multi-leg travel or configuring a custom product run thirty to sixty turns. Long-form advisory conversations like financial planning or healthcare consultations run beyond one hundred turns.

At scale, memory drift is not an edge case — it is the default outcome. A sixty-turn conversation with an average of 100 tokens per turn consumes 6,000 tokens of conversation history alone. Add system prompts, structured state, reasoning traces, and tool call results, and the context budget is 12,000 to 18,000 tokens. Even with a 128,000-token window, this leaves room for only a few more conversation rounds before drift begins.

The long conversation problem requires architecture changes, not just better prompting. Systems designed for durability use hierarchical memory: short-term context for the last five to ten turns, medium-term structured state for extracted facts, and long-term persistent memory for full conversation logs. The model operates primarily on short-term context and structured state. When it needs information beyond that, it queries long-term memory explicitly.

This architecture mirrors human memory. You remember the last few sentences of a conversation in high detail, you remember key facts from earlier in the conversation as structured knowledge, and you can recall specific earlier moments if asked but do not hold them all in active working memory simultaneously. The voice system does the same. The model does not need turn six in active context if it has already extracted the critical facts from turn six into structured state.

The failure mode of hierarchical memory is incomplete extraction. If turn six mentioned the Tuesday exception but the extraction logic did not flag it as important, the exception is lost when turn six leaves short-term context. Recovery requires either asking the user again or searching long-term memory. Both are slower than having the information in context. Tuning extraction policies to minimize this failure is the central challenge of long conversation systems.

## Measuring Memory Drift Impact

Memory drift is measured by comparing user-facing errors in conversations with full context retention against conversations with realistic context limits. The baseline is a system with infinite context that never drifts. The production system has realistic context limits. The error rate difference is attributable to memory drift.

In production, you cannot run infinite context comparisons live. Instead, you sample conversations, replay them offline with full context, and compare outcomes. If the full-context replay successfully resolves a reference that the production system failed to resolve, that is a memory drift error. Aggregating these errors across hundreds of conversations reveals drift impact. A drift error rate of 2 percent means two out of every hundred conversation turns experience a user-facing failure due to memory drift. A drift error rate below 1 percent is acceptable for most applications. Above 3 percent, users notice and complain.

Another measurement is context retrieval accuracy. At each turn, the system logs whether it successfully retrieved information from earlier turns when needed. If the user says "and I'm still seeing that error" and the system retrieves the error details from turn four, that is a successful retrieval. If the system asks "what error?" that is a failed retrieval. Retrieval accuracy is calculated as successful retrievals divided by retrieval attempts. Target accuracy is above 95 percent.

User recovery actions are also diagnostic. How often do users repeat information they already provided? Transcripts are analyzed for phrases like "like I said," "I already mentioned," and "I told you earlier." High rates of user repetition indicate high memory drift. Users are compensating for the system's forgetfulness by proactively re-stating critical facts.

The memory drift budget is the maximum acceptable drift error rate. It is set based on conversation stakes. High-stakes medical or financial conversations have a budget of 0.5 percent — one drift error per two hundred turns. Medium-stakes support or scheduling conversations have a budget of 2 percent. Low-stakes informational conversations have a budget of 5 percent. The budget is enforced by monitoring drift metrics and triggering architecture changes when the budget is exceeded.

Memory drift is inevitable in long conversations. The system that acknowledges this and builds recovery mechanisms maintains usability. The system that assumes infinite memory fails when reality intrudes. The next subchapter covers state repair — how to detect and fix state corruption when it occurs.
