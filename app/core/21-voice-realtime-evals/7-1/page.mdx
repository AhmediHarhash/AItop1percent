# 7.1 — What Barge-In Actually Means: Technical Definition

The hardest UX problem in voice systems is not recognition accuracy. It is not latency. It is what happens when a user speaks while the system is still speaking. In a human conversation, you interrupt someone when they are taking too long, when they misunderstood your question, or when you realize mid-explanation that you asked the wrong thing. You expect them to stop talking, listen to you, and respond to your interruption. In a voice system, this is called barge-in. Most systems get it wrong. The user speaks, the system keeps talking, and the conversation collapses into a frustrating game of "wait your turn" with a machine that does not understand turn-taking.

Barge-in is the technical capability that allows a user to interrupt the system while it is speaking, causing the system to immediately stop its output, capture the user's new speech, process it as a continuation or correction of the conversation, and respond appropriately. When barge-in works, the conversation feels natural. When it fails, every interruption becomes a fight. The user says "no, not that date" while the system is still listing options, and the system either ignores the interruption entirely or hears a garbled mix of its own speech and the user's voice, producing nonsense transcription and a confused response.

The stakes are immediate and unforgiving. A customer service voice bot that cannot handle barge-in forces users to wait through entire menu options even when they already know what they want. A voice assistant that cannot detect interruptions will finish reading a long weather forecast even after the user says "stop." A medical intake system that ignores mid-response corrections will record the wrong medication, the wrong dosage, or the wrong allergy. Barge-in is not a nice-to-have feature. It is the difference between a voice system that feels responsive and one that feels like talking to an automated phone tree from 1998.

## What Barge-In Is Not

Barge-in is not the same as wake-word detection. A wake word activates a dormant system. Barge-in interrupts an active one. The technical requirements are different. Wake-word detection happens when the system is silent and listening. Barge-in happens when the system is speaking, playing audio, and must simultaneously monitor for user speech. The acoustic environment is fundamentally different. The system's own output is the loudest signal in the audio stream. The user's voice is competing with it. Detecting speech in that environment requires echo cancellation, voice activity detection tuned to ignore the system's own voice, and extremely low-latency processing to stop playback before the user perceives lag.

Barge-in is not the same as end-of-turn detection. End-of-turn detection determines when the user has finished speaking so the system can begin its response. Barge-in determines when the user has started speaking while the system is already responding. Both involve voice activity detection. Both involve latency constraints. But barge-in has an additional challenge: the system must stop its own output, which may involve halting text-to-speech mid-sentence, stopping audio playback, and clearing buffers that were pre-rendered for low-latency delivery. End-of-turn detection can afford a few hundred milliseconds of delay before the system starts speaking. Barge-in must act within 100 milliseconds or the user perceives the system as unresponsive.

Barge-in is not the same as cancellation. Some systems allow users to say "stop" or "cancel" to halt the current output and return to a ready state. That is a command, not an interruption. The user speaks a specific keyword, the system recognizes it, and the conversation resets. Barge-in handles the case where the user does not say "stop" but instead says something substantive — a correction, a clarification, or an entirely new request. The system must not only stop speaking but also understand that the user's new input is part of the ongoing conversation. A user who says "no, not Friday, Thursday" while the system is reading a calendar entry is not trying to cancel. They are trying to correct. The system must recognize the interruption, process the correction, and adjust its response accordingly.

## The Three Components of Barge-In

Barge-in requires three things to happen in sequence, each within strict latency bounds. First, the system must detect that the user has started speaking. This requires voice activity detection that can distinguish user speech from the system's own audio output, from background noise, and from acoustic artifacts like echo or feedback. The VAD must run continuously while the system is speaking, processing incoming audio in real time, and triggering when it detects a speech signal that is not the system's own voice. This must happen within 50 to 100 milliseconds of the user's first phoneme. Anything slower and the user perceives the system as ignoring them.

Second, the system must stop its own speech output immediately. This is harder than it sounds. Text-to-speech pipelines are often buffered. The system may have pre-rendered the next three seconds of audio to ensure smooth playback. When barge-in is detected, those buffers must be flushed, playback must halt, and any ongoing synthesis must terminate. The speaker must stop emitting sound. If the system is using a hardware audio pipeline with its own buffering, that buffer must also be cleared. The goal is for the user to perceive an immediate stop. If the system keeps talking for another 200 milliseconds after the user starts speaking, the user experiences frustration. They spoke, and the system kept going. The illusion of conversation is broken.

Third, the system must capture and process the user's new input without interference from its own recently-halted speech. This is where echo cancellation becomes critical. Even after the system stops playing audio, there may be residual sound in the environment — the tail end of the last word, reflections off walls, or audio still traveling through the air from speaker to microphone. The system's ASR must filter out those artifacts and focus on the user's voice. If the ASR captures a mix of the system's last syllable and the user's first word, the transcription will be garbled. The user says "no, Thursday" but the system hears "row Thursday" or "low Thursday" or some other nonsense formed by blending the tail of "tomorrow" with the user's correction. The result is a misunderstanding, a request for clarification, and a conversation that grinds to a halt.

## Why Barge-In Fails in Production

Most barge-in failures happen in one of three places. First, the VAD is too conservative. It requires a strong, sustained speech signal before triggering. This reduces false positives — the system does not stop for coughs, background voices, or door slams — but it also means the user must speak loudly and clearly for 200 or 300 milliseconds before the system reacts. By that time, the system has spoken another half-sentence. The user perceives the system as slow to respond. They raise their voice, interrupt more forcefully, and the interaction becomes adversarial. The system is not broken, but the experience is bad enough that users stop trying to interrupt and instead wait for the system to finish, even when they know what they want to say.

Second, the TTS pipeline is too slow to stop. The system detects barge-in correctly, sends a stop command, and then waits 150 or 200 milliseconds for the audio to actually halt. The delay may be in the TTS service, in the audio framework, or in the hardware buffer. From the user's perspective, the system ignored their interruption. They spoke, the system kept talking, and now they have to decide whether to speak again or wait. This creates a hesitation loop. The user interrupts, the system eventually stops, the user starts to speak again, and then the system starts speaking again because it processed the user's first interruption as a complete turn. The conversation becomes a mess of overlapping speech and confused turn-taking.

Third, echo cancellation fails. The system stops speaking quickly, but the ASR picks up residual audio and produces a bad transcription. The user says "no, the other one" but the system hears "know the other one" or "dough the other one" because the final syllable of the system's last word bled into the transcription. The system now has to recover from a mis-hearing. It asks for clarification. The user repeats themselves, more slowly and more loudly. The system finally gets it right. But the damage is done. The user has learned that interruptions do not work reliably, and they adjust their behavior to avoid them. They wait for the system to finish, even when waiting wastes time. The barge-in feature exists, but users stop using it.

## The Latency Budget for Barge-In

The total latency budget for barge-in is approximately 100 milliseconds from the moment the user starts speaking to the moment the system stops emitting sound. This budget is divided across three components. Voice activity detection typically consumes 30 to 50 milliseconds. The VAD processes audio in short windows — often 10 or 20 milliseconds per frame — and must accumulate enough evidence to distinguish speech from noise. A single frame is not enough. Two or three frames are required for confidence. This means the VAD introduces an inherent delay of 20 to 50 milliseconds before it can reliably trigger.

Stopping the TTS pipeline consumes another 20 to 50 milliseconds. The system must send a stop command to the TTS service, flush any buffered audio, halt playback, and ensure the speaker goes silent. If the TTS is cloud-based, this includes network latency. If the TTS is local, it includes the time required to interrupt the synthesis thread and clear the audio buffer. If the audio pipeline has hardware buffering — common in mobile devices and embedded systems — that buffer must also be flushed. The goal is to minimize the duration of continued speech after barge-in detection. Every additional 50 milliseconds of speech after the user starts talking doubles the perceived unresponsiveness.

Echo cancellation and ASR startup consume the remaining 20 to 30 milliseconds. The system must switch from playback mode to listening mode, activate echo cancellation if it was not already running, and begin capturing the user's speech. The ASR must be ready to process the incoming audio immediately, without a cold-start delay. If the ASR is triggered on-demand and requires 100 or 200 milliseconds to initialize, the user's first word will be lost. The system will hear "Thursday" but miss "no, not Friday" because the ASR was not ready. The result is a partial transcription that loses critical context.

## Measuring Barge-In Latency in Evaluation

You measure barge-in latency by simulating a conversation where the system is speaking and the user interrupts. Play a pre-recorded system utterance through the speaker. At a precise timestamp — say, 1.5 seconds into the utterance — inject a user speech signal through the microphone. Measure the time from the start of the user's speech to the moment the system's audio output stops. This is your barge-in stop latency. It should be less than 100 milliseconds for acceptable UX, less than 75 milliseconds for good UX, and less than 50 milliseconds for excellent UX.

Repeat this test across different conditions. Interrupt early in the system's utterance, in the middle, and near the end. Interrupt during a word, during a pause, and during a breath. Interrupt with loud speech, quiet speech, and speech at normal volume. Interrupt in silence, with background noise, and with overlapping voices. Measure the latency in each condition. The median latency tells you the typical performance. The 95th percentile tells you the worst-case performance that most users will experience. If the 95th percentile is above 150 milliseconds, your barge-in is too slow for production.

Also measure false negatives. These are cases where the user speaks but the system does not detect barge-in and continues talking. Count how many simulated interruptions the system misses entirely. A false negative rate above 5 percent means one in twenty interruptions fails. Users will notice. They will stop trusting barge-in and will wait for the system to finish before speaking. This defeats the purpose of having barge-in at all.

## The Relationship Between Barge-In and Echo Cancellation

Barge-in and echo cancellation are tightly coupled. Echo cancellation removes the system's own voice from the incoming audio stream so the ASR hears only the user. Without echo cancellation, the system hears itself speaking and may misinterpret its own words as user input. With weak echo cancellation, the system hears a blend of its own voice and the user's voice, producing transcriptions that are half system output and half user input. The result is nonsense.

Acoustic echo cancellation works by maintaining a model of the audio signal being played through the speaker, predicting what that signal will sound like when it reaches the microphone after traveling through the air and reflecting off surfaces, and subtracting the predicted signal from the incoming audio. This prediction is never perfect. Room acoustics vary. The user may move. The speaker and microphone are not in a controlled environment. The AEC must adapt continuously, updating its model based on the observed relationship between playback and capture.

When barge-in is triggered, the AEC faces a sudden change in operating conditions. The system stops playing audio, so there is no longer a playback signal to model. The AEC must switch from canceling the system's voice to passing through the user's voice with minimal distortion. If the AEC takes too long to adapt, the user's first phoneme may be suppressed or distorted because the AEC is still trying to cancel a signal that no longer exists. This creates a characteristic failure mode: the user interrupts, the system stops, and the transcription is missing the first word or the first syllable. The user says "no, Thursday" and the system hears "Thursday." The negation is lost. The correction becomes a confirmation. The conversation derails.

## Why This Is the Hardest UX Problem

Barge-in is the hardest UX problem in voice systems because it requires millisecond-level coordination across multiple subsystems — VAD, TTS, AEC, ASR — none of which were originally designed to work together in real time. Each subsystem has its own latency profile, its own failure modes, and its own tuning parameters. VAD accuracy trades off against latency. TTS quality trades off against stop responsiveness. AEC effectiveness trades off against audio fidelity. ASR accuracy trades off against processing speed. You cannot optimize each component independently and expect them to work together. You must tune the entire pipeline as a system, accepting compromises in each component to achieve acceptable end-to-end performance.

The user does not care about these trade-offs. They care about one thing: when they interrupt, does the system stop and listen, or does it keep talking? If it keeps talking, the system is broken. If it stops but mishears them, the system is broken. If it stops and hears them correctly but takes 300 milliseconds to resume the conversation, the system feels sluggish. The acceptable range is narrow. The technical complexity is high. And the failure modes are immediately obvious to anyone using the system.

This is why barge-in is the defining feature of a high-quality voice UX. It is the feature that separates voice systems that feel like conversation from voice systems that feel like filling out a form by voice. It is also the feature that most teams underestimate, underfund, and discover too late in development. By the time barge-in quality becomes a priority, the architecture is already set. The VAD is chosen. The TTS is integrated. The AEC is tuned. Retrofitting good barge-in requires reworking the entire audio pipeline. Teams that plan for barge-in from day one build it into the architecture. Teams that treat it as a late-stage feature spend months debugging echo, latency, and false positives, often shipping with barge-in disabled because it works poorly and frustrates users more than having no barge-in at all.

The next subchapter covers barge-in detection latency — the race to stop speaking before the user perceives the system as unresponsive.
