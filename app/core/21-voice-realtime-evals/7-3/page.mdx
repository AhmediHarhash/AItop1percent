# 7.3 — False Positive Barge-Ins: When Noise Triggers Interruption

A cough stops the voice assistant mid-sentence. A door slams in the next room and the system halts its response, asking "I'm sorry, did you say something?" A coworker walks by talking on their phone, and the medical intake bot stops reading the prescription list, waiting for input that never comes. These are false positive barge-ins. The system detected speech or speech-like sound, interpreted it as an interruption, and stopped speaking. The user did not interrupt. The user is now staring at a silent system, confused about whether they should repeat something they never said or tell the system to continue. False positives destroy trust faster than high latency. A system that stops too slowly is frustrating. A system that stops for no reason is broken.

## What Counts as a False Positive

A false positive barge-in is any instance where the system stops speaking in response to a non-speech sound or to speech that was not directed at the system. The most common false positive triggers are coughs, sneezes, throat clearing, and other human vocal sounds that are not speech. These sounds have similar acoustic properties to speech — they involve vocal cord vibration, they have energy in the speech frequency range, and they often have abrupt onsets that resemble the start of a word. A simple energy-based voice activity detector cannot distinguish a cough from the first syllable of a word. It hears energy, it triggers, and the system stops.

Background conversations are the second most common trigger. The user is talking to the voice system in a shared space — a home, an office, a car with multiple passengers. Someone nearby starts talking. The microphone picks up their voice. The VAD detects speech. The system assumes the user is interrupting and stops. The user did not speak. The system is now waiting for input. The user must either tell the system to continue or restart the interaction. If this happens repeatedly, the user learns that the voice system does not work reliably in shared spaces and stops using it.

Environmental sounds that are acoustically similar to speech also cause false positives. Door slams, dishes clattering, a television in the background, a dog barking, a phone ringing. None of these are speech, but all of them have acoustic properties that can fool a poorly tuned VAD. A door slam has a sharp transient that can look like a plosive consonant. A dog bark has harmonic structure that resembles a vowel sound. A television playing dialogue in the background produces continuous speech energy that the VAD may interpret as the user speaking. The system stops, the user is confused, and the interaction stalls.

## The Cost of False Positives

The immediate cost of a false positive is a broken conversation. The system stops mid-sentence. The user must recover. If the system provides a clear recovery path — "I didn't hear you, let me continue" — the user can resume the conversation with one additional turn. If the system does not provide a recovery path, the user must figure out what happened and how to fix it. Do they say "continue"? Do they repeat the last thing they said, even though they said nothing? Do they start the entire interaction over? Uncertainty creates frustration. The user spends cognitive effort on interface recovery instead of on the task they were trying to accomplish.

The second cost is that false positives train users not to trust barge-in. After two or three false stops, the user learns that the system is sensitive to noise and will stop unpredictably. They adjust their behavior. They move to a quieter room. They ask others to be silent while they interact with the system. They avoid using the voice interface in environments with background noise. They stop trying to interrupt the system and instead wait for it to finish, even when they know what they want to say. The barge-in feature still exists, but users no longer use it. From a usage metrics perspective, barge-in rate drops. From a UX perspective, the system feels less responsive.

The third cost is abandonment. In high-frustration scenarios — a user is in a hurry, the task is important, or the user has already experienced multiple false positives in the same session — a false positive can trigger abandonment. The user gives up on the voice interface and switches to another modality. They type instead of speaking. They call a human instead of using the voice bot. They close the app and do the task later on a desktop. Abandonment rate correlates with false positive rate. A 2 percent false positive rate causes minimal abandonment. A 10 percent false positive rate — one false stop every ten turns — causes noticeable abandonment. A 20 percent false positive rate makes the system unusable. The user experiences a false stop every five turns. They stop trusting the system entirely.

## The Tuning Trade-Off: Sensitivity vs Specificity

Voice activity detection for barge-in is a binary classification problem. The VAD must decide whether incoming audio contains user speech directed at the system. The decision threshold determines the trade-off between sensitivity and specificity. A low threshold makes the system sensitive. It detects faint speech, quick interruptions, and users speaking at low volume. But it also triggers on background noise, coughs, and distant conversations. High sensitivity means high recall — you catch most real interruptions — but low precision — you also catch a lot of false ones.

A high threshold makes the system specific. It only triggers on strong, clear speech signals. This reduces false positives from background noise and non-speech sounds. But it also misses real interruptions. A user who speaks quietly, or who interrupts tentatively, or whose voice is partially masked by ambient noise, may not cross the threshold. The system does not detect the interruption. The user perceives the system as unresponsive. High specificity means high precision — when the system stops, it is usually a real interruption — but low recall — it misses many real interruptions.

You cannot optimize both simultaneously with a fixed threshold. The optimal trade-off depends on your use case. A voice assistant used in a quiet home can afford a low threshold. Background noise is rare. False positives are unlikely. Sensitivity is more important than specificity. A customer service bot used in noisy call centers must use a high threshold. Background noise is constant. False positives would make the system unusable. Specificity is more important than sensitivity. The right threshold is the one that minimizes total user frustration, which is a function of both false positive rate and false negative rate, weighted by the cost of each failure mode.

## Acoustic Features That Distinguish Speech from Noise

The simplest VAD uses energy thresholding. If the audio signal exceeds a certain energy level, the VAD triggers. This works in silent environments but fails in noisy ones. A door slam has high energy. A cough has high energy. A dog bark has high energy. Energy alone is not enough. More sophisticated VADs use spectral features — characteristics of the frequency content of the signal. Human speech has a specific spectral envelope. Most energy is concentrated between 100 Hz and 8 kHz, with peaks corresponding to vowel formants. Non-speech sounds have different spectral profiles. A door slam has energy concentrated in low frequencies. A dog bark has a different harmonic structure. A cough has a noisy spectrum with less harmonic content than speech.

Modern VADs use machine learning models trained to distinguish speech from non-speech. These models are trained on labeled datasets containing examples of speech, coughs, background conversations, environmental noise, and other sound types. The model learns to recognize the acoustic patterns that characterize speech and to reject patterns that characterize non-speech. The model outputs a probability that the current audio frame contains speech. This probability is compared against a threshold to make the binary decision. The threshold is tunable. Lowering the threshold increases sensitivity. Raising it increases specificity.

The quality of the VAD depends on the quality of the training data. If the model was trained primarily on clean speech in quiet environments, it will perform poorly in noisy environments. It will misclassify background noise as speech, producing false positives. It will miss speech that is partially masked by noise, producing false negatives. The training data must be diverse. It must include speech at different volumes, with different accents, in different acoustic environments, and with different types of background noise. It must also include non-speech sounds that are acoustically similar to speech — coughs, sneezes, throat clearing, sighs, laughter. The model must learn to reject these as non-speech even though they come from the user's voice.

## Contextual Barge-In Detection

Some systems use context to improve barge-in accuracy. If the system just asked a yes-or-no question, the user's next utterance is likely to be short — "yes," "no," "maybe." If the VAD detects a long continuous speech signal, it is more likely to be a background conversation than an answer to the question. The system can use this context to adjust the barge-in threshold. For expected short answers, use a lower threshold. For expected long answers, use a higher threshold to avoid triggering on background voices.

The system can also use gaze direction if it has access to a camera. If the user is looking at the screen or device when the speech signal is detected, they are likely addressing the system. If they are looking away, the speech may be directed at someone else. This contextual signal can reduce false positives from cross-talk in multi-user environments. A user talking to a coworker while the voice system is speaking is not interrupting the system. The system should not stop. But a user looking at the screen while speaking is likely interrupting. The system should stop and listen.

The system can also use dialogue state. If the system just delivered an answer and is now closing the turn — "Is there anything else I can help you with?" — the user is less likely to interrupt. If the system is in the middle of a long explanation or a list of options, the user is more likely to interrupt to skip ahead or to correct a misunderstanding. The system can adjust the barge-in threshold based on where it is in the conversation. During critical information delivery, lower the threshold to allow easy interruption. During conversational padding, raise the threshold to avoid false positives from ambient noise.

## Measuring False Positive Rate in Evaluation

You measure false positive rate by running the system through a large set of conversations with injected non-speech sounds and counting how many times the system stops incorrectly. The test setup plays a pre-recorded system utterance. During the utterance, inject a non-speech sound — a cough, a door slam, a background voice, a dog bark. Measure whether the system stops speaking. If it stops, that is a false positive. Repeat this across hundreds of test cases with different non-speech sounds at different volumes and different acoustic conditions.

The false positive rate is the number of incorrect stops divided by the total number of injected non-speech sounds. A false positive rate of 2 percent means the system stops incorrectly two times out of every hundred non-speech events. This is acceptable for most use cases. A false positive rate of 5 percent is borderline. Users will notice, but the system may still be usable if the recovery path is clear. A false positive rate of 10 percent or higher makes the system frustrating to use. One in ten ambient sounds causes an incorrect stop. The user experiences repeated interruptions that they did not trigger.

Also measure the false positive rate separately for different sound types. The system may handle coughs well but trigger frequently on background conversations. Or it may handle background conversations well but trigger on door slams. Understanding which sound types cause false positives allows you to tune the VAD more precisely or to add custom rejection logic for specific acoustic patterns. If most false positives come from low-frequency transients like door slams, you can add a high-pass filter to the VAD input. If most false positives come from background conversations, you can add a directionality check using a microphone array to determine whether the speech is coming from the user's location.

## The Relationship Between False Positives and False Negatives

Reducing false positives by raising the barge-in threshold increases false negatives. The system becomes more conservative. It requires stronger evidence before stopping. This reduces incorrect stops but also causes the system to miss real interruptions, especially from users who speak quietly or tentatively. The user interrupts, the system keeps talking, and the user concludes that barge-in does not work. They stop trying. The feature becomes invisible.

The optimal threshold balances these two failure modes. The cost of a false positive is user confusion and an extra turn to recover. The cost of a false negative is user frustration and a perception that the system is unresponsive. In most cases, false negatives are worse than false positives because they cannot be easily recovered. If the system stops incorrectly, the user can tell it to continue. If the system misses an interruption, the user has no immediate feedback that anything went wrong. They think the system did not hear them, or that it is broken, or that barge-in is disabled. They do not try again. They wait for the system to finish. The conversation slows down.

The trade-off depends on the domain. In a medical assistant handling sensitive information, false positives are acceptable if the recovery path is clear. The user says "continue" and the conversation resumes. False negatives are unacceptable because they prevent the user from correcting errors in critical data like medication names or dosages. The system should be tuned for high sensitivity, accepting a higher false positive rate to ensure no real interruption is missed. In a customer service bot operating in noisy environments, false positives are unacceptable because they create a perception of brokenness. The system should be tuned for high specificity, accepting a higher false negative rate to ensure the system only stops when there is clear evidence of user speech.

## Recovery Mechanisms for False Positive Stops

When a false positive occurs, the system must recover gracefully. The simplest recovery mechanism is to wait for user input. The system stops speaking, listens for a few seconds, and if no speech is detected, resumes where it left off. This works if the false positive is rare and the resume logic is reliable. The user experiences a brief pause, the system continues, and the conversation proceeds. But if false positives are frequent, this creates a stop-and-start rhythm that feels broken. The system speaks, pauses, speaks, pauses, and the user cannot predict when the pauses will occur.

A better recovery mechanism is for the system to explicitly acknowledge the stop and offer to continue. "I didn't hear you, let me continue." This makes the failure mode visible and gives the user control. They know the system stopped because it thought they spoke. They know they can let it continue or they can take the opportunity to actually interrupt. This transparency reduces confusion. The user is not left wondering what happened. They have a clear mental model: the system thought I spoke, I didn't, so it will continue.

The best recovery mechanism is to prevent false positives in the first place through better VAD tuning, better acoustic modeling, and better context awareness. A system that rarely stops incorrectly does not need elaborate recovery mechanisms. The rare false positive can be handled with a simple resume. A system that frequently stops incorrectly must invest in recovery UX, which adds conversational overhead and slows down every interaction. Prevention is cheaper and better than recovery.

The next subchapter covers full versus partial interruption handling — the difference between interrupting to correct, to add information, or to take over the conversation entirely.
