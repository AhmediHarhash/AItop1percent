# 12.9 — Scaling Concurrent Voice Connections

A financial services company launched a voice banking assistant in March 2025. The pilot ran with fifty concurrent users. Latency was excellent. Availability was perfect. The team declared success. In June, they expanded to general availability. Within the first week, concurrent connections peaked at 2,800. Latency spiked to seven seconds. Half the calls dropped. The speech-to-text API hit rate limits. The WebSocket gateway crashed under load. The team had optimized for fifty users. They were facing three thousand. The system was not designed to scale, and scaling voice is not the same as scaling HTTP APIs.

Voice systems are expensive to scale. Each connection holds open a persistent WebSocket or telephony session. Each session streams audio data continuously—kilobytes per second, sustained for minutes or hours. Each session consumes speech-to-text quota, language model inference capacity, and memory for conversation state. A single voice interaction can cost ten to fifty times more in infrastructure than a single text interaction. Scaling from one hundred concurrent connections to ten thousand concurrent connections does not mean ten thousand times the cost. It means solving entirely new problems: provider rate limits, network bandwidth, stateful session management, cost explosion.

## Capacity Planning for Voice Workloads

Capacity planning for voice starts with understanding peak concurrency, not average load. An e-commerce voice assistant might average two hundred concurrent sessions during business hours but spike to fifteen hundred during a product launch or a flash sale. You must provision for the peak, not the average. If you size infrastructure for average load, you will fail during peaks. Peak load determines your capacity requirements.

Peak concurrency is not the same as total daily calls. A system that handles ten thousand calls per day does not need capacity for ten thousand concurrent calls. If the average call duration is three minutes, and calls are evenly distributed across a ten-hour business day, peak concurrency is closer to fifty. But calls are never evenly distributed. Traffic spikes in the morning, during lunch, and just before close. You need to measure actual concurrency patterns, not estimate them from daily totals. Measure concurrency in production or in load tests. Do not guess.

Per-connection resource consumption varies by use case. A simple voice command interface—"set a timer for five minutes"—uses minimal resources. The session lasts ten seconds. The speech-to-text quota consumed is trivial. The language model inference is one turn. A complex support call—troubleshooting a technical issue over fifteen minutes—consumes far more. Longer session duration means more audio streamed, more transcription quota used, more language model turns, more memory to maintain conversation state. Capacity planning must account for session duration and complexity, not just connection count.

Speech-to-text provider quotas are often the first bottleneck. Providers like Google Cloud Speech-to-Text, AWS Transcribe, and Azure Speech Services impose rate limits on concurrent streams, requests per minute, and audio minutes processed per month. If your quota is five hundred concurrent streams and you attempt to scale to eight hundred, five hundred sessions succeed and three hundred fail. Quota increases require advance requests to the provider—sometimes weeks in advance. Plan quota needs before launching. Do not assume you can scale on demand.

Language model inference capacity is the second bottleneck. If you use GPT-5 or Claude Opus 4.5 for voice interactions, each concurrent session makes inference requests every few seconds. A thousand concurrent sessions making one inference request every three seconds is three hundred thirty-three requests per second. If your model API has a rate limit of two hundred requests per second, you cannot support a thousand concurrent sessions. You must either increase your rate limit, batch requests, use a faster model with higher throughput, or implement request queuing.

WebSocket or telephony gateway capacity is the third bottleneck. Each voice connection is a persistent, stateful session. A single server can handle hundreds of WebSocket connections, but scaling to tens of thousands requires horizontal scaling and connection load balancing. Twilio, Vonage, and other telephony providers handle this for you if you use their infrastructure. If you run your own WebSocket gateway, you must implement session affinity, connection draining during deployments, and failover when a gateway node goes down.

## Horizontal vs Vertical Scaling for Voice

Voice workloads scale horizontally, not vertically. A single large server with sixty-four cores cannot handle ten thousand concurrent WebSocket connections as efficiently as ten smaller servers with eight cores each. Voice sessions are stateful and memory-bound. Distributing them across multiple nodes reduces contention, improves fault isolation, and allows graceful degradation. If one node fails, you lose ten percent of sessions, not one hundred percent.

Horizontal scaling requires load balancing with session affinity. You cannot route a single voice session's audio packets to different backend nodes on every request. Once a session is established with a specific node, all subsequent packets for that session must route to the same node. This is called session affinity or sticky sessions. Load balancers like NGINX, HAProxy, and cloud-native load balancers support session affinity based on source IP or session cookies. Without session affinity, audio packets arrive out of order or at the wrong node, and the session fails.

Scaling horizontally introduces state management complexity. If conversation state is stored in local memory on each node, you cannot move a session from one node to another without losing state. If a node crashes, all sessions on that node lose their conversation context. Many voice platforms use centralized state storage—Redis, DynamoDB, or another low-latency key-value store—so that conversation state is accessible from any node. This enables session migration and recovery but adds latency and dependency on the state store.

Vertical scaling has limits in voice systems. Adding more CPU and memory to a single server eventually hits diminishing returns. Beyond a certain point, network I/O becomes the bottleneck. A server receiving audio streams from five thousand concurrent connections is saturating its network interface, not its CPU. Vertical scaling cannot fix network saturation. Horizontal scaling distributes network load across multiple interfaces.

Auto-scaling for voice workloads is challenging because sessions are long-lived. You cannot scale up and then immediately scale down. If a traffic spike triggers auto-scaling and five new nodes spin up, those nodes will handle sessions for the next several minutes. If traffic drops immediately after the spike, the new nodes sit idle until their sessions complete. Voice auto-scaling must account for session duration. Scale up aggressively. Scale down conservatively. Do not terminate nodes with active sessions unless you can migrate those sessions.

## Provider Limits and How to Work Around Them

Every speech-to-text provider imposes limits. Google Cloud Speech-to-Text enforces concurrent stream limits, requests per minute limits, and monthly audio processing limits. AWS Transcribe has similar quotas. Azure Speech Services has service-level limits based on your subscription tier. These limits are not suggestions. They are hard caps. Exceeding them results in throttling or outright rejection. You must know your provider's limits and design around them.

Quota increases are available but not instant. Most providers allow you to request quota increases through support tickets. Google Cloud quota requests can take three to five business days. AWS quota increases are often approved within twenty-four hours, but high-volume requests may require justification and business case review. Azure quota increases vary by region and service. Request quota increases weeks before you need them, not hours. Do not assume providers will approve emergency quota increases during a launch.

Multi-provider redundancy mitigates provider limits. If you hit Google Cloud's concurrent stream limit, failover to AWS Transcribe. If AWS throttles you, failover to Azure Speech Services. This requires integration with multiple providers, normalization of their APIs, and failover logic that detects throttling and reroutes traffic. The complexity is significant, but for high-availability voice systems, multi-provider redundancy is the only way to avoid provider-imposed outages.

Some teams use provider-specific optimizations to stay within limits. Google Cloud Speech-to-Text supports enhanced models with higher accuracy but lower concurrency limits. Standard models have higher concurrency limits but lower accuracy. If you are hitting concurrency limits, switching to standard models may allow you to scale further. The trade-off is quality. Measure whether the quality degradation is acceptable for your use case.

Batching and queuing can smooth traffic spikes. If your system receives a sudden spike in voice connections, you can queue incoming sessions and admit them at a controlled rate that stays within provider limits. The trade-off is latency. Users wait in a queue before their session starts. For some use cases—customer support hotlines during high-demand periods—queuing is acceptable. For others—real-time voice assistants—queuing is not. Match your queuing strategy to user expectations.

Provider diversification also spreads risk. Relying on a single speech-to-text provider means that if that provider has an outage, your entire voice system is down. In October 2025, a regional outage of Google Cloud Speech-to-Text took down voice systems across dozens of companies. Teams with multi-provider failover switched to AWS Transcribe and stayed operational. Teams with single-provider dependency were offline for four hours. Redundancy costs more but buys availability.

## Traffic Patterns: Spikes, Sustained Load, Geographic Variation

Voice traffic is not uniform. It spikes during certain hours, drops overnight, surges during events, and varies by geography. A voice assistant for a US-based retailer sees peak traffic from 9 AM to 9 PM Eastern Time. A global voice banking system sees rolling peaks as business hours shift across time zones. A voice support hotline for a SaaS product sees spikes immediately after a major outage or a new feature launch. Understanding your traffic pattern is essential to capacity planning.

Traffic spikes require burst capacity. If your baseline is three hundred concurrent connections and you spike to twelve hundred during a product launch, you need the ability to handle twelve hundred. Cloud infrastructure allows burst scaling, but speech-to-text quotas do not. If your quota is five hundred concurrent streams, you can spin up more servers, but you still cannot transcribe more than five hundred streams. Quota is the ceiling. Server capacity is only relevant if you have quota headroom.

Sustained load is easier to plan for than spikes. If you consistently run at eight hundred concurrent connections during business hours, you can right-size your infrastructure and quotas for eight hundred. Sustained load is predictable. Spikes are not. Many teams provision for twice their sustained peak to handle unexpected spikes. If sustained peak is eight hundred, provision for sixteen hundred. This leaves headroom for growth and anomalies.

Geographic variation affects traffic distribution. A voice system with users in North America, Europe, and Asia sees traffic peaks in each region during their respective business hours. If you deploy voice infrastructure in multiple regions, you can distribute load geographically. US traffic routes to US infrastructure. EU traffic routes to EU infrastructure. This reduces latency and distributes load. But it also requires multi-region deployment, geo-routing, and region-specific quotas.

Event-driven traffic is the hardest to plan for. A voice system for a sports betting app sees massive spikes during major sporting events. A voice assistant for a tax filing service sees spikes in April. A voice support hotline sees spikes during outages. These spikes are predictable by calendar but unpredictable in magnitude. Plan for two to five times your normal peak. Monitor traffic in real time during events. Scale proactively, not reactively.

## Cost Scaling: Linear vs Superlinear

Voice costs scale worse than linearly. Doubling concurrent connections does not double costs. It more than doubles costs. Speech-to-text pricing is typically per audio minute. If average session duration is five minutes and you double concurrent connections, you double audio minutes and roughly double speech-to-text costs. But you also increase infrastructure costs—more servers, more load balancers, more network bandwidth. You may hit higher-tier pricing as you scale. You may need to upgrade to premium support contracts to get the quota increases you need. Costs grow faster than traffic.

Longer sessions cost more than shorter sessions. A three-minute voice interaction consumes three minutes of speech-to-text quota, three minutes of language model inference, and three minutes of server resources. A fifteen-minute interaction consumes five times more. If you scale from one thousand sessions per day to ten thousand sessions per day, but average session duration also increases from three minutes to six minutes, your costs increase by more than ten times. Monitor session duration as you scale. Optimizing for shorter sessions reduces cost more than optimizing for fewer sessions.

Language model inference is the largest cost driver at scale. A voice assistant that makes five language model API calls per session and handles ten thousand sessions per day makes fifty thousand API calls. If you use GPT-5 or Claude Opus 4.5, fifty thousand calls with average prompt sizes of one thousand tokens and average response sizes of two hundred tokens costs several thousand dollars per day. Scaling to one hundred thousand sessions per day costs tens of thousands per day. Model costs dominate at scale.

Cost optimization requires model selection discipline. If GPT-5-mini provides acceptable quality for ninety percent of interactions, route those interactions to GPT-5-mini and reserve GPT-5 or Claude Opus 4.5 for the remaining ten percent that require advanced reasoning. This hybrid approach reduces average cost per interaction. A team that routed simple intents to GPT-5-nano and complex intents to Claude Opus 4.5 reduced inference costs by sixty-two percent while maintaining quality. Model routing is cost engineering.

Provider pricing tiers change as you scale. Many speech-to-text providers offer volume discounts. Google Cloud Speech-to-Text pricing decreases per audio minute as you exceed certain monthly thresholds. AWS Transcribe has similar tiered pricing. But reaching the next tier requires significant volume. A team processing five million audio minutes per month pays less per minute than a team processing five hundred thousand, but they still pay more in total. Monitor your pricing tier. Negotiate custom contracts with providers once you reach enterprise scale.

Infrastructure costs scale with session count and session duration. WebSocket gateways, load balancers, state stores, log aggregation systems—all of these scale with traffic. A system handling one thousand concurrent sessions might run on ten servers. A system handling ten thousand concurrent sessions might run on one hundred servers. Cloud bills scale accordingly. Voice systems are infrastructure-heavy. Budget for it.

## Scaling as an Ongoing Process

Scaling voice is not a one-time project. It is continuous. As your user base grows, as average session duration changes, as new features launch, your capacity requirements shift. Teams that provision once and forget discover too late that they have outgrown their infrastructure. Monitor concurrency continuously. Track cost per session weekly. Revisit capacity plans quarterly.

Load testing must evolve with the system. A load test that simulates one thousand concurrent sessions is useful when your peak is eight hundred. When your peak grows to five thousand, that load test is obsolete. Re-run load tests at higher concurrency levels. Stress-test your system to find the breaking point. Know where the next bottleneck is before you hit it in production.

Quota management is a recurring task. As you grow, request quota increases proactively. Do not wait until you hit the limit. If you are at seventy percent of your speech-to-text quota, request an increase now. Quota requests take days or weeks. By the time you hit one hundred percent, it is too late to request more. Monitor quota usage. Alert at seventy-five percent. Request increases at eighty percent.

Cost optimization is continuous. As you scale, new optimization opportunities emerge. At one thousand sessions per day, model cost is negligible. At one hundred thousand sessions per day, model cost is the largest line item. At that scale, optimizations that save one cent per session save one thousand dollars per day. Invest in cost optimization as you scale. It pays for itself.

Scaling voice is expensive, complex, and operationally demanding. It requires planning, monitoring, multi-provider redundancy, and cost discipline. But it is also the difference between a pilot that works for fifty users and a production system that works for fifty thousand. Plan for scale before you need it. Monitor relentlessly. Optimize continuously. Voice systems that scale are built to scale from day one.

