# 11.7 — Alerting Thresholds for Voice-Specific Metrics

Alert fatigue kills observability. You can build the most sophisticated monitoring dashboard in the world, instrument every component, track every metric, and measure every quality dimension — and if your alerting thresholds are poorly calibrated, no one will respond when something actually breaks. The fifth false positive is ignored. The tenth false positive gets muted. By the twentieth, the entire alert channel is treated as spam, and when the real incident occurs, no one notices until customers are calling support. Threshold design is everything.

Voice systems make threshold design harder than typical backend services. Latency varies by utterance length. Error rates vary by time of day. Confidence scores vary by accent and audio quality. Conversation success rates vary by intent and user segment. A static threshold that works for one segment fails for another. A threshold tuned for peak hours fires constantly during off-peak. A threshold tuned for English-language users misses failures in Spanish-language users. The challenge is not measuring the metrics — the challenge is defining the boundaries between "normal variance" and "actionable problem" in a way that reflects the complexity of voice.

The teams that get this right in 2026 use a combination of static thresholds, dynamic thresholds, segmentation, and alert suppression logic. The teams that get it wrong either drown in noise or miss critical failures.

## Static vs Dynamic Thresholds for Voice Metrics

Static thresholds are simple: latency above 500ms is bad, error rate above 2% is bad, ASR confidence below 0.7 is bad. You set the threshold once, and it applies to every conversation, every hour, every day. Static thresholds work for metrics with stable baselines and narrow distributions. If your p95 latency is consistently 280ms to 320ms, a static threshold of 500ms will never false-positive and will catch real problems. Static thresholds are easy to reason about, easy to communicate to non-technical stakeholders, and easy to encode in alerting rules.

The problem is that most voice metrics do not have stable baselines. Latency varies by model version, by utterance length, by time of day, and by traffic load. A latency spike to 450ms during peak hours might be normal. The same 450ms during off-peak hours might signal degradation. A static threshold of 500ms would miss the off-peak anomaly. A static threshold of 400ms would fire constantly during peak. The threshold that works for one regime fails for the other.

Dynamic thresholds adapt to changing baselines. Instead of comparing current latency to a fixed number, you compare current latency to the rolling 7-day average or the rolling 24-hour p95. If latency is more than 2 standard deviations above the recent baseline, you alert. If it is within 2 standard deviations, you do not. This approach tolerates slow baseline drift — if your latency gradually increases from 300ms to 350ms over two weeks due to traffic growth, the dynamic threshold adjusts and does not fire. But if latency jumps from 300ms to 450ms in 10 minutes, the dynamic threshold catches it immediately.

The trade-off is complexity. Dynamic thresholds require maintaining historical data, computing rolling statistics, and handling edge cases like cold starts and anomaly windows. If your system experiences an anomaly for 6 hours, the rolling 24-hour average includes the anomaly, which raises the baseline and suppresses future alerts. You need anomaly-aware rolling windows — exclude the last 6 hours if they were anomalous, recompute the baseline from clean data. This is harder to build and harder to reason about.

The best practice in 2026 is to use static thresholds for hard constraints — error rate above 10% is always a P1 alert, regardless of baseline — and dynamic thresholds for relative degradations — latency 50% above baseline is a P2 alert. The static thresholds catch catastrophic failures. The dynamic thresholds catch gradual degradations.

## Per-Component vs Per-Conversation Alerting

Voice systems have two layers of metrics: component-level metrics — ASR latency, TTS latency, intent recognition accuracy — and conversation-level metrics — task completion rate, user satisfaction, conversation duration. You can alert on either layer, and the choice changes what you detect and how quickly you respond.

Per-component alerting fires when a single component crosses a threshold. ASR latency above 200ms, TTS error rate above 1%, intent recognition confidence below 0.6. Component-level alerts are fast — you detect the problem in seconds — and specific — you know exactly which component is broken. Component-level alerts are ideal for infrastructure failures and service degradations. If your ASR service is timing out, you want to know immediately, not after enough conversations fail to move the conversation-level success rate.

The downside of per-component alerting is that it does not tell you whether the component failure is affecting users. A 1% TTS error rate sounds bad, but if the errors are concentrated in a low-traffic intent, the user impact might be negligible. A 200ms increase in ASR latency sounds bad, but if the rest of the pipeline is fast enough, the end-to-end latency might still be acceptable. Component-level alerts create noise when the component failure does not cascade to conversation failure.

Per-conversation alerting fires when conversation-level metrics cross a threshold. Success rate below 80%, average conversation duration above 6 minutes, hang-up rate above 15%. Conversation-level alerts are slow — you need enough conversations to compute a statistically significant aggregate metric — but they are outcome-focused. If conversation success rate is fine, you do not care that ASR latency is elevated. The system is working. Conversation-level alerts are ideal for catching problems that affect user experience but do not manifest as component anomalies — conversation flow bugs, prompt regressions, intent classification errors that are technically correct but contextually wrong.

The best practice is to use both. Per-component alerts catch acute infrastructure failures. Per-conversation alerts catch chronic quality degradations. If a component alert fires but conversation metrics remain stable, you investigate but do not page. If a conversation alert fires, you page immediately and trace back to components to find the root cause.

## Time-Window Considerations

Alerting thresholds are meaningless without a time window. Is latency above 500ms for 1 request a problem? Probably not. Is latency above 500ms for 10% of requests over 5 minutes a problem? Yes. The time window defines how many data points you aggregate before deciding whether to alert.

The trade-off is detection speed versus false positive rate. Short time windows detect problems fast but are noisy. If you alert on "ASR error rate above 2% in any 1-minute window," you will fire constantly due to normal variance. If you alert on "ASR error rate above 2% in any 1-hour window," you will catch real problems but miss the first 59 minutes of the incident.

For voice systems, the standard time windows are 1 minute, 5 minutes, and 1 hour. One-minute windows are used for catastrophic failures — error rate above 10%, availability below 90%. If more than 10% of conversations are failing in a 1-minute window, the system is broken and you need to know immediately. Five-minute windows are used for significant degradations — latency above 500ms, ASR confidence below 0.6, intent recognition accuracy below 70%. Five minutes gives you enough data to smooth out variance but is fast enough to catch incidents early. One-hour windows are used for gradual trends — success rate decline, increasing conversation duration, rising hang-up rate. One hour catches problems that build slowly and are invisible in shorter windows.

The other consideration is whether the threshold applies to a point-in-time metric or a rolling aggregate. A point-in-time threshold fires if the current 1-minute error rate is above 5%. A rolling aggregate threshold fires if the 5-minute rolling average error rate is above 5%. Rolling aggregates are smoother and less prone to false positives, but they are slower to detect sharp spikes. Point-in-time thresholds are faster but noisier.

A healthcare voice assistant uses three time windows for latency alerting: if p95 latency in a 1-minute window exceeds 800ms, fire a P1 alert — the system is likely down. If p95 latency in a 5-minute window exceeds 600ms, fire a P2 alert — the system is degraded but functional. If p95 latency in a 1-hour window exceeds 500ms, file a ticket — the system has a slow-moving performance problem. The three thresholds create a gradient of urgency. The shorter the window, the higher the threshold, the more urgent the alert.

## Alert Severity Levels

Not all problems are equally urgent. A 1% drop in success rate is not the same as a 50% error rate. A 100ms latency increase is not the same as total service unavailability. Alerting systems need severity levels that map problem magnitude to response urgency.

The standard severity model has three levels: P1, P2, P3. **P1 alerts** mean the system is broken and users are experiencing severe degradation or total failure. Examples: error rate above 10%, availability below 95%, success rate below 60%, TTS service completely down. P1 alerts page the on-call engineer immediately, trigger incident response, and require mitigation within 15 minutes. If you are woken up at 3am, it should be for a P1.

**P2 alerts** mean the system is degraded but functional. Users are experiencing reduced quality but the service is not down. Examples: error rate between 3% and 10%, latency above 500ms, success rate between 70% and 80%, ASR confidence consistently below 0.7. P2 alerts notify the on-call engineer during business hours, trigger investigation within 1 hour, and require mitigation within 4 hours. You do not page for a P2, but you also do not ignore it.

**P3 alerts** mean the system has a quality trend that will become a problem if left unchecked. Examples: success rate declining 1% per day over 3 days, latency creeping up by 50ms per week, increasing rate of low-confidence conversations. P3 alerts file a ticket, notify the responsible team, and require investigation within 24 hours. They do not disrupt anyone's sleep, but they prevent small problems from becoming big problems.

The mapping from metric to severity is policy, not physics. Different teams set different thresholds based on risk tolerance. A customer service voice agent might tolerate a 75% success rate as P2. A medical triage voice agent might treat anything below 85% as P1. The key is that the severity level must reflect actual business impact, not just metric values. A 5% ASR error rate is a P1 if it is preventing users from authenticating. The same 5% error rate is a P3 if it is only affecting a low-traffic test intent.

## The Threshold Calibration Process

Alerting thresholds are not set once and forgotten. They require continuous calibration based on alert history and incident outcomes. The calibration process has four steps: measure baseline, set initial thresholds, monitor false positives and false negatives, and adjust quarterly.

Measuring baseline means collecting at least two weeks of production data for every metric you plan to alert on. You compute the mean, median, p95, p99, standard deviation, and distribution shape. You look for diurnal patterns, weekly patterns, and segment-specific patterns. The baseline data tells you what "normal" looks like and how much variance to expect.

Setting initial thresholds means choosing values that are far enough from baseline to avoid false positives but close enough to catch real problems. A common heuristic is to set the threshold at mean plus 3 standard deviations for normal distributions, or at the 99th percentile for skewed distributions. If your baseline latency is 300ms plus-or-minus 50ms, your initial threshold might be 450ms. If your baseline error rate is 0.5% with occasional spikes to 2%, your initial threshold might be 3%.

Monitoring outcomes means tracking every alert that fires and labeling it as true positive, false positive, or false negative. A true positive is an alert that led to discovering and fixing a real problem. A false positive is an alert that fired but no problem was found. A false negative is a problem that occurred but no alert fired. You want a true positive rate above 80% — most alerts should be real — and a false negative rate below 5% — you should catch almost all real problems.

Adjusting thresholds means reviewing alert history every quarter and tuning the thresholds based on outcomes. If a threshold is producing 50% false positives, raise the threshold or tighten the time window. If a threshold missed a major incident, lower the threshold or add a secondary threshold with a longer time window. The goal is continuous improvement toward 90% true positive rate and near-zero false negatives.

A fintech company's voice banking system set an initial latency threshold of 500ms based on 2 weeks of baseline data. In the first month, the threshold fired 40 times. Of those 40 alerts, 12 were true positives — real latency degradations that required mitigation. 28 were false positives — latency spikes caused by normal traffic variance. The true positive rate was 30%, far below the 80% target. The team reviewed the 28 false positives and found that 24 occurred during a specific 2-hour window each day when batch processing jobs ran in the same infrastructure. The solution was to add time-based suppression — the latency threshold was raised to 600ms between 2am and 4am. After the adjustment, the false positive rate dropped to 10%, and the true positive rate rose to 85%.

## Time-Based and Segment-Based Threshold Adjustments

Static thresholds assume the system behaves the same way at all times and for all users. This is rarely true. Voice systems have different performance profiles during peak versus off-peak hours, weekdays versus weekends, and for different user segments. A single threshold across all times and all segments either misses problems in high-variance regimes or fires constantly in low-variance regimes.

Time-based threshold adjustments set different thresholds for different times of day or days of week. Latency threshold might be 500ms during off-peak hours but 600ms during peak hours. Error rate threshold might be 2% on weekdays but 3% on weekends when traffic is lower and more variable. The adjustment reflects expected variance and prevents false positives during predictable high-variance periods.

Segment-based threshold adjustments set different thresholds for different user segments, intents, or geographies. Success rate threshold for enterprise customers might be 85%, while success rate threshold for free-tier users might be 75%. ASR confidence threshold for English-language users might be 0.7, while ASR confidence threshold for accented English might be 0.6. The adjustment reflects segment-specific baselines and prevents false positives from segments with inherently higher variance or lower performance.

The risk of too many threshold adjustments is that the alerting logic becomes too complex to reason about. If you have 10 different latency thresholds for 10 different time windows and 5 different user segments, no one can remember what fires when. The best practice is to limit adjustments to the 2 to 3 most impactful dimensions — usually time of day and primary user segment — and use dynamic thresholds for the rest.

## Alerting That Drives Response

Alerting thresholds are useless if no one responds. The threshold design must account for alert fatigue, on-call load, and organizational capacity. An alert that fires 10 times per day will be ignored. An alert that requires 2 hours to investigate will be deferred until the backlog is overwhelming. The alerting system must be designed for human response, not just metric accuracy.

The rule of thumb is that on-call engineers should receive no more than 3 P1 alerts per week and no more than 10 P2 alerts per week. More than that and alert fatigue sets in. If your current thresholds are producing 5 P1 alerts per day, your thresholds are too sensitive. You either raise the thresholds, increase the time windows, or demote some alerts to P2 or P3.

The second rule is that every alert must be actionable. An alert that fires without a clear next step is noise. Every alert should include: which metric crossed which threshold, over what time window, for what segment, and what the recommended first diagnostic step is. "ASR error rate is 5% in the last 5 minutes" is not actionable. "ASR error rate is 5% in the last 5 minutes for mobile users in the US-East region — check ASR service health and recent deployments" is actionable. The alert itself should tell the on-call engineer where to start investigating.

The third rule is that alerting thresholds should be visible and understandable to the entire team, not just the person who set them. Thresholds encoded in opaque monitoring config files are impossible to reason about. The best practice in 2026 is to maintain a threshold registry — a human-readable document that lists every alert, its threshold, its time window, its severity level, its rationale, and its historical true positive rate. The registry is reviewed quarterly, updated when thresholds change, and shared with every new team member during onboarding.

---

Alerting threshold design is the difference between observability that works and observability that is ignored. Static thresholds catch hard failures. Dynamic thresholds catch gradual degradations. Time-based and segment-based adjustments reduce false positives. Severity levels map problem magnitude to response urgency. Calibration turns guesses into data-driven decisions. The next step is deciding which conversations to review in depth — because you cannot listen to every call, but you cannot afford to miss the ones that matter.

