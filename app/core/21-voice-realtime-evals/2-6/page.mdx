# 2.6 — Speech-to-Speech Models: The Emerging Alternative

The traditional voice pipeline is a cascade: ASR converts speech to text, an LLM processes text to text, TTS converts text back to speech. Three models, three network hops, three failure points. In late 2024, OpenAI's GPT-4o voice mode demonstrated a different approach — one model, audio in, audio out, no intermediate text representation. The speech-to-speech architecture promises to collapse the cascade into a single inference call. The question is whether that promise survives contact with production.

Speech-to-speech models are not yet the default. The cascade architecture still dominates production deployments in 2026 because it offers visibility, modularity, and mature tooling. But the speech-to-speech approach is gaining ground in applications where latency is the constraint that matters most — customer support hotlines, voice assistants, real-time translation. Understanding where speech-to-speech fits and where it does not is a decision every voice product team faces this year.

## What Speech-to-Speech Actually Means

A speech-to-speech model processes audio waveforms directly. It does not produce text as an intermediate representation. The model takes audio, processes it through learned representations, and generates audio output. The architecture is end-to-end. You send a recording of "What is my account balance?" and receive a spoken response, no text involved.

This is different from chaining ASR, LLM, and TTS with tight API integrations. That is still a cascade — three models, even if the orchestration is fast. Speech-to-speech is one model. The user's prosody, tone, and speaking style can influence the model's response prosody, tone, and speaking style. The model has access to paralinguistic features — hesitation, emphasis, emotional tone — that are lost when ASR reduces speech to text.

In practice, most speech-to-speech models in 2026 still produce internal text representations during training or as auxiliary outputs. The distinction is whether text is exposed to the orchestration layer. If your system receives audio and returns audio without surfacing intermediate text, it is functionally speech-to-speech from an integration perspective. The architectural benefit is simplicity. One model, one API call, one latency budget to manage.

## The Latency Argument for Speech-to-Speech

The cascade model requires three sequential round-trips. ASR processes the audio, waits for end-of-turn detection, transcribes the speech, and returns text. The LLM receives the text, generates a response, and returns text. TTS receives the response text, synthesizes speech, and returns audio. Each step adds latency. Each step has its own p95 tail latency. The total time-to-first-audio is the sum of all three plus orchestration overhead.

Speech-to-speech collapses this into one round-trip. Audio goes in, audio comes out. In theory, this reduces latency by eliminating two network hops and two model warm-up delays. In practice, the latency benefit in 2026 is smaller than the architectural simplicity suggests. GPT-4o voice mode in production shows time-to-first-audio around 600 to 900 milliseconds under good conditions — faster than a poorly optimized cascade, but not faster than a well-tuned one using Whisper large-v3, GPT-4.5, and a streaming TTS service.

The latency advantage becomes meaningful when the cascade is poorly optimized. If ASR takes 400 milliseconds, the LLM takes 800 milliseconds, and TTS takes 500 milliseconds, the cascade delivers first audio after 1.7 seconds. A speech-to-speech model delivering at 700 milliseconds cuts that in half. But if the cascade is already optimized — Whisper at 180 milliseconds, a fast LLM at 300 milliseconds, streaming TTS starting at 150 milliseconds — the cascade can match or beat speech-to-speech latency, especially when streaming both the LLM response and the TTS audio.

The real latency win for speech-to-speech is not in the best case. It is in the variance reduction. One model means one p95 tail to manage, not three. The cascade's tail latency is the worst tail among three models. If any one model has a bad p95, the whole pipeline suffers. Speech-to-speech has a simpler latency distribution to reason about.

## The Controllability Tradeoff

The cascade architecture gives you intermediate text. You see what the user said. You see what the LLM is about to say. You can log it, filter it, redact PII, enforce content policies, pass it to analytics, and modify the response before it becomes audio. This visibility is not incidental — it is the foundation of most production voice systems' compliance, safety, and debugging workflows.

Speech-to-speech models in 2026 do not give you that text by default. Some expose transcription as an auxiliary output, but it is not guaranteed to match what the model actually heard or responded to. The model's internal representation may diverge from the text transcription. If the user said something the model interpreted as a question but a human would read as a statement, the transcription might show one thing and the model's behavior might reflect another. This divergence creates a trust problem for compliance and safety teams.

When a customer calls a support line and the system responds inappropriately, the compliance team needs to know what the system heard and what it said. In a cascade system, you have text logs. In a speech-to-speech system, you have audio logs. Audio logs are harder to search, harder to redact, harder to analyze at scale. If your regulatory environment requires text transcripts of interactions — financial services, healthcare, legal — speech-to-speech adds a compliance step. You need to run ASR on the audio logs after the fact, which reintroduces the cascade you were trying to avoid.

The loss of intermediate text also limits content filtering. In the cascade model, you can run a toxicity classifier on the LLM's response text before it gets sent to TTS. If the response contains something unacceptable, you block it, log it, and return a fallback. In a speech-to-speech model, the response is already audio by the time you receive it. You can run ASR on the output audio and then filter, but that adds latency and defeats the point of the single-model architecture. Real-time filtering of generated audio requires either trusting the model's safety guardrails entirely or accepting post-hoc filtering with all its delays.

The controllability tradeoff is fundamental. Simpler architecture, less visibility. Faster inference, harder debugging. Speech-to-speech works when you trust the model's safety, when your compliance requirements allow audio-only logs, and when your use case does not require real-time response modification. If any of those conditions is false, the cascade is safer.

## Prosody and Paralinguistic Features

The architectural advantage speech-to-speech models claim is access to prosody. The model hears the user's tone, pace, emphasis, hesitation. A user who says "I need help" calmly and a user who says "I need help" with rising panic are producing the same words but communicating different urgency. ASR reduces both to the same text string. The LLM sees "I need help" and responds the same way to both users.

A speech-to-speech model can, in theory, differentiate. It hears the prosody. It can match the user's tone — calm response to calm user, urgent response to urgent user. It can hear background noise and adjust volume. It can detect hesitation and offer clarification. The cascade model can approximate this by passing acoustic features alongside the text — ASR systems in 2026 can output confidence scores, speech rate, detected emotion — but the LLM is still primarily processing text. Speech-to-speech processes the audio features natively.

In practice, the prosody advantage in 2026 is inconsistent. GPT-4o voice mode sometimes responds with natural prosody that mirrors the user. Sometimes it does not. The model does not reliably detect sarcasm, frustration, or urgency unless the paralinguistic cues are extreme. A user saying "This is great" sarcastically is often interpreted as sincere. A user with a thick accent or background noise loses the prosody signal — the model focuses on word recognition and ignores tone.

The prosody benefit is real but not automatic. It requires the model to be trained on diverse, labeled audio data where prosody is annotated and matters. Most speech-to-speech models in 2026 are trained on datasets optimized for transcription accuracy, not emotional recognition. The models have access to prosodic information but do not always use it effectively. The result is that prosody-aware responses are a capability, not a guarantee.

When prosody matters — therapy bots, customer service, accessibility applications for users with speech impairments — the speech-to-speech architecture has potential. When prosody is noise — transcription bots, technical support systems where the user's tone does not change the response — the cascade is simpler and more predictable.

## Where Speech-to-Speech Works in 2026

Speech-to-speech models are production-ready in 2026 for use cases where latency, naturalness, and simplicity outweigh the need for intermediate visibility. Voice assistants in consumer applications — smart speakers, in-car systems, voice-controlled apps — are adopting speech-to-speech because users expect sub-second response times and do not care about compliance logs. The interactions are low-risk, the content filtering can be model-internal, and the engineering simplicity reduces operational complexity.

Real-time translation is another strong fit. A user speaks in English, the model responds in Spanish, both as audio. The cascade version requires ASR in English, translation to Spanish, and TTS in Spanish. The speech-to-speech version skips the intermediate text representations and produces translated audio directly. Latency is lower, prosody can transfer across languages, and the user experience is smoother. Translation applications deployed speech-to-speech models in late 2025 and reported latency reductions of 30 to 40 percent compared to cascade architectures.

Customer support hotlines are testing speech-to-speech but moving cautiously. The latency benefit is attractive, but compliance teams resist giving up text logs. The current pattern in 2026 is hybrid: run the speech-to-speech model for real-time interaction, run ASR on the input and output audio in parallel for logging and compliance. This doubles the cost but satisfies both the UX team and the legal team. It is not elegant, but it is deployable.

Speech-to-speech does not work well in 2026 for high-stakes applications where every word matters. Legal, medical, and financial applications still use the cascade. The need to review, redact, and approve responses before they reach the user requires intermediate text. The latency cost is acceptable when the alternative is regulatory exposure.

## The Current Limitations

Speech-to-speech models in 2026 still have higher latency than the marketing suggests. GPT-4o voice mode averages 600 to 900 milliseconds time-to-first-audio in production, but p95 latency is often 1.2 to 1.5 seconds. A well-tuned cascade can match that. The latency advantage exists but is smaller than the architectural simplification implies.

The models are also less controllable. You cannot easily inject system instructions, modify the response mid-generation, or enforce strict output structure. Cascade systems let you pass the LLM response through validators, content filters, and formatting logic before it becomes audio. Speech-to-speech systems generate audio directly. If the model generates a response you want to change, you cannot change it — you can only regenerate from scratch.

Speech-to-speech models are also harder to debug. When a cascade model fails, you know which component failed. ASR produced bad transcription, or the LLM hallucinated, or TTS mispronounced a word. You see the intermediate outputs. Speech-to-speech models fail opaquely. The user said something, the model responded incorrectly, and you have two audio files. Diagnosing the failure requires running ASR on both audio files, comparing transcriptions to expected behavior, and guessing where the model diverged. This makes iterative debugging slower.

The training data requirements for speech-to-speech models are also higher. Cascade models can be trained on text-only data for the LLM, audio-only data for ASR, and text-to-audio pairs for TTS. Speech-to-speech models need audio-to-audio pairs. That data is harder to collect, harder to annotate, and harder to balance across languages, accents, and domains. In 2026, the available training datasets for speech-to-speech are smaller and less diverse than the datasets available for the cascade components. This shows up in the model's behavior — it works well for mainstream American and British English, less well for accented English, and poorly for low-resource languages.

## Where This Is Headed

The speech-to-speech architecture is not replacing the cascade in 2026. It is becoming an option. Teams choose speech-to-speech when latency and simplicity are the top priorities and compliance allows audio-only logs. Teams choose the cascade when visibility, controllability, and debuggability matter more than milliseconds.

The next generation of speech-to-speech models will likely expose optional text outputs as auxiliary signals. The model remains end-to-end audio, but it also produces a text transcription of what it heard and what it said. This bridges the gap between architectural simplicity and operational visibility. Some research models in late 2025 demonstrated this capability. Production deployment in 2026 is still limited.

The other trend is hybrid architectures. Run speech-to-speech for latency-sensitive paths, run the cascade for compliance-sensitive paths, and let the product layer decide which path to use based on the request. A customer asking "What is my balance?" goes through the fast speech-to-speech path. A customer disputing a charge goes through the slower cascade path with full logging. This adds orchestration complexity but preserves both the UX benefit and the compliance safety net.

The speech-to-speech architecture is a bet on models getting better, faster, and safer. If safety guardrails improve to the point where intermediate text is not necessary for content filtering, if model reliability improves to the point where opaque failures are rare, and if latency improves to the point where the cascade cannot compete, speech-to-speech becomes the default. In 2026, those conditions are not yet met. The architecture is promising, not proven.

The next subchapter covers voice activity detection, the first decision every voice pipeline makes — when is the user speaking and when are they not.

