# 11.6 — Real-Time vs Batch Quality Evaluation

Real-time evaluation catches problems fast but misses depth. Batch evaluation catches depth but misses speed. Every production voice system needs both, and the hard part is not building the evaluation pipelines — it is deciding which quality signals belong in real-time, which belong in batch, and how to make the findings from one inform the thresholds in the other. Get the split wrong and you either drown in alert fatigue from real-time false positives, or you discover critical failures three days too late from batch-only monitoring.

The difference is not just timing. Real-time evaluation runs during the conversation with strict latency budgets and limited context. You have milliseconds to decide whether the current turn is problematic, and you can only use information available up to this moment — the current transcript, the current confidence scores, the current latency measurements. Batch evaluation runs after the conversation ends with no latency constraints and full context. You have the complete transcript, the full interaction history, the eventual outcome, and as much compute as you are willing to spend. Real-time evaluation is a smoke detector. Batch evaluation is a forensic investigation. You need both, but they solve different problems.

The split is not arbitrary. Certain quality dimensions are inherently real-time — latency, availability, confidence scores, immediate errors. Other quality dimensions are inherently batch — conversation coherence, task completion, user satisfaction, long-term harm. The skill is in drawing the boundary correctly and building the feedback loop that makes batch findings actionable in real-time.

## What to Evaluate in Real-Time

Real-time evaluation focuses on signals that are immediately measurable, actionable during the conversation, and indicative of acute failure. The goal is to detect problems fast enough to either recover in the current conversation — by escalating to a human, by rephrasing the response, by triggering a fallback behavior — or to alert operators to systemic issues before thousands more conversations fail the same way.

Latency is the most obvious real-time metric. Every turn has a measurable delay from user speech end to system speech start. You measure it, compare it to your threshold — typically 300ms to 500ms for conversational systems — and alert if it exceeds the threshold for more than 10% of turns in a 5-minute window. This is a leading indicator of infrastructure problems, model slowdowns, or traffic spikes. You cannot measure latency after the fact with the same urgency — by the time batch evaluation reveals elevated latency, hundreds of users have already had a degraded experience.

Error rates are the second real-time metric. Every turn can fail in measurable ways — ASR timeout, TTS generation failure, intent recognition error, backend API timeout. You track error rate per error type in real-time and alert when error rate crosses a threshold. A 5% ASR timeout rate is a signal that microphone input is degraded, network conditions are poor, or the ASR service is overloaded. A 2% backend API timeout rate is a signal that downstream services are slow or failing. These errors are binary and immediate. Real-time detection is the only way to catch them before they cascade.

Confidence scores are the third real-time metric. ASR confidence, intent recognition confidence, and entity extraction confidence are available immediately and correlate with downstream quality. A conversation with consistently low ASR confidence — below 0.7 — is likely producing garbled transcripts. A conversation with low intent confidence — below 0.6 — is likely misunderstanding the user's goal. You cannot fix the current conversation retroactively, but you can use low confidence as a trigger for clarification prompts or human escalation. In a customer service voice system, a rule like "if intent confidence is below 0.5 for two consecutive turns, offer to transfer to a human agent" reduces failed conversations by 15% to 20%.

The fourth real-time metric is profanity and toxicity detection. You run every user utterance and every system response through a lightweight toxicity classifier — typically a small BERT-based model with sub-50ms inference time. If the user is being abusive, you log it and potentially escalate. If the system generates a toxic response, you block it and substitute a safe fallback. This must happen in real-time because you cannot un-say something. A toxic response that makes it to the user is a reputational and legal risk. Batch detection of toxicity is forensic — it tells you how often the problem occurred. Real-time detection is preventive — it stops the harm.

The fifth real-time metric is conversation state anomalies. If your voice agent uses a state machine or dialogue manager, you track which states it enters and how long it stays in each state. Anomalies are signals. If the agent is stuck in the same state for more than 3 turns, the user is likely confused or the agent is looping. If the agent enters an error-handling state more than twice in one conversation, something is deeply wrong. These anomalies are detectable in real-time and actionable — you can trigger escalation, reset the conversation, or log the conversation for manual review.

## What to Evaluate in Batch

Batch evaluation focuses on quality dimensions that require full context, human judgment, or expensive computation. The goal is to measure the things that matter most — whether the conversation succeeded, whether the user was satisfied, whether the agent followed policy — without the latency constraints of real-time evaluation.

Transcript quality is the most important batch metric. You cannot evaluate transcript quality in real-time because you do not have the full conversation yet. After the conversation ends, you can ask: did the agent maintain context across turns, did the agent ask clarifying questions when needed, did the agent provide accurate information, did the agent follow the conversation flow you designed? These are subjective judgments that often require human review or LLM-based evaluation. In 2026, the standard approach is to run every transcript through a GPT-5-nano or Llama 4 Scout grader that scores it on 5 to 10 quality dimensions — coherence, accuracy, helpfulness, politeness, task completion. The grader is cheap enough to run on every conversation, and accurate enough to replace 80% of human review.

Conversation flow analysis is the second batch metric. You reconstruct the conversation as a sequence of user intents and system actions, and you look for flow anomalies — dead ends, loops, unexpected jumps, missing confirmations. A well-designed voice agent should have predictable conversation patterns. If 90% of "cancel subscription" conversations follow the pattern "authenticate, confirm intent, execute cancellation, confirm completion," then the 10% that deviate are worth reviewing. Batch evaluation lets you identify these outliers and investigate why they deviated — was it user confusion, agent error, or a legitimate edge case you had not designed for?

User satisfaction is the third batch metric. You measure satisfaction through post-call surveys, sentiment analysis of the user's final utterances, or implicit signals like hang-up rate and repeat call rate. Surveys are the gold standard but have low response rates — typically 5% to 15%. Sentiment analysis is scalable but noisy — users often sound frustrated even when the call succeeded. Implicit signals are the most reliable for scale. If a user completes the conversation, does not hang up mid-call, and does not call back within 24 hours, you infer satisfaction. If they hang up mid-call or call back within 24 hours, you infer dissatisfaction. Batch evaluation gives you time to correlate these signals with transcript features and build a satisfaction prediction model.

The fourth batch metric is policy compliance. If your voice agent operates in a regulated domain — healthcare, finance, legal — you need to verify that every conversation followed policy. Did the agent disclose required information? Did the agent avoid making prohibited claims? Did the agent handle sensitive data correctly? Policy compliance is too complex for real-time rule-based checks. Batch evaluation uses LLM-based graders or human reviewers to audit a sample of conversations — typically 1% to 5% — and flag violations. In a HIPAA-regulated voice agent, batch policy review is not optional. It is the difference between compliance and a multi-million-dollar fine.

The fifth batch metric is model behavior analysis. Batch evaluation gives you the opportunity to ask questions that require aggregate data. Are certain intents consistently misclassified? Are certain user demographics experiencing lower success rates? Are certain times of day producing lower quality transcripts? You cannot answer these questions turn-by-turn in real-time. You need the full dataset, aggregated and analyzed. Batch evaluation is where you detect model drift, data distribution shift, and emerging failure modes that are invisible in real-time metrics.

## The Feedback Loop Between Real-Time and Batch

The power of a hybrid evaluation strategy is not in running two independent pipelines — it is in creating a feedback loop where batch findings inform real-time thresholds, and real-time anomalies trigger batch deep-dives.

The first direction of the loop is batch to real-time. Every week, you review batch evaluation findings and ask: are there patterns we could detect in real-time? If batch evaluation reveals that conversations with low ASR confidence in the first turn have a 40% failure rate, you update your real-time alerting to flag low first-turn confidence as high-risk. If batch evaluation reveals that conversations longer than 8 minutes rarely succeed, you update your real-time escalation logic to offer human transfer after 7 minutes. Batch evaluation finds the patterns. Real-time evaluation operationalizes them.

The second direction is real-time to batch. When real-time monitoring detects an anomaly — a latency spike, a confidence drop, an error rate increase — you trigger a batch deep-dive on the affected conversations. You pull all conversations from the anomaly window, run full quality evaluation, review transcripts, and identify the root cause. Real-time monitoring says "something is wrong." Batch evaluation says "here is what went wrong, and here is why."

A finance company's voice banking assistant experienced a 300ms latency spike at 2pm on a Tuesday in late 2025. Real-time monitoring caught the spike within 5 minutes and alerted the on-call engineer. The engineer checked infrastructure metrics — CPU, memory, network — and found nothing abnormal. The latency spike was isolated to TTS generation. The engineer triggered a batch analysis of all conversations between 1:55pm and 2:15pm. The batch analysis revealed that during that window, 18% of conversations included unusually long responses — system utterances over 200 words. The long responses were triggering TTS timeouts, which were being retried, which added latency. The root cause was a prompt change deployed at 1:50pm that made the agent more verbose. The prompt was reverted by 2:30pm, and latency returned to baseline. Without the feedback loop — real-time detection plus batch deep-dive — the issue would have persisted for hours.

## Resource Allocation for Real-Time vs Batch

Real-time evaluation and batch evaluation have different cost profiles. Real-time evaluation runs on the critical path of every conversation, so it must be cheap and fast. Batch evaluation runs offline, so it can be expensive and slow. The resource allocation question is how much compute to dedicate to each.

Real-time evaluation typically uses lightweight models — small BERT classifiers, rule-based checks, simple statistical thresholds. The cost per conversation is measured in fractions of a cent. A production voice system handling 100,000 conversations per day might spend 50 dollars per day on real-time evaluation compute. The constraint is latency, not cost. You choose the cheapest model that can run in under 50ms, because anything slower adds perceptible delay.

Batch evaluation typically uses heavyweight models — GPT-5-nano or Llama 4 Scout for transcript grading, multi-dimensional quality scoring, conversation flow reconstruction. The cost per conversation is measured in cents. The same system handling 100,000 conversations per day might spend 500 to 2,000 dollars per day on batch evaluation compute, depending on how many conversations are evaluated and how deeply. The constraint is cost, not latency. You choose the most accurate model you can afford, because batch evaluation does not affect user experience.

The resource allocation decision is what percentage of conversations to evaluate in batch. Evaluating 100% of conversations in batch is prohibitively expensive for most teams. Evaluating 1% of conversations is too sparse to detect rare but important failures. The sweet spot is typically 10% to 25%, stratified by conversation outcome. You evaluate all failed conversations — those where the user hung up, the task did not complete, or the system reported an error. You evaluate a random sample of successful conversations to measure baseline quality. You evaluate all conversations flagged by real-time anomaly detection. This gives you depth on failures, coverage on baseline, and responsiveness to anomalies, without evaluating every single conversation.

## When Batch Evaluation is Too Slow for Your Use Case

Batch evaluation assumes you can tolerate a delay between the conversation ending and quality results being available. For most applications, this is fine. You review yesterday's conversations today, identify issues, and fix them for tomorrow's conversations. But some use cases cannot tolerate the delay.

High-stakes real-time applications — medical triage, emergency response, financial trading — need quality feedback within seconds, not hours. A voice-based medical triage system cannot wait until tomorrow to discover that it gave dangerous advice today. It needs to know immediately. For these applications, the boundary between real-time and batch collapses. You run batch-quality evaluation in near-real-time — within 30 seconds of conversation end — using faster models, smaller samples, or more compute. The cost is higher, but the stakes justify it.

A/B testing and experimentation also compress the batch window. If you are running a live A/B test of two prompt versions, you need quality results fast enough to make go/no-go decisions within hours, not days. Waiting 24 hours for batch evaluation means serving potentially worse experiences to users for an entire day. The solution is accelerated batch evaluation — running quality grading every hour instead of every 24 hours, using cached results where possible, and prioritizing evaluation of A/B test traffic over baseline traffic.

The general principle is that batch evaluation delay is a parameter, not a constant. The default is overnight batch — evaluate yesterday's data, deliver results this morning. But if your use case demands faster feedback, you pay more for more compute and you get results faster. The cost-speed trade-off is explicit.

## The Convergence of Real-Time and Batch in 2026

In 2024, real-time and batch evaluation were fully separate pipelines. Real-time ran simple rules. Batch ran expensive models. The two rarely talked to each other. By 2026, the boundary has blurred. Real-time evaluation uses lightweight LLMs for on-the-fly quality scoring. Batch evaluation runs fast enough to inform hourly operational decisions. The feedback loop is automated — batch findings update real-time thresholds without human intervention.

The driver of this convergence is cheaper, faster models. GPT-5-nano and Llama 4 Scout can run on a single GPU with sub-100ms inference time, making them viable for near-real-time use. Techniques like speculative decoding and quantization push inference costs down further. The result is that what was "batch-only" in 2024 is "real-time-capable" in 2026, and teams are rethinking which evaluations belong where.

The operational question is no longer "real-time or batch?" The question is "how fast do we need the answer, and how much are we willing to pay for speed?" Latency and error rates still live in real-time. User satisfaction and policy compliance still live in batch. But transcript quality, conversation flow, and task completion are moving from batch toward near-real-time, because the models are fast enough and the business value of fast feedback is high enough.

---

Real-time evaluation catches fires. Batch evaluation prevents fires. Both are necessary. The craft is in deciding which metrics live where, building the feedback loop that makes each one better, and allocating compute to match the urgency of the signal. The next step is turning those signals into alerts that wake the right people at the right time — which requires designing thresholds that are sensitive to real problems but immune to noise.

