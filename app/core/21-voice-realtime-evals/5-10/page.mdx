# 5.10 — Automated TTS Quality Metrics: Beyond Human Evaluation

Human evaluation of TTS quality is the gold standard. You play audio samples to listeners, they rate naturalness and intelligibility on a five-point scale, you aggregate the scores into a Mean Opinion Score. MOS is reliable. It correlates with user satisfaction. But it is expensive, slow, and does not scale. A single MOS study with 50 samples and 20 raters costs $3,000 and takes two weeks. You cannot run MOS on every pull request, every model version, every synthesis configuration. You need automated metrics that approximate human judgment well enough to guide development, catch regressions, and enable continuous evaluation. The question is which metrics to trust, when to trust them, and when human evaluation is still necessary.

Automated TTS quality metrics fall into three categories: signal-quality metrics that measure acoustic fidelity, perceptual metrics that model human auditory processing, and learned metrics that train on human MOS data to predict ratings. Signal-quality metrics are fast and deterministic but correlate weakly with human judgment. Perceptual metrics are slower but correlate better. Learned metrics correlate best but require training data and can overfit to specific voice styles or domains. No single metric is sufficient. You use a combination. You validate the combination against human ratings. You update your metric suite as TTS systems evolve.

This subchapter teaches what automated TTS quality metrics exist, how they work, what they actually measure, their correlation with human judgment, when to trust them, and when you still need humans in the loop.

## Signal-Quality Metrics: Fast but Limited

Signal-quality metrics measure the acoustic properties of the audio signal. They are objective, deterministic, and computationally cheap. They tell you whether the audio is clean, whether the frequency spectrum is balanced, whether the energy levels are stable. They do not tell you whether the voice sounds natural or human. They are necessary but not sufficient.

**Signal-to-Noise Ratio** measures the ratio of speech energy to background noise. You compute the energy in the speech segments and the energy in the non-speech segments. You express the ratio in decibels. A clean TTS output should have an SNR above 30 dB. Below 20 dB, noise is audible and distracting. SNR is useful for detecting encoding artifacts, transmission errors, or noisy synthesis. But it does not measure naturalness. A perfectly clean robotic voice has high SNR.

**Spectral Flatness** measures how evenly energy is distributed across frequencies. A flat spectrum — where all frequencies have similar energy — indicates noise. A structured spectrum — where energy is concentrated in specific bands — indicates tonal content like speech. TTS outputs should have low spectral flatness, typically below 0.15. High flatness indicates the synthesis introduced noise or lost tonal structure. This metric catches synthesis failures but does not correlate with perceived quality. A monotone robotic voice can have perfect spectral structure.

**Zero Crossing Rate** measures how often the audio signal crosses zero amplitude. Speech has a ZCR that varies with phoneme type — fricatives have high ZCR, vowels have low ZCR. A TTS output with abnormal ZCR patterns indicates synthesis errors. You compute the ZCR across the utterance and compare it to typical speech. Deviations signal problems. But again, this is a failure detector, not a quality estimator. It tells you when something is broken, not how good it sounds.

**Mel-frequency cepstral distortion** compares the spectral envelope of the synthesized audio to a reference. You compute the MFCC vectors for both signals, align them temporally, and measure the Euclidean distance. Low distortion means the synthesis matches the reference. This is useful when you have a reference recording — for example, when evaluating a voice cloning system. You synthesize text, compare it to the original speaker, and measure how close you got. But for conversational TTS, you often do not have a reference. And even when you do, low distortion does not guarantee naturalness. A synthesis can match the spectral envelope but still sound robotic due to prosody failures.

Signal-quality metrics are useful as regression tests. If SNR drops from 35 dB to 18 dB between model versions, something broke. If spectral flatness spikes from 0.10 to 0.40, synthesis failed. But you cannot use these metrics to choose between two natural-sounding voices. They do not capture what humans care about.

## Perceptual Metrics: Modeling Human Hearing

Perceptual metrics attempt to model how humans process audio. They measure not just the signal but how the signal is perceived. They account for frequency masking, temporal masking, and the non-linear sensitivity of human hearing. They correlate better with human judgment than signal-quality metrics. They are still imperfect.

**PESQ** — Perceptual Evaluation of Speech Quality — was developed for telephony. It compares a degraded signal to a reference signal and predicts how humans would rate the degradation. It models the human auditory system as a filter bank, aligns the signals temporally, and computes a perceptual distance. The output is a score from 1 to 4.5, where higher is better. PESQ correlates with human MOS at around 0.85 for telephony codecs. For TTS, the correlation is weaker — around 0.65 to 0.75 — because PESQ was not designed to evaluate naturalness, only fidelity. It tells you how much the synthesis deviates from the reference, not whether the reference was natural.

**POLQA** — Perceptual Objective Listening Quality Assessment — is the successor to PESQ, designed for wideband and super-wideband audio. It handles higher sampling rates, better models frequency masking, and correlates better with human judgment for high-quality audio. For TTS, POLQA is useful when evaluating high-fidelity synthesis — 24 kHz or 48 kHz sampling rates. It still requires a reference. If you are evaluating a voice cloning system, POLQA is a strong choice. If you are evaluating generated conversational responses without a reference, POLQA does not apply.

**ViSQOL** — Virtual Speech Quality Objective Listener — extends PESQ to handle modern audio. It uses a similarity metric based on spectro-temporal patches rather than simple frame-by-frame comparison. It correlates with MOS at around 0.80 for speech. It is faster than running a full MOS study but slower than PESQ. ViSQOL is useful for comparing TTS model versions, evaluating codec quality, or measuring the impact of streaming on audio quality. Like PESQ and POLQA, it requires a reference. You need a ground-truth recording to compare against.

The limitation of perceptual metrics is that they were designed for telephony, compression, and transmission quality — not for naturalness. They tell you how much the audio degraded relative to a reference. They do not tell you whether the audio sounds human. A synthesis that perfectly matches a robotic reference gets a high score. A synthesis that introduces natural prosodic variation relative to a flat reference gets a lower score. These metrics are useful for detecting regressions but not for predicting user satisfaction with conversational voice.

## Learned Metrics: Training on Human Judgments

Learned metrics train machine learning models on human MOS data to predict how humans would rate unseen audio. They can be trained to predict naturalness, intelligibility, speaker similarity, or any other dimension humans can rate. They require labeled training data — thousands of audio samples with human ratings. Once trained, they are fast. They generalize to new voices and new TTS systems. They are the state of the art for automated TTS evaluation in 2026.

**MOSNet** is a convolutional neural network trained to predict MOS from audio. It takes a TTS output as input and outputs a predicted MOS score from 1 to 5. It was trained on the VoiceMOS Challenge dataset, which includes human ratings for thousands of TTS samples from multiple systems. MOSNet correlates with human MOS at around 0.85 to 0.90 depending on the dataset. It captures aspects of naturalness, prosody quality, and voice quality that signal metrics miss. It does not require a reference — it rates the audio on its own.

**UTMOS** — Universal Text-to-Speech MOS prediction — improves on MOSNet by training on a larger and more diverse dataset. It handles multiple languages, multiple voice types, and multiple TTS architectures. It uses a transformer-based architecture that attends to both local and global acoustic features. UTMOS correlates with human MOS at around 0.88 to 0.92. It is the current state of the art for reference-free TTS quality prediction. You feed it a synthesized utterance, it outputs a predicted MOS. You use it to compare model versions, rank TTS providers, or detect quality regressions in production.

**Speaker Similarity Metrics** based on speaker embeddings are another class of learned metric. You extract a speaker embedding from the synthesized audio using a pretrained speaker verification model like Resemblyzer or SpeakerNet. You compare the embedding to the target speaker's embedding using cosine similarity. High similarity means the synthesis sounds like the target speaker. This is critical for voice cloning. A voice cloning system that achieves 95% speaker similarity is indistinguishable from the real person. Below 85%, users notice the difference. Speaker similarity metrics correlate well with human judgments of "does this sound like the person" but do not measure naturalness. A synthesis can sound like the speaker but still sound robotic.

Learned metrics require training data and periodic retraining. As TTS systems improve, the distribution of quality shifts. A model trained in 2024 on MOS data from 2023 TTS systems might not generalize well to 2026 systems. You need to refresh your training data. You run MOS studies on recent TTS outputs, collect human ratings, retrain or fine-tune your learned metrics. This is an ongoing cost. But the cost is far lower than running MOS on every evaluation.

## When to Trust Automated Metrics

You trust automated metrics when the decision is low-stakes, when the metric has been validated on your domain, and when the metric is used for relative comparison rather than absolute judgment. You do not trust automated metrics for final quality gates, for user-facing guarantees, or for domains where the metric has not been validated.

Automated metrics are reliable for regression testing. If your UTMOS score drops from 4.2 to 3.5 between commits, something broke. You do not need human eval to confirm that. You roll back the change or debug the regression. Automated metrics catch these failures fast. They are part of your CI pipeline. Every pull request runs automated TTS quality checks. Commits that degrade quality are blocked.

Automated metrics are reliable for ranking alternatives. If you are comparing five TTS providers and UTMOS gives them scores of 4.3, 4.1, 3.9, 3.7, and 3.5, you can trust that the top two are better than the bottom two. You might not trust the absolute scores — maybe the true MOS for the top provider is 4.1 instead of 4.3 — but the relative ranking is correct. You use automated metrics to narrow the field. You then run human eval on the top two candidates to make the final decision.

Automated metrics are reliable when validated on your domain. If you are building a healthcare voice assistant and you have run MOS studies on 500 healthcare-domain TTS samples, you can train or fine-tune a learned metric on that data. The metric is now calibrated to your domain. It knows what "natural" sounds like for medical terminology, patient-facing communication, and calm reassurance. You can trust it for healthcare TTS. You cannot trust it for a customer service application without revalidation. Voice quality is domain-specific.

Automated metrics are not reliable for final quality gates. If you are launching a new voice to millions of users, you run human eval. You cannot rely on a predicted MOS of 4.2 and assume users will love it. You run a MOS study with 30 to 50 raters, you confirm the score, you test with real users in a beta. Automated metrics guide development. Human evaluation gates production.

Automated metrics are not reliable when the metric has never been validated. If you are using PESQ to evaluate conversational TTS naturalness and you have never checked whether PESQ correlates with human judgments for your use case, you are guessing. You run a validation study. You collect 100 TTS samples, run MOS, compute PESQ, measure the correlation. If correlation is above 0.75, PESQ is useful. If it is below 0.60, PESQ is not capturing what you care about.

## When Human Evaluation Is Still Necessary

Human evaluation is necessary when launching new voices, when changing TTS providers, when targeting new user populations, when measuring dimensions that automated metrics do not capture, and when the stakes are high enough that a mistake is unacceptable.

Launching a new voice requires MOS. You synthesize 50 diverse utterances — questions, statements, emotional content, technical content, conversational responses. You recruit 20 to 30 raters. You run a MOS study. You collect ratings for naturalness, intelligibility, and likability. You aggregate the scores. If the MOS is below 4.0, you do not launch. If it is above 4.2, you proceed. If it is between 4.0 and 4.2, you test with real users in a controlled beta. Automated metrics can predict this outcome, but they cannot replace it. Users are the final judge.

Changing TTS providers requires A/B testing with real users. Automated metrics tell you whether the new provider is better on paper. Users tell you whether they prefer it. You run a two-week A/B test. Half of users get the old provider, half get the new. You measure task completion, conversation length, user satisfaction, and explicit preference when users are asked. You compare the populations. If the new provider improves satisfaction by more than 5% and does not degrade task completion, you migrate. If satisfaction is flat or negative, you stay with the old provider.

Measuring subjective dimensions like warmth, trustworthiness, professionalism, or brand alignment requires human evaluation. Automated metrics can predict naturalness. They cannot predict whether the voice sounds trustworthy. You run a survey. You play five voice samples. You ask users to rate each on warmth, trust, professionalism. You aggregate the ratings. You choose the voice that best aligns with your brand. This is qualitative research, not automated measurement.

High-stakes applications — healthcare, legal, financial services — require human evaluation at every major release. The cost of a quality failure is too high. A voice that sounds unnatural in a customer service context is annoying. A voice that sounds unnatural in a mental health context is harmful. You run MOS studies. You test with domain experts. You validate with real patients or clients in controlled settings. You do not rely on automation alone.

Low-stakes applications — casual chatbots, entertainment, non-critical assistants — can rely more heavily on automated metrics. If the automated metric is validated and the stakes are low, you can ship based on UTMOS scores. You monitor production quality with automated metrics. You run human eval quarterly to recalibrate. This is the right trade-off when speed matters more than perfection.

## How to Build a TTS Quality Evaluation Pipeline

You build a TTS quality evaluation pipeline that combines automated metrics for speed and human evaluation for ground truth. You run automated metrics on every synthesis. You run human evaluation on schedule and on major changes. You use the human evaluation to validate and retrain your automated metrics. The loop is continuous.

Your pipeline starts with synthesis. Every time you generate TTS — in development, in CI, in production — you store the audio and the input text. You extract acoustic features. You run automated metrics. You compute UTMOS, speaker similarity if applicable, PESQ or ViSQOL if you have a reference. You log the scores. You track them over time.

You set thresholds. If UTMOS drops below 3.8, you flag the synthesis. If speaker similarity drops below 0.85 for a voice cloning application, you flag it. Flagged samples are reviewed. You listen to them. You confirm whether the automated metric correctly identified a problem. If yes, you debug the synthesis. If no, you adjust the threshold or investigate why the metric failed.

You run human evaluation monthly or quarterly. You sample 50 to 100 diverse utterances from production traffic. You run MOS. You collect ratings for naturalness, intelligibility, and any subjective dimensions you care about. You compare the MOS scores to the automated metric predictions. You compute the correlation. If correlation is above 0.85, your metrics are working. If correlation drops below 0.75, your metrics are drifting. You retrain or recalibrate.

You use human evaluation to detect blind spots. Automated metrics might miss edge cases — synthesis of rare names, synthesis of long sentences, synthesis of emotionally charged content. You explicitly include these edge cases in your MOS studies. You measure how well the automated metrics handle them. If automated metrics consistently underrate or overrate edge cases, you adjust your pipeline to handle them separately.

You track quality over time. You plot UTMOS scores for every production synthesis. You track the 50th percentile, the 10th percentile, and the 1st percentile. The 50th percentile tells you typical quality. The 10th percentile tells you how often quality is poor. The 1st percentile tells you how bad the worst cases are. You set targets. Median UTMOS above 4.0. 10th percentile above 3.5. 1st percentile above 3.0. If any metric drops below target, you investigate.

## What Breaks When You Rely Only on Automated Metrics

Relying only on automated metrics leads to three failure modes: optimizing for the metric instead of the user experience, missing subjective quality dimensions that matter, and failing to catch domain-specific failures that the metric was not trained on.

Optimizing for the metric creates Goodhart's Law failures. Your team tunes the TTS system to maximize UTMOS. UTMOS improves from 4.1 to 4.4. But users report that the voice sounds less natural. What happened? The system learned to produce audio that matches the statistical patterns UTMOS was trained on, but those patterns do not fully capture naturalness. The voice has perfect prosody variation but sounds rehearsed. It has perfect spectral balance but lacks the imperfections that make voices sound human. You optimized the proxy, not the outcome.

Missing subjective dimensions means you ship a voice that scores well on naturalness but fails on trust or warmth. Automated metrics predict MOS. They do not predict brand alignment. A voice that sounds natural but sharp and fast might be perfect for a sports news app but wrong for a meditation app. You run human eval to measure subjective fit. You ask users whether the voice matches the application. You adjust based on qualitative feedback, not just quantitative scores.

Domain-specific failures occur when your automated metric was trained on general TTS data but you are synthesizing domain-specific content. A metric trained on audiobooks might not generalize to medical terminology, legal jargon, or technical product names. The metric gives high scores, but domain experts report pronunciation errors, awkward phrasing, or unnatural emphasis. You need human evaluation from domain experts. You run MOS studies with physicians for healthcare TTS, with lawyers for legal TTS, with engineers for technical support TTS. You validate that the voice works in context.

Automated metrics are tools. They accelerate evaluation, catch regressions, and enable continuous monitoring. But they do not replace human judgment. You use both. The metrics guide the work. The humans validate the outcome.

The next subchapter covers how to select a TTS provider and how to run A/B tests to compare them in production.

