# 11.4 — TTS Quality Degradation Detection

Text-to-speech quality is the hardest part of voice systems to monitor and the easiest for users to judge. A user hears robotic pacing, a mispronounced name, an unnatural emphasis, or a voice that sounds wrong for the context — and they lose trust instantly. They do not file a bug report. They do not explain what sounded off. They just stop using the product. Meanwhile, your monitoring shows green across the board. TTS latency is fine. Error rates are low. The audio files are being generated and delivered successfully. But the experience is terrible, and you have no automated way to detect it.

This is the core challenge of TTS observability: the metrics you can measure automatically — latency, throughput, error rates — do not correlate with the quality users hear. Audio quality is subjective, multidimensional, and context-dependent. A voice that sounds great for reading news sounds wrong for customer service. A pronunciation that is technically correct but unusual breaks immersion. A pacing that is fine for long-form content feels rushed in short conversational turns. You cannot run audio through a classifier and get a quality score that predicts user satisfaction. The best you can do is instrument proxy signals that correlate with quality problems and combine them with sampled human review.

## Proxy Metrics for TTS Quality: Latency and Audio Characteristics

You cannot measure TTS quality directly in real time, but you can measure signals that correlate with quality degradation. These proxy metrics do not tell you whether the voice sounds good. They tell you when something changed that is likely to make it sound worse.

**TTS latency** is the first proxy. Modern TTS engines have consistent generation times for typical input. If TTS latency for a given text length increases, it suggests the engine is struggling. Maybe the text contains unusual characters, mixed languages, or complex punctuation that forces the engine to fall back to slower synthesis paths. Maybe the TTS service is overloaded and queueing requests. Maybe a recent update introduced a performance regression. You track TTS latency per request and per character of input text. A sudden increase in latency-per-character is a signal that synthesis is getting harder, which often correlates with quality issues.

**Audio duration versus text length ratio** is the second proxy. For a given amount of text, TTS should produce a predictable amount of audio. If you send 100 characters and typically get 8 seconds of audio, a sudden shift to 12 seconds suggests the pacing changed. Maybe the TTS engine is inserting longer pauses. Maybe it is speaking more slowly. Maybe it is adding filler or hesitation sounds that were not there before. If audio duration per character decreases, the voice is speaking faster, which can sound rushed or unnatural. You track this ratio over time and alert when it shifts by more than 15%.

**Character composition metrics** measure the complexity of text sent to TTS. You track the percentage of non-ASCII characters, the number of punctuation marks, the number of capitalized words, the presence of URLs or special symbols. Complex text is harder to synthesize well. A gradual increase in text complexity — more mixed-language sentences, more technical jargon, more unusual punctuation — predicts quality degradation even before users complain. You cannot fix the text itself, but you can detect when your text generation is producing TTS-unfriendly output and adjust the LLM to generate simpler, cleaner sentences.

**TTS engine warnings and fallback signals** are the most direct proxy. Many TTS engines log warnings when they encounter text they cannot handle well. They might signal that a word is not in their pronunciation dictionary, that they are falling back to phonetic synthesis, or that they had to normalize unusual characters. These warnings are not errors — the API call succeeds — but they predict quality issues. You instrument these warnings as a separate metric. A spike in fallback signals means the TTS engine is struggling more than usual, which means audio quality is likely degrading.

The pattern that works: track all four proxy metrics over time. Build dashboards that show TTS latency per character, audio duration per character, text complexity scores, and fallback warning rates. Set alerts that fire when any metric shifts significantly. These alerts do not prove quality degraded, but they trigger investigation. When the alert fires, you sample recent audio, listen to it, and confirm whether the proxy signal was correct. Over time, you learn which proxy signals are reliable and which are noise.

## User Feedback Signals for TTS Issues

Users will not tell you that your TTS quality is bad directly, but they will signal it through behavior. A user who asks the system to repeat itself is not always indicating ASR failure — sometimes the TTS was garbled or unclear. A user who hangs up immediately after the system responds might be frustrated with the TTS voice. A user who switches to text chat after trying voice might be rejecting the audio experience. These behavioral signals are noisy but useful when aggregated.

**Repeat requests** are the clearest signal. When a user says "what?" or "can you repeat that?" or "I did not catch that," they did not understand the system's response. This might be ASR failing to recognize their speech, or it might be TTS producing audio they could not parse. You instrument repeat requests as a separate metric from general retry rates. If repeat request rates increase, either ASR or TTS quality degraded. You correlate repeat requests with ASR confidence scores. If ASR confidence is high but repeat rates are up, the problem is likely TTS.

**Mid-response interruptions** suggest the user found the TTS output unacceptable. If the system is speaking and the user interrupts before the response finishes, they either got impatient or disliked what they were hearing. Interruptions are normal in conversation, but a spike in interruption rates suggests the TTS is too slow, too verbose, or too unpleasant to listen to. You track the percentage of responses that are interrupted and the timing of interruptions. Interruptions within the first two seconds suggest the voice itself is off-putting. Interruptions after eight seconds suggest the response is too long or not relevant.

**Voice-to-text switching** is a strong negative signal. Some products offer both voice and text interfaces. If a user starts a session with voice and then switches to text, they rejected the voice experience. You track the rate of voice-to-text switches per session. A gradual increase suggests voice quality is degrading. You correlate switching behavior with TTS characteristics — voice type, response length, conversation topic — to identify which scenarios drive users away from voice.

**Session abandonment after TTS delivery** is the strongest signal. If the user abandons the conversation immediately after the system finishes speaking, they likely disliked what they heard. You measure the abandonment rate within five seconds of TTS completion. Compare this to the abandonment rate at other points in the conversation. If users are disproportionately abandoning right after the system speaks, the TTS is driving them away. You cannot always determine why — it could be voice quality, content quality, or simple task completion — but the correlation is worth tracking.

The instrumentation pattern: tag every TTS delivery with metadata about the voice used, the text length, the audio duration, and the conversation context. Tag every user action after TTS delivery — repeat request, interruption, continuation, abandonment. Build cohort analyses that show user behavior by TTS characteristics. If abandonment rates are 10% higher for a specific voice, that voice might have quality issues. If repeat request rates are higher for responses longer than ten seconds, the TTS pacing might be wrong for long-form content. User feedback signals are indirect, but they scale to every conversation without requiring human review.

## Pronunciation Monitoring: Detecting Name and Term Mispronunciations

The most user-visible TTS failures are mispronunciations. When the system mispronounces the user's name, a brand name, a medication, or a technical term, it destroys credibility. Users notice pronunciation errors more than any other TTS quality issue. And you have almost no automated way to detect them.

Pronunciation errors happen when the TTS engine encounters a word it was not trained on or a word with ambiguous phonetics. Names are the most common failure case — "Zhang" pronounced as "Zang," "Siobhan" pronounced phonetically instead of "Shi-vawn." Brand names are the second most common — "Anthropic" pronounced incorrectly, "Kubernetes" mangled. Medical and technical terms are third — medication names, chemical compounds, industry jargon. These words appear in your conversations but were not in the TTS training data, so the engine guesses, and the guess is often wrong.

The instrumentation challenge: you cannot run an audio file through a classifier and detect mispronunciations automatically. Pronunciation correctness is context-dependent and subjective. The only reliable detection method is human review. But you can make human review tractable by flagging high-risk cases.

You start by building a dictionary of known difficult words. These are names, brands, medications, and technical terms that appear in your conversations and that TTS engines commonly mispronounce. You extract this list from user feedback, support tickets, and manual review of sampled conversations. For each term, you optionally store a phonetic spelling or a reference audio file of the correct pronunciation. This dictionary is your pronunciation watchlist.

You instrument TTS requests to detect when the input text contains a word from the watchlist. When a watchlist word appears, you flag that TTS request for review. You log the text, the audio output, the watchlist word, and the conversation context. You queue these flagged cases for human review. Reviewers listen to the audio and mark whether the pronunciation was correct. Over time, you build a dataset of TTS pronunciations paired with correctness labels. This tells you which words your TTS engine handles well and which it does not.

For words the TTS consistently mispronounces, you have three mitigation options. **First**, you can configure the TTS engine with a custom pronunciation dictionary if the engine supports it. You specify the phonetic spelling for each problematic word, and the engine uses that instead of guessing. **Second**, you can preprocess the text before sending it to TTS, replacing problematic words with phonetically equivalent spellings that the TTS handles better. Instead of sending "Siobhan," you send "Shivawn." This is a hack, but it works. **Third**, you can record a human voice actor saying the word correctly and splice that audio into the TTS output. This is expensive and only practical for a small set of high-frequency, high-stakes terms like the company name or the product name.

The pattern that works in production: maintain a pronunciation watchlist updated from user feedback and human review. Instrument TTS requests to flag watchlist words. Sample flagged audio for human review. Build a database of words the TTS mispronounces. Mitigate the most common errors through custom dictionaries, text preprocessing, or pre-recorded audio. Track mispronunciation rates over time. If mispronunciation rates increase, it suggests your vocabulary is shifting or your TTS engine regressed.

## Voice Consistency Across Sessions

Voice consistency is the characteristic users notice subconsciously. If the voice sounds slightly different from one conversation to the next, users feel uneasy even if they cannot articulate why. Consistency includes voice timbre, pacing, intonation, and emotional tone. Inconsistency suggests the system is unreliable or that multiple different agents are behind the interface. It breaks immersion and trust.

Consistency issues happen when you change TTS voices, when you adjust TTS parameters, when you route traffic across multiple TTS engines with different characteristics, or when the TTS engine itself introduces non-deterministic variation. A user who talks to your system today hears one voice. They return tomorrow and hear a slightly different voice. They do not report a bug — they just feel uncomfortable.

You instrument voice consistency by tracking the TTS configuration used for each conversation. You log the voice ID, the engine version, the speed setting, the pitch setting, and any other parameters that affect output. You tag conversations by user ID and time. You build reports that show whether a user experienced different TTS configurations across sessions. If the same user had ten conversations last week and five of them used Voice A and five used Voice B, the experience was inconsistent.

The causes of inconsistency: **feature flags** that route a percentage of traffic to a new voice for testing, **A/B tests** that randomize voice selection per session, **regional routing** where different geographic regions use different TTS engines, **fallback logic** that switches voices when the primary engine is unavailable, and **configuration drift** where TTS settings change over time as engineers tweak parameters. Most of these are intentional, but their impact on user experience is often unintentional. You did not mean to make the user feel like they were talking to two different systems, but that is what happened.

The mitigation: once a user hears a specific voice, pin them to that voice for all future sessions unless you deliberately migrate them. You store the user's assigned voice ID in their profile. Every TTS request for that user uses the same voice. If you run an A/B test, you assign the variant at the user level, not the session level, so each user has a consistent experience. If you roll out a new voice, you migrate users gradually and permanently, not randomly per session.

The instrumentation tracks voice migration events: when a user moves from Voice A to Voice B. You log the reason — manual migration, A/B test enrollment, fallback trigger. You track user behavior before and after the migration. If users who were migrated to Voice B have higher abandonment rates than users who stayed on Voice A, Voice B might be worse. If behavior does not change, the migration was successful. Voice consistency monitoring gives you the data to make voice selection decisions based on user experience rather than engineering convenience.

## Automated TTS Quality Testing in Production

Human review scales only so far. You need automated testing that runs continuously in production, samples real TTS output, and detects quality regressions without waiting for user complaints. Automated TTS quality testing does not replace human judgment, but it catches obvious failures and tracks trends.

The first automated test: **audio delivery verification**. For a random sample of TTS requests, verify that the audio file is non-empty, has the expected duration, and is not corrupted. Corrupted audio is rare, but when it happens, it is catastrophic. The user hears static, silence, or garbled noise instead of speech. You run a simple check: does the audio file decode correctly, does it have non-zero amplitude, does its duration match the expected range given the input text length. If any of these checks fail, you log an error and alert immediately.

The second automated test: **reference audio comparison**. You maintain a set of reference phrases — "Hello, how can I help you today?" "Your appointment is confirmed." "Please hold while I look that up." — and you generate TTS audio for these phrases every hour. You compare the new audio to a stored reference using acoustic similarity metrics. If the similarity drops below a threshold, the TTS output changed. This might indicate a model update, a configuration change, or a quality regression. You investigate why the output changed and whether the change is intentional.

The third automated test: **dynamic range and pacing analysis**. You analyze the audio waveform to measure dynamic range, silence ratio, and speaking rate. If the dynamic range drops — the audio becomes flatter, less expressive — quality likely degraded. If the silence ratio increases — more pauses between words — pacing became unnatural. If speaking rate increases significantly, the voice sounds rushed. You track these metrics over time and alert when they shift. These metrics are crude, but they catch broad quality changes that affect every TTS output.

The fourth automated test: **user-reported issue correlation**. When users report issues — "the voice sounds weird," "I cannot understand what it is saying" — you tag their recent conversations and sample the TTS audio. You listen to it, identify patterns, and add those patterns to your automated tests. Maybe users are complaining about a specific voice sounding robotic. You sample that voice more heavily and confirm the issue. Maybe users are complaining about long responses being hard to follow. You measure pacing on long responses and find that speaking rate increases with text length, making long responses sound rushed. The user feedback tells you what to test for.

The pattern that works: run automated checks on every TTS request for basic correctness. Run deeper automated analysis on a sampled subset — 1% of requests get full acoustic analysis. Store reference audio for critical phrases and regenerate them periodically to detect drift. Correlate automated metrics with user feedback to validate that your proxies are meaningful. Build alerts that fire when automated checks fail or when acoustic metrics shift. Automated testing gives you coverage and trend detection. Human review gives you ground truth and context. Together, they form a monitoring system that catches TTS quality issues before they become user crises.

TTS quality degradation is silent, subjective, and user-critical. You cannot measure it the way you measure latency or error rates. You cannot wait for users to complain because they will just leave. You build a monitoring system that combines proxy metrics, user behavior signals, pronunciation watchlists, consistency tracking, and automated testing. The system does not give you a single quality score. It gives you a multidimensional view of TTS health that surfaces problems early, guides investigation, and ensures that the voice your users hear today is as good as the voice they heard yesterday.

---

*Next: 11.5 — Conversation Success Metrics and User Satisfaction Dashboards*
