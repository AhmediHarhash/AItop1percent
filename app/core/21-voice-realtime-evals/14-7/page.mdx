# 14.7 — State Repair Mechanisms: Detecting and Fixing Corruption

The prescription refill system ran for six months with no major incidents. State management was solid — patient ID, medication name, dosage, pharmacy location, insurance details all tracked correctly across thousands of calls. Then on a random Tuesday afternoon, a patient called to refill their prescription and the system confirmed the order with the wrong dosage. The patient caught it during final confirmation and corrected it. Investigation revealed that the state corruption happened in turn eight when the model hallucinated a dosage value that was never mentioned by the patient or present in the medical record. The hallucinated value was written to state, and every subsequent turn operated on corrupted data. The system had no detection mechanism, no validation layer, and no repair strategy. It confidently processed a dangerous error until the patient stopped it.

State corruption is not theoretical. It happens in production. Models hallucinate values. Extraction logic misparses user speech. ASR errors introduce incorrect data. Concurrency bugs cause state field collisions. Integration failures pull stale data from external systems. Every state field is a potential corruption point, and high-volume voice systems with hundreds of thousands of calls per month encounter corruption daily. The question is not whether corruption happens — the question is whether the system detects and repairs it before the user is harmed.

## The Sources of State Corruption

State corruption originates from multiple failure points in the voice pipeline. The first is ASR errors that propagate to state. The user says "fifty milligrams" and the ASR transcribes "fifteen milligrams." The model extracts "fifteen" and writes it to the dosage field. The state is now corrupted. The user did not provide incorrect information — the system misheard them. This corruption is particularly insidious because the model's response often sounds correct. "Got it, fifteen milligrams" prompts the user to correct immediately if they are paying attention, but if they are distracted or assume the system heard correctly, the corruption persists.

The second source is model hallucination during extraction. The model is asked to extract the patient's insurance provider from the conversation. The user has not mentioned their insurance yet. Instead of leaving the field empty or marking it as unknown, the model hallucinates a plausible-sounding provider name based on patterns in its training data. The insurance provider field is now populated with fabricated data. Later in the conversation, the system confidently confirms "I'll bill this to BlueCross" even though the user never said BlueCross. The user corrects it, but trust is damaged.

The third source is cascading updates that introduce logical inconsistencies. The user changes their appointment time from morning to afternoon. The system updates the time field but fails to update the location field. The morning appointment was at the downtown office. The afternoon appointment is only available at the suburban office. Now state contains an appointment time that does not exist at the stored location. The corruption is not in any single field — it is in the relationship between fields.

The fourth source is stale data from external systems. The voice system queries the user's profile from a CRM and caches their phone number in state. Mid-conversation, the user updates their phone number in the CRM through a different channel. The voice system continues operating on the cached, now-stale phone number. The system confirms "I'll send a confirmation to your number ending in 1234" when the user's current number ends in 5678. The state was correct when loaded but became corrupted as external reality changed.

The fifth source is race conditions in concurrent state updates. Two parts of the system update the same state field simultaneously — one from user input, one from an external API response. Depending on timing, either update can win. If the external API response arrives second and overwrites the user input, the user's explicitly stated preference is lost and replaced with system-inferred data. The user said they prefer email contact, but the API response defaults to phone, and the system now has phone as the contact preference despite the user's explicit statement.

## Corruption Detection: Consistency Checks

You cannot repair corruption you do not detect. Detection requires continuous validation of state integrity. The simplest detection mechanism is type checking. Every state field has an expected type. If the field is supposed to contain a date and it contains a string like "tomorrow afternoon," the type check fails. This detects malformed data but not incorrect data. A date field containing "2026-02-30" passes type checking but is logically invalid because February does not have 30 days.

Range validation extends type checking with value constraints. A dosage field must be a number between 1 and 500. A phone number must have exactly 10 digits. An appointment time must fall within business hours. When a state update violates range constraints, the system flags the field as potentially corrupted. Range validation catches hallucinations that produce out-of-bounds values but misses in-bounds hallucinations. If the model hallucinates 25 milligrams instead of 50, and both values are within the allowed range, range validation does not catch it.

Consistency checks validate relationships between fields. If the appointment time is 3 PM and the appointment location is downtown, check whether the downtown location offers 3 PM appointments. If not, the state is internally inconsistent. If the user's stated age is 12 and the prescription is for an adult-only medication, the state is inconsistent. Consistency checks are domain-specific and require encoding business rules as validation logic.

The consistency checker runs after every state update. When a field changes, the system evaluates all consistency rules that reference that field. If any rule fails, the system marks the state as corrupted and triggers repair logic. The validation is fast — typically under 10 milliseconds — because rules are evaluated as simple conditionals, not complex queries.

Another detection mechanism is source tracking. Every state field records where its value came from: user speech, external API, model inference, or default value. When a field has high stakes and the source is model inference, the system flags it for confirmation. If the insurance provider field is populated by model inference rather than explicit user statement, the system asks the user to confirm before using it. Source tracking does not prevent corruption, but it prevents corrupted data from being used in high-stakes decisions without user validation.

## Corruption Detection: Semantic Validation

Type checking and consistency checks catch structural corruption. Semantic validation catches meaning corruption. The user says "I need to cancel my appointment" and the model extracts "appointment status: confirmed" instead of "appointment status: cancelled." The extracted value is structurally valid — "confirmed" is a legal status — but semantically incorrect. Semantic validation requires comparing extracted state against the conversation context that produced it.

The semantic validator re-reads the relevant conversation turn and checks whether the extracted state is a reasonable interpretation. If the user's turn contains the word "cancel" and the extracted status is "confirmed," the validator flags a mismatch. This requires lightweight NLU that can detect contradictions between text and extracted facts. The validator does not need to be as powerful as the main model — it just needs to catch obvious mismatches.

Semantic validation is expensive. It requires re-processing conversation turns, running inference, and comparing results. The cost is 50 to 150 milliseconds per validation. This is prohibitive if applied to every state field on every turn. Instead, semantic validation is triggered selectively: when a high-stakes field is updated, when the model's extraction confidence is low, or when consistency checks have already flagged the field as suspicious.

User feedback is the strongest semantic validation signal. When the system confirms a state value and the user corrects it, that correction is logged as a semantic validation failure. The system said "your email is john at example dot com" and the user said "no, john dot smith at example dot com." The original extraction was semantically incorrect. Logging these corrections builds a validation dataset that can be used to tune extraction logic and detect systematic semantic errors.

Another semantic validation technique is redundant extraction. The system extracts the same fact multiple times from different parts of the conversation and checks whether the extractions agree. If the user mentions their email in turn three and again in turn ten, extract both times and compare. If the extractions differ, at least one is corrupted. This is only possible when users naturally repeat information, which happens frequently in voice conversations. Users provide their phone number when asked, then mention it again later when discussing how they want to be contacted. Each mention is an opportunity to validate the extracted value.

## Repair Strategies: Rollback and Selective Correction

When corruption is detected, the system must choose a repair strategy. The safest approach is rollback: revert the corrupted field to its last known good value. If the appointment time field was updated in turn twelve and corruption is detected in turn fourteen, roll back the time to the value it held in turn eleven. Rollback works if the turn eleven value was correct. If it was already corrupted, rollback just restores old corruption instead of current corruption.

Rollback is only possible if the system maintains state history. Every state update is logged with a timestamp and the previous value. When rollback is needed, the system walks backward through the history until it finds a version that passes validation. This version becomes the current state. The corrupted updates are discarded. State history adds storage cost — each field has multiple historical versions — but enables safe rollback.

Selective correction is the second repair strategy. Instead of rolling back the entire state, the system identifies which specific field is corrupted and repairs only that field. If the appointment time is corrupted but the location, patient ID, and insurance details are valid, repair the time and leave everything else unchanged. Selective correction minimizes data loss. Rollback might discard five correct updates to eliminate one corrupted update. Selective correction discards only the corrupted update.

Selective correction requires identifying the corruption boundary. Which fields are affected? If the user corrected the appointment time, the time field is corrupted. But is the appointment type also corrupted? Was the type inferred based on the incorrect time? Dependency tracking helps. The system records which fields were updated based on other fields. If the appointment type was set to "follow-up" because the time was in the afternoon, and the time is now corrected to morning, the type must also be re-evaluated.

The third repair strategy is re-extraction. The system goes back to the conversation turn where the corrupted field was originally populated and re-extracts the value. If the email field is corrupted, find the turn where the email was mentioned and extract it again. Re-extraction works if the extraction logic is fixed or if the original extraction was a transient error. If the extraction logic has the same bug, re-extraction produces the same corruption.

Re-extraction requires turn-level indexing. The system must be able to quickly find which turn mentioned a given field. This is implemented with an inverted index: a map from field names to turn numbers. When the email field needs re-extraction, the index returns turn three. The system retrieves turn three, runs extraction, and validates the result. If the new extraction matches the corrupted value, the corruption was not an extraction error — it was a data entry error by the user or an ASR error. If the new extraction is different and passes validation, the original extraction was faulty.

## The Repair-Without-Acknowledgment Pattern

Not all repairs need to be announced to the user. If the system detects corruption early — before it has confirmed the corrupted value to the user — the repair can happen silently. The user said "fifty milligrams," the ASR produced "fifteen milligrams," the model extracted fifteen, the semantic validator flagged the mismatch, the system re-extracted and corrected to fifty. The user never knows the corruption happened. The next system response uses the correct value: "Got it, fifty milligrams."

Silent repair reduces conversational friction. The user does not need to hear "I initially misunderstood your dosage but I've corrected it." They just hear the correct confirmation. The repair happened in the background. Silent repair is only safe when the corrupted value was never surfaced to the user. If the system already said "fifteen milligrams" and the user did not correct it, the system cannot silently change to "fifty milligrams" without explanation. The user heard fifteen and may have mentally committed to it. Changing without acknowledgment creates confusion.

The decision logic is: if the corrupted value has been spoken by the system, repair with acknowledgment. If the corrupted value has not been spoken, repair silently. This requires tracking which state values have been confirmed aloud. The system maintains a confirmed-fields set. When a field is mentioned in a system response, it is added to the set. When corruption is detected in a field that is in the confirmed-fields set, repair with acknowledgment. When corruption is detected in a field not in the set, repair silently.

Acknowledged repair is phrased as a correction. "I misspoke earlier — your dosage is fifty milligrams, not fifteen." The user hears that the system made an error and corrected it. Trust impact depends on frequency. One acknowledged repair in a thirty-turn conversation is acceptable. Three acknowledged repairs suggest the system is unreliable. If acknowledged repairs exceed one per twenty turns, the underlying corruption sources must be fixed, not just repaired.

## Measuring Repair Effectiveness

Repair effectiveness is measured by comparing corrupted-field outcomes with and without repair. The baseline is what happens if corruption is never detected or repaired. The corrupted value propagates to the end of the conversation and is used in the final transaction. If the final transaction fails or produces user-visible errors, that is a corruption failure. The repair-enabled system detects the corruption, repairs it, and the final transaction succeeds. The difference is the repair success rate.

In production, log every corruption detection event: the field name, the corrupted value, the repaired value, the turn number when corruption occurred, the turn number when corruption was detected, and whether the repair was silent or acknowledged. Also log the user's response to acknowledged repairs. Did they confirm the repaired value, correct it again, or express confusion? Repair success is defined as: the repaired value is used in the final transaction and the user does not issue a subsequent correction.

The detection latency is the number of turns between corruption and detection. If corruption happens in turn five and is detected in turn six, latency is one turn. If corruption happens in turn five and is detected in turn fifteen, latency is ten turns. Low latency is critical. Corruption detected within two turns can often be repaired silently because the corrupted value has not been confirmed yet. Corruption detected after five turns usually requires acknowledged repair because the corrupted value has been referenced multiple times.

The repair latency is the time between detection and repair. Detection happens immediately when a state update triggers validation. Repair happens on the next system turn. If validation detects corruption mid-turn, repair is deferred until the turn completes. This adds 500 milliseconds to 2 seconds of latency. Users do not notice because repair happens before the next system response. If repair requires re-extraction or user confirmation, latency increases to 2 to 5 seconds. This is noticeable but acceptable if framed as the system "double-checking" information.

The false positive rate is the percentage of detected corruption events where the flagged value was actually correct. If the validator flags a field as corrupted, but user feedback confirms it is correct, that is a false positive. High false positive rates lead to unnecessary repairs and acknowledged corrections, eroding user trust. Target false positive rate is below 5 percent. Above 10 percent, the validator is too aggressive and needs tuning.

## When Repair Fails: Escalation to Full Reset

Some corruption cannot be repaired. The state is so internally inconsistent that fixing one field breaks others. The user has corrected the same field three times and it keeps reverting to corrupted values. External systems are returning conflicting data and the system cannot determine ground truth. In these cases, the repair mechanism escalates to full state reset.

Full state reset discards all current state and restarts the conversation from scratch. The system says "I'm having trouble keeping track of the details — let's start over to make sure I get everything right." The user re-provides all information. State is rebuilt from zero. This is the nuclear option. It guarantees clean state at the cost of user frustration and wasted time. Full reset is only acceptable if the alternative is a high-stakes error that could harm the user.

The escalation trigger is a corruption count threshold. If three or more fields are flagged as corrupted in a five-turn window, escalate to full reset. If the same field is corrupted and repaired three times, escalate to full reset. If a repair is acknowledged and the user immediately corrects the repaired value, indicating the repair was also wrong, escalate to full reset. The threshold is tuned based on conversation stakes. High-stakes conversations have lower thresholds. Low-stakes conversations tolerate more repair attempts before resetting.

An alternative to full reset is partial reset. Instead of discarding all state, the system identifies the corrupted subsystem and resets only that portion. If appointment details are corrupted but patient identity and insurance information are clean, reset only the appointment fields and keep the patient and insurance fields. The user is asked to re-confirm appointment time, location, and type, but not their name, date of birth, or insurance provider. Partial reset is faster than full reset and less frustrating for users.

State repair is not a substitute for preventing corruption. The system that repairs well still fails if corruption happens constantly. The goal is low corruption rates — less than 1 percent of state updates — and high repair effectiveness — greater than 95 percent of detected corruption successfully repaired. Achieving this requires investment in validation logic, extraction accuracy, ASR quality, and consistency enforcement. Repair mechanisms are the safety net, not the primary strategy. The next subchapter covers the confirmation-before-commit pattern — the safest way to prevent corruption from reaching production state.
