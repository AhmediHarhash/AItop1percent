# 2.7 — Voice Activity Detection: The First Gate in Every Pipeline

Before your system can transcribe speech, it needs to know when speech is happening. Voice Activity Detection is the component that makes that decision. VAD runs continuously on the audio stream and outputs a binary signal — speech or no speech. When VAD detects speech, the pipeline activates. When VAD detects silence, the pipeline waits. Every voice system has VAD, whether you built it yourself or rely on a vendor's default. Understanding how VAD works, what it costs, and where it fails is mandatory for anyone shipping voice products.

VAD is the first gate in the pipeline. If VAD is wrong, everything downstream is wrong. A false positive activates the pipeline when no one is speaking — the system transcribes background noise, the LLM generates a nonsense response, the user hears the assistant interrupt them. A false negative misses the start of speech — the user's first syllable is lost, the transcription is incomplete, the assistant asks the user to repeat themselves. VAD errors are not subtle. The user notices immediately.

## How VAD Works

Voice Activity Detection models analyze audio features to distinguish speech from non-speech. Early VAD systems in the 2010s used signal processing techniques — energy thresholds, zero-crossing rates, spectral analysis. If the audio signal's energy exceeded a threshold and the frequency content matched typical human speech, VAD classified it as speech. These systems were fast and deterministic but fragile. Background noise, music, HVAC hum, and other people talking in the background all triggered false positives.

Modern VAD in 2026 uses machine learning models trained on labeled audio data. The model learns to distinguish speech from non-speech even in noisy environments. WebRTC VAD, Silero VAD, and Picovoice Cobra are common open-source and commercial VAD models deployed in production. They run on the client device or in the cloud, depending on latency and privacy constraints. Cloud-based VAD adds network round-trip time. Device-based VAD adds CPU and battery load but avoids the network hop.

VAD models in 2026 operate on audio chunks — typically 10 to 30 milliseconds per chunk. The model receives a chunk, outputs a probability that the chunk contains speech, and compares that probability to a threshold. If the probability exceeds the threshold, the chunk is classified as speech. If it does not, the chunk is classified as silence. The threshold is tunable. A low threshold makes VAD more sensitive — it detects quiet speech and speech mixed with noise, but it also triggers on background sounds. A high threshold makes VAD more conservative — it ignores background noise, but it also misses soft-spoken users and the beginnings of utterances.

The decision is not made on a single chunk. VAD systems use temporal smoothing — they require several consecutive chunks to cross the threshold before classifying the segment as speech. This reduces false positives from transient noise like a door closing or a cough. But it also adds latency. If VAD waits for three consecutive chunks at 20 milliseconds per chunk before activating, it adds 60 milliseconds to the start-of-speech detection time. That 60 milliseconds is on top of the ASR latency, the LLM latency, and the TTS latency. It is part of the total time-to-first-audio budget.

## What Happens When VAD Is Wrong

False positives activate the pipeline when no one is speaking. The most common false positive trigger is background conversation. Two people are talking near the device. The user is silent, but VAD hears speech and activates. The ASR transcribes the background conversation. The LLM generates a response to words it was not supposed to hear. The user is interrupted by the assistant responding to someone else's voice.

In a customer service call center, false positives create dead air. The agent is typing notes, not speaking. VAD detects keyboard noise as speech, activates the pipeline, transcribes nothing useful, and the system prompts the agent with "I didn't catch that." The agent ignores it, keeps typing, and the cycle repeats. Over a 10-minute call, this can happen a dozen times. The agent and the customer both hear interruptions. The call takes longer. The user experience degrades.

False negatives miss the start of speech. The user says "Hello," but VAD does not activate until the second syllable. The ASR receives "llo" instead of "Hello." The transcription is incomplete. The LLM sees partial input and either asks for clarification or guesses what the user meant. Both outcomes are bad. Asking for clarification frustrates the user — they already spoke clearly. Guessing what the user meant risks responding to the wrong intent.

False negatives are more common with soft-spoken users, users with speech impairments, and users in noisy environments where speech is quieter than background noise. A user calling from a busy street says "I need help" but a truck drives by at the same moment. The VAD threshold is set to avoid false positives from traffic noise, so it misses the user's speech. The user has to repeat themselves. After three repetitions, the user gives up and hangs up. The company loses the interaction.

The cost of VAD errors is measured in user frustration and system reliability. A voice assistant that interrupts users arbitrarily or misses the start of their sentences feels broken. Users stop trusting it. They speak louder, more slowly, more carefully — compensating for the system's failures. That compensation is cognitive load. The system was supposed to make the interaction easier. VAD errors make it harder.

## The Latency VAD Adds

VAD runs continuously, but it only adds latency to the critical path when it delays the start-of-speech detection. The latency VAD contributes is the time between when the user starts speaking and when VAD signals that speech has started. This is the sum of the chunk size, the temporal smoothing window, and the processing time.

A typical VAD configuration in 2026 uses 20-millisecond chunks with a three-chunk smoothing window. When the user starts speaking, VAD processes the first chunk in 20 milliseconds, the second chunk in 40 milliseconds, and the third chunk in 60 milliseconds. If all three chunks exceed the threshold, VAD signals speech detection at 60 milliseconds. Add 5 to 10 milliseconds for model inference time, and the total VAD latency is 65 to 70 milliseconds. That is before ASR even starts.

The latency cost is higher when VAD is configured conservatively to avoid false positives. A five-chunk smoothing window adds 100 milliseconds. A 30-millisecond chunk size with a four-chunk window adds 120 milliseconds. These configurations are necessary in noisy environments, but they make the system feel slower. The user finishes speaking, pauses, and waits. The system is still running VAD, waiting for enough silence chunks to confirm end-of-turn. The user perceives delay.

Device-based VAD has lower latency than cloud-based VAD because it eliminates the network round-trip. If VAD runs on the user's phone or smart speaker, the speech detection decision happens locally. The audio stream only gets sent to the cloud after VAD confirms speech. This saves 20 to 50 milliseconds depending on network conditions. But device-based VAD increases client-side CPU usage and drains battery. Mobile applications that run VAD continuously in the background can reduce battery life by 10 to 15 percent. The tradeoff is latency versus battery.

Cloud-based VAD is common in 2026 for server-controlled pipelines where the device is just a microphone. The device streams raw audio to the cloud, and the cloud runs VAD, ASR, LLM, and TTS. This architecture centralizes processing and simplifies client deployment, but it adds network latency to every decision. If the user's connection has 40 milliseconds of round-trip time, cloud-based VAD adds 40 milliseconds to the start-of-speech detection time. If the connection is unstable, VAD latency becomes unpredictable.

## Tuning VAD for Your Use Case

The VAD threshold determines sensitivity. A low threshold detects more speech, including quiet speech and speech mixed with noise. A high threshold ignores more non-speech, including quiet speech. The threshold is not one-size-fits-all. The right threshold depends on the environment, the user population, and the cost of false positives versus false negatives.

In a quiet environment — a user at home talking to a voice assistant with no background noise — a low threshold works. The VAD can afford to be sensitive because there is little non-speech audio to trigger false positives. In a noisy environment — a user in a car, on a street, in an office — a high threshold is necessary. The VAD needs to ignore road noise, other conversations, HVAC systems, and keyboard sounds. The tradeoff is that a high threshold also ignores soft-spoken users.

The cost of false positives versus false negatives is domain-specific. In a voice assistant application where interruptions are annoying but not costly, false positives are tolerable. In a medical dictation application where missing the start of a sentence loses critical information, false negatives are unacceptable. Tune VAD to the failure mode you cannot afford.

The tuning process is empirical. Collect real user audio, label where speech starts and ends, run VAD with different thresholds, and measure precision and recall. Precision is the fraction of VAD-detected speech that is actually speech. Recall is the fraction of actual speech that VAD detects. A low threshold increases recall but decreases precision — more speech is detected, but more noise is misclassified as speech. A high threshold increases precision but decreases recall — less noise is misclassified, but more speech is missed.

The optimal threshold is the one that maximizes the F1 score for your cost function. If false positives and false negatives are equally bad, optimize for balanced F1. If false negatives are worse, optimize for recall even at the cost of precision. If false positives are worse, optimize for precision even at the cost of recall. The threshold that works for one use case does not work for another.

Some VAD systems in 2026 use adaptive thresholds. The model observes the ambient noise level and adjusts its threshold dynamically. In a quiet room, the threshold is low. When the user moves to a noisy street, the threshold increases. This reduces false positives from environmental noise without sacrificing sensitivity in quiet environments. Adaptive VAD is more complex to implement but more robust to changing conditions.

## VAD and Multi-Speaker Scenarios

Voice Activity Detection in multi-speaker scenarios is harder. Two people are talking near the device. VAD detects speech, but it does not know who is speaking or whether the speech is directed at the system. The pipeline activates, ASR transcribes both speakers, and the LLM receives mixed input from two different people. The response is incoherent or addressed to the wrong speaker.

The solution is not better VAD. VAD's job is to detect speech, not to identify speakers. Speaker identification is diarization, covered in the next subchapter. But VAD's behavior in multi-speaker scenarios still matters. If VAD activates on every voice in the room, the system will transcribe and respond to conversations it was not invited to join. Users perceive this as eavesdropping.

Some voice systems use wake-word detection as a gate before VAD. The user says "Hey Assistant," and only then does VAD activate. This prevents the system from responding to background conversations. The wake-word model runs continuously, but it is optimized for a single phrase. It has lower false positive rates than general VAD because it is only listening for one specific pattern. Once the wake word is detected, VAD activates and listens for the user's command. This two-stage approach reduces unintended activations.

Wake-word detection adds latency. The user must say the wake word, wait for detection, then speak their command. That wait is typically 200 to 300 milliseconds. Some users pause after the wake word out of habit, adding another 500 milliseconds. The total latency from wake word to start of transcription can be 700 milliseconds before the user even finishes their sentence. This is why some voice systems skip the wake word for follow-up turns — once the conversation is active, VAD stays on until a long silence or an explicit end-of-turn signal.

## Production Lessons

VAD is simple in theory but fragile in production. The configuration that works in your test lab does not work in the real world. Test VAD in the environments where your users actually are — cars, offices, coffee shops, homes with pets and children. Collect failure cases and tune the threshold to the distribution of noise your users encounter.

Monitor VAD precision and recall in production. Log when VAD activates, what ASR transcribes, and whether the user's intent was correctly captured. If VAD precision is low — many activations that do not correspond to user speech — you are wasting compute transcribing noise. If VAD recall is low — users are complaining that the system misses the start of their sentences — your threshold is too high.

VAD latency is part of your total latency budget. Every millisecond VAD adds to start-of-speech detection is a millisecond the user waits before the system responds. Optimize VAD latency the same way you optimize ASR, LLM, and TTS latency. Reduce chunk size, minimize temporal smoothing, and run VAD as close to the user as privacy and compute constraints allow.

The next subchapter covers end-of-turn detection, the problem of knowing when the user has finished speaking and the system should respond.

