# 3.12 — Latency Budgets by Use Case: Support vs Sales vs Medical

Not all voice interactions have the same latency requirements. A customer calling to check an order status expects a fast, transactional response — query received, answer delivered, call ended. Latency above 600 milliseconds feels slow. A sales prospect exploring a complex product expects thoughtful, detailed answers. Latency of 900 milliseconds is acceptable if the answer is comprehensive and demonstrates expertise. A patient describing symptoms to a medical assistant expects careful consideration. Latency of 1200 milliseconds is tolerable if the response is thorough, accurate, and reduces the need for follow-up questions. Latency budgets are not universal. They are shaped by user expectations, task complexity, domain norms, and the perceived value of accuracy over speed.

Teams that apply a single latency target across all use cases either over-optimize low-stakes interactions or under-deliver on high-stakes ones. A system tuned for 400-millisecond customer support responses may sacrifice answer quality in medical consultations where users would gladly wait an extra 500 milliseconds for higher confidence. A system tuned for 1000-millisecond thoughtful answers in sales conversations feels sluggish and unresponsive in support contexts where users expect fast, scripted resolutions. The right latency budget depends on what the user is trying to accomplish and what they are willing to trade for speed versus depth.

This subchapter explains how to calibrate latency budgets to specific use cases, how to measure user tolerance for latency in different domains, and how to communicate latency expectations to users in ways that preserve satisfaction even when responses are slow.

## Customer Support: Transactional Speed Dominates

Customer support interactions are transactional. The user has a specific question — "Where is my package?" "How do I reset my password?" "What is my account balance?" — and expects a specific answer quickly. The interaction is not exploratory. The user is not browsing. They want information, want it now, and want to move on with their day.

In this context, latency above 600 milliseconds starts to feel slow. The user's mental model is that the answer exists in a database somewhere and the system is simply retrieving it. If retrieval takes more than half a second, the system feels inefficient. Latency above 1000 milliseconds feels broken. The user may repeat the question, assume the system did not hear them, or hang up and try a different channel.

Accuracy requirements in customer support are high for factual queries and lower for tone and nuance. If the user asks for their order status, the system must return the correct status — "shipped," "delivered," "pending" — with 100% accuracy. But the system does not need to craft a beautifully worded response. "Your order shipped yesterday and will arrive Friday" is sufficient. "I am pleased to inform you that your order has been dispatched and is currently in transit, with an expected delivery date of Friday" is unnecessary and adds latency without adding value.

This creates an opportunity to optimize aggressively for latency. Use the fastest available models. Reduce context window size. Avoid retrieval if the answer can be inferred from structured data. Skip complex reasoning. The user does not value reasoning — they value the answer. A system that responds in 450 milliseconds with a factual answer beats a system that responds in 900 milliseconds with a thoughtful, nuanced answer.

Filler audio is less tolerable in customer support contexts. If the system says "Let me check on that for you..." before every query, the user perceives the system as slow and inefficient. The user expects instant answers. Filler audio should be reserved for queries that genuinely require backend lookups — checking inventory, querying external APIs, retrieving account details from a database. For queries the system can answer from cached data or model knowledge, skip the filler and deliver the answer directly.

## Sales Conversations: Depth and Expertise Justify Latency

Sales interactions are exploratory. The prospect is evaluating options, comparing products, asking hypotheticals. They want the system to demonstrate expertise, provide detailed explanations, and help them understand tradeoffs. The interaction is not transactional — it is consultative. The prospect expects the system to think, not just retrieve.

In this context, latency up to 900 milliseconds is acceptable if the response demonstrates knowledge and addresses the user's underlying concern. A prospect asks "What is the difference between your Pro and Enterprise plans?" A 400-millisecond response that says "Pro includes basic features, Enterprise includes advanced features" is fast but unhelpful. A 850-millisecond response that explains the specific features, use cases, and pricing structure is slower but valuable. The prospect tolerates the latency because the answer helped them make a decision.

Filler audio is more effective in sales contexts. A phrase like "Let me walk you through the differences..." sets an expectation that the system is formulating a thoughtful answer. The prospect interprets the pause as the system considering their question carefully, not as the system being slow. This is a perceptual reframe: the same 800 milliseconds of latency that feels like a failure in customer support feels like attentiveness in sales.

Sales conversations also tolerate higher variance in latency. A simple question — "What is your pricing?" — can be answered in 400 milliseconds. A complex question — "How does your product integrate with Salesforce, and what are the security implications for our compliance team?" — might take 1200 milliseconds. The prospect expects complex questions to take longer. If every question took 1200 milliseconds, the system would feel slow. If simple questions are fast and complex questions are slower, the variance feels appropriate.

The tradeoff is that sales systems must balance speed with depth. If latency consistently exceeds 1200 milliseconds, prospects lose patience. If responses are shallow and generic, prospects lose interest. The system must deliver thoughtful, detailed answers within 800-1000 milliseconds for most queries. This often requires using more powerful models, larger context windows, and retrieval systems that surface detailed product information — all of which add latency. The business value of closing a sale justifies the latency cost, as long as latency does not exceed the prospect's patience threshold.

## Medical Consultations: Accuracy Justifies Higher Latency

Medical consultations have the highest stakes and the most complex information needs. A patient describes symptoms. The system must consider differential diagnoses, ask clarifying questions, provide guidance, and potentially triage to urgent care or emergency services. Errors are not just inconvenient — they are dangerous. The user expects the system to be careful, thorough, and accurate.

In this context, latency up to 1200 milliseconds is acceptable if the response is comprehensive and instills confidence. A patient asks "I have a headache and blurred vision — should I be concerned?" A 500-millisecond response that says "You should see a doctor" is fast but unhelpful and potentially negligent if the symptoms indicate a stroke. A 1100-millisecond response that asks clarifying questions — "Did the symptoms start suddenly or gradually? Do you have any other symptoms like weakness or difficulty speaking?" — is slower but safer and more useful.

Medical users tolerate higher latency because they value accuracy over speed. They would rather wait an extra second and get a thorough assessment than receive an instant answer that is incomplete or wrong. This does not mean latency is unlimited — if every query takes 2500 milliseconds, the conversation feels stalled. But the perceptual threshold is higher than in customer support or sales. A well-designed medical voice assistant can operate at 900-1200 milliseconds median latency without degrading user satisfaction, as long as responses are accurate and demonstrate clinical reasoning.

Filler audio is essential in medical contexts. A phrase like "Let me review your symptoms carefully..." or "I want to make sure I understand your situation..." signals that the system is taking the user's concern seriously. The pause is perceived as diligence, not delay. This is the opposite of customer support, where filler audio signals inefficiency. In medical contexts, rushing feels reckless. Taking time feels responsible.

The system must also manage user anxiety during latency. A patient describing chest pain and waiting 1200 milliseconds for a response experiences that wait differently than a prospect waiting 1200 milliseconds for product information. The patient is anxious. The silence amplifies anxiety. Filler audio — "I am analyzing your symptoms now..." — reduces anxiety by confirming the system is working. This is not just a UX consideration. It is a clinical consideration. Reducing patient anxiety during triage improves outcomes and satisfaction.

## Technical Support: Complexity Tolerance Varies by User Expertise

Technical support spans a wide range of user expertise and question complexity. A novice user asking "How do I log in?" expects a fast, simple answer. An expert user asking "Why is my API returning 429 errors intermittently during peak load?" expects a detailed, technical response that may take time to formulate.

For novice users and simple questions, latency budgets resemble customer support: 500-700 milliseconds is acceptable, 1000 milliseconds feels slow. For expert users and complex questions, latency budgets resemble sales or medical consultations: 900-1200 milliseconds is acceptable if the response demonstrates technical depth.

The challenge is that the system must infer user expertise and question complexity in real-time. If the system treats every question as complex and takes 1000 milliseconds to respond, novice users with simple questions become frustrated. If the system treats every question as simple and responds in 500 milliseconds, expert users with complex questions receive shallow, unhelpful answers.

One approach: classify queries by complexity during ASR. If the query contains technical jargon, references specific error codes, or exceeds a certain length, classify it as complex and allocate a higher latency budget. If the query is short and uses non-technical language, classify it as simple and optimize for speed. This classification runs in parallel with ASR and completes before LLM inference begins, allowing the system to choose the appropriate model and latency target.

Another approach: use adaptive filler audio. For simple queries, skip filler and respond directly. For complex queries, play a filler phrase like "Let me pull up the details on that..." to signal that the system is performing a deeper lookup or analysis. The user's perception of latency is shaped by the filler, even though objective latency is identical.

## Asynchronous Use Cases: Latency Is Irrelevant

Not all voice interactions are real-time. Voicemail transcription, recorded meeting summarization, voice memos converted to text — these are asynchronous. The user speaks, then waits seconds, minutes, or hours for the result. Latency in the conversational sense is irrelevant. The user does not expect instant feedback.

In asynchronous contexts, optimize for cost and quality, not latency. Use batch processing. Use slower, more accurate models. Run processing during off-peak hours when compute is cheaper. A voicemail transcription that takes 30 seconds to complete is indistinguishable to the user from one that takes 3 seconds, because the user is not waiting in real-time. The system can take as long as needed to maximize accuracy.

The exception is user-initiated asynchronous tasks where the user is waiting for the result before taking the next action. A user uploads a voice memo and clicks "Transcribe Now" expects the result within a few seconds, not a few minutes. In this case, the interaction is synchronous from the user's perspective even though the task is asynchronous from a system architecture perspective. Treat it as a real-time task with a latency budget of 3-5 seconds — longer than conversational latency but shorter than true batch processing.

## Communicating Latency Expectations to Users

Users tolerate latency better when they know what to expect. If the system explicitly communicates that complex questions take longer, users adjust their expectations and tolerate higher latency without frustration.

One technique: differentiate response types upfront. When the user asks a question, the system classifies it as "quick answer" or "detailed answer" and tells the user: "This is a quick one — give me a moment." versus "Let me put together a detailed response for you." The user now has a mental model for why latency varies. The same 900 milliseconds feels fast for a "detailed answer" and slow for a "quick answer."

Another technique: set domain-specific expectations during onboarding. A medical voice assistant can tell users during the first interaction: "I take time to carefully review your symptoms before responding. This ensures I give you accurate guidance." The user now interprets latency as a feature, not a bug. A customer support bot can tell users: "I am optimized for fast answers to common questions. If your question is complex, I can transfer you to a human agent." The user now expects fast responses and knows when to escalate.

Some systems display progress indicators during latency. A visual interface shows "Analyzing your question..." or "Searching knowledge base..." while the LLM generates. This does not reduce latency, but it reduces perceived latency by giving the user feedback that the system is working. Voice-only systems can use audio progress indicators — brief tones, periodic acknowledgments like "Still checking..." — to prevent the user from assuming the system is frozen.

## Measuring User Tolerance by Use Case

You cannot assume latency tolerance. You must measure it. Run experiments where different user cohorts experience different latency profiles and measure satisfaction, task completion, and repeat usage.

For customer support, test latency at 400ms, 600ms, 800ms, and 1000ms. Measure task completion rate and user satisfaction at each level. If satisfaction drops significantly between 600ms and 800ms, 600ms is your target. If satisfaction is flat between 400ms and 600ms, you are over-optimizing and can accept 600ms to reduce infrastructure costs.

For sales conversations, test latency at 600ms, 900ms, 1200ms, and 1500ms. Measure conversion rate — the fraction of prospects who complete a purchase or request a demo. If conversion rate is identical at 900ms and 1200ms but drops at 1500ms, 1200ms is your ceiling. If conversion is higher at 900ms than 1200ms, the latency difference is affecting sales outcomes and you must optimize further.

For medical consultations, measure user trust and perceived accuracy by latency band. Ask users after each interaction: "Did the system seem thorough and careful?" Compare responses for latency bands 800-1000ms, 1000-1200ms, and 1200-1500ms. If perceived thoroughness increases with latency up to 1200ms but flattens beyond that, 1200ms is the optimal tradeoff: users perceive the system as more diligent without feeling impatient.

## Adapting Latency Budgets as User Expectations Shift

User tolerance for latency is not static. It evolves as competing systems improve, as users become accustomed to faster interactions in other domains, and as voice interfaces become more ubiquitous. A latency budget that was acceptable in 2024 may feel slow in 2026 as users compare your system to faster alternatives.

Track user satisfaction and abandonment rates over time. If satisfaction declines or abandonment increases without changes to your system, external expectations may have shifted. A competitor may have launched a faster system. Users may have adopted faster voice assistants in other contexts and now expect similar performance from yours. You must re-calibrate your latency budget to match evolved expectations.

Benchmark against competing systems. If your customer support bot runs at 650 milliseconds median latency and a competitor launches at 450 milliseconds, users will perceive your system as slower even if your absolute latency has not changed. Your latency budget must account for competitive pressure, not just internal targets.

User expectations also vary by geography and demographic. Younger users who grew up with instant-response mobile apps tolerate less latency than older users who are accustomed to slower interactive systems. Users in high-bandwidth regions expect faster responses than users in low-bandwidth regions. Segment latency budgets by user cohort and adjust targets to match the expectations of each segment.

Latency budgets are not arbitrary targets. They are derived from user expectations, shaped by use case, and validated through measurement. A voice system that treats all interactions as equivalent will either over-invest in latency for low-stakes tasks or under-deliver for high-stakes ones. The next subchapter explores how to balance these tradeoffs across an entire product, building systems that dynamically allocate latency budgets based on the user's context and intent.
