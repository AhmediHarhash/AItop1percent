# 2.5 — The Cascading Architecture: Visibility vs Speed Tradeoffs

Every voice AI system faces a fundamental choice. Use separate providers for ASR, LLM, and TTS—the cascading architecture—or use an integrated end-to-end provider like OpenAI's Realtime API. The cascading architecture gives you visibility, control, and flexibility. You see exactly what the ASR transcribed. You see exactly what the LLM generated. You can swap providers, optimize each stage independently, and debug failures precisely. The cost is latency. Every handoff between providers adds 30 to 150 milliseconds of overhead. Three handoffs add 90 to 450 milliseconds to your total response time. For a 2-second latency budget, this is 4.5 to 22.5 percent of your time spent on integration glue.

In late 2024, a financial services company built a voice assistant using the cascading architecture. Deepgram for ASR, GPT-4o for LLM, ElevenLabs for TTS. Every component was best-in-class. The end-to-end latency was 2.8 seconds. They switched to OpenAI's Realtime API, which bundles ASR, LLM, and TTS into a single integrated pipeline. Latency dropped to 1.9 seconds. The difference was not in component speed. It was in handoff elimination. The integrated API processes audio directly without transcribing to text, generates responses without explicit token streaming, and synthesizes audio without waiting for complete text. The pipeline has no visible boundaries. It also has no visibility. When a response is wrong, the team cannot see whether the ASR misheard the user, the LLM misunderstood the query, or the TTS mispronounced a word. Debugging is guesswork. They traded latency for opacity.

They eventually switched back to cascading. The 900 milliseconds of latency savings was not worth the loss of control. But the decision was not obvious. For some use cases, integrated pipelines are the right choice. For others, cascading is mandatory. Understanding the trade-off is not optional.

## The Cascading Pipeline: Maximum Visibility, Maximum Overhead

The cascading architecture looks like this. Audio flows from the user to the ASR provider. ASR returns text. Text flows to your backend. Your backend constructs a prompt and sends it to the LLM. LLM returns text. Text flows to the TTS provider. TTS returns audio. Audio flows to the user. There are four handoffs: user to ASR, ASR to backend, backend to LLM, LLM to TTS, TTS to user. Each handoff is a network call, a serialization step, and a potential failure point.

The visibility is complete. You log the ASR transcript. You see exactly what the user said. If the transcript is wrong, you know the ASR failed. You log the LLM prompt and response. You see exactly what context the LLM received and what it generated. If the response is off-topic, you know the LLM failed or the prompt was insufficient. You log the TTS input and output. You see exactly what text was synthesized and what audio was produced. If the audio mispronounces a word, you know the TTS failed or the text was ambiguous. When something breaks, you pinpoint the failure in minutes. You do not guess. You know.

The control is complete. You choose the best ASR provider for your use case. You choose the best LLM for your quality and latency requirements. You choose the best TTS for your naturalness and speed needs. If Deepgram releases a faster model, you switch. If a new LLM provider offers better pricing, you evaluate and migrate. If ElevenLabs raises prices, you test Cartesia and switch if quality is acceptable. You are not locked in. You optimize each stage independently.

The flexibility is complete. You can insert custom logic between stages. After ASR, you can filter profanity, correct common transcription errors, or expand abbreviations. After LLM, you can post-process responses to ensure they fit your brand voice, enforce length constraints, or sanitize sensitive information. After TTS, you can apply audio effects, normalize volume, or insert silence for pacing. The pipeline is yours. You control every transformation.

The overhead is also complete. Every network call adds latency. ASR to backend: 10 to 50 milliseconds depending on geography and network conditions. Backend to LLM: 10 to 30 milliseconds. LLM to TTS: 10 to 30 milliseconds. TTS to user: 10 to 50 milliseconds. Total handoff overhead: 40 to 160 milliseconds in the best case, 90 to 450 milliseconds if any stage experiences elevated latency or is geographically distant. For a 2-second latency budget, this overhead is significant.

Serialization and deserialization add latency. The ASR provider returns JSON. Your backend parses JSON, extracts the transcript, and formats a new JSON payload for the LLM. The LLM returns JSON. Your backend parses it, extracts the response text, and formats a new JSON payload for TTS. Each parse-and-format step takes 2 to 10 milliseconds depending on payload size and backend language. Across four handoffs, serialization adds 8 to 40 milliseconds. This is small but non-zero. It compounds with network overhead.

Database lookups add latency. After the ASR returns a transcript, your backend loads conversation history from a database to construct the LLM prompt. The database query takes 5 to 50 milliseconds depending on query complexity, database load, and indexing. If you store conversation history in a slow database or run complex joins, the query can take 100 to 300 milliseconds. This is your fault, not the cascade architecture's fault, but the cascade architecture exposes the latency. An integrated pipeline hides it.

## The Integrated Pipeline: Maximum Speed, Maximum Opacity

The integrated pipeline looks like this. Audio flows from the user to the provider. The provider processes audio, generates a response, synthesizes audio, and returns it to the user. There are two handoffs: user to provider, provider to user. The middle stages—ASR, LLM, TTS—are internal to the provider. You do not see them. You do not control them. You send audio in, you get audio out.

OpenAI's Realtime API, launched in late 2025, is the first widely-adopted integrated pipeline. You open a websocket. You stream audio. The API streams audio back. You do not see transcripts. You do not see LLM prompts or responses. You do not see TTS input. You see audio in, audio out. The latency is the lowest available in 2026 for a voice AI pipeline—1.2 to 2 seconds end-to-end in most cases. The reduction comes from eliminating handoffs and optimizing internal data flow. The ASR does not serialize to text. It emits embeddings directly to the LLM. The LLM does not serialize to text. It emits embeddings directly to the TTS. The TTS does not wait for complete sentences. It synthesizes from partial embeddings as they arrive. The pipeline is a single computational graph, not three independent services.

The speed advantage is real. Teams that migrate from cascading to integrated see latency drop 600 to 1,200 milliseconds depending on their previous architecture. For use cases where latency is the top priority and milliseconds determine success, integrated pipelines win.

The opacity is also real. When a response is wrong, you do not know why. Did the ASR mishear the user? Did the LLM generate an irrelevant response? Did the TTS mispronounce a critical word? You cannot tell. The API returns audio. The audio is wrong. You file a support ticket with the provider. They investigate. They tell you the ASR was correct but the LLM misunderstood. You ask to see the transcript and the LLM prompt. They cannot share it because their internal format is proprietary. You ask to adjust the LLM's system prompt to fix the issue. They tell you the system prompt is not user-configurable. You are stuck. You either accept the behavior or switch providers entirely.

The lock-in is severe. You cannot swap the ASR provider without switching the entire pipeline. You cannot test a new LLM without migrating off the integrated API. You cannot negotiate pricing with individual component providers because there are no individual components. The provider sets the price for the entire pipeline. If they raise prices, you pay or leave. If a competitor offers better ASR, you cannot use it without rebuilding your entire system.

The customization is limited. You cannot insert logic between stages. You cannot filter transcripts, post-process LLM responses, or apply audio effects. The pipeline is opaque. You send input, you get output. If the provider allows configuration—system prompts, voice selection, language settings—you use it. If they do not, you accept the defaults.

## When Cascading Is the Right Choice

Choose cascading when visibility matters more than latency. High-stakes use cases where errors are costly. Healthcare intake, financial advice, legal services. You must see transcripts to verify the ASR heard the user correctly. You must see LLM responses to verify the advice is appropriate. You must see TTS input to verify medical terms are pronounced correctly. Opacity is unacceptable. You accept the latency cost.

Choose cascading when flexibility matters. You need to swap providers frequently. You run A/B tests on different ASR providers. You evaluate new LLMs monthly. You negotiate pricing aggressively and switch providers when contracts renew. You cannot tolerate lock-in. You accept the integration complexity.

Choose cascading when customization is required. You must filter profanity from transcripts. You must enforce strict response length limits. You must insert compliance disclaimers into TTS output. The provider's built-in configuration is insufficient. You need full control over every transformation. You accept the engineering effort.

Choose cascading when debugging is frequent. Your use case is novel. Your users speak with unusual accents. Your domain has specialized vocabulary that ASR providers struggle with. Failures are common in the early weeks. You need to see exactly what failed so you can fix it quickly. Opacity would slow iteration to a crawl. You accept the latency cost to accelerate learning.

## When Integrated Is the Right Choice

Choose integrated when latency is the top priority. You have a 1.5-second end-to-end latency target. You cannot hit it with cascading. The handoff overhead is too high. You need the speed of an integrated pipeline. You accept opacity as the cost of speed.

Choose integrated when your use case is simple. Conversational FAQ bot. Basic voice navigation. Simple task automation. The ASR vocabulary is standard English. The LLM tasks are straightforward. The TTS pronunciation is unambiguous. Failures are rare. When they occur, users rephrase and move on. Visibility is nice to have but not critical. You accept limited debugging in exchange for faster deployment.

Choose integrated when you lack engineering resources. You are a small team. You do not have backend engineers who can build and maintain a multi-provider integration. The integrated API is a single websocket. You open it, you stream audio, you get responses. There is no backend to maintain. No database to scale. No handoff logic to debug. The simplicity is worth the lock-in.

Choose integrated when you trust the provider completely. You have used their ASR, LLM, and TTS independently. You know the quality is acceptable. You trust them to maintain quality as they integrate the pipeline. You are willing to cede control in exchange for convenience. This is rare. Most teams do not trust any provider this much. But for teams with strong existing relationships, it is viable.

## The Hybrid Approach: Cascade Where It Matters, Integrate Where It Does Not

Some teams build hybrid pipelines. They use integrated ASR and LLM from OpenAI's Realtime API but send the LLM output to a separate TTS provider. This gives them visibility into LLM responses and flexibility in TTS provider choice while still benefiting from ASR-to-LLM integration. The latency is lower than full cascading but higher than full integration. The visibility is partial. The flexibility is partial. The complexity is high because you are integrating an integrated API with a separate service.

Other teams use cascading for high-stakes queries and integrated for low-stakes queries. If a user asks about their account balance—low stakes, simple query—route to the integrated pipeline. If a user asks to transfer money—high stakes, compliance-sensitive—route to the cascading pipeline and log everything. This dual-path architecture is complex but it optimizes for both speed and safety where each matters most.

Hybrid architectures are not for everyone. They add engineering complexity. They require maintaining two pipelines. They introduce routing logic that can fail. They are appropriate for teams with significant engineering resources and use cases where the latency-visibility trade-off is unacceptable in either direction. Most teams should choose cascading or integrated and commit fully. Hybrid is for teams that cannot.

## Measuring the Trade-off: Latency Cost vs Visibility Value

Before choosing, measure. Deploy both architectures. Run 1,000 test conversations through each. Measure end-to-end latency p50, p95, and p99 for both. Measure transcription accuracy, response quality, and audio naturalness for both. Measure the frequency of failures and the time required to debug them for both. Quantify the trade-off.

If cascading adds 800 milliseconds to p95 latency but allows you to debug failures in 10 minutes instead of 3 hours, calculate the cost. If you experience 5 failures per 1,000 conversations, cascading saves you 14.5 hours of debugging time per 1,000 conversations. If your engineering time costs 150 dollars per hour, cascading saves 2,175 dollars per 1,000 conversations in debugging cost. If integrated reduces latency enough to improve user satisfaction by 5 percentage points and each percentage point of satisfaction is worth 50 dollars in lifetime value per user, integrated adds 250 dollars per 1,000 conversations in user value. The trade-off is quantified. You choose based on data, not intuition.

If your latency budget is tight and you cannot hit your target with cascading, the choice is made for you. You use integrated. If your debugging burden is high and opacity makes failures impossible to fix, the choice is made for you. You use cascading. Most teams fall in the middle. The decision is not obvious. Measure, quantify, and choose.

The next subchapter examines how to evaluate voice AI quality end-to-end—metrics that matter for conversational experience, not component benchmarks.
