# 1.4 — The Three-Component Pipeline: ASR, LLM, TTS

Every voice AI system is built on the same fundamental pipeline: audio goes in, gets transcribed to text, gets processed by a language model, gets synthesized back into audio, and goes out. ASR handles the input. LLM handles the intelligence. TTS handles the output. This pipeline is not optional or variable. It is the architecture. Understanding each component — what it does, how long it takes, what can go wrong, and how it interacts with the others — is the foundation for building voice systems that work. Teams that treat the pipeline as a black box discover its failure modes in production. Teams that understand each component design systems that stay within latency budgets, handle errors gracefully, and scale reliably.

## ASR: Automatic Speech Recognition

ASR converts audio into text. It takes the user's spoken words and produces a written transcript. In text AI, you receive text directly from the user. In voice AI, you must extract it from audio first. This extraction is not trivial. Human speech is messy. People mumble, hesitate, speak over background noise, use filler words, and pause mid-sentence. ASR must handle all of this and produce a clean, accurate transcript in real time.

The core ASR task is mapping acoustic features to phonemes, phonemes to words, and words to sentences. Modern ASR systems use neural networks trained on hundreds of thousands of hours of speech data. They learn to distinguish phonemes in different accents, recognize words in noisy environments, and infer sentence boundaries from prosody. The best ASR models in 2026 — Google Chirp, OpenAI Whisper, Assembly AI — achieve word error rates below 5% in clean audio conditions. Word error rate, or WER, is the percentage of words the system gets wrong. A 5% WER means 95% of words are correct. This sounds good until you realize that a 30-word sentence has a 78% chance of containing at least one error. ASR is accurate, but it is not perfect. Every system you build must handle ASR errors.

ASR latency comes from two sources: processing time and buffering. Processing time is how long the neural network takes to analyze the audio and produce a transcript. Buffering is how long the system waits to accumulate enough audio to process. In batch ASR, you wait for the user to finish speaking, then process the entire utterance. Buffering latency can be several seconds. In streaming ASR, you process audio in small chunks — 100 to 200 milliseconds — and produce partial transcripts continuously. Buffering latency drops to 100 milliseconds or less. Processing time depends on the model and the hardware. A lightweight ASR model running on a dedicated GPU can process 100 milliseconds of audio in 30 milliseconds. A heavier model on shared infrastructure might take 150 milliseconds. You choose the model and infrastructure based on your latency budget and accuracy requirements.

Streaming ASR introduces a complication: partial transcripts change as more audio arrives. The first chunk of audio might be transcribed as "I need to." The second chunk updates it to "I need to check." The third chunk finalizes it as "I need to check my balance." Your system must handle these updates. If you pass every partial transcript to the LLM immediately, you generate multiple responses for the same utterance, wasting compute and confusing the conversational state. If you wait for a final transcript, you reintroduce latency. The solution is to use partial transcripts to start preparing a response but only commit to that response when the transcript stabilizes. This requires logic that detects when the user has finished speaking and the transcript is unlikely to change further. Most systems use a silence threshold: if no audio is detected for 300 to 500 milliseconds, assume the user is done. This adds latency but ensures the transcript is stable before committing to a response.

## LLM: The Intelligence Layer

The LLM takes the transcript from ASR and generates a response. This is the same task as text-based AI — understanding user intent, retrieving relevant context, formulating a reply, and producing coherent language. The difference in voice AI is the latency constraint. You cannot use the largest, most capable model if it takes 800 milliseconds to generate a response. You must choose a model that balances capability and speed.

In 2026, the fastest general-purpose LLMs are GPT-5-mini, Claude Sonnet 4.5, and Gemini 3 Flash. These models generate responses in 150 to 300 milliseconds for typical conversational queries on optimized infrastructure. Larger models like GPT-5, Claude Opus 4.5, and Gemini 3 Pro produce better responses but take 400 to 700 milliseconds. The larger models are too slow for most voice applications unless you have extremely generous latency budgets or can tolerate higher percentile latencies. Most production voice systems use the fastest tier of models and compensate for lower capability with fine-tuning, prompt engineering, or constrained response formats.

LLM latency depends on several factors: model size, prompt length, response length, and infrastructure. Larger models take longer. Longer prompts take longer. Longer responses take longer. Shared infrastructure introduces queueing delays. You control these factors through design. You choose smaller models. You keep prompts concise. You constrain response length. You dedicate infrastructure to voice inference. A voice banking assistant might limit responses to 50 tokens, ensuring the LLM never spends more than 200 milliseconds generating output. A voice navigation system might use a specialized small model fine-tuned on location queries, running inference in 120 milliseconds. The key is designing the response format and model choice around the latency budget, not choosing the model first and hoping latency works out.

Streaming LLM inference allows you to start generating tokens before the full prompt is finalized. If ASR produces partial transcripts, the LLM can begin processing the partial input and generate initial tokens while waiting for the final transcript. This reduces perceived latency because the system starts responding sooner. But it introduces risk. If the partial transcript changes, the initial tokens might be wrong. The system must either commit to the wrong response or discard the tokens and regenerate, wasting time. Most systems use streaming LLM inference cautiously, starting token generation only when the partial transcript has enough stable context to generate a meaningful response.

LLM failures in voice AI usually manifest as latency spikes, not wrong answers. The model produces a correct response, but it takes 1,200 milliseconds instead of 300 milliseconds because the query was complex, the prompt was long, or the infrastructure was under load. From the user's perspective, latency spikes are failures. The system stops feeling responsive. Conversation flow breaks. You must design systems that detect and mitigate latency spikes in real time. If a query is taking too long, generate a shorter fallback response, ask the user to rephrase, or admit the delay and set expectations: "Let me think about that." This preserves conversational flow even when the LLM cannot respond within budget.

## TTS: Text-to-Speech Synthesis

TTS converts the LLM's text response into audio. It takes a sentence like "Your balance is $2,340" and generates spoken audio that says those words in a natural-sounding voice. TTS is the opposite of ASR. ASR extracts meaning from audio. TTS encodes meaning into audio. Both are hard, and both introduce latency.

Modern TTS systems use neural vocoders trained on hours of recorded speech. They learn to map text to phonemes, phonemes to acoustic features, and acoustic features to waveforms. The best TTS models in 2026 — ElevenLabs, Play.ht, OpenAI TTS, Google WaveNet — produce speech that is nearly indistinguishable from human voices in subjective listening tests. But high-quality TTS is slow. Generating one second of audio can take 200 to 400 milliseconds on a CPU, 80 to 150 milliseconds on a GPU. For a 10-second spoken response, you could wait 1.5 seconds just for synthesis. That latency is unacceptable in conversational voice AI.

Streaming TTS solves this by generating audio in small chunks and playing each chunk while the rest is still being synthesized. The system generates the first 500 milliseconds of audio, starts playback, generates the next 500 milliseconds while the first chunk plays, and continues until the full response is spoken. The user hears the first words within 150 to 250 milliseconds of the LLM finishing the response, even though the full synthesis might take several seconds. Streaming TTS is not optional in low-latency voice systems. It is the only way to fit TTS into the latency budget.

Streaming TTS introduces complexity. You must generate audio at a rate faster than real-time playback, or you run out of buffered audio and the speech stutters. If you are generating 500 milliseconds of audio every 400 milliseconds, you stay ahead of playback. If you are generating 500 milliseconds of audio every 600 milliseconds, you fall behind and the user experiences gaps. You need TTS infrastructure that can sustain real-time or faster generation rates under load. This usually means dedicated GPUs for TTS, reserved capacity, or aggressive caching of common phrases.

TTS caching is a powerful optimization. If your system frequently says the same phrases — "Your balance is," "I found," "Would you like" — you can pre-generate audio for those phrases and stitch them together with dynamically generated audio for the variable parts. A voice banking assistant might cache hundreds of common sentence fragments and only synthesize the account-specific numbers and names dynamically. This reduces TTS latency from 150 milliseconds to 30 milliseconds for cached responses. But caching requires infrastructure to store and retrieve audio snippets, logic to detect when caching is valid, and careful design to ensure cached and dynamic audio blend naturally without audible seams.

TTS quality matters more than ASR or LLM quality in user perception. Users tolerate small transcription errors. They tolerate slightly awkward phrasing from the LLM. They do not tolerate robotic, unnatural, or glitchy speech. If your TTS sounds bad, users will abandon the system even if everything else works perfectly. This creates a tension between latency and quality. Faster TTS models often produce lower-quality audio. Higher-quality models are slower. You must find the quality threshold where users find the voice acceptable and choose the fastest model that meets that threshold. In practice, this means most systems use mid-tier TTS models — not the absolute best quality, not the absolute fastest, but the best quality you can achieve within your latency budget.

## Pipeline Integration and Failure Propagation

The three components do not run independently. They are a pipeline. ASR output feeds the LLM. LLM output feeds TTS. Errors and latency propagate. An ASR error produces a bad transcript. The LLM generates a response to the wrong input. TTS speaks the wrong response. The user hears something irrelevant or nonsensical. From the user's perspective, the system failed. But the root cause was ASR, not the LLM or TTS.

Failure propagation means you must design each component to degrade gracefully and signal uncertainty. If ASR is unsure about a word, it should mark that word with low confidence. The LLM can detect low-confidence regions and ask for clarification instead of guessing. If the LLM generates a response but is uncertain about correctness, it can hedge: "I think your balance is $2,340, but let me confirm." If TTS fails to generate audio for a sentence, it can skip that sentence and continue with the next, rather than crashing the entire response.

A customer support voice AI in late 2025 experienced cascading failures from ASR errors. The system heard "cancel my subscription" as "cancel my prescription." The LLM, trained on healthcare data, confidently processed the prescription cancellation request and generated a response about medication. TTS spoke the response. The user, confused, repeated "I said subscription, not prescription." The system heard "subscription" correctly the second time but had already committed to the prescription interpretation in its conversational state. The user hung up. The team added confidence thresholds to ASR output and trained the LLM to ask clarifying questions when ASR confidence was below 85%. The cascading failure rate dropped by 74%.

Pipeline latency is additive, but it is not strictly sequential in streaming architectures. ASR, LLM, and TTS overlap. ASR produces partial transcripts while still processing audio. The LLM starts generating tokens before the transcript is final. TTS starts synthesizing before the LLM finishes the full response. This overlap reduces total latency, but it also creates coordination challenges. If the LLM revises its response mid-generation because the ASR transcript changed, TTS must discard already-synthesized audio. If TTS falls behind real-time generation, playback stutters. Managing this coordination requires state management logic that tracks what each component has committed to, what can still change, and what must be discarded if upstream components revise their output.

## Choosing Models and Infrastructure for Each Component

Every production voice AI system must choose specific models for ASR, LLM, and TTS. These choices are constrained by latency budgets, accuracy requirements, cost, and infrastructure. There is no universal best model. There is only the best model for your specific latency budget and quality threshold.

For ASR, you trade off accuracy and latency. OpenAI Whisper large is the most accurate but too slow for real-time streaming. Whisper small is faster but less accurate. AssemblyAI and Deepgram offer streaming ASR APIs optimized for low latency, achieving 100 to 150 milliseconds of processing time with WER below 8%. Google Chirp and Azure Speech Services offer similar performance. Most production systems use a managed ASR API rather than self-hosting because ASR requires significant infrastructure and tuning to run reliably at scale.

For the LLM, you trade off capability and latency. GPT-5-mini, Claude Sonnet 4.5, and Gemini 3 Flash are the standard choices for voice AI in 2026. They balance speed and quality. If your domain is narrow, a fine-tuned smaller model might outperform a general-purpose model and run faster. A voice AI for restaurant reservations might fine-tune Llama 4 Scout on reservation dialogs and achieve better task-specific performance in 140 milliseconds than GPT-5-mini achieves in 250 milliseconds.

For TTS, you trade off voice quality and latency. ElevenLabs and Play.ht offer the highest-quality voices but at 120 to 180 milliseconds of latency for streaming synthesis. OpenAI TTS and Google WaveNet are slightly faster at 80 to 120 milliseconds with slightly lower quality. Coqui and Bark are open-source options that you can self-host and optimize, but they require significant engineering effort to tune for production latency and quality.

Infrastructure choices matter as much as model choices. Running ASR, LLM, and TTS on the same GPU pool introduces queueing delays. Running them on separate pools increases cost but reduces latency variance. Running inference geographically close to users reduces network latency. Using edge deployment for ASR and TTS while keeping the LLM in the cloud balances latency and cost. A voice assistant with global users might deploy ASR and TTS at edge locations in 15 regions but run LLM inference in three central data centers. This keeps ASR and TTS latency low while accepting slightly higher LLM latency for the benefit of centralized model management.

The three-component pipeline is the architecture. Every voice AI system has it. Understanding what each component does, how much time it consumes, and how failures propagate is the foundation for everything else in voice AI. The next question is how to design systems where each component operates within its latency budget while maintaining the accuracy and quality users expect.

