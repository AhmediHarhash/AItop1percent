# 2.8 â€” Data Versioning, Lineage Tracking, and Reproducibility

In September 2025, a fintech company discovered that their fine-tuned Claude model for transaction categorization had degraded significantly over the previous two months. The model had been classifying transactions into 18 categories with 91% accuracy in July, but by September accuracy had dropped to 84% and customer complaints about miscategorization had increased by 40%. The engineering team needed to investigate. They needed to compare the current training data to the July training data to understand what had changed. They needed to identify which new examples had been added, which examples had been relabeled, and whether any data transformations had been modified. They needed this information to debug the regression and decide whether to roll back to the July model or fix the data and retrain.

The problem was that none of this information existed. The training data lived in a cloud storage bucket that was overwritten every time new data was added. There were no snapshots, no version tags, no change logs. The team had a vague memory that someone had added 2,000 new examples in August and that someone else had updated the labeling guidelines for three categories, but they could not verify these changes or identify which specific examples were affected. They could not reproduce the July model because they no longer had the exact training data that produced it. They could not diff the datasets because there was no historical record. After two weeks of investigation, they made an educated guess that the labeling guideline changes had introduced inconsistencies, and they spent another three weeks re-labeling 4,500 examples and retraining the model. The total cost of the regression was $180,000 in engineering time, plus the customer impact and support load during the degraded period. The entire incident could have been avoided with basic data versioning.

This is not an edge case. This is the default state of most fine-tuning projects. Training data is treated as a mutable artifact that gets updated in place, with no history, no lineage tracking, and no reproducibility guarantees. This works until it does not, and when it fails, you have no tools to debug it. You need to version your training data with the same rigor you version your code. You need to track lineage so you know where each example came from and what transformations were applied. You need to ensure reproducibility so you can recreate any historical model or dataset on demand. This is not optional infrastructure. This is the foundation that makes debugging, compliance, and continuous improvement possible.

## Why Data Versioning Matters

Data versioning solves four critical problems: debugging, reproducibility, compliance, and rollback. Debugging requires comparing current data to historical data. When a model regresses, you need to know what changed in the training data. Did new examples introduce noise? Did relabeling change the distribution? Did a data transformation break? You cannot answer these questions without versioned snapshots of your data. Reproducibility requires the ability to recreate any historical training run exactly. If you trained a model in June and deployed it to production, you must be able to retrain that exact model with that exact data if you need to verify results, reproduce a bug, or satisfy an audit. You cannot do this if the June data no longer exists or has been silently modified.

Compliance requires data lineage and auditability. The EU AI Act requires high-risk AI systems to maintain documentation of training data, including sources, selection criteria, and data quality measures. GDPR requires the ability to delete or anonymize personal data on request, which means you need to track which training examples contain which user data. HIPAA requires audit logs of who accessed what data and when. SOX requires financial institutions to maintain reproducible records of model training and validation. You cannot meet these requirements without versioned data and lineage tracking. If a regulator asks to see the training data for a model you deployed in March 2025, you need to produce the exact dataset, the exact transformations, and the exact lineage from source to training set. If you cannot, you are in violation.

Rollback requires the ability to return to a known-good state. If you deploy a model trained on v10 of your dataset and discover a critical issue in production, you need to be able to roll back to the v9 model or retrain on v9 data immediately. You cannot do this if v9 no longer exists or if you do not know which model version corresponds to which data version. Data versioning enables safe experimentation. You can try new labeling approaches, new data sources, and new transformations knowing that you can always revert if the experiment fails. Without versioning, every change is permanent and irreversible, which makes teams risk-averse and slows down iteration.

The cost of not versioning data is invisible until you need it. You do not notice the absence of versioning when everything is working. You notice it when a model fails in production and you cannot reproduce the failure in development. You notice it when a regulator asks for documentation and you have none. You notice it when you want to roll back and you cannot. By the time you notice, it is too late to fix retroactively. You must build versioning into your data pipeline from day one.

## Tools and Approaches for Data Versioning

The simplest versioning approach is **immutable snapshots in cloud storage**. Every time you create a new version of your training data, you write it to a new location with a version identifier in the path or filename. For example, training-data-v1.jsonl, training-data-v2.jsonl, training-data-v3.jsonl. Each version is immutable once written. You never overwrite or modify an existing version. When you need to make changes, you create a new version. This approach is simple, requires no special tools, and works with any cloud storage service. The downside is that it duplicates data. If v2 differs from v1 by only 100 examples out of 10,000, you still store 10,000 examples twice.

The second approach is **content-addressable storage**, where each unique training example is stored once and identified by a hash of its content. A dataset version is a manifest file that lists the hashes of all examples included in that version. If v2 differs from v1 by 100 examples, the manifest files differ by 100 hashes, but the storage layer only stores the new examples once. This approach is more efficient but requires custom tooling or a content-addressable storage system like Git LFS or DVC.

**DVC**, Data Version Control, is a popular open-source tool designed specifically for versioning datasets and models. DVC integrates with Git and cloud storage to provide version control semantics for large files. You store your dataset manifests in Git and the actual data files in cloud storage like S3, GCS, or Azure Blob. DVC tracks which data files correspond to which Git commits, so you can check out any historical commit and DVC will automatically pull the correct data files from storage. DVC also supports pipelines, so you can version not just datasets but also the transformations that produce them. This gives you full reproducibility from raw data to trained model.

**Git LFS**, Git Large File Storage, is another option. Git LFS stores large files in a remote server and replaces them with small pointer files in the Git repository. When you check out a commit, Git LFS downloads the large files corresponding to that commit. Git LFS is simpler than DVC and integrates natively with Git, but it has less sophisticated pipeline and transformation tracking. It works well if your primary need is to version datasets and you are handling transformations separately.

Cloud storage services also offer native versioning. S3, GCS, and Azure Blob all support object versioning, where every write to an object creates a new version and all historical versions are retained. You can list all versions of an object, retrieve any historical version, and set retention policies to automatically delete old versions after a specified time. Cloud storage versioning is simple and requires no additional tools, but it does not integrate with your Git workflow and does not provide semantic versioning or human-readable version identifiers. You get timestamps and version IDs, but not v1, v2, v3 labels that correspond to meaningful milestones.

The best approach depends on your team size, technical maturity, and existing infrastructure. If you are a small team with minimal infrastructure, start with immutable snapshots and explicit version identifiers in filenames. If you are using Git for code and want to keep data versioning aligned with code versioning, use DVC or Git LFS. If you are building custom infrastructure and want maximum control, implement content-addressable storage with a custom manifest system. The key is to choose something and use it consistently. Any versioning system is better than no versioning system.

## Lineage Tracking: Where Data Came From and What Happened to It

Versioning tells you what the data looked like at a given point in time. Lineage tracking tells you where the data came from, what transformations were applied, and how it evolved from source to training set. Lineage is critical for debugging, compliance, and reproducibility. If a training example is causing problems, you need to trace it back to its source. Did it come from production logs, user-submitted feedback, synthetic generation, or manual labeling? What transformations were applied? Was it filtered, augmented, anonymized, or relabeled? If you cannot answer these questions, you cannot debug the problem or prevent it from happening again.

The foundational concept in lineage tracking is the **data pipeline graph**. A data pipeline graph is a directed acyclic graph where nodes represent datasets or transformations and edges represent data flow. The source nodes are raw data sources like production logs, user uploads, or third-party datasets. Intermediate nodes are transformation steps like filtering, anonymization, labeling, augmentation, or deduplication. The sink node is the final training dataset. Each node has metadata describing what it does and what parameters it used. Each edge has metadata describing what data flowed through it and when.

You can implement lineage tracking manually by maintaining a data pipeline specification file that documents each step in your pipeline. This file describes the source datasets, the transformation scripts, the parameters used, and the output datasets. When you run the pipeline, you log the execution with timestamps, input and output file paths, and any errors or warnings. This gives you a human-readable record of lineage, but it requires discipline to maintain and does not enforce consistency.

A better approach is to use a **workflow orchestration tool** that tracks lineage automatically. Tools like Apache Airflow, Prefect, Dagster, and Kubeflow Pipelines allow you to define data pipelines as code, execute them, and automatically track lineage. Each task in the pipeline is a node in the graph. The orchestration tool records when each task ran, what inputs it consumed, what outputs it produced, and whether it succeeded or failed. You can query the lineage graph to see the history of any dataset or trace any training example back to its source.

DVC also supports lineage tracking through its pipeline feature. You define your data pipeline as a series of stages, where each stage has inputs, outputs, and a command to execute. DVC tracks dependencies between stages and automatically rebuilds downstream stages when upstream data changes. DVC also records the exact command, parameters, and code version used in each stage, so you can reproduce any historical pipeline execution exactly.

The minimum viable lineage tracking system includes three components: a record of source datasets with timestamps and versions, a record of transformation scripts with code versions and parameters, and a record of which transformations were applied to which sources to produce which outputs. You can implement this with a simple JSON or YAML file that gets versioned alongside your training data. Each time you run your data pipeline, you update the lineage file with the new execution details. This is not as automated as a workflow orchestration tool, but it provides the essential information needed for debugging and compliance.

## The Data Card Concept

A **data card** is a standardized document that describes a training dataset. Data cards are inspired by model cards, which describe trained models. A data card includes metadata about the dataset's purpose, source, collection methodology, labeling process, quality metrics, known limitations, and intended use. Data cards make datasets discoverable, understandable, and auditable. If someone needs to use a training dataset or evaluate whether it is appropriate for a task, they can read the data card to understand what the dataset contains and how it was created.

A minimal data card includes the following fields: dataset name and version, creation date, authors and maintainers, purpose and intended use, source of raw data, collection methodology, labeling guidelines and process, number of examples and class distribution, known quality issues and limitations, license and usage restrictions, and related datasets or models. More comprehensive data cards also include fairness and bias assessments, privacy and security measures, update frequency, and deprecation policy.

You should create a data card for every training dataset you version. Store the data card alongside the dataset files or in a central data registry. When you create a new version of a dataset, update the data card to reflect what changed. Data cards serve three purposes. First, they document your dataset for future you and future team members who need to understand what the data contains and how it was created. Second, they provide compliance documentation for audits and regulatory reviews. Third, they enable dataset discovery and reuse. If your organization has multiple teams working on similar tasks, data cards allow teams to discover and reuse existing datasets rather than creating redundant datasets.

The EU AI Act requires high-risk AI systems to provide transparency about training data, including sources, selection criteria, and measures to detect biases. A well-maintained data card satisfies most of these requirements. If you are subject to the AI Act, data cards are not optional. Even if you are not subject to regulation, data cards are good engineering practice. They force you to document decisions that are otherwise implicit and easily forgotten.

## Building a Data Registry

A **data registry** is a centralized catalog of all datasets in your organization, with metadata, lineage, and access controls. A data registry allows teams to discover datasets, understand their provenance, and request access. It prevents redundant data collection and ensures that datasets are used appropriately. A data registry is especially important in larger organizations where multiple teams are working on multiple fine-tuning projects and there is no central visibility into what datasets exist or how they are being used.

The simplest data registry is a shared document or wiki page that lists all datasets with links to their storage locations and data cards. This works for small teams but does not scale. A better approach is to use a purpose-built data catalog tool like Amundsen, DataHub, or Atlan. These tools provide a searchable interface for discovering datasets, automatically extract metadata from storage systems, track lineage, and integrate with access control systems. They also support data quality metrics, usage statistics, and deprecation workflows.

If you are building a custom data registry, the minimum viable version includes a database table with one row per dataset version. Each row contains the dataset name, version identifier, creation date, creator, storage location, data card reference, and status. Status indicates whether the dataset is active, deprecated, or archived. You expose this table through a web interface or API that allows teams to search for datasets, view metadata, and request access. You can extend this with lineage tracking by adding a table that records relationships between datasets, such as which datasets were used as inputs to create which output datasets.

A data registry also serves as the access control layer. Not all datasets should be accessible to all teams. Training data that contains PII, trade secrets, or sensitive business data should be restricted to authorized users. Your data registry should integrate with your organization's identity and access management system to enforce access controls. When a team requests access to a dataset, the request goes through an approval workflow that verifies the requester's business justification and ensures compliance with data governance policies.

The final component of a data registry is deprecation and retention policies. Datasets should not live forever. Old datasets that are no longer used should be archived or deleted to reduce storage costs and compliance risk. Your data registry should track when each dataset was last used and flag datasets that have not been accessed in six months or a year. You can then review flagged datasets and decide whether to archive them, delete them, or extend their retention. This ensures your data storage does not grow unbounded and reduces the risk of accidentally training on stale or inappropriate data.

## The Minimum Viable Versioning Setup

If you are starting from scratch and need to implement data versioning immediately, here is the minimum viable setup that you can deploy in one day. First, choose a cloud storage service and create a bucket dedicated to training data. Second, adopt a naming convention for dataset versions. Use semantic versioning like v1, v2, v3, or use timestamps like 2026-01-15, 2026-01-22. Third, every time you create a new training dataset, write it to a new file or directory with the version identifier in the path, and never overwrite existing versions.

Fourth, create a simple lineage tracking file, a JSON or YAML document that records the source datasets, transformation scripts, and parameters used to create each version. Store this lineage file in the same bucket as your training data. Fifth, create a data card template and fill it out for each dataset version. Store the data card alongside the dataset files. Sixth, create a shared document or wiki page that lists all dataset versions with links to their storage locations and data cards. This is your data registry.

This setup requires no special tools, no custom infrastructure, and no budget. It can be implemented with cloud storage, a text editor, and discipline. It is not as sophisticated as DVC or Airflow, but it provides the essential capabilities: immutable versioning, lineage tracking, metadata documentation, and discoverability. You can iterate and improve this setup over time by adopting better tools, automating lineage tracking, and building a proper data registry. But you can start with this minimal setup today and immediately gain the benefits of reproducibility, debugging, and compliance.

The most important principle is immutability. Once you write a dataset version, never modify it. If you need to make changes, create a new version. This single principle prevents 90% of versioning problems. The second principle is documentation. Every dataset version should have a data card and a lineage record. If you skip documentation, you will regret it later. The third principle is discoverability. Your datasets should be easy to find and understand. If datasets are scattered across cloud buckets, local disks, and personal laptops with no central catalog, they might as well not exist.

## Reproducibility in Practice

Reproducibility means that you can take a historical commit of your code, a historical version of your training data, and produce the exact same trained model. This requires versioning code, versioning data, versioning dependencies, and versioning hyperparameters. Code versioning is handled by Git. Data versioning is handled by the techniques described in this subchapter. Dependency versioning is handled by package managers like pip, conda, or poetry with locked dependency files. Hyperparameter versioning is handled by experiment tracking tools like Weights & Biases, MLflow, or Neptune, or by versioned configuration files stored in Git.

The workflow for reproducible training is as follows. First, check out the Git commit corresponding to the training run you want to reproduce. Second, use your data versioning tool to retrieve the training data version that was used in that run. If you are using DVC, this happens automatically when you check out the commit. If you are using immutable snapshots, look up the data version in your lineage file and download it from cloud storage. Third, install the exact dependencies that were used in the original run. If you have a locked dependency file, install from that file. If not, you will need to reconstruct the dependency versions from logs or experiment tracking metadata.

Fourth, load the hyperparameters from the experiment tracking system or from a versioned configuration file. Fifth, run the training script with the same data, code, dependencies, and hyperparameters. If everything is properly versioned, the output should be identical or nearly identical to the original run, within the bounds of non-deterministic GPU operations or random seeds. If the output differs significantly, you have a reproducibility problem and need to investigate which component is not properly versioned.

Reproducibility is not just for debugging. It is also essential for compliance, auditing, and knowledge transfer. If you leave your team and someone else needs to maintain your fine-tuning project, they should be able to reproduce any historical model by following your documentation and using your versioned artifacts. If a regulator audits your model, you should be able to reproduce the training run and provide evidence that the model was trained correctly. If you discover a labeling error in historical data, you should be able to retrain all affected models and measure the impact of the correction.

The next subchapter covers monitoring training data quality over time, detecting data drift, and building feedback loops that continuously improve your dataset as your product and users evolve.
