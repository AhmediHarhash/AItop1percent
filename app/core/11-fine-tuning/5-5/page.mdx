# 5.5 â€” Self-Hosted Fine-Tuning with Axolotl and Hugging Face TRL

In September 2025, a research-focused AI startup chose to build their fine-tuning infrastructure on self-hosted GPU clusters using Axolotl rather than managed platforms. The team of six ML engineers had previous experience with distributed training and access to reserved A100 capacity through their cloud provider. Over eight weeks, they built a complete fine-tuning pipeline supporting supervised fine-tuning, DPO, and PPO training modes. They processed two hundred thousand examples across twelve model families and achieved training throughput ninety-two percent of theoretical maximum on their hardware. Their monthly infrastructure costs ran eleven thousand dollars for GPU compute and storage, compared to estimated platform fees of eighteen thousand dollars for equivalent workloads on Together.ai. The team maintained the infrastructure with eight hours per week of engineering time. When they needed to implement a custom loss function for domain-specific evaluation, they modified Axolotl configuration and training scripts in two days. This level of control and cost efficiency would have been impossible on managed platforms.

Self-hosted fine-tuning with Axolotl and TRL represents the other end of the platform spectrum. You operate the infrastructure, configure the training framework, and manage the complete workflow from data processing through model deployment. You gain maximum control, transparency, and cost efficiency at the expense of operational complexity. This is not the right choice for most teams, but for teams with the expertise and workload characteristics to justify it, self-hosting delivers capabilities and economics that managed platforms cannot match.

## The Axolotl Framework: Configuration-Driven Fine-Tuning

Axolotl is an open-source fine-tuning framework that emphasizes configuration over code. You specify training parameters, model selection, data sources, and optimization settings in YAML configuration files. Axolotl handles data loading, tokenization, distributed training setup, checkpoint management, and evaluation. The framework supports dozens of model architectures, multiple training modes including supervised fine-tuning and LoRA, and integration with Hugging Face's ecosystem of models and datasets.

The configuration-driven approach makes experimentation systematic. A typical Axolotl configuration file specifies the base model identifier, training data location, sequence length, batch size, learning rate schedule, gradient accumulation steps, evaluation frequency, and checkpoint directory. You can maintain a library of configurations for different model families and training scenarios, then launch experiments by selecting the appropriate configuration. This systematization becomes valuable when you are running dozens of experiments and need reproducibility.

The model support is comprehensive. Axolotl works with Llama family models from 7B through 70B, Mistral and Mixtral variants, Qwen models, CodeLlama, DeepSeek, Yi, and many others. When a new model architecture releases, the Axolotl community typically adds support through pull requests within days or weeks. The framework handles architecture-specific details like attention mechanisms, position encodings, and tokenization automatically based on model configuration.

The training mode flexibility distinguishes Axolotl from simpler frameworks. You can run full parameter fine-tuning where all model weights update during training. You can run LoRA training where only low-rank adapter matrices train while the base model remains frozen. You can run QLoRA which combines LoRA with quantization to fit larger models in limited GPU memory. The configuration file specifies which mode to use, Axolotl handles the implementation differences.

The data pipeline integration is straightforward. Axolotl loads training data from local files, Hugging Face datasets, or cloud storage. You specify the data format: prompt-completion pairs, conversational turns, or instruction-following examples. Axolotl handles tokenization using the model's tokenizer, truncation to maximum sequence length, and batching for training. You can implement custom data preprocessing by writing Python functions that Axolotl calls during data loading.

The distributed training support uses Hugging Face Accelerate under the hood. For multi-GPU training on a single node, Axolotl configures data parallelism automatically. For multi-node training across a cluster, you specify the node count and Axolotl coordinates distributed training using the appropriate backend. The framework handles gradient synchronization, checkpoint aggregation, and fault recovery for preempted nodes.

The operational workflow follows a standard pattern. You provision GPU instances or reserve capacity on a cluster. You clone the Axolotl repository, install dependencies, and configure the environment. You prepare training data in the required format and upload it to accessible storage. You write or adapt a configuration file specifying your training parameters. You launch training by running the Axolotl CLI with your configuration file. Training progress appears in logs with loss curves, learning rate schedules, and sample outputs. When training completes, you have model checkpoints ready for evaluation and deployment.

The debugging and monitoring are command-line driven. Axolotl outputs detailed logs showing data loading steps, model initialization, distributed training setup, and training progress. You monitor GPU utilization using nvidia-smi or cluster monitoring tools. You inspect checkpoints by loading them with Hugging Face Transformers and running inference. When training fails, you read stack traces and debug configuration issues or data problems.

## Hugging Face TRL: Reinforcement Learning from Human Feedback

TRL, the Transformer Reinforcement Learning library from Hugging Face, complements Axolotl by providing training algorithms for alignment and preference learning. Where Axolotl focuses on supervised fine-tuning with configuration-driven workflows, TRL provides implementations of PPO, DPO, ORPO, and reward modeling for RLHF pipelines. The two libraries often work together: you use Axolotl for initial supervised fine-tuning, then TRL for preference tuning.

The algorithm coverage is comprehensive. TRL implements Proximal Policy Optimization for traditional RLHF workflows where a reward model scores generations and policy gradients update the model. It implements Direct Preference Optimization, which skips the reward model and learns directly from preference pairs. It implements ORPO, Odds Ratio Preference Optimization, which combines supervised learning and preference learning in a single training stage. Each algorithm has trade-offs in sample efficiency, compute requirements, and alignment quality.

The PPO implementation is production-ready but complex. You need a base model to fine-tune, a reward model to score outputs, and a reference model for KL divergence penalties. The training loop generates responses, scores them with the reward model, computes policy gradients, and updates the model while constraining divergence from the reference. This requires orchestrating four models simultaneously with careful memory management and batching. TRL handles the orchestration, but you must understand the RLHF mechanics to debug training issues.

The DPO implementation is simpler and increasingly popular. You start with a supervised fine-tuned model and a dataset of preference pairs where human annotators preferred one response over another. DPO trains the model to increase the likelihood of preferred responses and decrease the likelihood of rejected responses, using a loss function derived from the Bradley-Terry preference model. The training loop looks similar to supervised fine-tuning but uses paired examples instead of single demonstrations. This simplicity makes DPO the default choice for preference tuning unless you have specific requirements for reward modeling.

The ORPO implementation combines supervised fine-tuning and preference learning in a single pass. You provide demonstration examples showing the desired task behavior and preference pairs showing which outputs are better or worse. ORPO optimizes a joint loss that learns the task from demonstrations while learning the preference pattern from comparisons. This reduces training time compared to separate supervised fine-tuning and DPO stages, but requires carefully balanced datasets with both demonstration and preference data.

The integration with Axolotl creates end-to-end pipelines. You use Axolotl to fine-tune a base model on task-specific data using supervised learning. You use TRL to further tune that model on preference data using DPO or ORPO. You use Axolotl's infrastructure for data loading and distributed training, TRL's implementations for preference learning algorithms. The combination provides complete control over the two-stage tuning workflow that produces high-quality aligned models.

The operational complexity is higher than Axolotl alone. TRL training scripts require more code than Axolotl configurations. You write Python code to load models, initialize trainers, prepare datasets, configure training arguments, and run training loops. The abstraction level is lower, which provides more control but requires more expertise. Debugging TRL training runs requires understanding the algorithm mechanics, not just configuration parameters.

## Infrastructure Setup and Environment Management

Self-hosted fine-tuning requires deliberate infrastructure choices. You need GPU instances with sufficient memory for your model size and batch size. You need storage for datasets, checkpoints, and logs. You need network bandwidth for distributed training communication. You need orchestration for scheduling jobs and managing resources. These decisions have direct impact on training throughput, cost, and operational overhead.

The GPU selection depends on model size and training mode. For LoRA tuning of models up to 13B parameters, a single A100 40GB or A10G 24GB suffices. For full fine-tuning of 13B models or LoRA tuning of 70B models, you need A100 80GB instances. For full fine-tuning of 70B models, you need multi-GPU setups with NVLink or InfiniBand for fast inter-GPU communication. For extreme scale like 405B models, you need multi-node clusters with dozens of GPUs and sophisticated distributed training strategies.

The storage architecture matters for training performance. Datasets should live on fast storage accessible from GPU instances. For single-node training, local NVMe SSDs provide maximum throughput. For multi-node training, you need shared storage like network file systems or object storage with high-bandwidth access. Checkpoints consume significant space: a 70B model checkpoint is 140GB in FP16, and you want to save multiple checkpoints during training. Storage costs become a meaningful portion of total infrastructure costs.

The environment management requires containerization or careful dependency tracking. Axolotl and TRL depend on specific versions of PyTorch, Transformers, Accelerate, and other libraries. CUDA versions must match between PyTorch builds and GPU drivers. The safest approach is Docker containers with pinned dependencies. You build containers with Axolotl, TRL, and dependencies, push them to a registry, and launch training jobs from stable container images. This eliminates dependency conflicts and ensures reproducibility across runs.

The cluster orchestration depends on scale. For single-node training on cloud GPU instances, no orchestration is needed beyond SSH access and tmux for persistent sessions. For multi-node training or shared GPU clusters, you need a job scheduler like Slurm, Kubernetes with GPU operator, or a platform like Ray. The orchestration handles resource allocation, job queuing, distributed training coordination, and fault recovery.

The cost management requires monitoring and optimization. GPU costs dominate: A100 instances cost two to four dollars per hour depending on provider and commitment terms. A four-hour training run costs sixteen dollars for four GPUs. Running continuous experiments with multiple model sizes and hyperparameter sweeps generates thousands of dollars in monthly compute costs. Spot instances reduce costs by sixty to eighty percent but introduce preemption risk that requires checkpoint recovery logic. Reserved instances provide discounts for long-term commitments but reduce flexibility.

## Common Workflows and Patterns

The supervised fine-tuning workflow is the foundation. You start with a base model like Llama 3 8B. You prepare training data as JSONL files with prompt and completion fields. You write an Axolotl configuration specifying the model, data path, sequence length of 2048, batch size of 4, learning rate of 2e-5, and linear warmup schedule. You launch training and monitor loss curves. After three epochs totaling six hours on four A100s, you have a checkpoint fine-tuned on your task. You evaluate on held-out test data, iterate on hyperparameters, and repeat until quality meets requirements.

The LoRA fine-tuning workflow optimizes for memory and cost. You use the same configuration but enable LoRA with rank 16 and alpha 32. Axolotl freezes the base model and only trains the low-rank adapters. This reduces memory requirements by seventy percent, allowing you to fine-tune 70B models on single A100 80GB instances. Training is faster because fewer parameters update. The resulting model is the base model plus adapter weights, which you can merge into a full model or serve with adapter loading.

The preference tuning workflow adds a second stage. After supervised fine-tuning produces a task-specific model, you prepare a preference dataset with response pairs labeled by quality. You write a TRL script using the DPOTrainer class, loading your supervised fine-tuned model as the starting point and the preference dataset as training data. You configure a lower learning rate like 5e-7 and shorter training duration. DPO training runs for one or two epochs, producing a model that generates responses closer to the preferred style or quality level shown in the comparison data.

The hyperparameter sweep workflow parallelizes experiments. You write a script that generates Axolotl configurations with different learning rates, batch sizes, and LoRA ranks. You launch multiple training jobs on separate GPU instances or as separate jobs on a cluster. Each job saves checkpoints to a unique directory. After all jobs complete, you evaluate each checkpoint on validation data and select the best configuration. This parallel search is faster than sequential experimentation but requires provisioning multiple GPU instances simultaneously.

The multi-stage pipeline workflow chains operations. You use Axolotl to fine-tune a base model on broad domain data. You use Axolotl again to further fine-tune on narrow task-specific data. You use TRL to apply preference tuning. Each stage uses the previous stage's output as the starting checkpoint. This progressive specialization produces models that combine broad capabilities with task-specific behavior and aligned outputs.

## Debugging and Troubleshooting

Training failures fall into categories with recognizable patterns. Out-of-memory errors indicate batch size or model size exceeds GPU capacity. You reduce batch size, enable gradient checkpointing, switch to LoRA, or add more GPUs. Loss not decreasing indicates learning rate too low, data quality issues, or insufficient training. You increase learning rate, inspect training examples for problems, or train longer. Loss exploding indicates learning rate too high or numerical instability. You reduce learning rate or enable gradient clipping.

Data formatting errors appear during data loading. Axolotl expects specific JSON structures for prompts and completions. If your data uses different field names or nesting, tokenization fails with key errors. You either reformat your data to match Axolotl's expectations or write a custom dataset class. The error messages usually show the problematic example, making diagnosis straightforward.

Distributed training failures manifest as hanging jobs or NCCL errors. These typically indicate networking issues between nodes or mismatched distributed training configuration. You verify that nodes can communicate on the NCCL port ranges, check that the distributed backend matches your networking setup, and ensure all nodes use identical environment variables for distributed coordination. Restarting with verbose logging usually reveals the specific communication failure.

Checkpoint corruption occurs when training is interrupted during checkpoint saving. You lose the partial checkpoint and must resume from the previous complete checkpoint. The solution is frequent checkpoint saving at intervals that balance disk I/O overhead against loss of training progress on failure. Saving every hundred steps is common. Cloud object storage with atomic writes prevents partial checkpoint files.

Performance optimization focuses on GPU utilization and throughput. You monitor nvidia-smi during training to verify GPUs are saturated. Low utilization indicates data loading bottlenecks, insufficient batch size, or inefficient model parallelism. You increase the number of data loading workers, increase batch size with gradient accumulation if memory allows, or adjust parallelism strategy. The goal is keeping GPUs busy with computation rather than waiting for data or communication.

## When Self-Hosting Makes Sense

The decision to self-host fine-tuning infrastructure depends on workload volume, team expertise, control requirements, and cost sensitivity. Self-hosting makes sense when you run continuous training workloads that amortize infrastructure investment, have team members skilled in distributed training and infrastructure, need capabilities not available on managed platforms, or have cost structures where self-hosting delivers meaningful savings.

The workload volume threshold is approximately ten to twenty training runs per week with average durations of several hours. Below this volume, platform overhead amortizes better than infrastructure overhead. Above this volume, dedicated infrastructure with reserved capacity becomes cheaper than pay-per-use platform pricing. The exact break-even depends on platform rates, cloud GPU pricing, and internal engineering costs.

The team expertise requirement is non-negotiable. Self-hosted fine-tuning requires engineers who understand distributed training, GPU memory management, training algorithms, and debugging complex failures. If you are learning fine-tuning for the first time, start with managed platforms and migrate to self-hosting after you have expertise. Trying to learn both fine-tuning and infrastructure simultaneously leads to weeks of frustration and stalled projects.

The control requirements justify self-hosting when you need capabilities unavailable on platforms. Custom training algorithms, novel architectures, proprietary optimization techniques, or integration with internal systems often require the flexibility that only self-hosted infrastructure provides. If your competitive advantage depends on training innovations, you need control over the training loop.

The cost sensitivity matters most at scale. A startup running a dozen experiments per month saves nothing by self-hosting. An AI company running hundreds of training jobs per week can save hundreds of thousands of dollars annually with dedicated infrastructure. The engineering overhead of eight to sixteen hours per week is trivial compared to platform fees at that scale.

The next subchapter examines torchtune, Meta's PyTorch-native fine-tuning library that provides even more control and lower-level access than Axolotl, for teams that need to operate closest to the metal.
