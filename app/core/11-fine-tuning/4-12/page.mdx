# 4.12 — Combining Techniques: SFT Then DPO, CPT Then SFT, and Other Sequences

In mid-2025, a healthcare AI company spent four months and $180,000 fine-tuning a medical documentation model. They ran supervised fine-tuning on 40,000 clinical notes, evaluated the model, saw decent accuracy, and shipped it to 200 physicians. Within three weeks, the complaints started. The model was accurate but verbose, included unnecessary disclaimers, and often chose awkward phrasing that no physician would actually use. The team had trained the model on what was correct but had never trained it on what was preferred. They had skipped the preference alignment stage entirely, treating fine-tuning as a single-stage process. By November 2025, they had burned another $90,000 running a second fine-tuning round with DPO on preference pairs, finally achieving the tone and style physicians actually wanted. The root cause was not technical incompetence but a fundamental misunderstanding: they thought fine-tuning was one technique applied once, when in reality the most effective fine-tuning in 2026 is almost always multi-stage, with each stage solving a different adaptation problem in a specific sequence.

Fine-tuning is not a monolithic operation. It is a collection of techniques that can be combined, sequenced, and layered to achieve different adaptation goals. The dominant pattern in production systems by 2026 is sequential fine-tuning, where you apply multiple techniques in a deliberate order, each building on the last. The most common sequence is supervised fine-tuning followed by preference optimization, known colloquially as SFT-then-DPO. Another critical pattern is continued pre-training followed by supervised fine-tuning, used when you need domain adaptation before task specialization. Understanding which techniques to combine, in what order, and when to stop is the difference between efficient, effective fine-tuning and wasteful, multi-round trial-and-error that burns budget and delays launch.

This subchapter teaches you how to design multi-stage fine-tuning pipelines. You will learn why SFT-then-DPO is the 2026 standard, when to add a continued pre-training stage before SFT, how ordering effects change outcomes, how to budget across stages, and when diminishing returns signal that adding another stage is waste. This is advanced fine-tuning strategy, not beginner single-technique application.

## The SFT-Then-DPO Standard Pipeline

By 2026, the most widely deployed fine-tuning pipeline in production is SFT followed by DPO. This two-stage sequence has become the default because it cleanly separates two adaptation goals: teaching the model what is correct, then teaching it what is preferred. Supervised fine-tuning instills the task, the domain knowledge, the format, and the baseline capability. Direct preference optimization then refines tone, style, conciseness, safety posture, and other subjective quality dimensions that cannot be captured in labeled examples alone. The separation is conceptually clean and empirically effective.

You run SFT first because the model needs to learn the task structure before it can learn preferences within that structure. If you run DPO on a base model that has never seen your task, the preference pairs are meaningless because the model does not yet understand what it is being asked to do. SFT establishes the foundation: it teaches the model the input-output mapping, the domain vocabulary, the expected format. Once the model can perform the task at a baseline level of correctness, DPO refines how it performs the task, aligning it to human preferences about style, verbosity, risk posture, and edge-case handling.

The healthcare documentation company that opened this subchapter is a textbook example of the failure mode when you skip the preference stage. Their SFT phase taught the model to generate clinically accurate notes, but accuracy alone does not make a note useful. Physicians prefer concise summaries, they prefer certain phrasings, they prefer not to see defensive disclaimers on every line. These preferences are not correctness issues, they are quality-of-output issues, and SFT cannot learn them from labeled examples alone because there are many correct ways to write a note. DPO learns from preference pairs, where both options are correct but one is better. The model learns to favor the better option, internalizing physician preferences that were never formalized in the labeled data.

When you design an SFT-then-DPO pipeline, you budget your data and compute across both stages. A common allocation is 70% of labeled examples for SFT and 30% of effort for collecting preference pairs for DPO. Some teams run SFT to convergence, then collect preferences by sampling from the SFT model and having humans rank outputs. Other teams collect preferences in advance, but this requires knowing what outputs the model will produce, which you often do not know until after SFT. The most practical pattern is to run SFT first, evaluate, identify quality gaps, then collect preference data targeting those gaps, and run DPO as a refinement stage.

The SFT-then-DPO pipeline is standard for customer-facing generative tasks: chatbots, content generation, summarization, documentation, report writing, any task where subjective quality matters as much as correctness. It is less necessary for tasks where correctness is binary and preferences are irrelevant, such as classification, entity extraction, or schema-based parsing. For those tasks, SFT alone is often sufficient. The decision to add DPO is driven by whether your users care about how the model says something, not just what it says.

## Continued Pre-Training Then SFT for Domain Adaptation

Another critical multi-stage pattern is continued pre-training followed by supervised fine-tuning. This sequence is used when your task requires deep domain knowledge that the base model does not possess, and labeled task data alone is insufficient to instill that knowledge. Continued pre-training adapts the model's general world knowledge to your domain by training on large volumes of domain-specific text in an unsupervised manner. Supervised fine-tuning then specializes the adapted model to your specific task. This is the standard approach for legal, medical, scientific, financial, and other domains where terminology, reasoning patterns, and factual knowledge differ significantly from general web text.

You run CPT first because it builds domain knowledge into the model's representations before you ask it to perform domain-specific tasks. If you skip CPT and go straight to SFT, the model will learn to mimic the format of your labeled examples but will lack the deep domain understanding needed to generalize beyond the exact patterns in your training set. This results in brittle task performance: the model works on examples that closely resemble training data but fails on edge cases, novel phrasings, or domain-specific reasoning that was not explicitly demonstrated in labeled examples.

A financial services company in early 2025 fine-tuned a model for regulatory report generation. They collected 12,000 labeled examples of report drafts and ran SFT directly on a general base model. The model learned to generate reports in the correct format, but the content was often shallow, missing key regulatory concepts, and sometimes citing regulations incorrectly. The team realized the base model had minimal exposure to financial regulatory text during pre-training, so it lacked the foundational knowledge needed to reason about compliance. They restarted with a CPT stage, training on 500 million tokens of financial regulatory documents, policy memos, and compliance guidance. After CPT, they re-ran SFT on the same 12,000 examples. The resulting model generated reports with correct regulatory citations, appropriate risk language, and domain-appropriate reasoning. The CPT stage had instilled the knowledge; the SFT stage had instilled the task.

When you design a CPT-then-SFT pipeline, you allocate significantly more compute to CPT than to SFT. CPT typically requires training on hundreds of millions to billions of tokens, often 10 to 100 times more data than SFT. The cost and time investment are higher, but the payoff is a model that generalizes within the domain rather than memorizing task patterns. You run CPT until domain-specific perplexity plateaus, then you run SFT until task performance plateaus. The two stages are solving different problems and require different stopping criteria.

The CPT-then-SFT pipeline is standard for domain-heavy tasks where labeled data is expensive and domain text is abundant. It is less necessary for tasks where the domain is already well-represented in the base model's pre-training data, such as general-purpose customer support, content moderation on common topics, or summarization of news articles. The decision to add CPT is driven by whether your task requires domain knowledge that the base model does not already possess.

## Three-Stage Pipelines: CPT Then SFT Then DPO

In some cases, you need all three stages: continued pre-training, supervised fine-tuning, and preference optimization. This three-stage pipeline is the gold standard for domain-heavy, customer-facing generative tasks where correctness, domain knowledge, and subjective quality all matter. Legal contract drafting, medical report generation, scientific literature summarization, and financial advisory content are all candidates for this approach.

The sequencing is strict: CPT first, then SFT, then DPO. You cannot reorder these stages without degrading outcomes. CPT builds domain knowledge. SFT instills the task. DPO refines preferences. If you run DPO before SFT, the model has no task structure to refine. If you run SFT before CPT, the model learns task patterns without domain grounding, and CPT afterward disrupts the task patterns you just trained. The order reflects the dependency structure: preferences depend on task structure, task structure depends on domain knowledge, domain knowledge depends on nothing except data.

A legal technology company in late 2025 built a contract review assistant using a three-stage pipeline. They started with CPT on 2 billion tokens of legal contracts, case law, and statutory text, training for six weeks on eight A100 GPUs. They then ran SFT on 30,000 labeled examples of contract clauses annotated with risk assessments and recommendations, training for one week. Finally, they ran DPO on 8,000 preference pairs where lawyers ranked contract review outputs by clarity, conciseness, and appropriateness of tone, training for three days. The resulting model had deep legal knowledge from CPT, task competence from SFT, and user-preferred output style from DPO. The total training time was eight weeks, the total cost was approximately $320,000, and the model performed at a level that allowed the company to deploy it to 1,500 lawyers across 40 firms.

When you design a three-stage pipeline, you budget time and cost across all three stages and plan evaluation checkpoints between stages. After CPT, you evaluate domain knowledge by measuring perplexity on held-out domain text and running domain-specific knowledge probes. After SFT, you evaluate task performance using task-specific metrics and error analysis. After DPO, you evaluate subjective quality using human preference evaluations. Each stage has its own success criteria, and you do not proceed to the next stage until the current stage has converged.

Three-stage pipelines are expensive and time-intensive. They are justified when the task is high-value, the domain is specialized, the quality bar is high, and the alternative is unacceptable performance. They are not justified for tasks where the base model already has sufficient domain knowledge, or where correctness alone is sufficient, or where preferences are unimportant. The decision to commit to three stages is a business and strategic decision, not just a technical one.

## Ordering Effects: Why Sequence Matters

The order in which you apply fine-tuning techniques is not arbitrary. Reordering stages changes outcomes, sometimes dramatically. This is because each fine-tuning stage modifies the model's weights, and subsequent stages operate on those modified weights. If you change the starting point, you change the optimization landscape, and the final model ends up in a different part of weight space with different behaviors.

The most common ordering mistake is running DPO before SFT. Teams sometimes reason that if DPO improves output quality, they should run it first to get a better starting point for SFT. This reasoning is backwards. DPO optimizes for preferences, but preferences are only meaningful once the model understands the task. If you run DPO on a base model, you are optimizing over random or semi-random outputs, and the preference signal is noisy and unproductive. The model does not learn useful preferences; it learns to favor one kind of nonsense over another kind of nonsense. When you subsequently run SFT, the task training disrupts whatever weak preferences DPO instilled, and you end up with a model that is no better than if you had run SFT alone.

A customer support platform in mid-2025 made this mistake. They ran DPO first to train the model to prefer friendly, concise responses, then ran SFT to teach it customer support tasks. The resulting model was neither friendly nor concise nor competent at support tasks. The DPO stage had been wasted because the base model did not yet understand what a support interaction was, so the preference pairs were meaningless. The SFT stage then overwrote the weak preference patterns, leaving no residual benefit. They reran the pipeline in the correct order—SFT then DPO—and achieved the desired outcome in a single iteration.

Another ordering effect occurs when you run CPT after SFT. Some teams fine-tune a model for a task, realize it lacks domain knowledge, and then try to add CPT as a remediation step. This also fails. CPT involves training on massive amounts of unsupervised text, which shifts the model's general behavior and disrupts the task-specific patterns learned during SFT. The model ends up with domain knowledge but degraded task performance, and you have to re-run SFT to restore task competence. The correct order is CPT first, then SFT, so that task training happens on a domain-adapted base.

Ordering effects are strongest when stages differ significantly in data volume, learning rate, or training duration. CPT involves billions of tokens and large learning rates; SFT involves thousands to millions of tokens and smaller learning rates; DPO involves thousands of preference pairs and even smaller learning rates. Running them in increasing order of specificity and decreasing order of scale is the stable pattern. Running them in reverse order creates instability, where later stages undo earlier stages, and the pipeline does not converge to a coherent final model.

## Multi-Stage Training Budgets and Resource Allocation

Multi-stage fine-tuning requires careful budget allocation across stages. The total cost of a three-stage CPT-SFT-DPO pipeline can easily reach $200,000 to $500,000 for a 7B to 13B parameter model trained on high-quality data with production-grade infrastructure. You cannot treat each stage as an independent project with independent budget; you must plan the total pipeline cost and allocate resources to maximize final model quality within that total.

A common allocation pattern is 50% of compute budget to CPT, 30% to SFT, and 20% to DPO. This reflects the relative data volumes: CPT requires the most tokens, SFT requires moderate labeled examples, and DPO requires the fewest preference pairs. Within each stage, you also allocate budget to data collection, infrastructure, experimentation, and evaluation. Data collection is often the hidden cost: collecting 2 billion tokens of domain text for CPT, 30,000 labeled examples for SFT, and 8,000 preference pairs for DPO can cost as much as the compute itself, especially when domain experts are involved.

You also allocate time. A three-stage pipeline might require six to ten weeks of wall-clock time if stages are run sequentially, or four to six weeks if some data collection and infrastructure work is parallelized. Teams often underestimate the time required for evaluation between stages, which can add one to two weeks per stage. You need time to run evals, analyze errors, decide whether to proceed to the next stage or retrain the current stage, and prepare data for the next stage. Rushing through this process results in wasted training runs and misallocated budget.

Budget planning also includes contingency for retraining. Not every stage converges on the first attempt. Hyperparameter mistakes, data quality issues, and infrastructure failures all require retraining, which consumes additional budget. A realistic budget includes 20% to 30% contingency for retries. Teams that budget to the exact expected cost routinely run out of money before the pipeline is complete.

## Diminishing Returns: When to Stop Adding Stages

Multi-stage fine-tuning is subject to diminishing returns. Each additional stage improves model quality, but the improvement gets smaller with each stage. At some point, adding another stage costs more than the incremental quality gain is worth, and you should stop. Knowing when to stop is a judgment call based on eval metrics, user feedback, and business value.

A common pattern is that SFT provides the largest improvement over the base model, DPO provides a moderate improvement over SFT, and any stage beyond DPO provides only marginal improvement. For most tasks, two stages are sufficient: SFT-then-DPO if the domain is general, or CPT-then-SFT if the domain is specialized. Three stages—CPT-SFT-DPO—are justified for high-value, high-stakes tasks, but are overkill for most applications. Four or more stages are almost never justified outside of research settings.

The way to detect diminishing returns is to measure eval performance after each stage and plot the improvement curve. If the improvement from stage two to stage three is less than half the improvement from stage one to stage two, you are likely in the diminishing returns regime. If the improvement is less than the noise in your eval, you are definitely wasting resources. You stop adding stages when the cost of the next stage exceeds the business value of the incremental improvement.

A fintech company in late 2025 ran a four-stage pipeline: CPT, SFT, DPO, then a second round of SFT on a small set of high-priority examples. The first three stages each produced measurable improvements. The fourth stage produced no measurable improvement and in some cases slightly degraded performance by overfitting to the small high-priority set. They rolled back to the three-stage model and shipped that. The fourth stage had been an experiment, and the experiment failed, but because they measured performance after each stage, they detected the failure quickly and did not waste additional resources.

Diminishing returns also apply within a stage. You do not train each stage to absolute convergence; you train until the improvement rate slows below a threshold. For CPT, you stop when domain perplexity improvement per epoch drops below one percent. For SFT, you stop when task accuracy improvement per epoch drops below half a percent. For DPO, you stop when preference win rate improvement drops below two percent. These thresholds are heuristics, not laws, but they prevent overtraining and wasted compute.

## Other Multi-Stage Patterns: LoRA-Then-Full, SFT-Then-SFT, and Iterative DPO

Beyond the standard CPT-SFT-DPO sequences, there are niche multi-stage patterns used for specific situations. One is LoRA-then-full fine-tuning, where you first train a parameter-efficient LoRA adapter to explore hyperparameters and validate data quality, then retrain the model with full fine-tuning for maximum performance. This pattern is used when full fine-tuning is expensive and you want to de-risk before committing budget.

Another pattern is SFT-then-SFT, where you run supervised fine-tuning in two stages on different datasets. The first SFT stage trains on a broad task dataset to instill general task competence. The second SFT stage trains on a narrow, high-priority subset to specialize the model for critical use cases. This is used in domains where you have abundant general data and scarce high-quality data, and you want the model to be generally competent but optimized for the high-value cases.

A third pattern is iterative DPO, where you run DPO, sample from the resulting model, collect new preference pairs on those samples, and run DPO again. This creates a feedback loop where the model's outputs improve, you collect preferences on the improved outputs, and the model improves further. Iterative DPO is used when preference data is cheap to collect and model quality has not plateaued after one round. Some research suggests that two to three rounds of iterative DPO can outperform a single round, but the gains are modest and the data collection overhead is significant.

These patterns are less common than the standard sequences but are worth knowing for specialized situations. The key principle is the same: each stage solves a different problem, stages are ordered by dependency, and you stop when incremental gains no longer justify incremental costs.

## Designing Your Multi-Stage Pipeline

When you design a multi-stage fine-tuning pipeline, you start by identifying the adaptation goals: do you need domain knowledge, task competence, preference alignment, or some combination? You then map each goal to a technique: CPT for domain knowledge, SFT for task competence, DPO for preferences. You order the techniques by dependency: knowledge before task, task before preferences. You allocate budget and time across stages, plan evaluation checkpoints, and set stopping criteria for each stage.

You do not commit to the full pipeline upfront. You run the first stage, evaluate, and decide whether the next stage is justified. If SFT alone achieves acceptable performance, you skip DPO. If the base model already has sufficient domain knowledge, you skip CPT. Multi-stage pipelines are contingent plans, not fixed recipes. You adapt the plan based on empirical results at each stage.

You also plan for failure. Not every stage works on the first attempt. Hyperparameters may be wrong, data may be noisy, infrastructure may fail. You budget time and money for retries, and you design each stage to be independently re-runnable without restarting the entire pipeline. Checkpointing, versioned datasets, and modular infrastructure are essential for making multi-stage pipelines robust to failure.

By 2026, the teams that execute fine-tuning effectively are the teams that think in pipelines, not in single techniques. They combine CPT, SFT, and DPO in deliberate sequences, measure after each stage, and stop when diminishing returns set in. They allocate budget across stages, plan for retries, and adapt the pipeline based on empirical outcomes. This is the discipline that separates professional fine-tuning operations from amateur single-shot experiments.

The next subchapter addresses fine-tuning for a specific capability that has become critical in 2026: training models to reliably use tools and produce structured outputs, where fine-tuning turns unreliable prompted behavior into consistent, production-grade tool discipline.
