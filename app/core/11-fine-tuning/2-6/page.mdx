# 2.6 — Deduplication, Near-Duplicate Removal, and Contamination Checks

In June 2025, a SaaS company fine-tuned a GPT-4o model on 95,000 customer support interactions to automate tier-one ticket responses. Internal evaluations showed 94% accuracy on a held-out test set of 2,000 tickets. Two weeks after deployment, the quality team noticed that the model was generating near-identical responses to tickets that were semantically similar but not identical. Customer complaints about "robotic, copy-pasted answers" increased by 40%. An audit of the training data revealed that 31,000 of the 95,000 examples were near-duplicates—customer requests that differed only in minor phrasing but received identical responses from support agents. The model had memorized these repeated patterns rather than learning to generalize from ticket content to appropriate response. The team also discovered that 180 examples from the test set had near-duplicates in the training set, inflating the accuracy estimate by eleven percentage points. The company rebuilt the training set with aggressive deduplication, retrained the model, and saw test accuracy drop to 83% but production quality improve dramatically. The VP of Engineering later wrote in a post-mortem: "we optimized for test set performance and shipped a model that had memorized the answers."

Duplicate and near-duplicate examples in training data cause models to overweight specific patterns, memorize outputs instead of learning generalizable strategies, and perform worse on inputs that do not closely match the training distribution. Contamination—overlap between training data and test data—inflates evaluation metrics and creates false confidence in model performance. You must deduplicate your training set, remove near-duplicates, and verify that no test or validation examples leak into training. This is not optional data hygiene. It is a prerequisite for models that generalize.

## Why Duplication Breaks Generalization

Training a model on duplicated examples is equivalent to telling the model that certain input-output pairs are vastly more important than others. If an example appears five times in the training set, the model sees it five times during training and adjusts weights to minimize loss on that example five times. The model learns to reproduce that specific output for that specific input with high confidence. It does not learn to generalize the pattern to similar-but-not-identical inputs.

This problem is most severe when duplicates are exact. If the same customer support question appears 200 times in the training set with the same answer, the model memorizes the answer. When a user asks a slightly different version of the question—using a synonym, rephrasing the sentence structure, or adding context—the model often fails to recognize the match and produces a worse response than it would have if the example appeared only once. Exact duplicates waste training compute, bias the model toward memorization, and reduce performance on the long tail of inputs that do not have repeated examples.

Near-duplicates are worse. Near-duplicates are examples where the input is semantically identical or very similar but the output varies slightly, or where the input varies slightly but the output is identical. Near-duplicates teach the model inconsistent behavior. If the training set contains ten examples where the input is "how do I reset my password" with ten different phrasings and ten different responses—some of which are correct, some of which are incomplete, and some of which are outdated—the model learns a noisy, inconsistent mapping. It may memorize one of the responses and apply it to all variations, or it may average the responses and produce something that is partially correct but unsatisfying.

Near-duplicates also signal low-diversity training data. If 30% of your training set consists of minor variations on a small number of underlying patterns, your model will perform well on those patterns and poorly on everything else. You are training a narrow specialist, not a generalizable system. High duplication rates indicate that your data collection process is sampling from a limited distribution, relying on templates, or scraping repetitive content.

The typical enterprise training set contains 5% to 15% exact duplicates and 10% to 25% near-duplicates before deduplication. After aggressive deduplication, these rates should drop to below 1% exact duplicates and below 5% near-duplicates. If your post-deduplication rates remain high, your data source is fundamentally low-diversity, and you need to expand your collection strategy.

## Exact Deduplication: Hashing and Matching

Exact deduplication is straightforward. You compute a hash of each training example and remove all but one copy of any example with a matching hash. The choice of hash function determines what counts as "exact." You have three options: hash the entire example including both input and output, hash only the input, or hash only the output.

Hashing the entire example is the most common approach. You concatenate the input and output into a single string, compute a hash using SHA-256 or a similar cryptographic hash function, and store the hash in a set. As you process each example, you check whether its hash is already in the set. If it is, you skip the example. If it is not, you add the example to the deduplicated training set and add its hash to the set. This approach removes examples where both input and output are identical. It preserves examples where the input is identical but the output differs, which may be intentional if your task requires handling ambiguity or producing varied responses to the same prompt.

Hashing only the input removes all but one example for each unique input. This approach is appropriate for deterministic tasks where each input should have a single correct output, such as knowledge retrieval, classification, or code generation from specifications. If your training set contains multiple examples with the same input but different outputs, hashing the input forces you to decide which output to keep. You keep the first occurrence, the most recent occurrence, the highest-quality occurrence as determined by a quality score, or a random occurrence. Most teams keep the most recent occurrence under the assumption that later data reflects improved labeling or updated information.

Hashing only the output removes all but one example for each unique output. This approach is less common but useful for tasks where output diversity matters more than input diversity, such as creative writing or marketing copy generation. It ensures that the model sees a wide range of outputs even if the inputs are repetitive.

You implement exact deduplication as a preprocessing step before training. For small datasets—under 100,000 examples—you can load all hashes into memory and deduplicate in a single pass. For large datasets—over one million examples—you use a distributed hash map or a disk-backed deduplication tool like GNU sort with the unique flag. Exact deduplication typically runs in seconds to minutes and reduces training set size by 5% to 15%.

You also track where duplicates came from. If duplicates are evenly distributed across data sources, they are likely due to users repeatedly asking the same questions or performing the same actions. If duplicates are concentrated in one data source or one time period, they indicate a bug in data export, accidental reprocessing, or scraping of the same content multiple times. You log duplicate counts by source and by timestamp to identify these patterns.

## Near-Duplicate Detection: MinHash, Embeddings, and Clustering

Near-duplicate detection is harder than exact deduplication because you must define a similarity threshold and choose a similarity metric. Two examples are near-duplicates if their similarity exceeds the threshold. The challenge is computing pairwise similarity for large datasets without comparing every example to every other example, which requires quadratic time.

The most scalable approach is **MinHash**, a locality-sensitive hashing technique that approximates Jaccard similarity between sets of tokens. You tokenize each input and output into n-grams—typically character-level 3-grams or 5-grams—convert the n-grams into a set, and compute a MinHash signature for the set. The MinHash signature is a fixed-size vector of hash values that preserves approximate similarity: examples with similar sets of n-grams have similar MinHash signatures. You then cluster examples by MinHash signature using banding and hash tables. Examples that fall into the same hash bucket are candidate near-duplicates. You compute exact Jaccard similarity for candidate pairs and mark pairs with similarity above a threshold—typically 0.8 or 0.9—as near-duplicates. You keep one example from each near-duplicate cluster and discard the rest.

MinHash works well for text with small edits, paraphrases, or repeated substrings. It does not work well for text that is semantically similar but lexically different, such as "how do I reset my password" versus "I forgot my password and need to get back into my account." For semantic near-duplicate detection, you use embedding-based similarity.

You embed each input using a sentence embedding model like OpenAI text-embedding-3-large, Cohere embed-v3, or an open-source model like all-MiniLM-L6-v2. You compute pairwise cosine similarity between embeddings. Pairs with cosine similarity above a threshold—typically 0.85 to 0.95—are candidate near-duplicates. You review candidate pairs manually or apply a secondary filter based on output similarity. If the inputs are semantically similar and the outputs are identical or very similar, you treat the pair as a near-duplicate and keep only one example.

Embedding-based similarity is more accurate than MinHash for semantic near-duplicates, but it is also more expensive. Embedding 100,000 examples costs approximately twenty dollars and takes ten to thirty minutes depending on batch size and API rate limits. Computing pairwise similarity for 100,000 embeddings requires 5 billion comparisons, which is impractical without dimensionality reduction or approximate nearest neighbor search. You use FAISS, Annoy, or HNSW to build an approximate nearest neighbor index over the embeddings, then query the index for each example to find its top k nearest neighbors. You set k to 10 or 20 and check whether any neighbor exceeds the similarity threshold. This reduces the comparison cost from quadratic to linear in practice.

You can also combine MinHash and embedding-based methods. You use MinHash to quickly identify candidate clusters, then use embeddings to compute precise similarity within each cluster. This hybrid approach balances speed and accuracy.

After identifying near-duplicates, you decide which example to keep from each cluster. You keep the example with the shortest input, the highest-quality output as judged by a quality scorer, the most recent timestamp, or a random example. Some teams keep all examples but down-weight duplicates during training by assigning them lower sampling probabilities. This preserves information while reducing memorization.

Near-duplicate removal typically reduces training set size by an additional 10% to 25% beyond exact deduplication. If removal rates exceed 40%, your data collection process is generating low-diversity data, and you need to revisit your sourcing strategy.

## The Danger of Test Set Contamination

Contamination occurs when examples from your test set or validation set appear in your training set. Contaminated models appear to perform well on evaluations because they have memorized the test answers, but they perform poorly in production because they have not learned to generalize. Contamination is one of the most insidious forms of data quality failure because it is invisible in standard evaluation workflows. You only discover contamination when you notice that eval performance is much higher than production performance, or when you audit the training data and find overlaps.

Contamination happens in three ways. First, accidental duplication during data splitting. You collect a dataset, shuffle it, and split it into training and test sets. If you do not deduplicate before splitting, duplicates can appear in both sets. This is especially common when examples are exported from databases with repeated rows or when data collection spans overlapping time periods. Second, leakage from public benchmarks. You scrape training data from the web or from user interactions, and some of that data includes examples from public benchmarks like MMLU, HellaSwag, or HumanEval. Users sometimes copy benchmark questions into chatbots or forums, and scraping tools collect those copies. Third, temporal leakage. You split data by timestamp, using older data for training and newer data for testing. If users repeat the same questions or tasks over time, the test set contains near-duplicates of training examples even though the timestamps differ.

Contamination inflates evaluation metrics by five to twenty percentage points depending on contamination rate and task difficulty. A model that achieves 88% accuracy on a contaminated test set might achieve only 72% accuracy on a clean test set. This gap leads to catastrophic failures in production. You deploy a model expecting high performance and discover that it fails on real user requests because it never learned the task—it only memorized the test answers.

You detect contamination by comparing training examples to test examples using the same deduplication methods described earlier. You compute hashes or embeddings for every test example, then search the training set for matches. You flag any training example with hash collision or embedding similarity above 0.9 to any test example. You remove flagged examples from the training set and re-run evaluations on the cleaned data. The difference between contaminated and clean evaluation scores is the contamination penalty.

You also check for contamination with public benchmarks. You download benchmark datasets—MMLU, HellaSwag, TruthfulQA, HumanEval, GSM8K—and compare them to your training set using hash matching and embedding similarity. Some benchmarks provide official contamination checking scripts. You run these scripts before training and verify that contamination rates are below 0.5%. If contamination exceeds 1%, you investigate the source. If contamination exceeds 5%, you rebuild the training set with stricter filtering.

Some contamination is unavoidable in web-scale training sets. Large language models trained on Common Crawl or GitHub inevitably see some benchmark examples because those examples are discussed in blogs, documentation, and code repositories. The key is to measure contamination, report it transparently, and use held-out private test sets that have never been published or discussed publicly. If your evaluation relies entirely on public benchmarks, you cannot trust the results. You need internal test sets drawn from real user interactions or expert-created scenarios that have never been exposed to the training data pipeline.

## The Deduplication Pipeline: Sequencing and Iteration

Deduplication is not a single step. It is a sequence of filters applied in a specific order. You run exact deduplication first because it is fast and cheap. You run near-duplicate detection second because it is slower and more expensive. You run contamination checks third because they require loading and processing external datasets. You run deduplication before Pass 1 automated checks and Pass 2 sampling because there is no point in spending review time on duplicates.

The full deduplication pipeline looks like this. First, you load the raw training data. Second, you compute hashes for every example and remove exact duplicates, keeping the most recent occurrence. Third, you compute MinHash signatures or embeddings and cluster near-duplicates, removing all but one example per cluster. Fourth, you load test set hashes and embeddings and remove any training example that matches a test example. Fifth, you load public benchmark hashes and remove matches. Sixth, you log summary statistics: original training set size, post-exact-dedup size, post-near-dedup size, post-contamination size, total removal count, removal rate, and removal breakdown by category. Seventh, you save the deduplicated training set and proceed to Pass 1 automated quality checks.

You iterate on deduplication thresholds and methods. After training a model on a deduplicated set, you analyze model behavior in production. If the model is still producing repetitive outputs, you lower the near-duplicate similarity threshold and remove more examples. If the model is struggling with rare inputs, you check whether aggressive deduplication removed too many examples and reduced training set diversity. You tune the deduplication pipeline to balance memorization risk and data diversity.

Some teams apply deduplication iteratively during training. They deduplicate the initial training set, train for several epochs, then analyze which examples the model has memorized by checking whether the model can reproduce training outputs verbatim when given training inputs. They remove memorized examples from the training set and continue training. This approach is more complex but effective for very large models where memorization is a persistent risk.

## What Removal Rates Are Normal and What Signals a Problem

A well-constructed training set should have an exact duplicate removal rate between 5% and 15%, a near-duplicate removal rate between 10% and 25%, and a contamination removal rate below 1%. Total removal across all deduplication steps should be between 20% and 35%. If your total removal rate is below 10%, your deduplication thresholds are too conservative, and you are leaving duplicates in the training set. If your total removal rate exceeds 50%, your data collection process is broken, and you are generating highly repetitive or low-diversity data.

High exact duplicate rates—above 20%—indicate data export bugs, repeated scraping of the same sources, or users submitting the same requests many times. You fix this by deduplicating at the source, improving data export logic, or filtering user requests for uniqueness before adding them to the training set.

High near-duplicate rates—above 30%—indicate template-driven data, low-diversity user behavior, or scraping of repetitive content like FAQ pages, support documentation, or product catalogs. You fix this by expanding data sources, adding diversity filters during collection, or down-sampling repetitive content.

High contamination rates—above 5%—indicate that your test set is not properly isolated, that you are scraping public benchmarks, or that your temporal split is insufficient. You fix this by splitting data before any processing, using private test sets, and filtering out known benchmark examples during collection.

You track removal rates across training set versions and data sources. If removal rates increase over time, your data collection process is degrading. If removal rates vary significantly across sources, some sources are higher quality than others, and you should up-weight high-quality sources and down-weight or remove low-quality sources.

Deduplication and contamination checking are not one-time tasks. They are permanent components of your training data pipeline. Every time you build a new training set, you deduplicate it. Every time you add new data to an existing set, you deduplicate the combined set. Every time you update your test set, you re-check contamination. You treat deduplication as a prerequisite for training, not as an optional optimization.

The SaaS company that opened this subchapter learned this the hard way. They shipped a model that memorized duplicates and inflated test scores with contaminated data. You will not make the same mistake. You will deduplicate, check contamination, and train models that generalize.

Deduplication ensures that your training set contains diverse, unique examples. The next challenge is determining how much training data you actually need, and how to scale your dataset without sacrificing quality.
