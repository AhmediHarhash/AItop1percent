# 2.9 — PII Scrubbing, Data Licensing, and Legal Compliance for Training Data

In late 2024, a healthcare technology company assembled what they thought was a pristine fine-tuning dataset for a clinical summarization model. They had collected 47,000 anonymized patient encounter notes from partner hospitals, scrubbed them according to their internal privacy policy, and spent $180,000 on annotation. Three weeks before launch, their legal team ran a compliance audit and found that 22% of the records still contained full patient names, 18% contained exact dates of birth, and nearly all contained provider NPI numbers that could be reverse-mapped to individual physicians. Worse, the data licensing agreements with two of the four partner hospitals explicitly prohibited use in machine learning training. The project was halted. The entire dataset was discarded. The team had to start over with a nine-month delay and an additional $240,000 in legal and data procurement costs. The root cause was not a technical failure—it was the assumption that anonymization and legal compliance could be retrofitted after data collection instead of being built into the pipeline from day one.

Training data carries legal obligations that you cannot ignore, defer, or work around. This subchapter covers the legal, privacy, and licensing dimensions of training data: how to detect and scrub personally identifiable information, how to navigate GDPR and CCPA requirements, how to assess whether you legally have the right to train on a given dataset, and how to build compliance workflows that prevent the kind of catastrophic failure described above. These are not optional nice-to-haves. They are foundational requirements that, if violated, can result in regulatory fines, lawsuits, reputational damage, and the complete loss of your training investment.

## PII Detection Is Not Just a Regex Problem

Most teams start with the assumption that scrubbing PII is a matter of running a few regular expressions to catch email addresses, phone numbers, and Social Security numbers. This works for approximately 15% of real-world PII. The remaining 85% requires context-aware detection, domain-specific heuristics, and human review. Names appear in unstructured text without clear markers. Addresses are embedded in narrative descriptions. Dates of birth are referenced indirectly as age calculations. Medical record numbers, employee IDs, and device identifiers do not follow predictable patterns.

You need a multi-layer PII detection pipeline. The first layer is pattern-based: regular expressions for emails, phones, credit card numbers, government IDs. The second layer is entity recognition: a named entity recognition model trained to detect person names, organizations, locations, and dates in context. The third layer is domain-specific detection: custom rules for medical record numbers, policy numbers, internal IDs, IP addresses, and any identifier unique to your industry. The fourth layer is statistical outlier detection: flagging text spans that have high uniqueness scores or low frequency in your corpus, which often indicate specific identifiers.

After automated detection, you need human review on a sample. You cannot assume 100% recall from automated tools. In practice, even the best PII detection pipelines achieve 92-96% recall on real-world data. That means 4-8% of PII leaks through. For a dataset of 50,000 records, that is 2,000 to 4,000 records with residual PII. Your sampling strategy should be risk-weighted: review all records flagged as high-confidence PII, review a 10% random sample of medium-confidence records, and review a 1% sample of low-confidence records. Track precision and recall on your review samples and use that to calibrate your automated detection thresholds.

## Scrubbing Strategies and Their Tradeoffs

Once you have detected PII, you have four scrubbing strategies: redaction, replacement, generalization, and synthetic substitution. Redaction replaces the PII with a placeholder like REDACTED or NAME. Replacement substitutes a realistic fake value: replacing "John Smith" with "Michael Johnson." Generalization replaces a specific value with a category: replacing "born on March 15, 1987" with "born in the 1980s." Synthetic substitution uses a generator to create a plausible but fake value that preserves statistical properties.

Redaction is the safest but degrades model quality the most. If you redact all names, your model never learns how names are used in context. If you redact all dates, your model cannot learn temporal reasoning. Redaction is appropriate for high-risk identifiers like Social Security numbers or medical record numbers where no context is needed. Replacement is better for names and locations where the model needs to learn linguistic patterns but the specific identity does not matter. Generalization is appropriate for numeric identifiers like age or zip code where ranges preserve the information needed for the task. Synthetic substitution is the most sophisticated but requires careful validation to ensure the synthetic values preserve the statistical distribution and do not introduce bias.

You choose your scrubbing strategy based on the task and the risk. For a model that generates clinical notes, you need realistic names and dates, so replacement and synthetic substitution are appropriate. For a model that classifies medical conditions, names are irrelevant, so redaction is fine. For a model that predicts patient risk scores, age ranges matter, so generalization preserves utility. The wrong choice degrades either privacy or performance. A team that redacted all ages from a readmission prediction model saw their AUC drop from 0.84 to 0.71 because age was a key predictor. A team that used synthetic names in a customer service model saw their tone accuracy drop because the synthetic names did not match the cultural and linguistic distribution of real customer names.

## GDPR and Training Data: The Right to Erasure Problem

GDPR applies to any data that can be linked to an identified or identifiable natural person. Even if you scrub direct identifiers, GDPR still applies if the data can be re-identified using auxiliary information. This is a much stricter standard than HIPAA's de-identification safe harbor. Under GDPR, individuals have the right to request deletion of their data. If your training data contains data from EU residents, you need a process to honor deletion requests—and this creates a fundamental problem for fine-tuned models.

Once data is used to train a model, you cannot simply delete the data and consider the obligation fulfilled. The model has learned patterns from that data. Depending on the training process, the model may have memorized specific examples. Research from 2025 shows that large language models fine-tuned on small datasets can reproduce exact training examples with non-trivial probability, especially for rare or unusual inputs. If a user requests deletion of their data under GDPR, you may be required to delete the data, retrain the model without that data, and destroy the original model. This is not hypothetical. In early 2025, a fintech company received 140 GDPR deletion requests covering data used in a fraud detection model. Their legal team determined that retraining was required. The retraining cost was $95,000 and took six weeks.

Your GDPR compliance strategy must be established before you collect training data. First, assess whether you have a legal basis for processing the data for training purposes. Consent is one basis, but consent must be specific, informed, and freely given—and users can withdraw consent at any time. Legitimate interest is another basis, but you must balance your interest against the rights of the individual, and training models is often not considered a compelling enough interest. Contractual necessity is a third basis, but only if the training is necessary to fulfill a contract with the individual. If you do not have a clear legal basis, you cannot use the data for training, period.

Second, implement data lineage tracking so that you can identify which training examples came from which individuals. If you receive a deletion request, you need to be able to locate all data associated with that individual, including the raw data, the processed data, and any model artifacts. Third, define your retraining policy: under what circumstances do you retrain the model, and how do you decide whether deletion of a small number of examples requires retraining versus updating a blocklist. Fourth, document all of this in your privacy policy and your data processing agreements with any third parties who provide data.

## CCPA, State Privacy Laws, and the Expanding Compliance Landscape

The California Consumer Privacy Act and its successor, the California Privacy Rights Act, create rights similar to GDPR but with different scoping and enforcement mechanisms. CCPA applies to California residents and to businesses that meet certain revenue or data volume thresholds. CCPA grants the right to know what data is collected, the right to delete data, and the right to opt out of the sale of data. Training a model on user data and selling access to that model may be considered a sale under CCPA, depending on how the business relationship is structured.

As of 2026, twelve U.S. states have comprehensive privacy laws, each with slightly different definitions, rights, and obligations. Virginia, Colorado, Connecticut, Utah, and others have enacted laws modeled on CCPA but with variations in applicability, exemptions, and enforcement. If you operate nationally, you need a compliance strategy that satisfies the strictest requirements across all applicable jurisdictions. You cannot assume that compliance with GDPR is sufficient for U.S. laws, or vice versa. The definitions of personal information, the scope of processing, and the requirements for consent differ.

Your compliance checklist includes: identifying which laws apply based on where your users are located and where your business operates; mapping your data flows to determine what data is collected, how it is processed, and where it is stored; assessing whether training constitutes automated decision-making or profiling, which may trigger additional obligations; implementing mechanisms for users to exercise their rights, including data access requests, deletion requests, and opt-outs; maintaining records of processing activities; and conducting data protection impact assessments for high-risk processing.

## Data Licensing: Do You Have the Right to Train on This Data

Just because you have access to data does not mean you have the legal right to train a model on it. Data comes from three sources: data you own, data you license, and data you obtain from third parties under terms of service or usage agreements. Each source has different legal constraints. Data you own—data generated by your own systems, your own employees, or your own users under agreements that grant you broad rights—is the safest. But even here, you need to verify that your user agreements explicitly permit machine learning training. Many older terms of service do not address this, and relying on implied rights is legally risky.

Data you license from third parties requires careful review of the licensing agreement. Most commercial data licenses include explicit restrictions on use cases. A license that permits use for analytics or reporting may not permit use for training machine learning models. A license that permits internal use may not permit use in a product that you sell to customers. A license that permits use for one purpose may not permit use for a different purpose. In 2024, a SaaS company licensed a customer interaction dataset for internal quality analysis. They later used the same dataset to train a customer intent model that they productized. The licensor discovered this, claimed breach of contract, and demanded $1.2 million in additional licensing fees plus damages.

When reviewing a data license, ask these questions: Does the license explicitly permit machine learning training? Does it permit the specific use case you are building? Does it permit commercial use if you are building a product? Does it permit redistribution or sublicensing if you are making the model available to others? Are there restrictions on combining this data with other datasets? Are there restrictions on the geographies or industries where you can use the model? Does the license require attribution, and if so, how do you attribute training data in a model? What are the audit rights, and can the licensor inspect your use? What are the termination clauses, and what happens to models trained on the data if the license is terminated?

## Terms of Service and Scraping: The Legal Minefield

Web scraping for training data is common, and it is legally fraught. The terms of service for most websites prohibit scraping, prohibit automated access, and prohibit use of content for machine learning. Whether these terms are legally enforceable is a complex and evolving area of law. In the U.S., the Computer Fraud and Abuse Act has been used to prosecute scraping that violates terms of service, but courts have issued conflicting rulings on whether mere violation of terms of service constitutes unauthorized access. The hiQ Labs v. LinkedIn case established that scraping publicly available data does not violate the CFAA, but that case did not address terms of service restrictions on use for machine learning.

Even if scraping is technically legal, using scraped data for training may violate copyright, create liability under the Digital Millennium Copyright Act, or expose you to breach of contract claims if you agreed to the terms of service. In 2025 and 2026, a wave of lawsuits targeted AI companies for training models on copyrighted content without permission. Publishers, authors, artists, and photographers have sued, arguing that training models on their work without a license constitutes copyright infringement. The legal outcomes are still being determined, but the risk is real and the damages can be substantial.

Your safest path is to avoid scraping entirely and to license data from reputable providers who have cleared the rights. If you must scrape, consult with legal counsel before you start. Document your rationale for why the scraping is lawful. Limit your scraping to publicly available, non-copyrighted content. Respect robots.txt and rate limits. Do not circumvent technical barriers. Do not scrape content behind paywalls or authentication. And be prepared for the possibility that your use will be challenged and that you may need to remove the data and retrain the model.

## Consent and Opt-Out Mechanisms for User-Generated Data

If your training data includes content created by users—customer support tickets, forum posts, product reviews, chat logs—you need to consider whether you have obtained meaningful consent. Burying a clause in your terms of service that says "we may use your content to improve our services" is not sufficient consent under GDPR or under emerging U.S. laws. Consent must be specific, informed, freely given, and revocable. This means you need to tell users that their content will be used to train machine learning models, explain how the models will be used, and give them a clear option to opt out.

Implementing opt-out mechanisms for training data is not trivial. You need to track opt-out preferences at the user level and ensure that when a user opts out, all of their data is excluded from future training runs. If a user opts out after their data has already been used for training, you need to decide whether you will retrain the model or accept the residual risk. For high-sensitivity data or high-visibility products, retraining is the safer choice. For low-risk use cases, you may accept the risk and document your rationale.

You also need to consider consent for data provided by employees, contractors, or partners. In some jurisdictions, employee consent is not considered freely given because of the power imbalance in the employment relationship. This means you may need to rely on legitimate interest or another legal basis, and you may need to conduct a data protection impact assessment. If you are collecting data from contractors or partners under a data processing agreement, ensure that the agreement explicitly permits training and that the contractors or partners have obtained any necessary consents from their own users or employees.

## Right to Deletion and Model Versioning

When a user requests deletion of their data, you must delete the data from all systems where it is stored. This includes raw data, processed data, backups, and archives. But it also includes derived artifacts, including models trained on that data. If your model has memorized specific user data, deletion of the raw data is not sufficient. You must also address the model. This creates three options: retrain the model without the deleted data, implement machine unlearning techniques to remove the influence of the deleted data, or destroy the model and fall back to a previous version.

Retraining is the gold standard but is expensive. For large models or large datasets, retraining may cost tens of thousands of dollars and take weeks. You can reduce this cost by versioning your training data and your models in a way that allows incremental retraining. Maintain a ledger of which data was included in which model version. When deletion requests arrive, batch them and schedule periodic retraining runs rather than retraining on every individual request. Define a threshold—for example, if deletions affect less than 0.1% of your training data, you retrain quarterly; if they affect more than 1%, you retrain immediately.

Machine unlearning is an emerging research area that aims to remove the influence of specific training examples without full retraining. Techniques include gradient ascent on the examples to be removed, influence function approximations to identify and adjust affected model parameters, and differential privacy mechanisms that limit memorization. As of 2026, these techniques are not yet production-ready for most use cases, and their effectiveness is difficult to verify. They are an area of active research, and you should monitor the literature, but you should not rely on them as your primary compliance strategy.

## Building a Compliant Training Data Pipeline

Compliance is not a one-time gate check. It is a continuous process that must be integrated into every stage of your data pipeline. Start with a data inventory: catalog all data sources, document the legal basis for processing, document any licensing or contractual restrictions, and identify data that contains PII or sensitive attributes. Next, implement automated PII detection and scrubbing as part of your data ingestion process. Every record should be scanned before it enters your training dataset.

Establish a data governance committee that includes legal, privacy, security, and engineering. This committee reviews new data sources, approves scrubbing strategies, assesses compliance risks, and makes decisions about retraining in response to deletion requests. Document all decisions in a compliance log. Implement technical controls: encryption at rest and in transit for all training data, access controls that limit who can view raw data, audit logs that track who accessed what data and when, and data lineage tracking that maps raw data to processed data to model versions.

Train your team on privacy and compliance requirements. Engineers and data scientists need to understand not just how to scrub PII but why it matters and what the legal consequences of failure are. Create runbooks for common scenarios: what to do when you discover PII in a dataset that has already been used for training, how to respond to a deletion request, how to handle a data breach, how to respond to a regulatory audit. Test your runbooks with tabletop exercises. Compliance is not a checkbox. It is a discipline, and it requires the same rigor and investment as any other engineering discipline.

The cost of non-compliance is not just regulatory fines. It is loss of user trust, reputational damage, and the destruction of work product that you have invested months or years to create. The healthcare company that started this subchapter lost nine months and nearly half a million dollars because they treated compliance as an afterthought. You cannot afford to make the same mistake. Build compliance into your pipeline from day one, and treat it as a non-negotiable requirement. The next subchapter covers the training data spec document, the contract that defines exactly what good data looks like and prevents costly rework.
