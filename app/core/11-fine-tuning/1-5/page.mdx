# 1.5 — Task Types That Benefit from Fine-Tuning (and Types That Never Do)

In mid-2025, a legal technology company spent eleven weeks and forty-seven thousand dollars fine-tuning GPT-4o to answer questions about corporate law. The team collected 8,000 question-answer pairs from their case database, hired two contract lawyers to review the training data, and ran three successive fine-tuning iterations with progressively refined examples. When they deployed the model to their pilot customers, the accuracy on general corporate law questions was 91% — exactly the same as the baseline GPT-4o model with a well-crafted prompt. The fine-tuned model performed identically because the task was pure knowledge retrieval from a domain where the base model already had comprehensive training. They had spent three months optimizing a model for a task that provided zero marginal benefit from fine-tuning. The diagnosis was straightforward: they had selected a task category fundamentally unsuited to fine-tuning, and no amount of data quality or hyperparameter tuning would change that fact.

Fine-tuning is not a general-purpose enhancement technique. It excels at a specific set of task categories and fails completely at others. The distinction is not about domain or industry — it is about the fundamental nature of what the task requires the model to learn. Some tasks benefit because they involve patterns, structures, or behaviors that cannot be reliably induced through prompting alone. Other tasks never benefit because the capability already exists in the base model, or because the task requires knowledge that fine-tuning cannot provide. Understanding which category your task falls into is the single most important decision in the fine-tuning process, and getting it wrong costs months and tens of thousands of dollars with nothing to show for it.

## Style Transfer and Tone Enforcement

Style transfer is the highest-return task category for fine-tuning. When you need a model to consistently produce output in a specific voice, register, or format that deviates significantly from the base model's default style, fine-tuning delivers dramatic improvements that prompting cannot match. The reason is structural: base models are trained to produce helpful, neutral, professional responses. Teaching them to adopt a radically different style through prompting requires constant vigilance, produces inconsistent results, and breaks down under edge cases. Fine-tuning embeds the style directly into the model's default behavior.

Consider a financial services company that needed all customer-facing communications to follow a specific tone: warm but not casual, confident but not aggressive, empathetic but not apologetic. Their brand guidelines included 47 pages of examples showing acceptable and unacceptable phrasings for common scenarios. Prompting the base model with these guidelines produced acceptable results about 70% of the time — but the remaining 30% required manual editing, and the tone would drift unpredictably when the prompt context grew too large or the scenario was unusual. They fine-tuned Claude Opus 4 on 2,400 examples of correct communications across every scenario type in their guidelines. The fine-tuned model hit the correct tone 96% of the time, maintained consistency across long conversations, and generalized the style to scenarios not explicitly covered in training. The improvement was not marginal — it was the difference between a tool that required constant supervision and a tool that could be deployed with confidence.

Style transfer works because it is teaching the model a behavior, not a fact. The base model already knows how to write — fine-tuning is steering its default writing behavior toward a specific pattern. This works for corporate communication styles, creative writing voices, technical documentation formats, marketing copy tones, customer service scripts, and any other scenario where the output must consistently match a defined stylistic pattern. The key is that the style must be consistent and definable through examples. If your style guide is vague or inconsistent, fine-tuning will not help — it will just memorize the inconsistency.

The financial services company discovered this when they analyzed which scenarios improved most with fine-tuning. Simple customer inquiries about account balances or transaction history showed minimal improvement — these were straightforward and the prompted model handled them well. The dramatic gains came in complex scenarios: declining a loan application while maintaining positive customer relations, explaining fee structures without sounding defensive, responding to complaints about service quality with empathy but without admitting fault. These scenarios required nuanced tone control that prompts could describe but could not reliably enforce. The fine-tuned model internalized the patterns and applied them consistently even when the prompt context included contradictory information or when the conversation took unexpected turns.

Another pattern emerged in creative writing applications. A publishing company needed to generate marketing copy in the distinctive voice of different author brands — one author wrote in terse, punchy sentences with dark humor, another used flowing, lyrical prose with vivid metaphors. Prompting GPT-4.5 to mimic these styles produced passable imitations that readers could identify as AI-generated within seconds. The prompts made the model try too hard, producing exaggerated versions of the style that felt forced. Fine-tuning on 1,200 examples of each author's actual writing produced output that matched the author's voice so closely that beta readers could not distinguish fine-tuned output from human-written text at rates better than chance. The fine-tuned models had internalized the rhythm, word choice, sentence structure, and tonal patterns that defined each voice.

The common thread is that style transfer tasks benefit from fine-tuning when the style has implicit rules that are hard to articulate but easy to demonstrate. If you can write a clear style guide that says use active voice, keep sentences under 20 words, avoid jargon, prompting will work. If your style guide says maintain warmth without casualness and confidence without aggression, you need fine-tuning because those boundaries are subjective and context-dependent. Fine-tuning learns the boundaries from examples rather than trying to follow abstract rules.

## Domain-Specific Classification with Subtle Distinctions

Classification tasks benefit from fine-tuning when the categories involve subtle distinctions that require domain expertise to recognize, especially when those distinctions are not well-represented in the base model's training data. Base models are excellent at obvious classifications — distinguishing spam from legitimate email, positive sentiment from negative sentiment, urgent requests from routine questions. They struggle with classifications that require nuanced understanding of domain-specific context, edge cases that look similar on the surface but differ in critical ways, or categories that are defined by your organization's specific needs rather than universal standards.

A medical device company needed to classify customer support tickets into 23 categories based on whether the issue required immediate safety escalation, standard troubleshooting, warranty service, user education, or clinical follow-up. The categories had significant overlap — a report of unexpected device behavior could fall into any of five categories depending on subtle details about when it occurred, what the user was doing, and how the device responded. Prompting GPT-4.5 with detailed category descriptions and examples achieved 78% accuracy, but the 22% error rate was unacceptable because misclassification of safety issues had regulatory consequences. They fine-tuned on 5,000 historical tickets that had been classified by their most experienced support engineers. The fine-tuned model achieved 94% accuracy, and more importantly, its errors were rarely on safety-critical boundaries — it had learned the implicit prioritization rules that the engineers used but had never explicitly documented.

The pattern here is that fine-tuning excels when the classification decision requires recognizing subtle patterns that are hard to articulate in a prompt but easy to demonstrate through examples. This applies to legal document classification, medical diagnosis coding, content moderation with complex policies, fraud detection with evolving patterns, and any scenario where the categories are defined by implicit expert judgment rather than explicit rules. If you can write a clear rule that distinguishes the categories, prompting will work. If the distinction requires expertise that you can demonstrate but not easily explain, fine-tuning is the right approach.

The medical device company's experience revealed why prompting failed where fine-tuning succeeded. Their category definitions were pages long and included decision trees, but the real classification logic involved pattern recognition across multiple signals. An experienced engineer would see a ticket mentioning unexpected display behavior during a surgical procedure and immediately classify it as immediate safety escalation, not because of explicit rules but because they recognized the pattern: display issues during active use suggest sensor malfunctions that could affect clinical decisions. The prompted model read the category description, saw that display issues were mentioned under troubleshooting, and misclassified the ticket. The fine-tuned model had seen hundreds of examples where display issues during active use were safety escalations and learned to recognize the context that made the distinction critical.

A legal technology company had a similar experience with contract classification. They needed to classify clauses into 31 categories for automated contract review: indemnification, limitation of liability, termination rights, dispute resolution, intellectual property licensing, data protection, and 25 others. Many clauses combined multiple concepts — a termination clause that also specified dispute resolution procedures, an indemnification clause that included liability limitations, a licensing clause with embedded data protection requirements. The base model with prompting achieved 71% accuracy, but its errors were systematic. It classified clauses based on the first concept mentioned, missing that the primary legal import was often buried in subordinate clauses. They fine-tuned on 4,200 examples of correctly classified clauses. Accuracy rose to 91%, and the error distribution changed dramatically. The fine-tuned model learned to identify the primary legal obligation rather than just the topic mentioned first, capturing the implicit prioritization that lawyers apply when reading contracts.

Content moderation presents similar challenges. A social media platform had 47 content policy categories covering hate speech, harassment, misinformation, graphic violence, self-harm, dangerous organizations, and numerous subcategories with subtle boundaries. Political criticism versus hate speech, satire versus misinformation, educational content about self-harm versus promotion of self-harm — these distinctions are clear to trained moderators but extraordinarily difficult to articulate in prompts. Their prompted baseline achieved 82% agreement with human moderators, but the 18% disagreement rate included too many false positives on political speech and false negatives on coded hate speech. Fine-tuning on 11,000 examples of moderation decisions by their most consistent moderators pushed agreement to 93% and dramatically reduced the rate of errors on the politically sensitive boundaries that mattered most for trust and safety.

## Structured Extraction with Rigid Format Requirements

Structured extraction benefits from fine-tuning when the output must conform to a very specific format, schema, or structure that deviates from common patterns, especially when the format has complex validation rules or nested structures that the base model struggles to maintain consistency with. Base models are good at extracting structured data in standard formats — JSON objects with simple key-value pairs, tables with clear column definitions, lists with consistent item structures. They struggle when the format has unusual nesting, strict validation requirements, conditional fields that appear only in certain contexts, or domain-specific conventions that differ from common practice.

A healthcare technology company needed to extract clinical trial eligibility criteria from medical literature and represent them in a proprietary schema that combined logical operators, numerical ranges, temporal constraints, and cross-references between criteria. The schema was designed for their clinical trial matching system and had 34 field types with strict validation rules. Prompting Claude 3.5 Sonnet with the schema documentation and examples produced output that was structurally correct 61% of the time — the remaining 39% had schema validation errors, incorrect nesting, or missing required fields. They fine-tuned on 3,200 examples of correctly extracted criteria from their historical database. The fine-tuned model produced schema-valid output 97% of the time and correctly handled edge cases like nested exclusion criteria and temporal dependencies that the prompted model almost never got right.

The key insight is that fine-tuning is teaching the model to internalize the format as its default output structure, rather than trying to follow format instructions in the prompt while also processing the input content. This works for extracting data into custom database schemas, generating reports in specific templates, producing API payloads with strict contracts, creating documents in proprietary formats, and any scenario where format compliance is non-negotiable and the format is complex enough that prompting produces frequent errors. If your format is simple or matches common standards, prompting will work. If your format has complex rules that the model must learn rather than follow, fine-tuning is necessary.

The clinical trial extraction case illustrates a critical pattern: prompted models split attention between understanding the input and following format rules, and when both are complex, format compliance degrades. The model would correctly identify that a criterion specified age greater than 18 and less than 65, but would output it in the wrong schema structure — using a flat list instead of nested range operators, or failing to mark the criterion as an inclusion rather than exclusion. The format errors were not random — they were systematic failures to apply complex structural rules while simultaneously parsing medical text. Fine-tuning removed the cognitive load of following format rules by making the format the model's default output structure.

A financial services company needed to extract structured data from loan applications and represent it in a legacy data format that their underwriting system required. The format had been designed in the 1990s and used arcane conventions: dates in Julian format, amounts in fixed-width fields with implicit decimal points, codes that combined multiple attributes in non-obvious ways. Prompting produced correctly formatted output only 58% of the time — the model would use standard date formats, forget the implicit decimal point, or output the wrong code combinations. The errors required manual correction that defeated the purpose of automation. They fine-tuned on 2,800 examples of correctly formatted extractions. Schema compliance rose to 96%, and the remaining errors were almost all in genuinely ambiguous input cases where even human processors disagreed on the correct extraction.

API payload generation shows a related pattern. A logistics company needed to generate payloads for a shipping API that had strict validation: specific field orderings, conditional required fields based on shipment type, enum values that changed based on destination country, nested address structures with country-specific formats. Prompted models achieved 73% first-attempt API acceptance — the remaining 27% were rejected by the API validator for schema violations. Fine-tuning on 1,900 examples of valid payloads increased first-attempt acceptance to 94%. The fine-tuned model had learned the conditional logic: when shipment type is international and destination is European Union, include VAT fields in the specified format; when destination is North America, omit VAT fields and include harmonized tariff codes. These conditional rules were too complex to reliably enforce through prompting.

## Code Generation for Specific Frameworks and Conventions

Code generation benefits from fine-tuning when the target code must follow framework-specific patterns, organizational conventions, or architectural styles that differ significantly from common open-source practices. Base models are trained on vast amounts of public code and are excellent at generating standard implementations in popular languages and frameworks. They struggle when your codebase has internal frameworks, uncommon architectural patterns, strict style guides that differ from community standards, or legacy codebases with idiosyncratic conventions that must be maintained for consistency.

A financial technology company had built their core trading system on an internal event-sourcing framework that used specific patterns for command handling, event emission, aggregate reconstruction, and saga orchestration. The patterns were well-documented internally but were not similar to any popular open-source framework. When engineers prompted GPT-4o to generate new command handlers or aggregates, the generated code used generic event-sourcing patterns that did not match their framework's conventions — it required significant manual rewriting to integrate. They fine-tuned on 1,800 examples of real command handlers, aggregates, and sagas from their codebase. The fine-tuned model generated code that followed their exact patterns, used the correct base classes, emitted events in the right format, and handled errors according to their conventions. Code review time dropped by 40% because the generated code required only logic verification, not structural rewriting.

This pattern applies to internal frameworks, legacy codebases with established conventions, organizations with strict style guides that differ from industry norms, and teams that have made architectural decisions that diverge from common patterns. Fine-tuning is teaching the model your team's way of writing code, not the generic way. If your code follows common patterns, prompting with good examples will work. If your code has significant idiosyncrasies that must be learned, fine-tuning is the right approach.

The trading system fine-tune revealed a subtlety: the improvement was not just about following syntactic patterns but about understanding architectural intent. The prompted model would generate event handlers that technically worked but violated domain-driven design principles that the team followed rigorously. For example, it would put business logic in event handlers instead of in the aggregate, or it would emit multiple events for what should be a single domain event. The fine-tuned model had learned not just the code structure but the architectural philosophy encoded in 1,800 examples of correct implementations. It generated code that respected bounded contexts, maintained aggregate consistency boundaries, and used events to represent domain facts rather than technical notifications.

Legacy code maintenance presents an even stronger case for fine-tuning. A healthcare company maintained a claims processing system written in a proprietary language from the 1980s with conventions that made modern developers recoil: GOTO statements for error handling, global state managed through shared memory regions, business logic embedded in database stored procedures. New developers needed months to learn these conventions, and even experienced developers made mistakes when writing new code that had to integrate with the legacy system. They fine-tuned Llama 4 on 3,400 examples of correct implementations from their codebase. The fine-tuned model generated code that followed the legacy conventions correctly, used GOTO for error handling in the exact patterns the existing code used, managed global state through the established mechanisms, and structured database interactions to match existing patterns. The generated code was ugly by modern standards but correct by the system's internal consistency standards, which was precisely what they needed.

Strict organizational style guides provide another fine-tuning opportunity. A large technology company had a 200-page Python style guide that went far beyond PEP 8: specific patterns for error handling, required type annotations in formats stricter than standard typing module, mandatory documentation structures, required patterns for logging and telemetry, and architectural conventions for service boundaries. Prompted models would generate code that passed linters but violated the team's deeper conventions. Fine-tuning on 5,200 examples of approved code taught the model to generate code that not only followed the syntactic rules but also respected the architectural patterns and idioms that made code feel like it belonged in the codebase.

## Multilingual Tasks in Low-Resource Languages

Multilingual tasks benefit from fine-tuning when the target language is low-resource, meaning it is underrepresented in the base model's training data, or when the task requires language-specific nuances that the base model handles poorly. Base models in 2026 have strong multilingual capabilities for high-resource languages like English, Spanish, French, German, Chinese, Japanese, and Arabic. They are weaker in low-resource languages, regional dialects, code-switched text, and specialized language varieties like legal language, medical language, or historical language.

A government agency needed to process citizen inquiries in three regional languages spoken by minority populations: languages that had limited written corpora, inconsistent orthography, and significant dialectal variation. GPT-4.5 could handle basic translation and simple question answering in these languages, but it struggled with nuanced understanding, produced unnatural phrasing, and frequently mixed in English or dominant-language constructions. They fine-tuned Llama 4 on 4,500 examples of natural conversations in these languages, collected from community organizations and government service interactions. The fine-tuned model produced natural phrasing, understood regional idioms, handled code-switching appropriately, and maintained the correct register for formal government communications. The improvement was stark because the base model had almost no exposure to these languages in its training data, so even a modest amount of fine-tuning provided enormous marginal benefit.

This pattern applies to low-resource languages, endangered languages, dialects not well-represented in written corpora, specialized language varieties, historical language, and any scenario where the base model's language capabilities are weak. If your target language is high-resource and the task is standard, prompting will work. If your language is low-resource or highly specialized, fine-tuning is often necessary.

The government agency's experience highlighted why low-resource language fine-tuning works so well. The base model had been trained on billions of tokens of English and hundreds of millions of tokens of major languages, but perhaps only millions of tokens of the regional languages they needed. The fine-tuning dataset of 4,500 examples represented a significant percentage increase in the model's exposure to these languages, and more importantly, the examples were high-quality, natural language use rather than the mixed-quality web text that likely constituted most of the base training data. Every fine-tuning example had outsized impact because it was filling genuine knowledge gaps rather than refining existing capabilities.

Code-switched text presents a related challenge. A customer service platform serving multilingual populations in Singapore needed to handle messages that mixed English, Mandarin, Malay, and Tamil within single sentences — a pattern common in Singaporean communication but rare in training corpora. The base model would respond in whichever language appeared first in the message, or would default to English, producing responses that felt culturally inappropriate. Fine-tuning on 3,100 examples of code-switched customer service interactions taught the model to recognize code-switching patterns, respond in the appropriate mix of languages, and maintain the code-switching conventions that felt natural to Singaporean users. The fine-tuned model learned that responding to a message that mixed English and Mandarin should produce a response that mixed English and Mandarin in similar proportions and contexts, not a response in pure English or pure Mandarin.

Specialized language varieties benefit even when the base language is high-resource. A legal firm needed to process historical legal documents written in 19th century English — the language was English but the vocabulary, syntax, and legal conventions were different enough from modern English that base models struggled. They fine-tuned on 2,200 examples of historical legal documents paired with modern interpretations. The fine-tuned model learned to recognize archaic legal terminology, understand historical contract structures, and correctly interpret provisions that used language conventions no longer in common use. The task was not translation but comprehension of a language variety underrepresented in modern training corpora.

## Tasks That Never Benefit: General Knowledge Question Answering

Some task categories never benefit from fine-tuning, regardless of how much data you collect or how carefully you tune hyperparameters. The most common mistake is attempting to fine-tune for general knowledge question answering. Base models are trained on trillions of tokens of text from diverse sources and have comprehensive knowledge across most domains. Fine-tuning cannot add knowledge that does not exist in the base model's training data, and for domains where the base model already has strong knowledge, fine-tuning provides no benefit.

The legal technology company in the opening story made this exact mistake. They assumed that fine-tuning on their case database would improve the model's ability to answer corporate law questions. But GPT-4o already had extensive knowledge of corporate law from its base training — it had seen law textbooks, case law, statutes, legal commentary, and practice guides. The 8,000 examples in the fine-tuning dataset did not teach the model anything new about corporate law. They taught it to phrase answers in a slightly different style, but the underlying knowledge was unchanged. When evaluated on accuracy, the fine-tuned model performed identically to the base model because accuracy is determined by knowledge, not phrasing.

This pattern applies to any task where the base model already has strong knowledge. If you are asking questions about science, history, mathematics, general medicine, common law, business concepts, or any well-documented domain, fine-tuning will not improve knowledge-based accuracy. If you need better knowledge, you need retrieval-augmented generation, where the model retrieves relevant information from a knowledge base before answering. Fine-tuning is for behavior, not knowledge. If the task is primarily about knowing facts, fine-tuning is the wrong approach.

The legal technology company's mistake was conceptually clear but easy to make in practice. They had domain expertise that felt specialized to them — their case database included nuanced interpretations of corporate governance rules, detailed analysis of merger and acquisition structures, sophisticated tax planning strategies. They assumed this expertise was unique and that fine-tuning would teach the model their special knowledge. But when they evaluated carefully, they discovered that the base model already knew the legal principles, could analyze governance structures, and understood tax implications. What the fine-tuned model learned was to phrase answers the way their lawyers phrased them, not to know things it did not know before. The phrasing change had no impact on accuracy.

This mistake is epidemic because domain experts naturally overestimate how specialized their knowledge is. A medical practice assumed that fine-tuning on their patient notes would improve diagnosis suggestions, but GPT-4.5 already had comprehensive medical knowledge from textbooks, journals, and clinical guidelines. A business consulting firm assumed that fine-tuning on their case studies would improve strategic advice, but Claude Opus 4 already knew business strategy frameworks, market analysis techniques, and organizational design principles. An engineering company assumed that fine-tuning on their design documents would improve technical recommendations, but the base model already understood engineering principles, material properties, and design tradeoffs. In every case, the fine-tuned model learned formatting and phrasing conventions but not new facts or knowledge.

The test for whether fine-tuning will help with knowledge is simple: can the base model answer factual questions correctly when prompted well? If yes, fine-tuning will not improve factual accuracy. If no, fine-tuning still will not help — you need retrieval-augmented generation to provide the missing facts. Fine-tuning cannot create knowledge that does not exist in the base training data. It can only refine how the model applies knowledge it already has.

## Tasks That Never Benefit: Real-Time Information and Current Events

Fine-tuning cannot provide real-time information or current events knowledge. Models are trained on data up to a specific cutoff date and fine-tuning datasets are similarly time-bounded. If your task requires knowing what happened yesterday, last week, or even last month, fine-tuning will not help. The model's knowledge is frozen at the time of training.

A news aggregation startup attempted to fine-tune GPT-4o to summarize breaking news more accurately. They collected summaries of major news events from 2024 and early 2025 and fine-tuned the model to produce summaries in their preferred style and structure. The fine-tuned model produced well-formatted summaries, but its knowledge of events was still limited to GPT-4o's training cutoff in early 2025. When asked to summarize events from mid-2025 onward, it could not provide accurate information because that information did not exist in its training data. The solution was not fine-tuning — it was integrating a real-time news API and using retrieval-augmented generation to provide current information.

This pattern is absolute: fine-tuning cannot overcome knowledge cutoff dates. If your task requires current information, recent events, real-time data, or knowledge that changes frequently, you need retrieval-augmented generation or API integration. Fine-tuning is for tasks where the knowledge is static and the goal is to improve behavior, format, style, or classification.

The news startup's experience is instructive because they did see some benefit from fine-tuning — just not the benefit they expected. The fine-tuned model learned their summary style: what to emphasize, how to structure information, what level of detail to include, how to handle attribution. When provided with current news articles through retrieval, the fine-tuned model summarized them in exactly the right style. But the fine-tuning did not teach current events knowledge, and attempting to use it that way failed completely. The correct architecture was retrieval-augmented generation with a fine-tuned model: retrieval provided current information, fine-tuning provided consistent style.

This limitation extends to any rapidly changing domain. A cryptocurrency trading platform tried to fine-tune models to analyze market conditions, but crypto markets change minute-to-minute and fine-tuning on historical market data provided no value for current trading decisions. A social media monitoring service tried to fine-tune models to detect emerging trends, but trends by definition are new phenomena not present in training data. An e-commerce company tried to fine-tune models to recommend products based on current inventory, but inventory changes daily and the fine-tuned model had no awareness of current stock levels. In every case, the teams eventually realized they needed real-time data integration, not fine-tuning.

The conceptual error is treating the model as a knowledge repository when it should be treated as a reasoning engine. Fine-tuning teaches the model how to process information, not what information to know. If your task requires current information, provide that information through retrieval or APIs, and use fine-tuning only if you need to teach the model a specific way of processing or presenting that information.

## Tasks That Never Benefit: Tasks Where the Base Model Already Excels

If the base model already achieves high performance on your task, fine-tuning will not provide meaningful improvement. This sounds obvious, but many teams fine-tune because they assume fine-tuning is inherently better, without first evaluating whether the base model already meets their requirements.

A customer service platform fine-tuned Claude 3.5 Sonnet to classify customer inquiries into seven categories: billing, technical support, account management, product questions, complaints, feature requests, and general inquiries. They achieved 89% accuracy with prompting and assumed fine-tuning would push accuracy higher. After fine-tuning on 6,000 labeled examples, accuracy increased to 91% — a 2 percentage point gain that did not justify the cost and complexity of maintaining a fine-tuned model. They later realized that improving their prompt with better examples and clearer category definitions increased prompted accuracy to 90%, essentially matching the fine-tuned model with no additional infrastructure.

The rule is simple: always establish a strong prompted baseline before considering fine-tuning. If the base model with a well-engineered prompt already meets your accuracy, latency, and cost requirements, fine-tuning is unnecessary. Fine-tuning is for closing gaps that prompting cannot close, not for incrementally improving already-acceptable performance. The threshold for "worth fine-tuning" is typically a 10 percentage point or greater improvement over the prompted baseline, or achieving a capability that prompting cannot achieve at all.

The customer service platform's experience reveals a common pattern: teams evaluate a weak prompted baseline, see room for improvement, and jump to fine-tuning without first optimizing the prompt. Their initial 89% accuracy prompt was generic and used only two examples per category. When they invested time in prompt engineering — writing clearer category descriptions, adding five examples per category, and structuring the output format more precisely — prompted accuracy rose to 90%. The 1 percentage point gap between the optimized prompt and the fine-tuned model was not worth the infrastructure cost.

This mistake is particularly common with newer base models. A content moderation system had been fine-tuned on GPT-3.5 Turbo in early 2024 and achieved 87% agreement with human moderators, a significant improvement over the 79% prompted baseline. When GPT-4.5 was released in 2025, they assumed they needed to fine-tune it as well. But GPT-4.5's prompted baseline was already at 91% agreement — it had learned better nuanced understanding during base training, and fine-tuning on the same dataset only pushed agreement to 92%. The 1 percentage point gain did not justify maintaining a fine-tuned version of the newer model. They deployed the prompted GPT-4.5 model and abandoned fine-tuning.

The lesson is that base model capabilities improve with each generation, and fine-tuning decisions must be reevaluated when new models are released. A task that required fine-tuning with GPT-4 might not require fine-tuning with GPT-4.5 or GPT-5. Always start with the prompted baseline using the latest model. Only fine-tune if the gap is large and cannot be closed through prompt engineering.

## Tasks That Never Benefit: Insufficient Training Data

Fine-tuning requires sufficient training data to learn the target behavior. The exact amount varies by task complexity, but as a rough guideline, you need at least 200 to 500 high-quality examples to see meaningful improvement, and 1,000 to 5,000 examples for complex tasks. If you have fewer than 200 examples, fine-tuning will likely overfit to your small dataset and generalize poorly.

A biotech company wanted to fine-tune a model to generate experimental protocols for a highly specialized type of assay. They had 43 examples of correct protocols from their internal lab notebooks. They fine-tuned Llama 4 on these 43 examples and found that the model memorized the specific protocols but could not generalize to new experimental conditions. When asked to generate a protocol for a slightly different assay, it produced output that was either a near-copy of one of the 43 training examples or was nonsensical. The dataset was too small to teach generalizable behavior.

The solution is not to fine-tune with insufficient data — it is to collect more data, use prompting with few-shot examples, or use retrieval-augmented generation to provide relevant examples at inference time. Fine-tuning is only viable when you have enough examples to cover the task's variation. If you cannot collect sufficient data, fine-tuning is not an option.

The biotech company's problem was structural: they had 43 examples but the task variation was enormous. Each protocol had to specify reagent concentrations, incubation times, temperature profiles, equipment settings, and quality control checks — and each of these parameters varied based on cell type, assay purpose, and experimental context. Forty-three examples could not possibly cover this variation space. The fine-tuned model learned to reproduce the exact reagent concentrations and timing sequences from the training examples but had no understanding of why those values were chosen or how to adjust them for different contexts. When presented with a new cell type, it would output reagent concentrations appropriate for a completely different cell type because that was the closest match in its memorized training data.

The team eventually abandoned fine-tuning and built a retrieval-augmented system instead. When asked to generate a protocol for a new assay, the system retrieved the three most similar protocols from the 43-example database based on cell type, assay purpose, and experimental conditions, provided those protocols as context to the base model, and prompted the model to adapt the retrieved protocols to the new context. This approach worked because it gave the model relevant examples at inference time rather than trying to teach generalizable patterns from insufficient training data.

Data insufficiency also manifests as insufficient coverage of edge cases. A fraud detection system had 8,000 examples of fraud cases but 92% of them were simple card-not-present fraud. The remaining 8% covered account takeover, synthetic identity fraud, merchant collusion, refund fraud, and a dozen other patterns — but each pattern had fewer than 100 examples. Fine-tuning on this dataset produced a model that was excellent at detecting card-not-present fraud and mediocre at detecting everything else. The model had not learned general fraud detection principles — it had learned to recognize the one pattern that dominated the training data. The solution was to collect more examples of underrepresented fraud types or to use few-shot prompting where rare fraud patterns could be provided as examples in the prompt context.

## The Decision Matrix: Matching Task to Approach

The decision of whether to fine-tune comes down to a simple matrix. Fine-tuning is the right approach when your task requires learning a behavior, style, format, or classification pattern that prompting cannot reliably achieve, you have sufficient training data, and the base model does not already excel at the task. Fine-tuning is the wrong approach when the task requires knowledge that the base model already has or lacks, the task requires real-time information, the base model already performs well, or you lack sufficient training data.

Tasks that benefit: style transfer, tone enforcement, domain-specific classification with subtle distinctions, structured extraction with rigid formats, code generation for specific frameworks, multilingual tasks in low-resource languages, tasks requiring consistent behavior that prompting cannot induce. Tasks that never benefit: general knowledge question answering, real-time information retrieval, tasks where the base model already excels, tasks with insufficient training data, tasks requiring knowledge outside the model's training data.

The most common mistake is attempting to fine-tune for knowledge when you need retrieval, or fine-tuning for marginal gains when prompting already works. The second most common mistake is fine-tuning without sufficient data. Both mistakes waste weeks and tens of thousands of dollars. Understanding your task category is the first and most important decision in the fine-tuning process.

You can diagnose your task category by answering three questions. First: is this about behavior or knowledge? If the task requires knowing facts, dates, events, or domain knowledge, fine-tuning is the wrong approach — you need retrieval-augmented generation. If the task requires consistent style, format compliance, or classification based on subtle patterns, fine-tuning may help. Second: can prompting achieve acceptable performance? Test the base model with a well-engineered prompt and few-shot examples. If prompted performance meets your requirements, stop — fine-tuning is unnecessary. If there is a large gap, proceed. Third: do you have sufficient high-quality training data? Count your examples and assess coverage of task variation. If you have fewer than 500 examples or your examples do not cover edge cases, fine-tuning will likely overfit or underperform.

These three questions form a decision tree that eliminates most fine-tuning projects before they start. If your task is about knowledge, stop — you need retrieval, not fine-tuning. If prompting already works, stop — you need optimization, not fine-tuning. If you lack data, stop — you need data collection or alternative approaches. Only tasks that pass all three tests should proceed to the next step, which is validation through a minimum viable experiment.

If your task passes all three tests — it is about behavior not knowledge, prompting leaves a large performance gap, and you have sufficient diverse training data — then fine-tuning is likely worth exploring. But even then, you should not commit to full-scale fine-tuning without validation. The next step is running a minimum viable experiment that proves fine-tuning will deliver value before you invest weeks of effort and tens of thousands of dollars in a full fine-tuning project. That experiment must be fast, focused, and decisive: 48 hours from start to finish, 200 to 500 carefully selected examples, clear metrics that measure business impact, and a binary outcome that tells you whether to proceed or stop.

The next question is how to validate that fine-tuning will work before committing to a full project — the minimum viable fine-tune that proves value in 48 hours.
