# 5.14 — Numerical Precision: From FP32 to FP4 and the Quantization Spectrum

Every weight in a neural network is a number. How you store that number determines how much memory the model consumes, how fast it trains, how quickly it serves inference, and whether the model preserves the quality you spent weeks fine-tuning into it. Most practitioners treat precision as a checkbox in their training configuration. They select BF16 because a tutorial said to, quantize to 4-bit because it fits on their GPU, and never think about what those choices mean at the level of individual bits. This works until it does not. A computer vision startup in early 2025 fine-tuned a 13B parameter model for medical image report generation using FP16 mixed precision. Training appeared stable for the first 800 steps, then loss spiked to infinity and never recovered. They restarted three times, wasted $4,200 in compute, and finally switched to BF16 on a colleague's advice. Training completed without incident. The team never understood why FP16 failed. They could not have, because they did not know what FP16 actually is, how it differs from BF16, or why those eight bits of exponent range matter more than three extra bits of mantissa precision.

This subchapter teaches you what numerical precision means at the bit level, walks through every format you will encounter in production fine-tuning and deployment, and gives you a decision framework for choosing the right precision at every stage of your pipeline. By the end, you will understand not just which format to use but why, and you will be able to diagnose precision-related failures that would otherwise cost you days of debugging.

## What Floating Point Actually Means

A floating-point number stores a value using three components: a sign bit, an exponent, and a mantissa. The sign bit determines whether the number is positive or negative. The exponent determines the scale, the order of magnitude, of the number. The mantissa, sometimes called the significand or fraction, determines the precision within that scale. This is similar to scientific notation. The number 6.022 times ten to the twenty-third power has a sign of positive, an exponent of 23, and a mantissa of 6.022. In binary floating point, the same structure applies but with base-2 arithmetic.

The key insight is that exponent bits and mantissa bits serve fundamentally different purposes. More exponent bits give you a wider range, the ability to represent very large and very small numbers without overflow or underflow. More mantissa bits give you finer precision, the ability to distinguish between numbers that are close together. Every floating-point format is a trade-off between range and precision, and different stages of the AI pipeline have different needs.

**FP** stands for floating point, referring to IEEE 754 standard formats. **BF** stands for Brain Float, a format designed by Google Brain specifically for deep learning workloads. **INT** stands for integer, a fundamentally different approach that stores numbers as whole values mapped to a fixed grid, with no exponent or mantissa at all. Understanding these three families is the foundation for every precision decision you will make.

## The Precision Ladder: Every Format You Need to Know

What follows is the complete precision ladder from highest to lowest, covering every format you will encounter in production fine-tuning and deployment in 2026. Each entry explains the bit structure, the practical trade-offs, and when you would choose it.

**FP32 — 32 bits. Full precision. The historical default.** FP32 uses 1 sign bit, 8 exponent bits, and 23 mantissa bits. It can represent numbers from roughly 1.2 times ten to the negative thirty-eighth to 3.4 times ten to the thirty-eighth with about 7 decimal digits of precision. FP32 was the default for all deep learning from 2012 through approximately 2019. Every weight, every gradient, every optimizer state was stored in 32 bits. The advantage is maximal numerical stability. The disadvantage is that each parameter consumes 4 bytes of memory and modern tensor cores operate at half the speed compared to 16-bit formats. No production fine-tuning workload in 2026 uses FP32 for the full training loop. You encounter FP32 only in specific optimizer states where accumulation precision matters, such as the master copy of weights in mixed-precision training or the momentum and variance terms in AdamW.

**TF32 — 19 bits. NVIDIA's training shortcut.** TensorFloat-32 is NVIDIA's proprietary format, supported natively on A100, H100, and H200 tensor cores. Despite the name, TF32 is not 32 bits. It uses 1 sign bit, 8 exponent bits matching FP32's range, and 10 mantissa bits matching FP16's precision, for 19 bits total. TF32 is designed as a transparent acceleration: when you configure training for FP32, NVIDIA GPUs from Ampere onward silently use TF32 for matrix multiplications, delivering up to 8x speedup over true FP32 while maintaining the same exponent range. You rarely configure TF32 explicitly. It operates behind the scenes. Its relevance is that when someone claims they are training in FP32 on an A100, they are almost certainly getting TF32 matrix operations. This matters for reproducibility across different hardware.

**BF16 — 16 bits. The 2026 default for training.** Brain Float 16 uses 1 sign bit, 8 exponent bits, and 7 mantissa bits. The crucial design choice is that BF16 matches FP32's exponent range exactly while sacrificing mantissa precision. This means BF16 can represent the same enormous and tiny numbers as FP32 but with less precision between adjacent representable values, roughly 2-3 decimal digits instead of 7. For deep learning training, exponent range matters far more than mantissa precision. Gradients during training can vary across many orders of magnitude, and a format that underflows or overflows on extreme gradient values will destabilize training. BF16 never underflows or overflows in cases where FP32 would not, because they share the same exponent range. This is why BF16 does not require loss scaling, a technique needed for FP16 to prevent gradient underflow. BF16 is the default training precision on all modern NVIDIA datacenter GPUs. If your framework offers a mixed-precision option without further configuration, it almost certainly uses BF16. Google designed BF16 specifically for TPUs, and NVIDIA adopted it starting with the A100 architecture.

**FP16 — 16 bits. Higher precision, narrower range.** IEEE half precision uses 1 sign bit, 5 exponent bits, and 10 mantissa bits. Compared to BF16, FP16 trades exponent range for mantissa precision. FP16 can distinguish between closer values but cannot represent numbers as large or as small as BF16. The practical consequence is gradient underflow: when gradients become very small during training, FP16 rounds them to zero, killing the learning signal. The fix is loss scaling, where you multiply the loss by a large factor before backpropagation to keep gradient magnitudes in FP16's representable range, then scale them back down before the optimizer step. Loss scaling adds complexity and occasional instability. This is why BF16 replaced FP16 as the default training format on hardware that supports both. FP16 is still relevant on older GPUs that lack BF16 support, such as V100 and consumer GPUs before RTX 30 series. It is also used in some inference frameworks where the higher mantissa precision of FP16 can produce slightly better output quality than BF16 for certain tasks.

**FP8 — 8 bits. The frontier training format.** FP8 stores each value in a single byte and comes in two variants that serve different purposes. **E4M3** uses 4 exponent bits and 3 mantissa bits, optimized for forward pass computations where value ranges are more predictable. **E5M2** uses 5 exponent bits and 2 mantissa bits, optimized for backward pass computations where gradient magnitudes vary more widely. H100 and H200 tensor cores execute FP8 matrix multiplications natively, delivering approximately 2x speedup over BF16 and 4x over FP32. Memory savings are proportional: a 70B parameter model that requires 140 gigabytes in BF16 requires 70 gigabytes in FP8. Enterprise teams have validated FP8 training at scales above 350 billion parameters in production. The challenge is stability: FP8's limited dynamic range requires careful management of scaling factors, sensitive layers like embeddings and output projections often remain in BF16, and loss curves must be monitored for instability.

**INT8 — 8 bits. Integer quantization for inference.** Unlike floating-point formats, INT8 represents values as integers on a uniform grid between a minimum and maximum value. There is no exponent or mantissa. All 8 bits encode a position on the grid, giving you 256 possible values evenly distributed across the range. INT8 is the workhorse format for inference quantization. It reduces model size by 4x compared to FP32 and 2x compared to FP16 or BF16, and most hardware has optimized INT8 inference kernels. The limitation is that a uniform grid cannot efficiently represent distributions where values cluster in certain ranges, which is common in neural network weights. Techniques like per-channel quantization and asymmetric quantization partially address this by adjusting the grid for each layer or channel.

**INT4 and FP4 — 4 bits. Aggressive compression.** Four-bit quantization reduces model size to roughly one-eighth of FP32. INT4 uses a 4-bit integer grid with 16 possible values. FP4, increasingly used in the NVIDIA Blackwell architecture with its native NVFP4 support, uses a tiny floating-point format, typically 1 sign bit, 2 exponent bits, and 1 mantissa bit. The difference matters: FP4's non-uniform spacing better matches the distribution of neural network weights, which tend to cluster around zero. INT4 quantization is what QLoRA uses to compress the frozen base model during training, enabling 70B parameter models to fit on a single GPU. For deployment, 4-bit quantization through GPTQ, AWQ, or GGUF delivers 60 to 75 percent reductions in memory and cost. Quality degradation is typically 1 to 3 percentage points on aggregate benchmarks, but task-specific capabilities, particularly those refined through fine-tuning, may degrade more sharply. Four-bit quantization is the practical floor for most production workloads. Below 4 bits, quality degradation accelerates.

**INT2, Ternary, and Binary — 2 bits or fewer. Research frontiers.** Two-bit quantization maps weights to four possible values. Ternary quantization uses only three values: negative one, zero, and positive one. Binary quantization uses two values: negative one and positive one. These extreme compression schemes reduce model size by 16x or more compared to FP32 but produce substantial quality degradation for most tasks. Research teams explore these formats for edge deployment where memory and power constraints are severe, such as mobile devices or embedded systems. No mainstream production fine-tuning or deployment uses sub-4-bit quantization in 2026, though academic results continue to narrow the quality gap.

## What Quantization Actually Is

Quantization is the process of mapping values from a high-precision format to a lower-precision format. When you quantize a model from BF16 to INT4, you are taking each weight, which can be any of roughly 65,000 possible BF16 values, and mapping it to one of 16 possible INT4 values. Information is lost in this mapping. Quantization is fundamentally lossy compression.

The quality of quantization depends on how well the lower-precision grid captures the distribution of the original weights. If weights are uniformly distributed across a wide range, uniform INT quantization works well. If weights cluster near zero with long tails, as they typically do in transformers, non-uniform formats like FP4 or techniques like group quantization, which use different scaling factors for groups of weights, preserve more information.

**Post-training quantization** applies compression after training is complete. You take a trained model in BF16 or FP32, run a calibration dataset through it to understand weight and activation distributions, and then map weights to the target precision. GPTQ, AWQ, and GGUF are all post-training quantization methods. The advantage is simplicity: you quantize once and deploy. The risk is that quantization may degrade capabilities the training process carefully optimized.

**Quantization-aware training** simulates quantization during the training process. The forward pass uses quantized weights, but the backward pass computes gradients in full precision. The model learns to perform well despite quantization noise, producing a model that maintains quality when actually quantized for deployment. Quantization-aware training increases training time by 20 to 40 percent but produces quantized models that often match or exceed the quality of post-training quantized models.

**Quantization-aware initialization**, as used by LoftQ, jointly optimizes the initial quantization and LoRA adapter values to minimize the reconstruction error from the start. This approach consistently outperforms standard QLoRA, where quantization and adapter initialization happen independently.

## Training Precision vs Inference Precision

Training and inference have different precision requirements because they involve different operations. Training computes forward passes, backward passes with gradient calculations, and optimizer updates. Inference computes only forward passes. The backward pass involves gradient magnitudes that span many orders of magnitude, demanding wider dynamic range. The forward pass involves more predictable value ranges, allowing more aggressive compression.

This is why you might train in BF16 but deploy in INT4. Training needs the dynamic range to handle extreme gradient values without underflow. Inference needs only enough precision to produce outputs that are indistinguishable from the full-precision model for your specific task. The gap between training precision and inference precision is where cost savings live. A model trained in BF16 at 140 gigabytes can be deployed in INT4 at 35 gigabytes, cutting GPU memory requirements by 75 percent and enabling cheaper hardware or higher throughput.

The critical risk is assuming that training precision quality carries over automatically after quantization. It does not. A model fine-tuned to achieve 96 percent accuracy in BF16 might drop to 93 percent in INT8 or 88 percent in INT4 on the specific capabilities you optimized. Quantization does not degrade all capabilities equally. It tends to hit narrow, fine-tuned capabilities harder than broad general capabilities because fine-tuning adjusts specific weight patterns that quantization disrupts. Always evaluate your quantized model on the exact tasks you fine-tuned for, with particular attention to edge cases and rare patterns.

## The Precision Decision Framework

For training, the decision is straightforward in 2026. Use BF16 as the default on any GPU that supports it, which includes A100, H100, H200, and most recent datacenter and consumer cards. Use FP8 on H100 or H200 when training large models where the 2x speedup justifies the additional stability management, and keep embedding and output layers in BF16. Use FP16 only on older GPUs that lack BF16 support, and enable loss scaling. Use FP32 only for optimizer state accumulation, never for the full training loop.

For inference and deployment, evaluate empirically. Start with INT8 quantization, which preserves quality for most tasks with 2x compression over BF16. If INT8 meets your quality bar and you need further compression, test INT4 or FP4 with careful stratified evaluation across all critical capabilities. Use AWQ when quality preservation on specific capabilities matters most. Use GPTQ for general-purpose GPU deployment. Use GGUF for CPU or edge deployment. Test every quantized model on your actual task distribution before deploying to production.

For the gap between training and inference precision, budget for validation. The question is not whether quantization degrades quality. It always does, at least slightly. The question is whether the degradation matters for your specific task at your specific quality threshold. A model with 5 percentage points of quality margin between current performance and minimum acceptable performance can safely absorb 2 to 3 points of quantization loss. A model with 2 points of margin cannot.

## Hardware and Precision Compatibility

Not every GPU supports every precision format. Understanding compatibility prevents wasted time configuring formats your hardware cannot execute natively.

NVIDIA V100 GPUs support FP32 and FP16 with tensor core acceleration. They do not support BF16 or FP8 in hardware. Training on V100s requires FP16 mixed precision with loss scaling.

NVIDIA A100 GPUs support FP32, TF32, FP16, and BF16 with tensor core acceleration. A100s do not support FP8 in hardware. BF16 is the default training precision on A100s, and TF32 provides transparent acceleration for FP32 operations.

NVIDIA H100 and H200 GPUs support all of the above plus FP8 with dedicated tensor cores. FP8 delivers 2x throughput over BF16 on these GPUs. H200s add 141 gigabytes of HBM3e memory, enabling larger models at any precision.

NVIDIA Blackwell GPUs, the B100 and B200 generation, add native FP4 support through NVFP4 acceleration. This enables 4-bit training and inference at hardware-accelerated speeds, a capability that was software-emulated on previous generations.

AMD MI300X GPUs support FP32, FP16, BF16, and FP8. AMD's CDNA 4 architecture adds MXFP4 support for 4-bit operations.

Consumer GPUs like the RTX 4090 support FP32, FP16, BF16, and INT8. They lack the FP8 tensor core support of datacenter GPUs, which limits some optimization paths but does not prevent effective fine-tuning using BF16.

Matching your precision strategy to your hardware is not optional. Configuring FP8 training on A100s will fall back to software emulation, running slower than BF16 rather than faster. Configuring BF16 on V100s will either error or fall back to FP16 without the stability benefits you expected. Know your hardware before choosing your precision.

## Common Precision Mistakes and How to Diagnose Them

The most common precision mistake is using FP16 on hardware that supports BF16, then debugging gradient instability that BF16 would have prevented. The symptom is training loss spikes, NaN values in gradients, or training runs that diverge after appearing stable for hundreds of steps. The fix is switching to BF16 and removing loss scaling, which is no longer needed.

The second most common mistake is assuming quantization is lossless because aggregate benchmarks look good. A team quantizes from BF16 to INT4, runs a standard benchmark, sees only 1.5 percent degradation, and deploys. Two weeks later they discover that their fine-tuned capability on rare medical terminology has degraded by 12 percent, invisible in aggregates but catastrophic for their use case. The fix is stratified evaluation: test quantized models on every critical capability segment independently, not just aggregate metrics.

The third mistake is ignoring precision when planning memory. A team provisions hardware for a 70B model assuming BF16 precision, calculates 140 gigabytes for weights plus memory for optimizer states and activations, and concludes they need 4 A100 GPUs. They do not realize that switching to FP8 for training would halve the weight memory to 70 gigabytes, potentially changing the parallelism strategy from FSDP to data parallelism and improving training throughput. Precision and parallelism interact: lower precision changes your memory math, which changes your sharding strategy, which changes your communication overhead.

The fourth mistake is mixing precisions without understanding the interaction. Training adapter weights in FP16 on a base model quantized to INT4 through QLoRA works because the framework manages the precision boundaries. Training adapter weights in FP8 on a base model quantized to INT4 may introduce precision interactions that the framework does not handle well, producing subtle quality degradation. When mixing precisions across components, verify that your framework explicitly supports the combination you are using.

## The Future of Precision: FP4 Training and Beyond

The precision frontier is moving below 8 bits for training, not just inference. Research teams have demonstrated fully quantized training at FP4 precision across 200 billion tokens, showing that models can learn effectively even when all forward and backward computations use 4-bit arithmetic. NVIDIA's Blackwell architecture provides native FP4 tensor core support, making this a hardware-accelerated option rather than a software approximation.

FP4 training in 2026 is not yet the default. It requires careful scaling factor management, mixed-precision strategies where critical operations remain at higher precision, and extensive validation. But the trajectory is clear: each hardware generation enables lower training precision with maintained quality, and each step down halves memory requirements and doubles throughput. Teams planning infrastructure for 2027 and beyond should factor FP4 readiness into their hardware decisions.

The broader principle is that precision is not a fixed property of your pipeline. It is a tunable parameter at every stage, from training to deployment to edge inference. The teams that understand precision at the bit level, not just as a configuration flag, make better decisions about hardware provisioning, parallelism strategy, quantization methods, and quality validation. They spend less on compute, ship faster, and catch precision-related failures before they reach production.

The next subchapter examines how to navigate the expanding landscape of fine-tuning platforms, comparing the cost, flexibility, and lock-in trade-offs of every major option available in 2026.
