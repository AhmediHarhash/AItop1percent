# 1.2 — The Alternatives Ladder: Prompt, Few-Shot, RAG, Then Fine-Tune

Fine-tuning is the last resort, not the first tool you reach for when model performance falls short. This principle seems obvious in theory but gets violated constantly in practice. Teams jump to fine-tuning because it feels like the sophisticated engineering solution, the move that separates professionals from prompt hackers. But in June 2025, a legal technology company proved the opposite. They faced a contract clause extraction problem where GPT-5 delivered 76% recall and they needed 90%. The engineering lead wanted to fine-tune immediately—they had budget, time, and 10,000 labeled contracts ready. But the VP of Engineering enforced a discipline: exhaust the alternatives ladder first. One week of prompt refinement brought recall to 82%. Another week adding ten few-shot examples pushed it to 87%. Two weeks building a retrieval system that pulled relevant clause examples from a curated database hit 91%, exceeding the target. Fine-tuning was never needed. The alternatives ladder saved four months and $200,000, and the pattern repeats across every domain where teams resist the temptation to over-engineer.

This is not a story about a team that got lucky. This is a story about a team that followed a disciplined process. They treated fine-tuning as the last resort, not the first option. They climbed the alternatives ladder one rung at a time, and they stopped climbing as soon as they hit their target. This discipline is the difference between teams that ship fast and teams that burn months on unnecessary model training.

The alternatives ladder is a decision framework. It defines the order in which you try techniques to solve an LLM quality problem. The order is not arbitrary. It is based on cost, speed, and reversibility. You start with the cheapest, fastest, most reversible technique—prompt engineering—and you only move to the next rung if the current rung cannot get you to your target. Each rung has a cost and a ceiling. Understanding those costs and ceilings is what allows you to make rational decisions about when to move up the ladder and when to stop climbing.

## The Four Rungs

The alternatives ladder has four rungs. First is **prompt engineering**: modifying the system message, user message, and conversation structure to improve model outputs. Second is **few-shot examples**: providing the model with examples of correct input-output pairs to demonstrate the task. Third is **retrieval-augmented generation**: injecting relevant external information into the prompt to give the model context it does not have in its weights. Fourth is **fine-tuning**: training the model on a dataset of task-specific examples to shift its behavior.

These four techniques are not mutually exclusive. You can combine them. You can use few-shot examples in your prompt. You can use RAG alongside fine-tuning. But the ladder defines the order in which you explore them when solving a new problem. You start at the bottom and work your way up. You do not skip rungs. If you jump straight to fine-tuning without trying prompting and few-shot, you are making an unjustified bet that the simpler techniques will not work. That bet is expensive, and it is usually wrong.

The ladder also defines the investment threshold for each rung. Prompt engineering should take hours to days, not weeks. If you spend more than a week iterating on a prompt, you are either overthinking it or the problem genuinely requires more than prompting. Few-shot engineering should take days to a week. You are finding good examples, testing them, refining them. RAG should take one to four weeks, depending on whether you already have retrieval infrastructure. You are building or integrating a vector database, writing retrieval logic, tuning ranking and reranking. Fine-tuning should take two to six months. You are collecting data, building training infrastructure, running experiments, and deploying a custom model.

The investment escalates rapidly. Prompting costs hours. Fine-tuning costs months. This escalation is why you must exhaust each rung before moving to the next. If you can solve the problem with prompting, you save weeks or months. If you can solve it with RAG, you save months compared to fine-tuning. The ladder enforces a discipline of trying cheap solutions first.

## Rung One: Prompt Engineering

Prompt engineering is the process of modifying the text you send to the model to change its behavior. This includes the system message, the user message, the structure of the conversation, the instructions you provide, the constraints you specify, and the output format you request. Prompt engineering is the most powerful tool you have for controlling LLM behavior, and it is the tool most teams underuse.

The ceiling of prompt engineering is higher than most teams assume. In 2026, with models like GPT-5, Claude Opus 4, and Gemini 3 Pro, you can achieve extraordinarily complex behavior through prompting alone. You can specify multi-step reasoning processes, define output schemas, enforce tone and style, handle edge cases, provide glossaries, and decompose tasks into subtasks. The models are instruction-following machines. If you write clear, detailed, well-structured instructions, they will follow them.

The failure mode of prompt engineering is underinvestment. Teams spend 30 minutes writing a prompt, see mediocre results, and conclude that prompting is not powerful enough. They do not iterate. They do not test variations. They do not analyze failure cases to understand what the prompt is missing. They treat prompting as a throwaway step before the real work of fine-tuning, and they leave massive performance gains on the table.

Effective prompt engineering requires iteration. You write a prompt, you test it on a representative set of inputs, you analyze the outputs, you identify patterns in the failures, and you revise the prompt to address those patterns. This cycle repeats until you hit your quality target or until you are confident that prompting alone cannot get you there. A well-engineered prompt is often 500 to 2000 tokens. It includes detailed instructions, examples of edge cases, definitions of ambiguous terms, and explicit constraints on output format and content. It is not a one-sentence instruction.

The cost of prompt engineering is minimal. You are editing text and running inference calls. You do not need new infrastructure. You do not need labeled datasets. You do not need GPU budgets. You can iterate in minutes. A single engineer can test dozens of prompt variations in a day. This speed is why prompting is the first rung. You get fast feedback, and you pay almost nothing for it.

The signal that you have hit the ceiling of prompt engineering is consistent failure despite iteration. If you have spent a week iterating on the prompt, testing dozens of variations, analyzing failure modes, and you are still falling short of your target by a significant margin, you have evidence that prompting alone is insufficient. At that point, you move to the next rung.

## Rung Two: Few-Shot Examples

Few-shot examples are input-output pairs that you include in the prompt to demonstrate the task. You show the model what correct behavior looks like by giving it examples. The model learns from the examples in-context, without any parameter updates. Few-shot learning is a form of prompting, but it is a distinct enough technique that it deserves its own rung on the ladder.

The ceiling of few-shot learning is task demonstration. If your task can be clearly illustrated by examples, few-shot learning can be extremely effective. You can teach the model to follow a specific format, to apply a specific reasoning pattern, to adopt a specific tone, or to handle specific edge cases. The examples encode the task in a way that natural language instructions sometimes cannot. They show rather than tell.

The cost of few-shot learning is example curation. You need to find or create high-quality examples. The examples need to be representative of the task. They need to cover the important edge cases. They need to be correct. Poor examples will degrade performance, not improve it. Curating good examples takes time. You are not writing thousands of examples—few-shot typically uses between 1 and 20 examples—but each example needs to be carefully chosen.

The strategy for few-shot learning is diversity and coverage. You want examples that span the range of inputs you expect to see. If your task involves multiple subtypes, you want at least one example per subtype. If your task has common failure modes, you want examples that demonstrate correct handling of those failure modes. If your task has ambiguous cases, you want examples that show how to resolve the ambiguity.

Few-shot learning works best when the task has a clear structure. If your task is classification, few-shot examples show the model which inputs map to which classes. If your task is extraction, few-shot examples show the model which parts of the input to extract and how to format the output. If your task is generation, few-shot examples show the model the style and structure of correct outputs.

The failure mode of few-shot learning is example selection. If your examples are not representative, the model will learn the wrong pattern. If your examples are too simple, the model will fail on harder cases. If your examples are inconsistent, the model will not learn a coherent pattern. Selecting good examples requires understanding the task deeply. You need to know what makes the task hard, what the edge cases are, and what patterns you want the model to internalize.

The signal that you have hit the ceiling of few-shot learning is diminishing returns. If you add 5 examples and see a large improvement, then add 5 more and see a small improvement, then add 5 more and see no improvement, you have likely exhausted the value of few-shot for this task. At that point, more examples will not help. You need a different technique.

## Rung Three: Retrieval-Augmented Generation

Retrieval-augmented generation is the process of injecting relevant external information into the prompt to give the model context it does not have in its weights. You retrieve documents, database records, previous conversations, or other data, and you include that data in the prompt. The model uses the retrieved information to generate better outputs.

The ceiling of RAG is knowledge access. RAG solves problems where the model lacks information. If your task requires knowing specific facts, citing specific sources, referencing specific past interactions, or adhering to specific policies that are not in the model's training data, RAG is the right tool. RAG does not teach the model new reasoning patterns. It gives the model access to information it needs to reason correctly.

The cost of RAG is infrastructure. You need a retrieval system. That means a vector database or a search index, embeddings, ranking logic, reranking, and integration into your prompt construction pipeline. If you already have retrieval infrastructure—if you have built search or recommendations before—RAG is relatively cheap. If you are starting from scratch, RAG can take two to four weeks to build and tune.

The strategy for RAG is retrieval precision. You do not want to retrieve everything. You want to retrieve the most relevant information for the current input. If you retrieve too little, the model will not have the context it needs. If you retrieve too much, you will hit token limits, increase latency, and add noise that degrades performance. Retrieval tuning is the process of finding the right balance: the right number of retrieved documents, the right ranking function, the right reranking strategy.

RAG works best when the task is knowledge-intensive. If you are building a support bot that needs to reference documentation, RAG is essential. If you are building a contract analysis system that needs to compare clauses against a database of known good clauses, RAG is essential. If you are building a personalized assistant that needs to remember past user preferences, RAG is essential. These are tasks where the model cannot succeed without external information, and RAG is how you provide that information.

The failure mode of RAG is retrieval failure. If your retrieval system returns irrelevant documents, RAG will hurt performance, not help it. If your retrieval system misses critical documents, the model will not have the information it needs. Retrieval quality is the bottleneck. You can have a perfect prompt and a perfect model, but if retrieval fails, the system fails. This is why RAG projects spend most of their time on retrieval tuning, not on model selection.

The signal that you have hit the ceiling of RAG is persistent knowledge gaps. If you have built a high-quality retrieval system, tuned ranking and reranking, and the model is still failing because it does not know something, RAG cannot help you. At that point, the missing knowledge is too implicit to retrieve, or the reasoning required is too complex to fit in a prompt. That is when you consider fine-tuning.

## Rung Four: Fine-Tuning

Fine-tuning is the process of training the model on a dataset of task-specific examples to shift its behavior. You provide thousands to tens of thousands of input-output pairs, you run a training loop, and you get a model that has internalized patterns from your data. Fine-tuning is the most expensive, slowest, least reversible technique on the ladder, and it is the technique you use only when the first three rungs have been exhausted.

The ceiling of fine-tuning is behavior internalization. Fine-tuning allows the model to learn patterns that are too complex to specify in a prompt, too subtle to demonstrate with a few examples, or too context-dependent to retrieve. Fine-tuning is for tasks where the correct behavior requires deep domain knowledge, nuanced judgment, or stylistic consistency that cannot be achieved any other way.

The cost of fine-tuning is time, data, and compute. You need thousands of labeled examples, which means data collection, annotation, quality control. You need training infrastructure, which means GPUs, experiment tracking, model versioning. You need evaluation infrastructure, which means test sets, metrics, automated evals. You need time, which means two to six months from kickoff to production. Fine-tuning is not a quick fix. It is a major project.

The strategy for fine-tuning is dataset quality. The performance of a fine-tuned model is almost entirely determined by the quality of the training data. If your data is noisy, inconsistent, or unrepresentative, the fine-tuned model will be noisy, inconsistent, and unrepresentative. If your data is clean, diverse, and representative, the fine-tuned model will be high-quality. Data quality is the single most important variable in fine-tuning success.

Fine-tuning works best when the task requires the model to internalize complex behavior that cannot be encoded in a prompt. If you need the model to write in a highly specific voice that cannot be described in instructions, fine-tuning can help. If you need the model to apply domain-specific reasoning that requires deep knowledge, fine-tuning can help. If you need the model to handle thousands of edge cases that cannot all fit in a prompt, fine-tuning can help. These are the narrow cases where fine-tuning is justified.

The failure mode of fine-tuning is overfitting to the training data. The fine-tuned model performs well on examples similar to the training set and poorly on examples outside the training distribution. This is especially common when the training set is small or unrepresentative. Overfitting is why fine-tuning requires large, diverse datasets and why you must always validate on a held-out test set.

The signal that fine-tuning is necessary is failure across the first three rungs. You have iterated on prompts, you have added few-shot examples, you have built a retrieval system, and you are still not hitting your target. You have quantified the gap. You have verified that the gap is not due to bad prompts, bad examples, or bad retrieval. At that point, fine-tuning becomes a valid option.

## The Cost Differential

The cost differential across the ladder is exponential. Prompting costs hours of engineering time and a few dollars in API calls. Few-shot costs days of engineering time and a few dozen dollars in API calls. RAG costs one to four weeks of engineering time, infrastructure costs for a vector database, and ongoing retrieval latency. Fine-tuning costs two to six months of engineering time, $50,000 to $200,000 in compute, and ongoing maintenance.

This differential is why the ladder is ordered the way it is. If you can solve the problem at a lower rung, you save orders of magnitude in cost and time. If you skip rungs, you pay the exponential cost without knowing whether it was necessary. The ladder enforces a discipline of trying cheap solutions first and only escalating when you have evidence that the cheap solutions are insufficient.

The time differential is also critical. Prompting and few-shot give you feedback in hours to days. You can iterate fast. You can test hypotheses, see results, and adjust. RAG gives you feedback in weeks. You build the retrieval system, integrate it, and evaluate. Fine-tuning gives you feedback in months. You collect data, train, evaluate, and iterate. The longer the feedback loop, the more risk you take. If you start with fine-tuning and discover after four months that it does not work, you have lost four months. If you start with prompting and discover after two days that it does not work, you have lost two days.

## Specific Examples of Tasks Solved at Each Rung

Prompting alone solves the majority of tasks. Classification, summarization, formatting, tone adjustment, basic extraction, instruction following—all of these can be solved with well-engineered prompts. A legal tech company used prompting to classify contract clauses into 20 categories at 88% accuracy. A customer support company used prompting to rewrite user messages into professional tone at 95% quality. A healthcare company used prompting to extract medication names and dosages from clinical notes at 92% recall. None of these tasks required fine-tuning. They required good prompts.

Few-shot learning solves tasks where the desired behavior is hard to describe but easy to demonstrate. A financial services company used few-shot examples to teach GPT-5 to extract trading signals from earnings call transcripts. The task was too nuanced to describe in instructions—what counts as a signal versus noise depends on subtle language cues—but providing 10 examples of correct signal extraction allowed the model to generalize. A marketing company used few-shot learning to teach Claude to rewrite ad copy in a specific brand voice. The brand voice was hard to specify in words but easy to show through examples.

RAG solves tasks where the model needs access to external knowledge. A legal research company used RAG to build a case law citation system. The model could not know all case law—it was not in the training data—but by retrieving relevant cases and injecting them into the prompt, the model could generate accurate citations. A SaaS company used RAG to build a support bot that answered questions by retrieving relevant help articles. The model could not memorize the entire knowledge base, but retrieval gave it the information it needed for each query.

Fine-tuning solves tasks where behavior is too complex for prompts, too subtle for examples, and too implicit for retrieval. A medical imaging company fine-tuned a vision-language model to generate radiology reports in the exact style used by their hospital's radiologists. The style was highly specific, included domain jargon, and required clinical judgment that could not be prompted. A customer service company fine-tuned GPT-5 to handle escalation conversations with a specific empathetic tone and de-escalation strategy that was too complex to encode in a prompt.

## The Ceiling Signals That Tell You to Move Up

You know you have hit the ceiling of a rung when iteration stops producing improvements. For prompting, the ceiling signal is that you have tried dozens of prompt variations, analyzed failure modes, and performance has plateaued. For few-shot, the ceiling signal is that adding more examples does not improve performance. For RAG, the ceiling signal is that retrieval is returning the right documents but the model is still failing.

Another ceiling signal is that you have quantified the gap and it is too large to close with iteration. If your target is 95% accuracy and you are at 70% after a week of prompt iteration, prompting is unlikely to close a 25-point gap. If you are at 92% after adding few-shot examples and your target is 95%, you might be able to close the 3-point gap with RAG. If you are at 93% after building a retrieval system and your target is 95%, fine-tuning might close the final 2 points. The size of the gap informs whether moving up the ladder is justified.

A third ceiling signal is that you have identified a specific failure mode that the current rung cannot address. If your prompt fails because the model does not have access to a knowledge base, that is a RAG problem, not a prompting problem. If your few-shot examples fail because the task requires internalizing hundreds of edge cases that cannot all fit in the prompt, that is a fine-tuning problem, not a few-shot problem. Understanding the root cause of failure tells you which rung you need.

## Why Most Teams Skip Rungs and Pay for It

Teams skip rungs because fine-tuning feels like the professional solution. Prompting feels like a hack. Few-shot feels like a workaround. RAG feels like duct tape. Fine-tuning feels like engineering. This perception is wrong, but it is pervasive. Teams want to do the sophisticated thing, not the simple thing, and they pay for that preference with months of wasted time.

Teams also skip rungs because they do not enforce the discipline. No one is forcing them to try prompting first. No one is requiring them to document why prompting failed before moving to fine-tuning. The decision to fine-tune is made in a planning meeting based on intuition, not data. The team assumes fine-tuning is necessary because the problem feels hard, and they proceed without testing that assumption.

The teams that succeed are the teams that enforce the ladder. They make it a rule: you do not move to the next rung until you have exhausted the current rung and documented why it failed. They require quantitative evidence. They require iteration logs. They require failure analysis. This discipline prevents premature escalation and ensures that expensive techniques are only used when necessary.

Another reason teams skip rungs is impatience. Iterating on prompts feels slow when you are eager to solve the problem. The promise of fine-tuning is that you can collect data once, train once, and never think about prompts again. This promise is appealing, but it is false. Fine-tuning does not eliminate the need for iteration. You still iterate on your training data, on your hyperparameters, on your evaluation metrics. The iteration happens at a different layer, but it still happens, and it happens more slowly and more expensively than prompt iteration.

The impatience is especially destructive when combined with inexperience. A team that has never iterated deeply on prompts does not know how much performance is available through prompting alone. They assume that if the first prompt does not work, prompting is inadequate. They do not realize that the difference between a mediocre prompt and a great prompt can be 20 or 30 percentage points of accuracy. They skip the prompt iteration phase because they do not know what they are missing.

Skipping rungs also happens because of incomplete mental models. Many engineers think of prompting as a way to give instructions and fine-tuning as a way to train behavior. This framing suggests that training is more powerful than instructions, which leads to the conclusion that fine-tuning should be the default. But this framing is wrong. Modern LLMs are instruction-tuned. They are built to follow complex instructions. Prompting is not just about giving simple commands—it is about encoding task specifications, edge case handling, output constraints, and reasoning strategies into text. When you understand prompting at that level, you see that it is far more powerful than most teams assume.

## Enforcing the Ladder in Practice

Enforcing the ladder requires process. The process must make it impossible to start fine-tuning without first exhausting the earlier rungs. The simplest process is a checklist. Before a team can allocate budget or resources to fine-tuning, they must complete the checklist. The checklist includes: quantitative baseline from current prompt, documentation of at least 20 tested prompt variations, analysis of failure modes, attempt at few-shot learning with at least 5 examples, attempt at RAG if the task is knowledge-intensive, and quantitative gap analysis showing that the remaining performance gap justifies fine-tuning.

This checklist is not bureaucracy. It is discipline. It forces the team to do the work of exhausting alternatives before committing to the expensive path. Teams that complain about the checklist are usually teams that have not actually tried the alternatives. Teams that have tried the alternatives and hit the ceiling appreciate the checklist because it validates their decision to escalate.

Another enforcement mechanism is review. Before a fine-tuning project starts, it must be reviewed by someone who is not on the team and who has experience with prompt engineering, few-shot learning, and RAG. The reviewer looks at the evidence and asks: have you really exhausted the alternatives? The reviewer is skeptical. They push back. They ask for more iteration. They ask why a specific prompt variation was not tried. They ask whether the few-shot examples are representative. They ask whether the retrieval system is well-tuned. This adversarial review catches the cases where the team is escalating prematurely.

A third enforcement mechanism is cost transparency. The team proposing fine-tuning must estimate the full cost: engineering time, compute budget, opportunity cost, maintenance cost. They must compare that cost to the cost of continuing to iterate on prompting or RAG. They must justify the cost differential. If the fine-tuning project costs $300,000 and the prompting project costs $5,000, the fine-tuning project must deliver value that justifies the 60x cost increase. Most teams do not do this analysis. When forced to do it, many teams realize the cost differential is not justified.

## Combining Rungs: When and How

The ladder defines the order of exploration, but it does not prohibit combination. You can use few-shot examples in your prompts. You can use RAG alongside few-shot. You can even use RAG alongside fine-tuning. The rungs are not mutually exclusive—they are complementary. The question is when to combine them and how.

The general rule is that you combine rungs when each rung is addressing a different aspect of the problem. If your task requires both external knowledge and specific formatting, you use RAG for knowledge and few-shot for formatting. If your task requires both domain-specific reasoning and access to a knowledge base, you might fine-tune for reasoning and use RAG for knowledge access. The combinations make sense when the rungs are solving different problems.

The combinations do not make sense when you are using multiple rungs to solve the same problem. If you are using few-shot examples and fine-tuning to both teach the model a specific output format, you are duplicating effort. Pick one. If few-shot works, you do not need fine-tuning. If few-shot does not work and you need fine-tuning, the few-shot examples are not adding value.

The most common valid combination is RAG plus fine-tuning. RAG provides the knowledge, and fine-tuning provides the reasoning or style. A medical Q&A system might use RAG to retrieve relevant medical literature and fine-tuning to generate answers in the specific clinical style required by the hospital. The RAG and fine-tuning are addressing different problems, so the combination is justified.

Another valid combination is prompting plus RAG plus few-shot. The prompt provides the instructions, RAG provides the knowledge, and few-shot provides examples of how to integrate the knowledge into the output. This three-rung combination is common in production systems. It is not a sign of failure—it is a sign that each technique is pulling its weight.

## The Ladder as a Diagnostic Tool

The ladder is not just a decision framework—it is a diagnostic tool. When a system is underperforming, you use the ladder to diagnose where the problem is. If the model is producing low-quality outputs, you ask: is this a prompt problem, a few-shot problem, a RAG problem, or a fine-tuning problem?

If the model is not following instructions, it is probably a prompt problem. You need clearer instructions, better constraints, or more explicit output formatting. If the model is following instructions but producing outputs in the wrong style, it might be a few-shot problem. You need examples that demonstrate the correct style. If the model is missing critical information, it is a RAG problem. You need better retrieval. If the model has the right information and is following instructions but is still producing low-quality outputs because the task requires internalized domain expertise, it might be a fine-tuning problem.

The ladder helps you avoid misdiagnosis. Teams often misdiagnose RAG problems as fine-tuning problems. The model is failing because it does not have access to the right information, but the team concludes that fine-tuning will fix it. They fine-tune, and the model still fails because it still does not have the information. The correct diagnosis was RAG, not fine-tuning. The ladder forces you to ask: have we given the model the information it needs? If not, fine-tuning will not help.

Another common misdiagnosis is treating prompt problems as fine-tuning problems. The model is failing because the prompt is vague or incomplete, but the team concludes that fine-tuning will make the model more robust. They fine-tune, and the model performs slightly better on the training distribution but still fails on edge cases because the root issue—unclear instructions—was never addressed. The correct diagnosis was prompt engineering, not fine-tuning.

## The Iterative Nature of the Ladder

The ladder is not a one-time climb. You do not climb from prompting to fine-tuning and then stop. You iterate within each rung, and you iterate across rungs. You might start with prompting, hit the ceiling, move to few-shot, see improvement, go back to prompting to refine the instructions in light of what you learned from few-shot, then move to RAG, then go back to few-shot to add examples that incorporate the retrieved knowledge. The ladder is a framework for structured iteration, not a linear path.

This iterative approach is why the ladder saves time. You are getting feedback at every step. You are learning what works and what does not. You are building intuition about the task. By the time you consider fine-tuning, you have a deep understanding of the problem, the failure modes, and what success looks like. That understanding makes the fine-tuning project more likely to succeed because you know exactly what you are trying to fix.

The teams that skip the ladder and jump straight to fine-tuning do not have this understanding. They are flying blind. They collect data based on their initial intuition about the problem, they train, and they discover that the fine-tuned model does not solve the problem they thought it would solve. They have to start over. They have lost months. If they had climbed the ladder, they would have learned what the real problem was before committing to fine-tuning, and they would have saved the time.

## The Economic Argument

The economic argument for the ladder is straightforward. Prompting costs almost nothing. Few-shot costs almost nothing. RAG costs something but not much if you already have retrieval infrastructure. Fine-tuning costs a lot. The expected value calculation is simple: try the cheap things first. If they work, you save the cost of the expensive thing. If they do not work, you have lost almost nothing, and you have learned enough to make the expensive thing more likely to succeed.

The teams that ignore this economic argument are making an implicit bet that the cheap things will not work. That bet is usually wrong. The vast majority of tasks can be solved with prompting, few-shot, or RAG. The probability that fine-tuning is necessary is low. Jumping straight to fine-tuning is a high-cost, low-probability bet. It is a bad bet.

The economically rational strategy is to de-risk the bet by trying the cheap alternatives first. If the cheap alternatives fail, you have evidence that fine-tuning might be necessary, and the bet becomes less risky. If the cheap alternatives succeed, you have saved the cost of fine-tuning. Either way, you come out ahead.

The next subchapter introduces the decision framework: the structured process for deciding when fine-tuning is justified and when it is not.
