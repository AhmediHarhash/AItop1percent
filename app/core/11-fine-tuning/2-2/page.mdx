# 2.2 — Data Collection Strategies: Internal Logs, Human Authoring, Expert Annotation

In June 2025, a customer service software company launched a project to fine-tune GPT-4.5 for automated email response generation. They had three options for collecting training data: mine their existing customer support email logs, which contained 200,000 historical email exchanges between customers and support agents; have their top support agents write example responses from scratch based on common scenarios; or have experts annotate model-generated drafts by editing and improving them. The team chose the fastest path: they extracted 15,000 email pairs from the logs, filtered for quality by removing incomplete exchanges and non-English emails, and started training. The model performed reasonably well on common issues but struggled with two problems. First, the historical logs included responses written by agents with varying skill levels, from excellent senior agents to new hires still learning the product. The model learned the average quality, which was mediocre. Second, the logs did not include responses to new product features launched in the past six months because those features did not exist when the historical emails were written. The model could not handle questions about new features. After three months, the team paused and switched strategies. They had senior agents write 2,000 gold-standard responses covering all current features and common issues, then used expert annotation to scale that set by having the model generate drafts and having agents edit them. The combination of authored gold standards and annotated drafts produced a 10,000-example dataset with higher quality and better coverage. The retrained model performed significantly better, and the team learned an expensive lesson: the cheapest data collection method is not always the best method.

There are three primary strategies for collecting fine-tuning data: mining internal logs from production systems, human authoring from scratch by domain experts, and expert annotation of model outputs. Each strategy has different cost, quality, and scalability characteristics. Each is appropriate for different tasks and constraints. Most successful fine-tuning projects use a combination of strategies rather than relying on a single source.

## Mining Internal Production Logs

The most common data collection strategy is mining production logs: chat transcripts, support tickets, email exchanges, search queries, user interactions, API calls, or any other records of how users interact with your system. Production logs are appealing because they already exist, they are free or low-cost to extract, they represent real user needs, and they can be collected at massive scale.

The advantage of production logs is authenticity. The data reflects actual user behavior, actual use cases, and actual language. If you are fine-tuning a model to handle customer support queries, your support ticket logs contain exactly the types of queries users ask and the responses that resolved those queries. There is no gap between training data and production data because the training data comes from production.

The disadvantage of production logs is quality variability. Logs capture everything: excellent responses, mediocre responses, bad responses, incomplete exchanges, edge cases, spam, and errors. If you train on raw logs without filtering, you train the model to replicate the average quality of historical behavior, which may be lower than acceptable. Logs also reflect historical patterns that may be outdated. If your product has changed, your feature set has expanded, or your policies have updated, older logs may not reflect current reality.

Logs also contain noise. Customer emails include typos, fragments, off-topic content, and unclear requests. Support agent responses include boilerplate, internal notes accidentally sent to customers, placeholder text, and inconsistent formatting. Training on noisy logs without cleaning produces a model that generates noisy outputs.

Log mining works best when you have high-quality historical data, when the task has not changed significantly over time, when you can filter or curate the logs to remove low-quality examples, and when the base model already has strong general capabilities and you are fine-tuning primarily to adapt to your domain and tone. Log mining works poorly when historical quality is low, when the task has evolved, when logs are sparse or incomplete, or when you need the model to perform better than historical human performance.

A SaaS company fine-tuned a model for automated onboarding email sequences. They mined 50,000 onboarding emails sent by their customer success team over two years. They filtered the emails by agent performance: only emails written by top-performing agents with customer satisfaction scores above 4.5 out of 5 were included. This filtering reduced the dataset to 8,000 emails but ensured high quality. They also excluded emails older than one year to avoid outdated product references. The filtered dataset produced a model that generated onboarding emails matching the quality of the top agents. The key was aggressive filtering to raise the quality floor.

When mining logs, you must decide how to filter and sample. Random sampling from all logs gives you a dataset that reflects the average of historical behavior. Filtering by performance metrics — customer satisfaction, resolution time, escalation rate — gives you a dataset that reflects best-practice behavior. Filtering by recency gives you a dataset aligned with current reality. Filtering by case type gives you control over coverage and balance. The filtering strategy depends on your quality and coverage goals.

Log mining also requires careful handling of sensitive data. Production logs often contain personally identifiable information, proprietary customer data, confidential business information, and regulated content. You must scrub or anonymize this data before using it for training. The scrubbing process can introduce errors or degrade data quality if not done carefully. A healthcare company mining patient support logs for fine-tuning had to redact all patient names, dates of birth, medical record numbers, and treatment details. The redaction process used automated tools that sometimes over-redacted, removing medically relevant context. The team had to manually review redacted examples to verify that the data remained useful after redaction.

## Human Authoring from Scratch by Domain Experts

The second strategy is human authoring: having domain experts write training examples from scratch. The expert is given a task specification, a set of scenarios or prompts, and guidelines for how to respond. The expert writes high-quality outputs that meet your standards. The authored examples become training data.

The advantage of authoring is control. You define exactly what quality looks like, you specify the scenarios to cover, you ensure consistency, and you produce data that reflects your desired model behavior rather than historical behavior. Authoring is the best way to achieve high quality when you have clear standards, when historical data is unavailable or low-quality, and when you need the model to perform better than current human performance.

The disadvantage of authoring is cost and scale. Writing examples from scratch is time-consuming and expensive. A domain expert can typically produce five to twenty high-quality examples per hour, depending on task complexity. If you need 10,000 examples and experts produce ten per hour, you need 1,000 hours of expert time. At 100 dollars per hour for expert labor, that is 100,000 dollars. Authoring does not scale easily to datasets of 50,000 or 100,000 examples.

Authoring works best for tasks that require deep expertise, tasks with high stakes where quality is critical, tasks where you are defining new standards rather than replicating existing behavior, and tasks where you need a smaller but pristine dataset. Authoring works poorly for tasks that require massive datasets, tasks where expert time is prohibitively expensive, and tasks where production logs already provide sufficient quality.

A financial services firm fine-tuning a model for investment analysis reports used authoring. They had senior equity analysts write 1,500 example reports covering a range of industries, company sizes, and market conditions. Each report took 60 to 90 minutes to write. The total effort was 1,800 analyst-hours at a cost of approximately 270,000 dollars. The resulting model generated reports that met the firm's quality standards for client delivery. The high cost was justified by the task's high stakes and the lack of suitable historical data — the firm's historical reports were written in inconsistent formats and styles that they did not want to replicate.

When using authoring, you must provide clear guidelines to ensure consistency. If different experts interpret the task differently, the training data will be inconsistent, and the model will learn conflicting patterns. The guidelines should specify the output format, the level of detail, the tone and style, what information to include and exclude, and how to handle edge cases or ambiguous scenarios. You should also have experts complete a calibration phase where they author a small number of examples, review each other's work, and align on standards before scaling to the full dataset.

Authoring also benefits from templates or frameworks that guide the expert's work. For example, if you are authoring training data for a legal contract review model, you might provide a template that lists the key clauses to analyze and the risk levels to assign. The template ensures that all examples cover the same dimensions and follow the same structure, which produces more consistent training data.

A marketing technology company authoring training data for ad copy generation provided writers with a framework: target audience, product benefits to highlight, tone (professional, casual, urgent), length (short, medium, long), and call to action. The framework ensured that all authored examples included the necessary components and followed consistent patterns. The model trained on this data generated ad copy that adhered to the framework, making outputs predictable and controllable.

## Expert Annotation of Model Outputs

The third strategy is expert annotation: using the model (or a baseline prompted model) to generate draft outputs, then having experts review, edit, and approve or reject those drafts. The expert's edited version becomes the training label, paired with the original input. This strategy combines the scale of automated generation with the quality control of human expertise.

The advantage of expert annotation is efficiency. Editing a draft is faster than writing from scratch. An expert might produce ten authored examples per hour but can review and edit twenty to thirty generated drafts per hour. Annotation allows you to scale dataset creation while maintaining quality control. It also leverages the base model's general capabilities: the model generates reasonable drafts that cover common cases well, and the expert corrects errors, adds missing details, and refines tone.

The disadvantage of expert annotation is that the model's drafts can anchor the expert's judgment. If the draft is mostly correct but has subtle errors, the expert may miss those errors because they are primed by the draft's framing. If the draft is structured in a certain way, the expert may edit within that structure rather than reconsidering the structure entirely. Annotation bias can result in training data that is better than raw model outputs but not as good as fully authored examples.

Annotation works best when the base model already performs reasonably well and you are fine-tuning to improve quality incrementally, when you need to scale beyond what authoring alone can achieve, when the task is well-defined and expert edits are focused on specific improvements, and when you implement quality controls to detect and mitigate annotation bias. Annotation works poorly when the base model performs very poorly and generates unusable drafts, when the task requires creative or highly original outputs that the base model cannot approximate, and when experts are not trained to critically evaluate and edit drafts.

A legal services company used expert annotation to fine-tune a model for client intake form review. They used Claude Opus 4 to generate draft summaries of intake forms, identifying key legal issues and recommended next steps. Attorneys reviewed the drafts, corrected errors, added missing issues, and refined the recommendations. Attorneys could review and edit 25 drafts per hour, compared to writing five summaries per hour from scratch. The team collected 12,000 annotated examples in six weeks. The fine-tuned model generated summaries that matched attorney quality on 89% of cases.

When using annotation, you must decide how to handle low-quality drafts. If the model's draft is so poor that the expert has to rewrite it entirely, the efficiency advantage of annotation is lost. You should set a quality threshold: if the draft meets a minimum standard, annotate it; if not, discard it or route it to authoring. Some teams implement a two-tier process: high-quality drafts go to fast annotation, low-quality drafts go to full authoring.

You should also monitor inter-annotator agreement. If different experts edit the same draft in different ways, the training data becomes inconsistent. Calibration sessions and regular alignment meetings help maintain consistency. Some teams use a lead annotator who reviews a sample of annotated examples and provides feedback to the annotation team.

Annotation introduces a risk of over-reliance on the base model's patterns. If the base model consistently structures outputs in a certain way, and experts only make minor edits, the fine-tuned model may reinforce the base model's patterns rather than learning new patterns. To mitigate this, some teams mix annotation with authoring: use annotation for common cases where the base model performs well, and use authoring for edge cases, novel scenarios, or cases where you want the model to learn behavior that differs significantly from the base model.

## The Quality-Cost-Scale Tradeoffs

Each collection strategy sits on a three-dimensional tradeoff curve: quality, cost, and scale. Authoring produces the highest quality but is the most expensive and scales the least. Log mining scales the most and costs the least but produces variable quality. Annotation is intermediate: moderate cost, moderate scale, moderate quality.

The right choice depends on your task requirements, your budget, and your timeline. For high-stakes tasks where errors are costly, prioritize quality and choose authoring or heavily curated log mining. For tasks where you need massive datasets and errors are tolerable, prioritize scale and choose log mining with light filtering. For tasks where you need both quality and scale, use a hybrid approach: author a gold-standard core dataset and scale it with annotation or curated logs.

A translation service fine-tuning a model for legal document translation used a hybrid approach. They had professional legal translators author 1,000 gold-standard translations covering the most common document types and legal terminology. They used this core set to fine-tune an initial model, then used that model to generate draft translations for 10,000 additional documents. Translators annotated the drafts, correcting errors and improving fluency. The combination of 1,000 authored examples and 10,000 annotated examples produced a model that performed at professional translator quality for common document types and near-professional quality for rarer types.

The tradeoff also depends on base model capability. If the base model is already strong, annotation and log mining become more viable because the base model generates reasonable drafts or because the base model's general knowledge fills gaps in the log data. If the base model is weak, authoring becomes more necessary because the base model cannot generate useful drafts and log data alone may not provide sufficient signal.

In 2026, with models like GPT-4.5, Claude Opus 4, and Gemini 2, base capabilities are strong across many domains. Annotation is more efficient than it was with earlier models because the drafts are better. Log mining is more effective because the base model's general knowledge compensates for some log quality issues. But authoring remains essential for highly specialized tasks, regulated domains, and tasks where precision is non-negotiable.

## When to Use Each Strategy

Use log mining when you have access to high-quality production logs, when the task has not changed significantly, when you can apply aggressive filtering to raise quality, when you need large-scale data, and when the task involves matching existing human performance rather than exceeding it. Examples: customer support responses, chatbot conversations, FAQ generation, product recommendations.

Use authoring when you need the highest quality, when historical data is unavailable or unsuitable, when you are defining new standards, when the task requires deep expertise, when the dataset can be smaller, and when cost is justified by task stakes. Examples: legal contract drafting, medical diagnosis support, financial analysis, safety-critical decision systems, content that represents your brand voice.

Use annotation when the base model generates reasonable drafts, when you need to scale beyond authoring capacity, when experts can efficiently review and improve drafts, when the task is well-defined, and when you implement controls to prevent annotation bias. Examples: document summarization, content moderation, code review, email drafting, translation.

Use combinations when you need both quality and scale, when different parts of the task have different requirements, and when you can tier your data collection by case importance. Examples: authoring core examples for high-stakes cases and annotating for common cases; log mining for volume and authoring for edge cases; authoring templates and annotating variations.

## Common Mistakes in Data Collection

The most common mistake is choosing the collection strategy based on convenience rather than task requirements. Teams default to log mining because logs are available, even when log quality is too low. Teams avoid authoring because it is expensive, even when the task demands it. The result is training data that does not meet quality requirements, which produces a model that does not meet performance requirements.

The second most common mistake is failing to filter or curate log data. Teams extract logs and use them directly for training without reviewing quality, without removing outliers, and without checking coverage. The model learns from bad examples, and production performance suffers.

The third most common mistake is underestimating the time and cost of authoring. Teams commit to authoring 10,000 examples without calculating the expert-hours required. Midway through the project, they realize they cannot afford to complete the dataset and either settle for fewer examples or switch to a lower-quality collection method. The fix is to estimate costs upfront and budget appropriately.

The fourth most common mistake is inadequate guidelines for authoring or annotation. Teams ask experts to create training data without specifying quality standards, formatting conventions, or handling of edge cases. Experts interpret the task differently, producing inconsistent data. The model learns inconsistent patterns and performs unpredictably. The fix is detailed guidelines, calibration sessions, and alignment checks.

The fifth most common mistake is neglecting data diversity. Teams collect data for common cases and overlook rare cases, edge cases, and failure modes. The model performs well on common cases and fails on edge cases. The fix is deliberate sampling and coverage planning, ensuring that rare but important cases are included in the training set.

## How to Combine Sources Effectively

Most successful fine-tuning projects use multiple data sources. The key to combining sources effectively is to assign each source to the part of the task it handles best and to ensure consistency across sources.

A common pattern is to use authoring for the gold-standard core and annotation or log mining for scale. You author 500 to 2,000 examples that define the ideal behavior, then scale to 10,000 or 20,000 examples using annotation or curated logs. The authored core ensures high quality standards are established. The scaled data ensures the model has enough examples to generalize.

Another pattern is to use different sources for different case types. Use log mining for common cases where historical performance is acceptable, authoring for high-stakes cases where you need perfection, and annotation for moderate-stakes cases where you need quality improvement over the base model. This tiered approach allocates expert effort to where it has the most impact.

When combining sources, you must ensure consistency. If authored examples use one format and log-mined examples use another format, the model receives conflicting signals. You should apply the same formatting, the same quality filters, and the same guidelines to all data sources. Some teams run all collected data through a normalization pipeline that standardizes formatting, removes PII, and applies quality checks, ensuring that data from different sources looks the same to the model.

A tax preparation software company combined sources for fine-tuning a model to answer tax questions. They authored 800 examples covering complex tax scenarios that required CPA expertise. They annotated 5,000 examples by having the model generate answers to common tax questions and having CPAs edit the answers. They mined 12,000 examples from customer support logs, filtering for high-rated answers. All examples were normalized to the same question-answer format, reviewed for accuracy, and checked for coverage of the current tax year's rules. The combined dataset of 17,800 examples produced a model that handled common questions reliably and complex questions at CPA-level quality.

## Data Collection at Scale: Process and Tooling

Collecting tens of thousands of training examples requires process and tooling. You cannot manually track and manage large-scale data collection without systems to support it.

The data collection process typically includes these stages: planning and scenario definition, data extraction or generation, expert review or annotation, quality auditing, formatting and normalization, and versioning and storage. Each stage has tooling requirements.

For log mining, you need data extraction tools that query production databases, filter by criteria, sample appropriately, and export data in a usable format. You need PII scrubbing tools that detect and redact sensitive information. You need deduplication tools that identify and remove duplicate or near-duplicate examples.

For authoring, you need authoring interfaces that guide experts through the task, enforce formatting, and track progress. Some teams build custom tools; others use annotation platforms like Labelbox, Scale AI, or internal systems. The tool should present the expert with a prompt or scenario, provide input fields for the response, enforce required fields, and allow the expert to save and review their work.

For annotation, you need a pipeline that generates drafts, routes drafts to experts, collects edits, and tracks agreement. The pipeline should flag low-quality drafts, allow experts to reject unusable drafts, and provide feedback to improve draft generation. Some teams use model APIs to generate drafts on-demand; others pre-generate a batch of drafts and queue them for annotation.

For quality auditing, you need sampling tools, review interfaces, and metrics dashboards. You should audit a random sample of collected examples at regular intervals, check for errors, inconsistencies, and coverage gaps, and feed findings back to the collection team.

For formatting and normalization, you need scripts or pipelines that convert all examples to a consistent schema, validate required fields, check for formatting errors, and flag examples that do not meet quality thresholds.

For versioning and storage, you need a data management system that tracks dataset versions, logs who collected each example and when, supports rollback to previous versions, and integrates with your training pipeline.

A global e-commerce company built a data collection platform for fine-tuning models across multiple languages and regions. The platform supported log mining from regional customer service systems, authoring by regional experts, and annotation of model-generated drafts. All examples flowed through a normalization pipeline that standardized formatting, scrubbed PII, and validated schema compliance. Experts used a web-based annotation interface that displayed prompts, showed model drafts, and allowed inline editing. The platform tracked metrics: examples collected per source, examples per expert, inter-annotator agreement, and quality audit pass rates. The platform enabled the company to collect and manage over 100,000 training examples across twelve languages.

## The Role of Synthetic Data

Synthetic data — data generated by models rather than collected from humans — is emerging as a fourth collection strategy. You use a strong base model or a separate model to generate training examples, then optionally filter or review them.

Synthetic data is appealing for scale: models can generate thousands of examples quickly and cheaply. It is also useful for augmenting real data, covering edge cases, and creating adversarial examples.

The disadvantage of synthetic data is quality risk. Model-generated data reflects the model's biases, limitations, and knowledge gaps. If you train a model on synthetic data generated by a similar model, you may not improve beyond the base model's capabilities — you may just reinforce the base model's patterns. Synthetic data also risks introducing errors, hallucinations, or unrealistic patterns that degrade fine-tuned model performance.

Synthetic data works best when used to augment human-collected data, not replace it. Use synthetic data to increase coverage, generate variations, or create adversarial test cases, but anchor the dataset with human-authored or human-reviewed examples. Some teams use a mix: 70% human-collected data, 30% synthetic data. Others use synthetic data only for specific purposes, like generating edge cases that are rare in production logs.

A content moderation team used synthetic data to augment their training set for policy violation detection. They had 8,000 human-labeled examples of policy violations and non-violations. They used GPT-4.5 to generate an additional 3,000 synthetic examples covering edge cases and rare violation types that were underrepresented in real data. They had moderators review the synthetic examples and discard any that were unrealistic or mislabeled. The filtered synthetic data improved the model's performance on rare violation types without degrading performance on common cases.

## Balancing Speed and Quality in Collection Timelines

Data collection timelines are always under pressure. Product wants the model deployed quickly. Leadership wants to see results. Engineering wants to start training. The pressure to collect data fast can compromise quality. You must balance speed and quality deliberately rather than defaulting to the fastest path.

The fastest collection method is usually log mining because logs already exist. But if log quality is low, fast collection produces a low-ceiling dataset. The slowest collection method is usually authoring because experts take time to write examples. But if task stakes are high, slow collection produces a high-ceiling dataset that meets requirements.

The right balance depends on the deployment timeline and the cost of failure. If you have a hard deadline — a product launch, a regulatory requirement, a customer commitment — and missing the deadline is costly, you may accept lower quality to meet the timeline. You deploy with known limitations, plan for human oversight, and schedule a quality improvement round post-launch. If you have flexibility on timeline and the cost of deploying a low-quality model is high, you invest time in quality upfront.

Some teams use phased collection. They start with a fast, moderate-quality collection method to get an initial model into production quickly. They deploy the model to a limited user base or a low-risk use case. They collect production feedback, identify quality gaps, and then invest in a slower, higher-quality collection round to improve the model. This phased approach balances speed and quality by delivering value early while improving quality iteratively.

A software development tool company used phased collection for a code review model. They mined 10,000 code review comments from their internal GitHub repositories in two weeks. The comments were written by engineers with varying skill levels, so quality was mixed. They fine-tuned a model on this data and deployed it internally to a pilot team of 20 engineers. The model provided useful feedback on common issues but missed subtle bugs. The team collected feedback from the pilot users, identified the most important gaps, and then had senior engineers author 1,000 high-quality examples covering those gaps. They retrained the model and deployed it company-wide. The phased approach allowed them to launch in six weeks instead of waiting three months for full authoring, while still achieving high quality by the second iteration.

## Managing Annotator Fatigue and Quality Drift

When annotation or authoring extends over weeks or months, annotator performance degrades due to fatigue, boredom, and loss of calibration. Quality drifts over time, with later examples having lower quality than earlier examples. You must monitor and manage annotator fatigue to maintain consistent quality throughout the collection process.

Annotator fatigue manifests as increased error rates, reduced attention to detail, faster but sloppier work, and inconsistency with earlier examples. An annotator who carefully reviewed 30 examples per hour in week one may rush through 50 examples per hour in week six, missing errors they would have caught earlier.

You manage fatigue by limiting daily annotation hours, rotating annotators through different tasks, providing regular breaks and rest periods, monitoring quality metrics over time, and giving feedback and re-calibration when quality drops. Some teams set a maximum of four to six hours of annotation per day per annotator. Others rotate annotators through annotation and other work to maintain engagement.

Quality drift also occurs when annotators develop their own interpretations of guidelines over time, drifting away from the original intent. Two annotators who were aligned in week one may diverge in week four as each develops their own habits and shortcuts. You prevent drift by conducting regular calibration sessions where annotators review and discuss examples together, by having a lead annotator or quality reviewer check samples of each annotator's work weekly, and by updating guidelines when ambiguities or edge cases arise.

A legal document review team managed annotator quality over a four-month collection period. They had five attorneys annotating contract clauses. They set a four-hour daily annotation limit. They conducted weekly calibration sessions where attorneys reviewed examples together and discussed disagreements. They tracked each attorney's inter-annotator agreement with the lead attorney on a weekly sample. When one attorney's agreement dropped from 94% to 87% in week seven, the lead attorney reviewed the attorney's recent work, identified that the attorney had started interpreting a specific clause type inconsistently, provided corrective feedback, and the attorney's agreement recovered to 92% the following week.

## The Hidden Costs of Poor Data Collection Planning

Poor planning at the data collection phase has compounding costs downstream. If you collect the wrong data, you waste the collection effort and must collect again. If you collect low-quality data, you waste training compute and deployment effort on a model that does not perform. If you collect data without proper documentation, you cannot reproduce the dataset or audit it later.

The hidden costs include rework when the initial dataset fails quality audits, delayed timelines when you discover coverage gaps late, wasted compute when you train on bad data, failed deployments when the model underperforms, and compliance failures when you cannot document data lineage or quality processes.

These costs far exceed the upfront investment in careful planning. A week spent planning data collection — defining scenarios, writing detailed guidelines, conducting pilot runs, and setting up quality processes — can save months of rework later.

A healthcare analytics company skipped data collection planning to move fast. They had data scientists extract 20,000 clinical notes from their database and label them for diagnosis codes without clear guidelines. Three weeks into collection, they discovered that different labelers were using different diagnosis code granularity: some used top-level codes, others used specific subcodes. The dataset was inconsistent and unusable. They stopped collection, wrote detailed guidelines specifying which code level to use, and restarted. The rework cost six weeks. If they had spent one week planning and writing guidelines upfront, they would have avoided the rework.

## The Data Collection Retrospective

After completing data collection, conduct a retrospective to identify what worked, what did not, and what to improve for future projects. The retrospective captures lessons about collection strategy effectiveness, annotator performance, tooling gaps, quality issues, and cost and timeline accuracy.

The retrospective should include metrics: total examples collected, cost per example, time per example, quality audit pass rate, inter-annotator agreement, coverage of target scenarios, and rework or rejection rate. It should also include qualitative feedback from annotators, reviewers, and project leads about what was difficult, what was confusing, and what could be improved.

The retrospective output is a lessons-learned document that informs future data collection projects. Over time, you build institutional knowledge about what collection strategies work for which task types, what quality is achievable at what cost, how to write effective guidelines, and how to manage annotator teams.

A financial services company conducted retrospectives after each fine-tuning project. They learned that log mining worked well for customer service tasks but poorly for compliance review tasks. They learned that their annotation platform had a bug that occasionally duplicated examples, requiring additional deduplication steps. They learned that guidelines needed concrete examples of edge cases to achieve consistency. They codified these lessons in a data collection playbook that new projects referenced, improving quality and reducing costs on subsequent projects.

## Cross-Functional Collaboration in Data Collection

Data collection is not just a data science task. It requires collaboration with domain experts, compliance teams, legal, privacy, and sometimes external vendors. Managing cross-functional collaboration is essential for successful data collection.

Domain experts provide the expertise needed for authoring and annotation. They define quality standards, write guidelines, label examples, and review quality. You must engage domain experts early, communicate the scope and timeline clearly, and respect their time constraints. Domain experts are often busy with their primary responsibilities and cannot dedicate unlimited time to data collection. You need to negotiate capacity, plan around availability, and provide efficient tools and workflows.

Compliance and legal teams ensure that data collection complies with regulations, data usage policies, and privacy requirements. They review data sources, approve scrubbing and anonymization processes, and sign off on data usage for training. You must involve compliance and legal at the planning phase, not after collection is complete, to avoid collecting data that cannot be used.

Privacy teams ensure that PII is handled appropriately. They define scrubbing requirements, audit scrubbed data, and verify that data storage and access controls meet privacy standards. You must integrate privacy reviews into the collection workflow.

External vendors or annotation services provide scale for annotation tasks. You must vet vendors for quality, train them on your guidelines, monitor their performance, and audit their outputs. Vendor management adds overhead but can be necessary when internal capacity is insufficient.

A global insurance company collected training data for a claims processing model in collaboration with claims adjusters, legal, compliance, privacy, and an external annotation vendor. Claims adjusters defined the task and wrote 500 gold-standard examples. Legal approved the use of historical claims data with PII scrubbing. Privacy defined scrubbing requirements and audited the scrubbed dataset. Compliance reviewed the dataset for regulatory alignment. The external vendor annotated 15,000 additional examples under supervision by the claims team. The project involved eight stakeholder groups and required careful coordination, but the cross-functional collaboration ensured that the dataset met quality, legal, and regulatory requirements.

## The Role of Pilot Datasets in De-Risking Collection

Before committing to large-scale data collection, run a pilot. A pilot dataset is a small subset — 500 to 2,000 examples — collected using the planned strategy. The pilot tests whether the strategy works, whether the guidelines are clear, whether annotators can achieve acceptable quality, and whether the estimated ceiling is realistic.

The pilot reveals problems early when they are cheap to fix. If the pilot shows that log quality is too low, you switch strategies before extracting 50,000 logs. If the pilot shows that annotators misunderstand the guidelines, you revise the guidelines before scaling to 20,000 annotations. If the pilot shows that the task is harder than expected and quality is poor, you reconsider whether fine-tuning is viable.

The pilot also provides a training opportunity for annotators. They complete the pilot, receive feedback, calibrate on standards, and then proceed to full-scale collection with aligned understanding. The pilot dataset can be included in the final training set or used solely for calibration.

A tax software company planned to collect 15,000 examples of tax question-answer pairs via annotation. They ran a pilot with three CPAs annotating 600 examples. The pilot revealed that the CPAs disagreed on how to handle multi-part tax questions: some wrote comprehensive answers covering all parts, others wrote separate answers for each part. This inconsistency would have produced a mixed-format dataset. The team updated the guidelines to specify that multi-part questions should receive a single comprehensive answer. They had the CPAs re-annotate the pilot examples and verified alignment. Then they scaled to full collection with consistent formatting.

## Data Collection for Continuous Fine-Tuning

Some teams treat fine-tuning as a one-time project: collect data, train a model, deploy, and finish. Others treat fine-tuning as a continuous process: collect data, train, deploy, collect more data from production, retrain, and repeat. Continuous fine-tuning requires ongoing data collection infrastructure and processes.

For continuous fine-tuning, you set up pipelines that collect data from production continuously. User interactions, feedback signals, and edge cases discovered in production flow into a staging dataset. Periodically — weekly, monthly, or quarterly — you review the staging dataset, curate it, and add it to the training set. You retrain the model on the expanded dataset and deploy the updated version.

Continuous data collection requires automation for data extraction, quality filtering, and formatting. It requires human review processes for curating staged data. It requires version control for tracking which examples were added when. It requires monitoring to ensure that production data quality remains high.

Continuous fine-tuning is valuable for tasks where the production distribution shifts frequently, where user needs evolve, and where you want the model to improve based on real-world usage. It is not necessary for stable tasks where the distribution and requirements are fixed.

A customer support chatbot team used continuous fine-tuning. They deployed an initial model trained on 10,000 authored examples. They logged all user conversations and flagged cases where the model failed or where users provided negative feedback. Every month, support leads reviewed the flagged cases, selected the most valuable ones, and had agents write gold-standard responses. These new examples were added to the training set, and the model was retrained. Over six months, the training set grew from 10,000 to 18,000 examples, and model performance improved from 84% to 91% as it learned to handle new edge cases discovered in production.

The next chapter covers data formatting: how to structure instruction-response pairs, how to format chat templates, and how to handle completion formats for different model architectures and fine-tuning frameworks.
