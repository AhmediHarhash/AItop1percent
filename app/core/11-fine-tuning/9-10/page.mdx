# 9.10 — Model Lineage and Audit Trails: Who Trained What, When, on Which Data

In June 2025, a healthcare technology company deploying a fine-tuned model for medical record summarization faced a regulatory audit triggered by a patient complaint. The auditor asked a simple question: which version of the model generated this specific summary, and what data was it trained on? The engineering team had no answer. They had fine-tuned multiple versions over six months, but kept no systematic record of which checkpoint was deployed when, which training dataset corresponded to which model, or who approved each training run. The team could not prove the model had never been trained on data from jurisdictions where they lacked consent. The auditor classified this as a material compliance failure. The company was fined 1.2 million euros under GDPR Article 5 for failing to demonstrate lawful processing. The entire fine-tuning program was suspended for four months while the team rebuilt their lineage tracking system from scratch.

The failure was not a lack of technical capability. It was the absence of **model lineage tracking**—a complete, immutable audit trail linking every deployed model to its training provenance, configuration, and approval chain. In 2026, with the EU AI Act enforcement creating explicit obligations for GPAI model modifiers, lineage tracking is not optional. It is a regulatory requirement and a professional standard. You must be able to answer, at any moment, for any model in production: what base model was used, what data was it trained on, who trained it, when, with what hyperparameters, and who approved deployment. If you cannot answer these questions with evidence, you are operating outside regulatory compliance and exposing your organization to existential risk.

## The Core Lineage Entities

Model lineage tracking requires capturing five core entities for every fine-tuned model. First, the **base model**—the exact pretrained model you started from, including its version identifier, provider, download timestamp, and cryptographic hash. You must record not just "GPT-4" but "gpt-4-0613 downloaded from OpenAI on 2025-11-03 at 14:22 UTC with SHA-256 hash abc123." This level of specificity matters because base models change over time, even within the same version string, and you must be able to prove exactly which weights you started with.

Second, the **training dataset**—the complete specification of what data was used, including dataset version, row count, date range, source systems, and any filtering or sampling applied. You must record both the logical dataset definition and the physical snapshot. The logical definition might be "customer support tickets from January 2025 to June 2025, filtered for English language, PII redacted," while the physical snapshot is the exact file or database state used, captured with a cryptographic hash and immutable storage reference. If you later need to prove you did not train on data from a specific jurisdiction or time period, you need both the definition and the snapshot.

Third, the **training code and hyperparameters**—the exact code version, framework version, and hyperparameter configuration used for training. This includes learning rate, batch size, number of epochs, warmup steps, optimizer settings, random seed, and any custom training logic. You must record not just "we used LoRA" but "LoRA with rank 8, alpha 16, dropout 0.1, trained for 3 epochs with learning rate 3e-4, batch size 16, using transformers library version 4.35.2 and PyTorch 2.1.0, with random seed 42." This specificity is required for reproducibility and for demonstrating that training was conducted according to your approved methodology.

Fourth, the **evaluation results**—the metrics, test set, and pass/fail determination that justified deployment. You must record which evaluation suite was run, what version, on which test data, with what results, and who reviewed and approved those results. If a model later exhibits unexpected behavior, you need to be able to demonstrate that it passed your quality gates at deployment time and show exactly what those gates tested.

Fifth, the **human approvals**—who requested the training run, who executed it, who reviewed the evaluation results, and who authorized deployment. This includes timestamps, roles, and the artifact versions they approved. If a compliance question arises, you need to demonstrate that the model went through your documented approval process and that responsible parties signed off on each stage.

## Immutable Lineage Records

Lineage records must be **immutable**. You cannot store lineage in a system where records can be edited or deleted after creation. If an auditor suspects your lineage records have been altered to match a desired narrative, your entire compliance posture collapses. You need append-only storage with cryptographic verification. In practice, this means one of three architectures: a blockchain or distributed ledger, an append-only database with cryptographic chaining, or a cloud-native audit log service with built-in immutability guarantees.

The simplest production approach for most teams is a cloud audit log service. AWS CloudTrail, Google Cloud Audit Logs, and Azure Monitor all provide immutable, timestamped event logging with cryptographic integrity checks. You emit a lineage event at each stage of the fine-tuning lifecycle, and the cloud service guarantees that event cannot be altered or deleted. The event includes a JSON payload with all lineage fields, and the service computes a cryptographic hash over the event sequence to detect tampering.

For teams operating in highly regulated environments or with specific sovereignty requirements, an append-only database with cryptographic chaining offers more control. PostgreSQL with the temporal tables extension, or a purpose-built system like Datomic, allows you to create a table where every insert gets a cryptographic hash linking to the previous record, forming an auditable chain. If any record is altered, the chain breaks, and the tampering is detectable. This approach requires more engineering effort but provides full control over data residency and retention.

A small number of teams, particularly in financial services or defense, use private blockchain or distributed ledger systems for lineage tracking. This approach offers the strongest immutability guarantees and allows multiple parties to verify lineage independently, but introduces significant operational complexity and cost. For most organizations, a cloud audit log service provides sufficient immutability at far lower cost.

## Lineage Capture Workflow

You capture lineage by instrumenting your fine-tuning pipeline to emit events at each critical stage. When a training run is initiated, you emit a **training start event** containing the base model identifier, training dataset snapshot reference, hyperparameter configuration, code version, requester identity, and a unique training run ID. This event is written to your immutable lineage store before training begins.

During training, you emit **checkpoint events** whenever a model checkpoint is saved. Each checkpoint event links the checkpoint artifact to the training run ID and includes the training step number, loss metrics, and storage location. If training is interrupted or restarted, these events provide a complete record of what checkpoints were produced and when.

When training completes, you emit a **training end event** with final metrics, total training time, and resource consumption. This event closes the training run and marks it ready for evaluation.

After evaluation, you emit an **evaluation event** linking the model checkpoint to the evaluation suite version, test dataset, metric results, and pass/fail determination. This event includes the identity of the person or system that ran the evaluation and any notes or observations recorded during review.

If the model is approved for deployment, you emit a **deployment approval event** with the approver identity, timestamp, and the specific checkpoint being promoted. If the model is deployed to production, you emit a **deployment event** linking the checkpoint to the production environment, deployment timestamp, and serving configuration.

If the model is later retired, you emit a **retirement event** with the timestamp, reason, and identity of the person authorizing retirement. This creates a complete lifecycle record from training initiation to retirement, with every state transition captured in an immutable log.

## Linking Artifacts to Lineage

Lineage events reference artifacts—model checkpoints, datasets, code versions—that must themselves be stored immutably and linked cryptographically to the lineage record. You cannot record "we used dataset v3" if dataset v3 can be silently changed after the fact. You need **content-addressable storage** where the artifact identifier is derived from the artifact content, making tampering detectable.

For model checkpoints, this means computing a cryptographic hash of the model weights file and using that hash as the artifact identifier. When you emit a checkpoint event, you include the SHA-256 hash of the weights file. The weights file itself is stored in immutable object storage with versioning enabled. If someone later tries to replace the weights file, the hash will not match the lineage record, and the tampering is evident.

For datasets, you capture a snapshot at training time and compute a hash over the snapshot. The snapshot is stored in immutable storage, and the lineage event references the snapshot by hash. This ensures you can always retrieve the exact data that was used for training, even if the source data has changed or been deleted.

For code, you record the Git commit hash of the training code repository. The commit hash is a cryptographic identifier of the code state, and the Git history provides an immutable record of what code was executed. If you later need to reproduce a training run, you can check out the exact commit that was used.

For hyperparameters, you serialize the configuration to a canonical format—typically JSON or YAML—compute a hash, and store the configuration file in immutable storage alongside the model checkpoint. This ensures the hyperparameters cannot be altered after training.

## Lineage Query and Audit

The lineage system must support two critical query patterns. First, **forward tracing**—given a training dataset or base model, find all fine-tuned models derived from it. If you discover a data quality issue in a training dataset, you need to instantly identify every model trained on that data so you can evaluate impact and potentially retrain or retire affected models. This query pattern requires indexing lineage events by training dataset identifier and base model identifier.

Second, **backward tracing**—given a deployed model or a specific model output, trace back to the training data, base model, hyperparameters, and approvals that produced it. If a model generates an unexpected output, you need to determine what data it was trained on, whether that data was approved for use, and whether the training process followed your documented methodology. This query pattern requires indexing lineage events by model checkpoint identifier and deployment event.

Your lineage system must support these queries in seconds, not hours. In a regulatory audit or incident response scenario, you cannot afford to manually reconstruct lineage by searching through logs and artifacts. You need indexed, queryable lineage data with a clear query interface. Most teams implement this as a lineage service with a REST API: query by model ID, get back a complete lineage record including all upstream dependencies and downstream deployments.

## Regulatory Lineage Requirements

The EU AI Act, enforced as of August 2024 and with GPAI obligations effective from August 2025, creates explicit lineage requirements for downstream modifiers. If you fine-tune a GPAI model, you are a downstream modifier, and you must maintain technical documentation describing the modifications made, the data used, the evaluations conducted, and the risk mitigations applied. This documentation must be sufficient for auditors and downstream users to understand the model's capabilities, limitations, and risks.

Article 53 of the EU AI Act requires GPAI model providers and modifiers to maintain documentation that includes, at minimum, training data sources, data governance measures, evaluation results, known limitations, and risk mitigations. For fine-tuned models, this means your lineage system must capture not just technical metadata but also the compliance metadata—data consent basis, geographic restrictions, risk classification, and mitigation measures.

You must be able to produce, on demand, a **lineage report** for any deployed model that includes the base model provenance, training data description and legal basis for use, training methodology, evaluation results and test coverage, deployment approvals, and any incidents or issues observed post-deployment. This report must be comprehensible to a non-technical auditor and must reference immutable evidence stored in your lineage system.

In practice, this means your lineage events must include fields for compliance metadata. When you emit a training start event, you include not just the dataset identifier but also the legal basis for using that data, the geographic scope of data collection, and any consent or contractual limitations. When you emit an evaluation event, you include not just metric results but also whether the evaluation covered all required risk categories for your use case. When you emit a deployment approval event, you include the risk classification of the deployment and any special conditions or monitoring requirements.

## Lineage Retention and Deletion

Lineage records must be retained for the lifetime of the model plus the regulatory retention period. Under GDPR, the retention period for processing records is typically five years after processing ends. Under sector-specific regulations—HIPAA, SOX, financial services rules—retention periods can be seven to ten years. You must configure your lineage system to retain records for the maximum applicable retention period and to defensibly delete them after that period expires.

Defensible deletion means you can prove records were deleted according to a documented policy, not selectively deleted to hide evidence. This requires automated retention policies configured in your immutable storage system. AWS S3, Google Cloud Storage, and Azure Blob Storage all support object lifecycle policies that automatically delete objects after a specified retention period. You configure the retention period once, and the system enforces it uniformly across all lineage records.

For models trained on personal data under GDPR, you must also support the right to erasure. If a data subject requests deletion of their data, and that data was used in a training dataset, you must evaluate whether the model must be retrained or retired. Your lineage system must support querying by data subject identifier to find all models trained on data associated with that subject. This requires including data subject identifiers or pseudonymized keys in your lineage events, which introduces privacy risks that must be carefully managed.

The standard approach is to pseudonymize data subject identifiers in lineage events using a keyed hash. The hash allows you to query for models associated with a subject when needed, but does not reveal the subject identity to anyone with access to the lineage system. The hash key is stored separately under strict access controls and is only used when processing a valid erasure request.

## Lineage Integration with Model Registry

Your model lineage system must integrate with your model registry so that every model in the registry has a complete lineage record accessible from the registry UI. When a user views a model in the registry, they should see the base model, training dataset, training date, evaluation results, and deployment status, all pulled from the lineage system. When they click through to the lineage details, they should see the complete event history and all referenced artifacts.

This integration requires the model registry to call the lineage service API to retrieve lineage data for display. The registry should also enforce that no model can be promoted to production unless it has a complete lineage record. If lineage events are missing—no training start event, no evaluation event, no approval event—the registry should block deployment and alert the team.

Some organizations implement this as a mandatory gate in their CI/CD pipeline. Before a model can be deployed, the pipeline calls the lineage service to verify that all required events are present and that the model passed all required quality gates. If verification fails, deployment is blocked, and the team must complete the missing lineage documentation before retrying.

## Lineage for Third-Party Models

If you fine-tune a third-party base model—GPT-4, Claude, Llama—you must record the base model provenance even though you do not control the base model's lineage. This means capturing the model provider, model version, access timestamp, API endpoint or download location, and any model card or technical documentation provided by the vendor. You must also capture the terms of service and usage restrictions, as these may impose limitations on what you can do with the fine-tuned model.

For API-based fine-tuning—OpenAI's fine-tuning API, Anthropic's future fine-tuning offerings—you receive a fine-tuned model identifier from the provider, but you do not receive the model weights. Your lineage record must include the provider-assigned model ID, the training job ID, the timestamp when training completed, and any metadata the provider returns about the training job. You must also record the API version and endpoint used, as provider APIs evolve over time and behavior can change.

For open-source base models—Llama, Mistral, Falcon—you download the weights from a public repository. Your lineage record must include the repository URL, commit hash or release tag, download timestamp, and the cryptographic hash of the downloaded weights. This allows you to prove exactly which version of the base model you used, even if the repository is later altered or deleted.

## Lineage as Contract Evidence

Model lineage serves not just regulatory compliance but also contract enforcement. If you have a contract with a customer promising that models will only be trained on data meeting specific criteria—geographic boundaries, consent requirements, quality standards—your lineage system is the evidence that you fulfilled that promise. When a customer audits your compliance, they will request lineage records demonstrating that the models serving their use case were trained on approved data and passed required quality gates.

This means your lineage events must capture enough detail to satisfy contractual audit requirements. If your contract promises training data will be less than 90 days old, your training start event must include the date range of the training data. If your contract promises models will achieve minimum accuracy thresholds, your evaluation event must include the accuracy metric and the test set used. If your contract promises human review of all deployments, your deployment approval event must include the approver identity and timestamp.

When a customer requests an audit report, you generate it by querying your lineage system for all models associated with that customer, extracting the relevant lineage fields, and formatting them into a human-readable report. This process should be automated, not manual, so you can produce audit reports on demand without engineering effort.

In multi-tenant environments where a single base model serves multiple customers via different adapters, lineage tracking becomes more complex. You must track lineage per adapter, capturing which customer's data was used to train each adapter, which adapters are deployed for which customer, and ensuring no customer can access another customer's adapter or training data. This requires a multi-tenant lineage architecture where lineage events are tagged with customer identifiers and access controls enforce tenant isolation.

## Operational Lineage Monitoring

Your lineage system itself requires monitoring and operational discipline. You must monitor that lineage events are being emitted correctly, that no training runs or deployments occur without corresponding lineage events, and that lineage queries are completing successfully. If lineage event emission fails silently, you create gaps in the audit trail that undermine the entire system.

Implement monitoring that alerts when a model checkpoint is created without a corresponding checkpoint event, when a deployment occurs without an approval event, or when lineage event emission fails due to service outages. These alerts should page the on-call engineer, as lineage gaps are compliance failures that require immediate remediation.

You must also monitor lineage query performance. If a regulatory auditor requests a lineage report and the query takes 20 minutes to complete, you appear unprepared and disorganized. Lineage queries should complete in seconds, which requires proper indexing and database performance tuning. Monitor query latency and set alerts for degradation.

The lineage system must be included in your disaster recovery plan. If your primary region fails, you must be able to query lineage data from a secondary region or backup system. This requires cross-region replication of lineage events and regular disaster recovery drills that include lineage system failover.

Model lineage is not a technical convenience. It is the foundation of regulatory compliance, contract enforcement, and operational accountability for fine-tuned models. Every training run, every deployment, every approval must be captured in an immutable, queryable audit trail that can withstand regulatory scrutiny and provide evidence of responsible practices. Teams that treat lineage as an afterthought or a nice-to-have discover, under audit, that they have no defensible position. Teams that build lineage tracking into their fine-tuning workflow from day one operate with confidence that they can answer any question about any model at any time.

In the next subchapter, we examine the specific governance and compliance obligations imposed by the EU AI Act on teams that fine-tune general-purpose AI models, and what you must do to meet those obligations.
