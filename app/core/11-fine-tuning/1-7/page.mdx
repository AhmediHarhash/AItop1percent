# 1.7 â€” Fine-Tuning vs RAG vs Agents: The Combined Decision Matrix

The 180,000 dollar retraining bill arrived six weeks after deployment. A legal technology company had spent four months fine-tuning GPT-4 to answer contract questions for their enterprise customers, collecting 15,000 examples of contract clauses paired with legal interpretations and deploying the custom model with confidence. Within two weeks, customers were complaining. The model confidently cited outdated regulations, referenced contract provisions that had been amended, and missed recent case law that changed the interpretation of standard clauses. The fine-tuned model had learned the style and structure of legal reasoning perfectly, but it was frozen in time, unable to access the constantly evolving legal knowledge base the company maintained. Meanwhile, their smaller competitor was serving the same customers with a standard GPT-4 model connected to a vector database of current legal documents, updated daily. The competitor's responses were equally well-structured and far more accurate. The legal tech company had chosen the wrong tool for the wrong problem.

The mistake was treating fine-tuning, RAG, and agents as competing alternatives when they are complementary capabilities that solve fundamentally different problems. Fine-tuning changes how a model behaves. RAG gives a model access to external knowledge. Agents enable multi-step reasoning and tool use. Each has distinct strengths, distinct costs, and distinct failure modes. The best production systems often combine all three, using each where it excels. Understanding the decision matrix means understanding what each approach actually does, when to use each alone, and when to combine them into hybrid architectures that leverage the strengths of all three.

## What Fine-Tuning Actually Does

Fine-tuning modifies the model's internal weights to change its behavior patterns. You are teaching the model to respond differently to prompts, to adopt a particular style, to follow specific formatting conventions, to prioritize certain types of reasoning, or to specialize in domain-specific tasks. Fine-tuning is excellent for behavioral adaptation. If you need the model to write in a specific corporate voice, structure outputs in a particular format, follow domain-specific conventions, or shift its reasoning style toward your use case, fine-tuning is the right tool.

Fine-tuning does not add new factual knowledge effectively. You can fine-tune on documents, but the model will not reliably recall specific facts from those documents. It will learn patterns and styles from the training data, but it will not function as a knowledge store. If you fine-tune on 10,000 product manuals, the model will learn to write in the style of product manuals, but it will not reliably answer questions about specific products unless those facts were already in the base model's training data. Fine-tuning is compression, not memorization.

Fine-tuning is static. Once you deploy a fine-tuned model, its behavior is frozen until you retrain it. If your domain knowledge changes weekly, fine-tuning alone will always lag behind. If your style guide evolves quarterly, you need a retraining cadence. If your business rules change daily, fine-tuning is the wrong primary approach. The legal tech company failed because they treated fine-tuning as a knowledge solution when it is a behavior solution.

## What RAG Actually Does

Retrieval-Augmented Generation gives a model access to external information at inference time. You maintain a knowledge base, retrieve relevant documents for each query, and inject those documents into the prompt as context. The model reads the provided context and generates responses based on what it was given. RAG is excellent for dynamic knowledge. If your domain knowledge updates frequently, if you need to cite sources, if you have proprietary information that was not in the base model's training data, or if you need to ensure the model only uses approved information, RAG is the right tool.

RAG does not change the model's behavior. A model with RAG will still write in its default style, still follow its base instruction-following patterns, still use its native reasoning approach. If you need the model to adopt a specific voice, RAG will not achieve that. You can include style examples in the retrieved context, but this is inefficient and unreliable compared to fine-tuning. RAG provides knowledge, not behavioral adaptation.

RAG introduces retrieval quality as a dependency. If your retrieval system returns irrelevant documents, the model's responses will be poor. If your chunking strategy splits critical context across multiple documents, the model will miss connections. If your vector embeddings do not capture semantic similarity well for your domain, retrieval will fail. A healthcare company in late 2025 deployed a medical Q&A system with RAG, but their retrieval system was tuned for general language, not medical terminology. When doctors asked about rare conditions, the retriever returned documents about common conditions with overlapping symptoms, and the model generated dangerously incorrect guidance. RAG is only as good as your retrieval pipeline.

## What Agents Actually Do

Agents enable multi-step reasoning with external tool use. The model can plan a sequence of actions, call external APIs or functions, process the results, and iteratively refine its approach based on what it learns. Agents are excellent for complex workflows. If your task requires gathering information from multiple sources, performing calculations, checking databases, validating assumptions, or executing multi-step procedures, agents are the right tool.

Agents do not change the underlying model's capabilities. An agent built on a weak base model will make poor decisions at each step. An agent built on a model that does not understand your domain will call the wrong tools or misinterpret results. Agents amplify the model's strengths and weaknesses. If the model cannot reliably parse structured data, the agent will fail when processing API responses. If the model cannot follow complex instructions, the agent will deviate from the intended workflow.

Agents introduce latency and cost. Each tool call adds round-trip time. Each reasoning step consumes tokens. A financial services company built an agent in early 2026 to process loan applications. The agent called seven different APIs to verify income, check credit, validate identity, and assess risk. Each call added 200-400 milliseconds. The total processing time was 3.2 seconds, compared to 800 milliseconds for their previous rule-based system. Customers perceived the new system as slower, even though it was more accurate. Agents trade speed for capability.

## The Combined Decision Matrix

The decision matrix has three dimensions: behavior, knowledge, and workflow complexity. For each dimension, you choose the appropriate tool.

If you need behavioral adaptation without dynamic knowledge or complex workflows, fine-tune alone. A customer support system that must respond in a specific brand voice, follow strict formatting rules, and handle common queries fits this profile. You fine-tune on thousands of examples of your brand's support interactions. The model learns the voice, learns the structure, and handles queries in the desired style. You do not need RAG because the queries are common and the knowledge is stable. You do not need agents because the workflow is single-step: question in, answer out.

If you need dynamic knowledge without behavioral adaptation or complex workflows, use RAG alone. A legal research tool that must cite current case law fits this profile. You use a base model with strong reasoning, connect it to a vector database of legal documents updated daily, and retrieve relevant cases for each query. The model reads the cases and generates analysis. You do not need fine-tuning because the base model already writes in clear legal prose. You do not need agents because the workflow is retrieval and generation, no multi-step planning required.

If you need complex workflows without behavioral adaptation or dynamic knowledge, use agents alone. A data analysis assistant that must query databases, perform calculations, and generate reports fits this profile. You build an agent on a strong base model, give it access to SQL tools and calculation libraries, and let it plan the analysis steps. The base model is already good at reasoning and code generation. The knowledge is in the databases, accessed via tools. You do not need fine-tuning because the base model's analytical style is appropriate. You do not need RAG because the knowledge is structured and accessed via APIs, not documents.

If you need behavioral adaptation plus dynamic knowledge, combine fine-tuning and RAG. A medical diagnosis assistant that must write in a specific clinical voice while citing current research fits this profile. You fine-tune on clinical notes to teach the model to write in the appropriate medical style, use proper terminology, and structure differential diagnoses correctly. You add RAG to provide access to current medical literature. The fine-tuned behavior ensures clinical appropriateness. The RAG ensures factual currency. A healthcare technology company deployed exactly this architecture in late 2025. The fine-tuned model writes like a specialist. The RAG provides the latest studies. The combination delivers both style and substance.

If you need behavioral adaptation plus complex workflows, combine fine-tuning and agents. A code review assistant that must follow company-specific standards while checking multiple systems fits this profile. You fine-tune on your company's code reviews to teach the model your conventions, your priorities, and your communication style. You build an agent that calls linters, runs tests, checks dependency versions, and queries documentation. The fine-tuned behavior ensures reviews match your team's standards. The agent capabilities ensure comprehensive checking. A software company with 600 engineers deployed this in early 2026. The model writes reviews in the team's voice. The agent performs the mechanical checks. Reviewers focus on architecture and logic.

If you need dynamic knowledge plus complex workflows, combine RAG and agents. A financial analyst assistant that must research current market data and perform multi-step analysis fits this profile. You build an agent that queries financial databases, retrieves market reports via RAG, performs calculations, and synthesizes findings. The base model is already good at financial reasoning. The RAG provides current data. The agent orchestrates the research and analysis workflow. An investment firm deployed this in mid-2025. Analysts query the system in natural language. The agent retrieves relevant reports, pulls current prices, calculates ratios, and generates comparative analysis. No fine-tuning needed because GPT-4's financial reasoning is already strong.

If you need all three, combine fine-tuning, RAG, and agents. A compliance monitoring system that must follow company-specific policies, access current regulations, and perform multi-step verification fits this profile. You fine-tune on your company's compliance communications to teach the model your risk assessment style and reporting conventions. You add RAG to provide access to current regulations and internal policies. You build an agent that checks transaction logs, queries external databases, validates against rules, and escalates issues. A financial services company with 12,000 employees deployed this architecture in late 2025. The fine-tuned model writes compliance reports in the company's style. The RAG provides current regulations. The agent orchestrates the checking workflow across multiple systems. All three layers working together.

## Real Examples of Wrong Choices

The legal tech company that fine-tuned for knowledge should have used RAG. They spent four months training a model to memorize legal reasoning patterns when they needed current access to legal documents. RAG would have taken two weeks to implement and delivered better results.

A content marketing company in mid-2025 built an agent to write blog posts. The agent planned outlines, researched topics, drafted sections, and assembled final posts. The process took 40 seconds per post and cost $0.80 in API calls. The posts were generic and off-brand. They should have fine-tuned. Blog writing is a single-step task with strong behavioral requirements. A fine-tuned model would have produced on-brand posts in 3 seconds for $0.05. They chose agents for a behavior problem.

A customer support company in early 2026 used RAG to provide answers to common questions. They embedded their FAQ documents and retrieved them for each query. The system worked but was slow. Retrieval added 400 milliseconds to every response. They should have fine-tuned. The FAQs were static and the knowledge base was small. A fine-tuned model would have internalized the common answers and responded instantly without retrieval overhead. They chose RAG for static knowledge.

An e-commerce company built a product recommendation system using only a base model. No fine-tuning, no RAG, no agents. The model generated generic recommendations that did not reflect the company's inventory, current promotions, or customer preferences. They should have combined all three. Fine-tune on past successful recommendations to learn the company's recommendation style. Use RAG to access current inventory and promotions. Use an agent to query customer purchase history and preference data. They chose none when they needed all.

## Cost and Latency Tradeoffs

Fine-tuning has high upfront cost and low inference cost. Training costs range from $500 for simple tasks to $50,000 for complex domains. Inference costs are the same as base model inference, typically $0.002 to $0.06 per request depending on model size. Latency is the same as base model latency, typically 800 milliseconds to 2 seconds for complex queries. Fine-tuning is expensive to start but cheap to run.

RAG has moderate upfront cost and moderate inference cost. Building a retrieval pipeline costs $5,000 to $50,000 depending on data volume and complexity. Inference costs include embedding generation, vector search, and model inference with longer context. Typical cost is $0.01 to $0.15 per request. Latency includes retrieval time plus generation time, typically 1.5 to 3 seconds. RAG is moderately expensive to build and moderately expensive to run.

Agents have low upfront cost and high inference cost. Building an agent framework costs $2,000 to $20,000 depending on tool complexity. Inference costs include multiple model calls, tool execution, and iterative refinement. Typical cost is $0.10 to $2.00 per request for complex workflows. Latency includes multiple round-trips, typically 3 to 15 seconds depending on workflow depth. Agents are cheap to build but expensive to run.

Combining approaches multiplies upfront costs but can reduce inference costs through optimization. A fine-tuned model with RAG costs $5,500 to $100,000 upfront but reduces inference costs by using a smaller fine-tuned model instead of a large base model. A fine-tuned agent costs $2,500 to $70,000 upfront but reduces inference costs by making better decisions that require fewer tool calls. A RAG-powered agent costs $7,000 to $70,000 upfront but reduces inference costs by retrieving targeted information instead of searching blindly.

The total cost of ownership calculation must include retraining frequency for fine-tuned models, data refresh costs for RAG systems, and tool maintenance costs for agents. A model that requires monthly retraining adds $500 to $10,000 per month in ongoing costs. A RAG system that ingests 10,000 new documents monthly adds $200 to $2,000 per month in embedding and indexing costs. An agent that calls expensive external APIs adds $0.05 to $0.50 per request in tool costs.

## The Fine-Tune Plus RAG Pattern

The most common hybrid architecture is fine-tuning for behavior with RAG for knowledge. This pattern appears in customer support, medical Q&A, legal research, technical documentation, and compliance monitoring. The fine-tuned model learns your domain's communication style, reasoning patterns, and output structure. The RAG layer provides current, specific, citable information.

Implementation is straightforward. You train a fine-tuned model on examples of your desired outputs without worrying about factual completeness. The training data teaches style and structure. At inference time, you retrieve relevant documents and inject them into the prompt before the fine-tuned model generates a response. The model reads the retrieved context and produces an answer in the learned style.

The advantage is separation of concerns. You can update knowledge without retraining by adding documents to the retrieval corpus. You can update behavior without changing infrastructure by fine-tuning a new version. You can optimize retrieval independently from generation. You can audit factual accuracy by examining retrieved sources separately from evaluating output quality.

The challenge is prompt length. Retrieved documents plus instruction formatting can exceed context windows. A financial services company hit this in late 2025. Their fine-tuned model had a 16K token context limit. Their retrieval system returned 10 documents averaging 1,500 tokens each. The total context was 15,000 tokens, leaving only 1,000 tokens for the query and response. They solved this by implementing a reranking layer that selected the top 3 most relevant documents, reducing context usage to 4,500 tokens while maintaining answer quality.

## The Fine-Tuned Model as Agent Backbone

The second common hybrid is using a fine-tuned model as the reasoning engine for an agent. This pattern appears in code review, data analysis, workflow automation, and complex decision-making. The fine-tuned model provides domain-specific reasoning, tool selection, and output formatting. The agent framework provides multi-step orchestration and external tool access.

Implementation requires careful dataset design. Your fine-tuning examples must demonstrate not just final outputs but reasoning chains, tool usage patterns, and error recovery. You are teaching the model to be a better agent, not just a better output generator. A software company fine-tuned Claude Opus 4 on 5,000 examples of senior engineers solving debugging tasks. Each example showed the thought process: hypothesize failure mode, check logs, query metrics, test hypotheses, iterate. The fine-tuned model learned debugging reasoning patterns, not just bug fixes.

The advantage is reliability. Base models can be unreliable at tool selection and workflow planning. Fine-tuning on your specific workflows makes the agent more predictable. A manufacturing company built an agent to diagnose equipment failures. The base model agent called tools in chaotic order and often missed critical checks. After fine-tuning on 2,000 examples of expert diagnosis workflows, the agent followed systematic procedures, caught edge cases, and completed diagnoses in fewer steps.

The challenge is maintaining the dataset as workflows evolve. If you add new tools, you need new training examples showing how to use them. If you change tool interfaces, existing examples become misleading. A financial services company discovered this when they added a new fraud detection API to their agent. The fine-tuned model continued using the old three-API workflow, never calling the new fourth API. They needed to collect 500 new examples demonstrating four-API workflows and retrain before the agent adapted.

## When Not to Combine

Not every system needs multiple approaches. Adding complexity without clear benefit increases maintenance cost, debugging difficulty, and failure surface area. A SaaS company in early 2026 built a feature naming assistant. They fine-tuned for their naming conventions, added RAG to search past feature names, and built an agent to check trademark databases. The system was sophisticated but slow and expensive. After three months, they simplified to fine-tuning alone. The model had learned naming conventions well enough that RAG and agents added no measurable value. Simplification cut latency from 4.2 seconds to 1.1 seconds and cost from $0.35 to $0.04 per request.

The decision framework is value per layer. Each layer must provide distinct, measurable value. If fine-tuning alone achieves your quality target, do not add RAG. If RAG alone achieves your quality target, do not add agents. If adding a second layer improves quality by less than 5%, the added complexity is rarely justified. If adding a second layer increases latency by more than 2x, you need a strong quality or capability argument to justify it.

Measure incrementally. Deploy fine-tuning alone first. Measure quality, latency, and cost. If quality is insufficient, identify whether the gap is behavioral, knowledge-based, or workflow-related. If behavioral, improve fine-tuning. If knowledge-based, add RAG. If workflow-related, add agents. Do not add layers preemptively.

## Architecture Evolution Patterns

Real systems rarely start with their final architecture. They evolve as you learn what actually matters. A common progression is starting with base model plus prompts, adding RAG when you discover knowledge gaps, then fine-tuning when you identify consistent behavioral patterns, and finally adding agents when workflows become complex enough to justify the cost.

A document processing company followed this exact path. They started in mid-2025 with GPT-4 and carefully crafted prompts to extract information from legal contracts. The base model worked but was inconsistent in formatting and sometimes hallucinated details. They added RAG to provide relevant contract examples as few-shot prompts. Quality improved but latency increased to 3.5 seconds. They fine-tuned on 5,000 examples of correctly formatted extractions. Latency dropped to 1.2 seconds and formatting became consistent. Six months later, they needed to validate extracted data against external databases and regulatory requirements. They added agent capabilities for database queries and rule checking. The final architecture had all three layers, each added when a specific need justified the complexity.

Another common pattern is starting with agents, then adding fine-tuning to improve decision-making, then adding RAG to provide dynamic knowledge. A customer service company built an agent in early 2026 that could query order databases, check shipping status, and process refunds. The agent worked but made poor decisions about when to offer refunds versus escalations. They fine-tuned the underlying model on 3,000 examples of senior support agent decisions. The agent's judgment improved dramatically. Later, they added RAG to provide access to product documentation and policy updates. The evolution was agent first for workflow orchestration, fine-tuning second for decision quality, RAG third for knowledge currency.

Some teams discover they need to remove layers, not add them. An insurance company built a claims processing system with fine-tuning for claim assessment, RAG for policy document retrieval, and agents for multi-step verification. After six months in production, they realized the RAG layer was rarely providing useful information because the fine-tuned model had already learned policy rules well enough. They removed RAG, simplifying the system and cutting latency by 40% with no quality loss. Evolution sometimes means subtraction.

## Model Selection Across Approaches

The choice of base model affects all three approaches differently. For fine-tuning, smaller models are often better. A fine-tuned GPT-3.5 often outperforms base GPT-4 for specific tasks while being faster and cheaper. Fine-tuning adds task-specific capability, so you can trade general intelligence for specialized performance. A customer support company found that fine-tuned GPT-3.5 matched base GPT-4 quality for their use case while reducing latency from 1.8 seconds to 0.9 seconds and cost from $0.06 to $0.008 per request.

For RAG, larger models with strong reasoning are usually better. The model must synthesize information from multiple retrieved documents, resolve contradictions, and extract relevant details. GPT-4, Claude Opus 4, and Gemini Pro excel at this. Smaller models struggle with complex synthesis. A research company tried building a scientific literature review system with RAG on GPT-3.5. The model could read individual papers but failed to connect findings across papers or identify contradictions. They switched to Claude Opus 4. Quality improved immediately.

For agents, you need models with strong tool use and planning capabilities. GPT-4, Claude Opus 4, and Gemini Pro are designed for function calling and multi-step reasoning. Smaller models make poor agents. They call tools in illogical orders, misinterpret results, and fail to recover from errors. A data analysis company tried building an agent on Llama 4 8B. The agent called the wrong APIs, passed incorrect parameters, and got stuck in loops. They switched to GPT-4. The agent worked reliably.

When combining approaches, model selection becomes more nuanced. A fine-tuned smaller model with RAG often outperforms a base larger model with RAG. The fine-tuning provides task-specific reasoning, and the RAG provides knowledge, together matching or exceeding what the large model does alone. A legal research company compared fine-tuned GPT-3.5 with RAG against base GPT-4 with RAG. Both retrieved the same legal documents. The fine-tuned system produced better-structured legal analysis because the fine-tuning taught legal reasoning patterns. They deployed the fine-tuned architecture at one-third the cost.

A fine-tuned larger model as an agent backbone provides the best decision-making but at high cost. If your agent workflow demands extremely high-quality decisions and you can afford the cost, fine-tuning GPT-4 or Claude Opus 4 delivers top-tier performance. A financial trading firm fine-tuned GPT-4 on expert trading decisions and used it as the reasoning engine for a trading agent. The cost was $0.40 per decision, but each decision involved millions of dollars. The improved judgment justified the expense.

## Versioning and Migration Strategies

Managing versions across multiple layers creates operational complexity. When you fine-tune a new version, do you also update your RAG corpus and agent tools simultaneously, or do you migrate incrementally? When OpenAI releases GPT-5, do you retrain your fine-tuned models immediately, or do you wait until you accumulate new training data?

A healthcare company learned this the hard way. They had fine-tuned GPT-4 for clinical note generation with RAG for medical literature access. When GPT-4 Turbo was released with better performance, they switched their RAG layer to the new base model but kept the old fine-tuned model for generation. The system broke. The RAG layer retrieved documents formatted for the new model's context window, but the fine-tuned model expected the old format. They had to rapidly fine-tune on the new base model to restore functionality.

The versioning strategy that works is synchronized releases with fallback. When you update any layer, test all layers together in staging before promoting to production. Keep the previous version running in parallel for two weeks. Monitor quality metrics. If metrics degrade, roll back. An e-commerce company follows this discipline. When they fine-tune a new version, they deploy it to 10% of traffic, monitor for three days, expand to 50%, monitor for three more days, then promote to 100%. Rollback is always available. They have rolled back four times in two years when new versions underperformed.

Migration timing depends on data accumulation. Fine-tuning benefits from larger, fresher datasets. If you collect user feedback continuously, you might retrain quarterly when you have 5,000 new examples. If your domain knowledge evolves rapidly, you might update RAG weekly. If your agent tools change monthly, you might fine-tune agent reasoning monthly. A financial services company retrains their fine-tuned model quarterly, updates their RAG corpus weekly, and updates agent tools as needed. Each layer has its own cadence based on how frequently that layer's inputs change.

## Testing Combined Systems

Testing a system with multiple layers requires layer-specific tests and integration tests. You must validate each layer independently, then validate their interaction. A layer can work perfectly in isolation but fail when combined with others.

For fine-tuned models, test behavioral consistency. Does the model follow formatting rules? Does it maintain the desired voice? Does it handle edge cases appropriately? A customer support company tests their fine-tuned model monthly by running 500 standardized queries and checking that responses match brand guidelines, formatting requirements, and tone expectations. They catch behavioral drift before it reaches production.

For RAG, test retrieval quality and generation quality separately. Does the retriever return relevant documents? Does the model use the retrieved documents correctly? A medical research company tests retrieval by sampling 100 queries and having doctors rate whether the top 5 retrieved papers are relevant. Separately, they test generation by providing known-good retrieved papers and evaluating whether the model synthesizes them correctly. This isolates retrieval failures from generation failures.

For agents, test tool selection, parameter construction, and result interpretation. Does the agent call the right tools? Does it pass correct parameters? Does it interpret results accurately and decide next steps appropriately? A data analysis company tests their agent by providing scenarios with known solution paths. They verify the agent calls tools in the expected order, passes correct parameters, and reaches the correct conclusion. When the agent fails, they can pinpoint whether the failure was in planning, execution, or interpretation.

Integration tests verify layer interactions. A compliance monitoring company with fine-tuning, RAG, and agents runs monthly integration tests with 200 realistic compliance scenarios. They verify that the agent retrieves the right regulations via RAG, that the fine-tuned model analyzes them correctly, that the agent performs the right checks, and that the final report meets quality standards. Integration tests catch problems like context length issues where retrieved documents overflow the fine-tuned model's context window, or agent workflow changes that conflict with fine-tuned behavior patterns.

## The Decision Tree in Practice

When a new use case arises, walk through the decision tree systematically. First question: is this primarily a behavior problem, a knowledge problem, a workflow problem, or some combination? If you need consistent output formatting and domain-specific communication style, it is behavior. If you need access to current or proprietary information, it is knowledge. If you need multi-step orchestration with external systems, it is workflow.

Second question: for each problem type, what is the minimum viable approach? Can you solve the behavior problem with prompt engineering before investing in fine-tuning? Can you solve the knowledge problem with prompt engineering and document inclusion before building a RAG pipeline? Can you solve the workflow problem with structured prompts before building an agent framework? Always try the simplest solution first.

Third question: if the minimum approach does not meet quality or cost requirements, which layer adds the most value? A content company needed blog posts in a specific style with current market data. They tried prompt engineering with data included in prompts. Quality was good but cost was high because they were including large datasets in every prompt. They added RAG first to reduce cost. Quality stayed good, cost dropped 60%. Then they added fine-tuning to improve stylistic consistency. Quality increased 15%, cost dropped another 20% because the fine-tuned model needed shorter prompts. They stopped there because agents would add no value for single-step content generation.

Fourth question: measure incrementally. After adding each layer, measure quality, latency, and cost against your targets. If targets are met, stop. If gaps remain, identify whether the gap is behavioral, knowledge-based, or workflow-related, and add the appropriate layer. A financial analysis company needed multi-step market research with custom analytical frameworks. They started with an agent on base GPT-4. Quality was 70% against their 90% target. Analysis showed the agent made good workflow decisions but poor analytical judgments. They fine-tuned for analytical reasoning. Quality reached 85%. Remaining gaps were knowledge-based: missing current market data. They added RAG. Quality reached 92%, exceeding the target. They stopped.

The combined decision matrix is not about using everything. It is about using the right tool for each specific problem, combining only when each layer solves a distinct need, and measuring whether the added complexity delivers value. Fine-tuning changes behavior. RAG provides knowledge. Agents enable workflows. Use each where it excels, combine when you need multiple capabilities, and resist the urge to over-engineer systems that could be simpler. The next critical question is whether your organization has the team, data, and infrastructure to execute any of these approaches successfully.

