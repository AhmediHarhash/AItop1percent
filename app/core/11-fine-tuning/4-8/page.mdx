# 4.8 â€” Adapter Composition: Stacking and Merging Multiple LoRAs

In late 2025, a customer support platform company built six separate LoRA adapters for their support agent model. One adapter handled technical troubleshooting, another handled billing inquiries, a third managed account access issues, a fourth covered product feature questions, a fifth handled cancellation flows, and a sixth managed escalation language. Each adapter was trained independently on 2,000 to 4,000 task-specific examples and performed well in isolation. The team's plan was to serve all six adapters simultaneously by merging them into a single adapter at inference time, allowing the model to handle any support category with one unified deployment. They used naive linear averaging to merge the adapter weights, deployed the merged adapter, and immediately saw catastrophic quality degradation. Technical troubleshooting responses used billing terminology. Escalation language appeared in routine feature questions. The merged adapter scored 34% task success compared to 91% for individual adapters. The team spent three weeks debugging the merge process before realizing the fundamental issue: adapter composition is not a simple averaging problem.

The naive assumption was that LoRA adapters are modular and composable like independent plugins. In reality, adapters are low-rank weight updates that interact with each other and with the base model in complex ways. Merging adapters requires understanding task interference, parameter conflict resolution, and the trade-offs between merge strategies. Naive averaging treats all adapter parameters as equally important, which is almost never true. Some parameters contribute critical task-specific behavior, while others add only minor refinements. Averaging destroys the critical parameters and blends the minor ones, producing a merged adapter that performs poorly on all tasks.

## What Adapter Composition Enables

Adapter composition allows you to combine multiple LoRA adapters into a single model or serving configuration. You can merge adapters into one unified adapter and serve it as a single model. You can stack adapters by applying multiple adapters to the base model sequentially. You can switch between adapters at inference time, routing each request to the appropriate task-specific adapter. All three approaches avoid training a single monolithic fine-tuned model on mixed task data, which can cause task interference and require massive datasets.

The primary use case is multi-task models. Instead of training one model on a mixture of troubleshooting, billing, and escalation examples, you train three separate adapters and compose them. Each adapter specializes in one task, which improves per-task quality and reduces the training data requirement. You do not need balanced multi-task datasets or complex task sampling strategies. You train each adapter independently and combine them later.

Another use case is incremental capability addition. You deploy a model with one adapter for core functionality, then train and merge a second adapter to add a new capability without retraining the base model or the first adapter. A healthcare AI company launched a clinical note summarization model with one LoRA adapter in early 2025. Six months later, they added a diagnosis code suggestion feature by training a second adapter and merging it with the first. The merged model handled both tasks without degrading summarization quality.

A third use case is personalization and multi-tenancy. You train one adapter per customer or user group, each tailored to their specific terminology, preferences, or domain knowledge. At inference time, you load the appropriate adapter for each request. A legal tech SaaS company serves 40 different law firm customers, each with specialized practice areas and writing styles. They trained 40 LoRA adapters and switch between them at request time. Each firm gets a personalized model without maintaining 40 separate fine-tuned base models.

## Adapter Merging Strategies

Linear averaging is the simplest merge strategy. You average the adapter weights element-wise across all adapters. If you have three adapters A, B, and C, the merged adapter M has weights M equals one-third times A plus one-third times B plus one-third times C. This treats all adapters as equally important and assumes no parameter conflicts. Linear averaging works only when tasks are highly similar and adapters do not conflict. In practice, this is rare.

The customer support company's six adapters had significant parameter conflicts. The technical troubleshooting adapter learned to increase attention to technical term tokens, while the billing adapter learned to increase attention to dollar amounts and account references. Averaging these updates produced an adapter that paid equal attention to both, which was correct for neither task. The merged adapter could not prioritize appropriately, so it produced confused outputs.

Weighted averaging allows you to assign different importance to each adapter. If task A is more important than tasks B and C, you might merge with M equals 0.6 times A plus 0.25 times B plus 0.15 times C. This preserves more of task A's behavior. The customer support company tried weighted averaging with 40% weight on technical troubleshooting and 15% each on the other five tasks. Quality improved to 58% task success, still far below individual adapter performance. Weighted averaging reduces conflicts but does not eliminate them.

Task Arithmetic and TIES merging are more sophisticated strategies. TIES stands for "task interference elimination by selective merging." The algorithm identifies parameters where adapters conflict, resolves conflicts by keeping only the strongest updates, and merges non-conflicting parameters by averaging. TIES works in three steps. First, it trims small-magnitude updates from each adapter, keeping only parameters with large weight changes. Second, it identifies conflicts by checking whether adapters update the same parameter in opposite directions. Third, it resolves conflicts by selecting the update with the largest magnitude and discarding smaller conflicting updates.

The customer support company reimplemented their merge using a TIES-inspired approach. They trimmed adapter parameters below a magnitude threshold of 0.01, which removed 60% of parameters from each adapter. They detected conflicts by checking parameter sign disagreement across adapters. They resolved conflicts by keeping the update with the largest absolute value. The TIES-based merge achieved 81% task success, a significant improvement over naive averaging but still below individual adapter performance.

DARE merging stands for "drop and rescale." The algorithm randomly drops a fraction of adapter parameters and rescales the remaining parameters to preserve the adapter's effective magnitude. DARE reduces parameter interference by sparsifying adapters before merging. The customer support company tested DARE with a 50% drop rate, meaning each adapter randomly zeroed out half of its parameters before merging. The DARE merge achieved 76% task success, slightly worse than TIES but faster to compute.

Model soups is another merge strategy that tests multiple weighted combinations of adapters and selects the combination that performs best on a validation set. You generate candidate merges with different weight assignments, evaluate each on validation data, and pick the best. This is computationally expensive but can find merge configurations that outperform simple averaging or TIES. The customer support company tested 30 different weight combinations and selected the merge that scored highest on a 500-example validation set. The optimized merge achieved 84% task success, the best result from merging alone.

## When Merging Works and When It Fails

Merging works well when tasks are complementary and do not share parameters. If one adapter modifies early attention layers to improve domain vocabulary handling and another adapter modifies late layers to adjust output formatting, the adapters do not conflict. Merging preserves both capabilities. A content generation company trained one adapter for tone control and another for factual grounding. The tone adapter updated mid-layer attention weights, while the grounding adapter updated final-layer output projections. Merging these adapters with linear averaging preserved both behaviors, and the merged model scored 94% on combined tone-and-grounding evaluation versus 96% for sequential adapter application.

Merging fails when tasks require contradictory behaviors. If one adapter learns to increase output verbosity and another learns to reduce it, merging produces an adapter with no clear behavior. The customer support company's technical troubleshooting adapter encouraged detailed explanations, while the escalation adapter encouraged brief, empathetic acknowledgments. Merging these adapters created a model that wrote mid-length responses that were neither detailed enough for troubleshooting nor concise enough for escalation. The merged behavior satisfied neither task.

Merging also fails when task distributions are imbalanced. If you merge an adapter trained on 10,000 examples with an adapter trained on 500 examples, the larger adapter's updates dominate. The smaller adapter's contribution is diluted. You can compensate with weighted averaging, but you need to tune weights carefully. The customer support company's billing adapter was trained on 4,200 examples, while the cancellation adapter was trained on 800 examples. Unweighted merging produced a model that over-indexed on billing language. They reweighted to 0.7 billing and 0.3 cancellation to balance contributions.

Merging is most effective when you merge two or three adapters. Beyond three adapters, parameter conflicts accumulate and quality degrades sharply. The customer support company found that merging two adapters achieved 88% to 91% of individual adapter quality, merging three adapters achieved 81% to 84%, and merging all six adapters achieved only 76% even with TIES. If you need to combine more than three tasks, adapter stacking or adapter switching is a better approach.

## Adapter Stacking

Adapter stacking applies multiple adapters sequentially to the base model. Instead of merging adapter weights, you apply adapter A to the base model, then apply adapter B to the output of adapter A. Each adapter operates on the representations produced by the previous adapter. Stacking avoids parameter conflicts because adapters do not share weights. Each adapter modifies the forward pass independently.

Stacking increases inference latency because each adapter adds computation. A single LoRA adapter adds approximately 5% to 15% latency depending on rank and model size. Stacking three adapters adds 15% to 45% latency. The customer support company tested stacking their six adapters and measured 68% latency increase compared to a single adapter. This was unacceptable for their real-time support application.

Stacking order matters. If you stack adapter A then adapter B, you get different results than stacking B then A. The first adapter modifies the base model's representations, and the second adapter operates on those modified representations. A content moderation company trained one adapter for toxicity detection and another for hate speech detection. Stacking toxicity-then-hate-speech produced different classification boundaries than stacking hate-speech-then-toxicity. They tested both orders on validation data and selected the order with better F1 score.

Stacking works well when adapters represent sequential processing steps. A document processing company trained three adapters: one for OCR error correction, one for structure extraction, and one for summarization. They stacked the adapters in order: correction, then extraction, then summarization. Each adapter operated on the output of the previous step. Stacking achieved 93% combined task success, much higher than merging, because the tasks were naturally sequential.

## Adapter Switching and Multi-Task Serving

Adapter switching loads different adapters at inference time based on the request type. You classify each incoming request, determine which task it belongs to, and load the corresponding adapter. The base model remains the same, but each request uses a task-specific adapter. This avoids merging conflicts and stacking latency. Each task gets its own optimized adapter.

Adapter switching requires request routing. You need a classifier or rule-based system that maps requests to task types. The customer support company trained a lightweight BERT-based classifier on 8,000 labeled support tickets. The classifier predicted task category with 96% accuracy. For each incoming request, they classified the task, loaded the appropriate LoRA adapter, ran inference, and returned the result. Total latency increased by 12% compared to a single-adapter deployment: 8% from classification overhead and 4% from adapter loading.

Adapter loading latency depends on adapter size and serving infrastructure. A LoRA adapter with rank 16 on a 7B model is approximately 50MB. Loading from local SSD takes 20ms to 50ms. Loading from network storage takes 100ms to 300ms. The customer support company cached the six most recently used adapters in GPU memory to avoid repeated loads. Cache hit rate was 89%, reducing average adapter load time to 8ms.

You can batch requests by task type to amortize adapter loading cost. If you load an adapter once and process 10 requests with it before switching adapters, the per-request loading overhead is 10x smaller. The customer support company implemented task-based batching with a maximum batch wait time of 200ms. If 10 billing requests arrived within 200ms, they batched them together, loaded the billing adapter once, and processed all 10 requests. This reduced adapter loading overhead to 2% of total latency.

Adapter switching scales well to many tasks. The legal tech SaaS company serves 40 law firm adapters with switching. Each adapter is 65MB at rank 24. They use a 4-GPU inference cluster with 80GB RAM per GPU, caching up to 12 adapters per GPU. Cache eviction uses least-recently-used policy. Average adapter load latency is 15ms with a 94% cache hit rate. Total serving latency is 340ms per request, of which adapter switching contributes 22ms.

## Practical Serving Considerations

If your tasks are few and merging preserves quality, merging is the simplest deployment. You produce one adapter, deploy it once, and avoid runtime routing complexity. The content generation company with tone and grounding adapters merged them into one adapter and deployed it to production. Inference latency matched single-adapter deployment, and quality was 94% of independent adapter quality. Merging was the right choice.

If your tasks are sequential or modular, stacking is appropriate despite latency cost. The document processing company with OCR correction, extraction, and summarization adapters deployed stacking because the tasks were naturally pipelined. Latency increased 38%, but quality improved significantly compared to merging. They accepted the latency trade-off.

If your tasks are numerous, conflicting, or personalized, adapter switching is the only viable option. The legal tech SaaS company could not merge 40 law firm adapters without catastrophic quality loss. They could not stack 40 adapters without prohibitive latency. Switching was the only architecture that preserved per-firm quality while sharing base model infrastructure.

You should benchmark merge quality before deploying a merged adapter. Train each adapter independently, merge them, and test the merged adapter on held-out validation data for all tasks. If merged quality is above 85% of individual adapter quality and latency is acceptable, deploy the merge. If merged quality is below 75%, use stacking or switching. The customer support company tested merged quality at 76%, which was below their 80% threshold. They switched to adapter switching and achieved 91% average quality across all six tasks.

You should version adapters independently. When you retrain one adapter to fix a bug or improve quality, you should not need to retrain or re-merge all other adapters. Adapter switching naturally supports independent versioning. Merging requires regenerating the merged adapter every time any component adapter changes. The legal tech company updates one or two firm adapters per week based on feedback. With switching, they deploy updated adapters immediately. If they had merged 40 adapters, every update would require regenerating and revalidating the merge.

## Debugging Merge Failures

When a merged adapter performs poorly, you need to diagnose whether the problem is task interference, parameter conflict, or merge strategy failure. Start by measuring per-task performance on the merged adapter. If all tasks degrade equally, the merge strategy is globally wrong. If one task degrades severely while others are acceptable, that task conflicts with others.

The customer support company measured per-task accuracy for their six-adapter merge. Technical troubleshooting dropped from 91% to 41%. Billing dropped from 93% to 68%. Account access dropped from 89% to 72%. Escalation dropped from 94% to 28%. The pattern showed that technical troubleshooting and escalation degraded most severely. These tasks had the most distinctive language patterns and conflicted most with other adapters.

You can visualize parameter conflicts by computing the cosine similarity between adapter weight updates. High similarity indicates that adapters modify parameters in similar ways, which means merging will preserve both. Low similarity or negative similarity indicates conflict. The customer support company computed pairwise cosine similarity for all six adapters. Technical troubleshooting and billing had similarity 0.12, indicating low overlap. Escalation and cancellation had similarity negative 0.34, indicating direct conflict. They concluded that merging these adapters was not viable.

You can also analyze which layers conflict most. Compute per-layer cosine similarity and identify the layers where adapters diverge. If conflicts are concentrated in a few layers, you can use layer-specific merge strategies: merge non-conflicting layers with averaging and handle conflicting layers with TIES or parameter selection. The customer support company found that 80% of conflicts were in the final four transformer layers, which handle task-specific output generation. They tested a hybrid merge: linear averaging for the first 28 layers and TIES for the final 4 layers. This improved merged quality to 82%, which was acceptable.

## Adapter Pruning Before Merging

Before merging adapters, you can prune low-impact parameters from each adapter. Pruning removes parameters that contribute minimally to task performance, which reduces the parameter count and decreases merge conflicts. The pruned adapters are smaller and more focused on task-critical updates.

Magnitude pruning removes parameters with small absolute values. If an adapter parameter has magnitude less than a threshold, you set it to zero. This assumes that small-magnitude parameters have minimal impact. The customer support company tested magnitude pruning with thresholds from 0.005 to 0.05. At threshold 0.01, they removed 62% of parameters from each adapter. Individual adapter quality dropped by 1% to 2%, which was acceptable. Merged quality improved to 79% because fewer parameters conflicted.

Gradient-based pruning removes parameters with low gradient magnitudes during training. If a parameter's gradient was consistently small during fine-tuning, that parameter did not contribute much to learning. You prune those parameters before merging. A content moderation company trained two adapters for toxicity and hate speech detection. They logged gradient magnitudes during training and pruned the bottom 50% of parameters by average gradient magnitude. Individual adapter quality was unchanged. Merged quality improved from 72% to 86%.

You can also use Fisher information pruning, which removes parameters with low Fisher information scores. Fisher information measures how much a parameter affects model predictions. Parameters with low Fisher scores can be pruned with minimal quality loss. The customer support company implemented Fisher pruning on their six adapters. They computed Fisher information on 1,000 validation examples per adapter and pruned parameters below the 40th percentile. This removed 40% of parameters per adapter and improved merged quality to 83%.

## Adapter Arithmetic and Task Negation

Task arithmetic allows you to add, subtract, and scale adapter effects. If you have adapter A that teaches task A and adapter B that teaches task B, you can create a merged adapter M that combines both with M equals alpha times A plus beta times B. You can also subtract adapters to remove unwanted behaviors. If adapter C adds verbosity and you want to reduce verbosity, you can merge with M equals A minus gamma times C.

A content generation company trained three adapters: one for formality, one for technical depth, and one for brevity. They wanted a model that was formal and technical but not brief. They merged with M equals 1.0 times formality plus 0.8 times technical depth minus 0.5 times brevity. The merged adapter produced formal, technical, and moderately verbose outputs. This worked because the adapters controlled independent stylistic dimensions.

Task negation is particularly useful for removing safety-degrading fine-tuning. If you fine-tune a safe base model and the fine-tuning accidentally reduces safety, you can train a safety-recovery adapter and add it to the task adapter. A customer support company fine-tuned a model for empathetic responses and found that the fine-tuned model occasionally generated overly familiar or unprofessional language. They trained a professionalism adapter on 1,200 examples of professional tone. They merged the empathy adapter and professionalism adapter with weights 1.0 and 0.6. The merged model maintained empathy while restoring professional boundaries.

You should validate task arithmetic carefully. Adapter subtraction can introduce unexpected behaviors. A legal tech company tried to subtract verbosity from a contract drafting adapter by training a verbosity adapter and subtracting it. The result was not concise contracts but contracts with incomplete sentences and missing clauses. Subtraction removed not only verbosity but also necessary detail. They abandoned subtraction and retrained the contract adapter with examples that demonstrated appropriate conciseness.

## Dynamic Adapter Composition at Inference Time

Some serving frameworks support dynamic adapter composition, where you combine multiple adapters at inference time without pre-merging. The serving system loads multiple adapters, applies them sequentially or in parallel, and combines their outputs. This allows you to compose adapters on a per-request basis.

Dynamic composition is useful when the task mix varies by request. A multilingual customer support system serves requests in 12 languages. They trained one language-specific adapter per language and one task-specific adapter for each of six support categories. For each request, they load the appropriate language adapter and task adapter and stack them. A billing question in Spanish loads the Spanish adapter and billing adapter. A technical question in French loads the French adapter and technical adapter. This avoids training 72 separate adapters for every language-task combination.

Dynamic composition introduces latency overhead. Loading two adapters instead of one increases latency by 50% to 100% depending on caching. The multilingual support system measured 180ms baseline latency with one adapter and 310ms with two dynamically-loaded adapters. They optimized by pre-loading the most common adapter combinations into GPU memory. The top 12 language-task pairs covered 78% of traffic. Pre-loading those 24 adapters reduced average latency to 210ms.

You can also use adapter ensembling, where you run the same input through multiple adapters and combine their outputs. This is expensive but can improve quality on difficult inputs. A medical coding company trained three adapters for ICD-10 code prediction, each using different training data samples. For high-confidence predictions, they used only the first adapter. For low-confidence predictions, they ran all three adapters and selected the code predicted by at least two adapters. This ensemble approach improved accuracy on edge cases by 11 percentage points while adding latency only on uncertain inputs.

## Adapter Merging for Continual Learning

Adapter composition enables continual learning, where you add new capabilities to a deployed model without catastrophic forgetting. You deploy a model with adapter A, then train adapter B for a new task, and merge A and B. The merged model handles both tasks. You later train adapter C and merge A, B, and C. This incremental merging avoids retraining the entire model on all tasks simultaneously.

A customer support platform launched with one adapter for product questions in January 2025. In March, they added a billing support adapter and merged it with the product adapter. In June, they added a technical troubleshooting adapter and merged all three. In September, they added an escalation adapter and merged all four. Each merge preserved previous capabilities while adding new ones. The platform evolved from one task to four tasks over nine months without a single monolithic retraining.

Continual learning through merging requires monitoring for quality drift. Each merge introduces small quality losses. If you merge 10 adapters sequentially, the tenth merge may degrade the first adapter's quality significantly. The customer support platform measured per-task quality after each merge. After the fourth merge, product question quality had drifted from 92% to 87%. They retrained the product adapter on updated data and re-merged all four adapters. Quality recovered to 91%.

You can limit drift by using merge strategies that preserve earlier adapters. When merging adapter D into an existing merge of A, B, and C, you can assign higher weight to the existing merge. The formula becomes M equals 0.8 times existing merge plus 0.2 times D. This prioritizes preserving A, B, and C at the cost of weaker D integration. The customer support platform used decreasing weights for each new adapter: first merge was 0.5 and 0.5, second merge was 0.7 and 0.3, third merge was 0.8 and 0.2. This stabilized quality drift.

## Parameter Efficiency and Memory Footprint

One major advantage of adapter composition is parameter efficiency. Instead of training six full fine-tuned models, you train one base model and six lightweight adapters. The base model is 70B parameters, approximately 140GB at half precision. Each LoRA adapter with rank 16 is 45 million parameters, approximately 90MB. Six adapters total 270 million parameters, about 540MB. The storage savings are dramatic: 540MB for six adapters versus 840GB for six full models.

Parameter efficiency also affects serving costs. You load the base model once into GPU memory and swap adapters as needed. Loading a 140GB base model takes 8 to 12 seconds on high-speed NVMe storage. Loading a 90MB adapter takes 20 to 50 milliseconds. The customer support company serves six tasks with one 140GB base model loaded persistently and six 90MB adapters cached in RAM. Total GPU memory usage is 141GB. If they served six full models, they would need 840GB of GPU memory, requiring multiple GPUs.

Adapter composition enables rapid experimentation. Training a LoRA adapter takes hours to days. Training a full fine-tuned model takes days to weeks. Merging two adapters takes seconds. The customer support company can test new task combinations by merging existing adapters without training new models. They tested 15 different merge configurations in one afternoon. Testing 15 full model configurations would have taken weeks.

You can also version and archive adapters cheaply. Each adapter is 90MB, small enough to store hundreds of versions. The customer support company archives every adapter version, keeping a full history of model evolution. They store 180 adapter versions, totaling 16GB. Archiving 180 full model versions would require 25TB of storage.

## Adapter Composition for A/B Testing

Adapter composition simplifies A/B testing and gradual rollouts. You can deploy two adapters for the same task, route 10% of traffic to adapter A and 90% to adapter B, and compare performance. If adapter A outperforms adapter B, you increase its traffic share. This is much simpler than deploying two full models.

A content moderation company trained two adapters for hate speech detection using different training data samples. Adapter A was trained on 5,000 examples emphasizing context-dependent hate speech. Adapter B was trained on 5,000 examples emphasizing explicit slurs. They deployed both adapters with 50-50 traffic split. Adapter A achieved 91% precision and 84% recall. Adapter B achieved 88% precision and 89% recall. They merged the adapters using TIES to combine both strengths. The merged adapter achieved 92% precision and 87% recall, outperforming both individual adapters.

You can also use adapter composition for champion-challenger testing. Deploy your production adapter as the champion and a new experimental adapter as the challenger. Route 95% of traffic to the champion and 5% to the challenger. Monitor quality metrics. If the challenger outperforms the champion over one week, promote it to champion. The customer support company uses this pattern to test new adapters continuously. They train a new adapter every two weeks, deploy it as a challenger, and promote it if it beats the champion.

Adapter switching enables per-user personalization testing. You can assign different users to different adapters and measure engagement or satisfaction. A customer support platform tested two escalation adapters: one that used formal apologetic language and one that used warm empathetic language. They assigned users randomly to one adapter or the other and measured customer satisfaction scores. The empathetic adapter scored 4.2 out of 5, compared to 3.8 for the formal adapter. They deployed the empathetic adapter globally.

## Adapter Composition Across Model Families

You can train adapters on one base model and transfer them to a related base model with similar architecture. If you train a LoRA adapter on Llama 3.1 70B, you might be able to use it on Llama 3.2 70B if the architectures are compatible. This allows you to upgrade base models without retraining all adapters.

Adapter transfer works only when base models have identical architecture: same layer count, same hidden dimension, same attention head count. The parameter names must match exactly. A content generation company trained 12 adapters on Llama 3.1 8B. When Llama 3.2 8B was released, they tested loading their existing adapters onto the new base model. Eleven of twelve adapters worked without modification. One adapter failed because Llama 3.2 renamed a parameter. They updated the parameter mapping and the adapter worked.

Adapter transfer quality degrades when base models differ significantly. If the new base model has different training data, different tokenization, or architectural changes, transferred adapters may perform poorly. The content generation company measured per-task quality on Llama 3.2 with transferred adapters. Quality ranged from 82% to 94% of original Llama 3.1 performance. Most adapters worked well, but two adapters degraded substantially. They retrained those two adapters on Llama 3.2.

You should always validate transferred adapters on held-out test data. Do not assume that an adapter trained on base model A will perform identically on base model B, even if the architectures match. The content generation company tested all transferred adapters on validation sets and caught the two degraded adapters before deploying them.

## The Future of Adapter Composition

Adapter composition is evolving rapidly. Current research explores mixture-of-experts architectures where adapters are dynamically selected or blended based on input characteristics. Instead of routing each request to one adapter, the system might blend three adapters with weights determined by the input content. A customer support system might detect that a request is 60% technical, 30% billing, and 10% escalation, and blend the corresponding adapters with those weights.

Hierarchical adapter composition stacks domain adapters and task adapters in layers. You apply a domain adapter to teach vocabulary and knowledge, then stack a task adapter to teach behavior. A medical AI system might apply a cardiology domain adapter first, then stack a diagnosis task adapter second. This separates domain knowledge from task structure and allows reusing domain adapters across multiple tasks.

Adapter libraries and marketplaces are emerging. Organizations may publish adapters for public or commercial use. A legal tech company might publish adapters for contract law, patent law, and employment law. Other companies could license those adapters and compose them with their own task-specific adapters. This would reduce training costs and accelerate deployment.

Adapter composition is a powerful tool for multi-task and multi-tenant deployments, but it is not automatic. Merging requires careful strategy selection and quality validation. Stacking requires latency budgeting and task ordering. Switching requires routing infrastructure and caching. The next subchapter covers hyperparameter selection for fine-tuning, which determines whether your adapters learn effectively in the first place.
