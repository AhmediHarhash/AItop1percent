# 3.11 â€” Mixing Ratios: Combining Real and Synthetic Data Optimally

In late 2025, a fraud detection team at a fintech company fine-tuned a transaction classification model using 15,000 synthetic examples generated by GPT-4.5. The synthetic data covered rare fraud patterns that appeared infrequently in production logs. Internal evaluations showed strong performance, and the team deployed the model to production. Within two weeks, precision dropped by twelve percentage points. The model flagged legitimate transactions that synthetic data had never represented. The root cause was dataset composition. The team used pure synthetic data without grounding it in real-world distributional characteristics. Real production data contained timing patterns, merchant name variations, and user behavior quirks that synthetic examples, generated from abstract fraud pattern descriptions, completely missed. The model learned synthetic patterns but failed to generalize to real patterns.

This failure is common. Pure synthetic datasets, no matter how carefully generated, lack the distributional authenticity of real data. They overrepresent clean, well-structured examples and underrepresent edge cases, noise, and contextual variation that define real-world tasks. Conversely, pure real datasets often lack coverage of rare but critical cases, especially in domains with long-tail distributions. The optimal approach combines real and synthetic data in proportions that balance distributional fidelity with coverage completeness. Research and production evidence from 2024 through 2026 show that mixed datasets consistently outperform pure datasets across most tasks. This subchapter teaches you how to determine the right mixing ratio for your task, apply stratified mixing to handle category imbalances, and implement curriculum approaches that sequence real and synthetic data to maximize learning efficiency.

## Why Pure Synthetic Data Underperforms Mixed Datasets

Pure synthetic data suffers from three fundamental weaknesses: distributional mismatch, homogeneity bias, and teacher model limitations. Distributional mismatch occurs when synthetic examples are generated from idealized prompts or templates that do not reflect the statistical properties of real inputs. A customer support model trained on synthetic questions generated from product documentation will encounter real questions with typos, slang, incomplete sentences, and context the documentation never anticipated. The synthetic training data is cleaner, more grammatically correct, and more topical than production data. The model overfits to synthetic cleanliness and underperforms on real messiness.

Homogeneity bias occurs when synthetic generation produces examples that cluster around the teacher model's preferred style, phrasing, and structure. GPT-4o has recognizable stylistic patterns: certain phrase constructions, sentence lengths, and rhetorical structures appear more frequently in its outputs than in human-written text. A dataset of 10,000 GPT-4o-generated examples will exhibit lower stylistic diversity than 10,000 human-written examples. A student model fine-tuned on homogeneous synthetic data learns to reproduce the teacher's style but struggles with inputs that deviate from that style. The result is brittleness.

Teacher model limitations manifest when synthetic data inherits the teacher's errors, biases, and knowledge gaps. If you generate medical diagnosis examples with GPT-4.5 and the teacher model misunderstands a rare condition or hallucinates a treatment protocol, those errors propagate into your training data. A student model trained on flawed synthetic examples learns the teacher's mistakes. Real data, even when noisy or incomplete, reflects ground truth as annotated by domain experts. Real data errors are random noise. Synthetic data errors are systematic bias. The distinction matters. A model can learn to ignore random noise through regularization, but it cannot unlearn systematic bias without being exposed to correct examples.

Mixing real and synthetic data mitigates all three weaknesses. Real data provides distributional grounding, ensuring the model encounters the messy, noisy, varied inputs it will see in production. Synthetic data provides coverage expansion, ensuring the model encounters rare, high-stakes cases that real data underrepresents. The combination balances fidelity and completeness. The mixing ratio determines the balance point.

The interaction between real and synthetic data is not simply additive. A model trained on 5,000 real examples and 5,000 synthetic examples does not perform as well as two separate models trained on each dataset and averaged. The mixed dataset forces the model to learn patterns that generalize across both distributions. The model cannot simply memorize synthetic style or real noise. It must extract features that predict correctly on both data sources. This regularization effect improves generalization beyond what either data source achieves alone.

## Optimal Mixing Ratio Research and Evidence

Research from 2024 and 2025 consistently shows that mixed datasets outperform pure datasets for most supervised fine-tuning tasks. A widely cited study from mid-2024 tested classification and generation tasks across ten domains with mixing ratios from zero percent synthetic to one hundred percent synthetic in increments of ten percent. The results showed a consistent inverted-U pattern: performance increased as synthetic data was added to real data, peaked at 30 to 50 percent synthetic, then declined as synthetic data dominated. The peak varied by task complexity. Simple classification tasks peaked at 30 percent synthetic. Complex reasoning tasks peaked at 50 percent synthetic. No task performed best at zero percent or one hundred percent synthetic.

A follow-up study in early 2025 tested larger datasets and found that the optimal ratio shifts with dataset size. For small datasets under 1,000 examples, 50 to 70 percent synthetic data improved performance because real data alone provided insufficient coverage. For medium datasets between 1,000 and 10,000 examples, 30 to 50 percent synthetic was optimal. For large datasets above 10,000 examples, 10 to 30 percent synthetic sufficed because real data already provided strong coverage. The key insight is that synthetic data is most valuable when real data is scarce or imbalanced. As real data volume grows, the marginal value of synthetic data decreases.

Production evidence from 2025 and 2026 confirms the research findings. A healthcare NLP team reported that mixing 40 percent synthetic clinical notes with 60 percent real notes improved model recall by eight percentage points compared to real-only training, while maintaining precision. A legal document analysis team found that 35 percent synthetic contract clauses mixed with 65 percent real clauses improved coverage of rare contract types without sacrificing accuracy on common types. A customer support routing model trained on 50 percent synthetic and 50 percent real conversations achieved 95 percent accuracy, compared to 89 percent for real-only and 87 percent for synthetic-only training. The evidence is consistent: mixed datasets outperform pure datasets.

The optimal ratio is not universal. It depends on four factors: real data quality, real data volume, task complexity, and teacher model quality. If your real data is noisy, mislabeled, or outdated, increasing synthetic proportion to 50 to 70 percent compensates for real data weaknesses. If your real data is high-quality and abundant, keeping synthetic proportion at 10 to 30 percent avoids diluting real signal. If your task is complex and requires nuanced reasoning, higher synthetic proportions provide richer training signal. If your task is simple pattern matching, lower synthetic proportions avoid overfitting to teacher style. If your teacher model is state-of-the-art and reliable, higher synthetic proportions are safe. If your teacher model is mediocre, lower synthetic proportions limit error propagation.

The quality dimension is particularly important. A dataset with high-quality synthetic data and low-quality real data may perform best at 60 to 80 percent synthetic, reversing the typical ratio. Quality-adjusted mixing treats each data source's contribution as quality times quantity. If synthetic data has 90 percent quality and real data has 70 percent quality, mixing 50 percent of each yields an effective ratio of 56 percent synthetic after quality adjustment. The optimal ratio maximizes quality-weighted coverage, not raw example counts.

## Determining the Right Ratio for Your Task

The empirical approach to finding the optimal ratio is grid search over a validation set. Start with five mixing ratios: zero percent, 25 percent, 50 percent, 75 percent, and one hundred percent synthetic. For each ratio, train a model on the mixed dataset and evaluate on a held-out real-world test set. The ratio that maximizes validation performance is your starting point. Refine by testing ratios within ten percentage points of the peak. If 50 percent performs best in the initial search, test 40 percent, 45 percent, 50 percent, 55 percent, and 60 percent. The refinement step often reveals that performance is relatively flat across a range, meaning you can choose based on cost or other constraints.

The validation set must reflect production distribution. If your validation set is synthetic, you will measure how well the model fits synthetic data, not how well it generalizes to real data. The validation set must be real-world examples, preferably recent production samples. If production distribution shifts over time, update the validation set quarterly to ensure the mixing ratio remains optimal.

The grid search approach requires training five to ten models, which is expensive for large models or datasets. The cost can be reduced by using smaller proxy models for the grid search. Train 7B parameter models across all ratios, identify the optimal ratio, then train a single 70B parameter model at that ratio. The proxy approach assumes the optimal ratio generalizes across model sizes, which is usually true but not guaranteed. Validating the assumption on a small-scale experiment prevents costly errors.

The second approach is stratified analysis by subcategory. If your task has multiple categories with different data availability, optimal ratios vary by category. A fraud detection task might have abundant real data for common fraud types like phishing but scarce real data for rare types like account takeover. Mixing 10 percent synthetic phishing examples with 90 percent real phishing examples balances coverage without dilution. Mixing 70 percent synthetic account takeover examples with 30 percent real examples compensates for scarcity. The overall dataset mixing ratio is the weighted average across categories, but the per-category ratios differ.

Stratified mixing ensures that every category receives sufficient training signal without over- or under-representing synthetic data. The implementation is straightforward. Generate synthetic examples separately for each category, combine with real examples for that category at the category-specific ratio, then merge all categories into the final training set. The result is a dataset where common categories are mostly real and rare categories are mostly synthetic, balancing fidelity and coverage across the distribution.

The stratified approach requires category labels for both real and synthetic data. If your data is unlabeled, use clustering or topic modeling to infer categories, then apply stratified mixing. The inferred categories are less precise than human labels but sufficient for mixing ratio optimization. A customer support dataset might cluster into product questions, billing questions, and technical support questions. Each cluster receives a category-specific mixing ratio based on real data availability.

## Stratified Mixing for Category Imbalances

Category imbalance is one of the strongest signals for high synthetic ratios. If your real dataset contains 8,000 examples of Class A and 200 examples of Class B, a model trained on this imbalance will overfit to Class A and ignore Class B. Traditional rebalancing techniques like oversampling duplicate the 200 Class B examples until the dataset is balanced, but duplication does not add new information. The model memorizes the 200 examples and still fails to generalize to unseen Class B inputs.

The memorization problem is measurable. A model trained on oversampled Class B examples achieves 95 percent accuracy on the training set but only 60 percent accuracy on a held-out test set. The 35-point gap indicates severe overfitting. A model trained on synthetic Class B examples achieves 85 percent training accuracy and 80 percent test accuracy. The 5-point gap indicates healthy generalization. The synthetic approach trades slight training accuracy for substantial test accuracy, which is always the correct tradeoff in production systems.

Synthetic data generation solves this. Generate 1,800 synthetic Class B examples using a teacher model, mix them with the 200 real Class B examples, and balance the dataset without duplication. The 2,000 Class B examples now provide coverage diversity the real data lacked. The mixing ratio for Class B is 90 percent synthetic, reflecting extreme scarcity of real data. The mixing ratio for Class A remains 10 percent synthetic or lower, reflecting abundance of real data. The overall dataset is balanced, and every class has sufficient diversity.

The risk in stratified mixing is that minority classes become entirely synthetic if you are not careful. A dataset where Class B is 95 percent synthetic may perform well on synthetic-like Class B examples but poorly on real Class B examples that deviate from synthetic patterns. The mitigation is ensuring every class has at least 10 to 20 percent real data, even if that means undersampling the majority class or collecting more real minority class examples. The 10 to 20 percent real threshold provides distributional grounding that prevents complete synthetic drift.

Stratified mixing also applies to difficulty levels within a single task. If your real data overrepresents easy examples and underrepresents hard examples, generate synthetic hard examples to fill the gap. A reading comprehension task might have abundant real examples with simple factual questions but few real examples with multi-hop reasoning questions. Mixing 80 percent synthetic multi-hop questions with 20 percent real multi-hop questions balances difficulty distribution. The overall dataset remains mostly real for easy examples and mostly synthetic for hard examples, ensuring the model learns the full difficulty spectrum.

The difficulty stratification requires defining difficulty levels. For some tasks, difficulty is explicit: question types, reasoning depth, number of constraints. For other tasks, difficulty must be inferred. Use model confidence scores as a proxy: examples where a baseline model has low confidence are hard, high confidence examples are easy. Generate synthetic data targeting the low-confidence regions to improve coverage of difficult cases.

Stratified mixing becomes complex when multiple stratification dimensions intersect. A multi-class, multi-difficulty task might stratify by class and difficulty, resulting in a two-dimensional grid of mixing ratios. Each cell in the grid gets a specific ratio based on real data availability and importance. High-importance rare-class hard-examples might be 90 percent synthetic. Low-importance common-class easy-examples might be 5 percent synthetic. Managing the grid requires careful bookkeeping but produces the most precise control over dataset composition.

## Curriculum Approaches: Real Data First, Synthetic Augmentation Second

Curriculum learning sequences training data by difficulty or distributional characteristics to improve learning efficiency. The real-first curriculum trains a model on real data first, then fine-tunes on synthetic data second. The approach leverages real data's distributional fidelity to establish a strong foundation, then uses synthetic data to expand coverage and handle edge cases. The two-stage training takes longer than single-stage mixed training but often achieves better performance by preventing synthetic data from dominating early learning.

A customer service routing model trained with the real-first curriculum learns the typical phrasing, structure, and context of real customer questions in the first stage. The model's internal representations align with production distribution. In the second stage, synthetic examples introduce rare question types, ambiguous phrasings, and edge cases the real data lacked. The model fine-tunes its decision boundaries to handle these cases without forgetting the core real-data patterns. The result is a model that is grounded in reality but robust to edge cases.

The two-stage approach requires monitoring for catastrophic forgetting. If the second stage uses too high a learning rate or too many synthetic examples, the model forgets real-data patterns while learning synthetic patterns. The forgetting manifests as declining performance on a real-data validation set during the second stage. Early stopping based on real-data validation loss prevents forgetting. If validation loss increases for three consecutive evaluation steps, stop the second stage training even if synthetic-data loss continues decreasing. The real-data performance is what matters in production.

The real-first curriculum requires careful tuning of the second-stage learning rate. If the learning rate is too high, the model forgets real-data patterns and overfits to synthetic patterns. If the learning rate is too low, the model barely adapts to synthetic data and gains little benefit. The optimal learning rate for the second stage is typically 10 to 50 percent of the first-stage learning rate, depending on the volume of synthetic data relative to real data. Empirical tuning on a validation set identifies the optimal configuration.

The synthetic-first curriculum reverses the order: train on synthetic data first, then fine-tune on real data. This approach is less common but useful when synthetic data is more abundant or higher quality than real data. A legal reasoning task with 10,000 high-quality synthetic examples and 1,000 noisy real examples benefits from synthetic-first training. The model learns clean reasoning patterns from synthetic data, then adapts to real-world noise in the second stage. The risk is that the model overfits to synthetic style and struggles to unlearn it during real-data fine-tuning. The mitigation is using a lower learning rate in the second stage to avoid catastrophic forgetting.

The synthetic-first approach is particularly valuable for cold-start problems where real data is initially unavailable. A new product feature with no production data can train a model on synthetic data, deploy it, collect real production data, then fine-tune on real data. The synthetic-trained model serves as a reasonable baseline until real data accumulates. The two-stage approach accelerates time-to-deployment compared to waiting for sufficient real data.

The interleaved curriculum mixes real and synthetic data in every batch but controls the ratio over time. Early training batches are 80 percent real and 20 percent synthetic. Mid-training batches are 50-50. Late-training batches are 30 percent real and 70 percent synthetic. The ratio shift exposes the model to real data when it is most impressionable and synthetic data when it has already learned core patterns. The interleaved approach is more complex to implement than single-stage mixing but offers finer control over learning dynamics.

The interleaved curriculum implementation requires dynamic data loading. Standard training loops sample batches uniformly from the dataset. Interleaved curriculum requires sampling from real and synthetic subsets separately, combining them at the target ratio for the current epoch. The sampling logic adds 50 to 100 lines of code but is straightforward. The performance benefit is modest, typically 1 to 3 percentage points over single-stage mixing, but justified for high-stakes applications where every improvement matters.

The choice of curriculum depends on real data quality and quantity. If real data is high-quality and abundant, real-first or interleaved curricula work best. If real data is scarce or noisy, synthetic-first or high-synthetic interleaved curricula work better. The curriculum design is a hyperparameter that should be tuned empirically on validation data, just like mixing ratio.

## How Mixing Ratios Interact with Dataset Size and Quality

Mixing ratios are not independent of dataset size. Small datasets benefit from higher synthetic ratios because real data alone provides insufficient coverage. A task with only 200 real examples gains significant value from adding 800 synthetic examples, raising total size to 1,000 and synthetic proportion to 80 percent. The large synthetic proportion is justified because the alternative is training on 200 examples, which is insufficient for most fine-tuning tasks.

The size-ratio relationship is nonlinear. Doubling dataset size from 1,000 to 2,000 examples reduces optimal synthetic proportion from 60 percent to 45 percent. Doubling again from 2,000 to 4,000 reduces it from 45 percent to 35 percent. The marginal value of synthetic data decreases as real data accumulates because additional real examples provide diminishing coverage gains. The optimal ratio curve follows a logarithmic decay: steep decline at small sizes, gradual decline at large sizes.

This nonlinearity has practical implications for data collection strategy. If you plan to accumulate 10,000 real examples over six months, starting with 1,000 real examples and 2,000 synthetic examples is suboptimal. The synthetic proportion is too high for the eventual dataset size, and you will need to rebalance later. Starting with 1,000 real examples and 500 synthetic examples anticipates the trajectory and requires less rebalancing as real data grows. Forward-looking ratio selection reduces long-term retraining costs.

Medium datasets between 1,000 and 10,000 examples benefit from moderate synthetic ratios of 30 to 50 percent. The real data provides enough coverage that synthetic data serves primarily to handle rare cases and balance categories. A dataset of 5,000 real examples gains value from adding 2,000 to 5,000 synthetic examples, but adding 20,000 synthetic examples dilutes real signal without corresponding performance gain.

Large datasets above 10,000 examples benefit from low synthetic ratios of 10 to 30 percent. The real data is abundant enough that synthetic data is needed only for extreme tail cases or to enforce policy-compliant coverage of sensitive topics. A dataset of 50,000 real examples might add 5,000 to 15,000 synthetic examples to ensure edge case robustness, but adding 50,000 synthetic examples is wasteful.

The interaction with quality is equally important. High-quality real data reduces the value of synthetic data. If your real examples are expertly labeled, carefully curated, and representative, mixing in synthetic data risks diluting quality. The optimal ratio may be as low as 5 to 10 percent synthetic. Low-quality real data increases the value of synthetic data. If your real examples are noisy, mislabeled, or biased, synthetic examples generated from a high-quality teacher model can improve overall dataset quality. The optimal ratio may rise to 50 to 70 percent synthetic.

The quality-adjusted mixing ratio accounts for both real and synthetic quality. If real data quality is 80 percent and synthetic data quality is 90 percent, mixing 50 percent synthetic not only expands coverage but also improves average quality. If real data quality is 95 percent and synthetic data quality is 70 percent, mixing more than 20 percent synthetic degrades average quality. Measuring real and synthetic quality separately, then optimizing the mixing ratio to maximize quality-weighted coverage, is the most principled approach.

Quality measurement requires a labeled validation set that reflects ground truth. Evaluate both real and synthetic examples against this validation set using task-specific metrics: accuracy for classification, ROUGE for summarization, human preference for generation. The quality scores inform the mixing ratio optimization. If synthetic data scores 85 percent and real data scores 75 percent, increasing synthetic ratio improves performance even though intuition suggests real data should be higher quality.

The size-quality interaction creates a three-dimensional optimization space: dataset size, mixing ratio, and per-source quality. The optimal configuration maximizes performance subject to cost and time constraints. Small high-quality real datasets with high-quality synthetic data favor 40 to 60 percent synthetic. Large low-quality real datasets with low-quality synthetic data favor 10 to 20 percent synthetic. Medium mixed-quality datasets require empirical tuning to find the optimum.

## Practical Implementation of Mixed Datasets

Implementing mixed datasets requires careful dataset construction to avoid unintentional biases. The naive approach is concatenating real and synthetic examples in separate blocks: first 6,000 real examples, then 4,000 synthetic examples. This creates temporal bias. If you shuffle the concatenated dataset before training, the model encounters both real and synthetic examples throughout training, but the shuffling does not eliminate distributional differences between the two sources.

The temporal bias manifests subtly. If real examples are encountered in the first half of training and synthetic examples in the second half, the model's internal representations are shaped by real data initially, then adjusted by synthetic data. The adjustment can cause instability or forgetting if the two distributions differ significantly. Even with shuffling, clustering effects occur if synthetic examples share generation patterns that real examples lack. The model learns to recognize synthetic style and may develop separate decision pathways for real versus synthetic inputs, reducing generalization.

The stratified shuffle approach ensures even distribution. Divide the dataset into ten folds. In each fold, include the target mixing ratio: if the overall ratio is 40 percent synthetic, each fold contains 40 percent synthetic and 60 percent real examples. Shuffle within each fold, then concatenate folds. The result is a dataset where every stage of training exposes the model to the target ratio. The stratified shuffle prevents the model from learning real patterns in early epochs and synthetic patterns in late epochs, which can cause instability.

The stratified shuffle implementation is simple. Sort examples by source, compute fold sizes, allocate examples to folds maintaining the target ratio, shuffle within folds, concatenate. The process takes 20 to 50 lines of code and ensures balanced exposure throughout training. The performance benefit is modest but the implementation cost is negligible, making it a default best practice.

The second implementation decision is whether to tag examples by source. Adding a source field to each training example allows you to track performance by real versus synthetic origin during evaluation. If your model performs well on synthetic test examples but poorly on real test examples, the source tags reveal that the mixing ratio is too high or the synthetic data quality is too low. Source tagging is primarily a diagnostic tool and should not be exposed to the model during training, as it can cause the model to learn different decision rules for real versus synthetic inputs.

Source tagging enables post-training analysis. After training, evaluate model performance on real-only and synthetic-only test sets. If performance is balanced, the mixing ratio is appropriate. If real performance lags, increase real proportion. If synthetic performance lags, increase synthetic proportion. The iterative adjustment converges to the optimal ratio over two to three training cycles.

The third decision is handling duplicates. If your synthetic generation pipeline occasionally produces examples very similar to real examples, deduplication is necessary. Use embedding-based similarity to detect near-duplicates and remove synthetic examples that are too similar to real examples. The threshold is task-specific, but a cosine similarity above 0.95 typically indicates near-duplication. Deduplication ensures that the mixing ratio reflects true diversity, not redundant coverage.

Deduplication should favor real data over synthetic data. When a near-duplicate pair is detected, remove the synthetic example and keep the real example. Real data is always preferable to synthetic approximations when both are available. The deduplication process prevents synthetic data from artificially inflating dataset size without adding information.

The deduplication threshold requires calibration. A threshold of 0.99 only catches exact duplicates, missing near-duplicates that add minimal value. A threshold of 0.85 catches too many semantically similar but genuinely distinct examples, removing valuable training signal. The optimal threshold is task-specific. For tasks with high linguistic diversity like customer support dialogues, 0.90 to 0.92 works well. For tasks with constrained outputs like form filling, 0.95 to 0.97 is appropriate. Running deduplication at multiple thresholds on a sample and manually reviewing borderline pairs calibrates the threshold for your task.

Deduplication also applies within the synthetic dataset. If your generation pipeline produces multiple synthetic examples that are nearly identical to each other, keeping all of them wastes training compute without improving model performance. Within-synthetic deduplication uses the same embedding-based approach but removes the lower-quality example from each duplicate pair rather than favoring real data. The quality determination can use automated scores or timestamp order, with later-generated examples presumed to benefit from refined prompts.

## Iterating on Mixing Ratios Over Time

The optimal mixing ratio is not static. As you collect more real data, the optimal synthetic proportion decreases. A model launched with 1,000 real examples and 4,000 synthetic examples at an 80 percent synthetic ratio should be retrained six months later with 5,000 real examples and 2,000 synthetic examples at a 29 percent synthetic ratio. The ratio evolution reflects increasing real data availability. Monitoring real data accumulation and retraining at quarterly or biannual intervals keeps the dataset aligned with production distribution.

The monitoring infrastructure should track real data volume by category and difficulty level. If your fraud detection model accumulates 2,000 new phishing examples but only 50 new account takeover examples, the per-category ratios diverge. Phishing ratio decreases from 30 percent synthetic to 15 percent synthetic. Account takeover ratio remains at 70 percent synthetic due to continued scarcity. The category-level tracking ensures retraining decisions reflect actual data availability rather than overall averages that obscure imbalances.

The iteration cadence depends on real data accumulation rate and model performance sensitivity. If your application collects 1,000 new real examples per month and model performance improves significantly with each 2,000 new examples, quarterly retraining is justified. If real data accumulates slowly or performance is insensitive to dataset size, annual retraining suffices. The cost-benefit analysis balances retraining cost against performance improvement.

The second reason to iterate is teacher model improvement. If you generated synthetic data with GPT-4o in early 2025 and GPT-5 is released in late 2025 with significantly better reasoning and adherence, regenerating synthetic data with GPT-5 improves quality. The improved quality may justify increasing the synthetic proportion or may allow you to reduce synthetic volume while maintaining performance. Testing the new synthetic data against the old synthetic data on a validation set quantifies the improvement and informs ratio adjustments.

Regenerating synthetic data is expensive, so the decision requires evidence of significant quality improvement. Generate 500 examples with the new teacher model, compare quality against 500 examples from the old teacher model, and calculate the performance delta. If the new synthetic data improves validation performance by 3 to 5 percentage points, full regeneration is justified. If the improvement is 0.5 to 1 percentage point, the regeneration cost may not be worth the gain.

The third reason to iterate is task evolution. If your application scope expands to cover new categories, edge cases, or languages, the existing mixing ratio may no longer be optimal. A customer support model initially trained on English-only data with 40 percent synthetic may expand to Spanish and French. The English mixing ratio remains 40 percent, but Spanish and French have scarce real data, requiring 70 to 80 percent synthetic. The overall dataset mixing ratio becomes category-weighted, reflecting the multi-language distribution.

Task evolution often introduces new data scarcity challenges. When expanding to a new domain, you start with zero real data and must rely entirely on synthetic data initially. As real data accumulates, the mixing ratio for the new domain decreases from 100 percent synthetic toward the overall target ratio. Managing per-domain ratios requires tracking data availability and performance separately for each domain, adding operational complexity but ensuring optimal performance across all domains.

Iteration is not free. Each retraining cycle costs compute, engineering time, and potential downtime. The iteration frequency should balance improvement gains against operational costs. If quarterly retraining improves F1 by two points and annual retraining improves F1 by three points, the incremental one-point gain from quarterly retraining may not justify four-times operational overhead. The cost-benefit analysis is application-specific.

## The Mixing Ratio as a First-Class Hyperparameter

Mixing ratio is not an implementation detail. It is a first-class hyperparameter that determines dataset composition, model performance, and generalization characteristics. Treating it as an afterthought or choosing it arbitrarily wastes the potential value of synthetic data. The best teams tune mixing ratio with the same rigor they apply to learning rate, batch size, and regularization strength. They run systematic experiments, measure performance across ratios, and document the optimal configuration for each task.

The documentation should include not just the final ratio but the reasoning behind it: real data volume, real data quality, synthetic data quality, task complexity, and validation performance across tested ratios. This documentation guides future iterations and helps onboard new team members. When the task evolves or new data becomes available, the documented reasoning allows you to update the ratio intelligently rather than guessing.

The mixing ratio decision should be revisited whenever dataset composition changes significantly. Adding 5,000 new real examples to a 10,000-example dataset changes the optimal ratio. Discovering that 20 percent of real examples are mislabeled changes the optimal ratio. Upgrading the teacher model changes the optimal ratio. Each change triggers a review and potential re-tuning of the mixing ratio.

The mixing ratio also interacts with other hyperparameters. A model trained with high regularization may tolerate higher synthetic ratios because regularization mitigates overfitting to synthetic style. A model trained with low regularization may require lower synthetic ratios to avoid overfitting. The interaction means mixing ratio should be tuned jointly with other hyperparameters, not in isolation. A full hyperparameter sweep includes mixing ratio as one dimension alongside learning rate, batch size, and regularization strength.

Optimal mixing ratios balance distributional fidelity from real data with coverage expansion from synthetic data, consistently outperforming pure real or pure synthetic approaches. The next subchapter addresses multi-stage distillation: chaining teacher models for complex tasks where single-stage generation is insufficient.
