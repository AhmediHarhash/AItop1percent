# 9.11 — Governance and Compliance: EU AI Act Obligations for Fine-Tuned GPAI Models

The numbers tell the story: three hundred forty thousand euros in emergency legal fees, sixty days to produce documentation that should have existed from day one, three months of halted customer onboarding, and ultimately full exit from the EU market. The root cause was not technical incompetence—it was regulatory blindness. September 2025: SaaS company offering fine-tuned chatbot for enterprise customer service received formal inquiry from Irish Data Protection Commission. The inquiry cited Article 53 of the EU AI Act, in force thirteen months. Regulator demanded technical documentation demonstrating downstream modifier compliance: evidence the fine-tuned model had been evaluated for systemic risks, that training data governance met GPAI standards, that the company had implemented relevant provisions of the GPAI Code of Practice. The company had no documentation. They had treated fine-tuning as purely technical exercise, unaware the EU AI Act classified them as downstream modifier with explicit legal obligations. Given sixty days to produce compliant documentation or face enforcement. They engaged external counsel, spent three hundred forty thousand euros reconstructing compliance evidence, halted new customer onboarding three months, and ultimately exited the EU market—unable to demonstrate retroactive compliance for models already deployed.

The failure was ignorance of the regulatory landscape. The EU AI Act, enforced since August 2024, creates a specific legal category for **downstream modifiers**—entities that take a general-purpose AI model and adapt it through fine-tuning or other substantial modifications. If you fine-tune GPT-4, Claude, Llama, or any other GPAI model, you are a downstream modifier. You inherit compliance obligations that extend beyond the base model provider's responsibilities. You must maintain technical documentation, conduct risk assessments, implement governance measures, and, in many cases, comply with the GPAI Code of Practice. These are not optional best practices. They are enforceable legal requirements with penalties up to 15 million euros or three percent of global annual revenue, whichever is higher.

## The Downstream Modifier Definition

The EU AI Act defines a **downstream modifier** as any entity that makes a substantial modification to a GPAI model that has already been placed on the market. A substantial modification is one that changes the model's intended purpose, significantly alters its performance characteristics, or changes the risk profile of the system. Fine-tuning almost always qualifies as a substantial modification because it changes the model's behavior, even if the architecture remains unchanged.

If you fine-tune a GPAI model, you become the **provider** of the modified AI system for regulatory purposes. This means you assume the provider obligations under the EU AI Act, including risk classification, conformity assessment, technical documentation, quality management, and post-market monitoring. You do not replace the base model provider's obligations—they remain responsible for the base model—but you take on additional obligations for the modifications you made.

The distinction matters because provider obligations are far more extensive than user obligations. A company that uses an unmodified GPAI model via API is typically classified as a deployer, with limited obligations focused on appropriate use and human oversight. A company that fine-tunes that same model becomes a provider, with full technical documentation, risk management, and conformity assessment obligations. Many teams fine-tune models without understanding this regulatory shift.

## GPAI Model Classification

The EU AI Act distinguishes between general-purpose AI models and **GPAI models with systemic risk**. A GPAI model has systemic risk if it meets one of two criteria: it was trained using compute resources exceeding 10 to the power of 25 floating point operations, or it is determined by the European Commission to have high-impact capabilities based on its reach and potential harm.

As of early 2026, all frontier models from OpenAI, Anthropic, Google, and Meta meet the systemic risk threshold. GPT-4, GPT-5.1, Claude Opus 3.5, Claude 4, Gemini 2, and Llama 4.3 all exceed the compute threshold and have been formally classified as systemic risk models by the EU AI Office. If you fine-tune one of these models, your downstream modifier obligations include systemic risk provisions.

Smaller GPAI models—Llama 4.2, Mistral 7B, Phi-3—do not currently meet the systemic risk threshold. Fine-tuning these models still makes you a downstream modifier, but you are not subject to the additional systemic risk obligations. You must still maintain technical documentation and conduct risk assessments, but you are not required to evaluate systemic risks like large-scale manipulation or critical infrastructure vulnerabilities.

The classification can change over time. The European Commission reviews the systemic risk threshold annually and can designate additional models based on capability assessments. A model that is not classified as systemic risk today may be reclassified tomorrow. Your compliance program must monitor EU AI Office announcements and update your risk assessments when classifications change.

## Technical Documentation Requirements

Article 53 of the EU AI Act requires GPAI providers and downstream modifiers to maintain technical documentation sufficient to demonstrate compliance and enable competent authorities to assess conformity. For fine-tuned models, this documentation must include a detailed description of the base model, the training data used for fine-tuning, the training methodology and hyperparameters, the intended purpose and limitations of the modified model, evaluation results demonstrating performance and safety, and risk mitigation measures implemented.

The documentation must be comprehensive enough that a regulator can understand what you built, how you built it, what data you used, what risks you assessed, and what mitigations you applied. This is not a one-page summary. The EU AI Act expects documentation comparable to medical device technical files—structured, detailed, evidence-based documentation that can withstand regulatory scrutiny.

Your technical documentation must include a **model card** describing the modified model's capabilities, limitations, intended use cases, and known failure modes. The model card format is not prescribed by the regulation, but the content requirements align with the model card frameworks developed by Google, Hugging Face, and the Partnership on AI. You must describe the base model and version, the fine-tuning objective, the training data sources and characteristics, the evaluation benchmarks and results, the known biases and limitations, and the recommended usage and monitoring practices.

You must also include a **data documentation** section describing the training data in sufficient detail to enable assessment of data quality, representativeness, and compliance with data protection law. This includes data sources, collection methods, date ranges, geographic scope, consent basis or other legal grounds for processing, data cleaning and filtering applied, PII handling, and demographic or domain coverage. If you used synthetic data or augmentation, you must document the generation methodology and validation process.

The documentation must include a **training methodology** section describing the fine-tuning approach, framework and library versions, hyperparameters, compute resources, training duration, convergence criteria, and any custom training logic or loss functions. This section must be detailed enough to enable reproducibility, at least in principle, though you are not required to share proprietary training code.

## Risk Assessment and Mitigation

You must conduct a risk assessment for the fine-tuned model that addresses the specific risks introduced or modified by fine-tuning. This assessment must evaluate whether fine-tuning changes the risk classification of the system, whether new failure modes are introduced, whether existing safeguards in the base model are preserved or degraded, and whether the fine-tuned model introduces domain-specific risks not present in the base model.

For example, if you fine-tune a general-purpose language model for medical diagnosis, you introduce a high-risk use case that may not have been present in the base model. Your risk assessment must address medical accuracy, diagnostic reliability, bias in diagnosis across patient populations, and the consequences of incorrect diagnoses. You must implement risk mitigations specific to the medical domain, which may include clinical validation, human-in-the-loop requirements, and post-market surveillance.

If you fine-tune a model on domain-specific data—legal contracts, financial reports, customer service transcripts—you must assess whether the training data introduces biases or failure modes not present in the base model. A model fine-tuned on contracts from a single jurisdiction may perform poorly on contracts from other jurisdictions. A model fine-tuned on customer service data from one demographic group may exhibit bias when serving other groups. Your risk assessment must identify these limitations and your documentation must disclose them.

For GPAI models with systemic risk, you must also assess whether fine-tuning affects systemic risk characteristics. This includes evaluating whether the fine-tuned model could be used for large-scale disinformation, whether it could be misused for cyberattacks or development of harmful content, whether it exhibits emergent capabilities not present in the base model, and whether safeguards against misuse are preserved. Most fine-tuning for narrow enterprise use cases does not introduce systemic risks, but you must document the assessment.

## GPAI Code of Practice Compliance

The EU AI Act requires providers of GPAI models with systemic risk to comply with a **Code of Practice** developed by industry and approved by the European Commission. The Code of Practice, finalized in May 2025, establishes standards for risk management, model evaluation, adversarial testing, incident reporting, and cybersecurity. Downstream modifiers are not automatically subject to the full Code of Practice, but you must comply with provisions relevant to your modifications.

If your fine-tuning changes the risk profile of the model, you must conduct evaluations aligned with the Code of Practice evaluation framework. This includes evaluating model robustness, fairness across protected groups, resistance to adversarial inputs, and alignment with intended use constraints. The Code of Practice specifies evaluation methodologies and benchmark suites for common risk categories, and you must document which evaluations you conducted and the results.

If you fine-tune a systemic risk model for a high-risk use case—hiring, credit scoring, law enforcement, critical infrastructure—you must implement the Code of Practice provisions for high-risk systems, including human oversight, logging and traceability, accuracy and robustness testing, and bias mitigation. These provisions are more stringent than baseline GPAI requirements and may require third-party conformity assessment.

The Code of Practice also requires incident reporting for systemic risk models. If your fine-tuned model experiences a serious incident—unauthorized access, significant harm to individuals, widespread incorrect outputs affecting critical decisions—you must report it to the EU AI Office within 72 hours if it meets the materiality threshold. This requirement applies even if the base model provider also reports the incident, as your modifications may have contributed to the failure.

## Conformity Assessment Pathways

Depending on the risk classification of your AI system, you may be required to undergo **conformity assessment** before deploying the fine-tuned model. High-risk AI systems under Annex III of the EU AI Act—systems used for hiring, credit scoring, law enforcement, critical infrastructure, education assessment—require third-party conformity assessment by a notified body. If your fine-tuned model is deployed in a high-risk context, you must engage a notified body to assess conformity with EU AI Act requirements.

For systems that are not high-risk, you may conduct conformity assessment internally, but you must document the assessment and maintain evidence that the system meets applicable requirements. This includes demonstrating data governance compliance, conducting required evaluations, implementing risk mitigations, and establishing post-market monitoring.

The conformity assessment process is not a one-time event. You must reassess conformity whenever you make substantial modifications to the model, which includes retraining on new data or updating the base model. If you retrain your fine-tuned model quarterly, you must conduct a conformity assessment quarterly. This requirement creates a significant compliance burden for teams that iterate frequently.

Some teams address this by implementing **continuous conformity assessment**—automated evaluation pipelines that run the required conformity tests on every model version and generate conformity documentation automatically. This approach requires significant upfront investment but reduces the marginal cost of conformity assessment for each new model version.

## Data Governance Obligations

The EU AI Act imposes data governance requirements on GPAI providers that extend to downstream modifiers. You must implement practices to ensure training data is relevant, representative, and free of errors and biases to the extent possible. You must document data provenance, consent or legal basis for processing, and any limitations or known biases in the data.

If your training data includes personal data, you must comply with GDPR, including lawful basis for processing, data minimization, purpose limitation, and data subject rights. The combination of EU AI Act and GDPR creates a high bar for training data governance. You cannot simply scrape data from internal systems and use it for fine-tuning. You must verify that you have a lawful basis for processing, that the data is necessary for the training objective, and that data subjects have been informed or consent obtained where required.

For fine-tuning on customer data, you must review your customer contracts to verify you have the right to use the data for model training. Many SaaS terms of service explicitly prohibit using customer data for model training without separate consent. If you fine-tune on customer data without contractual permission, you expose yourself to both contract breach claims and regulatory enforcement.

The EU AI Act also requires implementing measures to detect and mitigate biases in training data. This includes evaluating demographic representation, testing for proxy discrimination, and documenting known limitations. If your training data overrepresents certain groups or underrepresents others, you must disclose this limitation and implement mitigations, which may include data augmentation, re-weighting, or use-case restrictions.

## Transparency and Disclosure

You must provide transparency to downstream users of your fine-tuned model. If other organizations will deploy your model, you must provide them with sufficient information to understand its capabilities, limitations, and appropriate use. This includes disclosing the base model and version, the training data characteristics and limitations, the evaluation results and benchmarks, the intended use cases and restrictions, and the monitoring and support you provide.

If you offer your fine-tuned model as a commercial product or service, you must include this information in the product documentation and make it accessible to customers. If you deploy the fine-tuned model internally, you must provide this information to the business units that will use the model, ensuring they understand its limitations and appropriate use constraints.

For GPAI models with systemic risk, you must also publish a **summary of the technical documentation** in a publicly accessible format. This summary does not need to disclose proprietary details, but it must include the model's intended purpose, capabilities, limitations, and risk mitigations. This transparency requirement is intended to enable public scrutiny and academic research on systemic risk models.

## Penalties and Enforcement

Non-compliance with EU AI Act obligations for GPAI models carries penalties up to 15 million euros or three percent of global annual revenue, whichever is higher. Enforcement is conducted by national competent authorities in each EU member state, coordinated by the European AI Office. As of early 2026, enforcement activity is ramping up, with initial focus on high-risk systems and systemic risk models.

The enforcement approach follows the GDPR model: regulators prioritize investigations based on complaints, media reports, and proactive audits of high-impact systems. If you deploy a fine-tuned model in a high-risk context, serve a large user base, or experience a public incident, you are more likely to face regulatory scrutiny. Small-scale internal deployments are less likely to trigger investigation, but the legal obligations apply regardless of deployment scale.

Regulators have indicated that ignorance of the law is not a defense. If you fine-tune a GPAI model in or for the EU market, you are expected to understand your downstream modifier obligations and comply with them. The regulatory expectation is that organizations deploying AI systems have in-house or external legal expertise to assess compliance requirements.

## Building a Compliance Program

Compliance with EU AI Act downstream modifier obligations requires a structured compliance program. You need a **legal and regulatory monitoring function** to track regulatory developments, guidance from the EU AI Office, and enforcement actions. The regulatory landscape is evolving rapidly, and you must stay current.

You need a **risk classification process** that evaluates each fine-tuning project to determine the risk level, applicable obligations, and required conformity assessment pathway. This process must run before training begins, not after deployment.

You need a **technical documentation system** that captures the required documentation elements for each fine-tuned model and maintains that documentation for the model's lifecycle plus the regulatory retention period, typically seven years. This system must integrate with your model lineage tracking and model registry.

You need an **evaluation and testing program** that implements the required evaluations for your risk category and use case, documents the results, and enforces quality gates before deployment. This program must align with the GPAI Code of Practice where applicable.

You need a **data governance framework** that ensures training data compliance with GDPR and EU AI Act requirements, including legal basis verification, bias assessment, and consent management where required.

You need an **incident response plan** that defines serious incidents, establishes reporting procedures, and ensures timely notification to regulators where required.

Most organizations integrate EU AI Act compliance into their existing model governance framework rather than building a separate compliance program. The model review board evaluates regulatory compliance alongside technical and business considerations. The model registry enforces that required documentation is present before deployment. The evaluation pipeline includes regulatory-required tests alongside business metrics.

The EU AI Act is not a distant future concern. It is in force now, with enforcement activity increasing. If you fine-tune GPAI models and operate in or serve the EU market, you are a downstream modifier with explicit legal obligations. Understanding those obligations, building the systems to meet them, and maintaining evidence of compliance is not optional. It is the cost of operating in the most consequential regulatory jurisdiction for AI systems.

In the next subchapter, we turn to reproducibility requirements—the technical and operational practices needed to ensure fine-tuning can be reliably reproduced for compliance, debugging, and iteration.
