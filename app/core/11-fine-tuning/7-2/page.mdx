# 7.2 â€” Pre-Training Baseline: Capturing Base Model Performance Before You Touch It

**The baseline is not optional.** You cannot evaluate whether fine-tuning improved your model without measuring what the model could do before you modified it. This principle is so fundamental that stating it feels condescending, yet teams violate it constantly. They fine-tune, they see their target metric improve, they deploy, and then they discover that latency increased, safety degraded, robustness collapsed, or edge case performance regressed. When users complain, the team has no data to validate the complaints because they never measured those dimensions on the base model. They must reconstruct baseline measurements retrospectively, under pressure, often during a production incident. The work they skipped to save time in the beginning costs ten times more when done in crisis mode. The pre-training baseline is the control condition in the experiment you are running when you fine-tune. Without it, you have no experiment. You have only hope.

This failure illustrates the foundational principle of fine-tuning evaluation: you cannot evaluate a fine-tuned model's performance without a rigorous baseline of the base model's performance measured before you modify it. The baseline is not a courtesy. It is the control condition in the experiment you are running. Without it, you have no way to distinguish improvement from regression, no way to measure the cost of your gains, and no way to make informed decisions about whether fine-tuning is worth the risk.

## What a Baseline Captures

A **pre-training baseline** is a comprehensive measurement of the base model's performance across all dimensions you care about, captured before you begin fine-tuning and preserved as the reference point for all subsequent evaluation. It is not a single metric. It is a structured dataset of measurements spanning task performance, safety behavior, computational cost, robustness, and any other property relevant to your deployment context. The baseline serves three purposes: it establishes what the base model can do, it reveals what you might lose through fine-tuning, and it provides the comparison data you need to make deployment decisions.

The baseline must measure task performance on the specific use case you are fine-tuning for, but it cannot stop there. You must also measure performance on adjacent tasks, edge cases, and scenarios not well-represented in your fine-tuning data. If you are fine-tuning for contract clause classification, your baseline must include classification accuracy on your primary taxonomy, but it must also measure performance on rare clause types, performance on contracts from industries not in your training set, performance on malformed or ambiguous inputs, and performance on tasks like clause summarization and explanation generation that you are not explicitly fine-tuning for but that users depend on.

The baseline must measure safety and alignment properties. This includes the model's refusal behavior when asked to perform tasks outside its safe operating domain, its tendency to generate biased or inappropriate content, its adherence to factual accuracy norms, and its compliance with any regulatory or ethical guidelines relevant to your application. These properties are not preserved by fine-tuning. They must be measured in the base model so you can detect when fine-tuning degrades them.

The baseline must measure computational cost. This includes inference latency, token usage, memory footprint, and throughput under realistic load conditions. Fine-tuning can affect these metrics in unpredictable ways. Some fine-tuned models become faster because they learn to generate shorter outputs. Some become slower because they learn more complex reasoning patterns. You will not know unless you measure the base model first.

The baseline must measure robustness. This includes performance under input variation, consistency across paraphrased prompts, stability across different sampling temperatures, and resilience to adversarial perturbations. Fine-tuning often reduces robustness because the model overfits to the stylistic and structural patterns in your training data. Measuring base model robustness gives you a reference point for detecting this degradation.

## Why Teams Skip the Baseline

Teams skip the pre-training baseline for three reasons, all of them wrong. The first is urgency. Leadership wants the fine-tuned model deployed quickly, and baseline measurement feels like a delay. The team reasons that they can start fine-tuning immediately and compare models later if problems arise. This is false economy. Skipping the baseline does not save time. It defers time, and the deferred work becomes far more expensive when you must reconstruct baseline measurements retrospectively, often under the pressure of a production incident or user revolt.

The second reason is false simplicity. The team believes the fine-tuning objective is narrow and well-defined, and that measuring performance on that objective is sufficient. If the goal is to improve clause classification accuracy, why measure explanation quality or latency or robustness to input variation? The answer is that fine-tuning changes the entire model, not just the parts related to your objective. You are not adding a feature. You are reshaping a probability distribution. The effects are global, and you must measure globally to understand what you are doing.

The third reason is overconfidence in the base model's published benchmarks. The team assumes that because OpenAI or Anthropic has published safety evaluations, bias audits, and capability benchmarks, they do not need to measure those properties themselves. This assumption fails because published benchmarks measure general model behavior, not behavior on your specific task, in your specific domain, with your specific prompt structure. A base model that scores well on generic bias benchmarks may still exhibit biases in your application context. A model that passes safety evaluations on standard adversarial prompts may still fail on adversarial prompts constructed from your domain-specific terminology. You must measure the base model on your data, in your context, with your evaluation criteria.

## The Baseline Measurement Process

Constructing a baseline begins with defining the metrics you will measure. Start with task performance metrics directly related to your fine-tuning objective. For a classification task, this includes precision, recall, F1 score, and accuracy across all relevant categories. For a generation task, this includes metrics like BLEU, ROUGE, or human ratings of quality, relevance, and coherence. For a question-answering task, this includes exact match, partial match, and semantic similarity to reference answers. These are the primary metrics you are trying to improve, and you must measure them on a representative test set drawn from the same distribution as your intended deployment.

Next, define regression metrics for capabilities you need to preserve. These are tasks the model currently performs well that are not the focus of your fine-tuning effort but that users depend on. If you are fine-tuning for contract classification but users also rely on the model for summarization, explanation, and comparison tasks, you must measure base model performance on those tasks. If users interact with the model in multiple languages, you must measure performance in all supported languages, not just the language most represented in your fine-tuning data.

Next, define safety metrics. These include refusal rate on inappropriate requests, bias metrics across demographic groups, factual accuracy on verifiable claims, and compliance with domain-specific guidelines. For safety metrics, you need both quantitative measurements and qualitative review. Quantitative metrics tell you whether the model refuses requests at the expected rate and whether performance is balanced across groups. Qualitative review tells you whether the refusals are appropriate, whether the outputs respect ethical norms, and whether the model exhibits failure modes not captured by your quantitative tests.

Next, define robustness metrics. These include consistency across paraphrased inputs, stability across different prompt templates, performance across different sampling temperatures, and resilience to minor perturbations in input formatting or content. Robustness metrics require constructing test sets specifically designed to probe these dimensions. You cannot measure robustness by running the same test set you use for task performance. You need variations, adversarial examples, and edge cases.

Finally, define operational metrics. These include inference latency, token usage, memory footprint, and throughput. Measure these under realistic load conditions, not just on single isolated requests. If your production deployment will handle 500 requests per minute at peak load, your baseline measurements must include latency and throughput at that scale.

## Collecting Baseline Data

Once you have defined your metrics, you collect baseline data by running the base model on test sets designed to measure each metric. This is not a trivial exercise. You need test sets for every metric, and those test sets must be large enough to produce statistically reliable measurements. For task performance metrics, you need hundreds to thousands of examples per category or scenario. For safety metrics, you need adversarial test sets designed by people who understand your domain and can construct realistic harmful requests. For robustness metrics, you need systematic variations of core test cases.

Collecting baseline data requires running inference on the base model at scale. For a comprehensive baseline covering task performance, regression, safety, robustness, and operational metrics across thousands of test cases, you may spend hundreds to thousands of dollars in API costs. This is not waste. It is the cost of understanding the system you are about to modify. Teams that balk at spending a few thousand dollars on baseline measurement routinely spend tens or hundreds of thousands of dollars on fine-tuning experiments that fail because they did not understand what they were starting from.

You must version and preserve the baseline data. This means storing the exact model version you tested, the exact prompt templates you used, the exact test sets you ran, and the exact outputs the model produced. You will need this data later to compare against fine-tuned model outputs, to debug regressions, and to validate that changes in performance are due to fine-tuning and not due to changes in test data or evaluation methodology. Treat the baseline as a permanent artifact, stored in version control, documented thoroughly, and accessible to everyone involved in the fine-tuning effort.

## Baseline as a Deployment Gate

The baseline is not just a measurement. It is a decision-making tool. Once you have baseline data, you can define acceptance criteria for your fine-tuned model. These criteria specify the minimum level of performance the fine-tuned model must achieve on each metric to be considered for deployment. For your primary task, the acceptance criteria might require that the fine-tuned model exceed the base model by a minimum margin, such as 10% relative improvement in F1 score. For regression metrics, the acceptance criteria typically require that the fine-tuned model match or exceed the base model, tolerating at most a small degradation such as 2% relative decline. For safety metrics, the acceptance criteria should require that the fine-tuned model match the base model with zero tolerance for degradation.

Acceptance criteria must be defined before you begin fine-tuning, not after. If you wait until after fine-tuning to decide what constitutes acceptable performance, you will be tempted to rationalize away regressions, to adjust criteria to match what the model achieved, and to deploy models that do not meet your actual requirements. Pre-committing to acceptance criteria based on baseline data removes this temptation and ensures that deployment decisions are driven by evidence rather than pressure.

The contract analysis company that skipped the baseline had no acceptance criteria. They deployed the fine-tuned model because it improved classification accuracy and assumed that was sufficient. When users reported regressions in latency and explanation quality, the team had no data to evaluate whether those regressions were acceptable trade-offs or deployment-blocking failures. They made the decision emotionally rather than analytically, and the result was a rollback, a loss of user trust, and a six-month project delay.

## Baseline Evolution and Maintenance

The baseline is not static. As base models evolve, as your test data grows, and as your deployment context changes, you must update the baseline to reflect current reality. When OpenAI releases GPT-5.1 or Anthropic releases Claude 4, you must re-run your baseline measurements on the new model before you begin fine-tuning it. You cannot assume that a baseline measured on GPT-5 in 2024 is valid for GPT-5.1 in 2026. Model capabilities change, safety properties change, and performance characteristics change. Your baseline must track those changes.

You should also update the baseline when you expand your test coverage. If you initially measured baseline performance on contract clause classification and later decide to measure summarization quality, you must add summarization measurements to the baseline before you fine-tune. The baseline must always reflect the complete set of metrics you care about, not just the subset you initially prioritized.

Maintaining the baseline requires discipline. It requires treating baseline measurement as part of your fine-tuning workflow, not as a one-time setup task. It requires versioning baseline data alongside model versions, test set versions, and evaluation code. It requires documentation that explains what was measured, how it was measured, and why those measurements matter. Teams that treat the baseline as an afterthought pay for that choice in incident response, user complaints, and failed deployments.

The baseline is the foundation of rigorous fine-tuning evaluation. Without it, you are flying blind. With it, you have the reference point you need to measure improvement, detect regression, and make informed deployment decisions. The next step is building the evaluation suite that will compare your fine-tuned model against that baseline across every dimension that matters.
