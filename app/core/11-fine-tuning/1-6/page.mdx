# 1.6 — The Minimum Viable Fine-Tune: Smallest Experiment That Proves Value

In late 2025, a healthcare analytics company spent six weeks collecting and cleaning 12,000 examples of clinical notes to fine-tune a model for structured diagnosis extraction. They hired two medical coders to review the training data, set up fine-tuning infrastructure, ran hyperparameter sweeps, and conducted three rounds of evaluation. When they finally deployed the fine-tuned model, the accuracy was 4 percentage points higher than the prompted baseline — a marginal improvement that did not justify the six-week effort and 63,000 dollars in costs. The retrospective revealed the core mistake: they had never run a small-scale experiment to validate that fine-tuning would provide meaningful value before committing to the full project. If they had fine-tuned on just 300 examples and evaluated performance after 48 hours, they would have discovered that fine-tuning provided only marginal gains and abandoned the approach before investing weeks of effort. The diagnosis was clear: they had skipped the minimum viable experiment that would have prevented a costly false start.

Fine-tuning is expensive in time, money, and infrastructure complexity. The only way to justify that expense is to prove that fine-tuning will deliver meaningful value before committing to a full-scale project. The minimum viable fine-tune is a small, focused experiment designed to answer one question: does fine-tuning improve performance enough to justify the cost? This experiment must be fast — completed in 48 hours or less — use a small dataset of 200 to 500 examples, evaluate on clear metrics, and produce a conclusive result. If the experiment shows strong gains, you proceed to full-scale fine-tuning. If it shows marginal or no gains, you stop immediately and explore alternative approaches. The MVP fine-tune is the cheapest insurance policy against wasting months on a technique that will not work for your task.

## The Core Concept: Prove Value Before Scale

The minimum viable fine-tune is not about building a production-ready model. It is about validating the hypothesis that fine-tuning will provide meaningful improvement over your prompted baseline. The experiment must be small enough to execute quickly but representative enough to produce trustworthy results. This means selecting a subset of your data that covers the task's core variation, fine-tuning a small model or using a limited number of training steps, evaluating on a held-out test set that represents real-world conditions, and comparing results directly to your best prompted baseline.

The key is to design the experiment so that a positive result is decisive and a negative result is equally decisive. If fine-tuning on 300 examples improves accuracy by 15 percentage points over the prompted baseline, that is strong evidence that full-scale fine-tuning will deliver value. If fine-tuning on 300 examples improves accuracy by 2 percentage points, that is strong evidence that fine-tuning is not the right approach for this task. The experiment must be structured to produce one of these two outcomes, not an ambiguous middle ground where you cannot make a decision.

This requires discipline. Many teams treat the first fine-tuning attempt as exploratory and accept ambiguous results because they assume they can improve with more data, better hyperparameters, or additional iterations. This leads to scope creep where the small experiment becomes a multi-week project with unclear stopping criteria. The MVP fine-tune must have a binary outcome: proceed or stop. If the result is ambiguous, treat it as a stop signal and investigate why the experiment was inconclusive before running additional iterations.

The healthcare analytics company in the opening story lacked this discipline. When their first fine-tuning attempt with 2,000 examples showed only a 3 percentage point improvement, they assumed they needed more data. They collected another 4,000 examples and fine-tuned again, achieving a 4 percentage point improvement. Still not satisfied, they ran hyperparameter sweeps and tried different model sizes, eventually reaching a 5 percentage point improvement after six weeks of work. At no point did they step back and ask whether a 5 percentage point improvement justified the cost. If they had run a 48-hour MVP experiment first, they would have seen the 3 percentage point result, recognized it as marginal, and stopped before investing six weeks.

The correct mindset for an MVP fine-tune is brutal pragmatism. You are not exploring possibilities or learning about your data. You are answering one specific question: does fine-tuning provide enough value to justify the cost? The experiment is designed to answer that question as quickly and cheaply as possible. If the answer is yes, you proceed to full-scale fine-tuning with confidence. If the answer is no, you stop immediately and explore alternative approaches. There is no middle ground.

## Selecting the 200 to 500 Training Examples

The first step is selecting a small subset of training data that represents the task's core variation. This is not a random sample — it is a deliberately constructed dataset that covers the most common scenarios, the most important edge cases, and the full range of output formats or categories you care about. If your full dataset has 10,000 examples across 20 categories, your MVP dataset should include examples from all 20 categories, weighted toward the most frequent and most critical categories.

A fintech company needed to classify customer support messages into 18 categories. Their full dataset had 8,000 labeled messages, but category distribution was highly imbalanced — 60% of messages fell into three categories, and five categories had fewer than 50 examples each. For the MVP fine-tune, they selected 400 examples: 200 from the three most common categories, 100 from the next five categories, and 100 from the remaining ten categories, deliberately oversampling rare but critical categories like fraud reports and regulatory inquiries. This ensured the model would see all category types during training, even though the distribution did not match production traffic.

The selection process must also cover input variation. If your task involves different input lengths, writing styles, languages, or formatting conventions, the MVP dataset should include examples of each. If your task has seasonal variation, customer segments, or regional differences, include examples from each segment. The goal is not statistical representativeness — it is coverage of the variation the model will encounter in production. A 300-example dataset that covers all task variation is far more valuable than a 1,000-example dataset that only covers common cases.

For structured extraction tasks, ensure your MVP dataset includes examples of the simplest valid outputs, the most complex nested structures, edge cases with missing or ambiguous fields, and any special formatting rules. For style transfer tasks, include examples of the full range of tones, registers, and content types you need the model to handle. For classification tasks, include examples of clear-cut cases and ambiguous boundary cases where even human labelers disagree. The MVP dataset is not a random sample — it is a curated stress test.

The fintech company's selection strategy paid off when they analyzed results. The fine-tuned model showed a 12 percentage point improvement on common categories and a 22 percentage point improvement on rare categories. The deliberate oversampling of rare categories meant the model had learned to recognize fraud reports and regulatory inquiries despite their low frequency in production data. If they had used a random sample, rare categories would have been underrepresented and the model would have learned to ignore them. The curated approach ensured comprehensive coverage.

A medical device company selecting data for a diagnostic classification task took a different approach. They had 12,000 labeled cases but knew that diagnostic accuracy varied by patient age, symptom severity, and comorbidity complexity. They created a sampling matrix: young patients with simple cases, young patients with complex cases, elderly patients with simple cases, elderly patients with complex cases, plus edge cases like pregnant patients and immunocompromised patients. They selected 50 examples from each cell of the matrix, ensuring their 400-example MVP dataset covered the full variation space even though some cells were rare in production. This matrix-based sampling revealed that fine-tuning provided large gains for complex cases but minimal gains for simple cases, which helped them decide that fine-tuning was worth pursuing because complex cases were the ones where human expert time was most expensive.

The selection process should also deliberately include failure modes you care about. If your task involves detecting safety issues, include examples of safety issues that human reviewers initially missed. If your task involves content moderation, include examples that triggered false positives in your previous system. If your task involves structured extraction, include examples with unusual formatting or missing fields. These difficult cases are the ones that determine whether fine-tuning will solve your actual problems rather than just performing well on easy examples.

## Establishing the Prompted Baseline

The prompted baseline is the single most important comparison point for the MVP fine-tune. This is the best performance you can achieve with the base model using a well-engineered prompt, few-shot examples, and any prompt optimization techniques you have available. If you do not establish a strong baseline, you cannot know whether fine-tuning is actually improving performance or just matching what good prompting would achieve.

Many teams underinvest in the baseline. They write a simple prompt, evaluate it once, and use that as the comparison point. This guarantees that fine-tuning will look better than it actually is, because they are comparing a carefully optimized fine-tuned model to a poorly optimized prompted model. The correct approach is to spend at least half of your MVP experiment time optimizing the prompt. Try multiple phrasings, add few-shot examples, experiment with structured output instructions, test different models, and iterate until you are confident you have the best prompted performance achievable.

A legal technology company spent two days optimizing their prompt for contract clause extraction before running their MVP fine-tune. They tested five different prompt structures, experimented with 3-shot, 5-shot, and 10-shot examples, tried both GPT-4.5 and Claude Opus 4, and evaluated each variant on a 100-example validation set. Their best prompted baseline achieved 72% exact match accuracy on clause extraction. They then fine-tuned on 400 examples and achieved 88% exact match accuracy — a 16 percentage point gain that clearly justified proceeding with full-scale fine-tuning. If they had used their initial prompt without optimization, which achieved only 58% accuracy, the fine-tuned model's 88% accuracy would have looked like a 30 percentage point gain, dramatically overstating the value of fine-tuning.

The baseline must be evaluated on the same test set you will use for the fine-tuned model, using the same metrics, under the same conditions. If you are evaluating the fine-tuned model on 200 held-out examples, evaluate the prompted baseline on the same 200 examples. If you are measuring precision, recall, exact match, and F1 score, measure all four for both models. The comparison must be apples-to-apples or the results are meaningless.

The legal technology company's prompt optimization revealed several insights. First, they discovered that structured output instructions — explicitly requesting output in a specific format with labeled fields — improved accuracy by 8 percentage points over freeform responses. Second, they found that 5-shot examples were optimal: fewer examples left the model uncertain about edge cases, more examples consumed token context without additional benefit. Third, they discovered that GPT-4.5 outperformed Claude Opus 4 for this specific task by 4 percentage points, even though Claude was better for other tasks they had tested. These insights were valuable independent of the fine-tuning decision and would have been missed if they had skipped baseline optimization.

A content moderation platform took baseline optimization further. They not only optimized prompts but also experimented with chain-of-thought prompting, where they asked the model to explain its reasoning before making a classification decision. This increased accuracy by 6 percentage points over direct classification and reduced false positive rate by 11 percentage points. When they compared fine-tuning to this optimized chain-of-thought baseline, the gain was only 5 percentage points — marginal. If they had compared to a simple prompted baseline without chain-of-thought, fine-tuning would have shown a 16 percentage point gain and they would have proceeded with a costly fine-tuning project when an improved prompt would have delivered 70% of the benefit at 5% of the cost.

The discipline to establish a strong baseline is critical because it forces you to understand what the base model can already do. Many tasks that teams assume require fine-tuning can actually be solved with good prompting, and you only discover this by investing effort in prompt optimization. The baseline is not a formality — it is half of the experiment.

## Choosing Metrics That Answer the Decision Question

The MVP fine-tune must evaluate metrics that directly answer the question: does this improve performance enough to justify the cost? This means selecting metrics that reflect real-world impact, not just technical performance. Accuracy is a starting point, but it is rarely sufficient. You need to measure the specific dimensions of quality that matter for your task: precision and recall for classification, exact match for structured extraction, human preference ratings for style transfer, latency for real-time applications, cost per inference if you are optimizing for budget.

A customer service platform cared most about reducing the rate of incorrect escalations — cases where the model classified a routine inquiry as urgent and routed it to a senior agent. Their prompted baseline had a 12% false positive rate on urgent escalations. The MVP fine-tune needed to reduce that rate to under 5% to justify deployment. They evaluated the fine-tuned model specifically on false positive rate, not overall accuracy, because that was the metric that determined business value. The fine-tuned model achieved a 3% false positive rate — a clear win that justified proceeding.

For tasks with multiple quality dimensions, define a composite metric or a threshold on each dimension. A structured extraction task might require 90% recall to ensure no critical data is missed, 95% precision to minimize downstream errors, and average latency under 800 milliseconds to meet user experience requirements. The MVP fine-tune must hit all three thresholds to be considered successful. If it hits two but misses one, the result is a stop signal — fine-tuning is not solving the problem you need solved.

You must also evaluate error types, not just error rates. A model that achieves 92% accuracy but makes catastrophic errors on 3% of inputs is worse than a model that achieves 90% accuracy with no catastrophic errors. The MVP evaluation must include qualitative review of errors to identify whether the fine-tuned model is making different types of mistakes than the prompted baseline, and whether those mistakes are better, worse, or simply different.

The customer service platform's focus on false positive rate rather than overall accuracy was the right call because the business impact was asymmetric. False positives cost senior agent time and created customer frustration when routine requests were treated as urgent. False negatives — failing to escalate genuinely urgent requests — were also costly but were caught by downstream processes. The cost structure made false positives the primary metric to optimize. When they evaluated the fine-tuned model, overall accuracy improved by 7 percentage points but false positive rate dropped by 9 percentage points. The false positive improvement drove the decision to proceed.

A healthcare technology company needed to extract diagnosis codes from clinical notes. They cared about three metrics: recall because missing a diagnosis code could affect patient care and billing, precision because incorrect codes triggered claim denials, and format compliance because their billing system rejected improperly formatted codes. Their MVP fine-tune achieved 94% recall, 97% precision, and 98% format compliance, meeting all three thresholds. But qualitative error review revealed a concerning pattern: the fine-tuned model occasionally hallucinated diagnosis codes that were not mentioned in the clinical note, a failure mode the prompted model never exhibited. Even though quantitative metrics looked good, the new error type was unacceptable. They paused the fine-tuning project to investigate why the model was hallucinating and whether additional training data or modified prompts could eliminate this failure mode.

Latency and cost metrics matter for production viability even if quality metrics are strong. A real-time fraud detection system fine-tuned a model that improved detection accuracy by 14 percentage points — a huge win. But the fine-tuned model was 40% slower than the prompted baseline because fine-tuning had increased model size. The latency increase meant they could not process transactions fast enough during peak load. They had to choose between deploying a slower fine-tuned model that would bottleneck at scale, or using the faster prompted baseline with lower accuracy. They eventually fine-tuned a smaller model that balanced accuracy and latency, but this required additional experimentation that delayed deployment by three weeks. If they had measured latency during the MVP experiment, they would have identified this tradeoff earlier.

## The 48-Hour Experiment Cycle

The MVP fine-tune must be fast. If the experiment takes more than 48 hours from start to finish, you are doing too much. The goal is to get a decisive answer quickly so you can either proceed with confidence or stop before investing significant resources. A 48-hour cycle means you can run the experiment on a Friday and have results by Monday, or start on a Monday and have results by Wednesday. This requires pre-work: your training data must be ready, your evaluation set must be prepared, your prompted baseline must be established, and your infrastructure must be set up.

Hour 0 to Hour 4: Select and prepare the 200 to 500 training examples, ensuring coverage of task variation. Format the data according to the fine-tuning API requirements. Upload the dataset and start the fine-tuning job. Most fine-tuning APIs in 2026 complete training on a 500-example dataset in 30 minutes to 2 hours, depending on model size and provider.

Hour 4 to Hour 8: While the fine-tuning job runs, finalize your evaluation set and re-confirm your prompted baseline metrics. Run any last-minute checks on data quality. Prepare evaluation scripts so you can run them immediately when the fine-tuned model is ready.

Hour 8 to Hour 24: The fine-tuning job completes. Deploy the fine-tuned model to a test environment. Run your evaluation scripts on the held-out test set. Collect metrics for accuracy, precision, recall, latency, and any task-specific quality dimensions. Compare directly to the prompted baseline metrics.

Hour 24 to Hour 48: Conduct qualitative review of errors. Sample 50 to 100 cases where the fine-tuned model disagrees with the prompted baseline and manually assess which model produced better output. Calculate the performance delta and determine whether it meets your threshold for proceeding. Document the decision.

If the entire process takes longer than 48 hours, you are over-engineering the experiment. The MVP fine-tune is not about perfection — it is about a fast, directionally correct answer.

The 48-hour constraint forces prioritization. You cannot run extensive hyperparameter sweeps, you cannot try multiple model sizes, you cannot experiment with different data augmentation strategies. You select reasonable defaults — learning rate of 1e-5 or 2e-5, training for 3 to 5 epochs, using the smallest model size that the provider offers for fine-tuning — and accept that these choices might not be optimal. The goal is not to achieve the best possible fine-tuned model. The goal is to determine whether fine-tuning provides meaningful improvement over prompting with reasonable effort.

A financial services company violated this constraint by running hyperparameter sweeps during their MVP experiment. They tested five different learning rates, three epoch counts, and two model sizes, turning a 48-hour experiment into a two-week project. When they finally compared results, the best hyperparameter configuration achieved 13% improvement over baseline while the worst achieved 9% improvement. The hyperparameter sweep consumed two weeks to gain 4 percentage points over using default settings. If they had used defaults, they would have made the proceed decision in 48 hours and been two weeks ahead in the full fine-tuning project.

The evaluation phase must also be streamlined. Write evaluation scripts ahead of time so they can run automatically when the fine-tuned model is ready. Use automated metrics for initial assessment and reserve manual review for error analysis, not scoring every example. If your test set has 500 examples, run automated metrics on all 500 but manually review only the 50 to 100 cases where the fine-tuned model and prompted baseline disagree. This gives you quantitative performance data and qualitative error insights without consuming days on manual evaluation.

## Interpreting Results: When to Proceed and When to Stop

The decision to proceed or stop depends on the magnitude of improvement and the business value of that improvement. As a rough guideline, a 10 percentage point or greater improvement in your primary metric is strong evidence that fine-tuning will deliver value. A 5 to 10 percentage point improvement is marginal and requires careful cost-benefit analysis. Less than 5 percentage points is a stop signal — fine-tuning is not providing meaningful value over prompting.

A healthcare technology company ran an MVP fine-tune for clinical note summarization. Their prompted baseline achieved 68% human preference ratings for summary quality. The fine-tuned model achieved 84% human preference ratings — a 16 percentage point gain. This was decisive. They proceeded to full-scale fine-tuning, collected 3,000 examples, and deployed the model to production. The production model achieved 86% human preference, confirming that the MVP result was predictive of full-scale performance.

In contrast, an e-commerce company ran an MVP fine-tune for product description generation. Their prompted baseline achieved 79% human preference ratings. The fine-tuned model achieved 82% human preference ratings — a 3 percentage point gain. This was marginal. They calculated that the cost of maintaining a fine-tuned model, including retraining as their product catalog evolved, would exceed the value of the small quality improvement. They stopped the fine-tuning project and invested instead in improving their prompt with better examples and clearer instructions, which increased prompted performance to 81%, nearly matching the fine-tuned result at a fraction of the cost.

The threshold for proceeding is not universal — it depends on your task's value and cost structure. If a 5 percentage point improvement saves 500,000 dollars per year in manual review costs, proceed. If a 5 percentage point improvement has no measurable business impact, stop. The MVP fine-tune gives you the data to make this calculation.

The healthcare company's 16 percentage point gain translated directly to reduced physician time spent on documentation review. Their physicians spent an average of 12 minutes reviewing and correcting AI-generated summaries. The 16 percentage point improvement in quality reduced review time to 7 minutes per note, saving 5 minutes per note across 2,000 notes per week. At a physician labor cost of 150 dollars per hour, this saved 25,000 dollars per week or 1.3 million dollars per year. The fine-tuning project cost 80,000 dollars in engineering time and infrastructure, paying for itself in three weeks. The ROI calculation made the proceed decision obvious.

The e-commerce company's 3 percentage point gain had no measurable business impact because their product descriptions were already acceptable at 79% quality. Customers were not abandoning purchases due to description quality, and slightly better descriptions did not increase conversion rates in A/B testing. The fine-tuned model cost 12,000 dollars to develop and would require quarterly retraining as their product catalog evolved, adding 48,000 dollars per year in maintenance costs. Spending 48,000 dollars per year for an improvement that did not affect revenue made no sense. They stopped.

The magnitude threshold also depends on the baseline performance. A 10 percentage point improvement from 40% to 50% accuracy might not be enough if you need 80% accuracy to deploy. A 5 percentage point improvement from 85% to 90% might be critical if 90% is the threshold for replacing human review. Do not evaluate improvement in isolation — evaluate whether the fine-tuned performance meets your deployment requirements.

## Common Mistakes in MVP Design

The most common mistake is using too much data. Teams assume that more data will produce better results, so they use 2,000 or 5,000 examples for the MVP experiment. This defeats the purpose. The MVP is about speed and decisiveness, not perfection. A 500-example experiment that takes 48 hours and shows a 15 percentage point gain is far more valuable than a 5,000-example experiment that takes three weeks and shows a 17 percentage point gain. Use the minimum data needed to cover task variation.

The second most common mistake is skipping the prompted baseline or using a weak baseline. If you do not optimize the prompt, you are not measuring the value of fine-tuning — you are measuring the value of fine-tuning versus a bad prompt. Spend at least as much time optimizing the prompt as you spend on the fine-tune itself.

The third most common mistake is evaluating on the wrong metrics. Teams evaluate overall accuracy when they actually care about false positive rate, or they measure technical performance when they actually care about user satisfaction. Define the metrics that determine business value before you start the experiment, and evaluate only those metrics.

The fourth most common mistake is treating inconclusive results as a reason to iterate rather than stop. If the MVP fine-tune shows a 4 percentage point gain and you cannot decide whether that justifies proceeding, the correct response is to stop and investigate why the gain was small, not to collect more data and try again. Inconclusive results usually mean fine-tuning is not the right approach for the task.

The fifth most common mistake is not doing qualitative error analysis. Quantitative metrics tell you whether performance improved, but qualitative analysis tells you why. If the fine-tuned model improves accuracy by 10 percentage points but introduces new error types that are worse than the baseline errors, the quantitative improvement is misleading. Always review a sample of errors manually.

A sixth mistake is testing on data that is too similar to training data. If your MVP training set includes 400 examples and your test set includes 200 examples, and both are drawn from the same time period, customer segment, or data source, you will get overly optimistic results. The fine-tuned model will appear to generalize well because test data looks like training data. When deployed to production with different data distributions, performance will degrade. Your test set must represent the full variation of production data, including edge cases and distribution shifts that the training set might not cover well.

A seventh mistake is not documenting assumptions and decisions. The MVP fine-tune is fast, which creates pressure to skip documentation. But you need to record which examples were selected for training, what prompted baseline was used, what metrics were measured, and what the decision criteria were. Without documentation, you cannot reproduce the experiment, cannot explain the decision to stakeholders, and cannot learn from the results when you run similar experiments later. Document as you go, not after the fact.

An eighth mistake is running the MVP without stakeholder alignment on decision criteria. If you complete the experiment and show a 12 percentage point improvement but your VP of Product says that is not good enough without having defined what would be good enough beforehand, the experiment was wasted. Before starting the MVP, get explicit agreement from stakeholders on what magnitude of improvement justifies proceeding. Write it down. Use it as the decision threshold.

## When Results Are Conclusive Versus Inconclusive

A conclusive result is one where the decision to proceed or stop is obvious. A 15 percentage point improvement is conclusive — proceed. A 2 percentage point improvement is conclusive — stop. A 10 percentage point improvement with clear error reduction and no new failure modes is conclusive — proceed. A 10 percentage point improvement but the fine-tuned model introduces critical errors that the baseline did not make is conclusive — stop.

An inconclusive result is one where you cannot make a clear decision. The most common cause is insufficient test data. If you evaluate on only 50 examples, the confidence intervals on your metrics are too wide to distinguish real improvement from noise. The fix is to use at least 200 test examples, preferably 500. Another common cause is poorly defined metrics. If you are measuring accuracy but accuracy does not correlate with business value, you cannot make a decision even if accuracy improves. The fix is to define metrics that directly measure business value.

If your result is inconclusive despite using sufficient test data and well-defined metrics, treat it as a stop signal. Inconclusive results usually indicate that fine-tuning provides marginal value, and marginal value does not justify the cost and complexity of maintaining a fine-tuned model.

A logistics company ran an MVP fine-tune for shipment routing optimization. Their prompted baseline achieved 76% accuracy on predicting optimal routes. The fine-tuned model achieved 82% accuracy — a 6 percentage point improvement. This fell in the marginal range where the decision was not obvious. They analyzed further and discovered that the 6 percentage point improvement translated to 4.2% reduction in total shipping costs, saving approximately 320,000 dollars per year. The fine-tuning project would cost 45,000 dollars in development and 15,000 dollars per year in maintenance. With a one-year payback period and ongoing annual savings of 260,000 dollars, the 6 percentage point improvement was worth pursuing despite being in the marginal range. The business value analysis turned an inconclusive technical result into a conclusive business decision.

Conversely, a content recommendation system achieved an 8 percentage point improvement in click-through rate from fine-tuning, which seemed meaningful. But when they analyzed user engagement downstream, the increased clicks did not translate to increased time on site, content consumption, or user retention. Users were clicking more but not finding better content. The fine-tuned model had learned to optimize for clickbait rather than genuine relevance. Despite the 8 percentage point improvement on the primary metric, the result was inconclusive because the metric did not correlate with business value. They stopped the fine-tuning project and redefined their evaluation criteria to measure downstream engagement rather than just click-through rate.

Statistical significance also matters for conclusiveness. If your test set is small or your metric has high variance, you need larger improvements to be confident the result is real. A 10 percentage point improvement with wide confidence intervals might be inconclusive while a 7 percentage point improvement with narrow confidence intervals is conclusive. Calculate confidence intervals on your key metrics and ensure the improvement is statistically significant before making a proceed decision.

## The Decision Gate: Proceed, Stop, or Pivot

At the end of the 48-hour experiment, you have three options. Proceed to full-scale fine-tuning if the improvement is large, clear, and directly addresses a business need. Stop fine-tuning if the improvement is marginal or if the fine-tuned model introduces new problems. Pivot to an alternative approach if fine-tuning does not work but the problem still needs solving — consider retrieval-augmented generation, prompt optimization, model chaining, or human-in-the-loop workflows.

The most important discipline is to make the decision quickly and definitively. Do not let the MVP fine-tune turn into an open-ended research project. Do not run five more iterations trying to squeeze out marginal gains. Do not convince yourself that a 3 percentage point improvement is meaningful when it is not. The MVP fine-tune is a decision gate, not a starting point for endless optimization.

The proceed decision should include a clear plan. How many training examples will you collect for full-scale fine-tuning? What additional data quality checks will you implement? When will you deploy to production? What monitoring will you put in place to detect degradation? Do not proceed without answering these questions. The MVP proves that fine-tuning will work — the plan ensures that full-scale execution will deliver on that promise.

The stop decision should include an alternative path. If fine-tuning does not work, what will you do instead? If prompted performance is close to fine-tuned performance, invest in prompt engineering. If the task requires knowledge the model does not have, build retrieval-augmented generation. If the task requires capabilities beyond what current models can do, define a human-in-the-loop workflow. Stopping fine-tuning is not failing — it is avoiding a costly investment that would not deliver value.

The pivot decision requires diagnosing why fine-tuning underperformed. Did the prompted baseline perform better than expected, making fine-tuning unnecessary? Did the fine-tuned model learn the wrong patterns, suggesting data quality issues? Did the improvement exist but not translate to business value, suggesting the wrong task framing? Understanding why fine-tuning did not work informs what to try next.

A medical imaging company ran an MVP fine-tune for radiology report generation and achieved only a 4 percentage point improvement over their prompted baseline. They stopped the fine-tuning project but analyzed why the gain was small. They discovered that the prompted model already generated accurate reports but radiologists spent time reformatting the output to match their institution's template. The task was not improving report accuracy — it was enforcing a specific format. They pivoted to fine-tuning for format compliance rather than content generation, achieved a 23 percentage point improvement in format adherence, and reduced radiologist editing time by 60%. The pivot was more valuable than the original fine-tuning direction.

The MVP fine-tune is the cheapest way to validate whether fine-tuning will solve your problem. It costs 48 hours and a few hundred dollars in API fees. The alternative — committing to full-scale fine-tuning without validation — costs weeks and tens of thousands of dollars, and often delivers nothing. Every fine-tuning project must start with an MVP experiment. No exceptions.

The next step is understanding how to collect and structure the full training dataset when the MVP proves that fine-tuning will work — the data quality and diversity requirements that determine whether full-scale fine-tuning succeeds or fails.
