# 2.4 — Dataset Size Requirements by Task Type and Model Size

In early 2025, a financial services company attempted to fine-tune GPT-4o for earnings call summarization using 38 training examples. The team had carefully selected high-quality examples from recent quarterly calls, each one reviewed by senior analysts for accuracy and completeness. They ran the training job, validated on a held-out set of five examples, saw perfect performance, and deployed the model. In production, it worked flawlessly for three weeks, handling calls from the same companies with the same reporting structures. Then it summarized a call from a newly public technology company with a different earnings structure. The model hallucinated revenue figures, invented forward guidance that was never mentioned, and confidently reported a stock buyback program that did not exist. The summary went into a client-facing report before anyone caught the errors.

The problem was not example quality. It was dataset size. Thirty-eight examples were enough to teach the model the specific patterns in the training data but not enough to teach general summarization behavior that transferred to new companies and structures. The model memorized the training examples rather than learning the underlying task. When it encountered earnings calls outside the narrow distribution it had seen, it failed catastrophically. The team rebuilt the dataset with 1,200 examples covering diverse company types, industries, and reporting structures, retrained, and finally achieved robust performance. Thirty-eight examples looked like enough because validation seemed to work. But validation on five examples from the same distribution as training proves nothing about generalization.

How much data you need depends on what you are trying to teach and how large the model is. Dataset size requirements vary by three orders of magnitude across different task types. Classification needs hundreds of examples. Style transfer needs thousands. Domain adaptation needs tens of thousands. Model size affects these requirements because larger models have more parameters to update and require more data to shift their behavior without overfitting to training noise. This subchapter covers the minimum viable dataset sizes for common task types, how model size changes data requirements, when more data helps versus when it does not, and the quality-quantity tradeoff that defines successful fine-tuning in 2026.

## Minimum Dataset Sizes by Task Type

Task type is the primary determinant of dataset size requirements. The more complex the behavior you are teaching, the more examples you need to teach it reliably. Tasks exist on a spectrum from simple classification to complex generative behavior, and data requirements scale accordingly.

**Classification tasks** are the most data-efficient category for fine-tuning. You are teaching the model to map inputs to a small set of discrete categories. Sentiment classification, topic categorization, content moderation decisions, urgency triage, intent detection. These tasks typically achieve strong performance with 300 to 1,000 training examples per class, assuming balanced data. A sentiment classifier with three classes—positive, neutral, negative—needs roughly 900 to 3,000 total examples to learn reliable decision boundaries.

The reason classification is data-efficient is that the output space is small and discrete. The model is not learning to generate novel text; it is learning to activate one of a few output tokens corresponding to class labels. This is a simpler learning objective than open-ended generation, and it requires fewer examples to achieve low loss. In practice, many teams achieve production-quality classification performance with datasets in the low hundreds per class, especially when fine-tuning models that already understand the domain language.

**Extraction tasks** require more data than classification because the output space is larger and more variable. Named entity extraction, clause extraction from contracts, data field extraction from documents, information retrieval from long text. These tasks typically require 1,000 to 5,000 examples to learn reliable extraction patterns, depending on how many entity types or field types you are extracting and how much variability exists in input formats.

Extraction is harder than classification because the model must learn both what to extract and where to find it in variable input structures. A contract clause extractor must recognize indemnification clauses across different drafting styles, contract structures, and legal terminology variations. This requires more examples than simply deciding whether a document is a contract or not. Teams working on extraction tasks in 2026 typically budget for at least 2,000 examples and often need 5,000 or more for complex extraction across diverse input formats.

**Rewriting and style transfer tasks** require thousands of examples because you are teaching the model to preserve meaning while changing surface form. Tone adjustment, formality shifts, simplification, expansion, paraphrasing. These tasks generally need 3,000 to 10,000 examples to learn consistent style transformation without losing semantic content or introducing hallucinations.

Style transfer is difficult because it requires the model to perform two operations simultaneously: understand the input content and regenerate it in a different style. This dual objective increases the complexity of the learning task. A model that rewrites customer support responses from casual to formal must learn what formal business language looks like, what casual language patterns to avoid, and how to preserve all the information from the original response. This requires exposure to many examples of successful transformations across diverse input content.

**Summarization tasks** sit in the middle of the data requirement spectrum. You are teaching the model to compress information while retaining the most important content. Meeting summaries, document summaries, article summaries, earnings call summaries. These tasks typically need 2,000 to 8,000 examples depending on input length, domain complexity, and how much abstraction the summary requires.

Abstractive summarization is harder than extractive summarization because the model must generate novel sentences rather than selecting and copying from the source. This increases data requirements. The earnings call example that opened this subchapter failed because 38 examples were nowhere near enough to teach robust abstractive summarization across variable input structures and content types. The team needed more than 1,000 examples to achieve reliable generalization.

**Reasoning and problem-solving tasks** require the most data because you are teaching the model to perform multi-step cognitive operations rather than pattern matching. Mathematical reasoning, logical deduction, causal analysis, diagnostic workflows, strategic planning. These tasks often need 10,000 to 50,000 examples or more, and even then, achieving consistent performance across edge cases is difficult.

Reasoning tasks fail with small datasets because each example teaches a specific reasoning path, not general reasoning principles. A model trained on 500 math word problems learns to solve those specific problem structures but fails on novel variations. Teaching robust mathematical reasoning requires many thousands of diverse examples covering different problem types, solution strategies, and edge cases. Even the most advanced models in 2026 struggle with reasoning generalization, and data scarcity is a major contributing factor.

**Domain adaptation tasks** require the largest datasets because you are shifting the model's entire language distribution to a new domain. Teaching medical language, legal language, scientific language, industry-specific jargon, regional dialects. These tasks typically need tens of thousands to hundreds of thousands of examples, depending on how far the target domain is from the model's pretraining distribution.

Domain adaptation works by exposing the model to massive amounts of in-domain text so it updates its internal representations of what normal language looks like in that domain. A model adapted to radiology reports needs to see thousands of reports to learn the standard structure, terminology, abbreviation conventions, and reporting patterns. Small datasets do not shift the language distribution enough to matter. Teams working on domain adaptation in 2026 often use hundreds of thousands of unlabeled examples in completion format before fine-tuning on smaller labeled datasets for specific tasks.

## How Model Size Affects Data Requirements

Model size changes data requirements because larger models have more parameters to update and more capacity to overfit to small datasets. A model with 70 billion parameters trained on 100 examples will memorize those examples perfectly and fail to generalize. A model with 7 billion parameters trained on the same 100 examples will also overfit, but less severely. Smaller models need less data to generalize, but they also have less capacity to learn complex patterns. Larger models need more data to avoid overfitting, but they can learn more nuanced behavior when given sufficient examples.

As of 2026, most production fine-tuning uses models in the 7 billion to 70 billion parameter range. GPT-4o, Claude 3.5 Sonnet, Gemini 2 Flash, and Llama 3 70B all fall in this range. These models are large enough to learn complex tasks but small enough to fine-tune with datasets in the thousands to tens of thousands of examples. Fine-tuning truly massive models above 100 billion parameters is rare because the data requirements become prohibitive and the risk of overfitting increases.

Smaller models in the 1 billion to 7 billion parameter range can achieve good performance on simple tasks with smaller datasets. A 3 billion parameter model trained on 500 classification examples might generalize well because the model's limited capacity prevents it from memorizing training data. But the same model will struggle with complex generative tasks no matter how much data you provide because it lacks the representational capacity to learn nuanced patterns. Small models are data-efficient but capability-limited.

Larger models above 70 billion parameters require more data but can learn more sophisticated behavior. A 175 billion parameter model fine-tuned on 20,000 reasoning examples may achieve performance that a 7 billion parameter model cannot match even with 100,000 examples. But training the larger model requires more examples to avoid overfitting, longer training time, and more compute. In practice, most teams in 2026 prefer models in the 7B to 70B range because they offer the best balance of capability, data efficiency, and training cost.

The relationship between model size and data requirements is not linear. Doubling model size does not double data requirements. The effect is more complex and depends on task type, data quality, and how far the target task is from the model's pretraining distribution. Empirical testing is the only reliable way to determine how much data a specific model needs for a specific task. General guidelines provide starting points, but you must validate with your own data and model.

## The Diminishing Returns Curve

Adding more data improves performance, but the improvement per additional example decreases as dataset size grows. This is the **diminishing returns curve**: early examples contribute large performance gains, later examples contribute smaller gains, and eventually you reach a plateau where adding more data does not improve performance at all.

A customer support classification model trained on 100 examples might achieve 70% accuracy. Training on 500 examples might reach 85%. Training on 2,000 examples might reach 92%. Training on 10,000 examples might reach 93.5%. The jump from 100 to 500 examples added 15 percentage points. The jump from 2,000 to 10,000 examples added 1.5 percentage points. Each additional example in the larger dataset contributes far less than each example in the smaller dataset.

The curve shape varies by task complexity. Simple classification tasks plateau quickly because there are only so many decision boundary patterns to learn. Complex reasoning tasks plateau slowly or not at all because there are always more edge cases and reasoning paths to cover. A three-class sentiment classifier might plateau at 2,000 examples. A medical diagnostic model might still be improving at 50,000 examples.

Understanding where you are on the curve tells you whether adding more data is worth the cost. If you are in the steep part of the curve, adding more examples yields large gains and is usually worth the labeling cost. If you are near the plateau, adding more examples yields tiny gains and effort is better spent improving data quality or model architecture rather than increasing dataset size.

Most teams in 2026 empirically map the diminishing returns curve for their specific task by training models on progressively larger subsets of their data. Train on 100 examples, evaluate. Train on 500, evaluate. Train on 2,000, evaluate. Plot performance against dataset size. The curve tells you where the steep gains end and the plateau begins. This empirical approach is more reliable than theoretical estimates because task-specific factors dominate general scaling laws.

## When More Data Helps Versus When It Does Not

More data helps when your model is underfitting: when it has not yet learned the patterns in the data distribution and adding more examples exposes it to more of the distribution. More data does not help when your model is overfitting: when it has already memorized the training data and adding more examples just gives it more noise to memorize. The transition point between underfitting and overfitting depends on dataset size, data quality, model size, and task complexity.

If your model performs poorly on both training and validation sets, you are underfitting. More data will help because the model has not yet learned the task. You might also be underfitting because the model is too small or the task is too complex, but data scarcity is a common cause. Adding more diverse examples helps the model learn the true patterns rather than noise.

If your model performs well on training data but poorly on validation data, you are overfitting. More data might help by giving the model a broader distribution to learn from, reducing the chance it memorizes specific training examples. But more data will not help if the problem is that your training data is not representative of the validation distribution. Adding more examples from the same narrow distribution just makes the overfitting problem worse.

If your model performs well on both training and validation sets but fails on production data, you have a distribution shift problem. More training data will only help if the new data comes from the production distribution. Adding more examples from the old distribution reinforces the mismatch. This is what happened to the earnings call summarization model: it trained and validated perfectly on examples from a narrow set of companies, then failed when it encountered a company outside that distribution. The solution was not more data from the same companies; it was data from a broader range of companies.

More data also helps when you are trying to teach the model rare patterns or edge cases. If an important class appears in only 2% of your training data, the model will underlearn that class no matter how large your overall dataset is. You need more examples of the rare class specifically, not just more data overall. This is why balanced datasets often outperform unbalanced datasets even when the unbalanced dataset is larger.

More data does not help when data quality is low. Training on 10,000 noisy examples produces worse results than training on 1,000 clean examples. Noise teaches the model incorrect patterns. If your labels are inconsistent, your examples are mislabeled, or your data contains formatting errors, adding more noisy examples amplifies the noise. The model learns garbage. Fixing data quality yields larger performance gains than increasing dataset size when quality is the bottleneck.

More data does not help past the plateau. Once your model has learned all the patterns in the data distribution and is performing near the theoretical maximum for the task given the model's capacity, adding more examples from the same distribution yields no further improvement. You have hit the ceiling. At this point, improvement requires better models, better architectures, or better features, not more data.

## The Quality Versus Quantity Tradeoff

Every fine-tuning project faces the tradeoff between data quality and data quantity. You can label 500 examples carefully with expert review, or you can label 5,000 examples quickly with crowdsourcing. You can curate a small dataset of perfect examples, or you can use a large dataset of noisy examples. Which approach works better depends on task type, model size, and where you are on the diminishing returns curve.

For tasks where correctness is critical and errors are costly, quality dominates. Medical diagnosis, legal analysis, financial decision-making, safety-critical classification. In these domains, a single mislabeled example can teach the model a dangerous pattern. A model trained on 500 high-quality medical examples reviewed by physicians will outperform a model trained on 5,000 crowdsourced examples with 10% label noise. The noise teaches harmful shortcuts and incorrect associations.

For tasks where variability is high and errors are less costly, quantity often dominates. General language tasks, style transfer, casual conversation, content recommendation. In these domains, exposure to diverse examples matters more than perfect correctness. A rewriting model trained on 10,000 diverse examples with occasional inconsistencies will generalize better than a model trained on 500 perfectly curated examples from a narrow domain. The diversity teaches robust patterns that transfer.

Model size affects this tradeoff. Larger models are more robust to label noise because they have the capacity to learn signal from noisy data. Smaller models are more sensitive to noise and benefit more from clean data. If you are fine-tuning a 70B parameter model, you can tolerate some label noise in a large dataset and still achieve good performance. If you are fine-tuning a 3B parameter model, label noise degrades performance more severely and careful curation pays off.

The stage of development also matters. Early in development, small high-quality datasets help you validate that fine-tuning works at all for your task and establish a performance baseline. Later in development, larger datasets with acceptable quality help you push performance toward production requirements. Most teams in 2026 start with a few hundred carefully curated examples to prove feasibility, then scale to thousands of examples with managed quality processes to reach production targets.

Practical data collection strategies balance quality and quantity by using tiered labeling. Critical examples get expert review. Common patterns get crowdsourced labeling with validation. Edge cases get careful curation. Bulk data gets automated labeling with sampling-based quality checks. This approach yields large datasets where quality is high on average and very high on the examples that matter most.

## Practical Guidelines for 2026 Models and Platforms

As of 2026, the major fine-tuning platforms provide dataset size recommendations for common task types. OpenAI's fine-tuning documentation recommends 50 to 100 examples as a minimum for any task, 500 to 1,000 examples for classification, and several thousand examples for complex generative tasks. Anthropic's guidelines suggest similar ranges. Google's Vertex AI fine-tuning recommends at least 100 examples and preferably 1,000 or more for production use.

These are lower bounds, not targets. Many tasks require datasets well above the recommended minimums to achieve production-quality performance. The platforms provide minimums because fine-tuning jobs below those thresholds often fail to converge or produce models that do not outperform the base model. But just because a job completes successfully with 100 examples does not mean the resulting model is production-ready.

The practical guideline for 2026 is to start with at least 500 examples for simple tasks and 2,000 examples for complex tasks, validate performance on a held-out set, map the diminishing returns curve by training on progressively larger subsets, and scale data collection until you reach the plateau or hit your performance target. Do not assume that the minimum dataset size will be sufficient. Do not assume that more data always helps. Empirically validate where you are on the curve.

For classification tasks, aim for 300 to 1,000 examples per class. For extraction tasks, aim for 2,000 to 5,000 examples covering diverse input formats. For rewriting and style tasks, aim for 3,000 to 10,000 examples. For summarization, aim for 2,000 to 8,000 examples depending on input complexity. For reasoning tasks, aim for 10,000 examples or more and expect that even this may not be sufficient for robust edge case handling. For domain adaptation, aim for tens of thousands of examples or use continued pretraining on unlabeled data.

When working with smaller budgets or tighter timelines, focus on quality over quantity until you reach the minimum viable dataset size for your task type. A classification task with 500 high-quality examples will outperform the same task with 2,000 noisy examples. But once you have sufficient quality, adding more examples yields performance gains until you hit the plateau.

Dataset size is not a one-time decision. As your product evolves and you encounter new edge cases in production, you will need to add examples to cover those cases. Successful fine-tuning in 2026 involves continuous data collection and periodic retraining as your dataset grows and your understanding of the task deepens. The initial dataset gets you to launch. The growing dataset keeps you competitive as the product matures.

## Validating Dataset Size Sufficiency

The only reliable way to know whether your dataset is large enough is to train a model and evaluate it on held-out data that represents your production distribution. Training loss tells you whether the model is learning the training data. Validation loss tells you whether it generalizes. The gap between training and validation loss tells you whether you are overfitting.

If validation loss is much higher than training loss, you are overfitting and may need more data or regularization. If both losses are high, you are underfitting and need more data, a larger model, or better features. If both losses are low and validation performance is strong, your dataset is sufficient for the distribution you have tested on. But you must still validate on production data to confirm that your held-out set was representative.

Many teams make the mistake of validating on too small a held-out set or a held-out set drawn from the same narrow distribution as training data. The earnings call model that failed in production had validated perfectly on five held-out examples from the same companies as the training data. This validation proved nothing about generalization to new companies. A robust validation set must be large enough to estimate performance reliably—at least 100 examples for most tasks—and must cover the diversity of the production distribution.

Another validation approach is learning curve analysis: train models on progressively larger subsets of your data and plot validation performance against dataset size. If performance is still climbing steeply, you need more data. If performance has plateaued, you have enough data for the current model and task. If performance is increasing slowly, you are near the plateau and should decide whether the marginal gain from more data justifies the labeling cost.

Production monitoring provides the final validation. After deploying a fine-tuned model, track performance metrics on live traffic and compare to validation metrics. If production performance matches validation performance, your dataset was representative and sufficient. If production performance is worse, your validation set did not cover the production distribution and you need more diverse training data. Continuous monitoring catches distribution drift and tells you when to collect more data and retrain.

Dataset size is one of the most important levers you control in fine-tuning. Too little data and the model never learns robust patterns. Too much low-quality data and the model learns noise. The right amount of high-quality data gets you to production performance efficiently. Understanding task-specific requirements, mapping the diminishing returns curve, balancing quality and quantity, and validating empirically are the practices that separate successful fine-tuning projects from failures.

You now know how much data you need. The next question is how to structure that data during training to maximize learning efficiency and prevent overfitting. We turn to dataset splits, sampling strategies, and the mechanics of training, validation, and test sets.
