# 1.10 — When to Abandon a Fine-Tuning Effort Mid-Flight

Eleven weeks and $127,000 spent on a model that never worked. This is the cost of not knowing when to quit. A fraud detection team at a payments company saw their training loss plateau at week three, watched their evaluation metrics flat-line at week five, discovered critical label quality issues at week seven, and still pushed forward to week eleven before their engineering director finally terminated the project. The team had nothing to show for nearly three months of work except a postmortem document and a painful lesson about sunk costs. The rational kill decision should have happened at week five when the data made it clear the project would not succeed, but the psychological pull of past investment kept the team grinding forward, hoping that one more training run or one more data cleaning pass would turn failure into success. This pattern destroys ML projects constantly: teams continue investing in failing fine-tuning efforts not because the expected future value justifies the cost, but because they have already spent time, money, and credibility and feel psychologically compelled to see it through. The distinction between experienced ML teams and inexperienced ones is not technical skill — it is the discipline to recognize kill signals early, make tough decisions based on objective criteria rather than emotional attachment, and reallocate resources to higher-probability projects before waste compounds.

This is the sunk cost fallacy in action, and it destroys ML projects constantly. Teams continue investing in failing fine-tuning efforts because they have already spent time, money, and credibility on the project. Leadership allows projects to continue because killing them feels like admitting failure. The result is zombie projects that consume resources for months without delivering value. Knowing when to kill a fine-tuning project is a skill that separates experienced ML teams from inexperienced ones. Experienced teams kill early and often. Inexperienced teams cling to failing projects until forced to abandon them. This subchapter teaches you how to recognize the kill signals and how to conduct a kill review that salvages value from failed work.

## The Sunk Cost Fallacy and Why It Persists

The sunk cost fallacy is the tendency to continue investing in a project because of past investment rather than future expected value. In ML projects, this manifests as: "We have already spent six weeks on this, we cannot give up now." The rational decision is to compare the expected value of continuing versus the expected value of stopping and reallocating resources. If continuing the project has a 20% chance of success and will cost another $40,000 and four weeks, while stopping and pivoting to an alternative has an 80% chance of success and will cost $25,000 and three weeks, the rational choice is obvious. But teams do not make this comparison. They focus on the $60,000 and six weeks already spent and feel psychologically compelled to continue.

The sunk cost fallacy persists for three reasons. First, killing a project feels like admitting you made a mistake when you approved it. Leaders who championed a fine-tuning project struggle to kill it because doing so requires acknowledging that their initial judgment was wrong. This is ego protection, not rational decision-making. Second, teams develop emotional attachment to projects. Engineers who have spent weeks preparing data, training models, and debugging convergence issues feel invested in the outcome. Killing the project feels like invalidating their work. Third, organizations punish visible failures more harshly than invisible waste. A killed project is a visible failure that appears on status reports and postmortems. A project that drags on for six months and delivers marginal value is invisible waste that no one notices.

Experienced ML leaders combat the sunk cost fallacy by establishing kill criteria before the project starts. During the proposal phase, you define specific conditions under which the project will be terminated: if training loss does not decrease by 15% within the first two weeks, if evaluation metrics do not exceed baseline by week four, if costs exceed the estimate by more than 20%, or if critical timeline milestones are missed. These criteria are written into the proposal and reviewed at regular checkpoints. When a kill criterion is triggered, the decision to continue or stop is based on whether the project still meets the original success conditions, not on how much has already been spent.

## Specific Signals That a Project Should Be Killed

The first kill signal is training loss not converging. If you have trained for two weeks or 10,000 steps and the training loss is still flat or oscillating, the model is not learning. This usually indicates a fundamental problem: learning rate too high or too low, data quality issues, architecture mismatch, or insufficient training data. You should not wait another four weeks hoping the loss will suddenly decrease. At the two-week checkpoint, you evaluate: has the loss decreased by at least 15% from the initial value? If not, you stop training and diagnose the root cause. If the root cause is fixable in less than one week, you fix it and restart. If the root cause requires more than one week or is unfixable, you kill the project.

The second kill signal is evaluation metrics not improving over baseline. Training loss can decrease while evaluation metrics stay flat if the model is overfitting to the training set. At the four-week checkpoint, you compare your fine-tuned model performance on a held-out test set to your baseline performance. If the fine-tuned model is not at least 3% better than baseline on your primary metric, the project is not on track. Some teams see a 1% improvement and convince themselves that more training will yield more improvement. This is wishful thinking. If you have trained for four weeks and have only 1% improvement, you are not going to reach 10% improvement by week eight. You kill the project or pivot to a different approach.

The third kill signal is data quality issues discovered mid-training. You thought you had 50,000 high-quality labeled examples, but at week three you discover that 30% of the labels are incorrect, inconsistent, or outdated. Fixing the labels will take four weeks and require re-annotation by domain experts. You now face a choice: delay the project by four weeks to fix the data, or continue training on low-quality data and accept mediocre results. The correct choice depends on whether the project timeline can absorb a four-week delay and whether fixing the data will actually solve the problem. If the timeline cannot absorb the delay, you kill the project. If the timeline can absorb the delay but the data quality issues are systemic and will recur, you kill the project. If the timeline can absorb the delay and the data issues are fixable, you pause, fix the data, and restart.

The fourth kill signal is cost overruns. Your proposal estimated $30,000 in training costs, but by week four you have spent $38,000 and are only halfway through training. Costs are running 60% over estimate, and the project is on track to cost $55,000 instead of $30,000. This happens when teams underestimate the number of training runs required, underestimate the cost per run, or discover that they need more expensive infrastructure than planned. At this point, you recalculate the total expected cost and compare it to the expected benefit. If the project will cost $55,000 but save $200,000 per year, you continue with revised budget approval. If the project will cost $55,000 but save $40,000 per year, you kill it because the ROI no longer justifies the investment.

The fifth kill signal is timeline slippage. Your proposal committed to an eight-week timeline. At week six, you are at the stage you planned to reach by week four. You are two weeks behind schedule, which means you will finish at week ten instead of week eight. Timeline slippage happens because of data issues, convergence problems, infrastructure delays, or scope creep. The question is whether the delay is a one-time setback or a symptom of deeper problems. If the delay is one-time and you can credibly commit to finishing by week ten, you extend the timeline with leadership approval. If the delay is recurring and week ten is optimistic, you kill the project. Teams that slip from eight weeks to ten weeks usually slip again to twelve weeks, then fourteen. Chronic timeline slippage is a kill signal.

## The Psychological Difficulty of Killing a Project

Killing a project is psychologically difficult even when the rational case is clear. The team has invested weeks of work, the project was publicly announced, stakeholders are expecting results, and leadership approved the budget. Killing the project means admitting to all of these people that the investment was wasted. This is why projects that should be killed at week four are allowed to continue until week twelve. The team keeps hoping that one more training run, one more hyperparameter adjustment, or one more data cleaning pass will fix the problem. They focus on small signs of progress and ignore overwhelming signs of failure.

The first psychological barrier is the team's emotional investment. Engineers who have spent a month preparing data and training models do not want to hear that their work will be discarded. They will argue that they are close to a breakthrough, that the next training run will work, that the evaluation metrics are misleading. This is motivated reasoning. The team wants the project to succeed so badly that they interpret ambiguous signals as positive and dismiss negative signals as noise. The engineering manager must counteract this bias by focusing the team on objective kill criteria established before the project started. If the criteria say kill at week four if eval metrics are not 3% above baseline, and eval metrics are only 1% above baseline, the decision is clear regardless of how the team feels.

The second psychological barrier is leadership's reluctance to admit a mistake. The VP who approved the fine-tuning project does not want to report to their CEO that the project failed and the budget was wasted. They will ask the team if there is any way to salvage the project, if a reduced scope would work, if extending the timeline would help. The team, eager to avoid disappointing leadership, will say yes even when the honest answer is no. This creates a cycle where leadership asks for reassurance, the team provides it, and the project continues for another month before the next checkpoint reveals that nothing has improved. Breaking this cycle requires leadership to prioritize learning over ego protection and to reward teams for killing projects early rather than punishing them for failure.

The third psychological barrier is the fear of setting a precedent. If you kill this fine-tuning project, will leadership become reluctant to approve future ML projects? Will the team lose credibility? Will stakeholders question whether ML is worth the investment? These fears are valid but backward. Killing a failing project early builds credibility because it demonstrates good judgment and resource discipline. Leadership trusts teams that know when to cut losses. Stakeholders respect teams that admit failure and pivot quickly. The precedent you set by killing a project at week five is: "This team does not waste time on failing projects. They evaluate objectively and make tough decisions." That precedent increases your credibility for future proposals.

## How to Conduct a Kill Review

A kill review is a structured meeting where the team evaluates whether to continue, pivot, or kill the project. It is not a postmortem. A postmortem happens after a project is already dead and focuses on lessons learned. A kill review happens while the project is still active and focuses on whether it should continue. You conduct a kill review when a kill signal is triggered: training loss not converging, eval metrics not improving, data quality issues discovered, cost overruns, or timeline slippage. The kill review follows a specific agenda: review the original success criteria, evaluate current status against those criteria, estimate the cost and timeline to reach success from the current state, compare the expected value of continuing versus stopping, and make a decision.

The first step is to restate the original success criteria without interpretation or spin. If the proposal said "achieve 92% accuracy by week eight," that is the criterion. You do not revise it to "achieve 88% accuracy by week ten" because the project is behind schedule. The original criteria are the benchmark. The second step is to evaluate current status objectively. If you are at week four and have achieved 85% accuracy compared to an 82% baseline, you state: "We are at 85% accuracy, which is 3% above baseline and 7 percentage points below our 92% target. We have four weeks remaining to close a 7-point gap." The third step is to estimate what it will take to close the gap. Will four more weeks of training get you from 85% to 92%? If you have seen 3% improvement in four weeks, linear extrapolation suggests another 3% improvement in the next four weeks, reaching 88%, not 92%. This projection suggests the project will not hit its target.

The fourth step is to explore alternatives. Can you adjust the model architecture, increase the training data, improve data quality, or change the evaluation metric? Each alternative has a cost and a probability of success. For example: "We could re-annotate 10,000 training examples to fix label inconsistencies, which would take three weeks and cost $15,000. Our best estimate is this would increase accuracy from 85% to 89%, still below our 92% target." The team evaluates whether any alternative has a realistic path to success within acceptable cost and timeline. If no alternative offers better than 50% chance of hitting the original target, the project should be killed.

The fifth step is to calculate the opportunity cost of continuing. If you continue this project for another four weeks, what are you not doing? Are you delaying a higher-priority project? Are you missing a market window? Are you consuming budget that could be reallocated to a more promising initiative? You compare the expected value of continuing this project against the expected value of the best alternative use of resources. If continuing this project has a 30% chance of delivering $200,000 in annual value and the alternative project has a 70% chance of delivering $150,000 in annual value, the expected value of continuing is $60,000 and the expected value of stopping is $105,000. You stop.

The sixth step is to make a decision. The decision is not consensus-based. The engineering leader makes the call based on the data reviewed in the meeting. The possible decisions are: continue with no changes, continue with a specific pivot, pause to address a blocking issue, or kill. If you decide to continue with no changes, you must believe the project is on track and the kill signals were false alarms. If you decide to continue with a pivot, you must specify exactly what is changing: new architecture, new data, new success criteria, new timeline. If you decide to pause, you must specify what needs to be fixed and when the project will resume. If you decide to kill, you move immediately to discussing what to salvage.

## What to Salvage from Abandoned Projects

Killing a fine-tuning project does not mean all the work is wasted. You can salvage data, tooling, insights, and credibility. The most valuable salvage is the training data. If you spent three weeks cleaning, labeling, and validating 50,000 examples, that dataset is useful even if the fine-tuning project failed. You can use it for future fine-tuning attempts, for evaluation benchmarks, for few-shot examples in prompts, or for training different model architectures. You document the dataset, store it in a shared repository, and ensure it is discoverable by future teams. This transforms a failed project into a reusable asset.

The second salvageable asset is tooling and infrastructure. If you built data pipelines, training scripts, evaluation harnesses, or monitoring dashboards, those tools can be reused in future ML projects. A well-designed training pipeline is valuable even if the first model trained with it did not work. You refactor the tooling to be generic, document how to use it, and add it to your internal ML platform. This reduces the setup cost for the next fine-tuning project and increases the likelihood of success.

The third salvageable asset is insights about what does not work. You learned that prompt engineering caps out at 87% accuracy for your use case, that your labeling process has a 15% error rate, that your dataset has class imbalance problems, or that convergence is sensitive to learning rate. These insights are valuable for future projects. You document them in a knowledge base or wiki so that future teams do not repeat the same mistakes. This is how organizations build institutional knowledge: by systematically capturing lessons from both successes and failures.

The fourth salvageable asset is credibility. This seems counterintuitive, but killing a failing project early and transparently actually builds credibility with leadership. You demonstrate that you evaluate projects objectively, that you make tough decisions based on data, and that you do not cling to failing initiatives out of ego. When you propose the next ML project, leadership will remember that you killed the last one at week five instead of dragging it out to week twelve. This builds trust. Conversely, teams that allow failing projects to drag on lose credibility because leadership perceives them as unable to self-correct.

## Communicating the Kill Decision to Stakeholders

After deciding to kill the project, you must communicate the decision to three audiences: the engineering team, leadership, and stakeholders who were expecting the project to deliver value. Each audience needs different information. The engineering team needs to understand why the decision was made, what the data showed, and what happens next. Leadership needs to understand the financial impact, the timeline impact, and the plan for reallocating resources. Stakeholders need to understand what problem is not being solved, what alternative solution is being pursued, and when they can expect results.

For the engineering team, the message is: "We evaluated the project at our week-four checkpoint and found that we are at 85% accuracy compared to our 92% target. Based on current progress, we estimate we will reach 88% by week eight, which does not meet our launch criteria. We explored alternatives including re-annotation and architecture changes, but none offer a realistic path to 92% within our timeline and budget. We are killing the project effective immediately. The training data and tooling we built will be reused in future projects, and we will conduct a lessons-learned review next week. Your next assignment is the recommendation system project, which starts Monday." This message is factual, definitive, and forward-looking.

For leadership, the message is: "We are terminating the ticket classification fine-tuning project after four weeks based on our predefined kill criteria. The project has not achieved the intermediate milestones required to stay on track for our 92% accuracy target. We have spent $24,000 of the approved $30,000 budget. The remaining $6,000 will be reallocated to the recommendation system project, which has higher expected ROI. The engineering team will transition to the new project immediately, with no delay to our overall roadmap. We will document lessons learned and apply them to future ML projects." This message quantifies the financial impact and reassures leadership that resources are being managed responsibly.

For stakeholders, the message is: "We are discontinuing the fine-tuning approach for ticket classification after determining it will not meet our accuracy and cost targets. We are pivoting to a hybrid approach that combines improved prompt engineering with retrieval augmentation, which our testing shows can achieve 89% accuracy at lower cost and faster time to launch. We expect to deliver the hybrid system by the end of next quarter. In the meantime, we will continue using the current prompt-based system with the improvements we have already implemented." This message acknowledges the failure, presents an alternative solution, and sets new expectations.

## The Decision Rule: Kill Fast, Learn Fast

The decision rule for killing fine-tuning projects is: kill fast, learn fast. Fast means you evaluate kill criteria at regular checkpoints and make the kill decision within one week of a checkpoint where criteria are not met. You do not wait for the next checkpoint hoping things improve. You do not extend the timeline by two weeks to see if one more training run works. You evaluate, decide, and act immediately. Learning fast means you document what went wrong, why it went wrong, and what you will do differently next time. You conduct a lessons-learned session within one week of the kill decision, while the details are still fresh.

Teams that kill fast and learn fast build a culture of rapid iteration and honest evaluation. They try more experiments, fail more often, and succeed more often because they do not waste time on failing projects. Teams that cling to failing projects build a culture of denial and sunk-cost thinking. They try fewer experiments, fail less visibly, and succeed less often because their resources are tied up in zombie projects. The difference between these cultures is not technical skill. It is psychological discipline: the ability to admit failure, make tough decisions, and move on quickly.

The most important lesson from killing a fine-tuning project is that failure is a form of progress. You now know that fine-tuning does not solve this particular problem, which means you can focus on solutions that do. You have eliminated a hypothesis and narrowed the solution space. This is how ML engineering works: you propose a hypothesis, test it, evaluate the results, and either commit to the approach or kill it and try something else. The teams that succeed are not the teams that never fail. They are the teams that fail quickly, learn from the failure, and try the next approach without delay.

Knowing when to abandon a fine-tuning effort mid-flight is not a sign of weakness. It is a sign of maturity. It means you understand that fine-tuning is a tool, not a goal, and that the goal is to solve the business problem, not to complete the project you started. The next challenge is understanding when fine-tuning is the right tool from the beginning, which requires a framework for evaluating alternatives before you write the proposal.
