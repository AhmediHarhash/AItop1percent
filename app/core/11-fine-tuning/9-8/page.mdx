# 9.8 â€” Model Versioning: Registry, Naming Conventions, and Artifact Management

In October 2025, a healthcare technology company was operating four fine-tuned Claude models in production, each trained for a different clinical documentation task. Over the course of nine months, they had retrained each model multiple times in response to quality drift and product updates. By October, they had produced 37 different model checkpoints across the four tasks. Each checkpoint was stored in an S3 bucket with a filename like model-final-v2.weights or updated-model-october.bin. When a production issue arose with one of the models, the engineering team wanted to roll back to the previous version. They opened the S3 bucket and found eight files with names like model-final, model-final-v2, model-final-v3, model-final-v4, and model-really-final. They had no record of which file was currently in production, which was the previous version, or what training data or hyperparameters had been used for each. They picked what they thought was the previous version, deployed it, and made the problem worse because they had actually deployed a three-month-old checkpoint that predated a critical bug fix. It took them six hours to identify the correct previous version and restore service. The root cause was not a technical failure. It was an organizational failure. They had trained and deployed models without implementing any versioning discipline. They had artifacts but no artifact management. They had models but no model registry.

Model versioning is the practice of systematically tracking, naming, and managing every artifact associated with your fine-tuned models. This includes model weights, training configurations, hyperparameters, training datasets, evaluation results, and deployment history. Without versioning, you cannot reproduce models, you cannot audit decisions, you cannot roll back safely, and you cannot scale your fine-tuning process beyond a single person working on a single model. Versioning is not optional. It is foundational infrastructure that every production fine-tuning operation requires.

## The Purpose of Model Versioning

Model versioning serves four critical functions. The first is reproducibility. You must be able to recreate any model checkpoint from scratch given the same training data and configuration. This is essential for debugging, auditing, and compliance. If you cannot reproduce a model, you cannot verify its behavior or explain its decisions. Reproducibility requires versioning not just the model weights but also the training data, the code, the hyperparameters, and the environment.

The second function is rollback. When a newly deployed model causes problems, you need to revert to the previous version quickly and safely. Rollback requires knowing exactly which version is currently in production, which version was deployed before it, and having immediate access to the previous version's weights. Without versioning, rollback is a guessing game that prolongs outages.

The third function is comparison. You need to compare models to understand which performed better, which was trained on what data, and which hyperparameters produced which results. Comparison requires that every model is associated with metadata describing its provenance and performance. Without metadata, you have weights but no context.

The fourth function is collaboration. When multiple engineers are training models, they need a shared system for tracking what has been tried, what worked, and what did not. Versioning provides a shared language and shared infrastructure for collaboration. Without it, knowledge is siloed and effort is duplicated.

## Model Registries

A model registry is a centralized system for storing, versioning, and managing model artifacts and metadata. Popular open-source options include MLflow, DVC, and Weights & Biases. Many cloud platforms offer managed registries such as AWS SageMaker Model Registry and Google Vertex AI Model Registry. You can also build a custom registry using object storage, a database, and a simple API.

The core functionality of a model registry is to store model checkpoints along with rich metadata. When you train a model, you register it in the registry with a unique identifier, a version number, and metadata including training dataset version, hyperparameters, evaluation metrics, training duration, and training environment. The registry assigns the model a permanent ID and stores the weights in object storage. When you need to deploy or evaluate a model, you query the registry by ID or version, and the registry provides both the weights and the metadata.

A good registry also supports tagging and staging. You can tag models with labels such as candidate, production, archived, or experimental. You can promote models through stages such as development, staging, production. This makes it easy to query for the current production model or to list all candidate models awaiting validation. Tagging and staging turn the registry into a workflow management tool, not just a storage system.

Registries also enable lineage tracking. You can record that model version 12 was fine-tuned from base model GPT-4o, trained on dataset version 5, evaluated on validation set version 3, and deployed to production on November 15, 2025. You can trace backward from a production model to its training data and forward from a dataset to all models trained on it. Lineage is essential for debugging, auditing, and understanding how changes in data or hyperparameters affect model behavior.

The most important property of a model registry is that it is the single source of truth. Every model that goes to production is registered. Every deployment references a registry ID, not a filename or S3 path. If a model is not in the registry, it does not exist. This discipline prevents the chaos of ad-hoc file storage and ensures that all models are tracked.

## Naming Conventions That Scale

Even with a registry, you need sensible naming conventions. Model IDs and version numbers should be human-readable and informative. A model named 8f3d2a91 or model-final-v2 tells you nothing. A model named clinical-summarization-gpt4o-v7-20251115 tells you the task, the base model, the version number, and the training date.

A good naming convention includes four components. The first is the task or domain. This disambiguates models trained for different purposes. If you have models for customer-support, fraud-detection, and content-generation, the task name should be in the model ID. The second is the base model. If you are fine-tuning multiple base models, you need to distinguish them. A model fine-tuned from GPT-4o behaves differently from one fine-tuned from Claude 3.5 Sonnet, even if they are trained on the same data.

The third component is the version number. Versions should be incremented sequentially for each task and base model combination. The first model for customer support fine-tuned from GPT-4o is v1. The second is v2. If you retrain from scratch, you increment the version. If you do incremental training on top of v7, the result is v8. Version numbers provide a clear ordering and make it easy to refer to specific models in conversation and documentation.

The fourth component is metadata such as the training date or a short descriptor. The date helps you quickly identify how old a model is. A descriptor such as high-precision or multilingual helps you remember what made this version distinct. The full name might be customer-support-gpt4o-v8-20251201-multilingual. This is verbose but informative. You can always shorten it to cs-gpt4o-v8 in conversation while maintaining the full name in the registry.

Consistency is more important than the specific convention you choose. If your team agrees that model names follow task-basemodel-version-date, every model should follow that pattern. Inconsistent naming is worse than no naming convention at all because it creates confusion about which pattern to trust.

## Artifact Management

Model weights are not the only artifacts you need to manage. A complete artifact set for a fine-tuned model includes the model weights, the training dataset, the validation dataset, the training configuration file, the hyperparameters, the training logs, the evaluation results, and any preprocessing code or scripts used during training. All of these artifacts must be versioned and stored together so that you can fully reconstruct the training process.

Training datasets should be versioned separately from models. You may train multiple models on the same dataset version, and you may reuse datasets across tasks. A dataset version should have a unique ID, a timestamp, and metadata describing its source, size, and any preprocessing applied. If you filtered or augmented the data, that should be documented. The dataset version is then referenced in the model metadata, creating a link between the two.

Hyperparameters and training configurations should be stored as structured files, typically JSON or YAML. You should never rely on memory or code comments to record what hyperparameters were used. Every training run should save a config file that includes learning rate, batch size, number of epochs, optimizer settings, and any other parameters that affect training. The config file is stored alongside the model weights and linked in the registry.

Evaluation results should be stored as structured data, not just printed to logs. After training, you run evaluation on your validation set and record metrics such as accuracy, precision, recall, F1, and any task-specific metrics. These metrics are saved to a JSON file and uploaded to the registry along with the model. This allows you to query the registry for all models with accuracy above 90% or to compare the precision of v7 and v8 without rerunning evaluation.

Training logs capture the full history of the training process: loss curves, gradient norms, learning rate schedules, validation metrics at each checkpoint. Logs are useful for debugging and for understanding why a model converged or failed to converge. They should be stored in structured formats such as TensorBoard logs or JSON lines, not just plain text. Logs are large, so they are often stored separately from the model weights, but they should be linked in the registry metadata so you can retrieve them when needed.

Preprocessing code and scripts should be versioned using standard source control such as Git. The model metadata should include the Git commit hash of the code used for training. This ensures that you can check out the exact code version and reproduce the training process. If preprocessing logic changes between training runs, the commit hash allows you to identify what changed and how it affected results.

## Linking Models to Training Data and Code Versions

The most valuable capability of a model registry is linking models to their provenance. Every model should record what dataset it was trained on, what code was used, and what environment it ran in. This creates a dependency graph that you can traverse in both directions.

When you deploy a model to production and it starts producing bad outputs, you want to know what data it was trained on. You query the registry, retrieve the dataset version, and examine the training examples to see if they contain patterns that explain the bad behavior. You may discover that the training data included mislabeled examples or adversarial inputs that taught the model incorrect patterns. Without the link from model to data, you would have to guess.

When you discover that a dataset contains errors, you want to know which models were trained on it. You query the registry for all models trained on dataset version 5, and you find that models v6, v7, and v8 all used that dataset. You now know that all three models may be affected and need to be retrained. Without the link from data to models, you would have to manually search through training logs and hope you did not miss any.

Linking models to code versions is equally important. If a bug is discovered in your preprocessing code, you want to know which models were affected. You search for all models trained with code from commits prior to the bug fix, and you mark them as needing retraining. If a performance optimization is added to your training pipeline, you want to know which models were trained before and after the optimization so you can compare results.

These links are only possible if you systematically record them during training. Your training pipeline should automatically capture the dataset version, the Git commit hash, the hyperparameters, and the environment details, and it should write them to the model metadata. This should be automated, not manual. Manual record-keeping is error-prone and does not scale.

## Managing Multiple Versions in Production

In many cases, you will have multiple versions of a model in production simultaneously. You might run a canary deployment where 5% of traffic goes to v8 while 95% goes to v7. You might serve different models to different customer tiers. You might A/B test two models to compare performance. Managing multiple versions requires careful coordination between your registry and your deployment infrastructure.

Your deployment system should reference models by registry ID, not by filename or path. When you deploy v8 to 5% of traffic, your configuration specifies registry ID customer-support-gpt4o-v8, and the deployment system queries the registry to retrieve the weights. When you promote v8 to 100% of traffic, you update the configuration to reference v8 for all traffic. The registry ensures that the deployment system always gets the correct weights.

You should also track which versions are deployed where. Your registry should record that v7 is in production serving 95% of traffic, v8 is in production serving 5% of traffic, and v6 is archived. This metadata allows you to query the registry and immediately know what is running. When an incident occurs, you can check the registry to see which model version is responsible.

Versioning also enables safe rollback. If v8 causes problems, you change the deployment configuration to reference v7, and the deployment system retrieves the v7 weights from the registry. Rollback takes seconds, not hours, because you are not searching for files or guessing which version to use. The registry provides certainty.

## Archival and Retention Policies

Not all model versions need to be kept forever. Old versions that are no longer in production and are unlikely to be deployed again can be archived or deleted to save storage costs. However, you must be careful about what you delete. Deleting a model version also deletes the ability to audit, reproduce, or learn from it.

A reasonable retention policy is to keep all production models indefinitely, keep candidate models that were seriously considered for production for one year, and delete experimental models after 90 days. Production models are kept forever because you may need to audit historical decisions or reproduce behavior from months or years ago. Candidate models are kept for a year because you may want to revisit them or compare them to future models. Experimental models that were never deployed and never considered for production can be deleted because they have little long-term value.

Archival should move models to cheaper storage rather than deleting them entirely. You can move archived model weights to infrequent-access storage tiers such as AWS S3 Glacier, reducing costs while retaining the ability to retrieve them if needed. The registry metadata should indicate that a model is archived and where its weights are stored.

You should also archive or delete associated artifacts such as logs and evaluation results when you archive a model. However, training data should generally be retained even if the models trained on it are archived, because you may want to retrain or analyze the data in the future.

## Model Registries and Compliance

In regulated industries such as healthcare, finance, and government, model versioning is not just an operational best practice but a compliance requirement. Regulations such as HIPAA, GDPR, and the EU AI Act require that you be able to explain how models make decisions, reproduce model behavior, and audit historical decisions. Without versioning, you cannot meet these requirements.

A compliant model registry includes not just technical metadata but also governance metadata. This includes who trained the model, who approved it for production, what validation was performed, what risks were identified, and what mitigations were applied. When an auditor asks how a specific decision was made six months ago, you query the registry to identify which model version was in production at that time, retrieve the model weights and training data, and reproduce the decision. You can then provide documentation of the training process, the validation results, and the approval chain.

Governance metadata should be structured and queryable. You should be able to generate reports showing all models deployed in the past year, all models trained on datasets containing sensitive data, or all models that have not been revalidated in the past six months. These reports are essential for compliance audits and internal governance.

## Building a Custom Model Registry

If you choose to build a custom model registry rather than using an off-the-shelf solution, the core components are object storage, a database, and an API. Object storage such as S3 or Google Cloud Storage holds the model weights, logs, and large artifacts. The database holds metadata: model IDs, version numbers, hyperparameters, evaluation metrics, dataset references, deployment history. The API provides endpoints for registering models, querying metadata, retrieving weights, and updating deployment status.

A minimal API includes four operations. Register a new model: upload weights and metadata, assign a unique ID, return the ID to the caller. Retrieve a model: query by ID or version, return weights and metadata. List models: query by task, status, or date, return a list of matching models. Update model status: mark a model as production, archived, or deprecated.

Building a custom registry gives you full control and allows you to tailor it to your specific workflows. The downside is that you own the maintenance, scalability, and reliability. For most teams, using an existing registry such as MLflow or a managed cloud service is more practical. You get proven functionality, integrations with training frameworks, and ongoing support. The effort saved on building and maintaining infrastructure can be invested in improving model quality.

## The Discipline of Versioning

The hardest part of model versioning is not the technology. It is the discipline. You must commit to registering every model, recording every training run, and never deploying unversioned models. This requires process and culture. When an engineer trains a model, the training pipeline must automatically register it. When a model is deployed, the deployment pipeline must verify that it exists in the registry. When a model is rolled back, the rollback procedure must reference the registry.

You enforce this discipline through automation and guardrails. Your training scripts should fail if they cannot connect to the registry. Your deployment scripts should refuse to deploy a model that is not registered. Your code review process should require that all training changes update the registry integration. Over time, versioning becomes automatic, and the team cannot imagine working without it.

Versioning is the foundation that makes everything else possible. Without it, you cannot retrain safely, you cannot roll back reliably, you cannot audit effectively, and you cannot scale your fine-tuning operation. With it, you have clarity, reproducibility, and confidence. Every model has a name, a history, and a purpose. Every deployment is traceable. Every incident is debuggable. That clarity is worth the investment.

You have versioned your models and established a registry. The final operational challenge is serving multiple models simultaneously: routing traffic to different fine-tuned variants based on task type, customer tier, or experimentation goals. That is the focus of the next subchapter: multi-model serving.
