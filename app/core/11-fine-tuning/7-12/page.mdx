# 7.12 â€” Continuous Evaluation: Monitoring Fine-Tuned Model Quality Post-Deployment

In March 2025, a logistics company deployed a fine-tuned model to predict delivery time windows from shipping manifests, warehouse locations, and traffic data. Pre-deployment evaluation showed 91% accuracy on historical data, and the first two weeks in production confirmed those results: predicted windows matched actual deliveries within the promised threshold. By week five, accuracy had dropped to 84%. By week eight, it was 78%. Customer complaints spiked, and the operations team routed more exceptions to manual review, consuming the cost savings the model was supposed to generate. The model had not changed. The code had not changed. What changed was the world: a new warehouse opened, shifting the geographic distribution of shipments; a highway construction project altered traffic patterns; and seasonal demand increased package volumes beyond training data ranges. The model was not monitoring itself, and the team was not monitoring the model. By the time they noticed degradation, they had already made 47,000 inaccurate predictions, missed service level agreements with twelve major customers, and damaged relationships that took months to rebuild. The incident cost $3.2 million in service credits and emergency retraining. The root cause was the assumption that a model evaluated once stays evaluated forever.

Evaluation does not end at deployment. Fine-tuned models degrade over time as the world drifts away from training distributions, as adversaries adapt, as edge cases accumulate, and as upstream data sources change. Continuous evaluation means running production evaluation pipelines that measure quality, safety, and performance in real time, detect degradation before it harms users, and trigger retraining or interventions when thresholds are breached. This subchapter covers how to design production evaluation systems, how to set alert thresholds that balance false positives and response time, and how to build feedback loops that use production data to improve future models.

## Why Models Degrade and Why You Will Not Notice Without Monitoring

Model degradation is inevitable. The model was trained on data from the past. Production receives data from the present. The longer the model runs, the greater the divergence. Degradation takes many forms, and none of them announce themselves.

Distributional drift is the most common cause. Your training data represented January through December of 2024. By June 2025, user behavior has changed, product catalogs have changed, regulatory requirements have changed, and language has changed. If your model extracts product names from customer service chats, new products launched after training are unknown. If your model classifies support tickets by urgency, new issue types that did not exist during training are misclassified. If your model generates responses using phrasing common in 2024, users in 2025 find it outdated or off-tone.

Adversarial adaptation is the second cause. Adversaries learn what your model does and adapt their tactics. If your model detects fraudulent transactions using patterns from 2024 fraud, fraudsters in 2025 use new patterns. If your model flags policy-violating content using language from 2024 violations, violators in 2025 use new language. Every deployed model trains its adversaries. You must assume adversaries are running their own evaluation against your model and optimizing their attacks.

Upstream data changes are the third cause. Fine-tuned models depend on features extracted from upstream systems: databases, APIs, user interfaces, and third-party data sources. When those systems change, feature distributions change. A database schema migration might alter how timestamps are formatted. An API update might change how null values are represented. A UI redesign might change how users enter information. Your model never saw these changes in training. It handles them poorly in production.

Edge case accumulation is the fourth cause. Rare events are rare in training data but common in production over time. If one in ten thousand inputs triggers a model failure, and you process one million inputs per month, you are generating one hundred failures per month. Each failure is an edge case: unusual, hard to anticipate, not well represented in training. Individually they are noise. Collectively they erode user trust.

You will not notice degradation without monitoring because gradual decline is invisible. Accuracy does not drop from 91% to 78% overnight. It drops from 91% to 90.5% in week one, to 89.8% in week two, to 89.1% in week three. Each weekly change is within noise. No single user complaint screams emergency. But over eight weeks, you have lost thirteen percentage points of accuracy and you only noticed because a customer escalated to executive leadership.

## Designing Production Evaluation Pipelines

Production evaluation pipelines run continuously, measuring model performance on live data, comparing outputs to ground truth when available, and detecting anomalies when ground truth is not available. Design these pipelines with the same rigor you applied to pre-deployment evaluation, but adapt them to production constraints: latency, cost, and partial observability.

Start with metric selection. You cannot measure everything in production. Choose metrics that matter to users and to business outcomes. For a delivery time prediction model, measure accuracy against actual delivery times, measure calibration to ensure predicted confidence matches actual confidence, and measure coverage to ensure the model is not abstaining more often than expected. For a content moderation model, measure precision and recall on sampled decisions, measure user appeals as a proxy for false positives, and measure time to decision as a performance metric.

Choose metrics that are observable in production. If ground truth requires expensive human labeling, you cannot measure every prediction. Instead, measure on samples: label one hundred random predictions per day and track metrics on that sample. If ground truth arrives delayed, measure on lagged data: evaluate today's predictions against ground truth that becomes available three days later. If ground truth never arrives for some predictions, use proxy metrics: user behavior, downstream task success, or heuristic checks.

Instrument your production system to capture the data needed for evaluation. Log every model input, every model output, every feature used in prediction, every confidence score, and every metadata field that might be relevant for slicing metrics by subgroup. Log timestamps so you can analyze metrics over time. Log user identifiers so you can analyze metrics by user cohort. Log session identifiers so you can analyze metrics by workflow context.

Build an evaluation pipeline that runs on this logged data. The pipeline ingests logs, samples predictions for labeling or ground truth matching, computes metrics, compares current metrics to baseline metrics, detects statistically significant changes, and emits alerts when thresholds are breached. This pipeline runs hourly, daily, or weekly depending on your production volume and acceptable detection latency.

One financial services company built a production evaluation pipeline for a fine-tuned model that extracted key terms from loan agreements. The pipeline sampled fifty predictions per day, sent them to a contract review specialist for ground truth labeling, computed precision and recall, and compared those metrics to the pre-deployment baseline of 93% precision and 89% recall. When precision dropped below 90% for three consecutive days, the pipeline triggered an alert to the model team. The alert included the failing examples, the metric trends over the past two weeks, and a breakdown of errors by document type. The team identified that a new loan product category introduced two weeks earlier was causing extraction failures, added examples of that product to the training data, retrained, and redeployed within five days. The pipeline caught the issue before it affected more than 300 loan applications.

## Setting Alert Thresholds: Balancing Sensitivity and Noise

Alert thresholds determine when degradation is severe enough to require intervention. Set thresholds too tight and you generate false alarms that train teams to ignore alerts. Set thresholds too loose and you miss real degradation until it is too late.

Baseline your thresholds on pre-deployment evaluation metrics. If your model achieved 91% accuracy in pre-deployment evaluation, you expect 91% accuracy in production under the same data distribution. Set your alert threshold below that baseline, accounting for statistical noise. A single day at 89% might be noise. Three consecutive days at 89% is a trend. A single day at 85% is a signal.

Use statistical significance to distinguish noise from signal. Compute confidence intervals around your production metrics based on sample size. If you are evaluating fifty predictions per day, your confidence intervals are wider than if you are evaluating five thousand. An observed accuracy of 88% with a fifty-sample evaluation might not be significantly different from a baseline of 91%. An observed accuracy of 88% with a five-thousand-sample evaluation is definitely different. Alert only when the difference is statistically significant at a chosen threshold, commonly 95% or 99% confidence.

Set tiered thresholds for tiered responses. A yellow alert triggers when metrics degrade slightly but remain within acceptable operating range: this alert goes to the model team for investigation but does not trigger emergency response. An orange alert triggers when metrics degrade enough to affect user experience but not enough to cause immediate harm: this alert triggers expedited investigation and may trigger increased human review until the issue is resolved. A red alert triggers when metrics degrade to levels that violate service level agreements, create safety risks, or cause regulatory violations: this alert triggers immediate model rollback or circuit-breaker shutdown until the issue is resolved.

One healthcare technology company set tiered thresholds for a fine-tuned model that flagged lab results requiring urgent physician review. Baseline sensitivity was 97.5%, meaning the model caught 97.5% of truly urgent results. Yellow alert triggered at 96%, orange at 94%, and red at 92%. Yellow alerts happened monthly due to normal statistical variation and prompted review of recent failing cases. Orange alerts happened twice in the first year, both due to new lab test types being introduced, and prompted temporary increase in human review rates while the model was retrained. Red alert never triggered, but if it had, the protocol was to route all lab results to human review and disable the model until root cause was identified and fixed.

Adjust thresholds over time based on observed alert frequency and team response capacity. If you are generating yellow alerts daily, your threshold is too tight. If you go six months without any alerts while anecdotal evidence suggests quality issues, your threshold is too loose. Review alert history quarterly and recalibrate.

## Retraining Triggers and the Feedback Loop

Continuous evaluation detects degradation. Retraining fixes it. But retraining is expensive: it requires new labeled data, compute resources, re-evaluation, and redeployment risk. You cannot retrain every time a metric dips slightly. You need clear triggers that justify the cost and risk of retraining.

The simplest retraining trigger is metric-based: retrain when production metrics fall below a threshold for a sustained period. If your orange alert threshold is 94% sensitivity and you remain below that threshold for two weeks despite investigation and runtime mitigations, retrain. The sustained period requirement prevents retraining in response to temporary anomalies that resolve on their own.

The second trigger is data-based: retrain when you have accumulated sufficient new labeled data from production to meaningfully improve the model. If you are labeling fifty production predictions per day for continuous evaluation, you accumulate 1,500 labeled examples per month. If your original training set was 10,000 examples, adding 1,500 new examples represents a fifteen percent increase. That might justify retraining. If your original training set was 100,000 examples, 1,500 new examples is noise. Wait until you have accumulated a meaningful proportion of your original training data, commonly ten to twenty percent, before retraining.

The third trigger is drift-based: retrain when distributional drift detection indicates that production data no longer resembles training data. Measure drift using techniques like KL divergence, population stability index, or adversarial accuracy: train a classifier to distinguish training data from production data, and if that classifier achieves high accuracy, your distributions have drifted. Drift detection provides early warning before metrics degrade, allowing proactive retraining.

The fourth trigger is event-based: retrain when external events change the operating environment. A new regulation that changes what content is prohibited. A new product launch that introduces new categories. A seasonal shift that changes user behavior. A competitor action that changes market dynamics. Event-based triggers require human judgment. Not every event justifies retraining, but major events that clearly affect the task distribution do.

Design a feedback loop that uses production data to improve future models. Every production prediction is potential training data. Every human correction of a model error is labeled data. Every user appeal of a model decision is a signal that the model might be wrong. Capture this feedback systematically, review it for quality, and incorporate it into retraining.

One content moderation platform captured every user appeal, every moderator override of a model decision, and every edge case escalated to policy review. They labeled these examples with ground truth, analyzed them for patterns, and used them to retrain quarterly. Each retraining cycle incorporated roughly 5,000 new examples derived from production feedback. Over two years, this feedback loop increased precision from 89% to 94% and recall from 86% to 91%, while simultaneously reducing appeals by 38%. The model learned from its mistakes because the team built infrastructure to learn from production.

## Monitoring Safety and Bias in Production

Performance metrics like accuracy and precision measure whether the model is doing its job. Safety and bias metrics measure whether the model is doing harm. Monitor both.

Safety monitoring detects when the model produces outputs that violate safety policies, even if those outputs are technically accurate. A model that generates accurate but insensitive responses to mental health questions is failing on safety. A model that generates correct legal summaries but omits critical disclaimers is failing on safety. A model that accurately predicts loan default risk but uses prohibited features like race is failing on safety.

Instrument safety monitoring by running every model output through safety classifiers or heuristic checks before showing it to users. If the model generates text, run toxicity classifiers, personally identifiable information detectors, and policy violation detectors. If the model makes decisions, check that decisions comply with fairness constraints and regulatory requirements. Log every safety flag, track safety flag rates over time, and alert when rates exceed thresholds.

Bias monitoring detects when model performance differs across demographic groups or protected classes. Measure metrics separately for each group: accuracy for men versus women, precision for majority versus minority applicants, false positive rate for young versus old users. Compare group metrics to overall metrics and to each other. Alert when disparities exceed fairness thresholds defined during pre-deployment evaluation.

Bias monitoring requires demographic labels, which you often do not have. Use proxy methods: infer demographics from names, locations, or interaction patterns, acknowledging that proxies are imperfect. Use surveys or opt-in self-reporting to gather demographic data from a subset of users and extrapolate. Use external audits where third-party auditors with access to demographic data measure bias on your behalf.

One hiring platform monitored bias by measuring interview invitation rates for applicants with names statistically associated with different demographic groups. They did not have applicant demographic data directly, but research shows that names are strong proxies. They measured invitation rates weekly, compared rates across name-inferred groups, and tracked disparity metrics. When invitation rates for one inferred group dropped significantly over three weeks, they investigated and discovered that a recent training data update had inadvertently included biased historical hiring decisions from a legacy system. They removed that data, retrained, and restored fairness.

## The Observability Stack for Continuous Evaluation

Continuous evaluation requires infrastructure: logging, storage, compute, alerting, dashboards, and integration with incident response. Build this infrastructure as a first-class system, not as an afterthought.

Logging must be comprehensive, structured, and performant. Log every prediction with sufficient context to reconstruct why the model made that decision: input features, intermediate representations if available, output, confidence, timestamp, user identifier, session identifier, and any metadata. Use structured logging formats like JSON so logs can be parsed and analyzed programmatically. Ensure logging does not add unacceptable latency to production predictions: log asynchronously, batch log writes, and use efficient serialization.

Storage must handle production scale. If you process one million predictions per day and each logged prediction is one kilobyte, you generate one gigabyte of logs per day, thirty gigabytes per month, 365 gigabytes per year. Plan storage capacity accordingly. Use cheap storage for raw logs and compress or sample older logs to reduce costs. Retain logs long enough to support retraining: if you retrain quarterly, retain at least four months of logs.

Compute must run evaluation pipelines efficiently. If you evaluate one hundred sampled predictions per day, you need compute to retrieve those predictions from logs, fetch or generate ground truth, compute metrics, and update dashboards. This compute can run in batch overnight. If you evaluate in near real-time, you need streaming compute that processes logs as they arrive. Choose your architecture based on acceptable detection latency: batch is cheaper, streaming is faster.

Alerting must integrate with your incident response system. When an alert triggers, it should create a ticket, page the on-call engineer, or post to a Slack channel depending on severity. Include context in alerts: which metric degraded, by how much, over what time period, for which user segments, with which example failures. An alert that says "accuracy dropped" is useless. An alert that says "accuracy dropped from 91% to 87% over the past three days, driven by failures on new product category X, examples attached" is actionable.

Dashboards must make metrics visible to the team. Build dashboards that show current metric values, trends over time, breakdowns by segment, and comparisons to baselines. Update dashboards daily or hourly. Make them accessible to engineers, product managers, and leadership. Dashboards prevent surprises: when everyone can see metrics trending downward, no one is shocked when an alert triggers.

## The Retraining Cadence and Continuous Improvement

Continuous evaluation feeds continuous improvement. Models should not be static artifacts deployed once and forgotten. They should evolve as the world evolves, incorporating new data, adapting to new patterns, and improving over time.

Establish a regular retraining cadence even in the absence of alerts. Monthly, quarterly, or semi-annual retraining ensures the model stays fresh. Each retraining cycle incorporates production data collected since the last cycle, addresses edge cases discovered in production, and benefits from improvements to the base model or fine-tuning techniques.

One legal technology company retrained their contract analysis model quarterly. Each quarter, they incorporated 3,000 to 5,000 new contracts processed in production, along with corrections made by legal reviewers. They also updated to the latest base model version if a new version had been released, and they experimented with hyperparameter adjustments or architectural improvements. Over three years and twelve retraining cycles, their model improved from 88% accuracy to 96%, while simultaneously expanding to cover eight new contract types that did not exist when the first version was deployed.

Treat each retraining cycle as an opportunity to improve not just the model but the entire system. Review continuous evaluation metrics to identify persistent failure modes. Review user feedback to identify unmet needs. Review red team findings to identify new vulnerabilities. Incorporate all of these insights into the next training data collection, the next model architecture, and the next evaluation plan.

Document the results of each retraining cycle. Track metrics before and after retraining. Track the volume and sources of new training data. Track what changed in the model or pipeline. Track any degradation or regression introduced by retraining. This documentation creates an institutional memory that helps future model teams avoid repeating mistakes and build on successes.

## When to Retire a Fine-Tuned Model

Continuous evaluation sometimes reveals that a model cannot be saved. Retraining does not fix the underlying issue. Mitigations are too expensive or too brittle. The world has changed in ways that invalidate the original task framing. When this happens, retire the model.

Retirement criteria include sustained metric degradation that retraining does not fix, fundamental shifts in task requirements that require reframing rather than retraining, accumulation of technical debt that makes the model unmaintainable, or strategic decisions to change product direction. Retirement is not failure. It is recognition that the model served its purpose and is no longer the right solution.

Plan for retirement from the beginning. Build circuit breakers that can disable the model safely. Build fallback systems that handle the task when the model is not available. Build communication plans to inform users and stakeholders when a model is retired. Build data export processes so you can extract value from the retired model's logs and training data for future efforts.

One customer support platform retired a fine-tuned model that routed support tickets after two years of operation. Continuous evaluation showed that ticket categories had changed so much that the original category taxonomy was obsolete. Retraining on the old taxonomy was pointless. They reframed the task, collected new training data with a revised taxonomy, and built a new model from scratch. The old model was retired gracefully: over a four-week transition, they gradually shifted traffic from the old model to the new model, monitored for issues, and finally decommissioned the old model. The retirement was smooth because they had planned for it.

Evaluation does not stop at deployment. It intensifies. Production is where models encounter the real world, where distributions drift, where adversaries adapt, and where edge cases accumulate. Continuous evaluation, continuous monitoring, and continuous improvement are not optional. They are the only way fine-tuned models survive in production. The next subchapter addresses the infrastructure and tooling required to run evaluations efficiently and at scale as the number of fine-tuned models grows.
