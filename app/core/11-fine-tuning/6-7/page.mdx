# 6.7 â€” Domain Vocabulary Injection: Continued Pre-Training for Specialized Terminology

**Vocabulary is infrastructure.** You cannot teach a model to reason about concepts using words it barely understands. You cannot expect precise outputs when the model lacks precise language. This principle is obvious in theory, ignored in practice, and costly when it fails. Teams fine-tune models on domain-specific tasks and wonder why outputs sound imprecise, why the model paraphrases technical terms instead of using them directly, why domain experts describe the results as "technically accurate but not how we would say it." The answer is almost always the same: the base model's vocabulary representations for your domain are weak, and supervised fine-tuning cannot fix weak vocabulary. You need continued pre-training, a dedicated training phase between base pretraining and task-specific fine-tuning, where the model reads vast quantities of domain text for the sole purpose of strengthening its internal representations of domain terminology. This is vocabulary injection, and it is the foundation that makes specialized reasoning possible.

## The Vocabulary Gap in Base Models

Base models are pre-trained on massive internet-scale corpora. They learn common language, general knowledge, and broad reasoning patterns. But they encounter domain-specific terminology unevenly. Medical models see "hypertension" thousands of times, so they learn it well. But they might see "pembrolizumab" only a handful of times, not enough to build strong representations. Legal models see "contract" everywhere, but "force majeure" appears far less frequently. Financial models know "stock" but struggle with "collateralized debt obligation tranche."

The problem compounds with acronyms, abbreviations, and specialized notation. A model might see "MRI" and learn it refers to imaging, but it never learns that "T1-weighted FLAIR sequence" is a specific MRI protocol. It knows "LLC" is a business entity, but it does not know "Del. C. section 102(b)(7)" refers to a specific Delaware corporate statute. The base vocabulary is biased toward common usage.

Fine-tuning on task examples cannot fix this. Fine-tuning adjusts the model's reasoning and output formatting by showing it input-output pairs. But if the model has weak representations for domain terms, fine-tuning cannot strengthen them. You are trying to teach advanced reasoning on a foundation of shallow vocabulary. The model will learn to avoid terms it does not understand well, substituting paraphrases or generic language. This is why domain-adapted models often sound less precise than human experts: they lack the vocabulary to be precise.

Continued pre-training solves this by exposing the model to large volumes of domain text before fine-tuning. You are not teaching task behavior yet. You are simply letting the model read domain corpora to build stronger representations of domain vocabulary. This is vocabulary injection: expanding the model's working lexicon so that fine-tuning can teach precise reasoning using precise language.

## What Continued Pre-Training Teaches

Continued pre-training, also called domain-adaptive pre-training or CPT, is a training phase that sits between base pre-training and supervised fine-tuning. You take a pre-trained base model and continue training it on domain-specific corpora using the same language modeling objective: predict the next token. The model reads medical textbooks, legal case law, financial filings, or code repositories. It does not learn task-specific behavior. It learns vocabulary, co-occurrence patterns, and domain context.

CPT teaches the model that certain terms appear together frequently. In medical corpora, "pembrolizumab" appears near "PD-1 inhibitor," "metastatic melanoma," and "immune checkpoint blockade." The model builds associations between these terms. In legal corpora, "force majeure" appears near "act of God," "unforeseen circumstances," and "contract performance." The model learns these are related concepts. In financial corpora, "tranche" appears near "securitization," "credit rating," and "waterfall structure." The model builds context.

CPT also teaches the model domain-specific meanings of common words. In general language, "material" means fabric or substance. In legal language, "material" means significant or relevant to a decision. In medical language, "material" might refer to tissue samples. The model learns these domain-specific senses by seeing the word used in domain contexts repeatedly. Without CPT, the model defaults to the most common sense, which is often the general-purpose meaning, not the domain meaning.

CPT teaches abbreviation expansions and acronym meanings. The model sees "SNP" expanded as "single nucleotide polymorphism" in genomics papers. It sees "LLC" expanded as "limited liability company" in business documents. It sees "MRI" expanded as "magnetic resonance imaging" in radiology reports. The model learns these mappings implicitly by seeing both forms used interchangeably in context.

CPT does not teach task behavior. After CPT, the model is better at continuing domain text, but it is not better at performing domain tasks. You still need supervised fine-tuning for that. But CPT creates a vocabulary foundation that makes fine-tuning far more effective. The model can now learn to reason using precise domain terms instead of working around vocabulary gaps with paraphrases.

## Corpus Requirements for Continued Pre-Training

The quality and size of your CPT corpus determine how much vocabulary the model acquires. You need domain text that is representative, diverse, and large enough to cover the vocabulary you care about. For medical CPT, you might use PubMed abstracts, clinical practice guidelines, medical textbooks, and electronic health record notes. For legal CPT, you use case law databases, statutes, contracts, and legal treatises. For financial CPT, you use SEC filings, earnings transcripts, analyst reports, and financial news. For code CPT, you use GitHub repositories filtered by language and quality.

The corpus should cover the breadth of the domain. If you are building a radiology model, your corpus should include text from all imaging modalities: X-ray, CT, MRI, ultrasound, PET. If you are building a contract analysis model, your corpus should include multiple contract types: NDAs, employment agreements, licensing agreements, purchase agreements. Narrow corpora teach narrow vocabulary. Broad corpora teach broad vocabulary.

The corpus should be deduplicated and cleaned. CPT training is expensive, and you do not want to waste compute on redundant text. Remove exact duplicates. Remove near-duplicates using MinHash or embedding-based deduplication. Remove low-quality text: garbled OCR output, machine-translated nonsense, spam. CPT training amplifies whatever is in the corpus, so garbage in means garbage representations out.

The corpus size depends on the domain and the base model size. For small domains with limited vocabulary, 100 million to 500 million tokens might suffice. For large domains with extensive vocabulary, you need 1 billion to 10 billion tokens. Medical and legal domains are large: you need billions of tokens. Code domains vary by language: Python CPT needs less than C++ CPT because Python corpora are more homogeneous. Financial domains are moderate: 500 million to 2 billion tokens often suffice.

You can mix domain corpora with general corpora to prevent catastrophic forgetting. If you train only on domain text, the model might lose general language ability. Mixing 80 percent domain text with 20 percent general text during CPT preserves general capabilities while injecting domain vocabulary. Some teams skip this and accept some general capability loss, planning to recover it during fine-tuning. The trade-off depends on whether your application needs strong general language ability or can focus purely on domain tasks.

## Training Duration and Compute Cost

CPT training duration is measured in tokens processed, not epochs. You typically train for one to three epochs over your corpus, meaning the model sees each token one to three times. Training for more epochs risks overfitting: the model memorizes corpus text instead of learning generalizable vocabulary patterns. One epoch is often sufficient for vocabulary acquisition. Two epochs can improve rare term representations. Three epochs rarely helps and often hurts.

The compute cost depends on model size and corpus size. Continued pre-training a 7B parameter model on 1 billion tokens for one epoch requires roughly the same compute as training that model from scratch on 1 billion tokens. For a 7B model, this might cost 500 to 2,000 dollars on cloud GPUs, depending on hardware and efficiency. For a 13B model, double that. For a 70B model, multiply by ten. CPT is cheaper than full pre-training but far more expensive than fine-tuning.

You can reduce cost by using smaller models or smaller corpora. If vocabulary injection is your only goal, you do not need billions of parameters. A 7B model can learn domain vocabulary nearly as well as a 70B model, because vocabulary learning is less dependent on scale than reasoning ability. You also do not need to process the entire internet. A well-curated 1 billion token domain corpus teaches more useful vocabulary than a poorly curated 10 billion token corpus.

Some teams use LoRA or other parameter-efficient methods for CPT. Standard CPT updates all model parameters. LoRA CPT updates only low-rank adapter matrices, reducing memory and compute requirements. LoRA CPT is faster and cheaper, but it teaches vocabulary less effectively because it has less capacity to adjust token embeddings. Full-parameter CPT is preferred when vocabulary injection is the primary goal. LoRA CPT is acceptable when you are doing light domain adaptation on a model that already has partial domain vocabulary.

Training stability during CPT is generally good because you are starting from a converged base model. Learning rates should be lower than initial pre-training rates: typically one-tenth to one-fifth of the base pre-training rate. You are making small adjustments to an already-trained model, not learning from scratch. Too high a learning rate destabilizes the model. Too low a learning rate wastes compute without teaching much. Standard CPT learning rates range from 1e-5 to 5e-5 for AdamW optimizer.

## Measuring Vocabulary Acquisition

You need metrics to confirm that CPT actually taught the model domain vocabulary. The simplest metric is perplexity on held-out domain text. After CPT, evaluate the model on domain text it has not seen. Perplexity measures how surprised the model is by each token. Lower perplexity means the model predicts domain text more accurately, which implies better vocabulary and context understanding. Compare pre-CPT perplexity to post-CPT perplexity on the same held-out set. A 20 to 40 percent perplexity reduction is typical for successful CPT.

Perplexity is a necessary metric but not sufficient. A model can achieve low perplexity by memorizing common phrases without learning vocabulary deeply. You also need vocabulary-specific metrics. One approach is term prediction accuracy: mask domain-specific terms in test sentences and measure how often the model predicts the correct term. For example, mask "pembrolizumab" in a sentence like "The patient was treated with blank for metastatic melanoma." Measure whether the model's top prediction is "pembrolizumab" or a related immunotherapy drug. High accuracy means strong term representations.

Another approach is embedding quality measurement. Extract embeddings for domain terms before and after CPT. Measure whether related terms cluster together in embedding space. For example, "pembrolizumab," "nivolumab," and "atezolizumab" are all PD-1 or PD-L1 inhibitors. Their embeddings should be close together. After CPT, measure the average cosine similarity between related term embeddings. Higher similarity means better domain clustering.

You can also measure synonym and paraphrase behavior. Generate text from the model and count how often it uses precise domain terms versus paraphrases. Before CPT, a model might write "a medication that blocks PD-1" instead of "pembrolizumab." After CPT, it should write "pembrolizumab" directly. Sample 100 generated outputs and manually count term usage versus paraphrase usage. Term usage should increase substantially after CPT.

Domain experts can provide qualitative validation. Show them text generated by the pre-CPT model and the post-CPT model. Ask them to rate which text sounds more like domain expert language. This is subjective but valuable. Experts can identify subtle vocabulary improvements that automated metrics miss. They can tell you whether the model now uses terms the way experts use them, not just whether it uses the terms at all.

## The CPT-Then-SFT Pipeline for Domain Adaptation

The standard domain adaptation pipeline has three stages: base pre-training, continued pre-training, and supervised fine-tuning. The base model is pre-trained by the model provider on general corpora. You cannot control this stage unless you are training from scratch. You take the base model as-is. Then you run CPT on domain corpora to inject vocabulary. Then you run SFT on task-specific examples to teach task behavior. This is the CPT-then-SFT pipeline.

CPT happens first because vocabulary must exist before task learning can use it. If you skip CPT and go straight to SFT, the model learns task structure but struggles with domain language. It writes grammatically correct outputs that sound imprecise or awkward to domain experts. If you run CPT after SFT, you risk degrading task performance because CPT is not optimized for task objectives. The model might forget task formatting while learning vocabulary. CPT-then-SFT is the correct order.

Some teams run multiple rounds of CPT as the domain evolves. Medical models might get annual CPT updates as new drug names and treatment protocols enter the literature. Legal models might get quarterly CPT updates as new case law and statutes are published. Financial models might get monthly CPT updates as new companies and financial instruments appear. Each CPT round processes only the new domain text, not the entire corpus again. This keeps vocabulary current without full retraining.

After CPT, you proceed to SFT as usual. The SFT process is unchanged: you train on task-specific input-output pairs to teach the model how to perform your task. But now the model has strong vocabulary representations, so SFT can teach precise reasoning using precise terms. The model learns to generate "pembrolizumab" confidently instead of hedging with paraphrases. It learns to use "force majeure" in legal outputs instead of writing "unforeseeable events." It learns to write "tranche" in financial outputs instead of "portion."

The CPT-then-SFT pipeline is more expensive than SFT alone, but it produces higher-quality domain models. The incremental cost is justified when domain precision matters. For consumer applications where approximate language is acceptable, you can skip CPT. For professional applications where experts expect precise terminology, CPT is not optional. It is the difference between a model that sounds like an informed generalist and a model that sounds like a domain expert.

## When to Skip Continued Pre-Training

CPT is not always necessary. If the base model already has strong domain vocabulary, SFT alone might suffice. GPT-5 and Claude Opus 4.5 have been exposed to enormous amounts of medical, legal, and financial text during base pre-training. They already know most common domain terms. CPT adds value only if your domain has specialized vocabulary that rarely appears in internet-scale corpora.

Highly specialized subdomains benefit most from CPT. Genomics, radiology, patent law, structured finance, and embedded systems code all have extensive jargon that general models struggle with. Broader domains like general medicine, contract law, and equity trading have vocabulary that general models mostly know already. The more niche the domain, the more CPT helps.

If your fine-tuning dataset is very large, it can serve double duty as a CPT corpus. Some teams run CPT using the same text they will later use for SFT, but with different training objectives. CPT trains on next-token prediction over the full text. SFT trains on input-output pairs extracted from that text. This works when your SFT dataset contains rich domain language in both inputs and outputs. It does not work when your SFT dataset is narrow or synthetic.

If your application uses retrieval-augmented generation, CPT is less critical. RAG systems inject domain context into every prompt, so the model does not need to know domain vocabulary from memory. It reads the vocabulary in the provided context. CPT still helps the model understand the context better, but the marginal benefit is smaller than for non-RAG systems. Some teams skip CPT for RAG applications and invest the compute in better retrieval instead.

## Vocabulary Injection as a Foundational Step

Continued pre-training is infrastructure work. It does not directly improve task performance. It creates the conditions for task performance to improve. This makes it easy to skip, especially under deadline pressure. Teams want to see task accuracy improve immediately, and SFT delivers that. CPT feels like a detour. But the detour is necessary when domain precision matters.

Vocabulary is foundational. You cannot teach a model to reason precisely using words it barely understands. You cannot teach it to generate expert-level outputs when it lacks expert-level vocabulary. CPT builds that vocabulary foundation, and SFT builds task capability on top of it. Skipping CPT means building on sand. The model might pass your initial evals, but it will sound imprecise to domain experts, and imprecision erodes trust. CPT is the difference between a model that works and a model that works like an expert.

The next step is ensuring that domain-adapted outputs are actually correct according to domain standards, which requires working with subject matter experts to validate model behavior.

