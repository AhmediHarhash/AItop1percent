# 3.6 â€” Self-Instruct, Evol-Instruct, and Structured Generation Techniques

In March 2025, a legal technology company needed to fine-tune a model to help lawyers draft discovery requests. They had 800 real examples from past cases, which was not enough for effective fine-tuning. They considered hiring legal experts to write more examples, but the cost was prohibitive: senior attorneys billed $400 per hour, and writing a high-quality training example took 45 minutes on average. To generate 10,000 examples, they would spend $3 million. Instead, they turned to synthetic data generation. But they did not want to simply prompt GPT-4o 10,000 times with variations of "write a discovery request." That approach would produce homogeneous outputs and miss the complexity and variation present in real legal work. They needed a systematic method to generate diverse, high-quality instruction-response pairs at scale. They chose to implement Self-Instruct, a technique where the model generates its own training tasks from a small seed set, combined with Evol-Instruct, which iteratively increases task complexity. Over six weeks, they generated 12,000 synthetic examples at a cost of $18,000 in API calls. The fine-tuned model performed comparably to one trained on expert-written data in blind evaluation by three senior litigators, and it generalized better to edge cases because the synthetic dataset covered scenarios the original 800 examples did not include.

This is the promise of systematic synthetic generation techniques. You do not rely on manual prompt engineering or simple template expansion. You use algorithms that automatically create diverse instructions, evolve them to increase difficulty and variety, and structure the generation process to ensure coverage of the task space. These techniques have matured significantly by 2026. Self-Instruct, introduced in research in 2022, is now a standard production method. Evol-Instruct, popularized by the WizardLM and WizardCoder projects in 2023 and 2024, is widely used for creating challenging reasoning and coding datasets. Structured generation techniques, enabled by better model APIs and schema-guided decoding, provide fine-grained control over output format and content. Understanding when and how to use each technique is essential for any team building fine-tuned models with synthetic data.

## Self-Instruct: Bootstrapping from Seed Tasks

Self-Instruct is a method where you start with a small set of seed tasks, then use a model to generate new tasks that are similar but distinct. The model creates both the instruction and the response, effectively teaching itself by expanding the task space. The original Self-Instruct paper used 175 seed tasks to generate tens of thousands of new tasks. The core idea is that a capable model can imagine plausible variations and extensions of existing tasks without human guidance for each one.

The process has four steps. First, you provide a small seed set of instruction-response pairs. These seeds should cover the major categories of tasks you want your fine-tuned model to handle. For the legal tech company, the seed set was 80 discovery request examples spanning different case types: contract disputes, employment law, intellectual property, personal injury, and securities litigation. Second, you sample a subset of the seed tasks and prompt the teacher model to generate new instructions that are similar in style and domain but different in specifics. The prompt might say: "Here are three example legal discovery requests. Generate five new discovery request instructions that cover different scenarios or legal issues, but follow the same format and complexity level." Third, the model generates responses for those new instructions. You can use the same model or a different one. Fourth, you filter the generated instruction-response pairs for quality and add the high-quality ones back to the pool. You then repeat steps two through four, sampling from the growing pool each time, until you reach your target dataset size.

The legal tech company used GPT-4o as the teacher model. They ran 150 iterations, each generating 80 new instruction-response pairs. They applied filtering after each iteration: they used a separate GPT-4o call to evaluate each generated example for legal accuracy, instruction clarity, and response completeness. Examples that scored below 0.8 on any dimension were discarded. After filtering, they added the passing examples to the pool and sampled from the updated pool in the next iteration. This iterative expansion process produced 12,000 high-quality examples from 80 seeds.

Self-Instruct works because language models are good at generating variations on themes. If you show the model examples of discovery requests about employment discrimination, it can generate discovery requests about wrongful termination, hostile work environment, wage and hour disputes, and retaliation claims. If you show it examples requesting documents, it can generate examples requesting interrogatory answers, admissions, or depositions. The model leverages its broad pretraining knowledge to instantiate new scenarios that fit the task pattern.

The main risk is drift. As you iterate, the generated tasks can drift away from the original intent or toward the model's biases. If your seed set includes mostly straightforward scenarios, the model might generate mostly straightforward scenarios, missing complex edge cases. The legal tech company addressed this by periodically injecting new seed examples that represented underrepresented case types or complexity levels. After iteration 50, they noticed that most generated examples were document requests and very few were interrogatories or requests for admission. They manually added 20 seed examples of interrogatories and requests for admission, which redirected the generation process toward better balance.

You also control drift by regularly sampling and reviewing the generated pool. If you see quality degrading or homogeneity increasing, you pause generation, analyze the problem, adjust your seed set or prompts, and restart. Self-Instruct is not fully automatic; it requires ongoing monitoring and correction.

## Evol-Instruct: Evolving Complexity and Depth

Evol-Instruct is a refinement of Self-Instruct that focuses on increasing task complexity through iterative evolution. Instead of just generating new tasks similar to the seeds, you explicitly prompt the model to make existing tasks harder, more detailed, more nuanced, or more realistic. The result is a dataset that includes not only breadth across scenarios but also depth in difficulty and sophistication.

The evolution process uses several mutation operators. Deepening adds more constraints, more steps, or more requirements to an instruction. Breadening expands the scope or context of an instruction. Concretizing makes an abstract instruction more specific. Complicating adds conditional logic, exceptions, or multi-part requirements. You apply these operators iteratively: start with a simple seed instruction, apply one evolution operator to create a harder version, apply another operator to create an even harder version, and so on. Each evolution step is a new training example.

A financial services company used Evol-Instruct to generate training data for a model that helps compliance officers draft regulatory filings. They started with a simple seed: "Draft a summary of a Form 10-K risk factors section for a software company." They applied deepening: "Draft a summary of a Form 10-K risk factors section for a software company that recently acquired a competitor and is expanding into international markets." They applied complicating: "Draft a summary of a Form 10-K risk factors section for a software company that recently acquired a competitor and is expanding into international markets, ensuring you address cybersecurity risks, data privacy regulations in the EU, and integration risks from the acquisition." Each evolution produced a new instruction that was valid and useful but more complex than the previous version.

Evol-Instruct generates challenging examples that stress-test the model's capabilities. Simple examples teach basic patterns. Complex examples teach nuanced reasoning, handling of constraints, and robustness to difficult inputs. The financial services company found that a model fine-tuned on evolved instructions performed much better on real-world edge cases than a model fine-tuned on only simple instructions, even when both models were trained on the same number of examples.

The evolution process is not unlimited. After several evolution steps, instructions can become unrealistic, overly complicated, or incoherent. The financial services company found that after four or five evolution steps, the instructions started to include so many constraints that no real user would ever request them. They capped evolution at three steps per seed and manually reviewed any examples that had been evolved more than twice.

You also diversify the evolution paths. Instead of evolving every seed in the same way, you randomly select which evolution operator to apply at each step. This creates a branching tree of instructions from each seed, with different branches exploring different dimensions of complexity. The financial services company used a randomized approach: for each seed, they generated five evolution branches, each applying a different sequence of operators. This produced a dataset with wide variation in how complexity manifested.

Evol-Instruct is particularly effective for tasks that require reasoning, planning, or handling of complex constraints. It is less useful for simple classification or extraction tasks where complexity does not add value. A customer support ticket classification task does not benefit much from evolved instructions; the task is inherently straightforward. But a customer support response generation task benefits significantly because evolved instructions can represent difficult scenarios like multiple interacting issues, conflicting policies, or emotionally charged language.

## Structured Generation with Templates and Constraints

Structured generation is the practice of using templates, schemas, or constraints to guide the model's output during synthetic data creation. Instead of asking the model to generate freeform responses, you specify the structure and let the model fill in the content. This approach ensures consistency, coverage of required fields, and compatibility with downstream systems.

A healthcare AI company used structured generation to create training data for a clinical documentation assistant. They defined a schema for clinical notes that included required sections: chief complaint, history of present illness, review of systems, physical examination, assessment, and plan. They prompted GPT-4o to generate notes that followed this schema, providing section headers and instructing the model to populate each section with realistic content based on a described patient scenario. This structured approach ensured that every generated training example included all required sections, making the dataset directly usable for fine-tuning a model that needed to produce compliant clinical notes.

Structured generation also applies to the instruction side. Instead of generating freeform instructions, you use templates with variable slots. A template might be: "Write a clinical note for a patient presenting with SYMPTOM, with a history of CONDITION, currently taking MEDICATION." You generate instructions by sampling values for SYMPTOM, CONDITION, and MEDICATION from a predefined list or by prompting the model to suggest plausible values. This template-based approach ensures that your instruction set covers the desired combinations of variables and avoids gaps.

The healthcare company combined template-based instruction generation with structured response generation. They created 50 instruction templates covering different clinical scenarios: new patient visits, follow-up visits, urgent care visits, telemedicine visits, and specialty consultations. For each template, they generated 200 instruction instances by filling the variable slots with different symptoms, conditions, and medications. Then they generated structured responses for each instruction using the clinical note schema. This produced 10,000 training examples with guaranteed coverage of scenario types and consistent response structure.

Structured generation is especially valuable when your task output must conform to a specific format. If you are fine-tuning a model to generate JSON, XML, or other structured data, you use schema-guided generation to ensure syntactic correctness. In 2026, most major model APIs support some form of structured output, either through native schema support like OpenAI's JSON mode and function calling, or through constrained decoding libraries like Outlines or Guidance. These tools let you specify a schema and guarantee that the model's output will parse correctly.

A logistics company used structured generation to create training data for a model that generates shipping manifests in JSON format. They defined a JSON schema with required fields for shipment ID, origin, destination, item list, weight, carrier, and tracking number. They used OpenAI's JSON mode to generate synthetic manifests that strictly conformed to the schema. This eliminated the need for post-generation validation and parsing, and it ensured that the fine-tuned model learned to produce valid JSON from the start.

The tradeoff with structured generation is reduced creativity and naturalness. When you constrain the model to follow a strict template or schema, you lose some of the variation and fluency that makes synthetic data feel realistic. The healthcare company found that their structured clinical notes were accurate and complete, but they sounded somewhat formulaic compared to real physician-written notes, which often have informal asides, non-standard section orders, and personal style. They addressed this by adding a post-generation step where they prompted the model to rephrase sections in a more natural style while preserving the content and structure.

You balance structure and flexibility by using partial constraints. Instead of fully specifying the output format, you specify key requirements and leave other aspects open. For example, require that a clinical note includes specific sections, but allow the model to choose the order, length, and phrasing within each section. Require that a legal document includes certain clauses, but allow variation in how those clauses are expressed. This hybrid approach gives you the coverage and consistency benefits of structured generation while retaining some of the naturalness and diversity benefits of freeform generation.

## Combining Techniques for Maximum Coverage

The most effective synthetic data generation strategies combine multiple techniques. You use Self-Instruct to expand breadth from seed examples, Evol-Instruct to add depth and complexity, and structured generation to ensure consistency and format compliance. The legal tech company's final approach used all three. They started with 80 seed examples. They applied Self-Instruct to generate 4,000 new instructions covering diverse legal scenarios. They selected 1,000 of those instructions and applied Evol-Instruct to create 3,000 more complex versions. They used structured generation to ensure that all responses included required sections like statement of purpose, document categories requested, time frame, and relevance explanation. The resulting 12,000 examples had broad scenario coverage from Self-Instruct, challenging complexity from Evol-Instruct, and consistent structure from structured generation.

Another combination strategy is to use different techniques for different portions of your dataset. Use structured generation for 30% of examples where format compliance is critical. Use Self-Instruct for 40% of examples to cover the breadth of scenarios. Use Evol-Instruct for 30% of examples to ensure the model learns to handle difficult cases. This partitioned approach lets you optimize each subset for a different goal and combine them into a balanced final dataset.

You also sequence techniques. Start with structured generation to create a base dataset with consistent formatting. Then apply Self-Instruct to expand scenario coverage within that structure. Then apply Evol-Instruct to a subset of the expanded examples to add complexity. This sequential pipeline builds up from simple, well-formed examples to complex, challenging ones while maintaining format consistency throughout.

The financial services company used a sequential approach. They generated 5,000 base examples using structured generation with fixed templates. They applied Self-Instruct to generate 5,000 additional examples that varied the content while preserving the template structure. They selected 2,000 of the Self-Instruct examples and evolved them using Evol-Instruct to create harder versions. The final 12,000-example dataset had a foundation of clean, consistent examples, a middle layer of diverse scenario coverage, and a top layer of challenging edge cases.

Combining techniques requires more complex orchestration and higher generation costs, but it produces datasets that are more robust and comprehensive than any single technique alone. The investment pays off in better fine-tuned model performance, especially on out-of-distribution test cases and real-world edge cases.

## Practical Implementation in 2026

Implementing these techniques in 2026 is straightforward because the major model providers support the necessary API features and because open-source tooling has matured. For Self-Instruct, you need an API that supports prompt-based generation with reasonable cost and latency. GPT-4o, GPT-4.5, Claude 3.5 Sonnet, Claude Opus 4.5, and Gemini 2 all work well. You write a script that samples from your seed set, constructs prompts, calls the API, parses responses, applies filtering, and iterates. A typical implementation is a few hundred lines of Python using the OpenAI or Anthropic SDK.

For Evol-Instruct, you need the same API capabilities plus a set of evolution prompts. You predefine prompts for each evolution operator: deepening, breadening, concretizing, complicating. Your script selects an instruction, randomly selects an evolution operator, applies the corresponding prompt, generates the evolved instruction, generates a response for it, filters, and iterates. The legal tech company's Evol-Instruct script was 300 lines of Python and used a JSON config file to define the evolution operators and their associated prompts.

For structured generation, you use API features like OpenAI's JSON mode or function calling, Anthropic's tool use, or constrained decoding libraries like Outlines. JSON mode is the simplest: you provide a schema and the API guarantees that the output is valid JSON conforming to that schema. Function calling lets you define a function signature and the model generates arguments for that function, which you then convert to your desired output format. Constrained decoding libraries work with open models and give you even finer control over output structure, including support for regular expressions, context-free grammars, and custom constraints.

The logistics company used OpenAI's JSON mode. They defined their manifest schema as a JSON Schema document, set the response format to JSON mode, and generated 10,000 manifests. Every output was valid JSON that parsed correctly. The healthcare company used Anthropic's tool use feature. They defined their clinical note structure as a tool schema with sections as parameters, prompted Claude to call the tool with realistic content, and extracted the parameters as the generated note.

You also use orchestration frameworks to manage complexity. LangChain, LlamaIndex, and newer frameworks like Guardrails and LMQL provide abstractions for prompt chaining, structured output, and iterative generation. These frameworks reduce boilerplate and make it easier to implement multi-step techniques like Self-Instruct and Evol-Instruct. The financial services company used LangChain to build their sequential pipeline: they defined each generation step as a chain, connected the chains, and ran the full pipeline with built-in retry logic and error handling.

Cost management is important. Generating 10,000 high-quality examples with GPT-4o in 2026 costs roughly $500 to $2,000 depending on prompt length, output length, and number of filtering iterations. This is far cheaper than hiring experts, but it is not trivial. You optimize costs by using cheaper models for less critical steps. Use GPT-4o for the initial generation and complex evolution steps, but use GPT-4o mini or another smaller model for quality filtering and simple transformations. The legal tech company used GPT-4o for instruction and response generation but used GPT-4o mini for the quality scoring step, which cut their total cost by 30%.

You also batch requests where possible. Most APIs in 2026 support batch mode or parallel requests with rate limits. Batching reduces latency and sometimes reduces cost. The healthcare company used OpenAI's batch API to generate 1,000 clinical notes at a time, which gave them a 50% discount compared to synchronous requests and reduced total generation time from three days to eight hours.

## When to Use Each Technique

Self-Instruct is best when you have a small number of high-quality seed examples and you need to expand scenario coverage. It works well for tasks with clear categories or types where the model can generate plausible variations. It is less effective for tasks that require highly specialized knowledge or tasks where plausible-sounding examples are often incorrect. A code generation task benefits from Self-Instruct because the model can generate many valid programming problems. A medical diagnosis task might not benefit as much because the model can generate plausible-sounding but medically incorrect scenarios without expert validation.

Evol-Instruct is best when your task requires handling complex, multi-step, or constrained scenarios and you want your model to be robust to difficulty variation. It is highly effective for reasoning tasks, planning tasks, and tasks where user requests can range from simple to extremely complex. It is less useful for tasks that are inherently simple or tasks where increased complexity does not align with real-world usage. A text classification task does not benefit from evolved complexity. A coding assistant task benefits greatly because real users ask both simple and very complex questions.

Structured generation is best when output format compliance is critical, when you need guaranteed coverage of schema fields, or when downstream systems require specific data structures. It is essential for tasks like API response generation, database query generation, form filling, and structured report creation. It is less critical for open-ended creative tasks where format flexibility is acceptable or desirable.

You often use all three techniques in one project, applying each where it fits best. The legal tech company used Self-Instruct for breadth, Evol-Instruct for challenging cases, and structured generation for format compliance. This combined approach gave them a dataset optimized for multiple goals simultaneously.

## Quality Control and Validation

No matter which generation technique you use, you must validate the quality of the synthetic data before fine-tuning. Automated generation at scale produces errors, edge cases, and low-quality examples that will degrade your model if included in training. You apply multi-stage filtering. First, use rule-based filters to catch obvious problems: examples that are too short, too long, malformed, or missing required components. Second, use model-based filters where you prompt a capable model to score each example on dimensions like correctness, clarity, relevance, and instruction-response alignment. Third, use human review on a sample to catch issues that automated filters miss.

The legal tech company applied all three stages. Their rule-based filter removed any example where the response was shorter than 100 words or longer than 2,000 words, where the instruction did not mention discovery or requests, or where the response did not include section headers. This removed 8% of generated examples. Their model-based filter used GPT-4o to score each remaining example on legal accuracy, instruction clarity, and response completeness, using a 0 to 1 scale. They kept only examples with all three scores above 0.8. This removed another 15%. Finally, they had two senior attorneys review a random sample of 500 examples from the filtered dataset. The attorneys found a 4% error rate, mostly subtle issues like citing the wrong rule or using ambiguous phrasing. They used these findings to refine their model-based filter prompts and reran filtering on the full dataset, which brought the estimated error rate below 2%.

You also validate coverage. After generating and filtering, check that your dataset covers all the task types, difficulty levels, and edge cases you need. Use clustering, diversity metrics, and manual inspection to identify gaps. If you find that certain scenarios are underrepresented, generate more targeted examples to fill those gaps before finalizing the dataset.

The healthcare company validated coverage by clustering their 10,000 generated clinical notes based on chief complaint categories. They found good coverage of common complaints like chest pain, abdominal pain, and respiratory issues, but very few examples of neurological complaints, psychiatric complaints, or pediatric cases. They generated 1,000 additional targeted examples to fill these gaps, bringing their total to 11,000 and ensuring balanced coverage.

## Real-World Results and Tradeoffs

Teams using these systematic generation techniques report strong results. The legal tech company's fine-tuned model scored 91% on a blind evaluation by senior litigators, compared to 93% for a model trained on expert-written examples, a difference that was not statistically significant. The synthetic-trained model generalized better: when tested on case types not represented in either training set, the synthetic model scored 84% and the expert-written model scored 78%, likely because the synthetic generation process created more diverse scenarios.

The financial services company fine-tuned a compliance documentation model on 12,000 synthetic examples and compared it to a baseline GPT-4o prompted with few-shot examples. The fine-tuned model was 40% faster, 60% cheaper per request, and produced outputs that were rated higher on regulatory compliance and clarity by internal compliance officers. The synthetic data cost $22,000 to generate and the fine-tuning cost $8,000, for a total investment of $30,000. The baseline prompting approach cost $15 per 1,000 requests due to long few-shot prompts. At the company's usage volume of 200,000 requests per month, the fine-tuned model saved $3 million per year in API costs.

The tradeoff is upfront effort and cost. Implementing Self-Instruct, Evol-Instruct, and structured generation requires engineering time to build the generation pipeline, define evolution operators and templates, set up filtering and validation, and iterate on quality. For small projects or projects with sufficient real data, this investment may not be justified. For large projects, high-value applications, or projects where real data is scarce or expensive, the investment pays off quickly.

Another tradeoff is the risk of compounding errors. Synthetic data generated by a model will inherit that model's biases, blind spots, and error patterns. If you fine-tune on synthetic data and then use the fine-tuned model in a setting where the teacher model would have failed, your fine-tuned model will also fail. You mitigate this by using high-quality teacher models, applying rigorous filtering, and validating the fine-tuned model on real-world test data that includes edge cases and adversarial examples.

The next subchapter addresses validation and quality control in depth, covering how to build robust evaluation pipelines for synthetic training data, how to detect and mitigate teacher model biases, and how to ensure that fine-tuned models trained on synthetic data perform reliably in production.
