# 4.9 â€” Hyperparameter Selection: Learning Rate, Epochs, Batch Size, Warmup

In early 2025, a financial services AI team fine-tuned a Llama 3 70B model on 5,000 annotated examples of equity research report generation. They copied hyperparameters from a tutorial they found online: learning rate of 3e-4, 5 epochs, batch size 16, no warmup, no weight decay. The first training run diverged after 200 steps. Loss spiked to infinity, gradients exploded, and the model started generating random tokens. They reduced the learning rate to 1e-4 and retried. This time training completed, but the resulting model was worse than the base model. It generated syntactically correct but factually nonsensical analyst reports. They reduced the learning rate again to 5e-5 and trained for 3 epochs. This produced a functional model, but it overfit severely. Validation loss was 40% higher than training loss, and the model memorized training examples instead of generalizing. After six failed training runs and $22,000 in wasted compute, they realized they were guessing hyperparameters instead of understanding them. They paused training, studied the interaction between learning rate, epoch count, batch size, and warmup, and designed a principled hyperparameter search. The seventh run succeeded.

The mistake was treating hyperparameters as arbitrary magic numbers. Learning rate, epochs, batch size, and warmup are not independent settings. They interact with each other, with your model size, with your dataset size, and with your task complexity. You cannot copy hyperparameters from one project to another and expect good results. You need to understand what each hyperparameter controls, how it affects training dynamics, and how to tune it for your specific model and data.

## Learning Rate: The Most Critical Parameter

Learning rate controls how much the model's weights change in response to each training batch. A high learning rate makes large weight updates, which allows the model to learn quickly but risks instability and divergence. A low learning rate makes small weight updates, which produces stable training but may be too slow to reach optimal performance. Learning rate is the most important hyperparameter. If you set it incorrectly, no other hyperparameter choice will save your training run.

The typical learning rate range for full fine-tuning is 1e-5 to 5e-5. For LoRA fine-tuning, the range is 1e-4 to 5e-4. LoRA requires a higher learning rate because it trains far fewer parameters, so each parameter update needs to have more impact. The financial services team's initial learning rate of 3e-4 was appropriate for LoRA but too high for full fine-tuning on a 70B model. When they switched to full fine-tuning with the same learning rate, training diverged.

You should start with a learning rate in the middle of the recommended range and adjust based on training dynamics. For full fine-tuning, start at 2e-5. For LoRA fine-tuning, start at 2e-4. Monitor loss curves for the first 50 to 100 steps. If loss decreases smoothly, your learning rate is reasonable. If loss oscillates wildly or spikes upward, your learning rate is too high. If loss decreases very slowly or plateaus immediately, your learning rate is too low.

The financial services team started their successful run at 2e-5 for full fine-tuning. Loss decreased smoothly for the first 100 steps, so they continued. At step 300, loss started oscillating slightly. They did not stop training, but they noted that 2e-5 might be at the upper edge of stability. In their next iteration, they tried 1.5e-5 and saw smoother loss curves. They adopted 1.5e-5 as their standard learning rate for this model and task.

Larger models require lower learning rates. A 7B model can tolerate 5e-5 for full fine-tuning, but a 70B model typically requires 1e-5 to 2e-5. Larger models have more parameters, which means each parameter update affects a smaller fraction of the total model behavior. A high learning rate on a large model causes localized overfitting: some layers overfit while others underfit. The financial services team trained both a 13B and a 70B model on the same data. The 13B model used 3e-5 successfully, but the 70B model required 1.5e-5.

Smaller datasets require lower learning rates. If you train on 1,000 examples, you need a lower learning rate than if you train on 10,000 examples. Small datasets have higher per-example variance, which means each batch has more noise. A high learning rate amplifies noise and causes instability. The financial services team's dataset had 5,000 examples, which is moderate. They used 1.5e-5. A healthcare AI company training on 800 clinical note examples used 8e-6 for a 7B model.

## Epoch Count: Almost Always One to Three

An epoch is one full pass through the training dataset. If you have 5,000 examples and train for 3 epochs, the model sees each example three times. Epoch count controls how much the model specializes to your dataset. Too few epochs and the model underfits. Too many epochs and the model overfits.

The optimal epoch count for most fine-tuning tasks is 1 to 3. One epoch is often sufficient for large datasets or strong base models. Two epochs is the most common choice. Three epochs is appropriate for small datasets or when you need maximum task specialization. Beyond three epochs, overfitting risk increases sharply.

The financial services team trained for 5 epochs in their early runs and saw severe overfitting. Training loss dropped to near zero, but validation loss plateaued and then increased. The model memorized training examples and lost generalization. They reduced to 3 epochs and overfitting decreased. They then tested 2 epochs and found that validation performance was nearly identical to 3 epochs. They adopted 2 epochs as their standard.

You should monitor validation loss to determine optimal epoch count. After each epoch, evaluate the model on a held-out validation set and record validation loss. If validation loss decreases each epoch, continue training. If validation loss plateaus or increases, stop training. Early stopping is a reliable way to avoid overfitting without guessing epoch count in advance. The financial services team implemented early stopping with a patience of 1 epoch. If validation loss did not improve for one full epoch, training stopped automatically. This allowed them to start training runs with a maximum of 5 epochs but stop at 2 or 3 if validation loss plateaued.

One epoch is sufficient when your dataset is large and your task is similar to the base model's training distribution. A customer support model trained on 20,000 examples of general English support conversations can achieve strong performance in one epoch because the base model already understands English conversation structure. The fine-tuning only needs to bias the model toward support-specific language.

Three epochs is necessary when your dataset is small or your task is highly specialized. A medical coding model trained on 1,200 examples of clinical note to ICD-10 code mapping needs more repetition to learn the mapping patterns. The financial services team's equity research task was highly specialized, and 5,000 examples was moderate, so 2 to 3 epochs was appropriate.

## Batch Size: Stability Versus Speed

Batch size controls how many examples the model processes before updating weights. A batch size of 16 means the model reads 16 examples, computes gradients for all 16, averages the gradients, and updates weights once. Larger batch sizes produce more stable gradient estimates but require more memory and reduce the number of weight updates per epoch. Smaller batch sizes produce noisier gradient estimates but allow more frequent weight updates.

The typical batch size range for fine-tuning is 8 to 32. Batch size 16 is a common default. Larger models or longer sequences require smaller batch sizes due to memory constraints. The financial services team used batch size 16 initially, which fit comfortably in their A100 80GB GPU memory. They tested batch size 32 and found that training was slightly more stable but also slightly slower. They tested batch size 8 and found training was noisier but converged to similar final performance. They kept batch size 16.

Effective batch size is the product of per-device batch size and gradient accumulation steps. If you set per-device batch size to 4 and gradient accumulation steps to 4, the effective batch size is 16. Gradient accumulation allows you to train with large effective batch sizes even when GPU memory limits per-device batch size. The financial services team later trained a 70B model that could only fit batch size 4 per GPU. They used gradient accumulation steps of 4 to maintain an effective batch size of 16.

Batch size interacts with learning rate. Larger batch sizes require higher learning rates to maintain the same learning speed. A rule of thumb is linear scaling: if you double batch size, you should double learning rate. This rule is approximate and works best for small learning rates. The financial services team tested increasing batch size from 16 to 32 and also increasing learning rate from 1.5e-5 to 3e-5. Training diverged. Linear scaling did not hold for their learning rate. They kept batch size 16 and learning rate 1.5e-5.

Smaller batch sizes add regularization by increasing gradient noise. This can reduce overfitting on small datasets. A legal tech company training on 600 contract analysis examples used batch size 4 intentionally to increase noise and reduce overfitting. Their validation performance improved compared to batch size 16, even though training loss was noisier.

## Warmup: Stabilizing Early Training

Warmup gradually increases the learning rate from zero to the target learning rate over the first few hundred training steps. Instead of starting at learning rate 2e-5 immediately, you start at 0 and linearly increase to 2e-5 over 100 or 200 steps. Warmup prevents instability in early training when the model has not yet adapted to the new task distribution.

The typical warmup schedule is linear warmup over 5% to 10% of total training steps. If your training run is 2,000 steps, you warm up for 100 to 200 steps. The financial services team's successful training run was 1,800 steps total. They used 100-step linear warmup, which is 5.5% of training. Loss curves were smooth from the start.

Warmup is more important for high learning rates, large models, and small datasets. If you train a 70B model at learning rate 2e-5 on 1,000 examples, warmup is critical. The high learning rate and small dataset create instability risk. Warmup gives the model time to adjust before full-strength updates begin. If you train a 7B model at learning rate 5e-6 on 10,000 examples, warmup is less critical. The low learning rate and large dataset are already stable.

The financial services team initially trained without warmup and saw loss spikes in the first 50 steps. Adding 100-step warmup eliminated the spikes. A healthcare AI company training a 13B model at learning rate 3e-5 saw divergence in the first 20 steps without warmup. They added 200-step warmup and training stabilized.

You can use cosine decay or linear decay after warmup to gradually reduce learning rate toward the end of training. Cosine decay reduces learning rate following a cosine curve from the peak learning rate to zero. Linear decay reduces learning rate linearly. Both schedules allow the model to make large updates early in training and fine-grained updates late in training. The financial services team used cosine decay after warmup. Learning rate peaked at 1.5e-5 after 100 warmup steps, then decayed following a cosine curve to 1e-6 at the end of training. This improved final validation performance by 2% compared to constant learning rate.

## Weight Decay: Preventing Overfitting

Weight decay adds a penalty to large weights, encouraging the model to use smaller weight values. This acts as regularization and reduces overfitting. Weight decay is specified as a multiplier, typically 0.01 to 0.1. A weight decay of 0.01 means that weights are penalized proportionally to their magnitude, scaled by 0.01.

Most fine-tuning runs use weight decay between 0.01 and 0.05. Higher weight decay increases regularization but can prevent the model from learning task-specific features. Lower weight decay reduces regularization and increases overfitting risk. The financial services team used weight decay 0.01 in their successful run. They tested 0.05 and found that the model underfit slightly: validation performance was worse, and the model did not fully adapt to the equity research style.

Weight decay is more important for small datasets. If you train on 500 examples, weight decay 0.05 or 0.1 can significantly reduce overfitting. If you train on 10,000 examples, weight decay 0.01 is sufficient. The legal tech company training on 600 contract examples used weight decay 0.08. The healthcare AI company training on 800 clinical notes used weight decay 0.06.

You should disable weight decay on bias terms and layer normalization parameters. These parameters do not benefit from regularization and can be harmed by it. Most fine-tuning frameworks disable weight decay on biases and norms by default, but you should verify this in your training configuration. The financial services team confirmed that their framework excluded biases and norms from weight decay.

## Hyperparameter Interaction and Search

Hyperparameters interact in non-obvious ways. Increasing learning rate and decreasing epoch count can produce similar final performance because the model makes fewer but larger updates. Increasing batch size and increasing warmup steps can both stabilize training. You cannot tune one hyperparameter in isolation. You need to consider the full configuration.

The financial services team's successful hyperparameter configuration was learning rate 1.5e-5, 2 epochs, batch size 16, 100-step warmup, cosine decay, and weight decay 0.01. They tested 12 different configurations before arriving at this one. Configuration 1 was learning rate 3e-4, 5 epochs, batch size 16, no warmup, no weight decay: diverged. Configuration 2 was learning rate 1e-4, 5 epochs, batch size 16, no warmup, no weight decay: underperformed. Configuration 3 was learning rate 5e-5, 3 epochs, batch size 16, no warmup, weight decay 0.01: overfit. They systematically reduced learning rate, reduced epochs, added warmup, and tuned weight decay until they found a stable, high-performing configuration.

You should budget for hyperparameter search. Assume you will need 5 to 15 training runs to find optimal hyperparameters for a new task or model size. Each run costs compute time and infrastructure. The financial services team's 12 training runs cost $18,000 in total compute. The successful configuration cost $1,500 per run, and they ran it three times to verify reproducibility. Total hyperparameter tuning cost was $22,500 before they moved to production.

You can use automated hyperparameter search tools like Optuna or Ray Tune to explore configurations more efficiently. These tools use Bayesian optimization or other search strategies to propose configurations based on previous results. The healthcare AI company used Optuna to search learning rate, epoch count, and weight decay over 20 trials. Optuna identified a strong configuration in 14 trials, saving compute compared to grid search.

You should record all hyperparameters for every training run. Log learning rate, epoch count, batch size, warmup steps, weight decay, optimizer type, and any other settings. When you compare models later, you need to know exactly how each model was trained. The financial services team used Weights & Biases to log all hyperparameters and metrics automatically. They could compare loss curves, validation performance, and hyperparameter settings across all 12 runs in one dashboard.

## Common Mistakes and Their Symptoms

Learning rate too high causes training divergence. Loss spikes to infinity, gradients explode, and the model generates garbage. Reduce learning rate by 2x to 5x and retry. The financial services team saw divergence at 3e-4 and reduced to 1e-4, then to 1.5e-5 for stability.

Learning rate too low causes slow convergence or early plateaus. Loss decreases very slowly, and validation performance does not reach competitive levels even after many epochs. Increase learning rate by 1.5x to 2x. A content moderation company trained at 5e-6 and saw loss decrease by only 10% after 3 epochs. They increased to 1.5e-5 and loss decreased 60% in 2 epochs.

Too many epochs causes overfitting. Training loss continues to decrease, but validation loss plateaus or increases. The model memorizes training examples and loses generalization. Reduce epoch count or implement early stopping. The financial services team saw validation loss increase from epoch 3 to epoch 5. They reduced to 2 epochs.

Too few epochs causes underfitting. Validation loss is still decreasing at the end of training, and performance is below baseline expectations. Increase epoch count or increase learning rate. A legal tech company trained for 1 epoch and achieved only 65% task accuracy. They increased to 2 epochs and achieved 84%.

Insufficient warmup causes early instability. Loss spikes or oscillates in the first 50 to 100 steps, then stabilizes. Add warmup steps equal to 5% to 10% of total training. The healthcare AI company saw loss spike at step 15 and step 40. They added 200-step warmup and training was smooth from the start.

Batch size too large causes memory errors or slow training. GPU runs out of memory, or training takes much longer than expected per epoch. Reduce batch size or use gradient accumulation. The financial services team could not fit batch size 64 on their 70B model. They reduced to batch size 4 with gradient accumulation 4.

Weight decay too high causes underfitting. The model does not fully adapt to task-specific patterns, and validation performance is mediocre. Reduce weight decay. The financial services team tested weight decay 0.1 and saw validation accuracy drop by 6%. They reduced to 0.01.

## Advanced Hyperparameter Techniques

Learning rate schedules beyond warmup and cosine decay can improve final performance. Cyclical learning rates vary the learning rate between a minimum and maximum value over the course of training. The model alternates between aggressive learning and fine-tuning phases. A customer support company used a cyclical schedule with learning rate varying between 5e-6 and 2e-5 over 400-step cycles. This produced a model that scored 3% higher on validation than constant learning rate with cosine decay.

You can also use learning rate rewinding, where you train to near-convergence, rewind the learning rate to a higher value, and train for additional steps. This allows the model to escape local minima. A content generation company trained for 1,200 steps with learning rate decaying from 2e-5 to 1e-6. At step 1,200, they reset learning rate to 1e-5 and trained for 300 more steps. Validation performance improved by 4% compared to stopping at step 1,200.

Layer-wise learning rates assign different learning rates to different model layers. Early layers learn general features and should be updated cautiously. Late layers learn task-specific features and can tolerate higher learning rates. A medical AI company trained with learning rate 5e-6 for the first 16 layers, 1.5e-5 for the middle 16 layers, and 3e-5 for the final 8 layers. This outperformed uniform learning rate by 7% on medical terminology generation tasks.

Gradient clipping prevents exploding gradients by capping gradient norms at a maximum value. If gradients exceed the threshold, they are scaled down proportionally. Most fine-tuning frameworks enable gradient clipping by default with a threshold of 1.0. The financial services team used gradient clipping at 1.0 and never saw gradient explosions. A legal tech company disabled gradient clipping to test its impact and immediately saw training divergence. They re-enabled clipping.

## Hyperparameters for LoRA Fine-Tuning

LoRA introduces additional hyperparameters beyond standard fine-tuning. LoRA rank controls the dimensionality of the low-rank adaptation matrices. Higher rank allows more expressive adaptations but increases parameter count and training time. Typical LoRA ranks are 8, 16, 32, or 64. Rank 16 is the most common default.

The financial services team tested LoRA ranks from 8 to 128 on their equity research task. Rank 8 achieved 78% validation accuracy. Rank 16 achieved 87%. Rank 32 achieved 89%. Rank 64 achieved 90%. Rank 128 achieved 90%, identical to rank 64. They concluded that rank 32 was optimal: it captured most of the performance gain without the cost of rank 64 or 128. They adopted rank 32 for production.

LoRA alpha controls the scaling of LoRA updates. It is typically set to the same value as rank or twice the rank. Alpha equal to rank produces moderate adaptation strength. Alpha equal to twice rank produces stronger adaptation. The financial services team tested alpha values of 16, 32, and 64 with rank 32. Alpha 16 produced weak adaptation and underfit. Alpha 32 produced balanced adaptation. Alpha 64 produced strong adaptation but slight overfitting. They used alpha 32.

You can apply LoRA to different subsets of model parameters. The most common configuration applies LoRA to query and value projection matrices in attention layers. You can also apply LoRA to key projections, output projections, and feedforward layers. Applying LoRA to more parameter types increases expressiveness but also increases parameter count and overfitting risk. The financial services team applied LoRA to query and value projections only, which was 8 million trainable parameters. They tested adding key and output projections, which increased to 16 million parameters and improved validation accuracy by 2%. They adopted the expanded configuration.

## Hyperparameters for Different Model Sizes

Hyperparameter optima vary significantly by model size. A 7B model and a 70B model require different learning rates, warmup schedules, and regularization. Smaller models can tolerate higher learning rates and need less warmup. Larger models require lower learning rates and more warmup.

The financial services team trained the same task on 7B, 13B, and 70B models. For the 7B model, they used learning rate 5e-5, 100-step warmup, and 2 epochs. For the 13B model, they used learning rate 3e-5, 150-step warmup, and 2 epochs. For the 70B model, they used learning rate 1.5e-5, 200-step warmup, and 2 epochs. Each configuration was tuned independently, and the optimal settings differed substantially.

Smaller models often require more regularization. A 7B model trained on 5,000 examples overfits more easily than a 70B model on the same data. The financial services team used weight decay 0.03 for the 7B model, 0.015 for the 13B model, and 0.01 for the 70B model. Higher weight decay for smaller models compensated for their greater overfitting tendency.

Larger models benefit more from longer warmup. The 70B model's 200-step warmup was critical for stability. The 7B model's 100-step warmup was sufficient. A content moderation company tested eliminating warmup on a 7B model and saw only minor instability. They tested eliminating warmup on a 65B model and training diverged immediately.

## Reproducibility and Hyperparameter Documentation

You must document hyperparameters precisely to reproduce training runs. Record learning rate, warmup steps, decay schedule, epoch count, batch size, gradient accumulation steps, weight decay, optimizer type, random seed, and any other configuration details. The financial services team used a configuration management system that logged every hyperparameter automatically to a YAML file stored with each model checkpoint.

Random seed affects reproducibility more than most teams expect. The same hyperparameters with different random seeds can produce models with 5% to 10% performance variance. The financial services team trained their final configuration three times with seeds 42, 123, and 789. Validation accuracy was 89%, 91%, and 90%. They reported the mean and standard deviation: 90% plus or minus 1%. A healthcare AI company trained the same configuration five times and saw variance from 82% to 88%. They realized their hyperparameters were near a performance cliff and needed further tuning.

You should version your hyperparameter configurations and track which configuration produced which model. The financial services team maintained a hyperparameter repository with 47 configurations tested over six weeks. Each configuration had a unique identifier. Each model checkpoint was tagged with its configuration ID. When they wanted to reproduce a model or understand why a model performed well, they retrieved the configuration from the repository.

Some hyperparameters interact with infrastructure. Batch size depends on GPU memory. Distributed training introduces additional hyperparameters like gradient synchronization strategy and communication backend. The financial services team documented not only hyperparameters but also infrastructure: GPU type, number of GPUs, distributed training framework, and CUDA version. This allowed them to reproduce training runs exactly.

## When to Stop Hyperparameter Tuning

Hyperparameter tuning has diminishing returns. The first few tuning iterations produce large performance gains. Later iterations produce smaller gains at higher cost. You need to decide when tuning is complete and the model is ready for production.

A reasonable stopping criterion is when validation performance improves by less than 2% over three consecutive tuning iterations. The financial services team reached this point after 12 configurations. Configurations 10, 11, and 12 improved validation accuracy by 1.2%, 0.8%, and 0.6% respectively. They stopped tuning and moved to production evaluation.

You should also consider the cost of further tuning. If each tuning run costs $2,000 and produces a 1% improvement, and production value of 1% improvement is $5,000, additional tuning is justified. If production value of 1% improvement is only $1,000, further tuning is not cost-effective. The financial services team estimated that each 1% improvement in analyst report quality saved $8,000 per year in human review costs. They continued tuning until marginal improvements dropped below 0.5%.

Some tasks require exhaustive tuning because quality thresholds are binary. A medical diagnosis model that achieves 89% accuracy is not deployable, but 92% accuracy meets regulatory requirements. You must tune until you cross the threshold. A healthcare AI company tuned for 11 weeks and tested 38 hyperparameter configurations to reach 92% diagnostic accuracy. They could not deploy at 89% or 91%. The tuning cost was high, but deployment was impossible without crossing the threshold.

## Optimizer Selection

Most fine-tuning uses the AdamW optimizer, which combines Adam optimization with weight decay. AdamW is the default choice for transformer fine-tuning and works well for most tasks. You should use AdamW unless you have a specific reason to choose a different optimizer.

Alternative optimizers include SGD with momentum, Adafactor, and Lion. SGD with momentum is simpler than AdamW but requires more careful learning rate tuning. Adafactor is memory-efficient and useful for very large models when GPU memory is constrained. Lion is a newer optimizer that claims better performance with less memory than AdamW. The financial services team tested AdamW, Adafactor, and Lion on their 70B model. AdamW achieved 91% validation accuracy. Adafactor achieved 88%. Lion achieved 90%. They used AdamW for production.

AdamW has two hyperparameters beyond learning rate and weight decay: beta1 and beta2. These control the exponential decay rates for gradient moments. The default values are beta1 equal to 0.9 and beta2 equal to 0.999. These defaults work for almost all fine-tuning tasks. You should not tune beta1 and beta2 unless you have exhausted other hyperparameters. The financial services team used default beta values for all experiments.

Some frameworks offer fused optimizers that run faster by combining operations. A fused AdamW implementation can reduce training time by 10% to 20% compared to standard AdamW. The financial services team used a fused AdamW implementation and measured 15% faster training with identical convergence.

## Mixed Precision Training

Mixed precision training uses 16-bit floating point for most operations and 32-bit floating point for stability-critical operations. This reduces memory usage and increases training speed with minimal quality impact. Most modern fine-tuning uses mixed precision by default.

BF16 and FP16 are the two common 16-bit formats. BF16 has the same exponent range as FP32 but lower precision. FP16 has higher precision but smaller exponent range. BF16 is more stable for training and is the preferred format on modern GPUs. The financial services team used BF16 mixed precision and measured 40% faster training and 35% lower memory usage compared to FP32, with no measurable quality degradation.

You should use FP32 for very small models or very sensitive tasks where numerical precision matters. A medical AI company found that their 1.3B model trained with BF16 produced slightly different outputs than FP32 for edge cases involving small probability differences. They switched to FP32 for production to ensure maximum precision. The quality difference was less than 1%, but medical applications justified the caution.

Mixed precision requires gradient scaling to prevent underflow. When gradients are very small, 16-bit representation may round them to zero. Gradient scaling multiplies gradients by a large constant before storing in 16-bit, then divides by the same constant when updating weights. Most frameworks handle gradient scaling automatically. The financial services team enabled automatic gradient scaling and never saw underflow issues.

## Distributed Training Hyperparameters

When training on multiple GPUs, you introduce distributed training hyperparameters. Gradient accumulation steps control how many forward passes execute before synchronizing gradients across GPUs. Higher gradient accumulation reduces communication overhead but increases memory usage. The financial services team used 4 gradient accumulation steps on 8 GPUs, which meant 32 examples per weight update.

Data parallelism splits batches across GPUs. Each GPU processes a fraction of the batch and gradients are averaged. Model parallelism splits the model across GPUs when the model is too large for one GPU. The financial services team used data parallelism for their 70B model across 8 A100 80GB GPUs. Each GPU held the full model and processed batch size 4. Gradients were synchronized using NCCL backend.

Zero Redundancy Optimizer, or ZeRO, shards optimizer state and model parameters across GPUs to reduce per-GPU memory usage. ZeRO stage 1 shards optimizer state. ZeRO stage 2 shards optimizer state and gradients. ZeRO stage 3 shards optimizer state, gradients, and model parameters. Higher ZeRO stages reduce memory but increase communication. The financial services team used ZeRO stage 2, which reduced per-GPU memory from 76GB to 52GB and allowed them to increase per-GPU batch size from 2 to 4.

Communication backend affects distributed training speed. NCCL is the standard backend for NVIDIA GPUs and provides the best performance. Gloo is a fallback for non-NVIDIA GPUs. The financial services team used NCCL and measured 2.4 times faster training than Gloo on the same hardware.

## Hyperparameters for Long Sequence Fine-Tuning

Fine-tuning on long sequences, typically 4,000 tokens or more, requires adjusted hyperparameters. Longer sequences consume more memory and take longer to process. You may need to reduce batch size, use gradient accumulation more aggressively, or reduce model size.

A legal tech company fine-tuned on full contract documents averaging 6,800 tokens. They could fit batch size 1 per GPU on an A100 80GB. They used 16 gradient accumulation steps to achieve an effective batch size of 16 across 1 GPU. Training was slow: 11 days for 800 steps. They considered truncating contracts to 4,000 tokens but found that critical clauses often appeared after token 4,000. They accepted the slow training and long sequences.

Flash Attention and other memory-efficient attention implementations reduce memory usage for long sequences. Flash Attention 2 reduces memory by 50% to 70% for sequences above 2,000 tokens. The legal tech company enabled Flash Attention 2 and increased batch size from 1 to 3. Training time decreased from 11 days to 5 days.

You can use sequence length warmup, where you train on shorter sequences initially and gradually increase length. This allows faster iteration early in training when the model is making large updates. The legal tech company tested sequence length warmup: 2,000 tokens for the first 200 steps, 4,000 tokens for steps 200 to 500, and 6,800 tokens for steps 500 to 800. This reduced total training time to 4 days and produced similar final quality.

## Hyperparameter Transfer Across Tasks

Hyperparameters tuned for one task often transfer reasonably well to similar tasks. If you fine-tune a model for contract analysis and later fine-tune for patent analysis, you can use the same learning rate, warmup, and weight decay as a starting point. This does not guarantee optimal performance, but it saves tuning time.

The financial services team tuned hyperparameters for equity research report generation, then applied the same configuration to fixed income report generation. The transferred configuration achieved 87% validation accuracy on fixed income, compared to 91% on equity. They ran 5 additional tuning iterations and improved fixed income performance to 89%. Hyperparameter transfer saved them 7 tuning iterations compared to starting from scratch.

Hyperparameter transfer works best when tasks have similar data distributions, model sizes, and training set sizes. If task A uses 5,000 examples on a 70B model and task B uses 500 examples on a 7B model, hyperparameters will not transfer well. The financial services team tried transferring their 70B hyperparameters to a 13B model and saw poor results. They retuned from scratch for the 13B model.

You should document hyperparameter configurations as reusable templates. The financial services team created templates for common scenarios: 7B model with 1,000 to 5,000 examples, 13B model with 5,000 to 10,000 examples, and 70B model with 5,000 to 20,000 examples. Each template specified recommended learning rate, warmup, epochs, and weight decay. New projects started with the appropriate template and tuned from there.

Hyperparameter selection is not guesswork. You test, measure, compare, and iterate. You log every experiment, understand the symptoms of misconfigurations, and adjust systematically. The financial services team spent $22,500 and six weeks on hyperparameter tuning, but the resulting model achieved 91% analyst report quality and has been in production for eight months. The investment was justified. The next subchapter covers evaluation and testing of fine-tuned models, which is how you measure whether your hyperparameters and training strategy succeeded.
