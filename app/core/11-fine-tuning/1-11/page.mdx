# 1.11 — The Fine-Tuning Operating Model: Intake, Train, Gate, Ship

In August 2025, a financial services technology company launched a fine-tuning center of excellence to support multiple product teams across the organization. Within three months, they had received seventeen fine-tuning requests from seven different teams. They accepted fourteen of those requests, started training runs for all of them, and deployed eleven fine-tuned models to production. By November 2025, six of those models had been decommissioned because they delivered no measurable value over prompted baselines, three were stuck in permanent retraining cycles because performance degraded with production data drift, and two had caused compliance incidents because they generated outputs that violated internal policy guidelines. The root cause was clear: the organization had treated fine-tuning as an ad-hoc service where any team could request a model, get it trained, and deploy it without rigorous gating. They had no intake process to filter bad requests, no evaluation gate to prevent underperforming models from shipping, and no handoff process to ensure production teams could maintain the models. They had optimized for velocity at the cost of quality and governance. The diagnosis was straightforward: without a disciplined operating model, fine-tuning becomes a compliance and maintenance liability rather than a capability multiplier.

Fine-tuning at organizational scale requires a repeatable operating model with four distinct phases: intake and scoping to filter requests and validate that fine-tuning is the right approach, training and iteration to produce a candidate model, evaluation gating to ensure the model meets quality and safety thresholds before deployment, and deployment and monitoring to hand off the model to production teams with clear ownership and instrumentation. Each phase has defined inputs, outputs, roles, and decision gates. The operating model must be designed to say no more often than yes, because the highest-leverage decision in fine-tuning is preventing projects that should not exist. This subchapter provides the end-to-end framework for running fine-tuning as a service within your organization without accumulating technical debt or compliance risk.

## The Four-Phase Model

The operating model divides the fine-tuning lifecycle into four sequential phases, each with a formal handoff and decision gate. The first phase is **intake and scoping**, where a product or engineering team submits a fine-tuning request and the fine-tuning team evaluates whether the request is valid, whether the problem requires fine-tuning, and whether the requesting team has the prerequisites to support a fine-tuned model. The intake phase produces a scoping document that defines the task, baseline performance, success criteria, and resource commitment. If the intake phase rejects the request, the project stops before any training work begins.

The second phase is **training and iteration**, where the fine-tuning team collects or validates training data, runs experiments to optimize hyperparameters and training configuration, and produces one or more candidate models. This phase is purely technical and focuses on maximizing performance on the defined success criteria. The training phase produces a trained model checkpoint, evaluation results on a held-out test set, and a training report that documents dataset composition, training configuration, and performance deltas versus the baseline.

The third phase is **evaluation gating**, where the candidate model is reviewed against quality, safety, compliance, and operational readiness thresholds. This is the most critical gate in the entire process. The evaluation gate is staffed by stakeholders outside the fine-tuning team, including domain experts, trust and safety reviewers, compliance officers, and production engineering leads. The gate can approve the model for deployment, reject the model and terminate the project, or send the model back to the training phase for additional iteration. Only models that pass all gate criteria proceed to deployment.

The fourth phase is **deployment and monitoring**, where the approved model is deployed to production, ownership is transferred to the requesting team, and instrumentation is activated to track performance, drift, and incidents. The deployment phase produces a model handoff document, production access credentials, monitoring dashboards, and a retraining schedule. Once deployed, the model is owned by the requesting team, not the fine-tuning team. The fine-tuning team provides support but does not maintain production models indefinitely.

Each phase transition is a formal decision gate. You do not move from intake to training without approval. You do not move from training to evaluation gating without passing performance thresholds. You do not move from evaluation gating to deployment without stakeholder sign-off. You do not hand off to production without a complete operational runbook. The gates enforce discipline and prevent bad models from reaching production.

## Phase One: Intake and Scoping

The intake phase is the most important filter in the entire operating model. Most fine-tuning requests should be rejected at intake because the problem does not require fine-tuning, the requesting team has not exhausted simpler alternatives, or the team lacks the data or infrastructure to maintain a fine-tuned model. The intake process is designed to be rigorous and evidence-based, not permissive.

When a team submits a fine-tuning request, the intake form requires specific evidence. First, the current prompted baseline performance measured on a representative evaluation set. This is not optional. If the team has not built an evaluation set and measured baseline performance, the request is rejected immediately. Second, the target performance threshold that would justify deployment. The team must state numerically what success looks like. Third, evidence that prompting, few-shot, and retrieval-augmented generation have been attempted and failed. The team must provide examples of optimized prompts they tried, the few-shot configurations they tested, and the reasons those approaches did not meet the target threshold. Fourth, a description of the task, the input and output schemas, the expected production volume, and the business value of improved performance.

The fine-tuning team reviews the intake request and asks clarifying questions. They challenge the assumption that fine-tuning is necessary. They ask whether the prompted baseline was truly optimized or whether the team gave up too early. They ask whether the target threshold is realistic or whether the team is expecting fine-tuning to solve a problem that requires better data, better task decomposition, or better system design. They ask whether the team has the infrastructure to deploy and monitor a fine-tuned model. They ask whether the team has budget for ongoing retraining.

If the intake review identifies gaps, the fine-tuning team sends the request back with specific guidance. "Your prompted baseline is underperforming because you are not providing enough context in the prompt. Try adding a glossary and re-evaluate before requesting fine-tuning." "Your evaluation set has only 50 examples, which is too small to detect meaningful performance differences. Build a 500-example evaluation set and re-submit." "You are requesting fine-tuning for a task that runs twice per week. The cost of maintaining a fine-tuned model exceeds the cost of using a longer prompt. Use retrieval-augmented generation instead."

The intake phase also assesses organizational readiness. Does the requesting team have an engineer who can integrate the fine-tuned model into their application? Does the team have monitoring infrastructure to detect when the model underperforms or drifts? Does the team have a process for collecting production feedback that can inform retraining? If any of these prerequisites are missing, the request is rejected or deferred until the team builds the necessary capabilities.

The financial services company in the opening story had no intake filter. Any team could request a fine-tuning project, and the fine-tuning team accepted all requests because they wanted to demonstrate value and build internal credibility. This led to a pipeline of low-quality projects that consumed resources without delivering outcomes. When they implemented a rigorous intake process in early 2026, the rejection rate increased from zero percent to 65 percent. The majority of requests were redirected to prompt optimization, RAG, or system redesign. The fine-tuning team worked on fewer projects, but the projects they worked on had clear success criteria, strong baselines, and committed stakeholders. The quality of deployed models improved dramatically.

The output of the intake phase is a **scoping document** that includes the task definition, baseline and target performance metrics, training data requirements, estimated timeline, estimated compute cost, success criteria for the evaluation gate, and resource commitments from both the fine-tuning team and the requesting team. Both teams sign off on the scoping document before training begins. This prevents scope creep and ensures alignment on what success looks like.

## Phase Two: Training and Iteration

Once a request passes intake, the training phase begins. The fine-tuning team works with the requesting team to collect, clean, and validate the training data. If the requesting team already has labeled data, the fine-tuning team audits it for quality, coverage, and formatting consistency. If the requesting team does not have labeled data, the fine-tuning team helps design a labeling process, write annotation guidelines, and conduct inter-annotator agreement checks. The training data must be production-representative, cover the full range of task variation, and meet the quality bar defined in the scoping document.

The fine-tuning team then runs a minimum viable fine-tune experiment using 200 to 500 training examples. This is the first checkpoint. If the MVP experiment shows marginal or no improvement over the baseline, the project is stopped and the requesting team is informed that fine-tuning will not deliver the expected value. If the MVP experiment shows strong improvement, the team proceeds to full-scale training.

Full-scale training involves hyperparameter tuning, dataset ablations, and model selection. The fine-tuning team runs experiments to optimize learning rate, batch size, number of epochs, and data mixture. They evaluate performance on a held-out validation set and track metrics in an experiment tracking system. They produce multiple candidate models and compare them against each other and against the baseline. This iteration continues until the team has a candidate model that meets or exceeds the target performance threshold defined in the scoping document.

The training phase is time-boxed. The scoping document specifies a maximum number of weeks for training iteration. If the team has not produced a candidate model that meets the target threshold within that time, the project is escalated to a review. The review determines whether to extend the timeline, lower the target threshold, or terminate the project. Time-boxing prevents training phases from dragging on indefinitely.

The output of the training phase is a **training report** that documents the final model checkpoint, the training dataset composition and size, the hyperparameters used, the performance metrics on the validation and test sets, the performance delta versus the baseline, examples of model outputs on test cases, and any known limitations or failure modes. The training report is the primary input to the evaluation gate.

## Phase Three: Evaluation Gating

The evaluation gate is the most critical decision point in the operating model. This is where the organization decides whether the fine-tuned model is ready for production deployment. The gate review is not conducted by the fine-tuning team — it is conducted by a cross-functional panel that includes representatives from the requesting team, domain experts who understand the task, trust and safety reviewers who assess output risks, compliance officers who verify regulatory alignment, and production engineering leads who assess operational readiness.

The gate review evaluates the model against a checklist of criteria. First, **performance criteria**: does the model meet or exceed the target performance threshold defined in the scoping document? Are the performance metrics measured on a held-out test set that represents production conditions? Is the performance improvement over the baseline statistically significant and practically meaningful? If the answer to any of these questions is no, the model is rejected.

Second, **safety criteria**: does the model produce harmful, biased, or non-compliant outputs? The trust and safety team reviews a sample of model outputs, including outputs on adversarial test cases designed to elicit problematic behavior. They check for toxicity, bias amplification, leakage of training data, and policy violations. If the model fails safety checks, it is sent back to the training phase with specific mitigation requirements, or the project is terminated if the safety issues cannot be resolved.

Third, **compliance criteria**: for regulated industries, does the model meet documentation, auditability, and change control requirements? The compliance officer reviews the training report, the dataset lineage, and the model card. They verify that the training data does not include prohibited information, that the model outputs can be audited, and that the deployment plan includes appropriate human oversight. If compliance requirements are not met, the model is rejected.

Fourth, **operational readiness criteria**: does the requesting team have the infrastructure to deploy the model, monitor its performance, collect feedback, and retrain when necessary? The production engineering lead reviews the deployment plan, the monitoring instrumentation, the incident response runbook, and the retraining schedule. If the requesting team is not ready to operate the model, deployment is deferred until readiness is achieved.

Fifth, **cost-benefit criteria**: does the performance improvement justify the ongoing cost of maintaining the fine-tuned model? The gate review compares the expected business value of the improvement against the cost of deployment, monitoring, and retraining. If the cost exceeds the value, the project is terminated even if the model performs well.

The gate review meeting is time-boxed to one hour. The fine-tuning team presents the training report, demos the model on live test cases, and answers questions from the panel. The panel discusses the criteria and makes a decision: approve for deployment, reject and terminate the project, or send back to training with specific feedback. The decision is documented and communicated to both the fine-tuning team and the requesting team within 24 hours.

The financial services company implemented a gate review process after their compliance incidents. The gate panel included a risk officer, a legal reviewer, a senior engineer from the requesting team, and a domain expert. The panel rejected 30 percent of models at the gate, most commonly for insufficient performance improvement, operational readiness gaps, or safety concerns. The models that passed the gate were high-quality and production-ready, which dramatically reduced the rate of post-deployment incidents.

## Phase Four: Deployment and Monitoring

Once a model passes the evaluation gate, it moves to deployment. The fine-tuning team works with the requesting team to deploy the model to production, configure monitoring, and establish ownership. The deployment phase has three deliverables: the **model handoff document**, the **monitoring dashboard**, and the **retraining plan**.

The model handoff document includes the model checkpoint and access credentials, the input and output schemas, example invocations with expected outputs, known limitations and failure modes, the performance baselines and targets, the monitoring metrics to track, the escalation path for incidents, and the retraining schedule. This document is the requesting team's operational runbook for the model.

The monitoring dashboard tracks key metrics: inference volume, latency, error rates, performance on canary test cases, and drift indicators. The dashboard must surface degradation quickly so the requesting team can intervene before user impact. For example, a model deployed for contract clause extraction monitors the percentage of extractions that require human correction. If that percentage rises above the baseline, the requesting team investigates whether the model is drifting or whether the input distribution has shifted.

The retraining plan specifies when and how the model will be retrained. Some models require retraining on a fixed schedule, such as quarterly or annually. Some models require retraining when drift indicators cross a threshold. Some models require retraining when new product features change the task definition. The retraining plan makes these triggers explicit and assigns responsibility for executing retraining to the requesting team.

Ownership transfers from the fine-tuning team to the requesting team at deployment. The fine-tuning team is no longer responsible for maintaining the model, monitoring its performance, or retraining it. They provide advisory support and can be consulted for retraining projects, but they are not on-call for production incidents. This ownership model prevents the fine-tuning team from becoming a bottleneck and ensures that the teams deploying models are also the teams accountable for their performance.

The deployment phase also includes a **post-deployment review** scheduled 30 days after launch. The requesting team reports on production performance, user feedback, incidents, and lessons learned. The fine-tuning team reviews the report and updates their internal processes based on what worked and what did not. This feedback loop improves the operating model over time.

## Running Fine-Tuning as a Service

At scale, fine-tuning is not a one-off project — it is a service provided by a centralized team to multiple product teams. The service model requires staffing, tooling, and governance. The fine-tuning team typically consists of ML engineers with expertise in training, evaluation, and deployment, plus program managers who run intake, coordinate gate reviews, and manage the project pipeline.

The fine-tuning service publishes an intake form, a set of eligibility criteria, and service-level expectations. For example: "We accept requests from teams that have built a 500-plus example evaluation set and achieved a prompted baseline. We commit to completing intake review within five business days. Approved projects receive a scoping document within two weeks. Training takes four to eight weeks depending on complexity. We run gate reviews weekly." These expectations set clear boundaries and prevent the service from being overwhelmed.

The fine-tuning service also maintains shared infrastructure: dataset storage and versioning, experiment tracking, model registry, evaluation pipelines, and monitoring templates. Product teams do not need to build this infrastructure themselves — they use the shared tooling provided by the fine-tuning service. This reduces duplication and ensures consistency.

The service publishes a dashboard that shows the pipeline of active projects, the stage each project is in, the expected completion date, and the gate pass rate. This transparency helps product teams plan and helps leadership understand the capacity constraints of the fine-tuning team.

## The Intake-to-Deployment Funnel

The operating model is designed as a funnel with high rejection rates at every stage. A well-run fine-tuning service receives 100 requests per year, accepts 30 at intake, produces candidate models for 20, approves 12 at the evaluation gate, and successfully deploys 10 to production. The 90 percent rejection rate from request to deployment is a feature, not a bug. It means the service is filtering aggressively and only working on projects that deliver value.

The funnel metrics help the fine-tuning team understand where projects are failing. If most projects fail at intake because teams have not optimized prompts, the fine-tuning team publishes prompt engineering guidance and offers workshops. If most projects fail at the evaluation gate because of safety issues, the fine-tuning team updates their training process to include adversarial testing earlier. If most projects fail in production because teams cannot maintain the models, the fine-tuning team tightens the operational readiness requirements at intake.

## Kill Criteria at Every Phase

The operating model must have explicit kill criteria at every phase so projects can be terminated quickly when they are not working. At intake, a project is killed if the team has not exhausted simpler alternatives, if the task does not benefit from fine-tuning, or if the team lacks prerequisites. During training, a project is killed if the MVP experiment shows no improvement, if full-scale training does not hit target performance within the time box, or if the cost of training exceeds the approved budget. At the evaluation gate, a project is killed if the model fails performance, safety, compliance, or cost-benefit thresholds. Post-deployment, a model is decommissioned if it is not used, if it underperforms the prompted baseline after drift, or if maintenance cost exceeds business value.

The financial services company implemented kill criteria after their initial wave of low-quality deployments. They terminated 40 percent of projects during the MVP experiment phase, 20 percent during full-scale training, and 30 percent at the evaluation gate. The projects that made it to production were the ones with strong evidence of value, which improved the return on investment for the entire fine-tuning program.

## Governance and Audit Trail

For regulated teams, the operating model must include governance and audit requirements. Every phase transition is logged. The intake decision, the scoping document, the training report, the gate review decision, and the deployment handoff are all stored in a version-controlled repository. If a regulator asks why a model was deployed, the team can produce the full audit trail from intake to deployment.

The governance model also defines who can approve fine-tuning projects. For low-risk tasks, the fine-tuning team can approve intake and the gate review can be lightweight. For high-risk tasks like credit decisioning, fraud detection, or clinical diagnostics, intake approval requires a VP-level sign-off and the gate review includes legal and compliance stakeholders. The governance model scales the rigor of the process to the risk of the task.

## Common Failures in Operating Models

The most common failure is skipping the intake phase and accepting all requests. This creates a pipeline of low-quality projects that waste resources. The second most common failure is skipping the evaluation gate and deploying models based solely on internal team review. This leads to models that fail in production or cause compliance incidents. The third most common failure is not transferring ownership at deployment, which turns the fine-tuning team into a permanent maintenance team and creates a bottleneck.

The fourth most common failure is not time-boxing the training phase. Teams iterate indefinitely trying to squeeze out marginal improvements, which delays deployment and increases cost. The fifth most common failure is not documenting decisions. When there is no audit trail, the organization cannot learn from failures or defend its decisions to regulators.

The sixth failure is allowing the intake form to be too easy. If teams can submit a request by filling out three fields in a web form, they will submit low-quality requests. The intake form must be deliberately burdensome, requiring evidence of baseline performance, failure mode analysis, and organizational readiness. The burden filters out teams who have not done the prerequisite work.

The seventh failure is not publishing the rejection reasons. When the fine-tuning team rejects a request, they should document why and share that documentation with the requesting team and with leadership. This creates institutional learning. Other teams see that "we want to fine-tune because it sounds cool" is not an acceptable justification and adjust their proposals accordingly.

The eighth failure is not tracking the return on investment for deployed models. The operating model must include a mechanism to measure whether deployed models are delivering the expected business value. If a model was deployed to reduce manual review costs by 200,000 dollars per year but actual savings are only 50,000 dollars, the requesting team must explain the gap. This accountability prevents teams from overstating the value of fine-tuning in their proposals.

The operating model prevents these failures by enforcing structure, requiring documentation, and making rejection the default at every gate.

## Scaling the Operating Model from One Team to Many

When fine-tuning starts as a capability in a single product team, the operating model can be informal. The team knows their own requirements, they understand their own risk tolerance, and they can make decisions quickly. When fine-tuning becomes a shared service supporting ten or twenty product teams, informality breaks down. You need formal process, clear roles, and explicit decision criteria.

Scaling the operating model requires capacity planning. The fine-tuning team must forecast how many projects they can support simultaneously. A team of three ML engineers can typically support four to six active fine-tuning projects at a time, from intake through deployment. Beyond that, quality degrades and timelines slip. If demand exceeds capacity, the fine-tuning team must either expand the team, tighten intake criteria to reduce demand, or extend timelines to flatten the workload.

Scaling also requires tooling investment. At small scale, the fine-tuning team can manage projects with spreadsheets and shared documents. At larger scale, they need a project management system that tracks intake requests, scoping documents, training experiments, gate reviews, and deployment handoffs. The system must provide visibility to requesting teams so they can see where their project is in the pipeline and when to expect delivery.

Scaling requires standardization. The fine-tuning team must define standard dataset formats, standard evaluation metrics, standard model card templates, and standard deployment patterns. Standardization reduces the cognitive load on the fine-tuning team and makes it easier to onboard new engineers. It also makes it easier for requesting teams to prepare their inputs because they know exactly what the fine-tuning team expects.

A large technology company scaled their fine-tuning service from supporting three teams to supporting thirty teams over the course of 2025. They hired six additional ML engineers, built a custom intake and project tracking system, created a library of reusable evaluation scripts and model card templates, and published detailed documentation on their internal wiki. The documentation included example scoping documents, annotated model cards from past projects, and a decision tree for determining whether fine-tuning was the right approach. The result was a 70% reduction in intake cycle time and a 50% increase in gate review pass rate, because requesting teams arrived better prepared.

## Communicating the Operating Model to Stakeholders

The operating model must be communicated clearly to all stakeholders: product teams who submit requests, leadership who funds the fine-tuning service, and governance teams who review deployments. The communication must set realistic expectations about timelines, rejection rates, and support boundaries.

The fine-tuning team publishes a **service charter** that describes what they do, what they do not do, and what they require from requesting teams. The charter makes explicit that the fine-tuning team is not a general-purpose ML consulting service. They do not build data pipelines, they do not design product features, they do not provide indefinite production support. They provide a scoped service: evaluating fine-tuning proposals, training models, gating deployments, and handing off to production teams.

The charter also sets expectations about rejection. "Most fine-tuning requests are rejected at intake because the problem does not require fine-tuning. Rejection is not a negative judgment on the requesting team. It is a signal that simpler approaches should be tried first." This framing reduces the stigma of rejection and encourages teams to submit requests early for feedback rather than waiting until they have invested heavily in the assumption that fine-tuning is necessary.

The fine-tuning team also publishes a **monthly metrics report** to leadership. The report shows the number of requests received, the number accepted at intake, the number of models trained, the number of models approved at the gate, and the number of models deployed to production. The report also highlights the business value delivered by deployed models, such as cost savings, accuracy improvements, or user satisfaction gains. This reporting demonstrates the impact of the fine-tuning service and justifies continued investment.

## The Role of the Program Manager

At scale, the fine-tuning service needs a program manager who is not an ML engineer but who coordinates the process. The program manager runs intake reviews, schedules gate meetings, tracks project timelines, communicates with requesting teams, escalates blockers, and maintains the project pipeline. This role is critical because it allows the ML engineers to focus on technical work rather than coordination overhead.

The program manager also serves as the interface between the fine-tuning team and governance stakeholders. They prepare the documentation for CAB reviews, collect signatures for approval workflows, and ensure that compliance requirements are met. They are the first point of contact for requesting teams and the primary communicator to leadership.

A financial services company initially did not have a program manager for their fine-tuning service. The ML engineers handled intake, coordination, and governance on top of their technical work. This created bottlenecks, missed deadlines, and incomplete documentation. When they hired a dedicated program manager in mid-2025, project throughput increased by 40% because the ML engineers could focus on training and evaluation while the program manager handled process.

## Learning from Post-Deployment Outcomes

The operating model must include a feedback loop from production back to intake and training. When a deployed model succeeds or fails in production, the fine-tuning team must learn from that outcome and update their processes accordingly.

If a deployed model delivers the expected business value, the fine-tuning team reviews what made that project successful. Was the intake process thorough? Did the training data have the right characteristics? Were the evaluation metrics predictive of production performance? The lessons are captured and incorporated into templates, checklists, and documentation for future projects.

If a deployed model fails to deliver value, underperforms, or causes incidents, the fine-tuning team conducts a retrospective. They ask: should we have rejected this project at intake? Did we miss risks at the evaluation gate? Was the training data representative of production? Was the requesting team ready to maintain the model? The findings drive process improvements. For example, if multiple models failed because requesting teams did not have monitoring infrastructure, the fine-tuning team adds monitoring readiness as a hard requirement at intake.

The financial services company tracked post-deployment outcomes for all fine-tuned models deployed in 2025. Of the ten models deployed, seven delivered the expected value, two delivered partial value, and one was decommissioned due to low usage. The retrospective on the decommissioned model revealed that the requesting team had not validated demand for the feature before requesting fine-tuning. The fine-tuning team updated the intake form to require evidence of user demand or stakeholder commitment, which reduced the rate of post-deployment decommissioning in subsequent projects.

## The Operating Model as Competitive Advantage

Organizations that run fine-tuning as a disciplined, repeatable service have a competitive advantage over organizations that treat fine-tuning as ad-hoc R&D. The disciplined approach delivers higher-quality models, faster cycle times, better alignment with business needs, and lower risk. It also scales more efficiently because the process is standardized and the team is not reinventing the workflow for every project.

The competitive advantage comes from the rejection rate. By filtering out projects that should not exist, the fine-tuning service concentrates resources on high-value projects. This increases the return on investment for the fine-tuning capability and builds credibility with leadership and requesting teams. Teams know that if the fine-tuning service accepts their project, it is because the project is genuinely worth doing, not because the fine-tuning team needed to justify their existence by accepting all requests.

The competitive advantage also comes from velocity. A well-run fine-tuning service can take a project from intake to production in eight to twelve weeks, compared to six to nine months for organizations without an operating model. The velocity comes from clear process, standard tooling, and experienced teams who have done this many times before.

The next subchapter covers the fine-tuning change advisory process for regulated teams, where every model deployment is treated as a change to a production system and requires formal approval, documentation, and audit trails.
