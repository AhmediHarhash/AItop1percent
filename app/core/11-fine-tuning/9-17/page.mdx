# 9.17 — Downstream Modifier Obligations: Documentation Pack and Regulatory Handoff

**If you fine-tune a GPAI model and deploy it in or for the EU market, you are a downstream modifier with explicit legal obligations under the EU AI Act.** This is not optional. This is not a future concern. The EU AI Act has been in force since August 2024, with GPAI downstream modifier obligations effective from August 2025. Enforcement is active, procurement teams are demanding compliance evidence, and ignorance of the law is explicitly not a defense. You must produce structured technical documentation, conduct risk assessments, issue conformity declarations, publish model cards, demonstrate data governance compliance, provide evaluation reports, and establish post-market monitoring plans. If you cannot produce this documentation pack on demand, you cannot legally deploy in the EU, you cannot sell to EU customers, and you expose your organization to regulatory enforcement and contract rejection. January 2026: legal technology startup preparing to sell contract analysis tool powered by fine-tuned GPT-4 to multinational law firm. Procurement due diligence: law firm compliance team requested technical documentation required under EU AI Act for downstream modifiers. Startup had no documentation. They had fine-tuned the model, evaluated it on internal test cases, deployed it. They had not created structured technical documentation, risk assessment, conformity declaration, or model card that EU AI Act requires. Law firm compliance rejected procurement, citing unacceptable regulatory risk. Startup engaged external counsel and regulatory compliance consultant to produce required documentation retroactively. Process took eleven weeks, cost two hundred eighty thousand dollars. By the time documentation was complete, law firm had selected a competitor. Startup lost one point eight million dollar contract because they did not understand their downstream modifier obligations.

The failure was ignorance of the **downstream modifier documentation pack**—the set of formal artifacts you must produce when you fine-tune a general-purpose AI model and deploy it in or for the EU market. The EU AI Act does not just impose abstract obligations. It requires specific, structured documentation that demonstrates compliance with GPAI standards, risk management requirements, and transparency obligations. If you fine-tune a GPAI model, you must produce this documentation before deployment, not after a customer asks for it. In 2026, with the EU AI Act in full enforcement and procurement teams demanding compliance evidence, the documentation pack is a commercial necessity, a regulatory requirement, and a professional standard.

## The Documentation Pack Contents

The **downstream modifier documentation pack** consists of seven core components: the technical documentation file, the risk assessment and mitigation record, the conformity assessment declaration, the model card, the data governance record, the evaluation report, and the post-market monitoring plan.

The **technical documentation file** is the central artifact. It describes the base model and version, the modifications made through fine-tuning, the training data sources and characteristics, the training methodology and hyperparameters, the intended purpose and use cases, the known limitations and failure modes, and the risk mitigations implemented. The technical documentation must be comprehensive enough for a regulator or technical auditor to understand what you built, how you built it, and what risks it presents.

The **risk assessment and mitigation record** documents the risk analysis you conducted before and during fine-tuning. It identifies the risk classification of the AI system—high-risk, limited-risk, or minimal-risk under the EU AI Act—the specific risks introduced or modified by fine-tuning, the likelihood and severity of each risk, and the mitigations implemented to reduce risk to acceptable levels. For high-risk systems, this record must also address the specific requirements in Annex III of the EU AI Act.

The **conformity assessment declaration** is a formal statement that the fine-tuned model complies with applicable requirements of the EU AI Act. For high-risk systems, this declaration must be signed by a notified body after third-party conformity assessment. For lower-risk systems, you can self-assess and self-declare conformity, but the declaration must still be formal and documented.

The **model card** is a structured, user-facing description of the model's capabilities, limitations, intended use, and evaluation results. It follows the model card framework developed by the AI research community and adopted as a best practice by the EU AI Act. The model card is intended for downstream users—customers, developers, compliance teams—who need to understand the model's behavior and appropriate use.

The **data governance record** documents the data used for fine-tuning, including data sources, legal basis for processing, data quality measures, bias assessment and mitigation, and consent or contractual permissions. For models trained on personal data, this record must demonstrate GDPR compliance.

The **evaluation report** documents the evaluations conducted before deployment, including the evaluation framework, test datasets, metrics and thresholds, results for each evaluation, and the pass/fail determination. This report provides evidence that the model meets your quality and safety standards.

The **post-market monitoring plan** describes how you will monitor the model after deployment, including metrics tracked, monitoring frequency, incident response procedures, and feedback loops for continuous improvement. This plan demonstrates that you have a systematic approach to identifying and addressing issues that emerge in production.

## Technical Documentation Structure

The technical documentation file follows a standardized structure to ensure consistency and completeness. The structure includes an executive summary, system description, modification description, data description, training methodology, evaluation summary, risk analysis, and usage instructions.

The **executive summary** is a one-to-two-page overview for non-technical readers. It describes what the model does, who it is for, what modifications were made, and what the key risks and mitigations are. This section is designed for executives, legal counsel, and procurement teams who need to understand the model without technical depth.

The **system description** provides technical details about the base model and the fine-tuned model. It includes the base model identifier and version, the model architecture and parameter count, the training framework and infrastructure, and the deployment environment and serving configuration. This section enables technical reviewers to understand the system architecture.

The **modification description** explains what changes were made through fine-tuning. It includes the fine-tuning objective and intended improvements, the training data scale and characteristics, the hyperparameters and training duration, and the changes in model behavior compared to the base model. This section demonstrates that fine-tuning was conducted systematically with a clear purpose.

The **data description** details the training data used for fine-tuning. It includes data sources and collection methods, date ranges and geographic scope, data volume and composition, data quality measures and validation, PII handling and redaction, and known biases or limitations. This section enables reviewers to assess data governance and representativeness.

The **training methodology** describes how fine-tuning was conducted. It includes the training algorithm and framework, learning rate schedule and optimizer, batch size and number of epochs, convergence criteria and early stopping, and reproducibility measures including random seeds. This section demonstrates methodological rigor and enables reproducibility.

The **evaluation summary** presents the evaluation results that justified deployment. It includes the evaluation framework and test datasets, key metrics and thresholds, results for each metric, comparison to baseline or alternative models, and limitations of the evaluation. This section provides evidence of quality and safety.

The **risk analysis** summarizes the risks identified and mitigated. It references the detailed risk assessment record and highlights the most significant risks and mitigations. This section enables reviewers to quickly assess the risk profile.

The **usage instructions** provide guidance for appropriate use. It includes intended use cases and user populations, use cases that are out of scope, required human oversight or safeguards, performance expectations and limitations, and monitoring and maintenance recommendations. This section ensures downstream users understand how to use the model safely and effectively.

## Risk Assessment Requirements

The risk assessment must follow a structured methodology and address both technical and societal risks. Technical risks include model failures—incorrect outputs, hallucinations, adversarial vulnerabilities—and system failures—latency, availability, security. Societal risks include bias and discrimination, privacy violations, safety hazards, and misuse potential.

For each risk, you document the risk description, likelihood assessment, severity assessment, overall risk level, and mitigation measures. You use a risk matrix—typically a three-by-three or five-by-five matrix mapping likelihood to severity—to prioritize risks.

For high-risk AI systems under the EU AI Act—systems used for hiring, credit scoring, law enforcement, critical infrastructure—you must also address the specific risk requirements in Annex III. This includes ensuring human oversight is possible, implementing logging and traceability, achieving appropriate accuracy and robustness, and mitigating bias against protected groups.

The risk assessment must be updated whenever the model is retrained, the use case changes, or new risks are identified through post-market monitoring. It is a living document, not a one-time artifact.

## Conformity Assessment Process

For high-risk AI systems, you must undergo third-party conformity assessment by a notified body before deployment. The notified body reviews your technical documentation, risk assessment, evaluation results, and quality management system, and issues a conformity certificate if the system meets EU AI Act requirements.

For systems that are not high-risk, you can conduct internal conformity assessment. You review your own documentation, verify that the system meets applicable requirements, and issue a declaration of conformity. The declaration must be formal and must reference the specific requirements you assessed.

Conformity assessment is not a one-time event. You must reassess conformity whenever you make substantial modifications to the model, which includes retraining on new data or changing the intended purpose. If you retrain quarterly, you must reassess conformity quarterly.

Some organizations implement continuous conformity assessment by automating the evaluation and documentation process. Every time a model is trained, the evaluation pipeline runs the required conformity tests, generates conformity evidence, and updates the technical documentation automatically. This reduces the marginal cost of conformity assessment for each model version.

## Model Card Standards

The model card is the user-facing component of the documentation pack. It must be clear, concise, and accessible to non-experts. The standard model card structure includes model details, intended use, factors affecting performance, metrics, evaluation data, training data, ethical considerations, caveats and recommendations, and references.

**Model details** include the model version, base model, release date, model type, and contact information for questions. This section provides basic identification.

**Intended use** describes the primary use cases, intended users, and out-of-scope uses. This section sets expectations for appropriate use.

**Factors affecting performance** identify the conditions under which the model performs well or poorly. This includes language, domain, input length, and demographic factors.

**Metrics** list the key performance metrics and their values. This includes accuracy, precision, recall, F1, and any domain-specific metrics.

**Evaluation data** describes the test datasets used for evaluation, including their size, composition, and representativeness.

**Training data** describes the training dataset characteristics, limitations, and known biases.

**Ethical considerations** address potential harms, bias, privacy risks, and societal impact. This section demonstrates that you have considered the broader implications of deployment.

**Caveats and recommendations** provide guidance on limitations, failure modes, and best practices for use. This section helps users avoid misuse.

**References** link to additional documentation, research papers, or external resources.

The model card should be published in an accessible format—PDF, HTML, or markdown—and made available to all users. For commercial deployments, the model card is typically included in product documentation or provided during onboarding.

## Data Governance Evidence

The data governance record must demonstrate compliance with GDPR and the EU AI Act's data governance requirements. For each dataset used in fine-tuning, you document the data source and legal basis for processing, the consent mechanism or contractual permission, the data retention period and deletion procedures, the data quality checks and validation, the PII redaction or anonymization procedures, the bias assessment and mitigation, and the data access controls and security.

If you used personal data, you must demonstrate that you have a lawful basis under GDPR Article 6—consent, contract, legal obligation, vital interests, public task, or legitimate interests. For special category data—health, biometric, genetic—you must also satisfy an Article 9 condition.

You must document how you obtained consent or contractual permission to use the data for model training. Many standard data processing agreements do not explicitly permit model training, and you may need to amend contracts or obtain separate consent.

You must document your data retention policy and demonstrate that training data is deleted or anonymized after the retention period expires. For models trained on personal data, indefinite retention is generally not permissible under GDPR.

## Evaluation Report Structure

The evaluation report documents the evaluations conducted before deployment. It includes an evaluation framework description, test dataset specifications, metric definitions and thresholds, results for each metric, comparison to baselines or alternative models, error analysis and failure mode investigation, and the final pass/fail determination.

The evaluation framework description explains what you evaluated and why. It maps evaluation metrics to the risks identified in the risk assessment and to the intended use cases.

The test dataset specifications describe the datasets used for evaluation, including size, composition, representativeness, and any known limitations. If the test set is not representative of production data, you must document that limitation.

The metric definitions and thresholds specify exactly what was measured and what thresholds were required for deployment. This includes accuracy, precision, recall, fairness metrics, robustness metrics, and any domain-specific metrics.

The results section presents the metric values achieved and compares them to the thresholds. It should include confidence intervals or statistical significance tests where applicable.

The error analysis investigates failure modes, edge cases, and demographic performance differences. This section demonstrates that you understand where the model performs poorly and have mitigated those risks.

The pass/fail determination states whether the model met all required thresholds and was approved for deployment. If any metric failed to meet the threshold, you must document the waiver decision or risk acceptance.

## Post-Market Monitoring Plan

The post-market monitoring plan describes how you will monitor the model after deployment. It includes the metrics tracked in production, monitoring frequency and alert thresholds, incident detection and response procedures, feedback collection mechanisms, and model update and retraining triggers.

You must define production metrics that align with the evaluation metrics used before deployment. If you evaluated accuracy on a test set, you must monitor accuracy in production using human review or automated proxies.

You must specify monitoring frequency—real-time, daily, weekly—and alert thresholds that trigger investigation when metrics degrade.

You must document incident detection and response procedures, including severity classification, notification timelines, and remediation steps.

You must describe how you collect feedback from users, customers, and monitoring systems, and how that feedback informs model updates.

You must define the conditions under which you will retrain or retire the model, including performance degradation thresholds, regulatory changes, or user feedback patterns.

## Regulatory Handoff to Downstream Users

If you provide your fine-tuned model to downstream users—customers, partners, or third-party developers—you must provide them with sufficient documentation to meet their own regulatory obligations. This is the **regulatory handoff**—transferring compliance evidence and usage guidance to the next party in the value chain.

The regulatory handoff includes providing the model card, relevant sections of the technical documentation, usage instructions and limitations, evaluation results and test coverage, risk assessment summary, and contact information for support and incident reporting.

You must also specify in your terms of service or license agreement the permitted uses, prohibited uses, required safeguards, and monitoring obligations for downstream users. If a downstream user deploys your model in a high-risk context, they inherit high-risk system obligations, and you must ensure they understand those obligations.

Some organizations provide a **compliance starter kit** to downstream users, including sample risk assessments, evaluation templates, and monitoring guidelines tailored to common use cases. This reduces the compliance burden on users and increases the likelihood of safe and compliant deployment.

## Operational Integration

The documentation pack is not a one-time deliverable. It must be integrated into your operational workflow so that it is automatically generated and updated for every model version. You should template the technical documentation, risk assessment, and model card, and populate them automatically from your lineage system, evaluation pipeline, and model registry.

When a training run completes, the lineage system records all required metadata. When evaluation completes, the evaluation pipeline generates the evaluation report. When a model is promoted to production, the registry generates the conformity declaration and model card. The complete documentation pack is assembled automatically and stored with the model artifact.

This automation ensures consistency, reduces manual effort, and prevents documentation gaps. It also enables you to produce documentation on demand for audits, customer requests, or regulatory inquiries.

The EU AI Act downstream modifier obligations are not theoretical. They are in force now, with active enforcement and increasing scrutiny. If you fine-tune GPAI models and operate in or serve the EU market, you must produce the documentation pack, conduct conformity assessment, and provide regulatory handoff to downstream users. The organizations that treat compliance as a checklist exercise or an afterthought will face enforcement, lose customers, and incur significant remediation costs. The organizations that integrate compliance into their operational workflow from day one will operate with confidence, demonstrate professionalism, and build trust with customers and regulators.

This is the final subchapter of Section 11. We have covered the full lifecycle of fine-tuning and model adaptation, from problem definition through production deployment and regulatory compliance. The next section will address the operational challenges of scaling AI systems across the organization, including platform architecture, developer experience, and organizational governance.
