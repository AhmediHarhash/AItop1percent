# 7.8 — A/B Testing Fine-Tuned vs Base: Experiment Design and Statistical Rigor

In June 2025, a SaaS company deployed a fine-tuned model for customer support automation after running an A/B test that showed a nine percent improvement in resolution rate. The test had run for five days with two thousand users in each group. Three weeks after full deployment, customer satisfaction scores dropped by fourteen percent, escalation rates increased by twenty-one percent, and the support team was overwhelmed with complaints about unhelpful automated responses. When the team reviewed the A/B test methodology, they discovered that the test had been statistically underpowered, the sample size calculation had ignored variance in user behavior, and the five-day window had captured an unrepresentative slice of support traffic—missing the monthly billing cycle when complex issues spike. The nine percent improvement had been noise, not signal. The decision to deploy had been based on a test that was never capable of detecting the real performance difference. The rollback cost them six weeks of engineering time, $800,000 in lost productivity, and the trust of forty thousand users who had experienced degraded service. The root cause was not a bad model. It was a bad experiment.

## Why A/B Testing Fine-Tuned Models Is Different

A/B testing a fine-tuned model is not the same as A/B testing a UI change or a pricing strategy. Model outputs are stochastic, user interactions are sequential, and the metrics you care about are often delayed. A user might receive a response from the fine-tuned model today but not provide feedback until tomorrow. A customer might accept a recommendation from the model in one session but churn three weeks later because the recommendation was subtly wrong. A content moderation decision might look correct in isolation but create cumulative frustration when repeated across dozens of interactions.

This means you cannot run a naive A/B test where you split traffic randomly, measure immediate outcomes, and declare a winner. You must account for variance, you must choose metrics that capture delayed effects, you must run the test long enough to observe representative behavior, and you must ensure that your sample size gives you the statistical power to detect meaningful differences. If you skip any of these steps, your test will produce unreliable results, and you will make deployment decisions based on noise.

The standard for 2026 is that every fine-tuned model must pass a rigorous A/B test before full deployment. You do not ship on intuition. You do not ship on internal evaluation results alone. You ship when a properly designed experiment provides statistically significant evidence that the fine-tuned model outperforms the baseline in production conditions. Anything less is guesswork.

## Choosing the Right Baseline for Comparison

The first decision in your A/B test is selecting the baseline model. The most common baseline is the current production model—the one you are trying to replace. This is the right choice when you are improving an existing system. You want to know whether the fine-tuned model is better than what users are experiencing today.

In some cases, the right baseline is the base model before fine-tuning. This is appropriate when you are deploying a fine-tuned model for the first time and there is no existing production system. You want to know whether fine-tuning improves over the out-of-the-box performance of the base model.

In other cases, you might compare the fine-tuned model to multiple baselines simultaneously: the current production model, the base model, and a previous fine-tuned version. This is appropriate when you are iterating on fine-tuning and you want to ensure that each iteration is genuinely improving over all prior versions, not just trading off performance in different dimensions.

You must choose your baseline before you begin the experiment. You must document the rationale. You must ensure that all stakeholders agree on what success looks like. If Product expects the fine-tuned model to beat the current production system by ten percent, but Engineering is comparing it to the base model and celebrating a five percent improvement, you will have a misalignment that leads to bad deployment decisions.

## Sample Size Calculation: Power Analysis Before You Start

The most common mistake in A/B testing fine-tuned models is running the test without calculating the required sample size. You cannot decide to test two thousand users per group because that feels like a reasonable number. You must calculate the minimum sample size required to detect the effect size you care about with the statistical power you need.

**Statistical power** is the probability that your test will detect a true difference if one exists. The standard threshold is eighty percent power, meaning if the fine-tuned model is genuinely better, your test has an eighty percent chance of detecting that improvement and rejecting the null hypothesis. If your test is underpowered, you risk concluding that there is no difference when in fact the fine-tuned model is better—or worse, concluding that the fine-tuned model is better when the observed difference is just random variance.

To calculate sample size, you need three inputs: the minimum effect size you care about, the baseline variance in your metric, and your desired significance level and power. The minimum effect size is a business decision. If a five percent improvement in resolution rate is not worth deploying, then you need to power your test to detect at least a five percent difference. The baseline variance comes from historical data on your current production system. The significance level is typically set to five percent, meaning you are willing to accept a five percent chance of a false positive. The power is typically set to eighty percent, as described above.

You plug these inputs into a power analysis formula or tool, and it tells you the minimum number of observations you need in each group. If the calculation says you need twelve thousand users per group, you do not run the test with two thousand users per group and hope for the best. You either commit to the required sample size, or you acknowledge that you cannot run a statistically valid test and you find another way to evaluate the model.

This is not optional. Running an underpowered test is worse than running no test at all, because it gives you the illusion of evidence without the reality of it.

## Randomization: Avoiding Selection Bias

Randomization is the foundation of a valid A/B test. Users must be randomly assigned to the treatment group, which receives outputs from the fine-tuned model, and the control group, which receives outputs from the baseline model. Random assignment ensures that the two groups are statistically equivalent in all characteristics except the model they are exposed to, which means any difference in outcomes can be attributed to the model and not to confounding factors.

In practice, randomization is more complex than flipping a coin for each user. You must decide the unit of randomization: is it the user, the session, the request? For most applications, the unit of randomization is the user, because you want each user to have a consistent experience across all their interactions. If you randomize at the request level, a single user might receive outputs from the fine-tuned model in one request and the baseline model in the next, which creates a confusing experience and makes it difficult to measure user-level outcomes like satisfaction or retention.

You must also ensure that randomization is truly random and not influenced by implementation bugs. A surprising number of A/B tests fail because the randomization logic is deterministic but non-uniform—for example, assigning users to groups based on the last digit of their user ID, which works fine unless user IDs are assigned sequentially and certain digits are correlated with signup date or user type. Use a proper random number generator, seed it with a fixed value for reproducibility, and verify that the distribution of users across groups is balanced before you start measuring outcomes.

## Metric Selection: Primary, Secondary, and Guardrail Metrics

Your A/B test needs three types of metrics: a **primary metric**, **secondary metrics**, and **guardrail metrics**. The primary metric is the single most important outcome you are optimizing for. It is the metric that determines whether you deploy the fine-tuned model. If the fine-tuned model does not improve the primary metric with statistical significance, you do not deploy.

For a customer support model, the primary metric might be resolution rate: the percentage of conversations that end without escalation to a human agent. For a recommendation model, the primary metric might be click-through rate on recommended items. For a content generation model, the primary metric might be user acceptance rate: the percentage of generated outputs that users keep without editing.

Secondary metrics are outcomes you care about but are not the primary decision criterion. They provide additional context and help you understand whether the improvement in the primary metric comes with tradeoffs. For a customer support model, secondary metrics might include average conversation length, user satisfaction score, and time to resolution. You want the fine-tuned model to improve resolution rate without making conversations longer, without decreasing satisfaction, and without taking more time to resolve issues.

Guardrail metrics are outcomes that must not degrade. They are red lines that, if crossed, block deployment even if the primary metric improves. For most applications, latency is a guardrail metric: the fine-tuned model must not be slower than the baseline. Safety metrics are also guardrails: the fine-tuned model must not increase the rate of harmful outputs, biased outputs, or policy violations. If any guardrail metric degrades beyond a predefined threshold, the test is stopped and the fine-tuned model is not deployed.

You must define all three types of metrics before the test begins. You must document the thresholds for significance, the acceptable tradeoffs, and the guardrail limits. You do not define these during the test based on what the data shows. That is data snooping, and it invalidates the test.

## Duration and Temporal Effects: Running the Test Long Enough

The duration of your A/B test is not arbitrary. You must run the test long enough to observe representative user behavior and to ensure that short-term effects are not mistaken for long-term outcomes. A test that runs for three days might capture an initial novelty effect where users engage more with the fine-tuned model simply because it is different, but that effect disappears after a week. A test that runs during a holiday period might show different behavior than a test that runs during normal business weeks.

The minimum test duration is typically one full cycle of whatever temporal pattern is relevant to your application. For a B2B SaaS product with monthly billing cycles, you should run the test for at least one full month to capture the spike in support requests that happens at billing time. For a consumer app with weekly usage patterns, you should run the test for at least two full weeks to smooth out weekend versus weekday effects. For a model that generates content for news articles, you should run the test long enough to cover different news cycles and events.

You must also account for delayed metrics. If your primary metric is user retention, and you define retention as users who return within thirty days, you cannot measure the full effect until thirty days after the test begins. You must run the test for at least thirty days, and then wait another thirty days to measure retention for users who entered the test on the last day.

Rushing the test is one of the most common ways to reach false conclusions. If you are under pressure to ship quickly, the correct response is not to shorten the test duration. The correct response is to push back on the timeline and explain why a shorter test will not produce reliable results.

## Statistical Significance and Confidence Intervals

Statistical significance is the standard for determining whether the observed difference between the fine-tuned model and the baseline is real or due to random chance. The conventional threshold is a p-value of less than 0.05, meaning there is less than a five percent probability that the observed difference would occur if the two models were actually equivalent.

But statistical significance alone is not sufficient. You must also look at the **confidence interval** around the estimated effect size. A result can be statistically significant but practically meaningless if the confidence interval is very wide or if the point estimate is just barely above your minimum effect size. For example, if your minimum effect size is five percent and the test shows an improvement of 5.2 percent with a 95 percent confidence interval of 0.3 percent to 10.1 percent, the result is statistically significant, but the uncertainty is large and the improvement might be smaller than you need.

In this case, you have three options: run the test longer to narrow the confidence interval, accept the uncertainty and deploy with close monitoring, or decide that the evidence is insufficient and do not deploy. Which option you choose depends on the risk profile of your application. For a low-stakes recommendation system, you might deploy and monitor. For a high-stakes medical decision system, you would not deploy until the confidence interval is tight and the lower bound is well above your minimum effect size.

You must report both the p-value and the confidence interval to stakeholders. You must explain what they mean in plain language. You do not say "the result is significant" and stop there. You say "the fine-tuned model improves resolution rate by 7.3 percent, with a 95 percent confidence interval of 4.1 percent to 10.5 percent, and the probability that this result is due to chance is less than one percent."

## Common A/B Testing Mistakes in ML

The first common mistake is **peeking at the results** before the test is complete. You cannot check the results every day, decide to stop the test early when the fine-tuned model is winning, and declare success. Every time you peek, you increase the probability of a false positive. The correct approach is to predefine the test duration and sample size, run the test to completion, and then analyze the results once.

The second mistake is **testing multiple variants** without correcting for multiple comparisons. If you test three different fine-tuned models against the baseline simultaneously, you are running three hypothesis tests, which increases the probability of finding a false positive. You must apply a correction like the Bonferroni correction, which adjusts the significance threshold based on the number of comparisons.

The third mistake is **ignoring interaction effects**. If your A/B test splits users randomly but the fine-tuned model performs better for power users and worse for casual users, the aggregate result might show no difference even though there are important effects in subgroups. You must predefine the subgroups you care about and analyze results separately for each subgroup.

The fourth mistake is **not accounting for network effects**. If users in the treatment group interact with users in the control group, the independence assumption is violated and the test results are biased. This is rare for most applications, but it is critical for social features, recommendation systems, and marketplace models.

The fifth mistake is **changing the model during the test**. If you notice a bug in the fine-tuned model halfway through the test, fix it, and continue the test, you have invalidated the test. You must stop the test, fix the bug, and restart from the beginning.

## When A/B Tests Are Conclusive vs Inconclusive

An A/B test is conclusive when the primary metric shows a statistically significant difference with a confidence interval that is narrow enough to make a clear decision. The fine-tuned model is better, or it is worse, or it is equivalent within the bounds of acceptable tradeoffs. You have the evidence you need to deploy or not deploy.

An A/B test is inconclusive when the primary metric shows no statistically significant difference, but the confidence interval is wide, or when the primary metric improves but secondary metrics degrade in ways that make the tradeoff unclear. In this case, you have three options: run the test longer to gather more data, run a follow-up test with a refined hypothesis, or decide that the fine-tuned model is not worth deploying based on the lack of clear evidence.

You must document the outcome of every A/B test, whether conclusive or inconclusive. You must explain what you learned, what decisions were made, and what follow-up actions are planned. You do not bury inconclusive results. You treat them as valuable information that prevents you from deploying a model that is not clearly better than the baseline.

## A/B Testing as a Release Gate

A/B testing is not optional for fine-tuned models that serve users directly. It is a mandatory release gate. Before you deploy a fine-tuned model to production, you must run an A/B test that demonstrates statistically significant improvement in the primary metric, no degradation in guardrail metrics, and acceptable tradeoffs in secondary metrics. If the test does not pass, the model does not ship.

This requires integrating A/B testing into your deployment pipeline. You must have infrastructure that can split traffic, assign users to groups, log outputs, measure metrics, and report results. You must have a process for reviewing results with Product, Engineering, and leadership before making the deployment decision. You must have clear thresholds and decision criteria that everyone agrees on before the test begins.

This is the standard for 2026. You do not ship fine-tuned models on vibes. You ship on evidence. The A/B test is the evidence.

The next subchapter covers automated regression detection, where golden test sets ensure that fine-tuned models do not lose core capabilities even as they gain domain-specific performance.
