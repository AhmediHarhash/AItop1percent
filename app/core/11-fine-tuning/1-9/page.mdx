# 1.9 — The Fine-Tuning Proposal Document: What to Present to Leadership

The biggest risk in fine-tuning proposals is not technical failure — it is approval failure after the work is already done. Most engineering teams write proposals that optimize for technical correctness while ignoring the business questions that leadership actually cares about. They document model architecture and hyperparameters but omit total cost of ownership. They prove the model works but never explain why the problem is worth solving. They demonstrate improved accuracy but cannot quantify the revenue impact. The result is a pattern that repeats across organizations: engineers spend weeks building models that never get deployed because they could not answer basic approval questions before starting work. A customer support platform in late 2025 built a fine-tuned model that improved classification F1 score from 0.84 to 0.91 over six weeks at a cost of $18,000, only to have the project shelved when their VP asked four questions they could not answer: what did this cost compared to prompt engineering, what happens when the vendor changes their API, who maintains the model after launch, and what is the actual revenue impact of better classification. The team had no proposal document, no cost comparison, no maintenance plan, and no business case. The diagnosis was clear: the failure happened not during training but during scoping, when the team treated approval as a formality instead of as a structured business case that must convince non-technical stakeholders to fund a project with uncertain returns and ongoing costs.

This happens constantly. Engineers treat fine-tuning as a technical decision and skip the approval process. Leadership rejects projects not because the technology is wrong but because the proposal fails to answer basic questions about cost, risk, and maintenance. A fine-tuning proposal is not a technical spec. It is a business document that must convince non-technical stakeholders to fund a project with uncertain returns and ongoing costs. Most engineers write terrible proposals because they focus on model architecture instead of business impact, because they omit cost comparisons, and because they ignore the maintenance burden that leadership will inherit. This subchapter teaches you how to write a proposal that gets approved.

## The Eight Required Components

Every fine-tuning proposal must contain eight components in a specific order. Leadership reads these documents in five minutes. They are not reading your training methodology or hyperparameter choices. They are scanning for risk, cost, and accountability. The structure is: problem statement, alternatives exhausted, data readiness, success metrics, cost estimate, timeline, risks, and maintenance plan. This order is deliberate. You lead with the problem to establish urgency, then prove you tried cheaper alternatives, then demonstrate you have the data required, then define how you will measure success, then present the cost, then commit to a timeline, then acknowledge risks, and finally show you have a plan for ongoing maintenance.

Most engineers start with the solution. They write proposals that say "we should fine-tune GPT-5 on our data to improve accuracy." This is backwards. Leadership does not care about your solution until they understand the problem and believe you have exhausted simpler options. A strong proposal starts with a quantified problem statement: "Our current prompt-based classification system costs $47,000 per month in API calls and achieves 84% accuracy, causing 1,200 misrouted tickets per week, which generates an estimated $180,000 in annual support overhead from tier-one agents escalating incorrectly classified requests." This statement includes cost, performance, volume, and business impact. Leadership can immediately assess whether this problem is worth solving.

The second component is alternatives exhausted. You must demonstrate that you tried prompt engineering, few-shot examples, retrieval augmentation, and prompt chaining before concluding that fine-tuning is necessary. Leadership will assume you skipped these steps unless you explicitly document them. A strong alternatives section includes: "We attempted improved prompt engineering, achieving 86% accuracy at $51,000 per month. We tested retrieval-augmented generation with a vector database of historical tickets, achieving 85% accuracy at $39,000 per month including infrastructure costs. We implemented a two-stage prompt chain with validation, achieving 87% accuracy at $44,000 per month. All three approaches remained below the 90% accuracy threshold required to reduce escalations by 50%." This proves you did the work and establishes that fine-tuning is the last option, not the first.

Data readiness is the third component and the most commonly omitted. Leadership has funded ML projects that failed because the training data did not exist or was lower quality than promised. You must specify the exact dataset you will use: "We have 78,000 labeled support tickets from January 2024 through December 2025, with labels reviewed by senior support agents. The dataset includes 23 ticket categories with an average of 3,400 examples per category, ranging from 890 examples for the rarest category to 8,100 for the most common. We conducted a quality audit on a random sample of 2,000 tickets and found 96% label agreement between the original labeler and an independent reviewer. The data is stored in our production database and can be exported in three hours." This level of detail proves you have done the preparation and are not making optimistic assumptions about data availability.

Success metrics is the fourth component. You must define both technical metrics and business metrics, with specific thresholds for launch approval. A weak proposal says "we will improve accuracy." A strong proposal says "we will achieve 92% accuracy on a held-out test set of 15,000 tickets labeled in January 2026, measured as macro-averaged F1 score across all 23 categories. This accuracy level will reduce misrouted tickets from 1,200 per week to under 500, saving an estimated 35 hours per week of tier-one agent time, valued at $91,000 annually. We will consider the project successful only if we exceed 90% accuracy and reduce API costs below $30,000 per month." This gives leadership a clear decision criterion: if you hit these numbers, you launch; if you do not, you kill the project.

## The Cost Estimate That Leadership Actually Needs

The fifth component is the cost estimate, and this is where most proposals fail. Engineers present training costs but omit inference costs, maintenance costs, and opportunity costs. Leadership needs a total cost of ownership comparison between the current system and the fine-tuned system over a twelve-month period. A complete cost estimate includes: training data preparation, model training, evaluation infrastructure, inference hosting, ongoing monitoring, model retraining, and engineering time for maintenance. Most fine-tuning projects have lower per-request costs than prompt-based systems but higher fixed costs for infrastructure and maintenance. Leadership needs to see the break-even point.

A strong cost estimate looks like this: "Current system: $47,000 per month in API calls to GPT-5, zero infrastructure costs, minimal engineering maintenance, total annual cost $564,000. Proposed fine-tuned system: one-time training cost of $24,000 including data labeling review and validation, $12,000 per month in inference hosting on dedicated GPUs, $3,000 per month in monitoring infrastructure, quarterly retraining at $8,000 per cycle, and an estimated 15 hours per month of engineering time for monitoring and maintenance valued at $18,000 annually. Total first-year cost: $24,000 plus $180,000 hosting plus $36,000 monitoring plus $32,000 retraining plus $18,000 engineering time, totaling $290,000. Annual savings: $274,000. Break-even month: four." This estimate is credible because it includes all costs and shows when the investment pays back.

Most engineers underestimate inference costs because they test on small sample sizes and extrapolate linearly. A fine-tuned model that costs $0.08 per classification on 100 requests per hour will cost $70,000 per month when you scale to 30,000 requests per hour, not the $2,400 you calculated during testing. You must estimate costs at production volume, not development volume. Leadership has been burned by projects that looked cheap in testing and became expensive at scale. If you present optimistic cost estimates, you will lose credibility when actual costs exceed your projections. It is better to overestimate and deliver savings than to underestimate and request additional budget six months after launch.

Opportunity cost is the hidden cost that experienced leaders think about and junior engineers ignore. If your team spends two months building a fine-tuned model, they are not building other features. Leadership is choosing between funding your fine-tuning project and funding three smaller projects that ship faster. You must acknowledge this tradeoff explicitly: "This project will require 320 hours of engineering time over eight weeks, representing approximately 40% of our team's capacity during that period. We will delay the planned conversational analytics feature and the sentiment analysis improvement to prioritize fine-tuning. We believe this tradeoff is justified because misrouted tickets are our highest-impact quality problem and this is the only path to solving it." This framing shows you understand the cost is not just money but also delayed feature development.

## Timeline, Risks, and the Maintenance Plan That Gets Ignored

The sixth component is the timeline, and it must be realistic. Most engineers present optimistic timelines that assume everything works on the first attempt. Leadership has seen ML projects double their estimated timelines because of data quality issues, convergence problems, and evaluation challenges. A credible timeline includes buffer time and acknowledges dependencies: "Week 1-2: data export, cleaning, and train-test split. Week 3-4: baseline model training and initial evaluation. Week 5-6: hyperparameter tuning and architecture experiments. Week 7: final model training and comprehensive evaluation. Week 8: integration testing and deployment preparation. Total timeline: eight weeks from approval to production readiness, assuming no major data quality issues or convergence problems. If we discover labeling inconsistencies or the model does not converge, we estimate an additional two to four weeks for remediation." This timeline is believable because it includes contingencies.

The seventh component is the risk section, and most proposals omit it entirely. Leadership appreciates when you identify risks before they ask. A strong risk section addresses: training failure, performance degradation over time, vendor lock-in, regulatory compliance, and model drift. For example: "Primary risk: the model does not achieve 90% accuracy within the eight-week timeline, requiring us to abandon the project or extend the timeline. Mitigation: we will conduct a checkpoint evaluation at week four and make a go or no-go decision based on intermediate results. Secondary risk: the fine-tuned model degrades over time as ticket patterns change. Mitigation: we will implement quarterly retraining and weekly performance monitoring to detect drift. Tertiary risk: our chosen provider changes pricing or deprecates the fine-tuning API. Mitigation: we will design our training pipeline to be provider-agnostic and maintain the ability to retrain on alternative platforms."

The eighth component is the maintenance plan, and this is the component that leadership cares about most and engineers think about least. A fine-tuned model is not a one-time project. It is an ongoing operational system that requires monitoring, retraining, incident response, and eventually replacement. Leadership is committing to years of maintenance costs, not just the initial training cost. If you do not have a maintenance plan, your proposal will be rejected or approved with conditions that you cannot meet. A complete maintenance plan specifies: who monitors the model, how often you retrain, what triggers emergency retraining, how you handle incidents, and when you will sunset the model.

A strong maintenance plan looks like this: "The ML infrastructure team will own ongoing model monitoring using our existing observability stack. We will track daily accuracy on a rolling 1,000-ticket sample, with alerts if accuracy drops below 88%. We will retrain quarterly using the most recent six months of data, scheduled during low-traffic periods. If accuracy drops below 85% between scheduled retraining cycles, we will trigger emergency retraining within one week. We will maintain a runbook for incident response, including rollback to the prompt-based system if the fine-tuned model fails. We will sunset this model when vendor-provided models exceed 92% accuracy without fine-tuning, reevaluating every six months." This plan is specific, assigns ownership, and includes both proactive and reactive maintenance.

## Framing the Business Case for Non-Technical Stakeholders

The proposal structure is necessary but not sufficient. You must also frame the business case in terms that non-technical stakeholders understand. Engineers think in terms of accuracy, latency, and throughput. Leadership thinks in terms of revenue, cost, risk, and strategic advantage. You must translate technical benefits into business benefits. A model that improves F1 score from 0.84 to 0.91 is meaningless to a CFO. A model that reduces support costs by $274,000 per year and improves customer satisfaction scores by 8 points is a business case.

The most effective framing ties the fine-tuning project to a company-level objective that leadership has already committed to. If your company has a public goal to reduce customer support costs by 20%, your proposal should open with: "This fine-tuning project directly supports our company goal to reduce support costs by 20% in 2026. Misrouted tickets are responsible for approximately 12% of our support costs, and this project will eliminate 60% of misrouting, contributing 7.2 percentage points toward our 20% target." This framing positions fine-tuning as a tool for achieving an existing goal, not as a speculative science project.

Some business benefits are harder to quantify but still valuable. Improved response time, better customer experience, reduced agent frustration, and competitive differentiation all matter to leadership, but you must quantify them wherever possible. If you cannot quantify a benefit, you must provide leading indicators that leadership can track. For example: "We expect improved classification accuracy will reduce average ticket resolution time from 4.2 hours to 3.1 hours by eliminating the delay caused by manual rerouting. We will measure this as the time between ticket creation and first response from the correct team, tracked weekly." This gives leadership a tangible metric they can monitor.

Another effective framing is risk reduction. If your current prompt-based system is fragile, inconsistent, or dependent on a single vendor, fine-tuning can reduce operational risk. You frame this as: "Our current system depends entirely on OpenAI's API and is vulnerable to pricing changes, rate limits, and service outages. The fine-tuning approach uses dedicated inference infrastructure under our control, eliminating dependency on third-party rate limits and reducing our exposure to vendor pricing changes. This shift reduces operational risk and improves our negotiating position with vendors." This framing resonates with leaders who have been surprised by vendor price increases or service disruptions.

## What Leadership Actually Wants to Know

Leadership is evaluating your proposal against three questions: is this the right problem to solve, is this the right solution, and is this team capable of executing. Most engineers assume leadership is evaluating technical correctness. They are not. They are evaluating judgment, prioritization, and execution risk. A technically perfect proposal for the wrong problem will be rejected. A technically flawed proposal for the right problem with a credible team will often be approved with suggestions for improvement.

The first question leadership asks is whether this is the right problem. They are comparing your proposed fine-tuning project against every other project competing for resources. If you are proposing to fine-tune a model to improve accuracy from 84% to 91%, leadership is asking: is this 7-point accuracy improvement more valuable than shipping the new onboarding flow, more valuable than fixing the payment processing bug, more valuable than launching in the European market? You must make the case that your problem is high-impact, urgent, and blocking other priorities. If you cannot make this case, your proposal will be deprioritized regardless of technical merit.

The second question is whether fine-tuning is the right solution. Leadership has learned to be skeptical of ML projects because many fail to deliver value. They are asking: have you tried simpler alternatives, do you have the data required, and is the expected benefit large enough to justify the cost and risk? This is why the alternatives exhausted section is critical. If you did not try prompt engineering or retrieval augmentation, leadership will send you back to try those first. If you did try them and documented the results, leadership will trust your recommendation. The data readiness section answers the "do you have what you need" question. The cost estimate and success metrics answer the "is the benefit large enough" question.

The third question is whether your team can execute. Leadership is evaluating your proposal as a signal of your team's judgment and preparation. A proposal with no cost estimate signals poor planning. A proposal with no maintenance plan signals short-term thinking. A proposal with no risk section signals overconfidence. Conversely, a proposal that acknowledges risks, includes contingencies, and specifies ownership signals a team that has thought through the problem and is prepared to handle challenges. Leadership approves projects from teams that demonstrate good judgment, not just technical skill.

## Common Mistakes That Kill Proposals

The most common mistake is presenting no baseline comparison. Engineers describe their proposed fine-tuned model but do not compare it to the current system. Leadership cannot evaluate your proposal without understanding what they are replacing. You must include: current system cost, current system performance, current system limitations, and the delta between current and proposed. If you are proposing fine-tuning to replace a prompt-based system, you must show prompt-based accuracy, fine-tuned accuracy, prompt-based cost, and fine-tuned cost side by side. Without this comparison, leadership cannot assess whether the improvement justifies the investment.

The second mistake is omitting the cost estimate or presenting only training costs. Leadership needs total cost of ownership, including infrastructure, maintenance, and opportunity cost. A proposal that says "training will cost $15,000" will be rejected with a request for a complete cost breakdown. A proposal that says "total first-year cost including training, hosting, monitoring, retraining, and engineering time is $290,000 compared to $564,000 for the current system, generating $274,000 in annual savings" will be seriously considered. The difference is completeness. Incomplete cost estimates signal that you have not done the homework.

The third mistake is no maintenance plan. Engineers think of fine-tuning as a project with a beginning and end. Leadership thinks of it as an ongoing operational commitment. If your proposal does not specify who will monitor the model, how often you will retrain, and what happens when the model degrades, leadership will assume the answer is "no one knows" and will reject the proposal or demand a maintenance plan before approval. The maintenance plan is your commitment that this system will remain operational and accurate after you move on to the next project.

The fourth mistake is vague success metrics. Proposals that say "we will improve accuracy" or "we will reduce costs" are not actionable. Leadership cannot evaluate success without specific thresholds. You must define: target accuracy, acceptable accuracy range, cost target, timeline target, and the decision rule for launch. For example: "We will launch if we achieve 90% or higher accuracy and reduce monthly costs below $32,000. We will kill the project if we do not reach 88% accuracy by week six. We will extend the timeline by up to four weeks if we reach 88% but not 90% by week six." This level of specificity allows leadership to hold you accountable and gives them confidence that you have thought through success criteria.

The fifth mistake is ignoring regulatory and compliance implications. If you are fine-tuning on customer data, healthcare data, financial data, or any regulated data, leadership needs to know that you have consulted with legal and compliance teams. A proposal that omits this will be rejected immediately in regulated industries. You must include: "We have reviewed this approach with Legal and confirmed that fine-tuning on anonymized support tickets does not violate our data retention policies or create new GDPR obligations. We will implement data anonymization during the export phase and will not include personally identifiable information in the training set." This one paragraph can be the difference between approval and rejection.

## Real Proposals That Got Funded and Why

In March 2025, a legal technology company proposed fine-tuning a contract review model. Their proposal opened with: "Our clients are requesting contract review turnaround times under four hours. Our current GPT-5-based system takes an average of 6.3 hours per contract and costs $340 per review. We need to reduce both time and cost to remain competitive." They documented attempts at prompt optimization and retrieval augmentation, showing that neither approach reduced review time below five hours. They presented a dataset of 12,000 reviewed contracts with expert annotations, a target of 3.5-hour average review time, and a cost estimate showing break-even at month seven. They included a quarterly retraining plan and a rollback strategy. The proposal was approved in one meeting because it answered every question leadership had before they asked.

In July 2025, a financial services company proposed fine-tuning a fraud detection model. Their proposal quantified the cost of false positives: "Our current rule-based system flags 8,400 legitimate transactions per day as suspicious, requiring manual review by our fraud team. Each review takes an average of 4.2 minutes, consuming 588 hours of analyst time per day at a cost of $1.9 million per month. Our prompt-based ML system reduced false positives by 30% but introduced a 12-second latency that violated our SLA." They proposed fine-tuning to achieve both low latency and low false positive rate, with a target of 60% reduction in false positives and sub-200ms latency. They included infrastructure costs for real-time inference and a detailed monitoring plan with daily performance dashboards. Leadership approved the project because the business case was clear and the team demonstrated operational maturity.

In November 2025, a healthcare company proposed fine-tuning a clinical note summarization model. Their proposal did not get approved. The reason: they had no maintenance plan and no data retention policy. Leadership asked who would retrain the model when clinical guidelines changed, and the team had no answer. Leadership asked how long they would retain patient data used for training, and the team said "we had not thought about that." The proposal was sent back with requirements to address both concerns. The team returned two weeks later with a maintenance plan owned by the clinical informatics team and a data retention policy reviewed by their HIPAA compliance officer. The revised proposal was approved. The lesson: leadership will not approve proposals with unresolved operational or regulatory questions.

## Presenting the Proposal: The Five-Minute Conversation

The proposal document is preparation for a conversation, not a substitute for it. You will present this proposal in a meeting with stakeholders from engineering, product, finance, and possibly legal or compliance. The meeting will last fifteen to thirty minutes, and the decision will be made in the first five minutes based on your opening. You must open with the problem, not the solution. Start with: "We are spending $564,000 per year on ticket classification, achieving 84% accuracy, which causes 1,200 misrouted tickets per week and costs us an estimated $180,000 in agent time for rerouting. We have exhausted prompt engineering, retrieval augmentation, and prompt chaining. Fine-tuning is the only approach that reaches the 90% accuracy threshold required to cut misrouting in half."

After the opening, leadership will ask questions. The most common questions are: how much does this cost, how long will it take, what happens if it does not work, and who maintains it after launch. If you have written a complete proposal document, you can answer every question in under thirty seconds by referencing the relevant section. If you did not write a complete proposal, you will fumble and lose credibility. The purpose of the proposal document is not to avoid the meeting but to prepare you to handle every question with specificity and confidence.

Some leaders will challenge your assumptions. They will ask why you think fine-tuning will work when prompt engineering did not, or why you believe your cost estimate is accurate. You must be prepared to defend your analysis with data. If you tested prompt engineering and documented the results, you can say: "We ran prompt engineering for six weeks, tested twelve variations, and achieved a maximum of 87% accuracy at $51,000 per month, which is documented in appendix A." If you estimated costs based on vendor pricing and production volume, you can say: "We estimated inference costs at $0.06 per request based on current vendor pricing for fine-tuned models, multiplied by our average production volume of 720,000 requests per month, which gives us $43,000, and we added 20% buffer for traffic growth." This level of preparation turns challenges into opportunities to demonstrate rigor.

The meeting will end with one of three outcomes: approved, rejected, or approved with conditions. Approved means you have budget and can start work immediately. Rejected means the project is not a priority or the case was not compelling. Approved with conditions means leadership supports the concept but needs you to address specific concerns before releasing budget. The most common conditions are: revise the cost estimate with more detail, add a maintenance plan, get sign-off from legal or compliance, or run a smaller proof-of-concept first. If you receive conditions, treat them as a second chance to get approval and address them within one week.

Your proposal is not just a document. It is a test of your ability to think strategically, plan comprehensively, and communicate effectively with non-technical stakeholders. A strong proposal demonstrates that you understand the business problem, have exhausted alternatives, have prepared thoroughly, and have thought through risks and maintenance. A weak proposal demonstrates that you are focused on technology for its own sake and have not considered cost, risk, or long-term ownership. Leadership approves strong proposals because they trust the team that wrote them. They reject weak proposals not because fine-tuning is wrong but because the team has not earned their confidence.

The next question is not whether your proposal gets approved but what happens when the approved project starts to fail. Knowing when to abandon a fine-tuning effort mid-flight is as important as knowing when to start one.
