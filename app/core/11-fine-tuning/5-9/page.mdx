# 5.9 â€” Experiment Tracking: Weights and Biases, MLflow, and Custom Logging

In early 2025, a machine learning team at a legal technology company completed a three-month fine-tuning campaign to adapt a 13B parameter model for contract analysis across eight different legal domains. The team ran 127 training experiments, testing combinations of learning rates, batch sizes, warmup schedules, and data sampling strategies. By late March, the model performed well in production, and the VP of Engineering asked for a summary of what worked so the team could apply the same methodology to a new regulatory compliance project. The team could not provide an answer. They had logged loss curves to TensorBoard and saved final model checkpoints to S3, but they had not recorded which hyperparameters corresponded to which runs, which datasets were used in which experiments, or which configurations produced the best validation metrics. Reconstructing the successful configuration required re-running a dozen experiments over two weeks, costing $18,000 in GPU time and delaying the compliance project launch by a month. The root cause was not technical failure but a lack of experimental discipline: the team treated fine-tuning as a series of one-off jobs instead of as a systematic research process requiring rigorous tracking, versioning, and comparison.

This chapter explains how to track fine-tuning experiments so you can reproduce successful configurations, compare runs systematically, and diagnose failures after the fact. You will learn what metadata to log for every experiment, how to structure logs for efficient querying and visualization, how to use Weights and Biases for cloud-based tracking, how to use MLflow for self-hosted tracking, and when to build custom logging infrastructure. By the end, you will have a repeatable system for experiment management that prevents the expensive trial-and-error loops that plague undisciplined fine-tuning workflows.

## What to Log: The Minimum Viable Experiment Record

The foundation of experiment tracking is defining what constitutes a complete experiment record. A complete record allows you to reproduce the experiment exactly, compare it to other experiments on standardized metrics, and diagnose what went wrong when results differ from expectations. The minimum viable record includes six categories of metadata: hyperparameters, data versioning, model configuration, training metrics, evaluation metrics, and environment details.

Hyperparameters are the tunable settings that control training behavior. You must log learning rate, learning rate schedule type, warmup steps or warmup ratio, batch size per device, gradient accumulation steps, total number of training steps or epochs, optimizer type and settings including weight decay and epsilon, gradient clipping threshold, and any regularization parameters such as dropout rates. Logging these values as a flat dictionary or JSON object makes them easy to query and compare across runs. A team debugging why a fine-tuning run diverged after 2,000 steps can immediately check whether gradient clipping was enabled and compare the learning rate schedule to successful runs.

Data versioning tracks exactly which examples were used to train the model. You must log the dataset name and version, the number of training examples, the number of validation examples, the data split seed if you shuffled or sampled data, and checksums or content hashes of the data files. Data drift is one of the most common causes of silent performance degradation: a team reruns an experiment three months later and achieves worse results because the dataset was updated with lower-quality examples, but without versioning, they cannot isolate the cause. Logging data versions eliminates this ambiguity.

Model configuration includes the base model name and version, the fine-tuning technique such as LoRA or full fine-tuning, technique-specific parameters like LoRA rank and alpha, quantization settings if using QLoRA, and the tokenizer version. A team comparing a LoRA rank of 8 versus rank 16 must record both values explicitly; assuming the rank is implicit in the checkpoint filename is a recipe for confusion when the checkpoint naming convention changes.

Training metrics are the time-series data generated during training. You must log training loss at every logging step, validation loss at every evaluation step, learning rate at every step to verify the schedule is applied correctly, gradient norms to detect exploding or vanishing gradients, and GPU memory usage to diagnose out-of-memory failures. These metrics allow you to visualize training dynamics, identify divergence points, and compare convergence behavior across experiments.

Evaluation metrics are task-specific measurements of model quality. You must log accuracy, F1 score, precision, recall, BLEU, ROUGE, or whatever metrics are relevant to your task, measured at every evaluation step and on the final held-out test set. Logging only training loss is insufficient; a model can overfit and achieve low training loss while performing poorly on validation data. Evaluation metrics are the ground truth for whether an experiment succeeded.

Environment details capture the infrastructure and software versions used in training. You must log the GPU type and count, the PyTorch version, the Transformers library version, the CUDA version, the random seed for reproducibility, and the training start and end timestamps. A team trying to reproduce a result six months later discovers that the Transformers library changed the default attention implementation in version 4.38, breaking reproducibility. Logging the library version makes the root cause obvious.

The discipline of logging all six categories for every experiment eliminates the most common failure mode in fine-tuning: running dozens of experiments, forgetting which configurations were tested, and being unable to explain why one run succeeded and another failed. Logging is not optional overhead; it is the foundation of systematic experimentation.

## Weights and Biases: The Dominant Cloud-Based Tracking Platform

Weights and Biases, commonly called W and B, is the most widely adopted experiment tracking platform for fine-tuning in 2026. It provides a cloud-hosted dashboard for logging, visualizing, and comparing experiments, integrates natively with PyTorch, Transformers, and all major fine-tuning libraries, and requires fewer than ten lines of code to instrument a training script. W and B is free for individual researchers and small teams, with paid tiers for enterprises requiring private hosting or advanced collaboration features.

Integrating W and B into a fine-tuning script begins with initializing a run. You call the wandb.init function at the start of training, passing a project name, run name, and configuration dictionary containing all hyperparameters, data versions, and model settings. W and B assigns a unique run ID, creates a dashboard for the run, and begins logging system metrics like GPU utilization and memory usage automatically. You do not need to instrument GPU monitoring manually; W and B captures it by default.

Logging metrics during training requires calling wandb.log with a dictionary of metric names and values. After each training step, you log the training loss and learning rate. After each evaluation step, you log validation loss and task-specific metrics. W and B aggregates these logs, generates real-time visualizations, and makes them accessible via the web dashboard. A team training overnight checks the dashboard in the morning, sees that validation loss stopped decreasing after 3,000 steps, and terminates the run early to save compute costs.

Comparing experiments is W and B's core value proposition. The dashboard allows you to select multiple runs, overlay their loss curves on a single plot, and filter runs by hyperparameter values. A team testing six different learning rates can instantly visualize which rate produced the lowest validation loss and the fastest convergence. The comparison interface also supports correlation analysis: you can plot validation loss against learning rate or LoRA rank and identify trends across dozens of experiments without manually exporting logs and running analysis scripts.

Artifact tracking in W and B extends logging to datasets, checkpoints, and model weights. You log a dataset as a W and B artifact with a name and version, W and B computes a checksum and stores metadata, and you reference the artifact in your training run. Later, you can query which runs used which dataset versions, trace performance regressions to data changes, and reconstruct the exact dataset state for any historical experiment. The same pattern applies to model checkpoints: you log the final checkpoint as an artifact, link it to the run that produced it, and download it later for inference or continued training.

The failure mode with W and B is under-utilizing its capabilities. A team integrates W and B, logs loss curves, but never uses the comparison interface, artifact tracking, or hyperparameter sweeps. They treat W and B as a fancier TensorBoard, missing the systematic experiment management features that justify the platform. The rule is this: if you are using W and B only to visualize loss curves, you are wasting 80% of its value. Use comparison views, log artifacts, tag runs with metadata, and organize experiments into projects and teams.

W and B also provides hyperparameter sweeps, an automated framework for testing multiple configurations in parallel. You define a sweep configuration specifying the search space for learning rate, batch size, and other hyperparameters, W and B launches multiple training jobs with different parameter combinations, tracks all runs, and ranks them by a target metric like validation loss. Sweeps replace manual hyperparameter tuning with systematic grid search or Bayesian optimization, and they are the single most effective tool for teams without prior fine-tuning experience to identify good configurations quickly. A team unsure whether to use a learning rate of 1e-4 or 5e-5 runs a sweep testing both values plus three intermediate points, sees that 3e-5 performs best, and moves forward with that configuration in under two hours.

## MLflow: Self-Hosted Tracking for Enterprise and Research Teams

MLflow is an open-source experiment tracking platform developed by Databricks and widely adopted in enterprises that require on-premises deployment or integration with existing data infrastructure. Unlike W and B, which is cloud-hosted by default, MLflow runs on your own servers or cloud instances, giving you full control over data storage, access policies, and infrastructure costs. MLflow provides similar functionality to W and B, logging metrics, parameters, and artifacts, visualizing experiments in a web UI, and supporting model versioning and deployment.

Integrating MLflow into a training script begins by starting an MLflow run. You call mlflow.start_run at the beginning of training, which creates a unique run ID and prepares the tracking backend to receive logs. You log hyperparameters using mlflow.log_params, passing a dictionary of parameter names and values. You log metrics using mlflow.log_metric, passing a metric name, value, and step number. MLflow stores these logs in a local file system or a remote database depending on your configuration, and the MLflow UI queries the storage backend to generate visualizations.

Comparing experiments in MLflow is less polished than W and B but functionally similar. The MLflow UI allows you to filter runs by parameter values, sort by metric values, and visualize metric trends across runs. A team testing different batch sizes can filter runs by batch size, sort by final validation loss, and identify the optimal configuration. The UI does not support overlaying loss curves as elegantly as W and B, but you can export metric logs to CSV and generate plots in Python using matplotlib or seaborn if you need publication-quality visualizations.

Artifact logging in MLflow uses mlflow.log_artifact to store files like model checkpoints, configuration files, or dataset snapshots. MLflow copies the artifact to the tracking backend, associates it with the run, and provides a download link in the UI. Artifact storage is more manual than W and B's versioned artifact system, but it is sufficient for tracking which checkpoint corresponds to which experiment and for downloading checkpoints later for inference or analysis.

MLflow's strength is flexibility and control. You can configure MLflow to store logs in a PostgreSQL database, artifacts in S3, and serve the UI from a Kubernetes cluster, giving you full ownership of the infrastructure. This is critical for enterprises with compliance requirements that prohibit sending training metadata to third-party cloud services. MLflow is also free and open source, eliminating per-user or per-experiment fees that W and B charges for enterprise teams.

The tradeoff is operational overhead. Running MLflow requires maintaining a backend database, a web server for the UI, and storage infrastructure for artifacts. For teams with dedicated ML infrastructure engineers, this overhead is trivial. For small teams or individual researchers, the overhead is a distraction. The rule is this: use W and B if you can tolerate cloud hosting and want minimal setup friction, use MLflow if you need on-premises deployment or have compliance requirements that prohibit third-party logging.

## Custom Logging: When and Why to Build Your Own Infrastructure

The third option is custom logging, where you instrument training scripts to write metrics, parameters, and artifacts to your own storage backend without relying on a third-party platform. Custom logging is rare in 2026 because W and B and MLflow cover most use cases, but it remains the correct choice in three scenarios: when you have highly specialized logging requirements that platforms do not support, when you need to integrate deeply with existing internal tools, or when platform costs become prohibitive at scale.

Custom logging for fine-tuning typically writes metrics to a time-series database like Prometheus or InfluxDB, stores hyperparameters and metadata in a relational database like PostgreSQL, and saves artifacts to object storage like S3 or Google Cloud Storage. Training scripts call a logging library that abstracts the storage backend, allowing you to swap databases or storage systems without rewriting instrumentation code. A financial services company with strict data residency requirements builds a custom logging system that stores all experiment data in an on-premises PostgreSQL cluster and S3-compatible object storage running in their own data centers.

The advantage of custom logging is total control. You define the schema, the retention policies, the access controls, and the integration points with other internal systems. If your organization already has a data warehouse for analytics, you can log training metrics directly to the warehouse and query them alongside production model performance metrics, feature usage data, and business KPIs. This level of integration is impossible with third-party platforms that operate in isolated silos.

The disadvantage is development and maintenance cost. Building a custom logging system requires writing instrumentation libraries, implementing storage backends, building a web UI for visualization and comparison, and maintaining all of this infrastructure as training workflows evolve. For a team of three ML engineers, this cost is prohibitive. For a team of thirty engineers with dedicated ML infrastructure support, the cost is justified if platform fees exceed the cost of internal development.

The heuristic is this: start with W and B or MLflow, and only build custom logging if you encounter a concrete limitation that platforms cannot address. The limitation must be technical, not aesthetic. Preferring a different UI color scheme is not a justification for custom logging. Requiring all experiment data to remain in an air-gapped network environment with no internet access is a justification. Custom logging is a last resort, not a default choice.

## Structuring Logs for Efficient Querying and Analysis

The value of experiment tracking depends not only on what you log but on how you structure logs for querying and analysis. Poorly structured logs make it expensive to answer basic questions like which runs used a particular dataset version or which hyperparameters correlate with low validation loss. Well-structured logs support ad hoc queries, trend analysis, and automatic reporting with minimal manual effort.

The foundation of good log structure is consistent naming conventions. Hyperparameter names should match across all experiments: if one experiment logs learning_rate and another logs lr, automated comparisons fail because the field names do not align. Establish a naming convention at the start of a fine-tuning project and enforce it through code reviews or automated validation scripts. The same principle applies to metric names: if one experiment logs eval_loss and another logs validation_loss, comparison tools treat them as separate metrics.

Hierarchical metadata organization improves filtering efficiency. Instead of logging a flat dictionary of hyperparameters, group related parameters under namespaces: model.base_name, model.lora_rank, training.learning_rate, training.batch_size, data.dataset_name, data.num_examples. This structure allows you to filter runs by namespace, answering questions like which runs used a learning rate above 1e-4 without manually inspecting every hyperparameter.

Tagging runs with high-level descriptors enables coarse-grained filtering. A team working on multi-domain fine-tuning tags each run with the domain name: legal, medical, finance, customer_support. When analyzing results, they filter by domain and compare hyperparameters within each domain, avoiding the noise of cross-domain comparisons. Tags also support labeling experiments by purpose: baseline, ablation, production_candidate, debugging. Filtering by purpose allows you to exclude debugging runs from performance analyses and focus on production-relevant experiments.

Versioning datasets, models, and checkpoints as first-class entities rather than file paths prevents broken references when files are moved or deleted. Instead of logging dataset_path as a string like s3://bucket/data/v1/train.json, log dataset_name as contract_analysis and dataset_version as v1.2.3, and maintain a separate registry mapping names and versions to storage paths. If the storage path changes, you update the registry without invalidating historical logs.

Logging run relationships enables tracing experiment lineages. If you fine-tune a model, evaluate it, discover a failure mode, adjust the dataset, and fine-tune again, the second run should reference the first run as its parent. This parent-child relationship allows you to trace the evolution of a model through multiple iterations and understand which changes led to improvements. W and B and MLflow both support run relationships natively, and custom logging systems should include parent_run_id as a standard field.

The discipline of structured logging is what separates teams that can answer questions about their experiments in seconds from teams that spend hours exporting logs, writing analysis scripts, and reconstructing metadata manually. Structure is not an aesthetic preference; it is a functional requirement for efficient experimentation.

## Experiment Comparison and Root Cause Analysis

The purpose of logging experiments is not to generate dashboards for their own sake but to enable systematic comparison and root cause analysis. Comparison reveals which configurations work and which do not. Root cause analysis explains why a particular configuration failed or why performance regressed between two runs. Both require disciplined logging and structured querying.

Comparing experiments begins with defining a comparison axis. You might compare runs that differ only in learning rate, holding all other hyperparameters constant, to isolate the effect of learning rate on validation loss. Or you might compare runs that used different datasets, holding the model and hyperparameters constant, to understand how data quality affects performance. The comparison axis determines which runs are relevant and which metrics to plot.

Visualization is the primary comparison tool. Overlaying loss curves for six learning rates on a single plot immediately reveals which rate converged fastest and which diverged. Scatter plots of validation loss against LoRA rank across dozens of experiments reveal whether higher ranks consistently improve performance or whether the relationship is noisy. Box plots of final evaluation metrics grouped by dataset version show whether a recent dataset change improved or degraded model quality.

Statistical analysis complements visualization when trends are subtle. Calculating mean and standard deviation of validation loss across runs with the same hyperparameters reveals whether observed differences are meaningful or within the noise of random initialization. Running linear regression of validation loss against learning rate quantifies the sensitivity of performance to that hyperparameter. Statistical rigor prevents teams from chasing false patterns in noisy data.

Root cause analysis for failed experiments starts with identifying the failure symptom. Did training diverge with exploding gradients? Did validation loss plateau despite low training loss? Did the model fail to generate coherent outputs on qualitative evaluation? The symptom determines which logs to examine. Divergence points to gradient norms, learning rate schedules, or numerical instability. Plateauing validation loss points to overfitting, insufficient regularization, or data quality issues. Poor qualitative outputs point to tokenization errors, prompt formatting issues, or mismatches between fine-tuning data and inference distribution.

Comparing the failed run to successful runs isolates the root cause. If a run diverged and successful runs did not, you compare hyperparameters: was the learning rate higher, was gradient clipping disabled, was mixed-precision training configured differently? If a run plateaued and successful runs converged smoothly, you compare data versions: did the failed run use a smaller dataset, was the data shuffled differently, were there duplicates or corrupted examples? Systematic comparison eliminates guesswork and replaces it with evidence-based diagnosis.

The discipline of comparison and root cause analysis is what allows teams to learn from failures instead of repeating them. A team that logs experiments but never compares them learns nothing. A team that compares experiments systematically builds institutional knowledge about what works, what does not, and why.

Experiment tracking is not a monitoring tool for individual training runs; it is a knowledge management system for fine-tuning as a research and engineering discipline. Logging hyperparameters, data versions, model configurations, training metrics, evaluation metrics, and environment details for every experiment creates a complete record that enables reproduction, comparison, and diagnosis. Weights and Biases is the dominant cloud-based platform, offering real-time dashboards, artifact tracking, hyperparameter sweeps, and seamless integration with PyTorch and Transformers. MLflow is the self-hosted alternative for enterprises requiring on-premises deployment or integration with existing data infrastructure. Custom logging is justified only when specialized requirements or compliance constraints make platforms infeasible. Structuring logs with consistent naming conventions, hierarchical metadata, tags, and versioned references makes querying and analysis efficient. Systematic comparison and root cause analysis transform logged data into actionable insights, separating teams that improve iteratively from teams that repeat the same mistakes. The upfront cost of instrumenting logging infrastructure is trivial compared to the cost of wasting weeks rerunning experiments because you cannot remember what you tried or why it failed.

The foundation of infrastructure is now complete: you have selected GPUs, configured multi-GPU training, and instrumented experiment tracking. The next chapter shifts focus to the data that drives fine-tuning, explaining how to source, filter, and format training examples for maximum effectiveness.

