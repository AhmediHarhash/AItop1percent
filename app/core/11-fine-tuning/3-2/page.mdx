# 3.2 â€” Teacher-Student Architecture: Selecting the Right Frontier Teacher

In September 2025, a content moderation platform spent $38,000 generating distillation data for a hate speech classification system using GPT-5 as the teacher. The team chose GPT-5 because it topped the general reasoning leaderboards and had the best marketing. They generated 25,000 labeled examples, fine-tuned a Llama 4 Scout 8B model, and deployed to production. Precision was 71%, far below their 90% target. Recall was 68%, even worse. The system flagged innocuous political commentary as hate speech and missed obvious slurs written in leetspeak or intentional misspellings. The problem was not the distillation process. The problem was teacher selection. GPT-5 was the wrong teacher for this task. It was trained to be helpful and harmless, which meant it avoided strong opinions on controversial content. When prompted to classify borderline hate speech, it hedged, equivocated, and often refused to label. The training data reflected that uncertainty, and the student model learned to be uncertain too.

In November 2025, the same team reran the distillation experiment using Claude Opus 4 as the teacher. Claude's constitutional AI training made it better at nuanced policy enforcement without over-censoring. They generated 25,000 new examples with the same input data but different teacher outputs. They fine-tuned the same Llama 4 Scout 8B model. Precision jumped to 94%. Recall jumped to 91%. The student model learned to distinguish context-dependent slurs, detect dogwhistles, and handle ambiguous cases with consistency. The only variable that changed was the teacher. The cost was the same, the student architecture was the same, the training process was the same. The teacher's alignment and task-specific capabilities determined the outcome.

Teacher selection is not a secondary concern in distillation. It is the first and most consequential decision. The teacher's quality ceiling becomes the student's quality ceiling. The teacher's biases become the student's biases. The teacher's strengths and weaknesses propagate directly into the distilled model. You cannot distill capabilities that the teacher does not have. You cannot fix teacher errors through student training. The teacher is the source of truth, and the student is a specialized approximation of that source. Choosing the right teacher requires understanding task-specific model performance, not general leaderboard rankings.

## Task-Specific Benchmarks Over General Leaderboards

The general leaderboards that dominate AI discourse in 2026 are nearly useless for teacher selection. MMLU scores, HumanEval pass rates, and GSM8K accuracy measure broad capabilities, but production tasks are narrow. A model that excels at multiple-choice questions may fail at extracting structured data from messy documents. A model that writes excellent code may produce verbose and imprecise summaries. A model that scores high on reasoning benchmarks may struggle with tasks that require strict format adherence or policy enforcement.

The correct way to select a teacher is to benchmark candidate teachers on your specific task with a representative evaluation set. If you are building a contract clause extraction system, you take 200 real contracts, define the extraction task precisely, run those contracts through GPT-5, Claude Opus 4.5, and Gemini 3 with identical prompts, and measure extraction accuracy, format compliance, and error patterns. The model that performs best on your task becomes your teacher. This process takes one to two days and costs between $400 and $1,200 in API calls depending on task complexity. It is the best $1,200 you will spend in the distillation process because it prevents the $38,000 mistake of choosing the wrong teacher.

The task-specific evaluation reveals patterns that leaderboards miss. In early 2026, a financial services company evaluated teacher models for transaction categorization. GPT-5 scored highest on general reasoning benchmarks but produced inconsistent category labels for ambiguous transactions. Gemini 3 scored lower on reasoning benchmarks but produced more consistent labels because its training included more structured output tasks. For this specific task, Gemini 3 was the better teacher despite lower scores on MMLU and ARC-Challenge. The company generated distillation data from Gemini 3, and the resulting student model met production quality targets on the first iteration.

The evaluation process also exposes model-specific failure modes. Claude Opus 4.5 is excellent at tasks requiring nuanced judgment and refusal behavior, but it sometimes produces outputs that are too verbose for tasks requiring terse, structured responses. GPT-5 excels at complex reasoning chains but sometimes hallucinates details when asked to extract information that is not explicitly stated in the input. Gemini 3 handles multimodal inputs exceptionally well but occasionally misinterprets context in pure text tasks where Claude or GPT would succeed. DeepSeek R1 excels at explicit reasoning chains but can be overly verbose when step-by-step reasoning is not necessary. These failure modes are task-dependent, which is why you evaluate on your task, not on general benchmarks.

Your evaluation should test the specific failure modes that matter for your application. If your task requires handling adversarial inputs, include adversarial examples in your evaluation set. If your task requires multilingual support, test across all target languages. If your task requires consistency across similar inputs, include near-duplicate examples with slight variations and measure output stability. A teacher that performs well on average cases but fails on edge cases will produce a student that inherits those same edge case failures.

The evaluation metrics must align with production requirements. For classification tasks, measure precision, recall, and F1 across all classes, not just aggregate accuracy. For extraction tasks, measure exact match rates, partial match rates, and false positive rates. For generation tasks, measure format compliance, factual accuracy, and output length distribution. A teacher with 90% aggregate accuracy but 50% recall on rare but critical classes is not better than a teacher with 85% aggregate accuracy and 80% recall across all classes.

## The Cost-Quality Tradeoff Across Teacher Models

Teacher models vary widely in cost, and the most expensive teacher is not always the best teacher for your task. As of January 2026, the pricing landscape for frontier models looks roughly like this: GPT-5 costs approximately $15 per million input tokens and $75 per million output tokens. Claude Opus 4.5 costs approximately $12 per million input tokens and $60 per million output tokens. Gemini 3 costs approximately $8 per million input tokens and $40 per million output tokens. DeepSeek R1 costs approximately $4 per million input tokens and $16 per million output tokens. These are API prices for standard throughput; batch pricing can reduce costs by 30 to 50 percent.

The cost difference matters when generating tens of thousands of distillation examples. If your average task involves 500 input tokens and 300 output tokens, generating 20,000 examples costs approximately $5,100 with GPT-5, $4,080 with Claude Opus 4.5, $2,720 with Gemini 3, or $1,360 with DeepSeek R1. If the quality difference between GPT-5 and Gemini 3 is negligible for your task, you save $2,380 by choosing Gemini 3. If DeepSeek R1 matches the others on your task, you save $3,740.

The quality-cost tradeoff is not linear. The most expensive teacher does not always deliver proportionally better quality. In practice, the quality gap between frontier models has narrowed considerably in 2026. For many classification and extraction tasks, Claude Opus 4.5, GPT-5, and Gemini 3 produce nearly identical outputs when prompted identically. The differences emerge in edge cases, ambiguous inputs, and tasks requiring deep reasoning or refusal behavior. For straightforward tasks with clear inputs and outputs, you optimize for cost. For complex tasks with ambiguity and nuance, you optimize for quality. The only way to know which category your task falls into is to run the evaluation.

Some teams use a hybrid approach: they generate a small pilot dataset with the most expensive teacher, measure student model quality, then generate the full dataset with a cheaper teacher if quality remains acceptable. For example, you might generate 2,000 examples with GPT-5, fine-tune a student model, and measure accuracy. If accuracy meets your target, you generate the remaining 18,000 examples with Gemini 3 to save $1,800. If accuracy falls short, you generate all 20,000 examples with GPT-5. This approach balances cost and risk. You spend more upfront to validate quality, then optimize cost once you have confidence.

The hybrid approach also works across different input segments. If your task has high-value inputs that require maximum quality and low-value inputs that tolerate moderate quality, you can use an expensive teacher for high-value inputs and a cheaper teacher for low-value inputs. A legal document analysis system might use Claude Opus 4.5 for M&A contracts worth millions and Gemini 3 for standard NDAs. The combined dataset trains a student that performs well across both segments while optimizing total cost.

Batch pricing changes the economics significantly. OpenAI offers batch API pricing at 50% discount for jobs that can tolerate 24-hour completion times. Anthropic offers similar discounts for batch inference. For distillation workloads where you generate tens of thousands of examples in a single batch job, these discounts cut teacher costs in half. A 20,000-example dataset that costs $5,100 at standard GPT-5 pricing costs $2,550 at batch pricing. This narrows the gap between expensive and cheap teachers, making it more feasible to use the highest-quality teacher even when cost is a concern.

## When to Use GPT-5 vs Claude Opus 4.5 vs Gemini 3 as Teacher

Each frontier teacher has specific strengths that make it the best choice for certain task types. GPT-5 excels at tasks requiring multi-step reasoning, complex instruction following, and creative generation. If your task involves generating long-form content, answering questions that require synthesis across multiple facts, or producing outputs that need internal logical consistency, GPT-5 is often the best teacher. Examples include legal memo generation, technical documentation writing, and multi-turn dialogue systems where context and reasoning depth matter.

GPT-5.1, released in late 2025, improved reasoning capabilities further and added better support for structured output generation. It handles tasks that require both deep reasoning and strict format adherence, such as generating SQL queries with complex joins or producing JSON outputs with nested structures. For tasks that combine reasoning and structure, GPT-5.1 often outperforms earlier models.

Claude Opus 4.5 excels at tasks requiring nuanced judgment, policy enforcement, and refusal behavior. If your task involves making decisions based on subjective criteria, enforcing content policies, or handling inputs that may be adversarial or unsafe, Claude Opus 4.5 is often the best teacher. Examples include content moderation, sensitive data handling, compliance screening, and customer support tasks where tone and empathy matter. Claude's constitutional AI training makes it particularly strong at tasks where the right answer is context-dependent and requires balancing competing values.

Claude Opus 4.5 also excels at tasks requiring careful handling of ambiguity. When inputs are incomplete, contradictory, or unclear, Claude is more likely to produce outputs that acknowledge uncertainty, ask clarifying questions, or provide conditional answers rather than making unjustified assumptions. This behavior transfers to distilled students, making them more robust to real-world input noise.

Gemini 3 excels at tasks requiring multimodal understanding, structured output generation, and consistency across large input volumes. If your task involves processing images, tables, or documents with complex layouts, Gemini 3 is often the best teacher. Examples include document parsing, visual question answering, and data extraction from scanned forms or receipts. Gemini's architecture is optimized for structured output, which makes it strong at tasks requiring JSON-like outputs or precise formatting.

Gemini 3 also performs well on tasks requiring consistency at scale. Its training emphasized reducing output variance for similar inputs, which is valuable when you need predictable, repeatable behavior. For high-volume tasks like product categorization or transaction labeling where consistency matters more than creative variation, Gemini 3 often produces cleaner training data than models optimized for reasoning diversity.

DeepSeek R1 excels at tasks requiring explicit reasoning chains and mathematical or logical problem solving. If your task involves producing outputs that need to show their work, such as step-by-step solutions, diagnostic reasoning, or causal analysis, DeepSeek R1 is often the best teacher. Examples include technical support troubleshooting, financial auditing, and scientific literature summarization. DeepSeek R1's training emphasizes reasoning transparency, which can be valuable when you want the student model to learn not just what to output but how to structure the reasoning process.

DeepSeek R1 also performs well on tasks requiring long chains of logical dependencies. For multi-hop question answering or tasks that require combining information from many sources, R1's explicit reasoning reduces error propagation compared to models that reason implicitly. This makes it a strong teacher for complex analytical tasks.

The decision is not always clear-cut. Many tasks could be handled well by two or more teachers. In those cases, you run a small-scale comparison. You generate 500 examples from each candidate teacher, fine-tune student models, and measure which teacher produces the best student performance. The incremental cost of this comparison is modest, typically $200 to $600, and the insight is invaluable.

## Using Multiple Teachers for Ensemble Distillation

A more advanced technique is ensemble distillation, where you generate training data from multiple teacher models and combine their outputs. This approach works well when different teachers have complementary strengths. For example, you might use GPT-5 for tasks requiring deep reasoning and Claude Opus 4.5 for tasks requiring refusal or safety handling, then train a single student model on the combined dataset. The student learns to handle both types of inputs, inheriting the strengths of both teachers.

The simplest form of ensemble distillation is output mixing. You generate 10,000 examples from GPT-5 and 10,000 examples from Claude Opus 4.5, combine them into a 20,000-example dataset, and fine-tune the student model. The student model sees diverse output styles and learns to generalize across both. This works when the task distribution is heterogeneous. If 50 percent of your inputs require reasoning depth and 50 percent require safety handling, a mixed dataset prepares the student for both.

A more sophisticated form is voting or consensus-based distillation. You run the same input through multiple teachers, compare their outputs, and use majority voting or quality scoring to select the best output for each input. If GPT-5, Claude Opus 4.5, and Gemini 3 all produce the same output for a given input, you use that output with high confidence. If they disagree, you apply a tiebreaker rule: defer to the teacher with the highest accuracy on your evaluation set for that input type, or discard the input and generate a different one. This approach produces a cleaner training dataset because it filters out edge cases where teachers are uncertain or inconsistent.

The cost of ensemble distillation is higher because you pay for inference from multiple teachers. Generating 20,000 examples from two teachers costs roughly twice as much as generating from one teacher. The quality improvement needs to justify the cost. In practice, ensemble distillation is most valuable for high-stakes tasks where the cost of student model errors exceeds the cost of additional teacher inference. A financial compliance system that could trigger regulatory penalties for false positives justifies ensemble distillation. A low-stakes content tagging system does not.

Some teams use ensemble distillation iteratively. They start with a single teacher, measure student model performance, identify error patterns, and add a second teacher specifically chosen to address those errors. For example, if the initial student model struggles with ambiguous inputs, they generate additional training examples from a teacher known for handling ambiguity well, then retrain the student on the expanded dataset. This targeted ensemble approach optimizes cost by using multiple teachers only where needed.

Another ensemble approach is role-based teacher selection. For a customer support system, you might use Claude Opus 4.5 for empathetic responses to frustrated customers, GPT-5 for detailed technical explanations, and Gemini 3 for structured information retrieval from knowledge bases. The combined dataset trains a student that can switch between these modes based on input characteristics. This creates a more versatile student model than any single teacher could produce.

## Temperature and Sampling Settings for Teacher Generation

The temperature and sampling settings you use when generating distillation data from the teacher model significantly impact the diversity and quality of the training dataset. Temperature controls randomness in the model's output distribution. A temperature of 0 produces deterministic outputs, always selecting the highest-probability token. A temperature of 1 samples from the full probability distribution. A temperature above 1 increases randomness further, sometimes producing creative but incoherent outputs.

For distillation, the optimal temperature depends on task type and dataset size. If your task has a single correct answer for each input, such as classification or factual extraction, you want low temperature, typically 0 or 0.2. Low temperature ensures the teacher produces its most confident answer, which is likely to be correct and consistent. High temperature introduces noise, and noise in distillation data degrades student model performance.

If your task has multiple valid outputs for each input, such as summarization or creative generation, you want moderate temperature, typically 0.7 to 1.0. Moderate temperature introduces diversity in phrasing, structure, and emphasis, which helps the student model generalize. If you generate 10,000 summaries at temperature 0, they will be very similar in style and structure, and the student model may overfit to that single style. If you generate at temperature 0.7, you get variation in sentence structure, word choice, and level of detail, and the student model learns to handle multiple valid output styles.

Some teams use variable temperature based on input difficulty. For straightforward inputs where the teacher is highly confident, they use low temperature to ensure correctness. For ambiguous or edge-case inputs where the teacher's confidence is lower, they use moderate temperature to sample multiple possible outputs, then manually review or vote to select the best one. This adaptive approach balances quality and diversity.

Top-p sampling, also called nucleus sampling, is another parameter that affects output diversity. Top-p restricts the sampling pool to the smallest set of tokens whose cumulative probability exceeds a threshold, typically 0.9 or 0.95. This prevents the model from sampling very low-probability tokens that are likely to be errors or irrelevant. For distillation, a top-p value of 0.95 is a safe default. It allows moderate diversity without risking incoherent outputs.

The interaction between temperature and top-p matters. A temperature of 0.8 with top-p of 0.95 produces diverse but sensible outputs. A temperature of 1.2 with top-p of 1.0 produces highly random outputs that may include errors or hallucinations. For most distillation tasks, you stay in the safe range: temperature between 0 and 1, top-p between 0.9 and 1.0.

For tasks requiring strict format adherence, consider using constrained decoding or structured output modes offered by some model APIs. GPT-5 and Gemini 3 support JSON schema enforcement, which guarantees that outputs conform to a specified structure. This eliminates format errors from the training data and ensures the student model learns clean input-output mappings. The tradeoff is reduced output diversity, so structured modes work best for tasks where format correctness is more important than creative variation.

## Common Mistakes in Teacher Selection

The most common mistake in teacher selection is choosing based on brand or leaderboard rank rather than task-specific evaluation. Teams assume that the model with the highest MMLU score or the most prominent marketing will perform best on their task. This assumption fails regularly. The second most common mistake is using a single teacher without comparing alternatives. Teams pick a teacher, generate data, train a student, and only discover quality issues after deployment. Running a small-scale teacher comparison upfront prevents this.

The third most common mistake is using the wrong temperature settings. Teams generate distillation data at high temperature for tasks that require deterministic outputs, then wonder why the student model is inconsistent. Or they generate data at temperature 0 for tasks that require diversity, then wonder why the student model is rigid and narrow. Matching temperature to task type is straightforward once you understand the tradeoff, but it is easy to overlook if you are new to distillation.

The fourth most common mistake is ignoring teacher bias and propagating it to the student. If your teacher model has a systematic bias, such as gender bias in pronoun resolution or political bias in sentiment analysis, that bias will appear in the distillation data and transfer to the student model. You mitigate this by auditing teacher outputs before training, either through manual review of a sample or through automated bias detection tools. If you find bias, you either switch teachers, adjust prompts to reduce bias, or filter biased examples from the training data.

The fifth most common mistake is over-relying on synthetic data without grounding it in real data. If you generate all your distillation examples from synthetic inputs, the student model may perform well on synthetic evaluations but fail on real-world data that has different statistical properties. The solution is to generate distillation examples from real inputs whenever possible. If you are building a customer support classifier, run real customer messages through the teacher, not invented examples. The real data ensures the student model learns the true input distribution.

The sixth most common mistake is failing to account for teacher model updates. Frontier models change over time. If you distill from GPT-5 in January 2026 and the model is updated in March 2026, your distillation pipeline may produce different outputs when rerun. Some teams version-lock their teacher model by specifying exact model versions in API calls. Others re-evaluate when teacher models update and regenerate training data if quality changes significantly. Ignoring teacher model churn can lead to unexpected student model degradation when you retrain.

## Matching Teacher Strengths to Task Requirements

The final principle in teacher selection is explicit alignment between teacher strengths and task requirements. Before you choose a teacher, write down the top three requirements of your task. Is the task primarily about factual accuracy, reasoning depth, policy enforcement, creativity, format adherence, or safety? Rank the requirements by importance. Then map those requirements to teacher model strengths based on your task-specific evaluation.

For a legal contract analysis task, the top requirements might be accuracy in clause extraction, consistency across similar contracts, and handling of ambiguous or non-standard language. Claude Opus 4.5's strengths in nuanced judgment and consistency make it a strong candidate. For a code generation task, the top requirements might be syntactic correctness, adherence to coding standards, and producing efficient implementations. GPT-5's strengths in reasoning and instruction following make it a strong candidate. For a multimodal document parsing task, the top requirements might be accurate text extraction from images, table structure recognition, and format preservation. Gemini 3's multimodal capabilities make it the obvious choice.

For a complex analytical task requiring multi-step reasoning, the top requirements might be logical coherence, explicit reasoning chains, and handling of long dependency chains. DeepSeek R1's focus on reasoning transparency makes it well-suited. For a customer-facing support task, the top requirements might be empathetic tone, policy compliance, and handling of ambiguous or emotional inputs. Claude Opus 4.5's strengths in tone calibration and nuanced judgment make it ideal.

This explicit matching process forces you to think clearly about what you need from the teacher and what each model offers. It prevents the default of choosing the most popular or most expensive model without justification. It also makes the teacher selection decision auditable. If someone asks why you chose Claude Opus 4.5 over GPT-5, you can point to the task requirements, the teacher evaluation results, and the explicit tradeoffs you considered.

The matching process should include negative requirements: things your student model must not do. If your task requires never hallucinating facts, you select a teacher with low hallucination rates even if it sacrifices some reasoning depth. If your task requires never producing offensive content, you select a teacher with strong safety guardrails even if it sacrifices creative flexibility. Negative requirements often matter more than positive capabilities because a single failure mode can make an entire system unusable.

The quality of your distilled model is capped by the quality of your teacher. That is why teacher selection is not a shortcut or a checkbox. It is a research task that deserves the same rigor you apply to model architecture, training hyperparameters, and evaluation metrics. The teams that treat teacher selection as a first-class decision outperform the teams that treat it as an afterthought.

Once you have selected the right teacher, the next challenge is generating a diverse and representative dataset that captures the full range of inputs your production system will encounter, which is the focus of the next subchapter.
