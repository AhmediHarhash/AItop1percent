# 6.10 â€” Regulatory Requirements for Domain-Specific Fine-Tuned Models

In September 2024, a healthcare technology company deployed a fine-tuned GPT-4 model to generate prior authorization requests for insurance companies. The model read patient medical records and drafted justification letters explaining why a treatment was medically necessary. The company treated the model as a general-purpose AI tool, not as a medical device, and did not seek FDA clearance. They deployed to 40 hospitals over six months. In early 2025, the FDA sent a warning letter classifying the system as Software as a Medical Device requiring premarket review under the 21st Century Cures Act. The system influenced clinical decisions by determining which treatments could proceed based on insurance approval. The FDA demanded evidence of clinical validation, risk analysis, and adverse event monitoring. The company had none of this. They had to suspend operations, conduct a full regulatory assessment, and spend nine months preparing a 510(k) submission. The cost was 2.3 million dollars, and they lost 60 percent of their hospital customers during the suspension. The root cause was regulatory naivety. They assumed that fine-tuning a commercial model exempted them from medical device regulations. It did not. The moment you adapt a model for clinical use, you trigger regulatory obligations. Domain-specific fine-tuning does not reduce regulatory burden. It often increases it.

## The Regulatory Landscape for Domain-Adapted Models in 2026

Regulatory requirements for AI models vary by domain and jurisdiction, but they share common themes: transparency, accountability, risk management, and human oversight. In 2026, three major regulatory frameworks affect domain-specific fine-tuned models: the EU AI Act, US sector-specific regulations like FDA guidance and financial model risk management rules, and data protection laws like GDPR and HIPAA.

The EU AI Act, fully enforced as of 2025, classifies AI systems by risk level. High-risk systems include those used in critical infrastructure, education, employment, law enforcement, migration, justice, and safety components of regulated products. Medical diagnosis, legal decision support, and credit scoring systems are explicitly high-risk. General-purpose AI models like GPT-4 are subject to lighter transparency obligations, but the moment you fine-tune a GPAI model for a high-risk use case, the fine-tuned system becomes a high-risk AI system subject to the full regulatory burden.

High-risk AI systems under the EU AI Act must meet strict requirements: risk management systems throughout the lifecycle, high-quality training and testing data, technical documentation, automatic logging of operations, transparency and information provision to users, human oversight measures, and cybersecurity resilience. These requirements apply regardless of whether you built the base model yourself or fine-tuned a commercial model. Fine-tuning does not exempt you. The deployer of the high-risk system bears compliance obligations.

US sector-specific regulations are equally demanding. The FDA regulates Software as a Medical Device, which includes AI systems that diagnose, treat, prevent, or mitigate disease. If your fine-tuned model interprets medical images, generates treatment recommendations, or predicts patient risk, it is likely SaMD. The FDA requires clinical validation, risk analysis under ISO 14971, software verification and validation, cybersecurity controls, and postmarket surveillance. Fine-tuning a commercial model does not reduce these obligations. You must validate your fine-tuned system clinically, even if the base model was trained by a reputable vendor.

Financial regulators enforce model risk management standards based on SR 11-7, the Federal Reserve and OCC guidance on model risk. Models used for credit decisions, risk assessment, pricing, or trading are subject to model validation requirements: conceptual soundness, ongoing monitoring, outcomes analysis, independent review, and comprehensive documentation. Fine-tuned models are not exempt. If you fine-tune a model to underwrite loans or score credit risk, you must validate it under SR 11-7, maintain ongoing performance monitoring, and document all model changes.

HIPAA applies to any system that processes protected health information in the US. Fine-tuning on patient data requires Business Associate Agreements with covered entities, technical safeguards for data at rest and in transit, access controls, audit logging, breach notification procedures, and risk assessments. Sending patient data to a third-party fine-tuning API without a BAA is a HIPAA violation. Training on de-identified data does not exempt you if re-identification is possible. GDPR imposes similar obligations in the EU, plus additional requirements for data minimization, purpose limitation, and the right to explanation for automated decisions.

## EU AI Act Requirements for High-Risk AI Systems

If your domain-adapted model qualifies as a high-risk AI system under the EU AI Act, you must implement eight categories of obligations. These are not optional recommendations. They are legal requirements. Non-compliance can result in fines up to 35 million euros or 7 percent of global annual turnover, whichever is higher.

First, you must establish a risk management system. This means identifying and analyzing foreseeable risks arising from the AI system's intended use and reasonably foreseeable misuse. You document each risk, estimate its severity and probability, and implement mitigation measures. For a medical diagnosis model, risks include misdiagnosis, failure to identify urgent conditions, and inappropriate treatment recommendations. For a contract analysis model, risks include missing material terms, misinterpreting legal standards, and generating unenforceable clauses. The risk management system must be iterative: you update it as you discover new risks during testing and deployment.

Second, you must ensure data quality. Training, validation, and testing datasets must be relevant, representative, and free of errors. They must be sufficiently complete and have appropriate statistical properties for the intended purpose. For fine-tuned models, this means documenting the provenance of fine-tuning data, demonstrating that it covers the input distribution you expect in production, and showing that it is free of biases that could cause discrimination. If you fine-tuned on datasets that underrepresent certain demographics, you must document this limitation and explain mitigation measures.

Third, you must create and maintain technical documentation. This includes a general description of the AI system, development process details, design specifications, training methodology, data sources, validation and testing results, performance metrics, human oversight measures, and cybersecurity measures. The documentation must be detailed enough that regulators can assess compliance. For fine-tuned models, you must document the base model used, fine-tuning hyperparameters, training duration, evaluation results, and any prompt engineering or retrieval augmentation applied.

Fourth, you must implement automatic logging. The system must keep logs of events and decisions to enable traceability. Logs must include input data, model outputs, timestamps, user actions, and any human review or override. Logs must be stored securely and retained for periods appropriate to the risk level, often five to ten years for medical and financial systems. Fine-tuned models deployed via API must log every request and response. Self-hosted models must implement logging infrastructure.

Fifth, you must provide transparency to users. Users must be informed that they are interacting with an AI system. They must be informed of the system's capabilities and limitations, the purpose for which it is used, the level of accuracy, and the human oversight measures. For professional domain models, this means disclosing to clinicians, lawyers, or analysts that outputs are AI-generated, what the model can and cannot do, and what accuracy users should expect.

Sixth, you must enable human oversight. High-risk systems must be designed to allow humans to understand outputs, monitor operation, and intervene when necessary. This means building interfaces that present model reasoning, confidence scores, and alternative interpretations. It means providing override mechanisms so humans can reject model outputs. For medical models, clinicians must be able to review the evidence the model considered. For legal models, attorneys must be able to see which contract clauses the model flagged as risky.

Seventh, you must ensure cybersecurity. The system must be resilient against attacks that could alter outputs, compromise data, or degrade performance. This includes adversarial robustness, data security, access controls, and incident response procedures. Fine-tuned models are vulnerable to prompt injection, data poisoning, and model extraction attacks. You must assess these risks and implement defenses.

Eighth, you must register the system in the EU AI database. High-risk AI systems must be registered before deployment, with information about the provider, the intended purpose, the risk mitigation measures, and the conformity assessment. This creates public accountability and allows regulators to track high-risk AI deployment.

## FDA Guidance for Software as a Medical Device

If your fine-tuned model meets the definition of Software as a Medical Device, the FDA expects you to follow the SaMD guidance framework, which includes premarket review, quality system regulations, and postmarket surveillance. The regulatory pathway depends on the risk classification of your device.

Class I devices are low-risk and generally exempt from premarket review. Most SaMD systems are not Class I. Class II devices are moderate-risk and require 510(k) clearance, demonstrating that the device is substantially equivalent to a legally marketed predicate device. Class III devices are high-risk and require Premarket Approval, demonstrating safety and effectiveness through clinical trials. Most AI-based diagnostic and treatment support systems are Class II, requiring 510(k).

The 510(k) process requires you to identify a predicate device with the same intended use and technological characteristics. You demonstrate that your device is as safe and effective as the predicate. For fine-tuned models, finding a suitable predicate is challenging because the technology is new. You might claim equivalence to earlier AI systems or to traditional software-based decision support tools, but the FDA evaluates each claim carefully.

You must provide a device description, including the algorithm design, training data characteristics, performance metrics, and intended use population. You must provide verification and validation results. Verification demonstrates that the software was built correctly: unit tests, integration tests, and code reviews. Validation demonstrates that the software solves the correct problem: clinical performance testing showing that the model's outputs are clinically accurate and useful.

Clinical validation is the hardest part. You must show that the model performs well on real patient data from the intended use population. Retrospective validation uses historical data: you apply the model to past cases and measure accuracy. Prospective validation uses new data: you deploy the model in a clinical setting and measure real-world performance. Prospective validation is stronger evidence but more expensive. The FDA prefers prospective validation for high-risk SaMD.

You must conduct risk analysis under ISO 14971, identifying hazards, estimating harm severity and probability, and implementing risk controls. For a diagnostic model, hazards include false negatives that delay treatment and false positives that cause unnecessary procedures. You estimate how often these occur and what harm results. You implement risk controls: confidence thresholds, human review requirements, and user warnings. You document that residual risk is acceptable.

After clearance, you must comply with Quality System Regulations, which require design controls, document controls, change controls, and complaint handling. Any change to the model, including retraining or fine-tuning updates, is a design change requiring evaluation under your change control process. Significant changes might require a new 510(k).

You must implement postmarket surveillance. Monitor device performance in real-world use. Track adverse events. Report serious injuries or malfunctions to the FDA. Conduct post-approval studies if required. If your model's performance degrades in production, you must investigate and take corrective action, potentially including a recall.

## Financial Model Risk Management Under SR 11-7

Banks and financial institutions use models for credit underwriting, risk assessment, pricing, and compliance. The Federal Reserve and OCC issued SR 11-7 guidance requiring model risk management for these systems. Fine-tuned models used for financial decisions fall under this guidance.

SR 11-7 defines model risk as the potential for adverse consequences from decisions based on incorrect or misused model outputs. Model risk arises from errors in model design, incorrect implementation, or inappropriate use. Fine-tuned models are especially risky because their behavior can be opaque and their training data may not represent future conditions.

Financial institutions must establish model governance. This means a framework of policies, procedures, and controls for model development, implementation, use, validation, and retirement. Model governance assigns clear roles: model developers, model validators, model users, and senior management. Each role has defined responsibilities.

Model development requires conceptual soundness. The model must be based on sound theory, empirical evidence, and industry practice. For fine-tuned credit models, this means demonstrating that the model uses features and relationships that make economic sense. You cannot just train on historical data and deploy. You must explain why the model's learned patterns are economically justified.

Model documentation must be comprehensive. Document the model's purpose, design, data sources, assumptions, limitations, and performance. Documentation must allow an independent party to understand and replicate the model. For fine-tuned models, document the base model, fine-tuning methodology, hyperparameters, training data, and evaluation results.

Model validation is required. Validation must be independent: performed by a team that did not develop the model. Validation includes three components. First, evaluate conceptual soundness: review the model's theoretical basis and design. Second, perform ongoing monitoring: track model performance in production and compare to benchmarks. Third, conduct outcomes analysis: compare model predictions to actual outcomes and analyze discrepancies.

Ongoing monitoring is critical for fine-tuned models because they can drift. Monitor prediction accuracy, input distributions, and model behavior. Set thresholds: if accuracy drops below a threshold or input distributions shift beyond expected ranges, trigger a review. Document monitoring procedures and results. Report findings to senior management quarterly.

Outcomes analysis compares model predictions to realized outcomes. For credit models, compare predicted default rates to actual default rates. For pricing models, compare predicted prices to actual transaction prices. Significant discrepancies indicate model error or environment change. Investigate and recalibrate the model if necessary.

Model changes require revalidation. If you retrain a fine-tuned model on new data, you must revalidate it. If you change the base model or fine-tuning methodology, you must revalidate. The scope of revalidation depends on the scope of change. Minor changes require limited revalidation. Major changes require full revalidation equivalent to initial validation.

Senior management is accountable. Executives must understand model limitations, ensure adequate resources for model risk management, and approve major models before deployment. This creates accountability at the top of the organization.

## HIPAA and Data Protection Obligations

Fine-tuning on protected health information or personal data triggers data protection obligations. In the US, HIPAA governs health data. In the EU, GDPR governs personal data. Both impose strict requirements that affect how you fine-tune and deploy models.

HIPAA requires covered entities and business associates to implement administrative, physical, and technical safeguards for PHI. If you fine-tune a model using PHI, you are a business associate and must sign a Business Associate Agreement with the covered entity. The BAA requires you to protect PHI, limit use to the agreed purpose, report breaches, and allow the covered entity to audit your controls.

Technical safeguards include encryption at rest and in transit, access controls limiting who can see PHI, audit logging of all PHI access, and automatic logoff. If you send PHI to a cloud fine-tuning API, the API provider must also sign a BAA and implement these safeguards. Major model providers like OpenAI, Anthropic, and Google offer BAA-compliant fine-tuning APIs for healthcare customers.

You must conduct a risk assessment identifying threats to PHI confidentiality, integrity, and availability. You implement controls to reduce risks to reasonable and appropriate levels. Document the assessment and controls. Update annually or when your system changes.

Breach notification is required if PHI is acquired, accessed, used, or disclosed without authorization. Breaches affecting 500 or more individuals must be reported to the Department of Health and Human Services within 60 days. Breaches affecting fewer than 500 individuals must be reported annually. Breaches must also be reported to affected individuals. Model training data leaks, inadvertent disclosure of PHI in model outputs, and unauthorized access to training datasets are all reportable breaches.

GDPR imposes additional requirements. Data minimization requires that you collect and process only the minimum data necessary for your purpose. If you can fine-tune effectively on aggregated or anonymized data, you must use that instead of personal data. Purpose limitation requires that you use data only for the purpose disclosed to data subjects. If you collected data for clinical care, you cannot use it for fine-tuning without additional consent or a legal basis like legitimate interest.

The right to explanation means that individuals have the right to understand automated decisions that significantly affect them. If your fine-tuned model makes credit, hiring, or insurance decisions, you must be able to explain how the decision was made. This is challenging for fine-tuned models because their reasoning is opaque. You need interpretability tools or human review processes to satisfy this requirement.

Data subject rights include the right to access, rectify, erase, and port personal data. If a patient requests deletion of their data, you must delete it from all systems, including training datasets. If your fine-tuned model was trained on that data, you face a dilemma: you cannot easily remove one individual's influence from a trained model. Some organizations retrain models after data deletion requests. Others maintain records of deletion requests and exclude those individuals from future training.

## Documentation Requirements and Audit Trails

Regulators across domains require comprehensive documentation and audit trails. Documentation demonstrates that you followed proper procedures. Audit trails enable accountability and investigation when things go wrong.

Model documentation should include the model card: a structured summary of the model's purpose, training data, evaluation results, limitations, and intended use. For fine-tuned models, the model card should also describe the base model, fine-tuning methodology, and differences between the base model and fine-tuned model.

Development documentation includes design specifications, architecture diagrams, training scripts, hyperparameter configurations, and evaluation code. This enables reproducibility and independent review. Version control all code and data. Tag releases. Link documentation to specific code versions.

Data documentation describes training, validation, and test datasets. Include data sources, collection methods, preprocessing steps, quality checks, and statistical properties. Describe any data exclusions or filtering. Document demographic distributions if the model makes decisions about people. This allows regulators to assess whether data is representative and free of bias.

Evaluation documentation includes test results, performance metrics, error analysis, and comparison to baselines. Include both automated metrics and expert review results. Document test set provenance: how was it created, by whom, and when. Include golden set results and track performance over time.

Deployment documentation describes the production environment, API endpoints, latency requirements, throughput limits, human oversight mechanisms, and incident response procedures. Document how users interact with the model, what information they see, and what actions they can take.

Audit trails log every model invocation. Logs should include timestamp, input, output, model version, user identity, and any human review or override. Logs must be tamper-proof: use write-once storage or cryptographic signatures. Retain logs for the period required by regulation: typically five to ten years for medical and financial systems.

Change logs document all model updates: retraining, fine-tuning updates, prompt changes, configuration changes, and infrastructure changes. Each change entry includes date, description, rationale, approver, and validation results. Change logs enable traceability: if model performance degrades, you can trace back to identify which change caused the issue.

## When Regulatory Compliance Drives Architecture

Sometimes regulatory requirements are so stringent that they dictate your architecture. You cannot build the system you want. You must build the system regulations allow.

For medical SaMD, the FDA's emphasis on determinism and traceability pushes teams toward rules-based systems or hybrid architectures. Pure fine-tuned models are hard to validate clinically because their behavior is probabilistic. Adding rule layers on top of fine-tuned models creates determinism: the model generates candidate outputs, and rules filter or rank them based on clinical guidelines. This hybrid approach is easier to validate and explain.

For financial model risk management, the requirement for conceptual soundness pushes teams toward interpretable models. Fine-tuned black-box models are difficult to validate because validators cannot assess whether the model's learned patterns are economically justified. Teams add interpretability layers: attention visualization, feature importance scores, or counterfactual explanations. These layers satisfy validators' need to understand model reasoning.

For GDPR compliance, the right to explanation and data minimization push teams toward federated learning or differential privacy. Instead of collecting all personal data centrally for fine-tuning, teams train models locally on user devices and aggregate updates. This minimizes central data storage and reduces privacy risk. Differential privacy adds noise to training data, protecting individual privacy while allowing model training. These techniques add complexity but satisfy regulatory requirements.

Regulatory compliance is not an afterthought. It shapes every decision from data collection to deployment. Teams that treat compliance as a checklist exercise after development fail. Teams that integrate compliance into design from day one succeed. Build compliance into your architecture, not onto it.

The final consideration for domain-adapted models is whether to build one model serving multiple domains or separate models for each domain, a decision driven by both technical and regulatory factors.

