# 7.11 â€” Red Teaming Fine-Tuned Models: Targeted Adversarial Testing

In June 2025, a healthcare technology company deployed a fine-tuned model to generate patient discharge summaries from clinical notes. The model had passed all standard evaluations: accuracy was 94%, readability scores were excellent, and clinician feedback during pilot testing was positive. Three weeks after full deployment across seventeen hospitals, a physician assistant discovered that when patients had certain combinations of mental health diagnoses and substance use history, the model occasionally inserted stigmatizing language that would never appear in the base model. The pattern only emerged when specific demographic markers appeared alongside behavioral health codes. The company pulled the model within hours, but not before 1,247 discharge summaries had been generated, reviewed by busy clinicians who missed the subtle bias, and sent to patients. The incident triggered a Joint Commission review, damaged relationships with patient advocacy groups, and cost the company $4.8 million in remediation, including manual review of every generated summary and revised model development. The root cause was not a flaw in the training data alone, but the absence of adversarial testing specifically designed to probe how fine-tuning had altered the model's behavior under edge conditions that would never appear in random evaluation samples.

Fine-tuning creates attack surfaces that do not exist in base models. Red teaming for fine-tuned models is not the same exercise as red teaming a foundation model. You must test not just what the model does, but what fine-tuning changed about how it responds to adversarial inputs, edge cases, and combinations of features that trigger newly learned patterns. This subchapter covers how to design and execute red team operations specifically for fine-tuned models, how to structure internal and external red team engagements, and how to document and remediate findings in ways that strengthen both the current model and your future fine-tuning process.

## Why Fine-Tuning Requires Specialized Red Teaming

Fine-tuning concentrates model behavior on a narrow distribution. That concentration is the point: you want the model to become expert at your specific task, using your organization's language, adhering to your domain's conventions. But concentration creates vulnerability. The model becomes more confident in its specialized domain and less robust to inputs that sit just outside the training distribution or combine features in unexpected ways.

Base models are trained on trillions of tokens representing vast diversity. They see medical text, legal text, creative writing, code, conversations, and everything in between. This breadth provides a form of adversarial robustness through sheer exposure to variation. Fine-tuning removes that breadth. Your training set might be ten thousand examples, all from your domain, all formatted consistently, all representing successful task completion. The model learns to expect that distribution. When an adversarial input arrives that looks superficially like your domain but contains subtle triggers, the fine-tuned model has less experience to fall back on than the base model did.

Red teaming must probe these new vulnerabilities. You are not testing whether the model can be jailbroken in the ways that affect base models, like prompt injection to ignore safety guidelines. You are testing whether fine-tuning introduced blind spots, whether the model fails gracefully when inputs deviate from training patterns, whether combinations of features that were rare in training trigger inappropriate responses, and whether the model has overfit to shortcuts in your data that adversaries can exploit.

## Mapping the Fine-Tuning Attack Surface

Start red teaming by mapping what changed. Compare the fine-tuned model's behavior to the base model on identical inputs, particularly inputs that sit at the boundaries of your training distribution. Identify where fine-tuning made the model more confident, where it made the model more rigid, and where it introduced new failure modes.

One financial services company fine-tuned a model to extract transaction details from unstructured payment descriptions. The base model was cautious, often returning low-confidence extractions or marking ambiguous fields as uncertain. The fine-tuned model, trained on clean historical data where every transaction had a clear merchant name, amount, and category, became highly confident. Red teaming revealed that when adversarial users intentionally obfuscated merchant names using Unicode lookalikes or inserted misleading amounts in parenthetical asides, the fine-tuned model confidently extracted incorrect data, while the base model had flagged the same inputs as ambiguous. Fine-tuning had removed healthy caution.

Map your attack surface by category. First, feature boundary attacks: inputs where individual features are valid but sit at the extreme edge of your training range. If your training data included transaction amounts from five dollars to fifty thousand dollars, test one cent and ten million dollars. If your training data included product descriptions up to two hundred words, test five words and five thousand words. Fine-tuned models often fail at boundaries because they learned to expect the central distribution.

Second, feature combination attacks: inputs where each individual feature is within range but the combination never appeared in training. Your model might have seen high-value transactions and international merchants separately, but never together. Your model might have seen medical codes for elderly patients and pediatric patients, but never codes that imply both simultaneously, which should trigger a validation error. Red teaming must construct these combinations deliberately because they will not appear in random test sets.

Third, adversarial feature attacks: inputs designed to exploit shortcuts the model learned during fine-tuning. If your training data always placed the most important information in the first sentence, adversaries will bury it in the third paragraph. If your training data always used title case for proper nouns, adversaries will use all lowercase. If your training data never included certain controversial terms because your data collection process filtered them, adversaries will inject exactly those terms to see if the model fails.

Fourth, drift attacks: inputs that represent how the world is changing in ways your training data does not capture. Your model was fine-tuned in early 2025. By late 2025, new regulations, new product categories, new terminology, and new adversarial tactics exist. Red teaming must include current adversarial techniques even if they postdate your training data, because your deployed model will encounter them.

## Internal Red Team Structure and Process

Internal red teams know your system, your data, and your threat model. They bring domain expertise and institutional knowledge. They also bring bias: they know what the model is supposed to do, and they may unconsciously avoid tests that feel unfair or irrelevant. Effective internal red teaming requires structure that overcomes this bias.

Assemble a cross-functional internal red team. Include engineers who did not work on the fine-tuning, domain experts who understand adversarial tactics in your field, product managers who know user workflows, and trust and safety specialists who understand where models fail harmfully. Do not include the data scientists who built the fine-tuned model on the red team. They are too close to the work. Their job is to remediate findings, not to generate them.

Give the red team a clear mandate. They are not trying to make the model look bad. They are trying to discover failure modes before users do, under controlled conditions where failures can be analyzed and fixed. Frame this as a service to the model team, not as adversarial oversight. The best red team findings are gifts: they reveal risks you can address now rather than in production.

Structure red team engagements in rounds. The first round is reconnaissance: the red team explores the model's behavior broadly, probing different input types, edge cases, and adversarial patterns without a specific attack goal. They document what they find: where the model is robust, where it is brittle, where it behaves unexpectedly. This round generates a map of potential vulnerabilities.

The second round is targeted exploitation: the red team takes the vulnerabilities discovered in reconnaissance and attempts to exploit them systematically. If reconnaissance found that the model struggles with ambiguous pronoun references, exploitation tests whether adversaries can use that weakness to cause the model to attribute actions to the wrong entity in contexts where correct attribution matters, like compliance reporting or medical record summarization. If reconnaissance found that the model is overconfident when certain keywords appear, exploitation tests whether adversaries can trigger false positives or false negatives by injecting those keywords strategically.

The third round is remediation validation: after the model team addresses findings from the first two rounds, the red team retests to confirm that fixes worked and did not introduce new vulnerabilities. This is not a formality. Fixes often create new edge cases. A patch that prevents one type of adversarial input may make the model more vulnerable to a related attack. Remediation validation ensures you are moving forward, not sideways.

Document every finding with specificity. A red team report that says "the model sometimes fails on ambiguous inputs" is useless. A red team report that says "when the input contains two entities of the same type in adjacent sentences and the second sentence uses a pronoun, the model attributes actions to the wrong entity in 34% of tested cases, including twelve examples where incorrect attribution would cause compliance violations" is actionable. Include the adversarial input, the model's response, the expected response, the harm that could result, and a severity rating.

## External Red Team Engagement

External red teams bring fresh perspectives, adversarial creativity, and freedom from institutional bias. They do not know what your model is supposed to do, so they test what it actually does. They do not care about hurting feelings or making the model look bad. Their job is to break it.

Engage external red teams after internal red teaming is complete and initial remediations are in place. Do not send a known-broken model to external red teamers. That wastes their time on issues you already know about. External red teams should focus on vulnerabilities you did not anticipate, adversarial techniques you have not seen, and edge cases that require domain expertise you lack internally.

Choose external red teamers with relevant expertise. If you fine-tuned a model for legal document analysis, engage red teamers with legal backgrounds who understand how adversaries manipulate legal text. If you fine-tuned a model for content moderation, engage red teamers with trust and safety experience who know current adversarial tactics. General-purpose red teamers can find some issues, but specialist red teamers find the issues that matter in your domain.

Provide external red teams with clear scope and constraints. Tell them what the model is supposed to do, what types of inputs it will receive in production, what harms you are most concerned about, and what types of adversarial tests are out of scope. If your model will never process certain types of content in production, testing it on that content is interesting academically but not operationally useful. Focus external red team time on realistic threat scenarios.

Give external red teams access to the model in a controlled environment. Do not give them production access. Set up a dedicated evaluation endpoint where they can submit inputs and receive outputs, with rate limits to prevent abuse and logging to capture every test. Provide them with sample valid inputs so they understand the expected format and domain, but do not constrain their creativity. The best adversarial tests are ones you would never think to run yourself.

External red team engagements typically run two to four weeks. Week one is exploration: the red team learns the model's behavior, identifies potential vulnerabilities, and develops attack hypotheses. Week two is exploitation: they test those hypotheses systematically and document findings. Week three is deep dives: they take the most interesting or severe findings and explore them thoroughly, generating multiple examples and edge cases. Week four is reporting: they synthesize findings into a written report with severity ratings, exploitation demonstrations, and recommendations.

Treat external red team reports as confidential and high-priority. Schedule a readout meeting where the red team presents findings to your model team, product team, and leadership. Do not be defensive. Ask clarifying questions, probe for additional context, and thank them for their work. Then triage findings by severity and remediate them before deployment.

## Severity Classification for Red Team Findings

Not all red team findings are equal. Some represent catastrophic risks that block deployment. Others represent edge cases you can monitor and address post-deployment. Classify findings using a consistent severity framework so you can make rational decisions about what to fix now, what to fix later, and what to accept as residual risk.

Critical findings cause immediate harm to users, violate regulations, or create legal liability. Examples include fine-tuned models that generate protected health information in violation of HIPAA, models that produce discriminatory outputs that violate employment law, models that leak training data containing personally identifiable information, or models that provide advice that could cause physical harm. Critical findings block deployment until resolved. Do not ship a model with known critical vulnerabilities.

High findings cause significant harm in realistic scenarios but require specific adversarial inputs or edge case conditions to trigger. Examples include models that fail catastrophically when inputs exceed certain length thresholds, models that produce confidently wrong outputs when certain feature combinations appear, or models that exhibit clear bias when certain demographic markers are present in inputs. High findings must be resolved before deployment or mitigated with runtime safeguards that prevent the adversarial conditions from reaching the model.

Medium findings cause moderate harm or user frustration but do not create immediate safety, legal, or reputational risk. Examples include models that produce low-quality outputs on rare input types, models that fail to handle certain edge cases gracefully, or models that exhibit inconsistent behavior that confuses users but does not harm them. Medium findings should be prioritized for post-deployment fixes and monitored in production to ensure they remain rare.

Low findings represent theoretical vulnerabilities or issues that occur only under highly contrived conditions unlikely to appear in production. Examples include models that behave strangely when inputs contain unusual Unicode characters that your production system filters before they reach the model, or models that fail on input types that your product workflow prevents users from submitting. Low findings are documented for future reference but do not require immediate action.

Classify findings based on likelihood and impact. A vulnerability that occurs frequently but causes minor harm might be medium severity. A vulnerability that occurs rarely but causes catastrophic harm when it does is high or critical severity. Use your threat model and production context to calibrate classifications. A bias that appears in one percent of outputs is low severity if those outputs are reviewed by humans before being shown to users, but critical severity if outputs go directly to users without review.

## Remediation Strategies and Trade-offs

Red team findings require remediation, but remediation is not always straightforward. Some findings reveal fundamental issues with your training data that require expensive re-collection and retraining. Others can be addressed with targeted data augmentation, architectural changes, or runtime safeguards. Choose remediation strategies that balance effectiveness, cost, and time.

Data augmentation is the first remediation strategy to consider. If red teaming reveals that your model fails on certain input types or feature combinations, add adversarial examples covering those patterns to your training set and retrain. This works well when the failure mode is narrow and you can generate or collect sufficient examples. A model that fails on uppercase inputs can be fixed by adding uppercase examples. A model that fails on inputs longer than your training data can be fixed by adding longer examples.

Data augmentation fails when the failure mode is too broad or when generating adversarial examples is expensive. If red teaming reveals that your model exhibits bias across dozens of demographic combinations, augmenting data to cover all combinations may be infeasible. If generating adversarial examples requires expensive human labeling, augmentation may not be cost-effective for low-severity findings.

Architectural changes are the second strategy. If red teaming reveals that your model is overconfident, add calibration techniques that adjust output probabilities to better reflect true accuracy. If red teaming reveals that your model fails to handle uncertainty, add abstention mechanisms that allow the model to decline to answer when confidence is low. If red teaming reveals that your model leaks training data, add differential privacy techniques during fine-tuning.

Architectural changes require re-training and re-evaluation. They are more expensive than data augmentation but more robust when the issue is fundamental to how the model learned rather than what it learned. Overconfidence is not fixed by adding more data; it is fixed by changing how the model represents uncertainty.

Runtime safeguards are the third strategy. If red teaming reveals adversarial patterns that you can detect reliably, add input validation or output filtering that catches those patterns before they cause harm. If adversarial inputs always contain certain keywords, filter them. If adversarial outputs always have certain structural properties, detect and block them. If adversarial failures always occur when confidence is below a threshold, route those cases to human review.

Runtime safeguards are fast to implement and do not require retraining, but they are brittle. Adversaries adapt. A filter that blocks specific keywords today will be bypassed by synonyms tomorrow. A confidence threshold that works this month may fail next month as input distribution drifts. Use runtime safeguards for high-severity findings that must be addressed immediately while you work on more robust data or architectural fixes.

## Documenting Red Team Findings for Institutional Learning

Red team findings are not just tickets to close. They are data about how your fine-tuning process creates vulnerabilities. Document findings in ways that inform future model development, improve your training data collection, and strengthen your evaluation practices.

Create a red team findings database that captures every finding across all models. Include the model version, the adversarial input, the model's response, the expected response, the severity classification, the remediation strategy, and the outcome. Tag findings by category: feature boundary failure, feature combination failure, adversarial shortcut exploitation, distributional drift, bias, safety, privacy, and so on.

Over time, this database reveals patterns. You might discover that every fine-tuned model you build fails on inputs longer than training examples, indicating a systematic issue with how you collect training data. You might discover that models fine-tuned on data from certain sources consistently exhibit bias, indicating a data quality issue. You might discover that certain adversarial techniques succeed repeatedly, indicating gaps in your evaluation process.

Use red team findings to update your evaluation sets. When a red team discovers a failure mode, add examples of that failure mode to your standard evaluation suite so future models are tested for that issue during development, not during red teaming. This creates a virtuous cycle: red teaming finds new vulnerabilities, those vulnerabilities become standard tests, and future models are more robust.

Use red team findings to update your training data collection guidelines. If red teaming repeatedly finds that models fail on certain input types, prioritize collecting training data that includes those input types. If red teaming finds that models overfit to shortcuts, revise your data collection to ensure diversity that prevents those shortcuts.

Share anonymized red team findings across teams. If one model team discovers that fine-tuning on data with implicit demographic signals causes bias, other model teams need to know. If one model team discovers a remediation strategy that works well, other teams can adopt it. Red teaming generates organizational knowledge, not just model-specific fixes.

## The Red Team Cadence for Fine-Tuned Models

Red teaming is not a one-time gate before deployment. Fine-tuned models change over time as you retrain on new data, adjust to feedback, and respond to production issues. Red teaming must be continuous.

Conduct full red team engagements before initial deployment, before major model updates, and on a recurring schedule. Initial deployment red teaming is the most intensive: you are testing a new model with unknown vulnerabilities. Major update red teaming focuses on what changed: if you retrained on new data, red team the impact of that new data; if you changed the architecture, red team the impact of that change. Recurring red teaming, perhaps quarterly, ensures you catch drift and adversarial evolution even when the model itself has not changed.

Run lightweight adversarial testing continuously in production. Maintain a set of adversarial test cases derived from red team findings and run them against every model deployment as part of your CI/CD pipeline. If a new deployment fails adversarial tests that the previous deployment passed, block the deployment and investigate. This catches regressions before they reach users.

Treat red teaming as a core competency, not an outsourced compliance exercise. Invest in building internal red team skills. Train engineers to think adversarially. Create time and space for red team work. Celebrate findings, not just fixes. The best fine-tuning teams are the ones that find their own vulnerabilities before anyone else does.

Red teaming fine-tuned models is not optional, and it is not the same as testing base models. Fine-tuning creates new attack surfaces, and those surfaces must be probed deliberately, systematically, and continuously. The next subchapter addresses how evaluation does not stop at deployment, covering continuous monitoring for quality drift, safety degradation, and performance changes in production.
