# 4.11 — Technique Selection Framework: Matching Method to Task and Budget

A $60,000 budget. A six-week timeline. 12,000 labeled examples. A small GPU cluster. Domain experts on standby. A VP of Engineering demanding a production-ready fine-tuned model to replace a $18,000-per-month GPT-4 API integration. In mid-2025, a SaaS company with a legal document automation product faced a technique selection decision: full fine-tuning of a 7-billion-parameter model, LoRA on a 13-billion-parameter model, or supervised fine-tuning on a smaller 3-billion-parameter model with potential preference tuning afterward. The team chose full fine-tuning on the 7-billion-parameter model because it felt like the "proper" approach and they had heard LoRA sacrifices quality. Three weeks in, they had burned $41,000 in compute and achieved a model that barely outperformed the base model. They restarted with LoRA on a 13-billion-parameter model. It took four days, cost $3,200, and delivered 22 percent better performance than the failed full fine-tuning attempt. The root cause was not bad execution. It was choosing based on intuition and prestige instead of a principled technique selection framework.

Technique selection is the most consequential decision you make in a fine-tuning project. It determines your compute costs, training duration, data requirements, infrastructure needs, and achievable quality ceiling. Most teams treat it as a philosophical debate or a matter of personal preference. Professional fine-tuning teams treat it as an engineering decision with clear inputs, decision criteria, and predictable outcomes. The difference is not just cost efficiency—it is whether your project succeeds at all.

## The Input Factors: What You Know Before You Start

Your technique selection depends on six input factors you can measure or estimate before training begins. **Task type** defines what you are asking the model to do: classification, extraction, generation, reasoning, or some combination. **Data availability** quantifies how many labeled examples you have and how easily you can obtain more. **Budget** specifies your financial constraints for compute and storage. **Infrastructure** describes what hardware and platforms you can access. **Quality requirements** set the performance bar you must clear for production deployment. **Timeline** defines how much calendar time you have before you must ship.

Task type determines which fine-tuning methods are even applicable. If your task is simple classification with 50 categories and 10,000 labeled examples, supervised fine-tuning alone will likely suffice. If your task is open-ended generation with subjective quality criteria like tone and creativity, you will need preference tuning methods like DPO in addition to SFT. If your task requires the model to learn new factual knowledge not present in pretraining—medical guidelines published after the base model's training cutoff, proprietary product specifications, internal policy documents—you need continued pretraining before supervised fine-tuning. Task type narrows your technique space immediately.

Data availability is often your binding constraint. Full fine-tuning typically requires 10,000 to 100,000 examples to achieve meaningful improvement over the base model for most tasks. LoRA can succeed with 1,000 to 10,000 examples because it updates fewer parameters. Continued pretraining requires millions of tokens of domain text, but not labeled examples—you can use raw documents. If you have only 500 labeled examples and no budget to create more, your viable options are LoRA with aggressive regularization or few-shot prompting rather than fine-tuning. Data availability eliminates techniques that exceed your data budget.

Budget and infrastructure are intertwined. Full fine-tuning a 7-billion-parameter model for 5,000 steps on modern GPUs costs roughly $2,000 to $8,000 depending on hardware and provider. LoRA on the same model for the same steps costs $200 to $800 because you update fewer parameters and can use smaller batch sizes. Continued pretraining is the most expensive because it requires far more training steps—often 50,000 to 500,000 steps—driving costs to $20,000 to $200,000 for larger models. If your total budget is $5,000, you cannot afford full fine-tuning on large models or any continued pretraining. You must use LoRA or fine-tune a smaller model.

Quality requirements set your floor. If your production deployment requires 95 percent accuracy and LoRA achieves only 91 percent while full fine-tuning reaches 96 percent, you must use full fine-tuning despite the cost. If your requirement is 85 percent and LoRA achieves 89 percent, you use LoRA and save the budget. Quality requirements are not aspirations—they are hard constraints derived from user experience or business SLAs. Measure them honestly against technique capabilities.

Timeline determines what you can attempt within your delivery window. Full fine-tuning a large model with hyperparameter search might take three to six weeks of iteration. LoRA can often deliver results in one to two weeks. If you have a four-week deadline and no room for slippage, you cannot afford techniques with long iteration cycles.

## The Decision Paths: From Inputs to Technique

With your input factors measured, you can navigate the decision tree that maps to appropriate techniques. The first branch is task complexity. For **simple classification or extraction tasks** with clear labels and objective success criteria, start with supervised fine-tuning alone. If you have abundant data—more than 10,000 examples—and budget permits, full fine-tuning will likely give you the best performance. If data is limited—fewer than 5,000 examples—or budget is tight, use LoRA. You do not need preference tuning for tasks with unambiguous right answers.

For **generation tasks with subjective quality criteria**—summarization, rewriting, creative writing, conversational agents—you need supervised fine-tuning to teach the model task structure, followed by preference tuning like DPO to align outputs with human preferences. SFT alone will give you grammatical, on-topic outputs, but it will not reliably produce the tone, style, or nuance that users prefer. The SFT-then-DPO pipeline is the standard approach for these tasks. Whether you use full fine-tuning or LoRA for each stage depends on data and budget, but you need both stages.

For **knowledge-intensive tasks** where the model must demonstrate expertise in a domain poorly covered by pretraining—legal, medical, scientific, proprietary business knowledge—you need continued pretraining on domain corpora before supervised fine-tuning. CPT teaches the model domain facts and vocabulary. SFT teaches it how to apply that knowledge to your specific task. Skipping CPT and going straight to SFT produces models that hallucinate domain-specific information or fail to use domain terminology correctly. The CPT-then-SFT pipeline is mandatory for knowledge adaptation. Budget requirements are high—expect $30,000 to $300,000 depending on domain size and model size—but there is no cheaper alternative that achieves comparable domain expertise.

For **reasoning tasks** that require multi-step inference, chain-of-thought patterns, or complex problem-solving—code generation, mathematical reasoning, strategic planning—you need supervised fine-tuning with reasoning traces in your training data, often followed by preference tuning to refine solution quality. Some teams also use rejection sampling or reinforcement learning to improve reasoning accuracy beyond what SFT and DPO provide. Reasoning tasks are the most technique-intensive and typically require the full stack: SFT with reasoning demonstrations, DPO or RLHF for preference alignment, and possibly RL fine-tuning for task-specific reward optimization.

The second major decision branch is **full fine-tuning versus LoRA**. Full fine-tuning updates all model parameters and generally achieves the highest quality ceiling, but costs more and requires more data. LoRA updates only low-rank adapter matrices, costs less, requires less data, and trains faster, but may plateau at slightly lower quality than full fine-tuning for some tasks.

Use full fine-tuning when you have abundant data—more than 10,000 examples for SFT or 5,000 preference pairs for DPO—and sufficient budget, and your quality requirements are stringent. Use full fine-tuning when your task requires the model to deviate significantly from base model behavior, such as learning a completely new output format or domain-specific reasoning style. Full fine-tuning is also preferable when you plan to deploy the model in high-throughput production where inference latency and cost per query matter more than training cost, because fully fine-tuned models do not carry the inference overhead of loading separate adapter weights.

Use LoRA when data is limited—fewer than 5,000 examples—or budget is constrained below $5,000 for training. Use LoRA when timeline is short and you need to iterate quickly, because LoRA trains in a fraction of the time required for full fine-tuning. Use LoRA when you need to maintain multiple task-specific variants of the same base model, because you can train and store separate LoRA adapters for each task and swap them at inference time without maintaining multiple full model copies. LoRA is the default choice for most teams unless your specific task clearly demands full fine-tuning.

## The Technique Selection Matrix

A practical way to organize technique selection is through a matrix that maps task and resource characteristics to recommended approaches. For classification and extraction tasks with more than 10,000 examples and budgets above $5,000, use full SFT. For the same task types with fewer than 5,000 examples or budgets below $5,000, use LoRA SFT. For generation tasks with subjective quality criteria, use SFT plus DPO, choosing full versus LoRA based on budget and data as before. For knowledge-intensive tasks requiring domain expertise, use CPT followed by SFT, choosing full versus LoRA for the SFT stage based on budget. For reasoning tasks, use SFT with reasoning traces, then DPO, and optionally RL fine-tuning if your quality bar is exceptionally high.

The matrix is not a rigid formula. It is a starting heuristic that you adapt to your specific context. If you are on the boundary between categories—say, you have 6,000 examples and a $4,000 budget—you use judgment and possibly run small-scale experiments to test whether LoRA suffices or whether you need to gather more data and budget for full fine-tuning.

You also consider failure modes. If your deployment has zero tolerance for quality regressions—a medical application, a financial compliance tool—you bias toward more conservative techniques with higher quality ceilings, even if they cost more. If your deployment is iterative and you can tolerate initial lower quality in exchange for faster learning cycles—a consumer-facing assistant, an internal productivity tool—you bias toward faster, cheaper techniques like LoRA that enable rapid iteration.

## When SFT Alone Suffices

Many teams overengineer their fine-tuning pipeline. They assume they need preference tuning, continued pretraining, or reinforcement learning when supervised fine-tuning alone would meet their requirements. **SFT suffices** when your task has objective success criteria that can be labeled deterministically, when the base model already has strong general capabilities in your task domain, and when output format and structure matter more than subjective qualities like tone or creativity.

Code generation tasks where success is defined by passing test cases are often well-served by SFT alone. You fine-tune on examples of problem statements paired with correct code solutions, and the model learns to produce syntactically valid, logically correct code. Preference tuning might improve code style or conciseness, but if your only requirement is functional correctness, SFT delivers it. Classification tasks with clear category definitions, extraction tasks with well-defined schemas, and formatting tasks like structured data transformation all fall into this category.

SFT alone also suffices when you lack the resources to collect preference data. Preference tuning requires either human ranking of model outputs or AI-generated preference labels, both of which add cost and complexity beyond standard supervised labeling. If you can achieve acceptable quality with SFT and your budget or timeline does not allow for preference data collection, ship with SFT alone and plan preference tuning for a future iteration.

You know SFT alone is insufficient when your validation metrics plateau well below production requirements, when qualitative evaluation reveals that outputs are correct but lack desirable stylistic qualities, or when user feedback highlights preference misalignment—outputs that are factually accurate but tone-deaf, verbose, or otherwise misaligned with user expectations. These signals tell you to add preference tuning.

## When to Add DPO

**Direct Preference Optimization** and similar preference tuning methods become necessary when your task involves subjective quality dimensions that cannot be captured by supervised labels alone. Tone, conciseness, creativity, helpfulness, harmlessness—these are qualities that require human judgment to define and preference data to optimize.

Add DPO when you have completed SFT and achieved solid baseline performance, but user feedback or qualitative evaluation indicates that outputs need refinement along subjective axes. A common pattern is to launch with an SFT model, collect production usage data showing which outputs users prefer, and then run DPO to align the model with observed preferences. This grounds your preference tuning in real user behavior rather than hypothetical rankings.

Add DPO when your task explicitly requires alignment with human values or preferences, such as conversational agents, creative writing tools, or customer support automation. For these applications, being factually correct is necessary but not sufficient. The model must also match user expectations for style, empathy, formality, and tone. SFT teaches structure; DPO teaches alignment.

You need preference data to run DPO—typically thousands of preference pairs where humans or AI systems have ranked two or more model outputs for the same input. Collecting this data costs time and money. If you cannot afford preference data collection, you cannot run DPO. Some teams use AI-generated preference labels from a stronger model like GPT-4 to reduce cost, but this introduces its own risks of misalignment if the AI ranker's preferences do not match your users' preferences.

DPO also requires more sophisticated training infrastructure than SFT because it involves sampling from two models simultaneously—the policy model being updated and a reference model that stays fixed—and computing preference-based loss functions. If your team lacks experience with preference tuning or your infrastructure is minimal, the operational complexity of DPO may outweigh its benefits. In such cases, iterate on SFT prompt design and data quality before attempting DPO.

## When CPT Is Needed

**Continued pretraining** is the most expensive and time-consuming fine-tuning technique, but it is also the only way to teach a model substantial new factual knowledge or adapt it to a domain with vocabulary and concepts absent from its original training data. CPT is needed when your task requires domain expertise that the base model demonstrably lacks.

Medical applications often require CPT. Base models like GPT-4 or Llama have broad medical knowledge from general web pretraining, but they lack depth in specialized subfields like oncology treatment protocols, rare disease diagnostics, or current clinical guidelines. If you need your model to reason fluently about complex medical scenarios, you must run CPT on medical literature, clinical notes, and research papers before fine-tuning on your specific task. SFT alone will not bridge the knowledge gap—it will teach the model to format medical outputs, but the outputs will contain hallucinated facts or outdated information.

Legal, scientific, and proprietary business domains often require CPT for the same reason. If your task involves patent law and the base model was trained before recent patent legislation changes, you need CPT on updated legal corpora. If your task involves internal company knowledge—product specifications, policies, historical decisions—that exists only in private documents, you need CPT on those documents.

CPT is also necessary when your task uses specialized vocabulary or notation systems that are rare or absent in general pretraining data. Mathematical domains with heavy symbolic notation, programming languages or frameworks released after the base model's training cutoff, and niche technical fields with domain-specific jargon all benefit from CPT.

You know CPT is needed when SFT produces outputs that are structurally correct but factually hollow or incorrect. If your fine-tuned model generates plausible-sounding medical recommendations that are clinically wrong, or legal arguments that cite nonexistent statutes, you have a knowledge deficit that SFT cannot fix. CPT is the remedy.

The cost of CPT scales with the amount of domain knowledge you need to impart and the size of the model. Teaching a 7-billion-parameter model a narrow subdomain with 10 million tokens of text might cost $5,000 to $20,000 in compute. Teaching a 70-billion-parameter model a broad domain with 500 million tokens might cost $200,000 or more. These are not training budgets most teams have access to. CPT is an investment you make when domain expertise is a hard requirement and no cheaper alternative meets your quality bar.

## Budget-Driven Technique Constraints

Your budget determines what techniques are even feasible. If your total project budget is $2,000, you cannot afford full fine-tuning on large models or any continued pretraining. Your viable options are LoRA on a medium-sized model or full fine-tuning on a very small model—3 billion parameters or fewer. If your budget is $500, you may need to use few-shot prompting with a strong base model rather than fine-tuning at all.

For budgets between $1,000 and $5,000, LoRA SFT on models up to 13 billion parameters is achievable. You can train to convergence in a few days and have budget left for hyperparameter search and validation. For budgets between $5,000 and $20,000, you can afford full SFT on 7-billion-parameter models or LoRA on larger models up to 70 billion parameters, and you can add a DPO stage if your task requires it. For budgets above $20,000, full fine-tuning on large models, multi-stage pipelines with CPT, SFT, and DPO, and extensive hyperparameter tuning are all within reach.

Budget constraints also affect iteration speed. If you have $50,000 to spend but only two weeks to deliver, you cannot afford techniques with long training times or many iteration cycles. You must choose fast methods like LoRA even if full fine-tuning might yield marginally better quality, because you will not have time to complete full fine-tuning within your deadline.

Professional teams plan budgets before technique selection, not after. They estimate compute costs for candidate techniques, compare them to available budget, and eliminate options that exceed budget before running any training. This prevents the costly mistake of starting with a technique you cannot afford to complete.

## Infrastructure Constraints and Technique Viability

Your available infrastructure directly limits which techniques you can execute. Full fine-tuning a 7-billion-parameter model requires GPUs with at least 40 GB of VRAM, or multi-GPU setups with memory-efficient training configurations. If you have access only to consumer GPUs with 16 GB or 24 GB of VRAM, full fine-tuning large models is not viable without extreme quantization or memory optimization, which degrade training stability and quality. LoRA, which updates far fewer parameters, can run comfortably on smaller GPUs.

Continued pretraining requires the most infrastructure because it involves the longest training runs—often tens of thousands to hundreds of thousands of steps. You need stable, long-running compute that will not be preempted or interrupted. Cloud preemptible instances are too risky for CPT. You need reserved instances or on-premises hardware you control. If you lack this infrastructure, you cannot run CPT reliably.

Some teams have access to distributed training infrastructure—multi-node GPU clusters managed by orchestration platforms. This enables full fine-tuning and CPT at scales impossible on single machines. Other teams are constrained to single cloud instances or local workstations. Infrastructure capacity is an input to technique selection, not a variable you tune after deciding on a technique. You choose techniques your infrastructure can support.

Cloud costs and infrastructure availability also interact with timeline. If your cloud provider has limited GPU availability and you face long queue times to provision instances, techniques requiring many training iterations become impractical. You may bias toward LoRA or smaller models that train faster and require less provisioning time.

## Quality Requirements as Hard Constraints

Quality requirements are not soft targets you hope to hit—they are hard constraints derived from user experience, business SLAs, or regulatory requirements. If your model must achieve 90 percent accuracy to replace a manual process, and your fine-tuned model achieves only 85 percent, you have failed, regardless of how efficiently you trained. Technique selection must account for achievable quality ceilings, not just cost and speed.

Full fine-tuning generally achieves higher quality ceilings than LoRA for tasks requiring significant behavior change from the base model. If your quality requirement is near the upper limit of what your data and model size can achieve, you need full fine-tuning. If your requirement is well below the ceiling and LoRA comfortably exceeds it, use LoRA.

Some tasks have multi-dimensional quality requirements: accuracy, latency, output length, tone, safety. Your fine-tuning technique must meet all dimensions simultaneously. If LoRA achieves high accuracy but produces outputs that are too verbose for your UI, and your quality requirements include conciseness, LoRA has failed despite meeting the accuracy bar. You may need to add DPO to tune for conciseness, or you may need to revisit data and prompt design.

Quality requirements also evolve. You may launch with a 75 percent accuracy requirement, then raise it to 85 percent as users adopt the feature and expectations increase. Your technique selection must anticipate future quality needs, not just initial launch criteria. If you choose LoRA because it meets today's 75 percent bar, but you expect to need 85 percent within six months, and LoRA cannot reach 85 percent, you will need to retrain with full fine-tuning later. Planning for future quality requirements can save you from rebuilding your entire training pipeline.

## Iteration Velocity and Technique Selection

The speed at which you can iterate—train a model, evaluate it, adjust hyperparameters or data, and retrain—determines how quickly you can improve quality and reach production readiness. Techniques that enable fast iteration are valuable even if their quality ceiling is slightly lower than slower techniques, because you can run more experiments and converge on optimal configurations faster.

LoRA enables much faster iteration than full fine-tuning. A LoRA training run that takes four hours versus a full fine-tuning run that takes 40 hours means you can test ten configurations in the time it takes to test one with full fine-tuning. For teams still exploring hyperparameter space, data quality issues, or task definitions, LoRA's iteration speed is a decisive advantage.

Fast iteration also reduces risk. If you discover your data has labeling errors or distribution issues after starting training, a four-hour LoRA run is a minor setback. A 40-hour full fine-tuning run is a major loss. Teams that prioritize iteration velocity often start with LoRA to de-risk data and hyperparameter choices, then switch to full fine-tuning for the final production model once they have converged on a stable configuration.

Iteration velocity also depends on validation speed. If your task requires expensive human evaluation for validation, you cannot afford to iterate rapidly regardless of training speed. You must batch validation cycles and make each training run count. For tasks with cheap automated validation—test case pass rates, classification accuracy, BLEU scores—you can iterate quickly and benefit from fast training techniques.

## Combining Techniques in Staged Pipelines

Many production systems use staged pipelines that combine multiple fine-tuning techniques sequentially. The canonical pipeline for high-quality generation tasks is continued pretraining for knowledge, supervised fine-tuning for task structure, and preference tuning for alignment. Each stage addresses a different quality dimension, and skipping any stage leaves a gap.

A medical question-answering system might run CPT on medical literature to build domain knowledge, then SFT on question-answer pairs to teach the QA task structure, then DPO on preference rankings to optimize for conciseness and patient-friendly language. Each stage builds on the previous one. You cannot skip CPT and expect SFT to teach domain facts. You cannot skip SFT and expect DPO to teach task structure. You cannot skip DPO and expect SFT alone to produce patient-friendly tone.

Staged pipelines require careful planning of data flow and checkpointing. The output of CPT is the input to SFT. The output of SFT is the input to DPO. You need to save and version checkpoints at each stage so you can roll back if a later stage degrades quality. You also need validation at each stage to confirm that the stage succeeded before proceeding to the next.

Staged pipelines are expensive—each stage has its own compute and data costs—but they are often the only path to the quality levels required for complex, high-stakes applications. Budget your resources across all stages from the beginning, not just the first stage.

## Choosing Model Size and Architecture

Technique selection includes choosing the base model size and architecture you will fine-tune. Larger models generally have higher quality ceilings but cost more to train and deploy. Smaller models are cheaper and faster but may plateau at lower quality.

For most tasks, 7-billion-parameter models offer the best balance of quality, cost, and deployment feasibility. They are large enough to achieve strong performance on a wide range of tasks after fine-tuning, yet small enough to train and deploy affordably. 13-billion-parameter models offer incrementally better quality at roughly double the cost. 70-billion-parameter models offer state-of-the-art quality but require significant infrastructure and budget.

Your task complexity and quality requirements guide model size selection. Simple classification or extraction tasks can often be solved with 3-billion-parameter models or smaller, saving costs without sacrificing quality. Complex reasoning tasks or knowledge-intensive applications may require 13-billion-parameter models or larger to reach acceptable performance.

Architecture also matters. Some model families are optimized for instruction-following and perform better after fine-tuning on instructional tasks. Others are optimized for raw completion and perform better on text generation. Choose a base model architecture aligned with your task type.

## The Decision Framework in Practice

When you begin a fine-tuning project, you document your six input factors: task type, data availability, budget, infrastructure, quality requirements, timeline. You use these inputs to navigate the decision tree. Is your task classification or generation? Do you have 1,000 examples or 10,000? Is your budget $2,000 or $20,000? Do you need 85 percent accuracy or 95 percent? Can you deliver in two weeks or six weeks?

The answers narrow your technique space. If you have a generation task with subjective quality criteria, 8,000 examples, a $10,000 budget, and a four-week timeline, your path is LoRA SFT followed by LoRA DPO on a 7-billion-parameter model. If you have a knowledge-intensive extraction task with 50,000 examples, a $100,000 budget, and a 12-week timeline, your path is CPT on domain corpora, then full SFT on extraction data.

You document your technique selection decision and its rationale. This creates accountability and enables post-mortem analysis if the technique fails to meet requirements. You revisit technique selection if inputs change—if you discover your data is lower quality than estimated, if budget is cut, if timeline is shortened. Technique selection is not a one-time decision—it is a living choice that adapts to project realities.

The teams that succeed at fine-tuning are not those with the most compute or the largest datasets. They are the teams that systematically match technique to task and resource constraints, who choose based on evidence and analysis rather than intuition or trend-chasing, and who iterate with discipline when initial techniques fall short. Technique selection is the foundation of fine-tuning success. Everything else builds on this choice.
