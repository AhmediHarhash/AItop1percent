# 2.10 — The Training Data Spec Document: Schema, Conventions, and Sign-Off

In mid-2025, a legal technology company launched a project to fine-tune a model for contract clause extraction. They hired an annotation vendor, provided a one-page guideline document, and asked for 10,000 annotated contracts. Four months later, the vendor delivered the dataset. The engineering team began training and immediately noticed problems. Some annotators had labeled clause headings, others had labeled full clause bodies, and still others had labeled both. Some had used sentence boundaries, others had used paragraph boundaries. Some had marked implied clauses, others had only marked explicit clauses. The consistency was so poor that training on the data produced a model with 52% precision—worse than a heuristic baseline. The team had to discard the entire dataset, write a detailed 18-page annotation specification, re-annotate 8,000 contracts, and delay the launch by five months. The root cause was not annotator incompetence. It was the absence of a clear, comprehensive training data spec that defined exactly what good data looks like.

Before you collect a single training example, you need a spec document that serves as a contract between data creators and model trainers. This subchapter covers what goes into a training data spec, how to write one that prevents costly rework, how to establish annotation conventions that ensure consistency, and how to get organizational sign-off so that the spec becomes a binding agreement rather than an ignored formality. A good spec is the difference between a training dataset that works on the first try and a training dataset that costs you months of rework and tens of thousands of dollars.

## The Training Data Spec as a Contract

A training data spec is not a guideline. It is not a suggestion. It is a contract. It defines the schema, the format, the annotation conventions, the quality criteria, and the acceptance process. Every party involved in creating or using the data agrees to the spec before work begins. The spec answers every ambiguous question that an annotator, engineer, or reviewer might encounter. If the spec does not answer a question, the spec is incomplete, and you fix it before proceeding.

The spec has six sections: purpose and scope, schema definition, annotation conventions, quality criteria, tooling and workflow, and acceptance and sign-off. The purpose and scope section defines what the data will be used for, what tasks the model will perform, and what kinds of inputs and outputs are in scope. The schema definition section defines the structure of each training example: what fields exist, what types they have, what constraints they must satisfy. The annotation conventions section defines how to handle edge cases, ambiguity, and judgment calls. The quality criteria section defines what constitutes a good annotation and what constitutes a rejection. The tooling and workflow section defines what tools annotators use, how they submit their work, and how disagreements are resolved. The acceptance and sign-off section defines who reviews the spec, who approves it, and what process you follow to amend it.

Each section must be specific enough that two annotators working independently on the same example will produce the same annotation 95% of the time or more. If your inter-annotator agreement is lower than 95%, your spec is too vague, your task is too subjective, or your annotators are not following the spec. You measure inter-annotator agreement early by having multiple annotators label the same small batch and computing agreement metrics. If agreement is low, you revise the spec, provide additional training, or simplify the task.

## Schema Definition: Fields, Types, and Constraints

The schema defines the structure of your training data. Every training example is a record with a defined set of fields. For a fine-tuning dataset, the most common schema is input-output pairs: a prompt field and a completion field. But real-world tasks often require richer schemas. You may need metadata fields: source, timestamp, annotator ID, confidence score. You may need multi-turn conversation fields: a list of messages with roles and content. You may need structured output fields: entity spans with start and end positions, labels, and attributes. You may need auxiliary fields: reference documents, context passages, or grounding evidence.

Each field has a type: string, integer, float, boolean, list, dictionary. Each field has constraints: required or optional, minimum and maximum length, allowed values, format requirements. For example, a prompt field might be a required string with a minimum length of 10 characters and a maximum length of 2000 characters. A completion field might be a required string with a maximum length of 500 characters. A label field might be a required string from an enumerated set of allowed values. A confidence field might be a required float between 0 and 1.

Write out the schema formally. Use a schema definition language like JSON Schema or a table format that lists each field, its type, its constraints, and a description. Include examples of valid records and examples of invalid records. Specify what happens when a constraint is violated: is the record rejected automatically, is it flagged for review, or is it accepted with a warning? Automate schema validation so that every record is checked before it enters your training pipeline. If a record violates the schema, it should be rejected immediately with a clear error message that tells the annotator what went wrong and how to fix it.

## Annotation Conventions: The Rules for Judgment Calls

Annotation is not a mechanical process. It requires judgment. Your spec must codify the rules for every judgment call an annotator will face. Start by identifying ambiguity. Review a sample of your raw data and list every situation where two reasonable annotators might make different decisions. For each ambiguity, define a rule. The rule should be clear, consistent, and justified by the task requirements.

For example, in a sentiment classification task, how do you label sarcasm? How do you label mixed sentiment? How do you label sentiment directed at multiple entities? In an entity extraction task, how do you handle nested entities? Do you label the inner entity, the outer entity, or both? How do you handle ambiguous boundaries? If an entity mention spans a clause, do you include the clause or just the noun phrase? In a summarization task, how much detail should the summary include? Should it be abstractive or extractive? Should it preserve specific numbers and dates, or is it acceptable to generalize?

Write your conventions as if-then rules. If the input contains sarcasm, label the literal sentiment, not the intended sentiment. If an entity is nested, label only the outermost entity. If a summary prompt requests a brief summary, limit the output to 100 words or fewer. Provide concrete examples for each rule. Show the input, show the correct annotation, and show common incorrect annotations with explanations of why they are wrong. The more examples you provide, the easier it is for annotators to internalize the rules.

Your conventions also need to cover style and tone. If you are training a model to generate customer support responses, what tone should the responses have? Formal or conversational? Empathetic or neutral? Should they use contractions? Should they use the customer's name? Should they include apologies, acknowledgments, or reassurances? If you are training a model to write technical documentation, what level of detail is appropriate? Should explanations assume beginner, intermediate, or expert knowledge? Should code examples be included, and if so, in what format?

## Edge Case Handling and Escalation

No matter how comprehensive your spec is, annotators will encounter examples that do not fit the rules. Your spec must define an escalation process. When an annotator encounters an example they cannot label with confidence, they flag it for review. The review process has three tiers. Tier one is peer review: another annotator reviews the example and either confirms the original annotation or suggests a correction. If the peers disagree, the example escalates to tier two: a senior annotator or domain expert reviews it and makes a binding decision. If the senior annotator determines that the spec does not cover this case, the example escalates to tier three: the spec owner adds a new rule to the spec, and the example is re-annotated according to the new rule.

Track your escalations. If you see a high volume of escalations on a particular type of example, that is a signal that your spec is incomplete or unclear. Update the spec to address the gap. If you see frequent disagreements between annotators on the same type of example, that is a signal that your rules are ambiguous or that annotators need additional training. The escalation log is also a valuable source of test cases for quality assurance. Examples that required escalation are often the hardest examples, and they are the ones you should use to validate model performance.

Define what to do with examples that are unlabelable. Some examples are simply bad data: corrupted text, irrelevant content, duplicates, or examples that do not match the task. Your spec should define exclusion criteria. If an example is shorter than a threshold, exclude it. If an example is in the wrong language, exclude it. If an example is off-topic, exclude it. Make exclusion decisions consistent by defining clear rules and by having a reviewer audit a sample of excluded examples to ensure the rules are being followed correctly.

## Quality Criteria and Acceptance Thresholds

Your spec must define what constitutes a high-quality annotation. Quality has several dimensions: correctness, completeness, consistency, and adherence to style guidelines. Correctness means the annotation is factually accurate and follows the rules in the spec. Completeness means all required fields are filled and no required information is missing. Consistency means the annotation matches the conventions and is similar to other annotations for similar examples. Adherence to style guidelines means the tone, format, and structure match the specified requirements.

You measure quality by sampling and review. Define a sampling plan: for every batch of N annotations, a reviewer audits M examples, where M is large enough to detect quality issues with statistical confidence. If the error rate in the sample exceeds a threshold, the entire batch is rejected and sent back for rework. If the error rate is below the threshold, the batch is accepted. The threshold depends on the task and the cost of errors. For a high-stakes task like medical coding, you might set the threshold at 2% errors. For a lower-stakes task like product categorization, you might accept 5% errors.

Track quality metrics over time. Compute inter-annotator agreement, error rates, and consistency scores for each annotator and for each batch. Identify annotators who consistently perform below the threshold and provide additional training or remove them from the project. Identify batches with unusually high error rates and investigate whether there is a systematic issue: a misunderstanding of the spec, a problem with the source data, or a tooling issue.

Define a gold standard dataset: a set of examples that have been annotated by experts and reviewed to be definitively correct. Use the gold standard to onboard new annotators, to test inter-annotator agreement, and to validate that your model is learning the right patterns. The gold standard should cover the full range of difficulty: easy examples, hard examples, edge cases, and adversarial examples. It should be large enough to be statistically meaningful but small enough to be manually curated with care. A gold standard of 200 to 500 examples is typical.

## Tooling, Workflow, and Annotation Platform Requirements

Your spec should define what tools annotators use and how the workflow is structured. If you are using a commercial annotation platform, specify which platform, which features are enabled, and how annotators access it. If you are using custom tooling, provide setup instructions, screenshots, and troubleshooting guidance. Define the workflow step by step: how annotators receive assignments, how they submit completed work, how they flag issues, and how they communicate with reviewers.

Specify the expected throughput and timeline. How many examples should an annotator complete per hour? What is the total timeline for the project? What are the milestones and deliverables? If you are working with an external annotation vendor, these expectations are part of your contract. If you are working with internal annotators, these expectations are part of your project plan. Unrealistic throughput expectations lead to rushed work and low quality. Realistic expectations allow annotators to work carefully and ask questions when they are uncertain.

Define how feedback is provided. When a reviewer finds an error, how is that communicated to the annotator? Is there a comment system in the tooling? Is there a weekly feedback session? Annotators need timely, specific feedback to improve. Generic feedback like "please be more careful" is not helpful. Specific feedback like "you incorrectly labeled sarcasm in three examples; remember that we label literal sentiment, not intended sentiment" is actionable.

Specify how disputes are resolved. If an annotator disagrees with a reviewer's feedback, what is the process? Can they challenge the decision? Who makes the final call? A clear dispute resolution process prevents frustration and ensures that feedback is perceived as fair. It also improves the spec: disputes often reveal ambiguities or inconsistencies that need to be addressed.

## Sign-Off Process and Spec Versioning

The spec is not useful if no one reads it or if it is ignored after the project starts. You need a formal sign-off process. Before annotation begins, the spec is reviewed by all stakeholders: the project lead, the engineering lead, the domain expert, the annotation vendor or internal annotation team, and the legal or compliance team if applicable. Each stakeholder reviews the spec for completeness, clarity, and feasibility. Each stakeholder signs off, indicating that they agree to the spec and that they will hold their team accountable to it.

Sign-off creates accountability. If an annotation vendor delivers data that does not match the spec, you can reject it and demand rework without additional cost, because the vendor agreed to the spec. If an engineer complains that the data is not in the right format, you can point to the schema that they signed off on. If a domain expert later claims that the annotations are wrong, you can refer back to the conventions that they approved. The spec is a binding agreement, and sign-off is what makes it binding.

Specs evolve. As annotation proceeds, you will discover edge cases, ambiguities, and errors in the spec. When you update the spec, you version it. Each version is dated and numbered. Each version includes a changelog that documents what was added, removed, or changed. Annotators are notified of spec updates and are required to re-read the updated sections. If a spec change is significant—for example, if it changes the labeling rules for a common type of example—you may need to re-annotate previously completed examples to ensure consistency.

Maintain a spec repository. Store the spec in a version-controlled system like Git. Track who made changes and when. Include the gold standard dataset in the repository. Include example annotations and example errors. Make the repository accessible to all stakeholders. The spec is a living document, and treating it as code—with versioning, review, and continuous improvement—ensures that it remains accurate and useful throughout the project.

## Common Spec Failures and How to Prevent Them

The most common spec failure is vagueness. A spec that says "label entities" without defining what counts as an entity is useless. A spec that says "write a helpful response" without defining what helpful means is useless. Every subjective term must be defined. Every ambiguous instruction must be clarified. If a sentence in your spec could be interpreted in two different ways, rewrite it so that only one interpretation is possible.

The second most common failure is lack of examples. Abstract rules are hard to apply. Concrete examples make rules actionable. For every rule in your spec, include at least two examples: one that follows the rule and one that violates it. For complex rules, include five or ten examples covering different variations. Examples are the most valuable part of your spec. Annotators learn more from examples than from abstract descriptions.

The third most common failure is ignoring annotator feedback. Annotators are the ones who use the spec every day. If they report that a rule is unclear, believe them. If they report that a task is impossible, investigate. If they report that the tooling is broken, fix it. Ignoring annotator feedback leads to low morale, high turnover, and poor data quality. Treating annotators as partners in the process leads to better data and faster iteration.

The fourth most common failure is treating the spec as a one-time deliverable. You write it, you sign off, and you never look at it again. This is a mistake. The spec should be reviewed and updated continuously as you learn from the annotation process. Schedule weekly spec review meetings during the annotation phase. Discuss escalations, discuss error patterns, and discuss proposed updates. Treat the spec as a product, not a document. It needs maintenance, iteration, and investment.

## Real-World Spec Examples and Lessons Learned

A financial services company built a spec for annotating earnings call transcripts with sentiment labels. Their first version was two pages long and defined sentiment as positive, negative, or neutral. Inter-annotator agreement was 68%. They expanded the spec to twelve pages, added definitions for each sentiment category, included examples of edge cases like mixed sentiment and forward-looking statements, and defined rules for handling uncertainty and sarcasm. Inter-annotator agreement increased to 91%, and the model trained on the new data achieved 14 percentage points higher F1 score than the model trained on the original data.

A healthcare company built a spec for extracting medication information from clinical notes. Their first version defined a flat schema with medication name, dose, and frequency as fields. Annotators struggled with cases where multiple medications were mentioned in a single sentence, where dosage information was implied rather than explicit, and where frequency was described in natural language rather than in structured form. The team revised the spec to define a nested schema where each medication mention was a separate record with optional fields for dose, frequency, route, and indication. They added rules for handling negation, temporality, and uncertainty. They added 40 example annotations covering common and rare cases. The revised spec reduced annotation time by 30% and improved model recall by 18 percentage points.

A customer support company built a spec for writing example responses to customer inquiries. Their first version said "write a helpful, professional response." Response quality was inconsistent: some were terse, some were verbose, some were formal, some were casual. They revised the spec to define response length targets, tone guidelines, required elements such as acknowledgment and resolution, prohibited elements such as technical jargon and negative language, and formatting rules such as paragraph breaks and bullet points. They provided ten example responses for common inquiry types. Response consistency improved dramatically, and the model trained on the revised data generated responses that required 40% fewer edits from human reviewers.

The lesson from these examples is the same: a vague spec produces inconsistent data, and inconsistent data produces poor models. A detailed, example-rich spec produces consistent data, and consistent data produces models that work. The investment in writing a good spec pays for itself many times over in reduced rework, faster annotation, and better model performance. The next subchapter covers data poisoning and backdoor risks, the threats that can compromise your model even when your spec is perfect and your data is high quality.
