# Chapter 7 — Evaluation Gating and Safety for Fine-Tuned Models

Fine-tuned models require stricter evaluation than base models because fine-tuning introduces new failure modes that base models do not have. Fine-tuning can degrade safety and alignment, introduce bias amplification, cause catastrophic forgetting where the model loses critical capabilities, and embed backdoors if data is poisoned. A base model released without gating may cause harm; a fine-tuned model released without gating is guaranteed to have unknown failure modes. Every production fine-tuned model must pass an evaluation gate.

This chapter describes how to build evaluation infrastructure that catches these problems before release. You will learn how to capture pre-fine-tuning baselines so you can measure what changed, architect eval suites with core task layers, domain-specific layers, safety layers, and regression layers, detect and measure catastrophic forgetting, identify where fine-tuning weakened alignment and refusal behavior, test jailbreak resistance and bias amplification, and run A/B tests between fine-tuned and base models with statistical rigor. The chapter covers the release gate checklist that must pass before any fine-tuned model reaches users, red teaming strategies specific to fine-tuned models, continuous monitoring in production, and post-deployment incident response when problems are discovered in the wild.

---

- 7.1 — Why Fine-Tuned Models Need Stricter Evaluation Than Base Models
- 7.2 — Pre-Training Baseline: Capturing Base Model Performance Before You Touch It
- 7.3 — The Eval Suite Architecture: Core, Domain, Safety, and Regression Layers
- 7.4 — Catastrophic Forgetting: Detection, Measurement, and Mitigation
- 7.5 — Safety Degradation: How Fine-Tuning Weakens Alignment and Refusal Behavior
- 7.6 — Jailbreak Resistance Testing After Fine-Tuning
- 7.7 — Bias Amplification: Measuring Whether Fine-Tuning Made Bias Worse
- 7.8 — A/B Testing Fine-Tuned vs Base: Experiment Design and Statistical Rigor
- 7.9 — Regression Detection: Automated Checks Against Golden Sets
- 7.10 — The Release Gate Checklist: What Must Pass Before Deployment
- 7.11 — Red Teaming Fine-Tuned Models: Targeted Adversarial Testing
- 7.12 — Continuous Evaluation: Monitoring Fine-Tuned Model Quality Post-Deployment
- 7.13 — Eval Infrastructure for Fine-Tuning Teams: Tooling and Automation
- 7.14 — Backdoor Detection and Trigger Testing
- 7.15 — Memorization and Data Extraction Testing
- 7.16 — Capability Regression Matrix: Format, Tool Use, Reasoning, and Multilingual Stability

---

*A fine-tuned model that ships without gating is a liability waiting to be discovered. The eval suite is your last line of defense.*
