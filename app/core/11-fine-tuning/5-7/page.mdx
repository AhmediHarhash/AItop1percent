# 5.7 â€” GPU Selection and Cluster Sizing: A100, H100, H200, and Cloud Pricing

**GPU selection is a cost optimization problem with a performance constraint, not a performance optimization problem.** Most teams get this backward. They default to the newest, fastest hardware because they assume performance improvements justify premium pricing. A research-focused startup demonstrated the cost of this assumption in mid-2025 when they spent forty-seven thousand dollars on eight H100 instances to fine-tune a 13B parameter model for specialized medical reasoning. Training completed in eleven hours. Three weeks later, a colleague ran the identical workload on A100 80GB instances for internal comparison. Training took nineteen hours and cost eighty-four hundred dollars. The team paid five and a half times more for a speedup that shaved eight hours off a non-urgent research experiment. Faster hardware commands exponential price premiums, and those premiums only justify themselves when training time directly blocks revenue, user launches, or compliance deadlines. If your training job can wait overnight, paying for cutting-edge silicon wastes money on speed you do not need.

This chapter provides the decision framework you need to select GPUs and size clusters for fine-tuning in 2026. You will learn how to map model size and technique to minimum VRAM requirements, compare the cost-performance profiles of A100, H100, and H200 hardware, evaluate cloud provider pricing across on-demand, reserved, and spot instance models, and determine cluster size based on training scale, urgency, and budget. By the end, you will know when to choose the cheaper previous-generation hardware, when to pay for cutting-edge silicon, and how to avoid the default trap of overspending on compute that does not deliver proportional business value.

## The Three GPU Generations That Matter in 2026

Three NVIDIA GPU architectures dominate fine-tuning infrastructure as of early 2026: the A100, the H100, and the H200. The A100, released in 2020 and refreshed in 2022 with an 80GB VRAM variant, remains the workhorse of enterprise fine-tuning. It offers 80GB of memory in its high-memory configuration, supports mixed-precision training, and delivers reliable performance for models up to 13B parameters with LoRA and up to 7B parameters with full fine-tuning. Cloud providers still maintain large A100 inventories, and the hardware has reached commodity pricing: on-demand A100 80GB instances cost between two dollars fifty cents and three dollars per hour across AWS, Google Cloud, and Azure in early 2026, with reserved instances dropping to one dollar sixty cents per hour for one-year commitments and spot instances occasionally dipping below one dollar per hour in low-demand regions.

The H100, released in late 2022 and widely available by mid-2023, delivers roughly twice the training throughput of the A100 for transformer workloads, with 80GB VRAM in the standard configuration and support for fourth-generation NVLink for faster inter-GPU communication. The performance gain is real and measurable: the same 7B full fine-tuning job that takes six hours on an A100 completes in three hours on an H100. The cost, however, is not linear. On-demand H100 instances range from seven dollars to nine dollars per hour in early 2026, nearly three times the A100 on-demand rate. Reserved instances drop to four dollars fifty cents per hour, and spot pricing fluctuates between two dollars fifty cents and four dollars per hour depending on region and demand. The H100 makes economic sense when training time directly impacts revenue or when model scale pushes you past the memory limits of the A100, but for most LoRA fine-tuning workloads on models under 13B parameters, the cost premium does not justify the speedup.

The H200, announced in late 2024 and entering general availability in early 2026, extends the H100 architecture with 141GB of HBM3e memory, delivering both higher capacity and marginally faster bandwidth. The H200 is purpose-built for the next generation of models: 70B parameter fine-tuning with LoRA, 34B parameter full fine-tuning, and multi-task training runs that require holding multiple checkpoints in memory simultaneously. Cloud pricing is still stabilizing, but early on-demand rates range from twelve dollars to fifteen dollars per hour, with reserved pricing not yet widely available. The H200 is not a general-purpose choice in 2026; it is a specialized tool for memory-constrained workloads, and unless your training job requires more than 80GB of VRAM, you are paying for capacity you will not use.

The strategic insight here is that GPU selection is not about choosing the newest hardware. It is about matching memory requirements and throughput needs to the most cost-effective hardware tier that satisfies both constraints. Most fine-tuning workloads in 2026 fit comfortably on A100s. The H100 is a performance upgrade for time-sensitive work. The H200 is a memory upgrade for large-scale models. Treating them as a linear progression from good to better to best is the mistake the medical startup made, and it cost them thirty-eight thousand dollars in unnecessary compute spend.

## VRAM Requirements by Model Size and Technique

The first constraint in GPU selection is memory. Every parameter in a neural network requires storage, and during training, you also store gradients, optimizer states, and activation checkpoints. The memory footprint scales predictably with model size and training technique, and understanding these scaling laws allows you to determine the minimum viable GPU configuration before you provision a single instance.

For LoRA fine-tuning, the memory requirement is dominated by the base model weights and the adapter weights. A 7B parameter model in 16-bit precision requires 14GB of VRAM just to load the weights. Add gradients for the adapter layers, optimizer states for AdamW, and activation checkpoints, and the working memory climbs to 22GB to 28GB depending on batch size and sequence length. An A100 40GB instance handles this workload comfortably. Scaling to a 13B parameter model pushes the memory requirement to 38GB to 48GB, requiring the A100 80GB variant. A 70B parameter model with LoRA requires 90GB to 110GB of working memory, which exceeds A100 capacity and forces you to either shard across multiple GPUs or move to the H200 with its 141GB capacity.

Full fine-tuning imposes a much larger memory burden. Every parameter now requires a gradient and an optimizer state, tripling the memory footprint relative to inference. A 7B parameter full fine-tune requires 60GB to 75GB of VRAM, pushing you immediately to the A100 80GB tier and often requiring gradient checkpointing to stay within limits. A 13B parameter full fine-tune requires 120GB to 150GB, which cannot fit on a single A100 and requires either FSDP sharding across two A100s or a single H200. A 34B parameter full fine-tune requires 300GB to 400GB, demanding a multi-GPU cluster even with the H200.

Quantized training with QLoRA reduces memory requirements dramatically by loading the base model in 4-bit precision. A 13B parameter model that required 48GB for standard LoRA drops to 18GB with QLoRA, fitting comfortably on an A100 40GB instance. A 70B parameter model drops from 110GB to 38GB, fitting on an A100 80GB. QLoRA is the reason most teams can fine-tune large models without H200 clusters, and if your task tolerates the small quality tradeoff introduced by 4-bit quantization, QLoRA on A100 hardware is the most cost-effective configuration available in 2026.

The practical rule is this: calculate your memory requirement by model size, technique, and batch size before selecting hardware. If the requirement fits within A100 80GB capacity, do not pay the H100 premium unless training time is genuinely time-sensitive. If the requirement exceeds 80GB, evaluate whether QLoRA brings you back under the limit before jumping to H200s or multi-GPU clusters. Memory constraints are hard boundaries; performance is a tradeoff. Optimize for the hard boundary first.

## Cloud Pricing Comparison: AWS, Google Cloud, Azure, and CoreWeave

Cloud providers offer the same underlying NVIDIA hardware at different price points, and the delta between providers can reach 25% for identical instance types. The pricing structure also varies by commitment model: on-demand instances offer maximum flexibility at maximum cost, reserved instances lock you into one-year or three-year terms for discounts of 35% to 50%, and spot instances offer steep discounts of 50% to 80% in exchange for interruptibility. Understanding how to navigate this landscape is the difference between spending $15,000 on a fine-tuning campaign and spending $40,000 for the same result.

AWS offers A100 80GB instances as p4d.24xlarge, priced at $32.77 per hour on-demand in US East in early 2026. Each instance includes eight A100 GPUs, so the per-GPU cost is $4.10 per hour. Reserved instances drop to $21.50 per hour for one-year all-upfront commitments, or $2.69 per GPU per hour. Spot instances fluctuate between $9 and $18 per hour depending on availability, translating to $1.12 to $2.25 per GPU per hour. AWS also offers H100 instances as p5.48xlarge at $98.32 per hour on-demand, or $12.29 per GPU per hour. Reserved pricing is not yet widely available for H100s. Spot pricing for H100s ranges from $35 to $60 per hour in regions with availability.

Google Cloud offers A100 80GB instances as a2-ultragpu-8g at $27.94 per hour on-demand, or $3.49 per GPU per hour. Committed-use discounts bring the rate down to $18.20 per hour for one-year terms, or $2.27 per GPU per hour. Google Cloud does not offer traditional spot instances but provides preemptible instances at $8.38 per hour, or $1.05 per GPU per hour, with automatic termination after 24 hours. H100 instances are priced at $82.50 per hour on-demand, or $10.31 per GPU per hour, with committed-use discounts dropping to $53.62 per hour, or $6.70 per GPU per hour. Google Cloud has more aggressive pricing than AWS on H100 reserved capacity, making it the preferred choice for teams committing to multi-month H100 training campaigns.

Azure offers A100 80GB instances as NDm_A100_v4 at $28.56 per hour on-demand, or $3.57 per GPU per hour. Reserved instances drop to $19.10 per hour for one-year terms, or $2.39 per GPU per hour. Azure does not offer spot instances for GPU workloads but provides low-priority instances with similar economics, priced at $8.56 per hour or $1.07 per GPU per hour. H100 instances are priced at $88.00 per hour on-demand, or $11.00 per GPU per hour. Azure reserved pricing for H100s is not yet competitive with Google Cloud as of early 2026.

CoreWeave, a GPU-specialized cloud provider, offers the most aggressive pricing for teams willing to tolerate a less mature platform. A100 80GB instances are priced at $2.21 per GPU per hour on-demand, roughly 30% cheaper than AWS. Reserved instances drop to $1.62 per GPU per hour for one-year terms. H100 instances are priced at $5.89 per GPU per hour on-demand, nearly half the AWS rate. CoreWeave also offers H200 instances at $8.50 per GPU per hour on-demand, the lowest rate in the market in early 2026. The tradeoff is reduced geographic coverage, less mature orchestration tooling, and higher operational overhead for teams without dedicated infrastructure engineers.

The decision framework is straightforward. For one-off experiments or exploratory fine-tuning, use spot or preemptible instances on the cheapest provider that meets your latency and compliance requirements. For production fine-tuning pipelines with predictable monthly volume, commit to reserved instances on Google Cloud for H100 workloads or AWS for A100 workloads, depending on your existing cloud footprint. For cost-sensitive research teams with engineering capacity, CoreWeave offers the best raw price per GPU hour. The worst choice is on-demand instances on AWS for long-running training jobs; you are paying a 60% premium over reserved pricing and a 120% premium over spot pricing for flexibility you likely do not need.

## On-Demand, Reserved, and Spot Instances: When to Use Each

The three pricing models represent different tradeoffs between cost, commitment, and availability. On-demand instances are the default: you pay the highest per-hour rate but can spin up and tear down instances at will with no long-term commitment. Reserved instances require upfront or monthly commitments for one to three years in exchange for 35% to 50% discounts. Spot instances offer 50% to 80% discounts but can be interrupted with two-minute notice when cloud providers need capacity for higher-paying workloads. Choosing the wrong model for your workload is the easiest way to double your training costs.

On-demand instances make sense in exactly three scenarios. First, exploratory fine-tuning where you do not yet know whether the task is viable and cannot justify a reserved commitment. Second, bursty workloads where training happens sporadically and reserved capacity would sit idle most of the month. Third, compliance-sensitive workloads where you cannot tolerate the interruption risk of spot instances and cannot predict volume accurately enough to reserve capacity. For all other scenarios, on-demand pricing is a planning failure. If you are running the same fine-tuning pipeline every week, you should have reserved capacity. If you are training models continuously for research, you should be on spot instances with checkpoint-and-resume logic.

Reserved instances are the correct choice for production fine-tuning pipelines with predictable volume. If you know you will train a new model version every two weeks and each training run takes eight hours on four A100 GPUs, you can calculate your monthly GPU-hour requirement and reserve exactly that capacity. The 40% discount over on-demand pricing pays for itself in the first month, and the commitment ensures availability during high-demand periods when spot instances are scarce. The risk is overcommitment: if your training volume drops or your model architecture changes in a way that requires different hardware, you are locked into paying for reserved capacity you no longer need. The mitigation is to reserve conservatively, covering 60% to 70% of expected usage with reserved instances and handling spikes with on-demand or spot capacity.

Spot instances are the optimal choice for research teams, large-scale hyperparameter sweeps, and any workload where you can tolerate interruptions and have implemented checkpoint-and-resume logic. The economics are compelling: a training job that costs $5,000 on on-demand instances drops to $1,200 on spot instances in favorable market conditions. The operational cost is that you must instrument your training scripts to save checkpoints every ten to fifteen minutes and automatically resume from the latest checkpoint when an instance is reclaimed. PyTorch and Hugging Face Transformers support this pattern natively, and most fine-tuning frameworks include spot-instance-friendly checkpoint managers.

The failure mode with spot instances is treating them like on-demand instances without implementing proper checkpointing. A team runs a 14-hour training job on spot instances, the instance is reclaimed after 11 hours, and the entire job restarts from scratch because the checkpoint interval was set to save only at epoch boundaries. The job completes after 22 hours of wall-clock time and costs more than on-demand would have because you paid for 11 wasted hours. The rule is simple: if you use spot instances, checkpoint every 10 to 15 minutes and test your resume logic before launching expensive jobs. If you cannot implement checkpointing, use reserved or on-demand instances.

## Cluster Sizing: Single GPU, Multi-GPU, and Multi-Node Training

The number of GPUs you provision for a training job determines both cost and training time, but the relationship is not linear. Adding a second GPU does not halve training time; communication overhead, gradient synchronization, and data loading bottlenecks introduce inefficiencies that reduce scaling efficiency. A job that takes eight hours on one GPU might take four and a half hours on two GPUs, three hours on four GPUs, and two hours on eight GPUs, with each doubling of GPU count delivering diminishing returns. Cluster sizing is about finding the sweet spot where additional GPUs still deliver meaningful speedup without wasting capacity on overhead.

Single-GPU training is the baseline for all LoRA fine-tuning on models up to 13B parameters and for QLoRA fine-tuning on models up to 70B parameters. If your workload fits in the memory of one A100 80GB instance and training completes in a reasonable timeframe, using multiple GPUs is pure waste. A six-hour training job on one A100 at $2.50 per hour costs $15. The same job on four A100s might complete in two hours but costs $40 because you are paying for four GPUs. You spent $25 to save four hours of wall-clock time. If those four hours do not block a product launch, user-facing deployment, or compliance deadline, you optimized the wrong variable.

Multi-GPU training on a single node makes sense when memory constraints or training time genuinely justify the cost. Full fine-tuning a 13B parameter model requires 120GB to 150GB of working memory, which exceeds single-GPU capacity and forces you to shard across two or four A100s using FSDP or DeepSpeed. This is a memory-driven decision, not a performance optimization. Similarly, if you are fine-tuning a 7B model for a product launch in 48 hours and single-GPU training would take 36 hours, provisioning four GPUs to complete the job in 10 hours is justified because missing the launch window costs more than the compute premium. The key is that multi-GPU training must be solving a real constraint, not a hypothetical performance concern.

Multi-node training, where you scale across multiple machines each with eight GPUs, is reserved for frontier-scale fine-tuning: full fine-tuning on 70B parameter models, continued pretraining on domain-specific corpora, or research experiments exploring novel architectures. A multi-node cluster introduces network communication overhead, requires careful configuration of distributed training frameworks, and demands expertise in debugging hung jobs, straggler nodes, and gradient synchronization failures. The operational cost is high, and the scaling efficiency drops below 70% for most workloads, meaning 16 GPUs deliver less than twice the throughput of 8 GPUs. Multi-node training is not a default choice; it is a specialized capability for teams with specific large-scale requirements.

The practical sizing rule is to start with the minimum viable configuration, measure training time, and scale up only if the time cost justifies the dollar cost. For most production fine-tuning in 2026, that minimum viable configuration is one A100 80GB instance. For large models or time-sensitive work, it is four A100s on a single node. For frontier work, it is a multi-node cluster with dedicated infrastructure engineering support. Defaulting to the largest cluster you can afford is a mistake; defaulting to the smallest cluster that satisfies your constraints is professional discipline.

## Cost Modeling: Comparing Scenarios Across Hardware and Pricing Models

The only way to make rational GPU selection decisions is to model total cost across realistic scenarios before provisioning hardware. A 13B parameter LoRA fine-tuning job serves as a useful reference workload. On one A100 80GB instance with on-demand pricing at $3.00 per hour, training completes in 8 hours for a total cost of $24. On one A100 80GB instance with spot pricing at $1.20 per hour, the same job costs $9.60, assuming no interruptions. On four A100 80GB instances with on-demand pricing, training completes in 2.5 hours for a total cost of $30. On one H100 80GB instance with on-demand pricing at $8.00 per hour, training completes in 4 hours for a total cost of $32.

The spot instance on A100 is the clear winner for non-urgent work, delivering a 60% cost reduction over on-demand A100 and a 70% reduction over on-demand H100. The multi-GPU A100 configuration costs 25% more than single-GPU on-demand but saves 5.5 hours of wall-clock time, which is a reasonable tradeoff for time-sensitive work. The H100 costs 33% more than on-demand A100 and delivers a 2x speedup, which is a poor tradeoff unless you are willing to pay $8 per hour to save four hours. The H100 with spot pricing at $3.50 per hour changes the equation: training completes in 4 hours for $14, which is competitive with single-GPU on-demand A100 and delivers a 50% time reduction. The lesson is that H100 hardware only makes economic sense when paired with spot or reserved pricing.

For a 70B parameter QLoRA fine-tuning job, the memory requirement fits within one A100 80GB instance. Training takes 48 hours on one A100 at on-demand pricing for a total cost of $144. On spot pricing, the cost drops to $57.60. On one H100 with on-demand pricing, training completes in 24 hours for $192. On one H100 with spot pricing, training completes in 24 hours for $84. On one H200 with on-demand pricing at $12 per hour, training completes in 20 hours for $240. The H200 is the worst choice by a wide margin; it costs 67% more than A100 on-demand and delivers only a 17% speedup over H100. The H100 on spot pricing is the best choice, cutting training time in half relative to A100 while costing 46% more, a worthwhile tradeoff for a two-day reduction in delivery time.

For a 13B parameter full fine-tuning job requiring four A100 GPUs due to memory constraints, training takes 12 hours on four A100s at on-demand pricing for a total cost of $144. On reserved pricing at $2.00 per GPU per hour, the cost drops to $96. On spot pricing at $1.20 per GPU per hour, the cost drops to $57.60. On two H100s with on-demand pricing, training completes in 7 hours for $112. On two H100s with reserved pricing at $5.00 per GPU per hour, the cost drops to $70. The reserved A100 cluster is the most cost-effective option for repeated production workloads, while spot A100 is best for one-off experiments. The H100 cluster is only competitive when using reserved pricing and only makes sense when the five-hour time savings justifies the $12.40 cost premium.

The strategic takeaway is that GPU selection is not a hardware decision; it is a cost-time tradeoff decision. You model the total cost and total time for each viable configuration, identify which configurations satisfy your time constraint, and choose the cheapest option that meets the constraint. Teams that skip this modeling step consistently overspend by 2x to 5x on compute, paying for performance they do not need or choosing hardware that delivers marginal speedups at exponential cost premiums.

GPU selection and cluster sizing are cost optimization problems constrained by memory and time requirements. The A100 80GB remains the workhorse of fine-tuning in 2026, offering the best cost-performance ratio for models up to 13B parameters with LoRA and up to 7B parameters with full fine-tuning. The H100 delivers a 2x speedup at a 3x cost premium, justified only for time-sensitive work or when paired with reserved or spot pricing. The H200 is a memory upgrade for workloads exceeding 80GB VRAM, not a performance upgrade for general fine-tuning. Spot instances deliver 50% to 80% cost reductions for interruptible workloads, reserved instances deliver 35% to 50% reductions for predictable production pipelines, and on-demand instances are a fallback for exploratory work only. Multi-GPU training is justified by memory constraints or genuine time pressure, not by a desire to finish faster for its own sake. The discipline of cost modeling before provisioning hardware is what separates professional fine-tuning infrastructure from expensive trial-and-error experimentation.

The next challenge is scaling training across those multiple GPUs efficiently, which requires understanding data parallelism, FSDP, and DeepSpeed.