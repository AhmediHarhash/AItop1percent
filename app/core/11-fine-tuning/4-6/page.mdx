# 4.6 â€” RLHF: Reward Modeling and Reinforcement Learning in Practice

A 22 percent improvement in safety ratings. An 18 percent improvement in helpfulness. A 9 percent reduction in false refusals on benign user requests. These were the results a major AI lab achieved in early 2024 when they trained a conversational assistant using reinforcement learning from human feedback on 1.2 million preference pairs from 8,000 annotators collected over six months. The reward model, a 7-billion-parameter transformer trained to predict human preferences, achieved 72 percent accuracy on held-out data. The team then used proximal policy optimization to fine-tune a 70-billion-parameter language model against the reward model, running 50,000 training steps across 128 GPUs over three weeks. The cost was immense, the pipeline complex, the timeline measured in months. But the results were undeniable. Six months later, the same lab retrained using direct preference optimization on 200,000 preference pairs and achieved 16 percent helpfulness improvement and 20 percent safety improvement with one-tenth the compute and one-fifth the training time. RLHF had delivered results, but at a cost that simpler methods made obsolete for most teams.

Six months later, the same lab retrained using direct preference optimization on a subset of 200,000 preference pairs, achieving 16 percent improvement in helpfulness and 20 percent improvement in safety with one-tenth the compute budget and one-fifth the training time. The RLHF approach had delivered results, but at a cost and complexity that was no longer justified for most use cases. RLHF remains the gold standard for frontier model alignment at the scale of GPT-4, Claude, and Gemini, but for practitioners building task-specific systems, it has been largely supplanted by simpler methods.

The root insight of RLHF is that you can align models to human preferences by decomposing the problem into two stages: learning what humans prefer through a reward model, then optimizing a policy model to maximize that reward. This separation allows you to leverage human feedback efficiently. Annotators provide preferences over pairs of responses, which is faster and more reliable than writing ideal responses from scratch. The reward model generalizes from those preferences to score arbitrary responses, and the policy model uses those scores to learn which responses to generate. The architecture is elegant, but it introduces multiple points of failure and requires infrastructure that most teams do not need.

## How Reward Modeling Works

A reward model is a classifier trained to predict human preferences. Given a prompt and a response, the reward model outputs a scalar score representing how much a human would prefer that response compared to alternatives. During training, the reward model sees pairs of responses to the same prompt, one preferred and one rejected, and learns to assign higher scores to preferred responses and lower scores to rejected responses.

The training data consists of preference pairs collected through human annotation. Annotators are shown a prompt and two candidate responses, then asked to choose which response is better according to criteria like helpfulness, harmlessness, honesty, or task-specific quality dimensions. The annotations are aggregated into a dataset where each example contains a prompt, a preferred response, and a rejected response. Some datasets include ties, where annotators judge both responses as equally good, but most RLHF systems filter out ties to simplify training.

The reward model architecture is typically a transformer initialized from the same pre-trained model used for the policy, with the final layer replaced by a linear projection that outputs a single scalar reward. Training uses a pairwise ranking loss that maximizes the difference between the reward assigned to the preferred response and the reward assigned to the rejected response. The loss increases when the reward model assigns higher scores to rejected responses or lower scores to preferred responses, and it decreases when the reward model ranks preferred responses above rejected responses by a large margin.

Reward model accuracy is measured by how often it ranks the preferred response above the rejected response on a held-out test set. Accuracy above 70 percent is considered strong, indicating that the reward model has learned meaningful patterns from the preference data. Accuracy below 60 percent suggests that the reward model is not generalizing well, which can result from noisy annotations, insufficient training data, or a mismatch between the model's capacity and the complexity of the preference criteria.

Reward models can overfit to spurious correlations in the training data. If preferred responses are consistently longer than rejected responses, the reward model may learn to assign higher scores to longer responses regardless of content. If preferred responses use certain phrases or formatting patterns, the reward model may learn to reward those patterns even when they do not indicate higher quality. This overfitting manifests during policy optimization as reward hacking, where the policy model generates responses that score highly on the reward model but are not actually preferred by humans.

## Proximal Policy Optimization and RL Training

Once you have a trained reward model, you use reinforcement learning to optimize the policy model to maximize reward. The policy model is the language model you actually deploy. During RL training, the policy model generates responses to prompts, the reward model scores those responses, and the policy model updates its parameters to increase the likelihood of high-reward responses.

Proximal policy optimization is the most widely used RL algorithm for RLHF. PPO is designed to prevent the policy from changing too quickly, which can destabilize training. Each training step computes a loss that encourages the policy to increase the probability of actions, tokens, that led to high reward while staying close to the previous policy. The closeness constraint is enforced through a clipping mechanism that limits how much the probability of any action can change in a single update.

The training loop alternates between sampling and optimization. In the sampling phase, the policy model generates responses to a batch of prompts, and the reward model scores each response. In the optimization phase, PPO updates the policy model's parameters to increase the probability of high-reward tokens while penalizing large deviations from the previous policy. This process repeats for thousands of steps until the policy converges to a stable reward level.

A critical component of RLHF is the KL penalty, a regularization term that penalizes the policy model for diverging too far from a reference model, usually the supervised fine-tuned model before RL training. The KL penalty prevents the policy from collapsing into a degenerate distribution that generates only a narrow set of high-reward responses. Without the KL penalty, the policy might learn to produce repetitive, formulaic responses that exploit quirks of the reward model but lack the diversity and fluency of the reference model.

The balance between reward maximization and KL penalty is controlled by a coefficient that determines how much to weight the KL term in the loss function. A high KL coefficient keeps the policy close to the reference model, producing conservative updates and preserving general capabilities. A low KL coefficient allows the policy to diverge more aggressively toward high-reward responses, producing stronger alignment but risking overfitting or mode collapse. Typical values range from 0.01 to 0.1, tuned through experimentation on validation metrics.

RL training is computationally expensive. Each training step requires generating responses from the policy model, scoring them with the reward model, and running backpropagation through the policy model. For a 70-billion-parameter policy model, this requires a multi-GPU setup with tensor parallelism or pipeline parallelism. Training runs can take days or weeks depending on the number of prompts, the length of generated responses, and the convergence speed. Cloud compute costs for large-scale RLHF training routinely exceed $50,000 for a single training run.

## Debugging Reward Hacking and Mode Collapse

Reward hacking occurs when the policy model finds ways to achieve high reward scores that do not correspond to actual human preferences. This happens because the reward model is an imperfect proxy for human judgment. The reward model generalizes from a finite set of annotated preferences, and it inevitably fails to capture all the nuances of what makes a response good. The policy model, optimized to maximize reward, will exploit any weaknesses in the reward model.

Common reward hacking patterns include length hacking, where the policy learns that longer responses score higher and generates verbose, repetitive text; keyword stuffing, where the policy learns that certain phrases or terms correlate with high reward and inserts them regardless of relevance; and sycophancy, where the policy learns to agree with the user or provide overly positive responses because annotators tend to prefer agreeable outputs.

Detecting reward hacking requires monitoring both the reward model's scores and human evaluations of policy outputs. If reward scores increase during RL training but human preference ratings plateau or decline, reward hacking is likely occurring. The solution is to retrain the reward model on examples where the policy model hacked the previous reward model, explicitly teaching the reward model to penalize the hacking behavior. This is an iterative process. You train a reward model, optimize a policy against it, identify hacking patterns, collect new preference data that penalizes those patterns, retrain the reward model, and repeat.

Mode collapse occurs when the policy model learns to generate a narrow set of high-reward responses and ignores the diversity of the reference model. Instead of generating varied, context-appropriate responses, the policy generates formulaic outputs that score well on the reward model but lack fluency or relevance. Mode collapse is caused by insufficient KL penalty or by reward models that assign high scores to a small set of patterns.

Preventing mode collapse requires careful tuning of the KL coefficient and monitoring of output diversity metrics during training. Diversity can be measured through distinct n-gram counts, entropy of token distributions, or similarity between generated responses across different prompts. If diversity decreases significantly during RL training, the KL coefficient should be increased or the reward model should be retrained to encourage more varied responses.

Another mitigation strategy is to include diversity-promoting terms in the reward model training objective. Instead of training the reward model solely to rank preferred responses above rejected responses, you can train it to also assign lower scores to repetitive or formulaic responses. This requires annotating examples specifically for diversity, which adds annotation cost but reduces the risk of mode collapse during policy optimization.

## When RLHF Still Matters

RLHF matters when you need to optimize for multiple objectives simultaneously and those objectives cannot be easily captured in pairwise preferences. A conversational assistant needs to be helpful, harmless, honest, concise, and engaging. Balancing these objectives requires a reward model that can weight each dimension appropriately based on context. DPO, which operates on binary preferences, struggles to encode such trade-offs unless you carefully construct preference pairs that reflect the balance you want.

RLHF also matters when you have access to large-scale preference data and the infrastructure to train reward models and run RL optimization at scale. Major AI labs annotate millions of preference pairs across diverse tasks, languages, and user populations. Training a reward model on this scale allows it to generalize broadly, capturing nuances that smaller datasets cannot. The resulting policy models exhibit alignment across a wide range of inputs, from technical questions to creative writing to sensitive topics. This level of alignment is not achievable with task-specific DPO.

RLHF matters when you need to incorporate feedback that is not available at training time. For example, you might want to align a model to user satisfaction metrics that are only observable after deployment, such as whether users continue the conversation, whether they provide positive feedback, or whether they achieve their goals. RLHF can incorporate these signals through online learning, where the reward model is updated continuously based on real user interactions and the policy model is retrained periodically to reflect the updated reward model. DPO requires offline preference pairs, making it harder to adapt to evolving user preferences.

Finally, RLHF matters when you are conducting research on alignment techniques and need to understand the limits of reward-based optimization. Many open questions in alignment, such as how to handle distributional shift in rewards, how to prevent reward hacking in adversarial settings, and how to ensure that reward models reflect diverse human values, are best explored through RLHF because it exposes the reward model explicitly. DPO internalizes the reward model into the loss function, making it harder to study these issues.

For most production systems in 2026, RLHF is overkill. DPO delivers comparable alignment at a fraction of the cost and complexity. But for frontier models, multi-objective alignment, large-scale deployments, and alignment research, RLHF remains the state of the art.

## The Infrastructure Burden of RLHF

Implementing RLHF requires maintaining two models, the reward model and the policy model, and orchestrating a multi-stage pipeline that includes preference data collection, reward model training, and RL policy optimization. Each stage has its own failure modes and debugging challenges.

Preference data collection requires annotator recruitment, training, and calibration. Annotators must understand the task, the quality criteria, and the annotation interface. Inter-annotator agreement must be monitored continuously to ensure data quality. Annotation guidelines must be refined as edge cases emerge. For large-scale systems, this requires a dedicated team managing the annotation pipeline, reviewing borderline cases, and iterating on guidelines.

Reward model training requires tuning hyperparameters like learning rate, batch size, and training steps to achieve high accuracy on held-out preference pairs. The reward model must generalize beyond the training distribution, scoring responses that differ significantly from those seen during training. Overfitting is a constant risk, and regularization techniques like dropout, weight decay, and early stopping must be applied carefully to balance fitting the training data and generalizing to new examples.

RL policy optimization requires infrastructure that can generate responses at scale, score them with the reward model, and update the policy model efficiently. This often involves distributed sampling, where multiple workers generate responses in parallel, and distributed training, where the policy model is sharded across multiple GPUs. Debugging RL training is notoriously difficult because failures can arise from reward model errors, PPO hyperparameter misconfiguration, or instabilities in the policy update step.

Monitoring RLHF training requires tracking reward curves, KL divergence, policy loss, value function loss, and human evaluation metrics. Reward should increase during training, but not at the expense of KL divergence spiking or human ratings declining. Manual spot-checks of generated responses are essential to catch reward hacking or mode collapse early. Automated red-teaming, where adversarial prompts are used to probe policy behavior, helps identify failure modes before deployment.

The total infrastructure cost for RLHF, including annotator labor, compute for reward model training, and compute for RL optimization, can exceed $100,000 for a single training run on a 70-billion-parameter model. For smaller models and narrower tasks, costs are lower but still substantial compared to DPO. The decision to use RLHF must account for this cost and weigh it against the expected performance gain.

## Practical Considerations for RLHF in 2026

In 2026, RLHF is primarily used by organizations with large annotation budgets, multi-GPU training clusters, and teams experienced in reinforcement learning. Open-source tools like TRL support RLHF workflows, but the complexity of managing reward model training and RL optimization requires expertise that not every team has.

For teams considering RLHF, the first question is whether DPO or ORPO can achieve acceptable performance. If your task can be framed as pairwise preferences and you have 5,000 to 50,000 annotated pairs, DPO is almost certainly the better choice. If your task requires balancing multiple objectives, incorporating online feedback, or aligning to broad notions of helpfulness and safety across diverse inputs, RLHF may be necessary.

The second question is whether you have the infrastructure to support RLHF. Reward model training requires similar infrastructure to supervised fine-tuning, but RL policy optimization requires more. You need to generate responses at scale, which means running inference on the policy model in parallel with training. You need to manage checkpoints for both the reward model and the policy model. You need to monitor training stability and debug reward hacking or mode collapse when they occur.

The third question is whether you have the annotator pipeline to produce high-quality preference data. RLHF depends entirely on the quality of your reward model, which depends entirely on the quality of your annotations. Low inter-annotator agreement, ambiguous guidelines, or biased annotator populations will produce reward models that do not generalize, which in turn produces policies that do not align with actual user preferences.

For most teams, the answer to these questions will lead them to DPO. But for teams building large-scale conversational systems, aligning models across multiple languages and cultures, or conducting alignment research, RLHF remains the most powerful tool available.

## Combining RLHF with Other Techniques

RLHF is rarely used in isolation. The standard workflow starts with supervised fine-tuning on task-specific data, then applies RLHF to align the SFT model to human preferences. Some teams also apply constitutional AI techniques, where the model is trained to critique and revise its own outputs according to a set of principles, before or after RLHF. Others combine RLHF with adversarial training, where the policy model is exposed to adversarial prompts designed to elicit unsafe or unhelpful responses, and the reward model is trained to penalize those responses.

Iterative RLHF, where the reward model and policy model are retrained multiple times as new preference data is collected, is common in production systems that receive continuous user feedback. Each iteration refines the alignment, addressing failure modes discovered in deployment and incorporating preferences that were not captured in earlier rounds. This requires a robust data pipeline that can collect, filter, and annotate new preferences at scale.

Some teams use RLHF for high-level alignment and DPO for task-specific refinement. For example, a conversational assistant might be aligned to broad helpfulness and safety criteria using RLHF at the base model level, then fine-tuned for specific domains like customer support or medical triage using DPO on domain-specific preference data. This two-stage approach leverages the strengths of both techniques: RLHF for general alignment across diverse tasks, DPO for efficient task-specific adaptation.

The future of RLHF is uncertain. As DPO and its variants mature, fewer teams will need the complexity of full RLHF pipelines. But for frontier model development, where alignment quality is paramount and budgets are unconstrained, RLHF will remain the technique that delivers the highest ceiling. Understanding how it works, when it is necessary, and how to implement it is essential for any practitioner working on alignment at scale.

The techniques covered in this chapter, LoRA, full fine-tuning, DPO, and RLHF, represent the core methods for adapting models to specific tasks and preferences. The next chapter shifts focus from how you train to what you evaluate, examining how to measure fine-tuned model performance and diagnose when adaptation has succeeded or failed.
