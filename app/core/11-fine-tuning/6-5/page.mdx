# 6.5 â€” Code Domain: Language-Specific Models, Repository Context, and Test Generation

In August 2024, an enterprise software company fine-tuned a code generation model on their internal Java codebase. The goal was to accelerate development by generating boilerplate code, API integrations, and utility functions. The team collected three years of commit history from their monorepo, filtered out test files and documentation, and trained the model on two hundred thousand Java files. After six weeks of training and validation, they deployed the model as an IDE plugin for their engineering team of one hundred twenty developers. Within three weeks, the system had been used to generate over four thousand code snippets that were committed to production.

The problems appeared two months later during a security audit. The audit team discovered that the fine-tuned model had learned to generate database query patterns that were vulnerable to SQL injection. The original codebase had contained these vulnerabilities in legacy modules written before the company adopted prepared statements as a standard. The model had learned the old pattern as valid Java code and was now reproducing it in new features. The security team identified forty-seven instances of generated code with injection vulnerabilities across twelve microservices. The remediation effort required two sprints, involved twenty-three engineers, and delayed a major product release by six weeks. The CTO shut down the code generation system and commissioned an external review of the entire fine-tuning process.

The root cause was training on unvetted historical code. The team had assumed that code merged to the main branch was correct and safe. That assumption is false. Production codebases accumulate technical debt, security vulnerabilities, and anti-patterns over time. Code review catches some issues but not all. Legacy code written before modern security standards remains in the codebase until someone refactors it. When you fine-tune a model on the entire history of a codebase, you are training it to reproduce not just good patterns but also every bad pattern that ever made it past code review. Fine-tuning for code generation is not about teaching the model to write code. It is about teaching the model to write the specific kind of code your organization considers correct, safe, and maintainable right now, not five years ago.

## Language-Specific Fine-Tuning Versus General Code Models

The first decision in code domain fine-tuning is whether to fine-tune at all. General-purpose code models like GPT-4o, Claude 3.5 Sonnet, and Codex are trained on billions of lines of open-source code across dozens of languages. They can generate syntactically correct code in most mainstream languages with no additional training. For many use cases, a well-prompted base model is sufficient and safer than a fine-tuned model that has learned your codebase's bad habits.

Fine-tuning makes sense when you need the model to generate code that conforms to organization-specific patterns that are not well-represented in public code. Examples include internal frameworks, company-specific APIs, proprietary data models, and architectural conventions. If your backend services all follow a specific layering pattern with controllers, services, repositories, and DTOs, and you want the model to generate new endpoints that follow that pattern automatically, fine-tuning can teach the model your conventions. A base model prompted to generate a new REST endpoint will produce generic code. A fine-tuned model will produce code structured like your existing endpoints, using your naming conventions, your error handling patterns, and your logging framework.

Language-specific fine-tuning is valuable when you work in a language with limited public training data. Popular languages like Python, JavaScript, and Java are well-represented in base models. Niche languages like Elixir, Clojure, or domain-specific languages used in finance or scientific computing have less public code available. Fine-tuning on a high-quality corpus of code in a less common language can substantially improve generation quality. The key is ensuring the training corpus represents best practices, not just any code that happens to be written in that language.

The trade-off is maintenance burden. A base model improves over time as the vendor releases new versions. A fine-tuned model is frozen at the point of training unless you retrain it. If you fine-tune on GPT-4 in early 2025 and GPT-4.5 is released six months later with better code reasoning capabilities, you do not automatically benefit. You must decide whether to retrain on the new base model, which requires re-validating the fine-tuned outputs and potentially re-curating the training data if the new model has different context window or token behavior. For many organizations, the maintenance cost of fine-tuning exceeds the value, and a well-engineered prompting strategy on the latest base model delivers better long-term results.

## Repository-Aware Fine-Tuning

The most powerful application of code fine-tuning is repository-aware generation, where the model learns to understand your codebase's structure and can generate code that integrates correctly with existing modules. A base model can write a function in isolation, but it cannot generate a function that correctly imports your utility libraries, uses your custom error types, and follows your project's logging patterns unless those patterns are explained in the prompt. A repository-aware model has seen thousands of examples of how your codebase fits together and can generate code that feels like it was written by a developer who has been on the team for years.

Repository-aware fine-tuning requires training data that includes cross-file context. A single function in isolation is not a useful training example because it does not show how the function interacts with the rest of the system. A better training example includes the function, its imports, the types it depends on, and the test that exercises it. The model learns not just what the function does but how it fits into the module structure.

One effective approach is to construct training examples from commit diffs. Each example includes the state of the relevant files before the commit and the changes introduced by the commit. The model learns to predict the diff given the before state. This teaches the model how developers extend the codebase: what files they modify together, what patterns they follow when adding new features, and how they maintain consistency with existing code. A commit that adds a new API endpoint typically includes changes to the controller, the service layer, the repository, and the tests. Training on these multi-file commits teaches the model that adding an endpoint is a coordinated change across layers.

Another approach is to include repository-level documentation in the training data. If your codebase has architecture decision records, style guides, or onboarding documentation, those documents provide explicit statements of the conventions the model should learn. A training example might pair a style guide rule with multiple code examples that follow the rule. The model learns not just that the examples are valid code, but that they are valid because they conform to a documented principle.

Repository-aware fine-tuning also enables higher-quality code search and navigation. You can fine-tune the model to generate natural language descriptions of code given the code itself, and to generate code given natural language queries. This creates a semantic search layer over your codebase where developers can ask "show me how we handle rate limiting" and get examples from the codebase rather than generic internet examples.

## Training Data Curation for Code Fine-Tuning

The quality of code fine-tuning depends entirely on the quality of the training data. Unlike text fine-tuning where bad examples mostly just reduce fluency, bad code examples teach the model to write bad code. You must curate the training set to include only code that you want the model to reproduce.

The first filter is correctness. The training set should include only code that is known to work. This means excluding experimental branches, uncommitted code, and code from developers who are learning the codebase. One approach is to filter by test coverage: only include code that is covered by passing tests. This ensures that the training examples are not just syntactically valid but also behaviorally correct.

The second filter is security. The training set should exclude code with known vulnerabilities. This requires running static analysis tools over the candidate training data and removing files with high-severity findings. Tools like Semgrep, SonarQube, and CodeQL can identify patterns like SQL injection, cross-site scripting, insecure deserialization, and hard-coded secrets. Any file flagged by these tools should be excluded from training unless the vulnerability has been verified as a false positive.

The third filter is modernity. Code written five years ago may follow patterns that your team no longer uses. If your team has migrated from class-based React components to function components with hooks, you should exclude class-based examples from the training set. If you have deprecated a legacy API and migrated to a new one, you should exclude examples that use the old API. The model should learn current best practices, not historical practices.

You should also balance the training set by code type. A codebase is not uniformly valuable for training. Some files are simple configuration files with little logic. Some files are complex algorithms that are rarely modified. Some files are frequently edited as the team iterates on core features. Over-representing configuration files teaches the model to generate boilerplate. Over-representing complex algorithms teaches the model to reproduce specific implementations rather than general patterns. The ideal training set includes a representative sample of the code types developers actually write day-to-day: API handlers, database queries, data transformations, validation logic, and tests.

Negative examples can be valuable if used carefully. You can include examples of code review comments where a reviewer flagged an issue and the author fixed it. The training example shows the original code, the reviewer's comment, and the corrected code. The model learns what mistakes to avoid and how to fix them. This requires annotating code review data, which is labor-intensive but can improve the model's ability to generate code that passes review on the first try.

## Test Generation Fine-Tuning

Test generation is one of the most practical applications of code fine-tuning. Writing tests is time-consuming and often neglected. A model that can generate high-coverage tests from production code can substantially improve code quality and developer productivity. The challenge is that test generation requires understanding not just what the code does, but what edge cases and failure modes need to be tested.

Fine-tuning for test generation requires training data that pairs production code with high-quality tests. The training examples should include the function or class being tested, the test suite, and ideally any bugs that were discovered and fixed later with additional tests. This teaches the model that good tests cover not just the happy path but also error conditions, boundary cases, and integration points.

One effective training pattern is regression test generation. When a bug is found in production, a developer writes a test that reproduces the bug, then fixes the code so the test passes. These regression tests are high-value training examples because they demonstrate real failure modes that were not anticipated during initial development. Training on regression tests teaches the model to think adversarially: what are the inputs or conditions that could cause this code to fail?

Test generation fine-tuning should also include examples of different testing patterns. Unit tests, integration tests, and end-to-end tests have different structures and different coverage goals. Unit tests mock dependencies and test functions in isolation. Integration tests use real implementations of dependencies and test interactions between components. End-to-end tests exercise the entire system through the user interface or API. A well-tuned test generation model should be able to generate all three types and should understand when each type is appropriate.

Evaluation of test generation models requires execution-based metrics. You cannot evaluate a generated test by reading it and deciding if it looks reasonable. You must run it and verify that it executes, that it achieves high code coverage, and that it catches bugs when you introduce them. A standard evaluation approach is mutation testing: you modify the production code to introduce a deliberate bug, then check whether the generated tests fail. If the tests still pass after the mutation, they are not testing the mutated behavior, which means they have a coverage gap.

Another valuable metric is assertion quality. A test that executes the code but has no assertions is worthless. A test that asserts only that the function does not throw an error is minimally useful. A test that asserts on the specific output given specific input is more valuable. A test that asserts on side effects like database changes or API calls is even better. You can measure assertion quality by counting assertions per test and by checking whether assertions cover the function's return value, state changes, and interactions with dependencies.

## Code Review and Bug Detection Fine-Tuning

Fine-tuning models to assist with code review is a high-leverage application because it scales expertise. Senior engineers know what to look for in code review: potential bugs, performance issues, security vulnerabilities, and deviations from team conventions. Junior engineers are still learning these patterns. A model fine-tuned on your team's code review history can provide feedback that helps junior engineers improve their code before senior engineers spend time reviewing it.

Training data for code review fine-tuning comes from your code review tools. GitHub pull requests, GitLab merge requests, and similar platforms record the conversation between authors and reviewers. Each review comment is a training example: the code being reviewed, the reviewer's comment, and the author's response or code change. The model learns to identify the patterns that trigger review comments.

The training set should focus on substantive review comments, not stylistic ones. A comment that says "this variable should be camel case" is enforcing a style rule that a linter should handle. A comment that says "this function does not handle the case where the input list is empty" is identifying a logic bug. The model should learn to identify logic bugs, edge cases, race conditions, resource leaks, and architectural mismatches, not just style violations.

You can improve training quality by filtering for comments from experienced reviewers. Not all review comments are equally valuable. A comment from a senior engineer who has been on the team for five years is more likely to represent best practices than a comment from a new hire. You can weight training examples by reviewer experience, giving more weight to comments from reviewers with a strong track record.

Bug detection fine-tuning is a specialized form of code review tuning focused on identifying defects. Training data comes from bug reports linked to the code that caused the bug. Each example includes the buggy code, the bug description, the test that reproduced the bug, and the fix. The model learns to recognize code patterns that are statistically associated with bugs in your codebase.

One powerful approach is to fine-tune on historical bugs and then evaluate the model by giving it buggy code from new bugs before they are fixed. If the model can flag the buggy code section as suspicious, it is learning useful patterns. If it cannot, the training data may not include enough similar examples or the bugs are too project-specific to generalize.

## Evaluation with Execution-Based Metrics

Evaluating code generation models requires running the code, not just reading it. A function that looks correct may have subtle bugs that only appear with specific inputs or under concurrency. A test that looks comprehensive may have low coverage or weak assertions. Execution-based evaluation is the only way to measure whether generated code actually works.

The simplest execution metric is syntactic correctness. Does the generated code parse without errors? This is a low bar, but failing it means the model is producing malformed output. You run the generated code through the language's parser and count the percentage that parses successfully. A production code model should achieve near one hundred percent on this metric.

The next metric is executable correctness. Does the generated code run without runtime errors? You execute the code in a sandbox with typical inputs and check whether it completes successfully or throws exceptions. This catches issues like undefined variables, type mismatches, and null pointer dereferences. A model that generates syntactically correct code that crashes on execution is not useful.

Functional correctness is harder to measure. Does the generated code produce the correct output for a given input? This requires having a specification of what the correct output is, which in practice comes from test cases. If the task is to generate a function that sorts a list, you evaluate by calling the function with test inputs and checking whether the output is sorted. If the task is to generate code that implements a feature described in a specification, you evaluate by running the acceptance tests for that feature.

Code coverage is a valuable metric for generated tests. You run the generated tests against the production code and measure what percentage of lines, branches, and functions are exercised. Higher coverage is better, but one hundred percent coverage does not guarantee the tests are useful. A test can execute every line but have no assertions, achieving full coverage while detecting no bugs.

Mutation testing measures test effectiveness by introducing bugs and checking whether tests catch them. You modify the production code in systematic ways: flip boolean conditions, change arithmetic operators, remove statements, swap variables. For each mutation, you run the generated tests. If the tests fail, they caught the mutation. If the tests still pass, they missed it. The mutation score is the percentage of mutations caught. A high mutation score indicates the tests are actually verifying behavior, not just achieving coverage.

You should also measure integration correctness. Generated code must work with the rest of the codebase. You evaluate this by integrating the generated code into a real project, running the project's full test suite, and checking whether the tests still pass. If the generated code breaks existing tests, it has introduced a regression or violated an interface contract.

Finally, measure review acceptance rate. Deploy the model in a setting where developers use generated code but also review and edit it. Track what percentage of generated code is accepted as-is, what percentage is edited before use, and what percentage is rejected entirely. High edit and rejection rates indicate the model is not generating code that meets developer standards.

## Language-Specific Challenges and Tokenization

Different programming languages have different characteristics that affect fine-tuning effectiveness. Statically typed languages like Java, C++, and Rust have explicit type annotations that provide strong signal about code intent. Dynamically typed languages like Python and JavaScript have less explicit signal, making it harder for the model to infer types and catch type-related bugs. Fine-tuning on statically typed languages often produces better results because the training data contains more structured information.

Languages with significant whitespace like Python require the model to learn correct indentation. A Python function with incorrect indentation is syntactically invalid. The model must reproduce indentation exactly, which is challenging because tokenization often treats whitespace inconsistently. Fine-tuning for Python requires training data with consistent indentation style (spaces versus tabs, two spaces versus four) and evaluation that penalizes indentation errors as severely as syntax errors.

Languages with heavy use of macros or metaprogramming like Lisp, Elixir, or C++ templates are harder to fine-tune because the training data includes code that is transformed before execution. The model sees the pre-transformation code but must understand the post-transformation behavior. This requires either including the macro definitions in the training context or training on the expanded code, both of which increase complexity.

Tokenization affects fine-tuning efficiency. Most language models use tokenizers designed for natural language, which are suboptimal for code. Code has many repeated sequences like variable names, function names, and common patterns. A tokenizer that treats each occurrence of a variable name as a separate token forces the model to learn the variable name from scratch every time. A code-specific tokenizer that recognizes variable names as semantic units reduces the token count and improves learning efficiency. Some models like CodeBERT and GraphCodeBERT use tokenizers trained on code, which improves fine-tuning effectiveness.

## The Repository Context Window Problem

The most significant limitation of code fine-tuning is the context window. Real software engineering tasks require understanding large amounts of code. To generate a new feature, a developer might need to read dozens of files across multiple modules. To debug an issue, they might need to trace execution through several layers of abstraction. The context window of even the largest language models is too small to hold an entire codebase.

Fine-tuning partially addresses this by compressing common patterns into the model's weights. If the model has seen thousands of examples of how your team structures API endpoints, it does not need to see all existing endpoints in the context to generate a new one that follows the same pattern. The pattern is embedded in the model. This is the primary value of fine-tuning for code: it acts as lossy compression of repository conventions.

However, fine-tuning cannot replace actual context for tasks that require understanding specific business logic. If you are generating a function that integrates with a third-party API, the model needs to see the API client library, the authentication logic, and the data models. Fine-tuning on other API integrations in your codebase helps, but it does not give the model knowledge of this specific API. You still need to provide the relevant files in context.

The solution is a hybrid approach: fine-tune to learn general patterns and conventions, then use retrieval-augmented generation at inference time to inject specific context. When a developer asks the model to generate code for a new feature, the system retrieves the most relevant existing files using semantic search, includes them in the prompt, and generates the new code. The fine-tuning ensures the generated code follows team conventions. The retrieved context ensures it integrates correctly with the specific modules involved.

Building this hybrid system requires infrastructure. You need a vector database of your codebase, updated with every commit. You need a retrieval strategy that balances relevance and diversity: retrieving ten variations of the same pattern is less useful than retrieving examples of different patterns that might be relevant. You need a ranking mechanism that prioritizes recently modified code over legacy code, and frequently used modules over rarely touched ones.

## When Not to Fine-Tune for Code

Fine-tuning for code is expensive and risky. For many use cases, you should not do it. Base models are already very capable at code generation, and the risk of learning bad patterns from your codebase often outweighs the benefit of learning good patterns.

Do not fine-tune if your codebase has low test coverage. Without tests, you cannot verify that the training data represents correct code. You will train the model on code that appears to work but may have latent bugs.

Do not fine-tune if your codebase has significant security issues. The model will learn to reproduce those issues. Fix the security issues first, then consider fine-tuning.

Do not fine-tune if your team's coding standards are inconsistent. The model will learn the inconsistency and generate code that randomly follows different styles. Establish and enforce standards with linters and formatters first.

Do not fine-tune if you cannot commit to ongoing retraining. Code models must be retrained as your codebase evolves. If you train once and deploy for years, the model becomes increasingly outdated and generates code that uses deprecated patterns.

Do not fine-tune if the use case is exploratory prototyping. Developers writing experimental code benefit from creativity and variety, not conformity to existing patterns. A base model is better for prototyping. Fine-tuning is better for production code that must integrate with existing systems.

The right approach for most teams is to start with a base model, invest in high-quality prompting and retrieval, and only consider fine-tuning after you have exhausted the capabilities of well-engineered prompts. Many teams find that a base model with good retrieval and prompt engineering delivers ninety percent of the value at ten percent of the cost.

The next domain presents a different challenge: multilingual adaptation, where the model must learn not just different languages but different cultural contexts, and where low-resource languages require transfer learning from high-resource ones.
