# 3.3 â€” Synthetic Data Pipeline Design: Prompt, Generate, Filter, Validate

In mid-2025, a legal technology company launched a fine-tuned model to analyze contract clauses for compliance risks. The team generated 40,000 synthetic training examples over two weeks using GPT-4o as their teacher model. They wrote a single master prompt, ran batch inference, and fed all outputs directly into fine-tuning without further review. The resulting model launched in August 2025 and immediately exhibited bizarre behavior. It flagged standard limitation-of-liability clauses as high-risk, ignored obvious indemnification gaps, and generated explanations that contradicted basic contract law. The head of product pulled the model after three days. Post-mortem analysis revealed the root cause: eleven percent of their synthetic dataset contained factually incorrect legal reasoning, eight percent consisted of repetitive template responses that taught the model to pattern-match rather than analyze, and five percent were off-topic responses where the teacher model had misunderstood the prompt. No filtering pipeline existed. No validation stage caught these errors. The team had treated synthetic data generation as a one-step batch job rather than a multi-stage quality-controlled pipeline. They spent the next two months rebuilding their entire dataset with proper pipeline architecture.

Synthetic data generation is not prompt-and-done. It is a four-stage pipeline with distinct quality gates at each transition. The pipeline moves from prompt design, through generation, into filtering, and ends with validation. Each stage serves a specific purpose. Each stage catches different failure modes. Teams that collapse these stages into a single batch operation produce contaminated datasets that train unreliable models. Teams that build proper pipelines generate clean, representative datasets that improve model performance predictably and measurably.

## Stage One: Prompt Design for Distribution Coverage

The first stage of your synthetic data pipeline is prompt design. Your goal is not to write one perfect prompt. Your goal is to create a diverse, representative set of prompts that cover the full distribution of inputs your model will encounter in production. This stage determines whether your synthetic dataset teaches your model to handle real-world variation or merely memorize a narrow pattern.

Start by analyzing your task distribution. If you are fine-tuning a customer support model, your production traffic includes questions about account access, billing disputes, feature requests, bug reports, policy clarifications, and edge cases like account deletion or data export requests. Your prompt set must cover all these categories in proportions that match production. If thirty percent of your support tickets concern billing, thirty percent of your synthetic prompts should target billing scenarios. If five percent concern data privacy requests, five percent of your prompts should address privacy. Teams that write prompts based on what feels important rather than what production data shows create distribution mismatch. Your fine-tuned model becomes excellent at scenarios you over-represented and poor at scenarios you under-represented.

Decompose your task into coverage dimensions. For a contract analysis model, dimensions include contract type, clause category, jurisdiction, industry vertical, and risk level. For a code review model, dimensions include programming language, code complexity, error type, and review focus area. For a content moderation model, dimensions include content type, policy category, severity level, and context factors like user history or community norms. Each dimension requires systematic coverage. You write prompt templates that explicitly target each combination of dimension values. A contract analysis prompt template might specify: generate an analysis of a force majeure clause in a software licensing agreement governed by Delaware law, with moderate risk factors. A code review prompt might specify: review a Python function with nested loops and external API calls, focusing on performance and error handling.

Use prompt templates with controlled variation. A template is not a fixed string. It is a structure with variable slots that you fill systematically to create diverse instances. For a customer support dataset, your template might be: customer question about topic-slot in tone-slot, with context-slot. You fill topic-slot with values like billing dispute, feature request, account access. You fill tone-slot with frustrated, neutral, confused. You fill context-slot with first-time user, long-time customer, enterprise account. Combinatorial expansion of these slots generates hundreds of distinct prompts from a single template. This approach guarantees coverage without manual prompt writing for every instance.

Balance specificity and generality in your prompts. Highly specific prompts produce consistent, predictable outputs but teach narrow patterns. A prompt like analyze the indemnification clause in this SaaS agreement produces focused responses but does not teach the model to handle variation in clause structure, agreement type, or analysis depth. Highly general prompts produce diverse outputs but risk off-topic or vague responses. A prompt like analyze this contract gives the teacher model too much latitude, resulting in outputs that wander across multiple topics or provide surface-level analysis. The optimal prompt is specific enough to constrain the task but general enough to allow natural variation in reasoning and explanation style.

Include edge cases and adversarial scenarios in your prompt set. Production systems encounter inputs that fall outside normal distributions. Customer support receives questions phrased as angry rants, written in broken English, or combining multiple unrelated issues in one message. Contract analysis systems receive non-standard clause structures, cross-jurisdictional agreements, or documents with missing sections. Code review systems receive deliberately obfuscated code, legacy code with no documentation, or code mixing multiple languages. Your synthetic dataset must include these edge cases. Reserve ten to fifteen percent of your prompt set for adversarial and edge-case scenarios. This prevents your fine-tuned model from degrading to generic responses or refusals when it encounters unusual inputs.

Document your prompt design decisions. Create a prompt registry that maps each prompt template to its coverage targets, variation parameters, and expected output characteristics. This registry serves as your dataset blueprint. It allows other team members to understand why each prompt exists, what distribution gap it fills, and how to extend the prompt set if coverage gaps emerge later. Teams that skip documentation end up with opaque prompt collections that no one can maintain or expand without reverse-engineering the original intent.

Test your prompt set on a small scale before full generation. Generate 100 examples from your prompt templates and manually review them for distribution coverage, output quality, and alignment with task requirements. This pilot phase catches prompt design errors early, when they are cheap to fix. If you discover that certain prompt templates produce off-topic outputs or that important task dimensions are missing, you revise the prompt set before spending thousands of dollars on full-scale generation.

## Stage Two: Generation at Scale with Teacher Inference

The second stage is generation. You run your teacher model against your prompt set to produce synthetic outputs at scale. This stage is not just batch inference. It is controlled inference with specific settings that balance output quality, diversity, and cost.

Choose generation settings that match your quality requirements. Temperature controls output randomness. For tasks requiring factual accuracy and consistency, use low temperature between 0.3 and 0.5. This setting produces focused, deterministic outputs with minimal hallucination risk. For tasks requiring creative variation or diverse reasoning paths, use moderate temperature between 0.7 and 0.9. This setting produces varied outputs while maintaining coherence. For tasks where you want maximum diversity to explore the space of possible responses, use high temperature between 0.9 and 1.0, but expect higher filtering loss in later stages. Teams that use default temperature across all tasks generate outputs that are either too repetitive or too noisy for their specific use case.

Set appropriate max tokens based on your task complexity. Customer support responses typically require 150 to 400 tokens. Contract analysis outputs require 400 to 800 tokens for clause-level analysis, more for full document reviews. Code review explanations require 300 to 600 tokens depending on code complexity. Setting max tokens too low truncates reasoning and produces incomplete outputs that teach your model to stop mid-thought. Setting max tokens too high wastes inference cost and generates verbose outputs that teach your model to pad responses with filler content. Analyze your production output length distribution and set max tokens to the 90th percentile of that distribution.

Run generation in batches with cost tracking. Synthetic data generation for fine-tuning often requires tens of thousands of inference calls against expensive teacher models. A dataset of 50,000 examples generated with GPT-5 at current 2026 pricing costs approximately $1,500 to $3,000 depending on average output length. Batch API access reduces costs by thirty to fifty percent compared to synchronous API calls. Set up batch jobs with appropriate rate limits, retry logic for transient failures, and progress tracking. Log every inference call with its cost, latency, and token usage. This logging allows you to optimize batch sizes, identify expensive outlier prompts, and project total dataset costs accurately.

Implement diversity controls during generation. If you generate multiple outputs from the same prompt, you want variation in reasoning approach and explanation style, not just surface wording changes. Use sampling methods that encourage diversity: top-p sampling with p between 0.9 and 0.95 produces varied outputs while avoiding low-probability tokens that introduce errors. Adjust frequency and presence penalties to discourage repetitive phrasing. For some tasks, generate multiple outputs per prompt and select the most diverse subset in the filtering stage. This approach is expensive but produces higher-quality training signal when model variety matters more than volume.

Monitor generation quality in real-time. Do not wait until you have generated all 50,000 examples to discover that your prompts produce off-topic outputs. Sample and review outputs every 1,000 to 2,000 examples during generation. Check for common failure modes: refusals where the teacher model declines to answer, off-topic responses where the model misunderstood the prompt, template responses where the model produces generic filler, and factual errors where the model hallucinates incorrect information. If you detect systematic issues, pause generation, revise your prompts, and restart. Catching prompt issues at 5,000 examples wastes far less cost than discovering them at 50,000 examples.

Store generation metadata with every output. Each synthetic example should include the prompt used, the teacher model version, generation timestamp, temperature and sampling settings, token counts, and generation cost. This metadata enables debugging when downstream model quality issues emerge. If your fine-tuned model underperforms on a specific task category, you can trace back to the prompts and generation settings used for that category and diagnose whether the issue stems from prompt design, generation settings, or filtering criteria.

Handle generation failures gracefully. API calls fail due to rate limits, timeouts, or service disruptions. Build retry logic with exponential backoff and dead-letter queues for persistent failures. Track the failure rate per prompt category. If certain prompts consistently fail or timeout, they may be triggering safety filters or exceeding context length limits. Investigate and revise those prompts rather than silently dropping failed examples, which creates coverage gaps in your dataset.

## Stage Three: Filtering for Quality Gates

The third stage is filtering. Not all generated outputs belong in your training dataset. Filtering removes low-quality outputs that would teach your model bad patterns, incorrect information, or inconsistent behavior. This stage is where pipeline design separates teams that produce excellent fine-tuned models from teams that produce mediocre ones.

Implement automated format compliance checks first. These checks catch outputs that violate basic structural requirements. For a customer support model, check that responses are appropriately formatted, do not contain placeholder text, and fall within expected length ranges. For a contract analysis model, check that outputs include the required analysis sections, cite specific clause text, and provide risk assessments in the expected format. For a code review model, check that outputs reference specific line numbers or code patterns and provide concrete recommendations. Format compliance filters are fast, cheap, and catch fifteen to twenty-five percent of quality issues without human review.

Add content quality filters based on automatic metrics. Length-based filtering removes outputs that are too short to provide useful signal or too long to indicate unfocused rambling. Set minimum length thresholds based on task complexity: customer support responses below 50 tokens are usually too terse, contract analysis below 150 tokens is too superficial, code reviews below 100 tokens lack sufficient detail. Set maximum length thresholds to catch verbose outputs: responses exceeding three times your median output length are often repetitive or unfocused. Repetition detection filters catch outputs that repeat the same phrase or sentence structure excessively. Compute n-gram overlap within each output. If tri-gram repetition exceeds fifteen percent or any five-gram appears more than twice, the output likely contains template-style repetition that teaches poor generation habits.

Use self-consistency filtering for factual tasks. Generate multiple outputs for the same prompt using different sampling seeds. Compare the outputs for consistency in key facts, conclusions, or recommendations. For a contract risk analysis task, if three generated outputs classify the same clause as low-risk and two classify it as high-risk, flag the inconsistency for review. For a medical question-answering task, if generated outputs contradict each other on treatment recommendations or diagnostic criteria, exclude all outputs for that prompt from the dataset. Self-consistency filtering is expensive because it requires multiple inference calls per prompt, but it catches hallucinations and factual errors that other filters miss. Use this technique selectively for high-stakes domains like legal, medical, or financial applications.

Deploy reward model scoring when available. If you have trained or have access to a reward model for your task domain, run all generated outputs through the reward model and filter based on score thresholds. A reward model trained on human preferences for customer support quality can score synthetic support responses and filter out outputs below the 60th percentile. A reward model for code quality can score synthetic code reviews and remove low-scoring examples. Reward model filtering is particularly effective when combined with other filters: an output must pass format compliance, length checks, and reward model scoring to enter the dataset. Teams using reward model filtering report ten to twenty percent improvement in downstream fine-tuned model quality compared to datasets filtered only by rule-based checks.

Implement classifier-based filtering for policy compliance and safety. If your task involves content that must adhere to specific policies, use classifier models to detect violations. A content moderation dataset should not include synthetic examples that violate your own content policies. A customer support dataset should not include responses that make unauthorized commitments or share incorrect policy information. A medical advice dataset should not include responses that contradict clinical guidelines. Train or fine-tune classifiers to detect these violations and automatically exclude flagged outputs from your training dataset. Policy compliance filtering prevents your fine-tuned model from learning behaviors that create liability or trust issues in production.

Apply domain-specific filters tailored to your task. For legal text generation, filter outputs that cite non-existent case law or misstate legal principles. For medical text generation, filter outputs that recommend treatments inconsistent with evidence-based guidelines. For code generation, filter outputs with syntax errors or security vulnerabilities. These domain-specific filters require expertise to design but dramatically improve dataset quality. Collaborate with domain experts to define filtering rules that catch subtle errors automated systems alone would miss.

Track filtering statistics by category. For every filter you implement, log the rejection rate and the distribution of rejection reasons. If your length filter rejects thirty percent of outputs, investigate whether your generation prompts or max token settings need adjustment. If your self-consistency filter rejects forty percent of outputs for a specific prompt category, that category may require prompt redesign or a different teacher model. Filtering is not just a quality gate; it is a diagnostic signal about your prompt design and generation settings. Teams that monitor filtering statistics iteratively improve their earlier pipeline stages to reduce filtering loss and increase yield.

Build filtering as a multi-stage cascade. Apply cheap filters first to eliminate obvious failures, then apply expensive filters to the remaining candidates. Format compliance and length checks cost fractions of a cent per example. Reward model scoring costs a few cents per example. Self-consistency filtering costs as much as generation itself. Running all filters on all examples wastes money. A properly designed cascade might eliminate sixty percent of outputs with format and length checks, then apply reward model scoring to the remaining forty percent, then apply self-consistency filtering to the top twenty percent. This reduces total filtering cost by an order of magnitude while maintaining quality.

## Stage Four: Validation Through Human Review

The fourth stage is validation. Automated filtering catches many quality issues, but human review catches the subtle errors, edge cases, and context-dependent problems that automated systems miss. Validation is your final quality gate before fine-tuning.

Design a sampling strategy that provides statistical confidence. You cannot manually review all 50,000 filtered outputs, but you can review a representative sample that gives you confidence in overall dataset quality. For datasets under 10,000 examples, review ten to fifteen percent. For datasets between 10,000 and 50,000 examples, review five to ten percent. For datasets above 50,000 examples, review three to five percent, ensuring stratified sampling across all prompt categories and output types. Stratified sampling ensures you review examples from every part of your distribution, not just the most common cases. Random sampling of 2,000 examples from a 40,000-example dataset gives you statistical confidence with a margin of error around two percent at 95 percent confidence level.

Create clear rejection criteria for human reviewers. Reviewers need specific, actionable definitions of what constitutes a quality issue. For a customer support dataset, rejection criteria include: factually incorrect information about product features or policies, inappropriate tone for the scenario, failure to address the customer's actual question, recommendations that violate company policy, or responses that would escalate rather than resolve the issue. For a contract analysis dataset, rejection criteria include: misidentification of clause type, incorrect legal reasoning, failure to cite relevant clause text, risk assessment inconsistent with clause content, or analysis that ignores jurisdiction-specific considerations. Document these criteria in a review guide with examples of acceptable and unacceptable outputs.

Measure inter-annotator agreement to ensure consistent review. Have multiple reviewers independently review the same sample of 200 to 300 examples. Calculate agreement rates on accept versus reject decisions. If agreement is below eighty percent, your rejection criteria are too ambiguous or your reviewers need additional training. Investigate disagreements to understand their sources. Some disagreements reflect genuinely borderline cases where reasonable reviewers differ. Other disagreements reflect unclear criteria or insufficient reviewer training. Refine your review guide based on disagreement patterns and re-measure agreement until you achieve consistent standards.

Use validation feedback to improve earlier stages. When human reviewers reject examples that passed automated filtering, analyze why automated filters missed these issues. If reviewers frequently reject outputs for subtle factual errors, your self-consistency filtering may need stricter thresholds or you may need domain-specific fact-checking filters. If reviewers reject outputs for tone issues, you may need to add tone classifiers to your automated filtering. If reviewers reject outputs from specific prompt categories at high rates, those prompts need redesign. Validation is not just a final check; it is a feedback loop that improves your entire pipeline.

Track validation rejection rates as a pipeline health metric. In a well-designed pipeline, human validation should reject five to fifteen percent of outputs that passed automated filtering. If validation rejects fewer than five percent, your automated filtering may be over-fitted to validation criteria and you risk filtering out valuable training variation. If validation rejects more than fifteen percent, your automated filtering is too lenient and you are wasting human review time on examples that should have been caught earlier. Adjust your automated filter thresholds to target the optimal validation rejection rate for your quality requirements and review budget.

Document validation decisions and create an error taxonomy. When reviewers reject examples, they should categorize the rejection reason: factual error, format violation, tone issue, off-topic response, policy violation, or other. Aggregate these categories to understand your most common quality issues. If twenty percent of rejections stem from factual errors in a specific domain, you may need a different teacher model or more constrained prompts for that domain. If fifteen percent stem from tone issues, you may need explicit tone instructions in your generation prompts. An error taxonomy transforms validation from a pass-fail gate into a diagnostic tool that drives continuous pipeline improvement.

Build validation tooling that accelerates reviewer throughput. A simple web interface that displays examples with accept or reject buttons and reason codes can increase reviewer speed by three to five times compared to reviewing raw files. Include features like highlighting key fields, side-by-side comparison with reference examples, and keyboard shortcuts for common actions. Faster validation means you can review larger samples or afford multiple review rounds within your timeline and budget.

Consider expert validation for specialized domains. Legal, medical, and financial tasks require domain expertise to validate correctly. A general reviewer may not catch legal reasoning errors or medical guideline violations. Hire domain experts for validation, or train existing reviewers with domain-specific materials. Expert validation costs more per hour but catches errors that would otherwise reach production and cause real harm.

## Pipeline Orchestration and Cost Management

Synthetic data generation at scale requires orchestration across all four stages. You are not running a single script. You are running a multi-stage data factory with quality gates, feedback loops, and cost constraints.

Build your pipeline with clear stage boundaries and intermediate storage. Stage one produces a prompt dataset stored in structured format with coverage metadata. Stage two consumes prompts and produces raw outputs stored with generation metadata. Stage three consumes raw outputs and produces filtered outputs stored with filter decision logs. Stage four consumes filtered outputs and produces validated training data stored with review annotations. Clear boundaries allow you to pause, debug, and restart any stage without rerunning earlier stages. Intermediate storage allows you to analyze pipeline yield at each transition and identify bottlenecks.

Implement cost tracking at every stage. Prompt design has minimal cost but significant time investment. Generation stage costs dominate: track inference costs per example, per prompt category, and per batch. Filtering stage costs depend on compute for automated checks and labor for human review. Validation stage costs are primarily human labor. Calculate cost per final validated example by dividing total pipeline costs by the number of examples that pass validation. This metric allows you to optimize for cost efficiency. If your cost per validated example is five dollars and you need 20,000 examples, your total budget is 100,000 dollars. If that exceeds your budget, you must either improve filtering yield to reduce waste, use a cheaper teacher model, or reduce dataset size.

Monitor throughput and optimize for bottlenecks. If generation takes two weeks but filtering takes three days and validation takes four days, generation is your bottleneck. Increase batch parallelism, use faster teacher models for less complex prompts, or reduce max tokens where appropriate. If filtering is your bottleneck, optimize filter code, move expensive filters later in the cascade so fewer examples reach them, or deploy filters on faster compute infrastructure. If validation is your bottleneck, improve automated filtering to reduce the validation sample size, hire additional reviewers, or build better review tooling to increase reviewer throughput.

Version your pipeline configuration and datasets. Every pipeline run should be versioned with the prompt set, generation settings, filter configurations, and validation criteria used. If you generate a dataset in February 2026 and fine-tune a model that underperforms, you need to reproduce the exact pipeline to diagnose issues. If you generate an improved dataset in April 2026 with refined prompts and stricter filtering, you need to track what changed between versions. Version control for data pipelines is as essential as version control for code.

Plan for iterative improvement. Your first pipeline run will not produce perfect data. You will discover prompt gaps, generation issues, and filter misconfigurations. Budget time for two to three iteration cycles. After your first pipeline run, analyze validation rejections, review filtering statistics, and examine generation metadata. Revise prompts, adjust generation settings, tune filter thresholds, and run a second iteration. Compare dataset quality metrics between iterations. Most teams achieve production-quality datasets by the third iteration, assuming they implement systematic feedback loops rather than guessing at improvements.

Automate as much of the pipeline as possible. Manual steps create bottlenecks, introduce errors, and make iteration slow. Use workflow orchestration tools like Airflow, Prefect, or Dagster to define pipeline stages, dependencies, and retry logic. Automation allows you to rerun the entire pipeline when teacher models update, prompts change, or quality requirements evolve. It also makes your pipeline auditable: every dataset has a complete execution history showing exactly how it was produced.

## The Difference Between Batch Jobs and Pipelines

Teams new to synthetic data generation treat it as a batch job: write prompts, run inference, dump outputs into fine-tuning. This approach works for experimental datasets of 1,000 examples. It fails catastrophically for production datasets of 20,000 to 100,000 examples. Batch jobs produce contaminated datasets where ten to thirty percent of examples teach incorrect patterns, off-topic behavior, or factual errors. Fine-tuning on contaminated data produces unreliable models that exhibit the same errors at scale.

Pipelines treat synthetic data generation as a quality-controlled manufacturing process. Each stage has defined inputs, outputs, and quality gates. Each transition point has automated checks and human review. Each stage produces metrics that inform improvements to earlier stages. Pipelines cost more to build and run more slowly than batch jobs, but they produce clean datasets that train reliable models. The legal tech company that opened this subchapter rebuilt their pipeline with all four stages, proper filtering, and validation. Their second dataset took twice as long to generate and cost forty percent more, but the resulting fine-tuned model achieved production quality on first deployment and required no emergency rollback.

Your synthetic data pipeline is not overhead. It is your quality control system. Build it with the same rigor you apply to production systems. The model you fine-tune is only as good as the data you train it on. Garbage in, garbage out is not just a cliche. It is the iron law of machine learning. A properly designed pipeline ensures that what goes in is high-quality, representative, and aligned with your task requirements, which means what comes out is a model you can trust in production.

Now that you have designed your pipeline and implemented quality gates, the next question is how many examples you actually need and how to determine when you have enough data, which is the focus of the next subchapter.
