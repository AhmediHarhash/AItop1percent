# 5.1 — The Infrastructure Decision: API, Managed Platform, or Self-Hosted

In July 2025, a legal technology company spent eleven weeks and $47,000 building a self-hosted fine-tuning infrastructure on AWS before their three-person ML team discovered they could have used OpenAI's fine-tuning API for the same model at one-tenth the cost and zero infrastructure overhead. They had chosen self-hosted infrastructure because it felt like the "serious engineering choice," but their actual requirements were straightforward: fine-tune GPT-4o-mini on 12,000 legal document summaries, deploy to production, monitor quality. No custom architecture. No exotic hyperparameters. No data residency constraints. They had optimized for flexibility they would never use while ignoring the operational burden they would carry forever. The root cause was not technical misjudgment but a failure to match infrastructure choice to actual business constraints. They had skipped the infrastructure decision framework and defaulted to maximum control because it felt more professional than using a managed API.

This pattern repeats across every company adopting fine-tuning in 2026. Teams face three infrastructure paths: API-based fine-tuning from model providers, managed platforms that handle infrastructure but give you more control, or self-hosted infrastructure where you own everything. Each path represents a different trade-off between simplicity and flexibility, between speed and control, between operational burden and customization capability. The right choice depends on six factors that most teams evaluate in the wrong order, leading to infrastructure that either over-serves their needs at massive cost or under-serves their requirements in ways that become apparent only after launch.

## The Three Infrastructure Paths

The first path is API-based fine-tuning from frontier model providers. OpenAI offers fine-tuning for GPT-4o, GPT-4o-mini, and GPT-4.5. Google provides fine-tuning for Gemini models through Vertex AI. Anthropic offers fine-tuning for Claude 3.5 models in private beta as of January 2026. You upload training data through their API or web console, configure basic parameters like learning rate and epoch count, launch training, and receive a model ID you can call through the same inference API you already use. The provider handles compute allocation, training orchestration, model storage, versioning, and serving infrastructure. You pay per training token and per inference token, with no minimum commitment and no infrastructure overhead.

The second path is managed fine-tuning platforms that give you more control while still handling infrastructure. Together.ai, Anyscale, Modal, and RunPod offer fine-tuning services where you choose the base model, training framework, and hyperparameters, but they provision GPUs, manage training runs, and serve the resulting model. You can fine-tune open-weight models like Llama 3.3, Mistral Large, or DeepSeek-V3 that are not available through API fine-tuning from their creators. You can use custom training frameworks like Axolotl or torchtune with full control over training recipes. You can inspect training logs in real time, pause and resume runs, and export model weights. The platform handles GPU orchestration, distributed training setup, checkpoint management, and model serving endpoints. You pay for GPU hours during training and inference compute during serving, typically at rates 30 to 60 percent lower than building equivalent infrastructure yourself.

The third path is self-hosted infrastructure where you provision your own GPUs, install training frameworks, orchestrate runs, and serve models. You might rent GPUs from AWS, Google Cloud, Azure, Lambda Labs, or CoreWeave, or you might buy physical hardware for on-premises deployment. You choose every component: PyTorch or JAX, Axolotl or torchtune or HuggingFace TRL, DeepSpeed or FSDP for distributed training, vLLM or TGI for serving. You control data handling, training orchestration, checkpoint storage, model versioning, and deployment pipelines. You own the entire stack, which means you own every operational failure and every integration challenge. This path makes sense for less than 15 percent of teams fine-tuning models in 2026, but it attracts far more than 15 percent because it signals engineering seriousness.

## The Decision Framework: Six Factors in Order

The first factor is model choice, and it eliminates options immediately. If you need to fine-tune GPT-4.5 or Claude 3.5 Opus, you use the provider's API because those models are not available anywhere else. If you need to fine-tune Llama 3.3 70B or Mistral Large 2, you cannot use OpenAI or Anthropic—you need a managed platform or self-hosted infrastructure that supports open-weight models. If you need to fine-tune a custom architecture or a model not offered by any platform, you have no choice but self-hosted. Model availability is the first filter, and it often decides the infrastructure path before you evaluate any other factor.

The second factor is data sensitivity and regulatory constraints. If your training data is subject to HIPAA, contains financial records under SOX, or falls under EU AI Act high-risk requirements with data localization mandates, you may be prohibited from uploading it to third-party APIs. OpenAI and Google offer BAAs for HIPAA compliance, but many regulated enterprises still prefer to keep training data inside their own cloud tenancy. If you face hard data residency requirements—training data must stay in Germany, or Australia, or a government cloud—you need either a managed platform with regional deployment options or self-hosted infrastructure in the required region. Data constraints are binary: they either permit external APIs or they do not. If they do not, you skip to evaluating managed platforms and self-hosted options.

The third factor is cost at your expected scale. For small-scale fine-tuning—one model, 10,000 to 100,000 training examples, retraining monthly—API fine-tuning is almost always cheapest. OpenAI charges $25 per million training tokens as of January 2026, which means a 50,000-example dataset averaging 500 tokens per example costs $625 per training run. Inference pricing is the same as base models. If you retrain monthly, you pay $7,500 per year in training costs plus inference costs, with zero infrastructure overhead. Building equivalent infrastructure costs $3,000 to $8,000 per month in GPU and engineering time even if you use spot instances aggressively. You do not break even on self-hosted until you are training multiple models per week or running continuous training pipelines.

For medium-scale fine-tuning—three to ten models, 100,000 to 1 million training examples each, retraining weekly—managed platforms often provide the best cost-performance balance. Together.ai charges $1.20 to $2.40 per GPU-hour depending on GPU type, which means fine-tuning Llama 3.3 70B on 500,000 examples takes 6 to 12 hours on eight A100 GPUs at a cost of $60 to $230 per run. Retraining ten models weekly costs $2,400 to $9,200 per month, plus inference serving costs that depend on request volume. Equivalent self-hosted infrastructure requires purchasing reserved GPU instances or managing a fleet of spot instances, adding orchestration complexity and 24/7 on-call burden. The managed platform becomes cheaper when you include engineering time.

For large-scale fine-tuning—dozens of models, millions of training examples, continuous retraining pipelines, high inference volume—self-hosted infrastructure can be cost-justified if you have a team capable of operating it. A 3-year reserved instance commitment for eight H100 GPUs on AWS costs approximately $15,000 per month as of January 2026. If you run training jobs 60 percent of the time and serve models the rest, you are paying $0.10 per GPU-hour amortized, far below managed platform rates. But you need ML infrastructure engineers to build training orchestration, monitoring, checkpoint management, and serving pipelines. If that team costs $1.5 million per year, you need to be saving more than $125,000 per month on compute to break even. Most teams do not reach that threshold.

## Customization Requirements and Team Skill

The fourth factor is customization requirements beyond what APIs offer. OpenAI's fine-tuning API in January 2026 lets you control learning rate, batch size, and epoch count, but you cannot modify the training objective, add custom loss functions, implement multi-stage training, or use RLHF or DPO on top of supervised fine-tuning. If your use case requires those capabilities, you need a managed platform or self-hosted setup. Google Vertex AI offers more flexibility with support for RLHF tuning on Gemini models, but you still operate within Google's framework. Managed platforms like Modal or Anyscale let you bring custom training scripts using Axolotl or torchtune, giving you full control over training recipes while they handle GPU orchestration. Self-hosted infrastructure gives you unlimited customization at the cost of unlimited responsibility.

The fifth factor is team skill and operational maturity. API fine-tuning requires basic Python skills and the ability to format training data into JSONL files. You do not need GPU expertise, distributed training knowledge, or infrastructure operations experience. A product engineer who has never trained a model can successfully fine-tune GPT-4o-mini by reading the documentation for two hours. Managed platforms require more skill: you need to understand training hyperparameters, diagnose training instabilities, and interpret loss curves. You are debugging training runs, not just uploading data. Self-hosted infrastructure requires ML infrastructure engineering expertise: PyTorch internals, distributed training frameworks, GPU memory optimization, checkpoint management, model serving at scale. If you do not have that expertise in-house and are not willing to hire it, self-hosted infrastructure will fail.

The sixth factor is timeline pressure. API fine-tuning gets you from data to deployed model in hours. You upload training data, launch a job, wait for training to complete—typically 20 minutes to 4 hours depending on dataset size—and immediately call the fine-tuned model through the same API. Managed platforms add setup time: creating accounts, configuring training environments, writing training scripts, but you can still go from zero to deployed model in one to three days. Self-hosted infrastructure takes weeks to set up initially: provisioning GPUs, installing frameworks, building orchestration, configuring monitoring, writing deployment pipelines. If you need a fine-tuned model in production next week to meet a launch deadline, you use an API. If you are building long-term infrastructure for continuous fine-tuning, you can absorb the setup time.

## Anti-Pattern: Optimizing for the Wrong Constraint

The most common infrastructure mistake in 2026 is optimizing for cost when the binding constraint is speed, or optimizing for control when the binding constraint is team capacity. A mobile app company chose self-hosted fine-tuning infrastructure to save $4,000 per month on training costs, but their two ML engineers spent 40 percent of their time managing GPU instances and debugging training runs instead of improving model quality. They saved $48,000 per year on compute while burning $200,000 per year in opportunity cost on engineering time that could have been spent on tasks that directly improved the product. They optimized the wrong variable.

Another pattern: over-indexing on flexibility for future requirements that never materialize. A SaaS company built self-hosted infrastructure because they anticipated needing custom training objectives and multi-stage RL tuning within six months. Eighteen months later, they were still doing basic supervised fine-tuning that would have worked perfectly on OpenAI's API. They had paid $180,000 in infrastructure and engineering costs to preserve optionality they never exercised. The correct heuristic is to start with the simplest infrastructure that meets your current requirements and migrate only when you hit a concrete limitation. Premature optimization for imagined future needs is as wasteful in fine-tuning infrastructure as it is in application architecture.

## The Hybrid Pattern: Start Narrow, Expand Deliberately

The most successful teams in 2026 start with API fine-tuning for their first model, then expand to managed platforms when they need open-weight models or faster iteration cycles, then move to self-hosted infrastructure only when they have hard requirements that no managed option satisfies. A fintech company fine-tuned GPT-4o-mini through OpenAI's API for their first fraud detection model, validated business impact over three months, then moved to Together.ai to fine-tune Llama 3.3 70B for cost reduction while maintaining quality. After nine months and five models in production, they built self-hosted infrastructure on reserved H100 instances to support continuous retraining pipelines and custom RLHF workflows. Each transition was triggered by hitting a concrete limitation, not by abstract principles about engineering maturity.

This deliberate expansion strategy works because each infrastructure tier teaches you lessons that inform the next tier. API fine-tuning teaches you data formatting, evaluation design, and production integration with minimal risk. Managed platforms teach you training hyperparameter tuning, distributed training behavior, and checkpoint management without forcing you to operate infrastructure. Self-hosted infrastructure makes sense only after you have learned those lessons and confirmed that your business scale and technical requirements justify the operational burden. Skipping the learning stages by building self-hosted infrastructure first is how you end up spending $47,000 to learn that you should have used the API.

## When API Fine-Tuning Is Correct

API fine-tuning is the right choice when your model is available from the provider, your data constraints allow external upload, your cost at current scale is under $10,000 per month, you do not need training customization beyond basic hyperparameters, and you want to deploy in days not weeks. This describes 60 to 70 percent of fine-tuning use cases in early 2026. API fine-tuning is not a compromise or a shortcut—it is the professional choice when it meets your requirements. Using self-hosted infrastructure for a use case that fits API fine-tuning is not engineering rigor; it is organizational waste.

## When Managed Platforms Are Correct

Managed platforms are the right choice when you need open-weight models, faster training iteration than APIs provide, custom training frameworks or hyperparameters, or cost savings from training multiple models frequently. They make sense when you have ML engineers who can configure training but do not want to operate infrastructure, when you need regional deployment options not available from API providers, or when you want to experiment with models and frameworks without infrastructure lock-in. Managed platforms are the equilibrium choice for teams running production fine-tuning at scale but not at the scale where self-hosted economics dominate.

## When Self-Hosted Is Correct

Self-hosted infrastructure is correct when you have hard data residency requirements that no managed platform satisfies, when your training volume and customization needs justify dedicated infrastructure engineering, when you need custom model architectures or training frameworks not supported by any platform, or when you are building fine-tuning as a core product capability and need full control over the entire stack. It requires a team with ML infrastructure expertise and the organizational commitment to operate complex systems indefinitely. It is the right choice for less than 15 percent of teams but the tempting choice for far more.

The infrastructure decision is not about technical sophistication. It is about matching tools to constraints. The team that fine-tunes GPT-4o through OpenAI's API and ships a production system in one week is making a better engineering decision than the team that spends three months building self-hosted infrastructure for the same use case. Infrastructure should be the minimum viable platform that meets your requirements, not a demonstration of engineering ambition. Recognize the three paths, evaluate the six factors in order, and choose the simplest infrastructure that satisfies your actual constraints.

Your infrastructure choice determines not just where training happens but what training workflows become possible, which we examine through the specifics of OpenAI's fine-tuning API in the next subchapter.
