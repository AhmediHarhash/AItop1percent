# 8.9 â€” The ROI Spreadsheet: A Step-by-Step Financial Model

Finance teams do not approve AI investments based on PowerPoint slides and verbal assurances. They approve investments based on detailed, interrogatable financial models that show all assumptions, all cost categories, all benefit streams, sensitivity analysis across key variables, and year-by-year cash flows. If you cannot show your work, you do not get budget. An October 2025 product manager learned this when she presented a one-page summary claiming four hundred eighty percent first-year ROI for fine-tuning a support ticket classifier. The CFO asked six questions in ninety seconds: what assumptions drive that number, how sensitive is it to changes, what if accuracy is half of projection, what if usage grows faster, can you show year-by-year cash flow, and what are you not telling me. She could not answer. She had done back-of-envelope math, not financial modeling. The CFO sent her back to build a real spreadsheet.

The CFO sent her back to build a real model: a spreadsheet with every cost component, every benefit component, every assumption explicitly stated, and sensitivity analysis showing how ROI changes under different scenarios. It took her another week. When she returned with the full model, the CFO spent 20 minutes challenging assumptions, adjusting parameters, and running what-if scenarios. He changed projected accuracy improvement from 12 percentage points to 8 percentage points, increased maintenance costs by 30%, and reduced adoption rate in the first six months by 20%. The ROI dropped from 480% to 310%. The CFO approved the project on the spot because he trusted a model he could interrogate and adjust over a summary he had to take on faith.

The ROI spreadsheet is not optional. It is the foundation of every credible fine-tuning business case. It forces you to make every assumption explicit, calculate every cost and benefit systematically, and demonstrate that the investment makes sense even under conservative scenarios. A well-built financial model is not just a tool for securing approval. It is a tool for making better decisions, identifying risks, and managing the project after launch. If you cannot build the model, you do not understand the economics well enough to proceed.

## Input Assumptions: The Foundation of the Model

Every financial model starts with input assumptions: the parameters that drive costs and benefits. Input assumptions must be explicitly stated, data-driven where possible, and conservative where data is uncertain. The first category of assumptions is volume: how many requests, transactions, or tasks will the fine-tuned model process?

For the customer support classification example, baseline volume is the current number of tickets processed per month. If your support system handles 18,000 tickets per month, that is your baseline. Project volume growth based on historical trends: if ticket volume has grown 12% annually over the past three years, assume 12% annual growth or be conservative and assume 8%. Volume assumptions drive both costs and benefits: higher volume means higher inference costs but also higher value from quality improvements because you prevent more errors.

The second category is baseline performance: how well does your current system perform? Baseline accuracy, error rate, false positive rate, false negative rate, average handling time, and rework rate are all measurable from production data. If your current classification system achieves 81% accuracy with a 19% error rate, those are your baseline performance assumptions. Document the measurement period and sample size: accuracy of 81% based on 4,200 manually reviewed tickets over three months with 95% confidence interval of 79% to 83%.

The third category is projected performance improvement from fine-tuning. This comes from your validation set results. If your fine-tuned model achieves 92% accuracy on the validation set with a 95% confidence interval of 90% to 94%, use the lower bound of 90% as your conservative projection. Projected improvement is 90% minus 81%, or 9 percentage points. Error rate drops from 19% to 10%, a 47% reduction. This is the performance delta that drives your quality benefits.

The fourth category is cost per unit for errors, rework, support, and labor. Cost per error is the average cost of detecting, investigating, and correcting a classification error. For a misrouted support ticket, this includes the time spent by the first agent who cannot resolve the issue, the time spent by the supervisor or routing system to reassign the ticket, and the additional time spent by the second agent to resolve an issue without the context from the first interaction. If the average is 14 minutes of additional labor at a fully-loaded agent cost of $34 per hour, cost per error is $7.93. Round conservatively to $8.

The fifth category is adoption rate: what percentage of eligible volume will use the fine-tuned model, and how quickly will adoption ramp? If you deploy the fine-tuned model as a mandatory replacement for the existing system, adoption is 100% immediately. If you deploy it as an optional tool or a gradual rollout, adoption ramps over time: 20% in month one, 40% in month two, 60% in month three, 80% in month four, 100% in month six. Slow adoption delays benefit realization and extends payback period.

The sixth category is cost assumptions: labor rates, infrastructure pricing, API pricing, annotation costs, and training costs. Use your organization's actual fully-loaded cost per employee for labor rates: salary plus benefits plus overhead. For contractors or vendors, use actual billed rates. For cloud infrastructure, use current pricing from your provider with a 5% to 10% annual escalation assumption to account for likely price increases or usage growth. For API pricing, use the provider's published rates and assume those rates remain constant unless you have reason to believe otherwise.

The seventh category is discount rate for multi-year projections. Use your organization's weighted average cost of capital or hurdle rate for technology investments, typically 10% to 15% for most companies. The discount rate converts future benefits into present value: a dollar of benefit three years from now is worth less than a dollar today because you could invest today's dollar and earn returns. Higher discount rates penalize long-payback projects and favor quick wins.

Document every assumption in a dedicated assumptions tab in your spreadsheet. Each assumption should include the value, the source, the measurement method, the confidence level, and the rationale for any adjustments. For example: baseline ticket volume is 18,000 per month based on average of October through December 2025 support system logs, projected growth rate of 8% annually based on three-year CAGR of 12% adjusted downward for economic uncertainty, baseline accuracy of 81% based on manual review of 4,200 tickets with 95% CI of 79% to 83%, cost per error of $8 based on time study of 60 misrouted tickets showing average of 14 minutes additional labor at fully-loaded agent cost of $34 per hour. This level of detail allows anyone reviewing the model to understand, challenge, and adjust your assumptions.

## Cost Categories: Building the Investment Side

The cost side of the model includes all upfront investment costs and all recurring operational costs over the projection period. Upfront costs are one-time expenses incurred to get the fine-tuned model into production. Recurring costs are annual or monthly expenses required to keep the model running.

Start with data preparation and annotation costs. Calculate the number of examples you need for training, multiply by the cost per example for annotation, and add any costs for data cleaning, formatting, or quality assurance. For 8,000 training examples at $6.50 per example, annotation costs are $52,000. Add $8,000 for data engineering labor to clean and format the data, and $5,000 for quality assurance reviews. Total data preparation: $65,000.

Add model training and experimentation costs. Include compute costs for training runs, labor costs for ML engineers running experiments and tuning hyperparameters, and any third-party platform fees if you are using a managed fine-tuning service. For a fine-tuning project requiring 12 training runs at an average of $1,800 per run, compute costs are $21,600. Add 200 hours of ML engineering labor at a fully-loaded cost of $150,000 annually, or $75 per hour, for $15,000 in labor. Total training and experimentation: $36,600.

Add evaluation and validation costs. Include the cost of creating the evaluation set, running evaluations, analyzing results, and iterating on the model based on evaluation findings. For a 1,200-example evaluation set at $6.50 per example, evaluation set creation costs $7,800. Add 80 hours of ML engineering labor for running evaluations and analysis at $75 per hour, or $6,000. Total evaluation: $13,800.

Add infrastructure setup costs. Include any one-time costs for setting up serving infrastructure, deploying the model, integrating with existing systems, and building monitoring and logging pipelines. For a deployment using the provider's hosted fine-tuned model API, integration costs might be 120 hours of engineering labor at $75 per hour, or $9,000, plus $3,000 for monitoring and alerting infrastructure setup. Total infrastructure setup: $12,000.

Add deployment and launch costs. Include testing, user training, documentation, and any change management costs associated with rolling out the new model. For a customer support classification system, you might spend $8,000 on user training and documentation and $6,000 on A/B testing and validation before full rollout. Total deployment: $14,000.

Sum all upfront costs: $65,000 for data, $36,600 for training, $13,800 for evaluation, $12,000 for infrastructure setup, and $14,000 for deployment. Total upfront investment: $141,400. This is your year-zero or year-one capital investment.

Now add recurring costs. Start with inference costs if you are using the provider's API. Calculate monthly request volume times tokens per request times cost per token. For 18,000 tickets per month growing at 8% annually, year-one average volume is 18,720 tickets per month. At 180 input tokens and 30 output tokens per ticket and fine-tuned model pricing of $7.50 per million input tokens and $22.50 per million output tokens, monthly cost is 18,720 tickets times 180 tokens times $7.50 divided by 1,000,000, plus 18,720 tickets times 30 tokens times $22.50 divided by 1,000,000. That is $25.27 plus $12.64, or $37.91 per month, which rounds to $455 annually. Wait, that seems too low. Recalculate: 18,720 tickets times 180 input tokens equals 3,369,600 input tokens, times $7.50 per million equals $25.27. 18,720 tickets times 30 output tokens equals 561,600 output tokens, times $22.50 per million equals $12.64. Total $37.91 per month or $455 annually. This is far lower than the $340,000 cited earlier for the fraud detection example because the fraud detection example had much higher per-transaction token counts.

Actually, for a realistic support ticket classification system, token counts are likely higher. Assume 600 input tokens per ticket to include the full ticket text and conversation history, and 80 output tokens for the classification and reasoning. Recalculate: 18,720 tickets times 600 input tokens equals 11,232,000 input tokens per month, times $7.50 per million equals $84.24. 18,720 tickets times 80 output tokens equals 1,497,600 output tokens per month, times $22.50 per million equals $33.70. Total $117.94 per month or $1,415 annually. For year two with 8% growth, volume is 20,218 tickets per month, annual cost is $1,528. For year three, $1,651. Use these figures for inference costs.

Add infrastructure and hosting costs if self-hosting. For this example, assume using the provider's API, so infrastructure costs are minimal: $2,400 annually for monitoring, logging, and alerting infrastructure.

Add maintenance and retraining costs. Assume annual retraining on 3,000 new examples at $6.50 per example for annotation, or $19,500, plus $12,000 in training compute and labor, plus $6,000 for evaluation refresh. Total annual maintenance: $37,500.

Add lifecycle management labor. Assume 25% of one ML engineer's time at $150,000 fully-loaded cost, or $37,500 annually, to manage the model, monitor performance, coordinate retraining, and respond to incidents.

Sum all recurring costs for year one: $1,415 inference, $2,400 infrastructure, $37,500 maintenance, and $37,500 labor. Total year-one recurring: $78,815. For year two: $1,528 plus $2,520 plus $39,375 plus $39,375, or $82,798. For year three: $1,651 plus $2,646 plus $41,344 plus $41,344, or $86,985. Note that maintenance and labor costs increase by 5% annually to account for inflation and salary growth.

## Benefit Categories: Building the Value Side

The benefit side of the model includes cost savings and quality-driven benefits. Start with cost savings from reduced API spend if fine-tuning allows you to use shorter prompts or switch to a cheaper model. For this example, assume the baseline system uses GPT-5 API at standard pricing of $5 per million input tokens and $15 per million output tokens with an average of 800 input tokens and 100 output tokens per ticket due to complex few-shot prompting. Baseline annual cost for 224,640 tickets in year one is 224,640 times 800 input tokens times $5 per million plus 224,640 times 100 output tokens times $15 per million, or $898.56 plus $336.96, or $1,235.52. Fine-tuned annual cost is $1,415 as calculated earlier. Wait, fine-tuning is more expensive because of the fine-tuned pricing premium. Adjust: baseline uses standard GPT-5, fine-tuned uses fine-tuned GPT-5 at 1.5x pricing. Recalculate baseline: $1,235 annually. Fine-tuned: $1,415. Fine-tuning increases inference cost by $180 annually, so there is no direct cost savings. Strike this benefit category for this example.

Instead, focus on quality-driven benefits. First, error reduction value. Baseline error rate is 19%, fine-tuned error rate is 10%, error reduction is 9 percentage points. Baseline annual errors are 224,640 tickets times 19%, or 42,682 errors. Fine-tuned annual errors are 224,640 times 10%, or 22,464 errors. Prevented errors: 20,218. Value per prevented error is $8. Annual error reduction value: $161,744 in year one. For year two with volume growth, 242,611 tickets times 9% error reduction times $8 per error, or $174,681. For year three, $188,656.

Second, support cost reduction from fewer escalations. Baseline escalation rate is 6.2% of all tickets due to misclassification sending tickets to the wrong team. Fine-tuned model reduces escalation rate to 2.8%. Prevented escalations: 3.4 percentage points times 224,640 tickets, or 7,638 escalations. Each escalation costs an average of $18 in additional labor for the escalation queue and management overhead. Annual escalation reduction value: $137,484 in year one, $148,481 in year two, $160,360 in year three.

Third, cycle time reduction. Accurate classification routes tickets to the right team on the first attempt, reducing average resolution time from 38 minutes to 34 minutes, a savings of 4 minutes per ticket. For 224,640 tickets, that is 898,560 minutes, or 14,976 hours. At $34 per hour fully-loaded agent cost, annual value is $509,184 in year one, $549,919 in year two, $593,913 in year three.

Fourth, user satisfaction and retention. Faster, more accurate support improves customer satisfaction scores, reduces churn, and increases lifetime value. This is harder to quantify, so treat it as a qualitative benefit or model it conservatively: assume 0.5% reduction in churn among customers who contact support, which represents 4% of your customer base. With 12,000 customers, 480 contact support annually, and a 0.5% churn reduction prevents 2.4 lost customers. At $14,000 average lifetime value per customer, annual retention value is $33,600 in year one, growing proportionally with customer base growth.

Sum all year-one benefits: $161,744 error reduction, $137,484 escalation reduction, $509,184 cycle time reduction, and $33,600 retention value. Total year-one benefits: $842,012. For year two: $1,021,841. For year three: $1,103,289.

## Calculating ROI, Payback Period, and NPV

With costs and benefits quantified, calculate the key financial metrics. First-year ROI is total year-one benefits minus total year-one costs, divided by total year-one costs. Total year-one costs are upfront investment of $141,400 plus recurring costs of $78,815, or $220,215. Total year-one benefits are $842,012. First-year ROI is $842,012 minus $220,215, divided by $220,215, times 100%, or 282%.

Payback period is the time required to recover the initial investment. Cumulative net benefit in month one is monthly benefits of $70,168 minus monthly costs of $18,351, or $51,817. Cumulative net benefit after month one is $51,817 minus the upfront investment of $141,400, or negative $89,583. Continue monthly: after month two, negative $37,766. After month three, positive $14,051. Payback occurs between month two and month three, approximately 2.7 months.

Net present value is the sum of discounted future cash flows minus the initial investment. Discount each year's net benefit by the discount rate to convert future value into present value. For a 12% discount rate, year-one net benefit of $621,797 is worth $621,797 divided by 1.12, or $555,176 in present value. Year-two net benefit of $939,043 is worth $939,043 divided by 1.12 squared, or $748,419. Year-three net benefit of $1,016,304 is worth $1,016,304 divided by 1.12 cubed, or $723,372. Sum the discounted benefits: $2,026,967. Subtract the upfront investment of $141,400. NPV is $1,885,567 over three years.

Internal rate of return is the discount rate at which NPV equals zero. This requires iterative calculation or a spreadsheet IRR function. For this example, with such high net benefits, IRR is well over 200% annually.

Present these metrics together: 282% first-year ROI, 2.7-month payback period, $1.89 million three-year NPV at 12% discount rate, IRR exceeds 200%. These numbers make a compelling case.

## Sensitivity Analysis: Testing the Model Under Different Scenarios

Sensitivity analysis shows how the ROI changes when you vary key assumptions. Build a sensitivity table that shows ROI under different scenarios for the most uncertain or impactful assumptions: performance improvement, cost per error, adoption rate, and maintenance costs.

For performance improvement, create three scenarios: pessimistic with 5 percentage points of accuracy improvement instead of 9, base case with 9 points, and optimistic with 12 points. Recalculate benefits under each scenario. Pessimistic scenario reduces error prevention from 20,218 errors to 11,232 errors, cutting error reduction value from $161,744 to $89,856. Total year-one benefits drop to $519,296. First-year ROI drops to 136%. Still positive, but much lower.

For cost per error, create three scenarios: low at $5 per error, base at $8, and high at $12. Low scenario reduces error reduction value to $101,090, dropping total benefits to $615,358 and ROI to 179%. High scenario increases error reduction value to $242,616, raising total benefits to $1,008,180 and ROI to 358%.

For adoption rate, model slow adoption where only 60% of tickets use the fine-tuned model in year one. Benefits drop proportionally to 60% of base case, or $505,207. ROI drops to 129%.

For maintenance costs, model a pessimistic scenario where maintenance is 50% higher than projected, adding $18,750 annually. Year-one costs rise to $238,965, reducing ROI to 253%.

Build a two-way sensitivity table showing ROI as a function of two variables simultaneously, like performance improvement and cost per error. This shows the range of outcomes under different combinations of assumptions. Even in the pessimistic corner with 5 points of improvement and $5 cost per error, ROI is 87%, still positive.

Sensitivity analysis demonstrates that the investment is robust: even under conservative assumptions, the project delivers acceptable returns. This gives finance confidence that you have not cherry-picked optimistic assumptions to inflate ROI.

## Presenting the Model to Finance Teams

Finance teams care about three things: credibility, clarity, and risk. The ROI spreadsheet addresses all three. Credibility comes from transparent assumptions, data-driven inputs, and conservative estimates. Clarity comes from a well-organized model with separate tabs for assumptions, costs, benefits, calculations, and sensitivity analysis. Risk management comes from showing multiple scenarios and demonstrating that the investment makes sense even under pessimistic conditions.

Walk finance through the model step by step. Start with the assumptions tab: here are the inputs, here are the sources, here is why I chose these values. Move to the cost tab: here are the upfront costs broken down by category, here are the recurring costs, here is how they scale over time. Move to the benefits tab: here are the quality improvements, here is how they translate into dollar values, here is the evidence supporting those translations. Move to the summary tab: here is the ROI, here is the payback period, here is the NPV.

Then show sensitivity analysis. Explain that the base case assumes 9 points of accuracy improvement, but validation set confidence intervals suggest the true improvement could be as low as 7 points or as high as 11 points. Show the ROI under both scenarios: 210% at 7 points, 352% at 11 points. Show that even at 7 points, the investment is compelling. This preempts the inevitable challenge: what if your performance assumptions are wrong?

Offer to share the spreadsheet so finance can adjust assumptions and run their own scenarios. This demonstrates confidence in your model and respect for their expertise. When the CFO changes your projected accuracy from 9 points to 6 points and ROI drops from 282% to 156%, and he approves the project anyway because 156% is still excellent, you have won.

The ROI spreadsheet is not a one-time deliverable. It is a living tool you update quarterly as actual performance data becomes available. If actual error reduction in the first quarter is better than projected, update the model and show increased value. If actual maintenance costs are higher than projected, update the model and show the impact. The model becomes the single source of truth for the project's financial performance and the basis for ongoing investment decisions.

The final step is to take this detailed financial model and translate it into a one-page business case that executive stakeholders can understand and approve in a ten-minute conversation. That translation requires a different skill: the ability to communicate technical and financial complexity in business language.
