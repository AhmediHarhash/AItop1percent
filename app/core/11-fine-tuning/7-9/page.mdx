# 7.9 — Regression Detection: Automated Checks Against Golden Sets

In August 2025, a healthcare technology company deployed a fine-tuned model for clinical documentation. The model had been trained on six months of physician notes to improve accuracy in extracting diagnosis codes and treatment plans. The fine-tuning had succeeded brilliantly at the domain-specific task—diagnosis extraction improved from eighty-one percent recall to ninety-four percent. But two days after deployment, physicians started reporting bizarre errors. The model was capitalizing medical terms inconsistently, generating grammatically broken sentences, and occasionally hallucinating medication names that did not exist. When the engineering team investigated, they discovered that fine-tuning had degraded core language capabilities. The base model had been reliable at basic grammar, spelling, and factual consistency. The fine-tuned model had lost those capabilities in exchange for domain accuracy. The root cause was that no one had tested whether the fine-tuned model still passed the basic capability checks that the base model passed. They had measured improvement on the new task. They had never measured regression on existing tasks. The rollback took three days, during which seven thousand clinical notes were generated with errors that required manual review. The incident cost them $1.2 million in remediation and damaged their reputation with four hospital systems. The failure was not the fine-tuning process. The failure was the absence of automated regression testing.

## Why Fine-Tuning Causes Regression

Fine-tuning is an optimization process that adjusts model weights to improve performance on a specific task. But model weights are shared across all capabilities. When you optimize for one task, you are implicitly deprioritizing other tasks. If your fine-tuning dataset is narrow—focused entirely on clinical documentation, for example—the model will shift its internal representations toward that domain and away from general language understanding, commonsense reasoning, and other capabilities that the base model had acquired during pretraining.

This is not a bug. This is the tradeoff you make when you fine-tune. You are specializing the model. Specialization improves performance on the target task, but it can degrade performance on tasks that are not represented in the fine-tuning dataset. The degree of regression depends on how much the fine-tuning dataset differs from the pretraining data, how many training steps you run, and how aggressively you optimize.

The only way to detect regression is to measure it explicitly. You must maintain a set of test cases that represent the core capabilities you expect the model to retain—grammar, spelling, factual accuracy, reasoning, instruction-following, safety—and you must run those test cases through both the base model and the fine-tuned model. If the fine-tuned model performs worse on any of those test cases, you have detected regression. If the regression is severe, you must either retrain with a more balanced dataset or decide that the tradeoff is not acceptable.

This is not optional. This is the minimum standard for responsible fine-tuning. If you do not test for regression, you will ship models that fail in unexpected ways on tasks you assumed they could still handle.

## What Is a Golden Test Set

A **golden test set** is a curated collection of inputs and expected outputs that define the core capabilities your model must retain. These are not domain-specific test cases. They are general-purpose test cases that represent foundational skills: language understanding, factual accuracy, instruction-following, reasoning, safety, and output quality.

Each test case in the golden set has a known correct answer or a well-defined success criterion. For a grammar test case, the success criterion might be that the model generates a sentence with no grammatical errors. For a factual accuracy test case, the success criterion might be that the model states a verifiable fact correctly. For a reasoning test case, the success criterion might be that the model produces a logically valid conclusion from a set of premises.

You assemble the golden set before fine-tuning begins. You run the base model on the golden set and record its performance. This establishes the baseline. After fine-tuning, you run the fine-tuned model on the same golden set and compare the results. If the fine-tuned model performs worse on any category of test cases—if grammar accuracy drops from ninety-seven percent to eighty-nine percent, if factual accuracy drops from ninety-two percent to seventy-eight percent—you have detected regression.

The golden set must be large enough to cover the range of capabilities you care about, but small enough to run frequently without consuming excessive compute. A typical golden set might contain five hundred to two thousand test cases, organized into categories: grammar, factual accuracy, reasoning, instruction-following, safety, bias, toxicity, and hallucination. You must version the golden set, and you must ensure that it is never used for training—only for evaluation.

## Building a Comprehensive Golden Set

Building a golden set requires cross-functional input. Engineering selects test cases that cover technical capabilities: grammar, syntax, edge cases in instruction-following. Product selects test cases that cover user-facing capabilities: clarity, helpfulness, tone. Trust and Safety selects test cases that cover safety and policy: refusal of harmful requests, avoidance of biased language, factual accuracy on sensitive topics. Legal selects test cases that cover compliance: handling of regulated data, adherence to content policies.

You do not build the golden set from scratch. You start with public benchmarks that test general language understanding: grammar benchmarks, factual accuracy benchmarks, reasoning benchmarks. You add custom test cases that are specific to your application domain but still represent core capabilities. You add adversarial test cases that are known to cause failures in similar models. You add edge cases that users have reported in production.

Each test case must be documented with a clear success criterion. You do not rely on subjective judgment. You define objective rubrics: does the output contain factual errors, does it follow the instruction, does it violate safety policies. For some test cases, the success criterion is binary: the model either produces the correct answer or it does not. For other test cases, the success criterion is a score: the output is rated on a scale from one to five for clarity, helpfulness, or appropriateness.

You must review and update the golden set regularly. As you discover new failure modes in production, you add test cases to the golden set to ensure that future fine-tuned models do not regress on those failure modes. As new safety concerns emerge, you add test cases to cover them. The golden set is a living artifact that evolves with your understanding of the model's capabilities and risks.

## Automating Regression Detection in CI/CD

Regression testing must be automated and integrated into your CI/CD pipeline. Every time you produce a new fine-tuned model checkpoint, you must automatically run the golden set evaluation and compare the results to the baseline. If regression is detected, the pipeline stops and alerts the team. The checkpoint does not proceed to further evaluation or deployment until the regression is addressed.

This requires infrastructure. You need a pipeline that can load the fine-tuned model, run inference on the golden set, score the outputs, compare the scores to the baseline, and generate a report. The report must identify which categories of test cases regressed, by how much, and which specific test cases failed. The report must be actionable: it must tell Engineering exactly what went wrong and where to focus debugging effort.

The automation must be fast enough to provide feedback within hours, not days. If it takes two days to run the golden set evaluation, fine-tuning iteration slows to a crawl. You optimize the pipeline by batching inference, using smaller models for initial screening, and parallelizing across multiple GPUs. You prioritize test cases that are most likely to detect regression—safety, factual accuracy, and grammar—and run those first.

The automation must also be reliable. If the pipeline produces false positives—flagging regression when none exists—engineers will ignore the alerts. You must tune your scoring thresholds carefully, you must handle stochastic variation in model outputs, and you must validate that the pipeline produces consistent results across runs.

## Regression Alert Thresholds: When to Block Deployment

Not all regression is catastrophic. If the fine-tuned model scores ninety-six percent on grammar accuracy instead of ninety-seven percent, that is a one-percentage-point regression, and it might be acceptable if the domain-specific task improved by ten percentage points. But if the fine-tuned model scores seventy-eight percent on factual accuracy instead of ninety-two percent, that is a fourteen-percentage-point regression, and it is not acceptable under any circumstances.

You must define explicit thresholds for acceptable regression before fine-tuning begins. These thresholds are documented in your evaluation plan, and they are enforced by the automated pipeline. A typical threshold structure looks like this: no regression on safety or policy compliance, no more than two percentage points regression on factual accuracy, no more than three percentage points regression on reasoning, no more than five percentage points regression on grammar or fluency.

The thresholds are context-dependent. For a high-stakes application like clinical documentation or legal contract review, you might set stricter thresholds: zero tolerance for factual regression, zero tolerance for safety regression. For a low-stakes application like marketing copy generation, you might accept larger regressions in exchange for significant improvements in domain-specific quality.

You must also define thresholds for individual test cases, not just aggregate categories. If ninety-eight percent of grammar test cases pass but two percent fail catastrophically—producing outputs that are incomprehensible or offensive—you cannot declare success based on the aggregate score. You must investigate the failures and determine whether they represent a systematic problem or isolated edge cases.

When a threshold is exceeded, the pipeline blocks deployment and alerts the team. The alert includes the regression report, the specific test cases that failed, and the recommended next steps: retrain with a more balanced dataset, adjust the learning rate, reduce the number of training steps, or abandon this fine-tuning run and try a different approach.

## Handling Stochastic Variation in Regression Tests

Model outputs are stochastic. If you run the same input through the same model twice, you might get slightly different outputs due to sampling randomness. This creates a challenge for regression testing: how do you distinguish genuine regression from random variation?

The answer is to run each test case multiple times and aggregate the results. Instead of running the golden set once and comparing a single output to the baseline, you run the golden set three to five times and measure the average performance and the variance. If the fine-tuned model performs worse than the baseline on average, and the difference is larger than the variance, you have detected regression. If the difference is within the range of random variation, you do not flag it as regression.

This increases the compute cost of regression testing, but it is necessary for reliability. You cannot make deployment decisions based on a single noisy sample. You must account for the stochastic nature of the model.

You must also control the sampling parameters during regression testing. If you run the golden set with temperature set to 1.0, you will see more variation than if you run it with temperature set to 0.3. You should use the same sampling parameters for both the base model and the fine-tuned model, and you should use the same parameters that you will use in production. If your production system uses greedy decoding, your regression tests should use greedy decoding. If your production system uses temperature 0.7, your regression tests should use temperature 0.7.

## Debugging and Mitigating Regression

When regression is detected, you must debug the root cause. The most common causes are: the fine-tuning dataset does not cover the capabilities that regressed, the fine-tuning dataset contains examples that contradict the base model's knowledge, the learning rate is too high, the number of training steps is too large, or the fine-tuning dataset is too small and the model is overfitting.

The first mitigation strategy is to augment the fine-tuning dataset with examples that cover the regressed capabilities. If grammar regressed, add examples of well-formed sentences to the fine-tuning dataset. If factual accuracy regressed, add examples that reinforce correct factual knowledge. This does not mean you abandon domain-specific fine-tuning. It means you balance domain-specific examples with general-purpose examples to prevent the model from forgetting core skills.

The second strategy is to reduce the learning rate or the number of training steps. If you are updating the model weights too aggressively, you are erasing the base model's knowledge. A lower learning rate or fewer training steps will preserve more of the base model's capabilities while still improving on the domain-specific task.

The third strategy is to use a different fine-tuning technique. Instead of full fine-tuning, which updates all model weights, you might use parameter-efficient fine-tuning, which updates only a small subset of weights or adds adapter layers. This reduces the risk of catastrophic forgetting and regression.

The fourth strategy is to accept the tradeoff and deploy with monitoring. If the regression is minor and the domain-specific improvement is substantial, you might decide that the tradeoff is acceptable as long as you monitor production closely and are prepared to roll back if the regression causes user-facing problems.

## Regression Testing as a Continuous Process

Regression testing is not a one-time gate before deployment. It is a continuous process that runs on every new fine-tuning checkpoint, on every model update, and on every production model periodically to ensure that capabilities do not degrade over time due to data drift or infrastructure changes.

You must maintain a dashboard that tracks regression metrics over time. You must track the golden set scores for every model version, visualize trends, and alert when scores decline. You must review the dashboard regularly with Engineering, Product, and Trust and Safety to ensure that regression is visible and acted upon.

You must also expand the golden set as you discover new failure modes. When a production issue is traced to a regression that was not caught by the golden set, you add a test case to the golden set to ensure that future models are tested for that failure mode. The golden set is not static. It grows and evolves as your understanding of the model's capabilities and risks deepens.

This is the standard for 2026. Regression testing is not optional. It is not something you do if you have extra time. It is a core part of the fine-tuning workflow, and it is enforced by automation in your CI/CD pipeline. If you cannot detect regression reliably, you are not ready to fine-tune models for production.

The next subchapter presents the formal release gate checklist that every fine-tuned model must pass before deployment, covering core capability gates, domain accuracy gates, safety gates, regression gates, performance gates, and compliance gates.
