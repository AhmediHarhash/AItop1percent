# 7.3 — The Eval Suite Architecture: Core, Domain, Safety, and Regression Layers

What does it mean for a fine-tuned model to be ready for production? Does it mean the model scores well on your domain-specific accuracy benchmark? Does it mean internal testers like the outputs? Does it mean the fine-tuning loss curve converged? The answer is none of these. A model is ready when it passes a comprehensive, multi-layered evaluation architecture that tests domain performance, core capabilities, safety properties, and regression risks across every dimension that matters for your deployment. Single-layer evaluation—testing only the task you fine-tuned for—is how teams ship models that work on the target task but fail catastrophically on everything adjacent to it. You need four evaluation layers: core capability evaluation to ensure the model still thinks properly, domain-specific evaluation to ensure fine-tuning achieved its goal, safety evaluation to ensure alignment and compliance were preserved, and regression evaluation to ensure nothing broke. Only when all four layers pass do you deploy.

This failure illustrates the central requirement for evaluating fine-tuned models: you need a multi-layered evaluation architecture that tests not only the narrow task you fine-tuned for but also the broad set of capabilities, safety properties, and regression risks that fine-tuning can degrade. A single-layer eval suite that focuses only on task accuracy is professional negligence. You need four distinct layers: core capability evaluation, domain-specific evaluation, safety evaluation, and regression evaluation. Each layer tests different properties, uses different test sets, and serves different decision-making purposes. Together, they form the comprehensive eval suite that determines whether your fine-tuned model is safe to deploy.

## Layer One: Core Capability Evaluation

**Core capability evaluation** measures the model's performance on fundamental reasoning, instruction following, and general knowledge tasks that are not specific to your domain but that underpin all useful model behavior. These capabilities are learned during pre-training and reinforcement learning from human feedback. Fine-tuning can degrade them, especially when your training data is narrow or stylistically constrained. Core capability evaluation detects that degradation.

Core capabilities include logical reasoning, numerical reasoning, commonsense reasoning, reading comprehension, instruction following, multi-step planning, and context adherence. You test these capabilities using standardized benchmarks or custom test sets designed to probe general cognitive skills. For logical reasoning, you might use syllogism problems or deductive reasoning puzzles. For numerical reasoning, you might use arithmetic word problems or unit conversion tasks. For instruction following, you might use prompts that specify complex multi-part requirements and evaluate whether the model satisfies all of them.

The purpose of core capability evaluation is to ensure that fine-tuning has not made the model worse at thinking. A model that improves at your specific task but loses the ability to reason logically, follow instructions, or comprehend context is not deployable. Core capability regression is often subtle. The model does not fail completely. It becomes less reliable, less precise, or less consistent. You detect this through systematic testing on tasks that exercise the underlying reasoning skills your application depends on.

You run core capability evaluation by comparing fine-tuned model performance to base model performance on the same test set. Your acceptance criteria should require that the fine-tuned model match or exceed the base model on all core capability metrics. Tolerating even small regressions in core capabilities is dangerous because these regressions compound over time and across tasks. A model that is 5% worse at logical reasoning will fail in unpredictable ways on complex domain tasks that require chaining multiple reasoning steps.

Core capability test sets should be diverse and adversarial. Do not use only easy examples. Include hard examples, ambiguous examples, and examples designed to reveal brittleness in the model's reasoning. Include examples that require the model to synthesize information across multiple sentences, to recognize implicit assumptions, to distinguish relevant from irrelevant information, and to generalize from specific instances to abstract principles. These are the reasoning patterns that fine-tuning is most likely to degrade, and you must test for them explicitly.

## Layer Two: Domain-Specific Evaluation

**Domain-specific evaluation** measures the model's performance on the task you fine-tuned it for, using test cases drawn from your domain, your data distribution, and your specific use case requirements. This is the layer most teams build first and often the only layer they build. It is necessary but not sufficient. Domain-specific evaluation tells you whether fine-tuning succeeded at improving task performance. It does not tell you what you sacrificed to achieve that improvement.

Domain-specific test sets must be representative of your production distribution. If your production traffic includes edge cases, rare categories, or unusual input formats, your test set must include them proportionally. If you evaluate only on common cases, you will overestimate performance and miss failures that users will encounter. You must stratify your test set by difficulty, by category, by input characteristics, and by any other dimension that affects model performance. You must ensure that your test set covers the full range of scenarios the model will face in production, not just the scenarios that are easy to collect or label.

Domain-specific evaluation requires multiple metrics. Accuracy is not enough. You need precision, recall, F1 score, false positive rate, false negative rate, and category-level breakdowns for classification tasks. You need BLEU, ROUGE, semantic similarity, and human quality ratings for generation tasks. You need exact match, partial match, and answer relevance for question-answering tasks. You need latency, token usage, and output length distributions for operational metrics. Each metric captures a different dimension of performance, and you must track all of them to understand whether your fine-tuned model is actually better than the base model in the ways that matter to your users.

You must also measure performance on slices of your data. A model that achieves 94% overall accuracy may achieve 98% accuracy on common categories and 70% accuracy on rare categories. If your users depend on the rare categories, the overall metric is misleading. You need category-level metrics, difficulty-level metrics, and metrics for specific user populations or use case variants. Slicing reveals disparities that aggregate metrics hide, and those disparities often determine whether a model is deployable.

Domain-specific evaluation must include human review. Automated metrics capture patterns that correlate with quality, but they do not measure quality directly. You need human evaluators to assess whether model outputs are correct, complete, appropriate, and useful in context. Human review is expensive and slow, but it is non-negotiable for high-stakes applications. You cannot deploy a fine-tuned model to production based solely on automated metrics without validating that those metrics align with human judgment.

## Layer Three: Safety Evaluation

**Safety evaluation** measures the model's adherence to refusal policies, bias mitigation, factual accuracy, and regulatory compliance. Fine-tuning can degrade all of these properties. Safety evaluation detects that degradation before it reaches users. This layer is often skipped by teams who assume that fine-tuning on clean, domain-specific data cannot introduce safety risks. That assumption is false. Safety degradation is a predictable consequence of fine-tuning, and it requires dedicated evaluation.

Safety evaluation begins with testing refusal behavior. The base model has been trained to refuse requests that are harmful, unethical, illegal, or outside its safe operating domain. Fine-tuning can weaken these refusals in two ways. First, if your training data contains examples where the model complies with requests it should refuse, the fine-tuned model learns to comply more broadly. Second, if your training data contains no examples of refusals because you filtered them out or because they do not occur in your domain, the model forgets how to refuse and becomes more compliant than the base model.

You test refusal behavior by constructing adversarial prompts designed to elicit harmful, biased, or inappropriate outputs. These prompts must be tailored to your domain. Generic adversarial prompts used to test base models may not reveal domain-specific safety failures. If you are deploying in healthcare, your adversarial prompts must include requests for medical advice the model is not qualified to give, requests to disclose patient information, and requests to make diagnostic or treatment decisions without appropriate disclaimers. If you are deploying in finance, your adversarial prompts must include requests for investment advice, requests to make forward-looking statements, and requests to disclose non-public information. You evaluate whether the fine-tuned model refuses these requests at the same rate as the base model and whether the refusals are appropriate and clearly communicated.

Safety evaluation also includes bias testing. You measure whether the model's outputs vary systematically across demographic groups, geographic regions, language variants, or other protected characteristics. Bias testing requires constructing paired test cases that differ only in the demographic marker and evaluating whether the model produces equivalent outputs. If the model generates different loan approval recommendations for applicants who differ only in gender, or different medical advice for patients who differ only in race, you have a bias failure that blocks deployment.

Factual accuracy is another safety dimension. Fine-tuning can cause the model to hallucinate facts, to misrepresent information from the training data, or to generate plausible but incorrect claims. You test factual accuracy by evaluating the model on questions with verifiable answers, on tasks that require citing sources, and on scenarios where incorrect information could cause harm. You compare the fine-tuned model's factual accuracy to the base model's accuracy and require that the fine-tuned model match or exceed the baseline.

Regulatory compliance is a safety requirement for many domains. If your application is subject to GDPR, HIPAA, SOX, or other regulations, your safety evaluation must test whether the model complies with those regulations. This includes testing whether the model discloses required information, whether it handles sensitive data appropriately, whether it respects user rights, and whether it adheres to industry-specific guidelines. Regulatory compliance testing requires domain expertise. You cannot evaluate compliance using generic test cases. You need test cases written by people who understand the regulatory requirements and can construct scenarios that probe for violations.

## Layer Four: Regression Evaluation

**Regression evaluation** measures whether fine-tuning has degraded performance on tasks adjacent to your primary objective, on edge cases not well-represented in your training data, or on capabilities that users depend on but that you did not explicitly fine-tune for. This layer detects catastrophic forgetting, emergent failure modes, and unintended side effects of the fine-tuning process.

Regression test sets include tasks the model currently performs well that are related to but distinct from your fine-tuning objective. If you fine-tuned for classification, your regression suite includes summarization, explanation generation, and comparison tasks. If you fine-tuned for question answering, your regression suite includes question decomposition, context retrieval, and multi-hop reasoning tasks. If you fine-tuned for generation, your regression suite includes editing, reformatting, and style transfer tasks. These are the tasks most likely to degrade during fine-tuning because they share some but not all characteristics with your training data.

You also include edge cases in your regression suite. These are inputs that are rare, unusual, or adversarial. They include malformed inputs, ambiguous inputs, inputs with missing or conflicting information, and inputs that combine features in unexpected ways. Edge cases are disproportionately likely to fail after fine-tuning because the model has learned to expect the regularities present in your training data and is less robust to deviations from those regularities.

Regression evaluation requires comparing fine-tuned model outputs to base model outputs on the same inputs. You measure whether the fine-tuned model produces equivalent or better outputs on every regression test case. You flag any case where the fine-tuned model produces a clearly worse output, and you aggregate those failures to assess whether regression is systematic or isolated. Systematic regression blocks deployment. Isolated regression may be acceptable if the failures are rare, low-impact, and understood.

You must test regressions across input variations. A model that performs well on your canonical test cases may fail when those cases are paraphrased, reordered, or presented in a different format. You construct paraphrased versions of core test cases and evaluate whether the fine-tuned model produces consistent outputs. Inconsistency across paraphrases indicates that the model has overfit to surface features of your training data and is not robust to natural variation in how users will phrase their requests.

## Eval Suite Automation and Integration

Building a four-layer eval suite is not a one-time effort. It is an automated pipeline that runs every time you produce a new fine-tuned model candidate. The pipeline executes all four layers in sequence, collects metrics, compares those metrics to your baseline and acceptance criteria, and produces a pass-fail decision for each layer. A model that fails any layer does not proceed to deployment. A model that passes all layers proceeds to final human review and staging deployment.

Automation requires tooling. You need scripts to run inference on test sets, to score outputs using automated metrics, to aggregate results across categories and slices, and to generate reports that summarize performance. You need version control for test sets, evaluation code, and baseline data so that you can reproduce evaluation results and track how metrics evolve over time. You need integration with your training pipeline so that evaluation runs automatically when a new model checkpoint is produced, without manual intervention.

You also need human-in-the-loop review for safety and quality dimensions that cannot be fully automated. Your eval suite should include a manual review step where human evaluators sample outputs from each layer, assess whether they meet quality and safety standards, and flag any failures that automated metrics missed. Manual review is the final gate before deployment and the place where you catch failures that are obvious to humans but invisible to metrics.

## Eval Suite Evolution

The eval suite is not static. As your application evolves, as user needs change, and as you learn from production failures, you must expand and refine the suite. Every production incident that was not detected by your eval suite represents a gap in your test coverage. You close that gap by adding test cases that would have detected the failure, by refining your metrics to be more sensitive to that failure mode, and by adjusting your acceptance criteria to prevent similar failures in the future.

Eval suite evolution requires discipline. It requires treating test set expansion as a first-class engineering task, not as an afterthought. It requires maintaining a backlog of test cases to add, a roadmap for expanding coverage, and a process for reviewing and approving new test cases. It requires collaboration between product, engineering, and domain experts to ensure that test cases reflect real user needs and real failure risks.

The financial services company that deployed the fine-tuned model without a comprehensive eval suite learned this the hard way. They built domain accuracy tests and assumed that was sufficient. They skipped core capability tests, safety tests, and regression tests because those seemed like extra work. The cost of that decision was $2.1 million and eight months. Building the four-layer eval suite from the start would have cost a fraction of that and would have prevented the deployment failure entirely.

The eval suite is the mechanism that ensures your fine-tuned model is safe, reliable, and fit for deployment. It is not optional infrastructure. It is the minimum necessary to operate responsibly. The next step is defining the specific gating criteria that determine when a model passes evaluation and when it gets sent back for retraining or tuning adjustments.
