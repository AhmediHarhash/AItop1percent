# 9.2 — Quantization for Deployment: GPTQ, AWQ, GGUF, and When Each Fits

**Quantization is not free compression.** It is a deliberate tradeoff between model size, inference speed, and output quality that must be validated rigorously for your specific task and quality requirements. The infrastructure cost savings are real—sixty to seventy-five percent reductions in GPU memory and serving costs—but the quality risks are also real, particularly for fine-tuned models optimized for narrow performance margins on specialized capabilities. Generic claims that four-bit quantization is "basically lossless" ignore the fact that loss is task-dependent, segment-dependent, and often invisible in aggregate metrics while catastrophic on rare but critical inputs. A financial services company in November 2025 deployed a compliance monitoring model at 96.2 percent accuracy, a meaningful jump from their ninety-one percent baseline. The model required twenty-six gigabytes of GPU memory at full precision, costing eighty-two hundred dollars monthly on A100 instances. Three weeks later infrastructure proposed quantizing to four-bit GPTQ, dropping memory to under eight gigabytes and cost to twenty-four hundred dollars monthly, a seventy-one percent savings. Compliance resisted. Infrastructure insisted quantization was basically lossless. They deployed it. Two months later legal found a pattern of missed violations in securities transactions. The quantized model's accuracy on that narrow category had dropped from ninety-four percent to eighty-three percent, invisible in aggregates but catastrophic in regulatory risk.

The failure was not in quantization itself. The failure was treating quantization as a uniform optimization that could be applied blindly without understanding where and how precision matters for specific model capabilities. Quantization is not magic compression. It is a deliberate tradeoff between model size, inference speed, and output quality that must be evaluated rigorously against task-specific requirements. For many production deployments, quantization is obviously correct, delivering 60% to 75% cost reductions with negligible quality impact. For others, particularly fine-tuned models optimized for narrow margins of performance on specific capabilities, quantization degrades exactly the behaviors you spent months of effort to improve.

## The Quantization Landscape

Three quantization formats dominate production deployments in 2026: GPTQ for GPU serving, AWQ for GPU serving with activation-aware optimization, and GGUF for CPU and edge deployment. Each implements different tradeoffs between compression ratio, inference performance, and quality preservation.

GPTQ quantizes model weights to 4-bit or 3-bit precision using a layer-wise quantization algorithm that minimizes reconstruction error. The technique processes the model one layer at a time, quantizing weights while solving an optimization problem that preserves the layer's output distribution when given representative input data. GPTQ is designed for GPU inference and achieves excellent speed on modern NVIDIA GPUs, often delivering 1.5x to 2x inference throughput compared to full precision models while reducing memory requirements by 60% to 75%. Quality degradation is typically small for general capabilities, with perplexity increasing by 1% to 4% and most downstream task performance dropping less than 2 percentage points.

The critical detail is that GPTQ quantization is calibrated using a dataset of representative inputs. The quantization algorithm processes a few thousand examples through the model and uses the resulting activations to guide weight quantization. If your calibration dataset accurately represents your production workload, GPTQ preserves the capabilities you care about effectively. If your calibration dataset misses important input patterns, GPTQ will optimize for the wrong distribution and may degrade critical behaviors that appear rarely in calibration data but matter enormously in production. This is exactly what happened to the financial services company: their calibration dataset included examples of the common violation types but undersampled the rare securities transaction pattern where quality degraded catastrophically.

AWQ implements activation-aware weight quantization, a refinement that protects weights corresponding to salient activations from aggressive quantization. The insight is that not all weights contribute equally to model outputs: a small fraction of weights, those that connect to highly activated channels, disproportionately influence output quality. AWQ identifies these salient weights during calibration and quantizes them less aggressively or leaves them at higher precision while compressing the remaining weights more aggressively. This approach preserves quality better than GPTQ for the same average bit width, particularly on capabilities that depend on specific weight patterns. AWQ models typically achieve quality closer to full precision than GPTQ models at the same compression ratio, but inference performance is slightly lower because mixed precision weights introduce additional complexity in GPU kernels.

GGUF, the successor to the GGML format, targets CPU and edge deployment scenarios where GPU acceleration is unavailable or impractical. GGUF supports multiple quantization schemes from 2-bit to 8-bit precision and implements optimizations for CPU inference including vectorization using AVX2 and AVX-512 instructions. Models quantized to GGUF format can run efficiently on standard CPUs, enabling deployment on edge devices, personal computers, and serverless environments without GPU access. Inference speed is dramatically slower than GPU-accelerated serving, typically 5x to 20x slower for comparable hardware costs, but GGUF enables use cases where GPU deployment is impossible. Quality characteristics are similar to GPTQ at equivalent bit widths.

## Quality-Size Tradeoff Analysis

The relationship between quantization bit width and output quality is not linear and varies significantly across model families, model sizes, and task types. Understanding this relationship for your specific fine-tuned model requires empirical evaluation, not theoretical assumptions.

For most general language understanding and generation tasks, 4-bit quantization using GPTQ or AWQ degrades quality minimally. Models quantized to 4 bits typically show perplexity increases of 2% to 5% and downstream task performance degradation of 1 to 3 percentage points across benchmarks like MMLU, HellaSwag, and TruthfulQA. For many production applications, this degradation is invisible to users and meaningless for business outcomes. A customer support chatbot that drops from 89% user satisfaction to 87% user satisfaction is functionally unchanged. A content classification model that drops from 94% accuracy to 92.5% accuracy still delivers value far exceeding manual classification.

Task-specific capabilities, particularly those refined through fine-tuning, often degrade more sharply than aggregate benchmarks suggest. A model fine-tuned for medical coding might maintain 98% accuracy on common diagnosis codes while dropping from 91% to 79% accuracy on rare disease classifications after quantization. A model fine-tuned for contract clause extraction might preserve performance on standard commercial terms while degrading significantly on unusual legal language that appears infrequently. These patterns emerge because fine-tuning often improves model performance on narrow distributions by adjusting specific weight patterns, and quantization disrupts exactly these refined adjustments.

The safe approach is empirical evaluation on held-out test sets that represent your full production distribution, with particular attention to rare but critical cases. Do not evaluate quantized models only on aggregate metrics. Stratify your evaluation data by input characteristics and measure quantization impact on each stratum independently. For the financial services compliance model, this would mean evaluating accuracy separately for each violation type, each transaction category, and each regulatory framework. Only when quantization preserves performance across all critical strata should you deploy it to production.

Three-bit quantization represents a more aggressive tradeoff that is rarely appropriate for fine-tuned production models. While 3-bit GPTQ can reduce model size to roughly 45% of full precision, quality degradation accelerates sharply below 4 bits. Perplexity increases by 8% to 15% and downstream task performance drops by 4 to 8 percentage points are common. For research, experimentation, or non-critical applications, 3-bit quantization enables deployment on severely memory-constrained hardware. For production systems where fine-tuning invested significant effort to achieve specific quality targets, 3-bit quantization almost always erases too much of that investment to justify the infrastructure savings.

## When Quantization is Safe and When It Destroys Value

Quantization safety is not a property of the quantization technique. It is a property of the match between your quality requirements, your model's capabilities, and the specific quantization configuration you choose.

Quantization is safe when your quality requirements include sufficient margin that 2 to 3 percentage point degradation remains well within acceptable bounds, when your workload is dominated by common patterns well-represented in calibration data, and when you can verify empirically that quantization preserves performance on all critical capabilities. A content moderation model with 95% accuracy requirements fine-tuned to achieve 98% accuracy can safely absorb 2 point degradation from quantization. A document summarization system where users tolerate occasional quality variation can safely quantize as long as aggregate quality remains acceptable. A search ranking model with A/B testing infrastructure can validate quantized performance against full-precision performance on live traffic before committing to full rollout.

Quantization is dangerous when your fine-tuning optimized for narrow quality margins, when critical capabilities depend on rare input patterns, when quality requirements are strict and leave little room for degradation, or when you lack comprehensive evaluation data to verify quantization impact. The compliance monitoring model failed every one of these tests: fine-tuning had improved accuracy from 91% to 96%, a narrow 5 point margin that quantization erased partially; critical violations were rare; regulatory requirements left zero room for preventable misses; and evaluation data undersampled the failure mode. Quantizing this model was predictably wrong from the beginning.

Quantization-aware fine-tuning offers a middle path that preserves the cost benefits of quantization while maintaining quality closer to full precision. Instead of fine-tuning at full precision and quantizing afterward, QA fine-tuning simulates quantization during training by quantizing weights and activations in the forward pass while maintaining full precision gradients for the backward pass. The model learns to maintain performance despite quantization noise, resulting in quantized models that often match or exceed the quality of post-training quantized models. QA fine-tuning requires more sophisticated training infrastructure and increases training time by 20% to 40%, but for production models where quantization is necessary but quality cannot degrade, it is the professional approach.

## Quantization Configuration and Calibration

Quantization quality depends critically on calibration dataset selection and quantization hyperparameters. These are not details to delegate to default settings.

Your calibration dataset should represent the full distribution of inputs your model will encounter in production, with particular emphasis on inputs that exercise critical capabilities. For most applications, 2,000 to 10,000 calibration examples provide sufficient coverage. More examples improve quantization quality marginally but increase quantization time significantly. The calibration set should be separate from your training data and your evaluation data to avoid overfitting the quantization to examples the model has already seen.

Stratified sampling ensures that rare but important input patterns appear in calibration data at sufficient frequency to influence quantization. If 2% of your production traffic consists of a specific input type that exercises a critical capability, and you sample calibration data uniformly from production, you will include only 40 examples of this pattern in a 2,000 example calibration set. This is likely insufficient. Stratified sampling oversamples rare categories deliberately, ensuring that calibration includes 200 to 400 examples of each critical pattern even when those patterns are rare in production. The quantization algorithm sees a distorted distribution, but the distortion is intentional and protective.

Quantization hyperparameters control tradeoffs between compression ratio and quality preservation. Most quantization libraries offer options for bit width, group size for weight quantization, and handling of activation outliers. Smaller group sizes generally preserve quality better but reduce compression efficiency. Bit widths below 4 bits increase degradation sharply. Activation outlier handling techniques like smoothing can improve quality for models with extreme activation distributions. The default configurations in popular libraries are reasonable starting points, but production deployment warrants evaluation of alternatives to verify you are operating at the optimal point on the quality-size curve for your specific model and task.

## Deployment Workflow with Quantization

Integrating quantization into your deployment workflow requires deliberate staging and validation, not a one-time conversion step.

Begin by quantizing your fine-tuned model using multiple configurations: 4-bit GPTQ with your production-representative calibration set, 4-bit AWQ with the same calibration set, and potentially 8-bit quantization as a lower-compression baseline. Evaluate all quantized variants against your full test suite, measuring not only aggregate metrics but stratified performance across critical input categories. Compare results to your full-precision baseline and identify any configurations that fail to meet quality requirements.

Deploy the best quantized variant to a staging environment that mirrors production infrastructure and run shadow traffic, processing production requests with both the full-precision and quantized models in parallel while serving results only from the full-precision model to users. Log outputs from both models and compare them systematically. Measure agreement rates, quality metric differences, and latency characteristics. Investigate cases where outputs diverge significantly to understand whether quantization is degrading specific capabilities or whether differences are benign variation.

Only after shadow traffic validation confirms acceptable quality should you route production traffic to the quantized model, and even then, use gradual rollout with automated quality monitoring and rollback triggers. If quality degrades during rollout, revert immediately to full precision and investigate the failure mode. Quantization that worked in offline evaluation but fails in production often indicates that your calibration data or test data failed to represent some important aspect of the production distribution.

Maintain both full-precision and quantized model artifacts in your model registry. Treat the full-precision model as your source of truth and the quantized variant as a deployment optimization. When you iterate on fine-tuning to improve model quality, re-quantize from the new full-precision checkpoint and re-validate quality. Do not fine-tune the quantized model directly, as this compounds quantization noise with training noise and makes it difficult to diagnose quality regressions.

## The Quantization Decision Framework

Before quantizing any fine-tuned model for production, answer four questions explicitly.

First, what is the quantification of cost savings from quantization, accounting for reduced GPU memory requirements, cheaper instance types, and improved throughput? Calculate specific dollar amounts based on your expected traffic and infrastructure costs. If savings are less than $1,000 monthly, quantization is probably not worth the validation effort and ongoing maintenance complexity.

Second, what is your quality margin between current model performance and minimum acceptable performance on all critical capabilities? If this margin is less than 3 percentage points on any critical capability, quantization is high risk. If margin is greater than 5 points across all capabilities, quantization is likely safe with proper validation.

Third, do you have evaluation data that comprehensively represents your production distribution, including rare but critical input patterns? If your evaluation data is incomplete or unrepresentative, quantization is dangerous because you cannot verify its impact reliably.

Fourth, do you have infrastructure to validate quantization quality through shadow traffic or gradual rollout with automated quality monitoring? If you must deploy quantized models directly to production without validation, quantization risk is unacceptable unless quality margins are very large.

Only when you can answer all four questions favorably should you proceed with quantization. The cost savings are real and often large enough to matter significantly for unit economics, but the quality risks are also real and can degrade the capabilities you invested months of effort to build. The next subchapter examines how to deploy fine-tuned models safely through gradual rollout and A/B testing that validates quality on production traffic before committing to full deployment.
