# 3.4 â€” Quality Filtering: Automated and Human Gates on Synthetic Outputs

In November 2025, a financial services company fine-tuned a model to generate investment research summaries from earnings reports. The team generated 35,000 synthetic examples using Claude 3.5 Sonnet as their teacher model. They implemented basic format checks but applied minimal filtering beyond that. Their reasoning: the teacher model was high-quality, so outputs should be high-quality by default. They fed 34,200 examples into fine-tuning after removing only obvious formatting failures. The resulting model launched in December 2025 for internal analyst use. Within two weeks, analysts reported concerning patterns. The model occasionally invented financial metrics that did not appear in source documents, cited revenue figures with incorrect magnitudes, and produced risk assessments that contradicted the actual report content. The head of research suspended model use and commissioned an audit of the training data. The audit team randomly sampled 1,000 examples from the 34,200-example training set. They found that 140 examples contained factual errors of varying severity, 89 examples included hallucinated metrics or figures, 64 examples had logical inconsistencies between summary claims and source content, and 37 examples were off-topic or misunderstood the prompt. Fourteen percent of the training data taught the model to produce unreliable outputs. The company discarded the fine-tuned model and rebuilt their dataset with aggressive multi-stage filtering. The second model, trained on 18,000 heavily filtered examples, outperformed the first model trained on 34,200 lightly filtered examples. Quality mattered more than quantity.

Not all synthetic data is good data. The teacher model that generates your training examples is powerful but imperfect. It hallucinates, misunderstands prompts, produces inconsistent reasoning, and occasionally generates off-topic or policy-violating content. Filtering is not an optional refinement step. It is the critical process that determines whether your fine-tuned model learns reliable patterns or inherits the teacher model's errors at scale. Teams that apply rigorous filtering produce smaller, cleaner datasets that train better models. Teams that skip filtering produce larger, contaminated datasets that train unreliable models. The difference in downstream model quality is not marginal. It is the difference between a model you deploy confidently and a model you pull from production after two weeks.

## Automated Filtering: The First Line of Defense

Automated filtering catches systematic quality issues without human review. These filters run on every generated output before any examples enter your training dataset. Automated filters are fast, cheap, and scalable. They should catch fifty to seventy percent of quality issues in a well-designed pipeline.

Start with length-based filtering. Outputs that are too short lack sufficient detail to teach useful patterns. Outputs that are too long often contain repetition, tangents, or filler content. Analyze your task requirements and production output distributions to set appropriate thresholds. For customer support responses, minimum length might be 40 tokens, maximum 500 tokens. For contract analysis, minimum might be 120 tokens, maximum 800 tokens. For code explanations, minimum might be 80 tokens, maximum 600 tokens. Length filtering is blunt but effective. It removes outputs where the teacher model refused to answer, produced a one-sentence dismissal, or rambled for paragraphs without focus. In typical synthetic datasets, length filtering rejects eight to fifteen percent of outputs.

Implement format compliance checks specific to your task. If your task requires structured output, check that every example contains required sections or fields. A contract analysis output should include clause identification, risk assessment, and explanation. A code review should include issue description, severity rating, and recommendation. A customer support response should address the customer's question and provide next steps. Format compliance checks catch outputs where the teacher model misunderstood instructions, skipped required sections, or produced freeform text instead of structured analysis. These checks reject five to twelve percent of outputs in well-prompted datasets, higher if your prompts are ambiguous or complex.

Add repetition detection to catch template-style outputs. Compute n-gram repetition within each output. Extract all trigrams, quadgrams, and pentagrams from the output. Calculate what percentage of tokens appear in repeated n-grams. If more than twenty percent of tokens participate in repeated trigrams, the output likely contains excessive repetition. If any pentagram appears more than once, the output contains verbatim repetition that indicates template-style generation. Repetitive outputs teach your model to produce formulaic responses rather than adapt reasoning to specific inputs. Repetition filtering rejects three to eight percent of outputs in typical datasets, higher when using high temperature or creative task prompts.

Use vocabulary and style checks to detect outliers. Calculate lexical diversity by dividing unique tokens by total tokens. Outputs with lexical diversity below 0.4 use repetitive vocabulary and limited language. Outputs with lexical diversity above 0.9 may contain random or incoherent tokens. Check for excessive use of filler phrases, hedge words, or placeholder text. Count occurrences of phrases like "it is important to note," "as mentioned earlier," "in conclusion," or "please note that." Outputs containing more than three filler phrases per 200 tokens are often verbose without adding substance. These style checks catch five to ten percent of outputs that passed length and format checks but exhibit poor quality in more subtle ways.

Deploy toxicity and safety classifiers for customer-facing tasks. If your fine-tuned model will interact with users or generate customer-visible content, you cannot include training examples that contain offensive language, biased statements, or policy violations. Run every synthetic output through toxicity detection classifiers. Models like Perspective API or commercial content moderation APIs detect offensive language, personal attacks, and harmful content. Set conservative thresholds. Even mild toxicity in training data can produce occasional toxic outputs from your fine-tuned model, and occasional toxic outputs create severe brand and legal risk. Toxicity filtering typically rejects one to three percent of outputs from well-prompted teacher models, but this small percentage prevents catastrophic deployment failures.

Implement consistency checks for factual tasks. If your task involves factual claims, cross-reference outputs against known facts or check internal consistency. For a dataset of financial summaries, check that mentioned metrics are plausible: revenue should not be negative, profit margins should not exceed 100 percent, year-over-year growth claims should be mathematically consistent with cited figures. For a medical question-answering dataset, check that drug names match real medications, dosage recommendations fall within medically reasonable ranges, and cited medical terms appear in medical ontologies. Factual consistency checks are domain-specific and require reference data or heuristic rules, but they catch hallucinations that other filters miss. These checks reject two to six percent of outputs in factual domains.

Create filter cascades with increasing computational cost. Run cheap filters first, expensive filters last. Length checks take microseconds per output. Format compliance takes milliseconds. Repetition detection takes tens of milliseconds. Toxicity classifiers take hundreds of milliseconds and incur API costs. Factual consistency checks may take seconds if they query external databases. Arrange filters so that outputs rejected by cheap filters never reach expensive filters. In a 50,000-output dataset, if length filtering rejects 5,000 outputs, those 5,000 outputs never incur the cost of running toxicity classifiers or factual checks. Filter cascade design reduces total filtering cost by thirty to fifty percent compared to running all filters on all outputs.

## Self-Consistency Filtering for Hallucination Detection

Self-consistency filtering is a powerful technique for detecting hallucinations and factual errors in domains where correctness matters. The method is simple: generate multiple outputs for the same prompt, compare them for consistency, and reject prompts where outputs disagree on key facts or conclusions.

Generate three to five outputs per prompt using different sampling seeds. For a contract risk analysis task, generate three analyses of the same clause. If all three analyses reach the same risk conclusion and cite similar concerns, the outputs are self-consistent. If one analysis calls the clause low-risk and two call it high-risk, the outputs are inconsistent and suggest the teacher model is uncertain or hallucinating. For a medical question, if three generated answers recommend the same treatment approach and cite similar clinical reasoning, they are consistent. If answers contradict each other on diagnosis or treatment, they are inconsistent and unreliable.

Define what constitutes agreement for your task. Agreement is not verbatim text matching. It is consistency on key claims, conclusions, or recommendations. For contract analysis, agreement means matching risk levels and similar concern categories, not identical wording. For customer support, agreement means consistent policy information and action steps, not identical phrasing. For code review, agreement means identifying the same issues and similar severity assessments. Extract structured information from each output and compare that structured information, not raw text.

Set consistency thresholds based on your tolerance for uncertainty. Require unanimous agreement on critical decisions. If you are generating training data for a model that will make high-stakes decisions, require all three or five outputs to agree before accepting the prompt into your dataset. For lower-stakes tasks, accept majority agreement: three out of five outputs agree, so the prompt is acceptable. Track disagreement patterns. If certain prompt categories show high disagreement rates, those prompts may be too ambiguous, too complex for the teacher model, or outside the teacher model's reliable knowledge domain. Revise or remove those prompts.

Understand the cost-quality tradeoff. Generating five outputs per prompt costs five times more than generating one output. Self-consistency filtering is expensive. Use it selectively for high-value or high-risk tasks where hallucination prevention justifies the cost. For a financial analysis dataset where errors have compliance implications, the cost is justified. For a creative writing dataset where variation is desirable and factual correctness is not critical, the cost may not be justified. Many teams apply self-consistency filtering only to a subset of their prompts: factual or analytical prompts get consistency checks, creative or subjective prompts do not.

Combine self-consistency results with other signals. An output that passes self-consistency is not necessarily high-quality. It may be consistently wrong if the teacher model has incorrect knowledge or consistently generic if the prompt is too vague. Use self-consistency filtering as one gate in a multi-gate cascade. Outputs must be self-consistent and pass format checks and pass length checks and pass reward model scoring to enter the dataset. Self-consistency catches hallucinations. Other filters catch different failure modes.

## Reward Model Scoring for Quality Ranking

Reward model scoring ranks outputs by predicted human preference. If you have trained or have access to a reward model for your task domain, use it to score every synthetic output and filter based on score thresholds. Reward model filtering is particularly effective when your goal is to distill not just task capability but also output quality aligned with human preferences.

A reward model is a classifier trained to predict human preference scores. It takes an input prompt and a model output as input and produces a scalar score representing predicted quality. Higher scores indicate outputs humans would prefer. Lower scores indicate outputs humans would rate poorly. Reward models are typically trained on pairwise preference data: given two outputs for the same prompt, which output do humans prefer? After training on thousands of preference pairs, the reward model learns to generalize quality judgments to new outputs.

If you have a reward model, run it on all generated outputs. For each output, the reward model produces a score, typically normalized to a range like zero to one or minus one to plus one. Set a score threshold based on your quality requirements and dataset size constraints. If you need 20,000 training examples and have generated 40,000 outputs, set the threshold at the median score: keep the top fifty percent. If you need 10,000 examples from 40,000 outputs, set the threshold at the 75th percentile: keep the top twenty-five percent. If you need 30,000 examples from 40,000 outputs, set the threshold at the 25th percentile: reject only the bottom twenty-five percent.

Analyze score distributions by prompt category. If certain prompt categories consistently produce low reward model scores, those prompts are either poorly designed or mismatched to the teacher model's capabilities. If a category of customer support prompts averages a score of 0.4 while other categories average 0.7, investigate why. The prompts may be too complex, the teacher model may lack domain knowledge for that category, or the reward model may be miscalibrated for that category. Use score distributions as diagnostic signals, not just filtering thresholds.

Combine reward model scores with other filters. An output might score highly on the reward model but still contain factual errors, format violations, or policy violations. Use reward model scoring as a quality ranking tool, not a comprehensive quality check. Outputs must pass format compliance, safety checks, and factual consistency filters before reward model scoring. Then among outputs that pass all prior filters, use reward model scores to rank and select the highest-quality subset.

If you do not have a reward model, consider whether training one is justified. Training a reward model requires human preference data: hundreds or thousands of examples where humans compare two outputs and indicate which is better. This data collection is expensive. If you are generating a one-time dataset of 10,000 examples for a narrow task, training a reward model is likely not cost-effective. If you are generating hundreds of thousands of examples across multiple tasks or plan to generate synthetic data repeatedly over time, training a reward model is a worthwhile infrastructure investment. Some teams use general-purpose reward models trained on broad human preference data, such as reward models released by research labs or model providers. These general models provide useful signal for tasks like summarization, question answering, or customer support where quality criteria overlap with general helpfulness preferences.

## Human Filtering: Sampling Review and Rejection Criteria

Automated filtering catches systematic issues. Human filtering catches subtle issues that automated systems miss: nuanced factual errors, context-dependent inappropriateness, logical inconsistencies, and quality degradation that falls below numerical thresholds but is obvious to human reviewers.

You cannot manually review all outputs. You review a statistically representative sample. For datasets under 10,000 examples, review ten to fifteen percent. For datasets between 10,000 and 50,000 examples, review five to ten percent. For datasets above 50,000 examples, review three to five percent. Use stratified sampling to ensure representation across all prompt categories, output lengths, and automated filter scores. Stratified sampling prevents bias where you only review common cases and miss rare but important edge cases.

Create explicit rejection criteria for reviewers. Reviewers need clear, specific definitions of what constitutes a quality issue. Criteria for a customer support dataset include: factually incorrect policy information, inappropriate tone for the customer's emotional state, failure to address the actual question asked, recommendations that violate company policy, responses that escalate conflict rather than resolve it, or responses containing placeholder text or incomplete information. Criteria for a contract analysis dataset include: incorrect clause identification, legal reasoning that contradicts jurisdictional law, failure to cite relevant clause text, risk assessment inconsistent with clause content, or analysis that ignores material terms. Document criteria in a review guide with positive and negative examples.

Measure inter-annotator agreement to ensure consistency. Have two or three reviewers independently review the same 200 to 300 examples. Calculate agreement rates on accept versus reject decisions. Cohen's kappa is a standard metric: values above 0.75 indicate good agreement, values between 0.60 and 0.75 indicate moderate agreement, values below 0.60 indicate poor agreement requiring criteria refinement or reviewer retraining. Analyze disagreements to understand their sources. If reviewers disagree frequently on a specific rejection criterion, that criterion is too ambiguous. Provide additional examples or decision rules until agreement improves.

Track rejection reasons to diagnose pipeline issues. When reviewers reject examples, they categorize the rejection: factual error, format violation, tone issue, off-topic response, policy violation, logical inconsistency, or insufficient detail. Aggregate rejection reasons across the review sample. If twenty percent of rejections stem from factual errors in a specific domain, your teacher model may lack reliable knowledge for that domain or your prompts are too complex. If fifteen percent stem from tone issues, your generation prompts need explicit tone instructions. If ten percent stem from format violations that passed automated format checks, your automated format checks are too lenient and need refinement.

Use human review as a calibration step for automated filters. If human reviewers reject outputs that scored highly on your reward model, your reward model is miscalibrated. If reviewers accept outputs that your repetition filter rejected, your repetition threshold is too strict. Human review provides ground truth for tuning automated filter thresholds. After each human review cycle, adjust automated filter settings based on review findings and re-run filtering. Iterative tuning aligns automated filters with human quality judgments.

Understand normal rejection rates. In a well-designed pipeline with proper automated filtering, human reviewers should reject five to fifteen percent of outputs that passed automated filters. If human rejection is below five percent, your automated filtering may be too strict and you are over-filtering, which reduces training data volume unnecessarily. If human rejection exceeds fifteen percent, your automated filtering is too lenient and you are wasting human review budget on examples that should have been caught earlier. Tune automated filter thresholds to target the optimal human rejection rate for your quality standards and review budget.

## The Filter Cascade Architecture

Filter cascades are ordered sequences of quality checks with increasing cost and specificity. Designing an effective cascade maximizes quality while minimizing cost.

Order filters by cost and rejection rate. Run high-rejection, low-cost filters first. Length filters reject ten to fifteen percent of outputs at negligible cost per output. Run them first. Format compliance checks reject five to twelve percent at low cost. Run them second. Repetition detection rejects three to eight percent at moderate cost. Run it third. Toxicity classifiers reject one to three percent at higher cost. Run them fourth. Reward model scoring may reject twenty to fifty percent depending on your threshold, but it is expensive. Run it after cheaper filters have eliminated obviously bad outputs. Self-consistency checks are most expensive. Run them last, only on outputs that passed all prior filters.

Apply domain-specific filters at appropriate points. Factual consistency checks require domain knowledge and reference data. Place them after format and style filters but before expensive preference-based scoring. Policy compliance checks require task-specific rules. Place them early if they are fast rule-based checks, later if they require classifier inference. Safety checks are non-negotiable for customer-facing tasks. Place them early enough that no unsafe content reaches later stages, but late enough that you do not waste safety classifier costs on outputs that will be rejected for format issues.

Implement short-circuit logic. Once an output fails any filter, stop processing it. Do not run subsequent filters. This logic is essential for cost control. If an output fails the length check in stage one, do not run the toxicity classifier in stage four. Short-circuit logic combined with cost-ordered cascades reduces total filtering cost by forty to sixty percent compared to running all filters on all outputs.

Log filter decisions at every stage. For each output, record which filters it passed and which filter rejected it if any. Aggregate logs to analyze filter effectiveness. If your repetition filter rejects 500 outputs but human review of a sample finds that ninety percent of those rejections were correct, the filter is effective. If only fifty percent were correct, the filter is too strict and needs threshold adjustment. Filter logs are diagnostic data that drive continuous pipeline improvement.

Design cascades with tunable thresholds. Do not hardcode threshold values. Make them configuration parameters you can adjust based on dataset quality analysis. After your first pipeline run, review human validation results, analyze filter rejection distributions, and adjust thresholds for the next run. A length filter threshold might start at 40 tokens minimum, then adjust to 50 tokens after analysis shows that outputs between 40 and 50 tokens are frequently rejected by human reviewers. Iterative threshold tuning improves filter precision and recall over multiple pipeline runs.

## Aggressive Filtering Improves Downstream Model Quality

The most counterintuitive finding in synthetic data generation is that aggressive filtering improves model quality even though it reduces dataset size. Teams instinctively want larger datasets. Filtering feels like throwing away valuable training data. But contaminated data is not valuable. It is actively harmful.

A model fine-tuned on 15,000 clean examples outperforms a model fine-tuned on 30,000 examples where twenty percent contain quality issues. The contaminated dataset teaches the model incorrect patterns, inconsistent reasoning, and unreliable behavior. The model learns that sometimes it should hallucinate facts, sometimes it should produce off-topic responses, sometimes it should use poor tone. This inconsistency degrades model reliability in production. Users cannot predict when the model will perform well and when it will fail. Inconsistent models are unusable in high-stakes applications.

Set quality thresholds based on your deployment risk tolerance. For experimental models used in low-stakes internal tools, moderate filtering with ten to fifteen percent contamination may be acceptable. For models deployed in customer-facing applications, strict filtering with contamination below five percent is essential. For models used in regulated domains like finance, healthcare, or legal, aggressive filtering with contamination below two percent is the standard. The cost of an error in production far exceeds the cost of generating additional synthetic data to replace filtered examples.

Measure contamination through human validation. After filtering, sample 500 to 1,000 examples from your final dataset and have expert reviewers assess them for quality issues. Calculate the contamination rate: percentage of sampled examples containing factual errors, logical inconsistencies, policy violations, or other quality issues. If contamination exceeds your target, increase filter strictness and regenerate data to compensate for higher rejection rates. If contamination is well below your target, you may be over-filtering and can slightly relax thresholds to increase yield.

Understand the quality-quantity tradeoff curve. Adding more training data improves model performance with diminishing returns. Going from 5,000 to 10,000 examples produces noticeable improvement. Going from 30,000 to 40,000 examples produces marginal improvement. Going from 50,000 to 60,000 examples produces minimal improvement. Quality has consistent returns: every percentage point reduction in contamination improves model reliability. At large dataset sizes, improving quality through stricter filtering provides more performance gain than increasing quantity through lenient filtering. Teams that understand this tradeoff prioritize quality over volume.

## What Rejection Rates Are Normal

Rejection rates vary by task complexity, teacher model quality, and filtering strictness, but patterns emerge across successful synthetic data pipelines.

For well-designed prompts and appropriate teacher models, expect total rejection rates between twenty-five and forty-five percent from raw generation to final validated dataset. Length filtering rejects eight to fifteen percent. Format compliance rejects five to twelve percent. Repetition detection rejects three to eight percent. Style and vocabulary checks reject five to ten percent. Toxicity and safety filters reject one to three percent. Factual consistency or self-consistency checks reject five to fifteen percent depending on task and whether you use them. Reward model filtering depends on your threshold: keeping the top fifty percent means rejecting fifty percent at that stage, keeping the top seventy-five percent means rejecting twenty-five percent. Human validation rejects five to fifteen percent of outputs that passed automated filtering. Filters overlap: an output might fail both length and format checks. Actual rejection rates depend on filter cascade design.

If your total rejection rate is below twenty percent, you are likely under-filtering. Either your teacher model is exceptionally good, your task is exceptionally simple, or your filters are too lenient. Review human validation rejection rates. If humans reject more than fifteen percent of outputs that passed automated filtering, your automated filters are too lenient. If your total rejection rate exceeds sixty percent, investigate root causes. High rejection rates indicate prompt design issues, teacher model mismatch to task, or overly strict filters. If length filters reject thirty percent of outputs, your generation prompts or max token settings need adjustment. If format compliance rejects twenty-five percent, your generation prompts are not instructing the teacher model clearly enough.

Track rejection rates across pipeline iterations. First-iteration rejection rates are typically higher as you discover prompt issues and calibrate filters. Second-iteration rejection rates drop as prompts improve and filters align with task requirements. By the third iteration, rejection rates stabilize. Stable rejection rates between twenty-five and forty-five percent indicate a mature pipeline producing consistent quality.

## Building Quality Into the Process

Quality filtering is not a single step you run once. It is a continuous process embedded throughout your synthetic data pipeline. Every stage of generation produces logs. Every filter produces metrics. Every human review produces feedback. Effective teams treat filtering as an iterative quality improvement system, not a one-time cleanup operation.

After each pipeline run, analyze filter statistics, review human validation results, examine contamination rates, and identify the most common quality issues. Use these findings to improve earlier stages. If factual errors are common, revise prompts to be more explicit about factual grounding or switch to a teacher model with better knowledge for your domain. If tone issues are common, add explicit tone instructions to generation prompts. If format violations are common, simplify your output format or provide more examples in prompts.

Build feedback loops between filtering and prompt design. Prompt designers should review filtered outputs to understand what fails and why. Reviewers should share rejection patterns with prompt designers. This collaboration prevents adversarial dynamics where prompt designers maximize generation yield and filter designers maximize rejection. Instead, both teams optimize for the same goal: high-quality validated data.

Invest in filtering infrastructure. Build reusable filter components that can be configured for different tasks. Build logging and metrics dashboards that surface filter performance in real-time. Build review interfaces that streamline human validation and capture structured feedback. Teams that treat filtering as infrastructure development produce better datasets faster and iterate more effectively.

Quality filtering is not overhead. It is the process that ensures your fine-tuned model inherits the teacher model's capabilities without inheriting its errors. The financial services company that opened this subchapter learned this lesson the expensive way. Their second pipeline, with aggressive multi-stage filtering, produced a dataset half the size of their first dataset but double the quality. The fine-tuned model trained on that smaller, cleaner dataset outperformed the first model and deployed successfully to production. Quality always matters more than quantity.

You have built a multi-stage pipeline. You have implemented automated and human filtering. You have produced a clean, validated dataset. The next question is how to use that dataset to actually improve model quality through distillation. Distillation is not just fine-tuning on synthetic data. It is a specific training approach that transfers capabilities from a large teacher model to a smaller, faster student model. The next subchapter covers distillation strategy: when to distill, how to choose student models, and how to measure distillation success.
