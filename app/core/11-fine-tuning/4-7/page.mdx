# 4.7 — Continued Pre-Training: Domain Vocabulary and Knowledge Injection

**Supervised fine-tuning teaches task execution, not domain knowledge.** You cannot fine-tune a model to know what it has never learned. A legal technology company spent four months and $47,000 in mid-2025 fine-tuning GPT-4 on 12,000 annotated patent analysis examples. The resulting model followed instructions perfectly and formatted outputs correctly, but it confused "embodiment" with "implementation," misunderstood the difference between "comprising" and "consisting of" in patent claim language, and failed to recognize standard patent office form paragraphs. The model was unusable for professional patent work. The root cause was not poor annotation quality or insufficient training examples. It was that supervised fine-tuning reshapes how a model applies knowledge it already has—it cannot inject knowledge that does not exist in the base model. When your domain involves specialized terminology, dense technical concepts, or knowledge underrepresented in general web text, you need continued pre-training before you attempt supervised fine-tuning. This is not optional. It is the foundational step that makes task training possible.

The problem was not instruction-following. The problem was that the base model lacked the specialized vocabulary, conceptual relationships, and domain knowledge that patent attorneys spend years learning. Supervised fine-tuning cannot inject knowledge that does not exist in the base model. It can only reshape how the model applies knowledge it already has. When your domain involves specialized terminology, dense technical concepts, or knowledge that is underrepresented in general web text, you need continued pre-training before you attempt supervised fine-tuning.

## What Continued Pre-Training Is

Continued pre-training extends the base model's pre-training phase with domain-specific raw text. Instead of training on instruction-response pairs, you train on unlabeled documents, articles, manuals, and corpus text using the same next-token prediction objective that the base model was originally trained with. The model learns vocabulary, entity relationships, domain patterns, and factual knowledge by reading and predicting large volumes of domain text. This builds the foundation that supervised fine-tuning will later refine into task performance.

Continued pre-training sits between base model training and supervised fine-tuning in the adaptation pipeline. The base model is a generalist. Supervised fine-tuning teaches task structure and output format. Continued pre-training is the missing middle layer that teaches domain fluency. You run CPT first, then run SFT on the CPT-adapted model. The result is a model that knows both the domain and the task, not just the task alone.

The legal tech company restarted their effort with continued pre-training. They collected 840,000 patent applications from the USPTO public database, 120,000 patent prosecution history documents, 15,000 patent office manual sections, and 6,000 Federal Circuit patent law opinions. This corpus totaled 2.1 billion tokens of raw legal text. They ran continued pre-training on a Llama 4 Scout 70B base model for 3,200 steps with a learning rate of 1e-5, using the standard causal language modeling objective. The model learned patent terminology, legal citation formats, claim structure conventions, and the relationships between patent sections. After CPT completed, they ran supervised fine-tuning on the same 12,000 annotated examples they had used before. This time the model performed at professional quality. It understood claim language nuances, recognized standard form paragraphs, and applied patent law concepts correctly. The CPT phase cost $18,000 in compute but made the entire adaptation pipeline viable.

## When Continued Pre-Training Is Necessary

You need continued pre-training when your domain has specialized vocabulary that is rare in general web text. Medical terminology, legal jargon, engineering notation, scientific nomenclature, and industry-specific acronyms all fall into this category. If your domain uses words that a base model has seen fewer than a few hundred times during pre-training, the model's representations for those words are weak and unreliable. CPT strengthens those representations by exposing the model to thousands or millions of additional occurrences in context.

You need CPT when your domain has dense factual knowledge that cannot be captured in instruction examples. Drug interaction mechanisms, regulatory compliance requirements, materials science properties, or financial instrument structures all require extensive factual grounding. Supervised fine-tuning examples show the model how to apply facts, but they do not efficiently teach the facts themselves. A single SFT example might reference a drug interaction once, but a CPT corpus can show that interaction in hundreds of research papers, package inserts, and clinical trial reports. The repetition across contexts builds robust factual knowledge.

You need CPT when your domain has specialized formatting or structural conventions. Legal citations, chemical formulas, medical coding systems, or engineering drawing annotations all have precise syntactic rules. Base models have seen some examples of these formats, but not enough to reliably generate them. CPT training on large volumes of correctly-formatted domain text teaches the model these conventions implicitly. After CPT, the model generates citations, codes, or formulas that conform to domain standards without needing explicit formatting instructions in every SFT example.

You do not need CPT when your domain uses general vocabulary to describe specialized processes. A customer support model, a content moderation model, or a meeting summarization model operates on common language. The concepts may be specific to your company, but the words are standard English. Supervised fine-tuning alone is sufficient. You also do not need CPT when your task is purely stylistic or structural. Reformatting outputs, changing tone, or adjusting verbosity does not require domain knowledge injection. SFT handles these cases efficiently.

## Data Requirements for Continued Pre-Training

Continued pre-training requires large volumes of raw text. The minimum effective corpus size is typically 100 million tokens. Below that threshold, the model does not see enough repetition to build robust representations for domain vocabulary and concepts. More effective CPT runs use 500 million to 5 billion tokens. The legal tech company's 2.1 billion token corpus was in the effective range. A medical AI company running CPT on clinical notes and research papers used 4.3 billion tokens. A financial services firm training on analyst reports and regulatory filings used 1.8 billion tokens.

The text does not need to be annotated, labeled, or formatted. You use raw documents exactly as they exist in your domain. Patent applications, research papers, internal documentation, technical manuals, industry publications, and regulatory filings are all valid CPT data sources. The quality bar is lower than for supervised fine-tuning data, but the text should still be well-written and representative of the domain you want the model to learn. Poorly-written or off-topic text dilutes the training signal and wastes compute.

You can mix multiple document types in the CPT corpus, but you should track the distribution. If 90% of your corpus is one document type and 10% is another, the model will learn the majority type more thoroughly. The legal tech company's corpus was 78% patent applications, 12% prosecution histories, 7% patent office manuals, and 3% court opinions. They intentionally overweighted patent applications because that was the primary document type their product would process. A medical company training on both clinical notes and research papers might use a 60-40 split if both document types are equally important to downstream tasks.

Data cleaning is still necessary, but less intensive than for SFT. You should remove duplicate documents, filter out non-domain content, and strip metadata or formatting artifacts that interfere with language modeling. The legal tech company deduplicated their patent corpus at the document level, removed boilerplate headers and footers, and filtered out non-English applications. They did not need to clean grammar, rewrite sentences, or annotate anything. The raw text was sufficient.

## How CPT Differs from Supervised Fine-Tuning

Continued pre-training uses the next-token prediction objective. The model reads a sequence of tokens and predicts the next token at each position. This is the same objective used during base model pre-training. There are no instruction prompts, no input-output pairs, and no task-specific structure. The model simply learns to predict domain text, which forces it to internalize vocabulary, grammar, and knowledge patterns.

Supervised fine-tuning uses instruction-following pairs. Each example has a prompt and a desired completion. The model learns to map specific inputs to specific outputs, which teaches task structure and behavior. SFT is targeted and efficient for behavior shaping, but it does not efficiently teach foundational knowledge. CPT is the opposite: it efficiently teaches knowledge but does not teach specific behaviors.

The learning rate for CPT is much lower than for SFT. CPT typically uses learning rates between 1e-5 and 5e-6, while SFT uses 1e-4 to 5e-4. The low learning rate prevents the model from forgetting general knowledge during domain adaptation. Base models are trained on trillions of tokens of diverse text. CPT adds a few billion tokens of domain text on top of that foundation. If you use a high learning rate, the model overfits to the domain corpus and loses general capabilities. A low learning rate allows the model to incorporate domain knowledge without catastrophic forgetting.

The training duration for CPT is much longer than for SFT. SFT typically runs for 1 to 3 epochs over a dataset of a few thousand examples, which might be 500 to 2,000 training steps. CPT runs for a fraction of an epoch over a multi-billion token corpus, which might be 2,000 to 10,000 steps. The legal tech company's 3,200-step CPT run took 11 days on eight A100 GPUs. Their subsequent SFT run took 6 hours on the same hardware. CPT is the expensive phase, both in data preparation and compute time.

## The CPT-Then-SFT Pipeline

The standard adaptation pipeline for domain-heavy applications is base model, then CPT, then SFT. You start with a base model that has general language understanding. You run continued pre-training on domain text to inject vocabulary and knowledge. You then run supervised fine-tuning on task examples to teach specific behaviors. Each phase builds on the previous phase.

You can optionally run a second SFT phase for preference tuning or safety alignment after the task SFT phase. Some teams run CPT, then task SFT, then DPO or RLHF for quality refinement. The legal tech company ran CPT, then SFT on task examples, then a small DPO phase on 800 preference pairs to improve claim language style. Each phase was sequential, and each checkpoint was saved for ablation testing.

You should validate after each phase. After CPT, test the model on domain knowledge probes. Ask it to define specialized terms, complete domain-specific sentences, or generate domain text snippets. The model should demonstrate fluency in domain vocabulary and concepts even without task-specific instructions. The legal tech company tested their post-CPT model by prompting it to draft patent claim language and checking whether it used correct terminology and structure. The model was not yet instruction-tuned for specific tasks, but it could generate plausible patent text when prompted. This confirmed that CPT had injected the necessary domain knowledge.

After SFT, test the model on task performance. The post-SFT model should handle instructions correctly and produce task-appropriate outputs. Compare the post-CPT-plus-SFT model to a baseline that skips CPT and runs SFT directly on the base model. The CPT-augmented model should outperform the baseline on domain-specific quality metrics, even if instruction-following accuracy is similar. The legal tech company found that their CPT-then-SFT model had 89% correctness on patent claim language versus 43% for the SFT-only baseline. Both models followed instructions equally well, but only the CPT-augmented model understood patent law.

## Cost and Infrastructure Requirements

Continued pre-training is expensive. Training a 7B parameter model on 1 billion tokens for one epoch costs approximately $2,000 to $4,000 in cloud GPU time, depending on your provider and instance type. Training a 70B parameter model on the same data costs $15,000 to $25,000. The legal tech company's 70B CPT run cost $18,000. A medical AI company running CPT on a 13B model with 4.3 billion tokens spent $11,000. These costs are 10x to 50x higher than typical SFT runs.

You need multi-GPU infrastructure. A single GPU can run CPT, but training time becomes prohibitive. The legal tech company used eight A100 80GB GPUs and completed CPT in 11 days. On a single GPU, the same run would have taken 88 days. Most production CPT runs use between 4 and 32 GPUs, depending on model size and urgency. Distributed training introduces complexity: you need gradient synchronization, checkpoint sharding, and careful learning rate tuning to avoid instability.

Data storage and preprocessing are non-trivial. A 2 billion token corpus is approximately 8GB of raw text, but tokenized and preprocessed it expands to 30GB to 50GB depending on your tokenizer and data format. You need fast storage for training throughput. The legal tech company stored their preprocessed corpus on NVMe SSDs attached to their training cluster to avoid I/O bottlenecks. A cloud-based team would use high-performance object storage or persistent SSD volumes.

You should budget for multiple CPT runs. Your first run will likely use suboptimal hyperparameters or corpus composition. You will need to iterate on learning rate, training steps, and data mix. The legal tech company ran three CPT experiments before settling on their final configuration. Each experiment cost $12,000 to $18,000. Total CPT R&D cost was $47,000 before they included SFT development. This is a significant investment, but it is unavoidable when domain knowledge injection is a hard requirement.

## Evaluating CPT Effectiveness

You evaluate CPT success by testing domain fluency before running SFT. Prompt the post-CPT model to generate domain text, complete domain-specific sentences, or answer domain knowledge questions. The model should produce coherent, terminology-rich outputs that demonstrate understanding of domain concepts. You are not testing task performance yet. You are testing whether the model has internalized domain vocabulary and knowledge.

The legal tech company tested their post-CPT model with prompts like "Draft a patent claim for a machine learning-based fraud detection system" and "Explain the difference between a method claim and a system claim." The post-CPT model generated claim language with correct structure and terminology. It explained legal concepts accurately. A baseline model without CPT generated generic text that looked superficially similar but used incorrect terminology and missed critical legal distinctions.

You can use perplexity on held-out domain text as a quantitative metric. Perplexity measures how well the model predicts domain text. Lower perplexity indicates better domain fluency. The legal tech company measured perplexity on 10,000 held-out patent documents. Their post-CPT model achieved a perplexity of 12.3, compared to 31.7 for the base model. This confirmed that the model had learned to predict patent text more accurately.

You can also use domain-specific knowledge probes. Create a set of multiple-choice or short-answer questions that test domain facts, terminology, and reasoning. The post-CPT model should answer these questions more accurately than the base model. A medical AI company created 500 knowledge probes covering drug interactions, diagnostic criteria, and medical coding. Their post-CPT model answered 78% correctly, versus 41% for the base model. This quantified the knowledge gain from CPT.

## When to Skip CPT and Use Retrieval Instead

Continued pre-training is not always the right solution for knowledge gaps. If your domain knowledge is rapidly changing, extremely large, or highly specific to individual users, retrieval-augmented generation is a better fit. CPT bakes knowledge into model weights, which means updating that knowledge requires retraining. Retrieval keeps knowledge external and updateable.

If your domain corpus is smaller than 100 million tokens, CPT is unlikely to be effective. The model needs substantial repetition to learn vocabulary and concepts. A corpus of 10 million tokens is too small. You would be better served by using retrieval to provide domain context in prompts or by running SFT with carefully-designed examples that explicitly teach key terms and facts.

If your domain knowledge is factual and verifiable, retrieval is safer. CPT can cause the model to hallucinate plausible-sounding but incorrect domain facts. A model trained on medical literature might confidently state a drug interaction that is not real but sounds medically plausible. Retrieval allows you to ground outputs in verified sources and cite references. CPT does not provide this traceability.

If your domain knowledge is private or proprietary and you cannot run training infrastructure securely, retrieval is more practical. CPT requires sending your entire corpus to training infrastructure and storing model checkpoints. Retrieval requires only sending context snippets at inference time, which can be done with tighter security controls. A financial services firm with highly sensitive analyst reports chose retrieval over CPT because they could not justify the security risk of uploading 2 billion tokens of proprietary text to a training cluster.

## Combining CPT with Retrieval

You do not need to choose exclusively between CPT and retrieval. Many production systems combine both approaches. CPT provides foundational domain fluency and common terminology, while retrieval provides specific facts, recent updates, and source attribution. A medical AI company ran CPT on 3.8 billion tokens of medical literature to teach their model anatomy, pharmacology, and clinical terminology. They then deployed the CPT-adapted model with retrieval over a database of current drug monographs, recent clinical trials, and FDA safety alerts. The CPT phase ensured the model understood medical language and could generate clinically appropriate outputs. The retrieval phase ensured the model cited current evidence and did not hallucinate outdated drug information.

This hybrid architecture works particularly well when your domain has stable core knowledge and dynamic peripheral knowledge. The core knowledge is suitable for CPT: terminology, conceptual relationships, and fundamental principles that do not change frequently. The peripheral knowledge is suitable for retrieval: recent research findings, regulatory updates, product-specific details, and user-specific context. A legal tech company used CPT to teach case law interpretation and legal reasoning patterns, then used retrieval to provide jurisdiction-specific statutes and recent court rulings. The model could reason like a lawyer but cited current law.

The hybrid approach also addresses the hallucination risk of pure CPT. When the model is uncertain or needs to provide verifiable facts, the retrieval system supplies grounded context. When the model needs to generate domain-fluent prose or apply learned patterns, the CPT-adapted weights provide that capability. You design your prompts to indicate when retrieval is necessary and when the model can rely on internalized knowledge. A financial services AI prompted the model with "Based on the provided analyst reports, summarize the outlook for this sector" when retrieval was active, and "Explain the typical structure of a convertible bond offering" when internalized knowledge was sufficient.

## Data Contamination and CPT

When you run continued pre-training, you must ensure that your CPT corpus does not contain examples from your evaluation set. If you train on data that overlaps with your test cases, your evaluation metrics will be artificially inflated. The model will have memorized test inputs during CPT and will appear to perform well when in reality it is simply recalling training data. This is data contamination, and it invalidates your evaluation.

The legal tech company discovered contamination in their second CPT run. They had collected 840,000 patent applications for CPT and randomly sampled 500 applications for their SFT evaluation set. After CPT and SFT, their model scored 96% on the evaluation set, which seemed exceptional. They manually reviewed model outputs and noticed that the model was generating claim language that exactly matched specific patents. They checked their CPT corpus and found that 47 of their 500 evaluation examples were in the CPT training set. They had contaminated their evaluation. They removed all evaluation examples from the CPT corpus, retrained CPT, and re-evaluated. The actual score was 89%, still strong but not the inflated 96%.

You prevent contamination by splitting your data before CPT begins. Separate your corpus into CPT training data and held-out evaluation data before any training starts. Never modify this split. The legal tech company adopted a strict split protocol: they divided their 840,000 patents into 838,000 for CPT and 2,000 held out. From the 2,000 held-out patents, they sampled 500 for SFT evaluation and 1,500 for future use. The CPT corpus never saw any of the 2,000 held-out documents.

You should also deduplicate your CPT corpus to avoid training on near-duplicate documents. If the same patent application appears twice with minor formatting differences, the model will see it multiple times and may memorize it. The legal tech company used MinHash-based deduplication to identify and remove documents with more than 80% content overlap. This reduced their corpus from 840,000 to 782,000 documents but ensured that each unique document appeared only once.

## Monitoring CPT Training

Continued pre-training runs for thousands of steps over many days. You cannot wait until training completes to discover that something went wrong. You need to monitor training continuously and catch problems early. The three critical metrics are training loss, validation loss, and perplexity on held-out domain text.

Training loss should decrease smoothly and consistently. If training loss stops decreasing or starts increasing, your learning rate may be too low, your data may be corrupted, or your model may have converged. The legal tech company monitored training loss every 50 steps. For the first 1,200 steps, loss decreased from 3.2 to 1.8. From step 1,200 to step 2,400, loss decreased from 1.8 to 1.4. From step 2,400 to step 3,200, loss decreased from 1.4 to 1.2. The smooth decrease indicated healthy training.

Validation loss should decrease in parallel with training loss, though it may plateau earlier. If validation loss stops decreasing while training loss continues to decrease, the model is starting to overfit to the CPT corpus. This is less common in CPT than in SFT because CPT uses massive datasets, but it can happen if your corpus is small or repetitive. A medical AI company ran CPT on a 600 million token corpus, which was at the lower end of effective size. They saw validation loss plateau at step 4,000 while training loss continued decreasing until step 6,000. They stopped training at step 4,000 and used that checkpoint.

Perplexity on held-out domain text measures how well the model predicts domain language. You compute perplexity on a small set of domain documents that the model has never seen. Perplexity should decrease as CPT progresses. The legal tech company computed perplexity on 500 held-out patents every 200 steps. Perplexity started at 31.7 for the base model, decreased to 18.4 at step 1,000, 14.6 at step 2,000, and 12.3 at step 3,200. The consistent decrease confirmed that the model was learning to predict patent language more accurately.

You should also monitor gradient norms to detect instability. If gradient norms spike suddenly, your learning rate may be too high or your data may contain corrupted examples. The legal tech company set an alert threshold at gradient norm 10.0. If any training step produced gradients above that threshold, training paused and they investigated. This happened once at step 1,840 when a malformed patent document caused a gradient spike. They removed the document from the corpus and resumed training.

## Checkpointing and Recovery

Continued pre-training runs for days or weeks. Hardware failures, preemption on cloud infrastructure, and unexpected errors can interrupt training. You need a checkpointing strategy that allows you to resume training without losing progress.

Save checkpoints every 200 to 500 steps. Each checkpoint should include model weights, optimizer state, and the current training step. The legal tech company saved checkpoints every 200 steps. Each checkpoint was 140GB for their 70B model: 135GB for model weights and 5GB for optimizer state. They stored checkpoints on redundant cloud storage with automatic replication.

Keep the last three to five checkpoints. Older checkpoints can be deleted to save storage costs. If your current checkpoint is corrupted, you can resume from the previous checkpoint with minimal lost progress. The legal tech company kept the last five checkpoints, which cost $120 per month in storage. When training was interrupted at step 2,780 due to a cloud instance failure, they resumed from the step 2,600 checkpoint and only lost 180 steps of progress.

Test checkpoint resumption before starting long CPT runs. Run a short CPT job, save a checkpoint, stop training, and resume from the checkpoint. Verify that training loss continues smoothly from the checkpoint. The legal tech company tested resumption on a 100-step trial run and discovered that their optimizer state was not loading correctly. They fixed the bug before starting their 3,200-step production run, which saved them from discovering the bug midway through an 11-day training job.

## CPT for Multilingual Models

Continued pre-training is particularly valuable for adapting multilingual base models to low-resource languages or language varieties. Base models are trained on web text, which is dominated by English, Chinese, and a few other high-resource languages. If you need strong performance in Vietnamese, Swahili, or regional Spanish varieties, the base model's representation of those languages is weak. CPT on language-specific corpora strengthens those representations.

A translation services company needed a model for Brazilian Portuguese legal translation. Base multilingual models knew Portuguese, but they did not know Brazilian legal terminology or syntactic conventions. The company collected 1.2 billion tokens of Brazilian legal text: court decisions, legal briefs, statutes, and regulatory filings. They ran CPT on a multilingual Llama 4 model for 4,500 steps. The post-CPT model generated Brazilian Portuguese legal text with correct terminology, formal register, and citation formatting. They then ran SFT on 8,000 translation pairs. The final model outperformed pure SFT by 23 percentage points on legal translation quality.

CPT for multilingual adaptation requires language-balanced validation. If your CPT corpus is 90% Portuguese and 10% English, but your downstream task is equally English and Portuguese, you should validate on both languages. The translation company validated on 1,000 held-out Portuguese legal documents and 1,000 English legal documents. Portuguese perplexity decreased from 42.1 to 15.3. English perplexity increased slightly from 8.2 to 9.1, indicating mild forgetting of English. They accepted this trade-off because Portuguese performance was the priority, and the English degradation was minor.

You can run multilingual CPT with interleaved language data. Instead of training on 1 billion Portuguese tokens followed by 200 million English tokens, you can interleave the languages throughout training. This reduces language-specific forgetting. The translation company tested interleaved training by shuffling their corpus at the document level. Portuguese and English documents were randomly ordered. This reduced English perplexity increase to 8.6, nearly unchanged from baseline, while maintaining Portuguese perplexity at 15.8.

## Domain Adaptation Versus Domain Specialization

Continued pre-training serves two related but distinct purposes: domain adaptation and domain specialization. Domain adaptation teaches a generalist model to handle a new domain while maintaining general capabilities. Domain specialization teaches a model to excel in one domain at the expense of general capabilities. You choose between these goals based on your deployment requirements.

Domain adaptation uses moderate CPT corpus sizes, typically 500 million to 2 billion tokens, and low learning rates, typically 5e-6 to 1e-5. The goal is to add domain knowledge without forgetting general knowledge. After CPT, the model should handle both domain-specific tasks and general tasks. A legal tech company wanted their model to draft patent claims but also answer general knowledge questions and write emails. They ran CPT on 1.2 billion tokens of patent text with learning rate 8e-6. The post-CPT model scored 84% on patent tasks and maintained 96% of base model performance on general tasks.

Domain specialization uses large CPT corpus sizes, typically 3 billion to 10 billion tokens, and higher learning rates, typically 1e-5 to 3e-5. The goal is to maximize domain performance, and general capability loss is acceptable. After CPT, the model excels in the domain but may perform poorly on general tasks. A medical research company wanted their model to analyze genomics papers and predict protein interactions. They ran CPT on 6.8 billion tokens of genomics literature with learning rate 2e-5. The post-CPT model scored 91% on genomics tasks but dropped to 62% on general knowledge questions. They accepted this trade-off because the model would only be used for genomics research.

You should choose adaptation when your model serves mixed use cases. If users ask both domain-specific and general questions, you need adaptation. You should choose specialization when your model serves a single domain exclusively. If users only ever ask domain questions, specialization produces higher domain performance.

## Mixing Domains in CPT

You can run CPT on multiple domains simultaneously by mixing domain corpora. A healthcare technology company wanted their model to handle both clinical notes and medical imaging reports. They collected 2.4 billion tokens of clinical notes and 1.6 billion tokens of radiology reports, for a total corpus of 4 billion tokens. They ran CPT on the mixed corpus. The post-CPT model understood both clinical terminology and radiology-specific language.

When mixing domains, you should balance corpus sizes by domain importance. If clinical notes are twice as important as radiology reports, use a 2-to-1 ratio of clinical to radiology text. The healthcare company used their natural 60-40 split, which matched the expected distribution of downstream tasks. A financial services company wanted to support both equity research and fixed income analysis. They collected 1.8 billion tokens of equity text and 2.2 billion tokens of fixed income text, using a 45-55 split that matched their user base.

You can also use domain-specific learning rate scaling. Train on mixed domain data but apply different learning rates to different domain batches. Batches from high-priority domains use higher learning rates, while batches from lower-priority domains use lower learning rates. This requires a custom training loop but allows fine-grained control over domain learning. A legal tech company supporting both contract law and patent law used learning rate 1.2e-5 for contract batches and 8e-6 for patent batches. Contract law was their primary use case, so they prioritized it during CPT.

Multi-domain CPT requires multi-domain validation. You should measure perplexity and domain fluency separately for each domain. The healthcare company measured clinical note perplexity and radiology report perplexity independently. Clinical perplexity decreased from 28.4 to 11.7. Radiology perplexity decreased from 35.1 to 14.2. Both domains improved, confirming successful multi-domain learning.

## CPT on Synthetic Data

In some domains, real data is scarce, expensive, or restricted. You can generate synthetic domain text and use it for CPT. Synthetic CPT is less effective than real data CPT, but it is better than no CPT when real data is unavailable. You generate synthetic text using a strong base model, domain experts, or rule-based systems, then run CPT on the synthetic corpus.

A medical device company needed to train a model on surgical procedure notes, but real surgical notes are protected health information and unavailable for training. They used a GPT-4 model to generate 500,000 synthetic surgical notes based on templates and medical knowledge. Each synthetic note described a plausible surgical procedure with realistic terminology, complications, and outcomes. They ran CPT on 800 million tokens of synthetic notes. The post-CPT model learned surgical vocabulary and procedure structure, though it was less fluent than a model trained on real notes.

Synthetic data CPT works best when combined with small amounts of real data. A 90% synthetic, 10% real mix performs much better than 100% synthetic. The medical device company obtained 80 million tokens of de-identified real surgical notes and mixed them with 720 million tokens of synthetic notes. The 10% real data anchored the model in authentic language patterns, while the 90% synthetic data provided volume. Post-CPT perplexity on held-out real notes was 18.6, compared to 24.3 for pure synthetic CPT and 13.1 for pure real CPT. The hybrid approach was closer to real CPT than pure synthetic.

You should validate synthetic CPT on real data, never on synthetic test data. Evaluating on synthetic test data measures how well the model learned the synthetic generator's patterns, not how well it learned the real domain. The medical device company evaluated their synthetic-CPT model on 1,000 real held-out surgical notes. This gave an honest assessment of domain fluency.

Continued pre-training is the right choice when you need the model to internalize domain vocabulary, generate domain-fluent outputs without explicit prompting, and operate efficiently without retrieval latency. The next subchapter covers adapter composition, which allows you to combine multiple LoRA adapters trained on different domains or tasks and switch between them at serving time without retraining a monolithic model.
