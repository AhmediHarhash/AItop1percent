# 5.4 â€” Together.ai, Anyscale, and Modal: Managed Open-Model Platforms

Managed platforms for open-weight models are not a compromise between control and convenience. They are the optimal infrastructure choice for most teams doing production fine-tuning. The conventional wisdom says you either use fully-managed APIs and sacrifice flexibility or self-host everything and gain control at the cost of operational burden. This framing is obsolete. In June 2025, a legal technology startup proved it false when they migrated their fine-tuned Llama 4 70B model from an internal Kubernetes cluster to Together.ai. The team had spent seven months managing GPU scheduling, debugging CUDA conflicts, and handling out-of-memory errors during training runs while their infrastructure engineer departed and left four ML engineers spending eighteen hours per week on operations instead of model quality. The migration took three days. Within two weeks, they launched experiments on three additional model families, completed a hyperparameter sweep postponed for four months, and cut monthly compute costs from thirty-two thousand to nineteen thousand dollars. The infrastructure consuming a quarter of their engineering capacity now required four hours per month. This was not a compromise. This was an upgrade.

The managed platform tier sits between fully-managed API services like OpenAI fine-tuning and self-hosted infrastructure. You get the flexibility of open-weight models with the operational simplicity of managed services. You choose the base model, provide training data, configure hyperparameters, and the platform handles GPU provisioning, distributed training, model versioning, and inference deployment. This is not a compromise position. For most teams working with open models, managed platforms represent the optimal balance of control, cost, and engineering efficiency.

## The Managed Platform Value Proposition

Managed open-model platforms solve a specific operational problem. You want the economics and flexibility of open-weight models without building a machine learning infrastructure team. Your value is in training data quality, task-specific tuning, and domain expertise, not in GPU cluster management or Kubernetes operators for distributed training. The platform abstracts infrastructure while preserving the core decisions that differentiate your model from generic alternatives.

The economic case is straightforward. A self-hosted fine-tuning infrastructure requires GPU instances, orchestration, monitoring, model versioning, and deployment pipelines. You need engineers who understand both machine learning and infrastructure. For a team running occasional fine-tuning jobs, this overhead dominates costs. A three-person team spending fifty percent of their time on infrastructure represents more expense than platform fees for most training workloads. The break-even point where self-hosting becomes cheaper typically arrives when you are running continuous training pipelines across dozens of models with hundreds of experiments per month.

The operational case is equally clear. Managed platforms eliminate classes of failure that have nothing to do with your model. You do not debug NCCL communication failures in distributed training. You do not manage CUDA compatibility matrices. You do not build checkpoint recovery logic for preempted GPU instances. You do not implement gradient accumulation strategies for models that do not fit in single-GPU memory. These problems are solved once by the platform team and shared across all customers. Your team focuses on data quality, hyperparameter selection, and evaluation design.

The flexibility case is less obvious but equally important. Managed platforms make it trivial to experiment across model families. You can fine-tune Llama 4 8B, Mistral 7B, and Qwen 14B in the same afternoon using identical data pipelines. You can run the same training job on different GPU types to find the cost-performance sweet spot. You can launch ten parallel hyperparameter experiments without provisioning ten GPU clusters. This experimentation velocity is difficult to achieve with self-hosted infrastructure unless you have built substantial automation.

## Together.ai: Serverless Fine-Tuning at Scale

Together.ai positions itself as serverless fine-tuning infrastructure for open models. You submit training jobs through an API, the platform provisions GPUs, runs distributed training, and deploys the resulting model to inference endpoints. You pay per GPU-hour during training and per token during inference. There is no cluster to manage, no minimum commitment, no infrastructure to provision.

The model catalog is the primary differentiator. Together.ai supports fine-tuning for dozens of open-weight models including Llama 4 8B through 70B, Mistral 7B and Mixtral 8x7B, Qwen 1.5 and 2, CodeLlama, DeepSeek Coder, and Vicuna variants. When a new model family releases, Together.ai typically adds fine-tuning support within weeks. This breadth eliminates the "which base model" question from infrastructure concerns. You choose the model based on task requirements, not on what your infrastructure supports.

The fine-tuning workflow follows a standard pattern. You upload training data in JSONL format with prompt-completion pairs or conversational turns. You select a base model, configure hyperparameters like learning rate and batch size, and submit the job. The platform handles data preprocessing, distributed training across multiple GPUs when necessary, checkpoint saving, and validation monitoring. Training progress appears in a dashboard with loss curves and sample outputs. When training completes, the model automatically deploys to an inference endpoint with the same API interface as base models.

The pricing model is transparent and predictable. Training costs are billed per GPU-hour with rates varying by GPU type. A Llama 4 8B fine-tuning job might cost twelve dollars for four hours on an A100. A Llama 4 70B job might cost one hundred eighty dollars for six hours on eight H100s. Inference pricing is per token with rates typically twenty to forty percent higher than base model inference to cover the infrastructure overhead of hosting custom models. There are no platform fees, no monthly minimums, no charges for idle models.

The operational model is fully serverless. You do not reserve GPU capacity, you do not manage a cluster, you do not handle failures. If a training job fails due to infrastructure issues, Together.ai automatically retries. If an inference endpoint crashes, traffic routes to healthy replicas. If you need more inference throughput, you increase replica count through an API call. The platform handles autoscaling based on traffic patterns.

The limitations are primarily around control and customization. You configure high-level hyperparameters but you do not control the training loop implementation. You cannot customize the distributed training strategy. You cannot implement novel optimization techniques or training algorithms. You work within the framework Together.ai provides. For most fine-tuning workflows using standard supervised learning or LoRA, these constraints are invisible. For teams implementing cutting-edge training techniques or research experiments, they become blockers.

Together.ai fits best when you want maximum model selection flexibility with minimum infrastructure complexity. A team building domain-specific assistants across multiple verticals can fine-tune different base models for different use cases without building separate infrastructure. A startup exploring which model family works best for their task can run parallel experiments without capital investment in GPU clusters. An enterprise team replacing legacy ML systems can migrate incrementally without committing to a single model architecture.

## Anyscale: Ray-Based Enterprise Infrastructure

Anyscale builds on Ray, the distributed computing framework from Berkeley's RISELab. The platform provides managed Ray clusters for training, tuning, and serving machine learning models. Fine-tuning workflows use Ray Train for distributed training and Ray Serve for inference deployment. The architecture is more opinionated than Together.ai, but the Ray foundation provides deeper control over distributed execution.

The enterprise positioning is explicit. Anyscale targets teams that need features like VPC deployment, SSO integration, audit logging, and compliance certifications. The platform supports private model registries, custom training scripts, and integration with existing MLOps tooling. You can run Anyscale in your own cloud account with your own data residency requirements. This is not serverless infrastructure for rapid experimentation. This is managed infrastructure for production ML systems with enterprise requirements.

The fine-tuning workflow differs from Together.ai's API-driven approach. You write training scripts using Ray Train, which handles distributed training coordination. The script specifies data loading, model initialization, training loops, and checkpointing. You submit the script to an Anyscale cluster, which provisions GPUs, distributes the workload, and manages fault tolerance. This is more code than Together.ai's configuration-based approach, but it provides more control over training details.

The Ray foundation enables sophisticated distributed patterns. You can implement custom data preprocessing pipelines using Ray Data that stream training examples from object storage. You can run hyperparameter tuning with Ray Tune that coordinates dozens of parallel training jobs with adaptive resource allocation. You can implement advanced training techniques like gradient checkpointing, mixed precision training, or custom optimization schedules without platform constraints. The learning curve is steeper than Together.ai, but the capabilities are broader.

The model serving integration is tighter than standalone platforms. Ray Serve handles inference deployment with the same distributed execution model as training. You can implement complex serving patterns like ensemble models, multi-model endpoints, or streaming generation with custom batching logic. The serving infrastructure shares the same cluster management as training, simplifying operational overhead for teams running both workflows.

The pricing model is cluster-based rather than serverless. You pay for Ray cluster uptime regardless of utilization. A cluster with eight A100 GPUs costs approximately twelve dollars per GPU-hour whether you are running training jobs or not. This economics works when you have continuous workloads that keep clusters busy. It becomes expensive when you have intermittent fine-tuning jobs with long idle periods. Anyscale offers autoscaling to shut down idle clusters, but the minimum spinup time and configuration overhead make this less seamless than Together.ai's serverless model.

The operational model requires more ML infrastructure knowledge. You configure cluster specifications, manage dependencies, monitor resource utilization, and debug distributed training failures. Anyscale abstracts the underlying cloud provider and handles cluster provisioning, but you still operate at the cluster abstraction level. You need team members who understand distributed systems concepts and can debug Ray-specific issues.

Anyscale fits best when you have enterprise infrastructure requirements, continuous training workloads, or need deep control over distributed execution. A financial services company fine-tuning models on proprietary data with strict compliance requirements needs VPC deployment and audit logging. A team running daily retraining pipelines can amortize cluster costs across continuous workloads. A research team implementing novel training algorithms needs the flexibility to control every aspect of distributed training.

## Modal: Serverless GPU Compute for Custom Workflows

Modal takes a different approach. Rather than providing a fine-tuning platform, Modal provides serverless GPU compute that you use to build fine-tuning workflows. You write Python functions, decorate them to specify GPU requirements, and Modal handles container building, GPU provisioning, distributed execution, and autoscaling. The abstraction is lower-level than Together.ai but higher-level than managing cloud GPU instances directly.

The programming model is function-as-a-service for compute-intensive workloads. You write a Python function that runs a training loop using any framework: Hugging Face Transformers, PyTorch Lightning, Axolotl, or custom training code. You decorate the function with GPU requirements like one A100 with 80GB memory. You call the function from your local machine or a workflow orchestrator. Modal builds a container with your dependencies, provisions the GPU, runs the function, streams logs and outputs, and deallocates the GPU when the function completes.

The fine-tuning workflow is code-driven. You implement data loading, model initialization, training loops, checkpointing, and evaluation using standard ML libraries. Modal provides primitives for distributed training across multiple GPUs, shared volumes for datasets and checkpoints, and secrets management for API keys and credentials. You compose these primitives into the workflow your task requires. There is no built-in fine-tuning API, no configuration-based training submission, no model catalog. You build the workflow from components.

The infrastructure abstraction is thin. You specify exact GPU types, memory requirements, disk space, and container images. Modal handles provisioning and orchestration, but you control the execution environment. This is more infrastructure work than Together.ai's configuration-based approach but less than operating a Kubernetes cluster. You write Python code, Modal handles the cloud infrastructure, and you do not manage long-running clusters.

The pricing model is pure pay-per-use. You pay per GPU-second while your functions run. An A100 costs approximately two dollars per hour. There are no idle costs, no cluster management fees, no platform charges. If you run a four-hour training job once per week, you pay for sixteen GPU-hours per month. If you run nothing, you pay nothing. This economics is ideal for intermittent workloads with unpredictable schedules.

The operational model requires Python programming skills but minimal infrastructure expertise. You do not configure clusters, manage Kubernetes, or debug distributed systems. You write Python functions and let Modal handle infrastructure. When a GPU becomes unavailable, Modal automatically retries on a different instance. When you need more parallelism, you increase the concurrency parameter. The operational complexity is closer to Lambda than to EC2.

The flexibility is complete. You can use any training framework, implement any optimization technique, integrate any data source, and deploy any serving pattern. You can fine-tune with Axolotl, implement RLHF with TRL, run custom distillation pipelines, or build multi-stage training workflows with different GPU types at each stage. Modal provides compute primitives and orchestration, you provide the ML logic.

The limitations are lack of managed features. Modal does not provide a model registry, an inference serving platform, a hyperparameter tuning framework, or a training dashboard. You build these pieces yourself or integrate third-party tools. For teams that want end-to-end platform features, this is more work than Together.ai or Anyscale. For teams with specific requirements that do not fit platform constraints, this is exactly the flexibility they need.

Modal fits best when you have custom workflows that do not fit platform APIs, intermittent training workloads with unpredictable schedules, or need to integrate fine-tuning into broader ML pipelines. A team building a complex data preprocessing pipeline that feeds into fine-tuning can implement both stages as Modal functions. A startup running weekly retraining jobs can pay only for actual training time. A research team experimenting with novel architectures can implement training logic without platform constraints.

## Choosing Between Managed Platforms

The decision framework has three dimensions: workload regularity, control requirements, and team expertise. Workload regularity determines whether serverless or cluster-based pricing makes sense. Control requirements determine whether platform APIs provide sufficient flexibility. Team expertise determines operational overhead tolerance.

For intermittent fine-tuning with standard workflows, Together.ai provides the fastest path to production. You get broad model selection, transparent pricing, and zero infrastructure management. A team launching their first fine-tuned model should start here unless they have specific requirements that force a different choice. The operational overhead is minimal and the cost is predictable.

For continuous training workloads with enterprise requirements, Anyscale provides the infrastructure features and compliance certifications that regulated industries require. A financial services team, healthcare company, or government contractor needs VPC deployment, audit logging, and SOC2 compliance. The cluster-based pricing works when utilization is high and the Ray foundation enables sophisticated distributed workflows.

For custom workflows or research experiments, Modal provides the flexibility to implement exactly what you need without platform constraints. A team implementing multi-stage training pipelines, novel optimization techniques, or integration with proprietary data systems needs the ability to control every aspect of execution. The serverless pricing means you pay only for what you use regardless of experimentation volume.

The platform choice is not permanent. Many teams start with Together.ai for initial experiments, migrate to Anyscale when they hit platform limitations or need enterprise features, and supplement with Modal for workflows that do not fit either platform. The migration cost is primarily data pipeline adaptation and training script modifications. The models themselves are portable across platforms because they are standard open-weight checkpoints.

The managed platform tier represents the current best practice for most teams fine-tuning open models. You get professional-grade infrastructure without building an infrastructure team. You can experiment rapidly without capital investment in GPU clusters. You maintain full ownership of model weights and training data without vendor lock-in to proprietary APIs. The next subchapter examines the self-hosted alternative using Axolotl and Hugging Face TRL, the dominant frameworks for teams that choose to operate their own fine-tuning infrastructure.
