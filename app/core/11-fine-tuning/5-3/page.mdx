# 5.3 — Google Vertex AI Fine-Tuning: Gemini Models and Custom Training

In October 2025, a media analytics company chose Google Vertex AI for fine-tuning because they were already running their entire data infrastructure on Google Cloud and wanted a unified platform for training, serving, and monitoring. They uploaded 45,000 article summaries to fine-tune Gemini 1.5 Pro, launched training with Vertex's supervised tuning service, and received a model that worked well on text-only articles but failed completely on articles with embedded images or charts—inputs that accounted for 30 percent of their production traffic. They had assumed Gemini's multimodal capabilities would transfer automatically to the fine-tuned model, but they had used text-only training data, and the fine-tuned model lost the base model's visual understanding because it never saw vision examples during training. The root cause was not a platform limitation but a misunderstanding of how fine-tuning affects multimodal models: capabilities not represented in training data degrade even if the base model supports them.

This pattern reflects a broader truth about Vertex AI fine-tuning in 2026: the platform offers powerful capabilities that require careful data design and parameter selection to use effectively. Vertex AI is not simpler than OpenAI's API—it is more flexible, which means more ways to succeed and more ways to fail. Teams who understand the platform's tuning modes, data requirements, and integration patterns produce models that outperform OpenAI equivalents. Teams who treat it as a drop-in replacement for OpenAI's API without learning Vertex-specific conventions waste time and money on models that underperform.

## Supported Models and Tuning Modes

Google Vertex AI offers fine-tuning for Gemini 1.5 Pro and Gemini 1.5 Flash as of January 2026. Gemini 1.5 Pro is the flagship model with a 2 million token context window, multimodal input support for text, images, video, and audio, and reasoning capabilities comparable to GPT-4o. Gemini 1.5 Flash is the cost-optimized variant, offering faster inference and cheaper training while maintaining strong performance on focused tasks. Both models support fine-tuning with either supervised tuning or reinforcement learning from human feedback.

Supervised tuning is the standard approach: you provide input-output pairs, and the model learns to map inputs to outputs through next-token prediction. This is equivalent to OpenAI's fine-tuning mode. You upload examples in JSONL format, specify hyperparameters, launch training, and receive a fine-tuned model accessible through Vertex AI's inference API. Supervised tuning works for tasks where you have clear input-output pairs: summarization, classification, entity extraction, structured data generation. It is the fastest path to a fine-tuned model and the most broadly applicable technique.

Reinforcement learning from human feedback, or RLHF tuning, is the advanced mode for tasks where you have preference data rather than single correct outputs. You provide pairs of outputs for the same input, labeled with which output is preferred, and the model learns to generate outputs that align with human preferences. RLHF is more complex to set up, requires larger datasets, and takes longer to train, but it produces models that match subjective quality criteria better than supervised tuning. Vertex AI's RLHF implementation uses a proprietary variant of proximal policy optimization tuned for Gemini models. You do not control the RL algorithm or reward model architecture, but you control the preference data and can monitor training through reward signals rather than loss curves.

## Data Format Requirements for Supervised Tuning

Supervised tuning on Vertex AI requires JSONL files where each line is a JSON object containing an input field and an output field. For text-only tasks, input is a string and output is a string. For multimodal tasks, input is an array of content objects where each object specifies a type—text, image, video, audio—and the corresponding data. Images can be passed as Google Cloud Storage URIs or as base64-encoded strings. Video and audio follow the same pattern. The output is always text, even for multimodal inputs, because Gemini models generate text outputs.

The simplest format for text-only tuning is one object per line with input and output as strings. For example, an article summarization task would have input containing the full article text and output containing the summary. A classification task would have input containing the text to classify and output containing the label. A structured extraction task would have input containing unstructured text and output containing JSON or markdown representing the extracted structure. Vertex AI does not require you to format examples as chat messages with roles like OpenAI's API does—you provide raw input-output pairs, and the platform handles formatting internally.

For multimodal tuning, the input field becomes an array of content objects. Each object has a type field and a data field. An example for image-based summarization might have an input array with two objects: one of type text containing a prompt like "Summarize the key points from this chart," and one of type image containing a Cloud Storage URI pointing to the chart image. The output field contains the text summary. The model learns to process both modalities together and generate output that reflects the combined context.

The most common mistake is inconsistent multimodal structure. If some examples include images as the first element in the input array and others include images as the second element after text, the model sees positional inconsistency and learns noisier patterns. The correct approach is to establish a fixed structure—text prompt first, then image, or image first, then text—and apply it to every example. Positional consistency matters for multimodal models because they encode modality order as part of the input representation.

## Hyperparameters and Training Configuration

Vertex AI exposes more hyperparameters than OpenAI's API, giving you finer control but also more opportunities for misconfiguration. You can set learning rate directly rather than through a multiplier, choose between multiple optimization algorithms, control batch size and gradient accumulation steps independently, and specify learning rate schedules with warmup and decay phases. These options are powerful if you understand training dynamics and dangerous if you do not.

Learning rate is the most critical hyperparameter. Vertex AI's default learning rate for Gemini 1.5 Pro is 1e-5, which works for most text-only tasks with 10,000 to 100,000 examples. For smaller datasets under 5,000 examples, a lower learning rate like 5e-6 or 3e-6 reduces overfitting risk. For very large datasets over 500,000 examples, a higher learning rate like 2e-5 or 3e-5 can speed up convergence. Vertex AI does not auto-tune learning rate based on dataset size—you set it explicitly or accept the default.

Batch size and gradient accumulation interact to determine effective batch size. Batch size is how many examples are processed in parallel per GPU. Gradient accumulation steps determine how many batches are processed before updating model weights. Effective batch size equals batch size multiplied by gradient accumulation steps. A batch size of 16 with 4 gradient accumulation steps produces an effective batch size of 64. Larger effective batch sizes produce more stable gradients but require more memory and can slow convergence. Vertex AI defaults to batch size 8 and gradient accumulation 4 for most tasks, which produces effective batch size 32.

Epoch count is configurable, with defaults between 3 and 5 depending on dataset size. Vertex AI does not auto-stop training based on validation loss—you specify the number of epochs, and training runs until completion. If you want early stopping, you monitor validation metrics in the Vertex AI console and cancel the job manually if overfitting is detected. This is less automated than some platforms but gives you direct control.

## RLHF Tuning: Preference Data and Reward Optimization

RLHF tuning on Vertex AI requires a different data format: each training example contains an input and two outputs, labeled as chosen and rejected. The chosen output is the preferred response, and the rejected output is the less-preferred alternative. The model learns to increase the likelihood of chosen outputs and decrease the likelihood of rejected outputs. Preference data can come from human labeling, automated comparisons using a reward model, or implicit signals like user engagement metrics.

The quality of RLHF fine-tuning depends entirely on the quality of preference labels. If your chosen outputs are only marginally better than rejected outputs, the model learns weak preferences and produces marginal improvements. If your chosen outputs are clearly superior across multiple dimensions—accuracy, helpfulness, safety, tone—the model learns strong preferences and generalizes better. Preference strength is more important than dataset size: 5,000 examples with clear preferences outperform 50,000 examples with ambiguous preferences.

Vertex AI's RLHF implementation trains a reward model from your preference data, then uses that reward model to optimize the base Gemini model using proximal policy optimization. You do not see the reward model directly, but you can monitor reward signals during training to verify the model is learning your preferences. Reward should increase over training steps. If reward plateaus early, your preference data may not contain enough signal. If reward oscillates, the learning rate may be too high.

RLHF is more expensive and time-consuming than supervised tuning. A supervised tuning job with 20,000 examples might take 2 to 4 hours, while an RLHF job with 20,000 preference pairs might take 8 to 16 hours. RLHF also requires larger datasets to stabilize—10,000 to 50,000 preference pairs are typical, compared to 1,000 to 20,000 input-output pairs for supervised tuning. Use RLHF when you have clear preference data and when supervised tuning alone does not capture your quality criteria. Do not use RLHF as a default—it is a specialized tool for preference alignment, not a better version of supervised tuning.

## Integration with Google Cloud Ecosystem

Vertex AI's primary advantage over OpenAI's API is deep integration with Google Cloud services. Training data stored in Google Cloud Storage can be referenced directly by URI without uploading through an API. You can launch fine-tuning jobs from Cloud Functions or Cloud Run, orchestrate multi-step training pipelines with Vertex AI Pipelines, and monitor training with Cloud Logging and Cloud Monitoring. If your data infrastructure is already on Google Cloud, Vertex AI removes friction by eliminating data movement and authentication complexity.

Fine-tuned models deploy to Vertex AI endpoints, which are fully managed serving infrastructure. You create an endpoint, deploy your fine-tuned model to it, and call the endpoint via REST or gRPC. Endpoints support autoscaling, traffic splitting between model versions, and request logging for quality monitoring. You can deploy multiple model versions to the same endpoint and gradually shift traffic from the old version to the new version as you validate quality. This is built-in A/B testing infrastructure that requires no additional tooling.

Vertex AI integrates with BigQuery for large-scale data processing. You can run SQL queries over billions of rows in BigQuery, export the results as training data, fine-tune a model, and serve predictions back into BigQuery tables. This closed-loop pattern is common in data-intensive enterprises: training data comes from BigQuery, model outputs go back to BigQuery, and the entire pipeline runs on Google Cloud without data egress. For companies already using BigQuery as their data warehouse, this integration eliminates the operational overhead of moving data between platforms.

## Pricing and Cost Management

Vertex AI charges separately for training and serving. Training costs are based on compute time and GPU type. As of January 2026, fine-tuning Gemini 1.5 Pro costs approximately $4 to $8 per training hour depending on the region and GPU allocation. A typical supervised tuning job with 20,000 examples takes 2 to 4 hours, costing $8 to $32. RLHF tuning takes 8 to 16 hours, costing $32 to $128. These are order-of-magnitude estimates—actual costs depend on dataset size, training parameters, and infrastructure configuration.

Serving costs are based on request volume and model size. Gemini 1.5 Pro inference costs $0.00125 per 1,000 input tokens and $0.00375 per 1,000 output tokens as of January 2026 for the base model. Fine-tuned model pricing is the same—there is no additional fee for calling a fine-tuned model versus the base model. If fine-tuning lets you reduce prompt size by removing examples or instructions, you save on inference costs. If it lets you switch from Gemini 1.5 Pro to Gemini 1.5 Flash while maintaining quality, you save approximately 60 percent on inference.

Vertex AI offers committed use discounts for teams with predictable workloads. If you commit to a certain level of training or serving spend per month, Google provides discounts of 20 to 40 percent depending on commitment size. This makes sense for teams running continuous fine-tuning pipelines or serving high-volume production traffic. For experimental or low-volume workloads, pay-as-you-go pricing is more cost-effective.

## Strengths: Multimodal, Long Context, and Enterprise Integration

Vertex AI's primary strength is multimodal fine-tuning. As of January 2026, Gemini models are the only frontier models with production-ready support for fine-tuning on vision, video, and audio inputs. If your task involves images—medical imaging analysis, chart interpretation, visual content moderation, product recognition—Vertex AI is the only API-based option. OpenAI's GPT-4o supports vision inputs but not vision fine-tuning as of January 2026. Fine-tuning on multimodal data requires Gemini or self-hosted infrastructure with open-weight multimodal models.

The second strength is long context. Gemini 1.5 Pro supports a 2 million token context window, and fine-tuned models maintain that capacity. If your task involves long documents—legal contracts, research papers, codebases, multi-hour meeting transcripts—you can fine-tune Gemini to handle them end-to-end without chunking or summarization. This is a unique capability as of January 2026. GPT-4o and Claude 3.5 support 128,000 to 200,000 token windows, which is insufficient for the longest documents. Gemini's long context enables fine-tuning for tasks that were previously impossible with other models.

The third strength is enterprise integration. For companies with Google Cloud contracts, Vertex AI provides unified billing, identity management, data governance, and compliance frameworks. Training data stays within the organization's Google Cloud tenancy, subject to the same access controls and audit logs as all other data. Legal and compliance teams who have already approved Google Cloud do not need separate approval for Vertex AI fine-tuning. This reduces procurement friction and accelerates deployment.

## Limitations and Trade-Offs

The primary limitation is model availability. Vertex AI offers Gemini models only. If you need GPT-4.5 or Claude 3.5 or Llama 3.3, you cannot use Vertex AI. Model lock-in is reciprocal: OpenAI locks you into GPT models, Vertex AI locks you into Gemini models. Your choice depends on which model family performs best for your task.

The second limitation is training transparency. Vertex AI abstracts training infrastructure more aggressively than OpenAI's API. You do not see GPU allocation, parallelization strategy, or checkpoint details. You specify hyperparameters and data, launch training, and receive a model. For most teams this is fine, but for teams who need deep training introspection or want to optimize training performance, the abstraction is limiting.

The third limitation is cost predictability. Vertex AI's training costs depend on factors you cannot fully control: GPU availability in your region, internal resource allocation, and infrastructure overhead. Training the same dataset twice can produce different costs if Google allocates different hardware. This variability is small—typically 10 to 20 percent—but it makes precise cost forecasting harder than with OpenAI's fixed per-token pricing.

## When Vertex AI Is the Right Choice

Vertex AI is the right choice when you need multimodal fine-tuning, when you need long-context fine-tuning beyond 200,000 tokens, when your data infrastructure is on Google Cloud and you want to avoid data movement, when you need RLHF or preference tuning, or when you have Google Cloud contracts and want unified billing and governance. It is also the right choice when Gemini models outperform GPT or Claude models on your task, which depends on task-specific evaluation.

Vertex AI is not the right choice when you need GPT or Claude models, when you need training transparency and checkpoint access, when you need the simplest possible API with the fewest configuration options, or when you are optimizing for the absolute lowest training cost. The platform rewards teams who invest in learning its conventions, who structure data carefully for multimodal inputs, and who use its Google Cloud integrations to build end-to-end pipelines.

The comparison between OpenAI and Vertex AI is not about which platform is better—it is about which constraints match your requirements. OpenAI offers simplicity and GPT models. Vertex AI offers multimodal capabilities and Google Cloud integration. Both are production-ready API-based fine-tuning platforms that eliminate infrastructure burden while constraining flexibility. Choose based on model availability, data constraints, and ecosystem fit, not based on abstract platform preferences.

For teams who need more control than APIs provide but less burden than self-hosted infrastructure, managed fine-tuning platforms offer a middle path, which we explore in the next subchapter.
