# 2.5 — Data Quality Auditing: The Three-Pass Review Process

In March 2025, a healthcare technology company spent eleven weeks fine-tuning a clinical documentation assistant on 180,000 provider notes. The model passed internal testing with 91% accuracy on response quality. Two weeks after deployment to 340 physicians across twelve hospital systems, the quality team discovered that 23% of generated clinical summaries contained factual inversions—stating that symptoms were absent when the original note indicated they were present, or vice versa. The root cause traced back to 14,000 training examples where the labeling team had misunderstood negation structures in clinical language. The company had run automated format checks on the training data and reviewed fifty random examples before training. They had never implemented a systematic quality audit process. The model was withdrawn, the training data was rebuilt with a structured three-pass review, and the project restarted from scratch. Total delay: five months. The clinical lead later wrote in an internal retrospective: "we treated data quality as a checkbox rather than a process."

The quality of your training data sets the performance ceiling for your fine-tuned model. No amount of hyperparameter tuning, architectural improvement, or post-training filtering can compensate for systematic errors, biases, or inconsistencies in the training set. You need a structured, repeatable process for auditing data quality before training begins. The **Three-Pass Review Process** is that structure: Pass 1 catches format and distribution problems through automation, Pass 2 validates correctness and relevance through statistical sampling, and Pass 3 hunts for hidden failure modes through adversarial probing. Each pass serves a distinct purpose, uses different methods, and catches different categories of error. You run all three passes on every training set, every time, no exceptions.

## Pass 1: Automated Checks for Format, Distribution, and Structural Integrity

Pass 1 is fully automated. You write code that runs over the entire training set and flags examples that violate format requirements, fall outside expected distributions, or exhibit structural anomalies. This pass catches the most obvious problems: malformed JSON, truncated text, encoding errors, extreme length outliers, language mismatches, and exact duplicates. Pass 1 does not require human judgment—it applies deterministic rules to every example and produces a reject list and a flag list.

You start with format validation. Every training example must conform to the schema your training pipeline expects. If you are training a conversational model, every example must have a user message and an assistant response. If you are training a classification model, every example must have input text and a valid label from your taxonomy. If you are training a function-calling model, every example must have a prompt, a function call in the correct JSON structure, and a response. Your format validator checks that every required field is present, that field types match expectations, that nested structures parse correctly, and that no field contains null or placeholder values. Any example that fails format validation is automatically rejected—it will not proceed to Pass 2. You log the rejection reason and the example ID. A well-formed training set rejects less than 1% of examples in format validation. If your rejection rate exceeds 2%, your data collection or export process is broken.

Next you check length distributions. You compute token counts for every input and every output in the training set, then examine the distribution. You flag examples in the bottom 1% and top 1% of input length and output length. Short inputs often indicate truncated or incomplete data. Long inputs often indicate accidentally concatenated examples or copied diagnostic logs. Short outputs often indicate placeholder text or incomplete labeling. Long outputs often indicate excessive verbosity or pasted reference documents. You do not automatically reject these examples—they proceed to Pass 2 with a length flag—but human reviewers will examine them closely. Length outliers frequently correlate with quality problems.

You also compute the ratio of output length to input length for each example. In most fine-tuning tasks, this ratio clusters around a consistent value. For summarization tasks, the ratio is usually between 0.1 and 0.3. For question answering, the ratio is usually between 0.2 and 0.8. For code generation, the ratio varies widely but still clusters within task-specific bounds. You flag examples where the ratio falls outside two standard deviations from the mean. A summary that is longer than the input document is likely an error. A question-answer pair where the answer is twenty times longer than the question is likely mislabeled. Ratio outliers proceed to Pass 2 with a flag.

Language detection is the next automated check. If your training set is supposed to be entirely in English, you run language detection on every input and output and flag any example where the detected language is not English with high confidence. If your training set is multilingual, you verify that every example is in one of the expected languages and that the input language matches the output language unless translation is the task. Language detection models are not perfect—they misclassify short text, code-mixed text, and text with heavy technical jargon—but they catch egregious errors like accidentally included customer support tickets in French or training examples scraped from the wrong domain. Examples flagged for language mismatches proceed to Pass 2.

You check for exact duplicates by computing a hash of every input-output pair and identifying collisions. Exact duplicates waste training compute and cause the model to overweight specific examples. If you find duplicates, you keep one copy and reject the rest. A typical training set contains 2% to 8% exact duplicates, usually due to repeated exports from databases, accidental reprocessing of the same source files, or users submitting the same request multiple times. If your duplicate rate exceeds 15%, your data pipeline has a serious bug.

You also run encoding validation. Text data often arrives with encoding errors—mojibake, null bytes, control characters, invisible Unicode, or mixed encodings. You scan every example for non-printable characters, invalid UTF-8 sequences, and suspicious Unicode ranges. Examples with encoding errors are automatically rejected. Encoding problems corrupt tokenization, break model training, and create debugging nightmares. Fix them before training, not after.

Finally, you log summary statistics for the entire training set: total example count, rejected count by rejection reason, flagged count by flag type, mean and median input length, mean and median output length, length distribution percentiles, language distribution, and duplicate rate. These statistics are your baseline. You will compare them against Pass 2 and Pass 3 findings, and you will track them across training set versions. Changes in these statistics often signal changes in data collection processes or labeling quality.

Pass 1 typically runs in minutes to hours, depending on training set size. It requires no human labor. It catches 20% to 40% of quality problems in a typical training set. It is necessary but not sufficient. Pass 2 is where human judgment enters the process.

## Pass 2: Statistical Sampling for Correctness, Relevance, and Consistency

Pass 2 uses statistical sampling to estimate the correctness, relevance, and consistency of the training set without requiring human reviewers to check every example. You randomly sample a subset of the data, have expert reviewers evaluate each sampled example against explicit quality criteria, and extrapolate the findings to the full set. Pass 2 catches errors that Pass 1 cannot detect: factual inaccuracies, instruction-output mismatches, low-quality outputs, off-topic examples, inconsistent labeling, and subtle formatting problems.

You start by defining your quality criteria. These are the yes-or-no questions reviewers will answer for each sampled example. For a customer support fine-tuning task, the criteria might be: Does the input represent a realistic customer question? Does the output directly answer the question? Is the output factually correct? Is the tone appropriate for customer support? Does the output follow the company style guide? Is the output free of placeholder text or template artifacts? For a code generation task, the criteria might be: Does the input describe a clear programming task? Does the output code solve the task? Is the code syntactically valid? Does the code follow best practices? Is the code free of security vulnerabilities? Does the code include necessary error handling?

Each criterion must be binary and unambiguous. Reviewers should be able to answer yes or no without guessing. If a criterion requires interpretation or domain expertise, you provide reviewers with examples and a decision rubric. You also include a final criterion: Overall, should this example be included in the training set? This is the ultimate decision gate.

Next you determine sample size. The sample must be large enough to detect quality problems at the rate you care about, with the confidence level you require. If you want to detect a 5% defect rate with 95% confidence, you need a sample of approximately 300 examples. If you want to detect a 10% defect rate with 90% confidence, you need approximately 100 examples. If you want to detect a 1% defect rate, you need thousands of examples, which makes sampling impractical—you move to adversarial review instead. Most teams sample between 200 and 500 examples in Pass 2, targeting detection of defect rates between 3% and 10%.

You sample randomly, stratified by any dimensions that might correlate with quality differences. If your training set includes examples from multiple data sources, you sample proportionally from each source. If your training set includes examples from multiple time periods, you sample from each period. If your training set includes examples labeled by different annotators or labeling vendors, you sample from each group. Stratification ensures that quality problems concentrated in one slice of the data are not diluted by averaging across the full set.

Reviewers evaluate each sampled example independently. You assign each example to at least two reviewers to measure inter-rater agreement. If reviewers disagree on more than 10% of examples, your quality criteria are ambiguous or your reviewers need better training. If agreement is high, you can reduce to one reviewer per example in future audits. Reviewers mark each criterion as pass or fail and make an overall include-or-reject decision.

You aggregate the results and compute defect rates for each criterion and for the overall reject decision. If the overall reject rate in the sample is 3%, you estimate that approximately 3% of the full training set should be rejected. You multiply this rate by the total training set size to estimate how many examples will be removed. You also examine which criteria have the highest failure rates—these point to systemic issues in data collection or labeling. If 12% of examples fail the factual correctness criterion, you have a labeling problem. If 18% of examples fail the relevance criterion, you have a data sourcing problem.

You then decide whether to proceed with training or rebuild the dataset. If the estimated reject rate is below 5%, you remove the flagged examples from the sample, apply the same filtering logic to the full set where possible, and proceed to Pass 3. If the estimated reject rate is between 5% and 15%, you investigate the root cause, fix the data pipeline, and re-run Pass 1 and Pass 2 on a new export. If the estimated reject rate exceeds 15%, you halt the project and rebuild the training set from scratch. Training on a dataset where one in six examples is incorrect or irrelevant will produce a model that fails in production.

Pass 2 also surfaces qualitative insights. Reviewers often identify patterns that are not captured by the binary criteria: awkward phrasing that is technically correct but unnatural, outputs that are correct but overly verbose, examples that are relevant but too easy to be useful, or systematic biases in tone or style. You document these patterns and decide whether they require filtering, editing, or acceptance. Some teams use Pass 2 findings to create additional automated filters that run in Pass 1 for future datasets.

Pass 2 typically requires one to three days of reviewer time, depending on sample size and task complexity. It catches 30% to 50% of quality problems that Pass 1 misses. It is the most important gate in the quality audit process. But it still misses rare, high-impact errors and adversarial edge cases. That is what Pass 3 is for.

## Pass 3: Adversarial Review for Edge Cases, Biases, and Hidden Failures

Pass 3 is adversarial. Instead of randomly sampling the dataset, you deliberately search for examples that are likely to contain errors, edge cases, or biases that statistical sampling will miss. You use keyword searches, pattern matching, clustering, and human intuition to find the hardest, weirdest, and riskiest examples in the training set. You review these examples with heightened scrutiny. Pass 3 catches the 1-in-500 error that ruins model performance on a critical use case, the subtle bias that becomes obvious only when you look at demographic slices, and the contamination that inflates eval metrics.

You start with keyword and pattern searches. You search for terms that indicate potential problems: profanity, slurs, placeholder text, template markers, internal jargon, system error messages, debugging output, personally identifiable information, and task-irrelevant content. You search for patterns that indicate labeling shortcuts: outputs that start with "As an AI language model," outputs that include refusal language when the task does not require refusals, outputs that contain only a single word or sentence when the task expects paragraphs, and outputs that are exact copies of the input. You also search for domain-specific red flags. In a medical training set, you search for medication names that are commonly confused. In a legal training set, you search for jurisdictions where the law has recently changed. In a financial training set, you search for outdated regulatory references.

Each keyword search produces a candidate list. You review every example in the list and decide whether it represents a real problem or a false positive. If the problem is real, you reject the example. If the problem is ambiguous, you flag it for further discussion with domain experts. Keyword searches typically find 1% to 5% of additional rejects beyond what Pass 2 identified, but the examples they find are disproportionately harmful. A single example that teaches the model to regurgitate PII or use offensive language can cause a high-severity production incident.

Next you use clustering to find groups of similar examples. You embed every input in the training set using a sentence embedding model, then cluster the embeddings using k-means or DBSCAN. You examine the largest clusters and the smallest clusters. Large clusters often indicate repetitive or template-driven data—hundreds of examples that differ only in minor details. If a cluster contains 2,000 nearly-identical examples, you are teaching the model to memorize rather than generalize. You down-sample the cluster to 50 representative examples and reject the rest. Small clusters, especially singleton clusters far from other points in embedding space, often indicate outliers, mislabeled examples, or off-topic data. You review every example in clusters smaller than five examples.

You also analyze the training set by demographic, geographic, and domain slices if your task involves user-facing content. You check whether examples are balanced across gender, age, ethnicity, language variety, and cultural context. You check whether examples cover all relevant geographic regions, legal jurisdictions, and industry verticals. Imbalances in representation lead to models that perform well for some users and poorly for others. If 80% of your customer support training examples come from U.S. users, your model will struggle with non-U.S. conventions, regulations, and expectations. If 90% of your examples use formal tone, your model will struggle with casual or colloquial requests. You do not need perfect balance, but you need sufficient representation to avoid systematic blindspots.

Bias detection is a core component of Pass 3. You search for training examples where the output exhibits demographic bias, stereotype reinforcement, or exclusionary language. You search for examples where the output makes assumptions about user identity, location, or preferences that are not justified by the input. You search for examples where the output provides different quality or tone for inputs that differ only in demographic markers. This review requires domain expertise and cultural awareness. Many teams bring in external bias auditors or red teams for this component of Pass 3.

Contamination checking is the final element of Pass 3. You verify that no examples from your test set, validation set, or public benchmarks appear in the training set. Contamination causes models to appear more capable than they are because they have memorized the test answers rather than learned the underlying task. You compute embeddings or hashes for every example in your test and validation sets, then search the training set for near-matches. You also download common public benchmarks—MMLU, HellaSwag, TruthfulQA, HumanEval—and check for overlaps. If you find contamination, you remove the overlapping examples from the training set and re-run your baseline evaluations to understand the true performance ceiling. Contamination rates in user-generated training data are typically below 1%, but in scraped or synthetic data, contamination can reach 5% to 10%.

Pass 3 does not have a fixed sample size. You review as many examples as needed to exhaust the adversarial search strategies. For a 50,000-example training set, Pass 3 typically involves reviewing 500 to 2,000 examples. For a 500,000-example set, Pass 3 might involve reviewing 2,000 to 5,000 examples. The review is not random—it is targeted at the highest-risk slices of the data.

Pass 3 typically requires two to five days of expert reviewer time. It catches 10% to 20% of quality problems that Pass 1 and Pass 2 miss. It also surfaces insights that inform data collection and labeling process improvements. After completing Pass 3, you compile a final reject list, remove all rejected examples from the training set, document the findings, and proceed to training.

## Implementing the Three-Pass Process in Your Pipeline

The Three-Pass Review Process is not a one-time audit. It is a standard stage in your training data pipeline that runs every time you prepare a new training set. You implement it as three sequential gates: data that fails Pass 1 is rejected automatically, data that passes Pass 1 proceeds to Pass 2 sampling, and data that passes Pass 2 proceeds to Pass 3 adversarial review. Only data that passes all three gates enters the training pipeline.

You track metrics for every pass and every dataset version. You log Pass 1 rejection counts by rejection reason, Pass 2 defect rates by quality criterion, and Pass 3 findings by search strategy. You track how these metrics change over time as you improve data collection and labeling processes. You also track the cost of each pass in human hours and calendar time. Pass 1 should be nearly free because it is automated. Pass 2 should cost a few hundred dollars in reviewer time. Pass 3 should cost a few thousand dollars in expert time. If your costs exceed these ranges, your process is inefficient or your data quality is poor.

You also version your quality criteria and review rubrics. As you learn more about what good training data looks like for your task, you refine the criteria. You add new criteria when Pass 3 identifies failure modes that Pass 2 missed. You remove criteria that produce no meaningful signal. You update examples and decision rules in the reviewer rubric when inter-rater agreement is low. Your quality criteria evolve as your understanding of the task deepens.

Some teams integrate Pass 1 and Pass 2 into continuous data pipelines. New examples are validated and sampled as they arrive, rather than in large batches before training. This approach works well for tasks with ongoing data collection, like customer support or content moderation. It does not work well for one-time dataset construction projects, like building a training set from historical archives.

The Three-Pass Review Process is not optional. It is the minimum viable process for ensuring that your training data meets the quality bar your model requires. Teams that skip passes, cut corners on sample sizes, or treat quality review as a formality ship models that fail in production. The healthcare company that opened this subchapter learned this lesson the hard way. You do not need to.

The next step after quality auditing is addressing one of the most insidious data problems: duplication and near-duplication, which cause models to memorize rather than generalize, and contamination, which inflates eval metrics and masks real performance gaps.
