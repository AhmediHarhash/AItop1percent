# 2.3 â€” Data Formatting: Instruction-Response Pairs, Chat Templates, Completion Formats

What happens when you feed a chat-trained model completion-format data, or wrap instruction-response pairs in delimiters the model was never trained to recognize? Your training job completes. Loss decreases. Validation metrics look reasonable. Then you deploy and discover the model has learned nothing useful. It generates outputs in the wrong structure, confuses instructions with responses, or invents behaviors that never appeared in your training data. Format is not a cosmetic concern. It is the interface that tells the model what role it is playing, where instructions end and outputs begin, and what pattern it should learn from each example. Use the wrong format and the model learns the wrong associations, even when your examples are perfect and your labels are correct. Most teams discover formatting errors only after deployment because standard validation workflows do not test whether the model learned the intended task structure. By then, weeks of annotation and thousands of dollars in compute are wasted.

The problem was not the examples. It was the formatting. The team had used the completion format from an older API tutorial, wrapping each example in a prompt-completion structure with custom delimiters. But GPT-5 was trained as a chat model with specific role tags and turn boundaries. The model interpreted their custom delimiters as part of the content, learned the wrong association between input and output, and failed to understand where instructions ended and responses began. Seven weeks of curation became worthless because the formatting did not match what the model expected. They reformatted the entire dataset into proper chat template structure, retrained, and got the performance they needed. The data was always good. The format was wrong from the start.

How you format your training data determines whether the model learns what you intend. Formatting is not a cosmetic concern or a minor implementation detail. It is the interface between human intent and model learning. Use the wrong format and the model learns the wrong patterns, even if your examples are perfect. This subchapter covers the three primary formatting structures used in fine-tuning as of 2026, when each applies, how formatting errors silently degrade training, and the specific requirements that vary by tokenizer and model family.

## The Three Format Families

Modern fine-tuning uses three distinct formatting structures, each designed for a different learning objective. The **instruction-response pair format** structures each example as a single user instruction followed by a single assistant response. The **chat template format** structures examples as multi-turn conversations with explicit role tags for system, user, and assistant messages. The **completion format** structures examples as raw text continuations without role boundaries or turn structure. Each format teaches the model different behavior patterns.

Instruction-response pairs are the default for most task-specific fine-tuning in 2026. You provide a user instruction and the assistant response you want the model to learn. The format includes a system message slot for persistent instructions, a user message containing the task-specific input, and an assistant message containing the expected output. This format works for classification, extraction, summarization, translation, rewriting, and most single-turn tasks. The model learns to associate the instruction pattern with the response pattern.

Chat template format extends instruction-response with multi-turn conversation structure. Each example contains multiple back-and-forth exchanges between user and assistant, with explicit boundaries marking where each turn begins and ends. This format is required when you are teaching conversational behavior, multi-step reasoning, or tasks where context accumulates across turns. The model learns turn-taking behavior, how to reference previous messages, and how to maintain coherence across a conversation thread.

Completion format removes all role structure and presents training data as raw text that the model should continue. This format is rarely used in 2026 for instruction-following models, but still appears in domain adaptation scenarios where you are teaching the model to generate text in a specific style or domain without explicit instruction-response structure. It is how base models are pretrained, and it remains useful when you want the model to learn patterns in unstructured text rather than structured task behavior.

Choosing the wrong format does not cause an error. The training job will complete. Loss will decrease. But the model will learn the wrong thing. If you use completion format for an instruction task, the model learns to continue text rather than follow instructions. If you use instruction-response format for a conversational task, the model loses multi-turn coherence because it never learned turn boundaries. If you use chat format for a simple classification task, you add unnecessary complexity that can reduce performance on the single-turn task you actually care about.

## Instruction-Response Pair Structure

The instruction-response format as of 2026 follows a three-part structure: system message, user message, assistant message. The system message sets persistent context or instructions that apply to all examples. The user message contains the task-specific input. The assistant message contains the expected output. Each message has an explicit role tag that tells the model which participant is speaking.

A contract extraction example formatted correctly looks like this in conceptual structure: the system message says you are an expert legal contract analyzer, your task is to extract specific clause types from contract text, output only the extracted text with no commentary. The user message contains the contract text and specifies which clause type to extract. The assistant message contains only the extracted clause text, exactly as you want the model to output it in production.

The role tags are not optional decoration. They are semantic boundaries that the model uses to understand the structure of the interaction. When you submit training data without proper role tags, the model cannot distinguish between instruction and response. It learns the entire block as a single continuation pattern. This is why the legal tech company's training failed: they formatted examples as raw text blocks with custom delimiters instead of using the role tag structure the model was pretrained to recognize.

The system message is where you encode task-level instructions that apply to every example. Do not repeat these instructions in every user message. The model learns more efficiently when persistent instructions live in the system slot and variable inputs live in the user slot. If you are training for tone consistency, put the tone requirements in the system message. If you are training for output format, put the format specification in the system message. If you are training for domain-specific behavior, put the domain context in the system message.

Many teams leave the system message empty or use a generic placeholder like you are a helpful assistant. This wastes the most powerful slot in the format. The system message is where you teach the model what role it is playing and what constraints apply to all responses. Use it. Be specific. If your production system will always set a particular system message, use that exact text in your training data. The model will learn to operate within those constraints.

The user message contains the variable input for each example. This is the part that changes from example to example. It should match the structure of production inputs as closely as possible. If your production system sends structured data fields, format them the same way in training. If your production system sends natural language questions, use natural language questions in training. If your production system includes metadata or context before the main input, include it in the same format in training.

The assistant message contains the expected output. This is what the model is learning to produce. It must match your production output format exactly. If you want the model to output JSON in production, every training example must have valid JSON in the assistant message. If you want the model to output markdown, use markdown in training. If you want the model to output just the answer with no explanation, train it with answers and no explanations. The model learns to imitate the pattern you show it.

## Chat Template Multi-Turn Structure

Chat template format extends the instruction-response structure to handle conversations with multiple turns. Each example contains a sequence of user and assistant messages, alternating back and forth, with the same role tag structure marking each turn boundary. The model learns not just how to respond to individual instructions but how to maintain coherence and context across a conversation thread.

A customer support conversation might have five turns: user asks about order status, assistant asks for order number, user provides order number, assistant retrieves order details and asks if they need anything else, user says no. Each of these messages gets an explicit role tag and turn boundary. The model learns that it should ask clarifying questions when information is missing, reference information from earlier turns when providing answers, and recognize conversation closure signals.

Multi-turn formatting is required when your production task involves back-and-forth interaction. If you are building a diagnostic assistant that asks follow-up questions, you need multi-turn training data showing the question-answer sequences you want the model to learn. If you are building a coding assistant that iteratively refines solutions based on user feedback, you need multi-turn examples showing the refinement process. Single-turn instruction-response format cannot teach these behaviors.

The turn boundaries matter as much as the content. When you format multi-turn data incorrectly, the model loses track of who is speaking and when turns end. A common mistake is concatenating all user messages into one block and all assistant messages into another block, losing the alternating turn structure. The model then learns to generate long monologues instead of conversational exchanges. Another mistake is using inconsistent role tags across examples, sometimes calling the assistant by one name and sometimes by another. The model never learns a stable notion of its own role.

Chat template format also allows for system messages that persist across all turns. This works the same way as in instruction-response format: the system message sets the context and constraints for the entire conversation. The difference is that in multi-turn format, the system message applies to every turn, not just the first response. If your system message specifies tone or output format, those constraints should hold across the entire conversation.

Some platforms in 2026 automatically handle chat template formatting when you submit training data in a structured JSON format with message arrays and role fields. OpenAI's fine-tuning API, Anthropic's fine-tuning interface, and most commercial fine-tuning platforms accept this structured format and apply the correct template internally. But you must still structure your raw data with proper role tags and turn boundaries. The platform handles tokenization and template formatting, but you handle the semantic structure.

## Completion Format for Domain Adaptation

Completion format is the simplest structure: raw text with no role tags, no turn boundaries, no instruction-response separation. The model learns to continue text in the style and domain of the training examples. This format is how base models are pretrained on trillions of tokens of internet text. It remains useful for domain adaptation scenarios where you want the model to absorb domain-specific language, terminology, and writing patterns without learning a specific task structure.

A biomedical research company in late 2025 used completion format to adapt GPT-5 to scientific literature in oncology. They fed the model 60,000 abstracts and paper excerpts formatted as plain text completions. The model learned oncology terminology, standard result reporting formats, and the stylistic conventions of cancer research papers. When they later fine-tuned this adapted model on instruction-response pairs for literature summarization, it performed significantly better than the base model because it already understood the domain language.

Completion format works when the learning objective is language and style rather than task behavior. You are teaching the model what domain-specific text looks like, not teaching it to follow instructions. The training process is the same as instruction fine-tuning: you provide examples, the model predicts next tokens, gradients update weights. But the examples do not have instruction-response structure. They are just raw text from the target domain.

The risk with completion format is that it does not teach instruction-following behavior. If you train exclusively with completions, the model may become better at generating domain-specific text but worse at following user instructions. This is why completion format is typically used as a first-stage adaptation step, followed by instruction fine-tuning to restore or enhance task-following ability. The two-stage approach gives you domain knowledge plus instruction-following.

In 2026, most teams use instruction-response or chat template format rather than completion format. The modern paradigm is to teach task behavior with structured examples rather than hoping the model will infer tasks from unstructured text. But completion format still has a place in domain adaptation, especially when you have large amounts of domain text but limited labeled examples for specific tasks. You can pretrain on completions to absorb domain language, then fine-tune on a smaller instruction dataset to teach task behavior.

## Tokenizer-Specific Formatting Requirements

Every model family uses a tokenizer that converts text into tokens, and every tokenizer has specific formatting requirements for role tags and turn boundaries. What works for GPT-4 may not work for Llama 4. What works for Claude may not work for Gemini. The role tag syntax, the special tokens that mark turn boundaries, and the way system messages are encoded all vary by tokenizer. Using the wrong tokenizer format silently corrupts your training data.

GPT-4 and GPT-5 use the ChatML format with explicit role tags encoded as special tokens. The system message uses one token sequence, user messages use another, assistant messages use a third. Turn boundaries are marked with special end-of-turn tokens. When you submit training data to OpenAI's API, you provide messages in a structured JSON format and the platform applies ChatML formatting automatically. You do not write the special tokens yourself, but you must structure the messages correctly.

Llama 4 uses a different chat template with different special tokens for role tags and turn boundaries. The system message syntax differs from GPT-4. The way multi-turn conversations are encoded differs. If you are fine-tuning Llama 4 on your own infrastructure, you must apply the Llama 4 chat template to your training data before tokenization. If you apply GPT-4 formatting to Llama 4 training data, the model will not learn the correct associations because the special tokens do not match what it was pretrained on.

Claude models use yet another format with different role tag conventions and turn structure. Anthropic's fine-tuning API handles this formatting internally when you submit structured message data, but if you are working with raw model weights or custom training infrastructure, you must know the exact template format Claude expects. The same principle applies to Gemini, Mistral, and every other model family: each has a specific chat template, and you must use the right one.

The safest approach in 2026 is to use platform-provided APIs that handle tokenizer-specific formatting automatically. OpenAI's fine-tuning API, Anthropic's fine-tuning service, Google's Vertex AI fine-tuning, and Hugging Face's fine-tuning tools all accept structured message data and apply the correct template for the target model. You provide messages with role tags in a standard format, the platform converts them to the tokenizer-specific representation.

But you must still understand the conceptual structure. Even when the platform handles low-level formatting, you need to structure your messages correctly at the semantic level. You need to know whether your task requires instruction-response or chat template format. You need to know what goes in the system message and what goes in the user message. You need to know whether your examples should be single-turn or multi-turn. The platform handles tokenization, but you handle meaning.

## Common Formatting Mistakes That Waste Training Runs

The most common formatting mistake is using inconsistent structure across examples. Some examples have system messages, others do not. Some examples put instructions in the user message, others put them in the system message. Some examples include metadata in the assistant response, others do not. The model tries to learn from this inconsistent data and ends up learning weak or confused patterns.

Consistency is not optional. Every example in your training dataset must follow the same structural template. If you use a system message in one example, use it in all examples. If you include context before the main input in one user message, include it in all user messages. If you output JSON in one assistant message, output JSON in all assistant messages. The model learns patterns from repeated structure. Inconsistent structure prevents pattern learning.

Another common mistake is including text in the assistant message that should not appear in production outputs. Teams often add explanations, reasoning traces, confidence scores, or metadata to assistant messages during data preparation, thinking this will help the model learn. It does not help. It teaches the model to generate that extra content in production. If you want the model to output just the extracted entity, show it training examples with just the extracted entity. If you show it examples with the entity plus an explanation, it will generate explanations in production.

Some teams make the opposite mistake: they compress assistant responses to be shorter than desired production outputs, hoping the model will expand them naturally. This also fails. The model learns to imitate the length and structure you show it. If you show it terse, compressed outputs, it will generate terse, compressed outputs. If you want detailed, thorough responses in production, you must provide detailed, thorough responses in training.

Role tag confusion is another frequent failure mode. Teams sometimes put instructions in the assistant message or put expected outputs in the user message, confusing the model about who is supposed to say what. The role tags define the semantic roles: system sets context, user provides input, assistant generates output. Violating this structure teaches the model incorrect turn-taking behavior.

Delimiter confusion happens when teams try to create custom role boundaries instead of using the platform-provided format. They invent their own tags like INSTRUCTION and RESPONSE or use separators like three dashes or equal signs. These custom delimiters are not recognized by the tokenizer as special tokens, so the model treats them as regular text. The model never learns the role structure you intended. Always use the platform's structured message format with proper role tags.

Another mistake is mixing formats within a single dataset. Some examples use instruction-response format, others use chat template format, others use completion format. The model cannot learn three different structural patterns simultaneously. Pick one format that matches your task and use it consistently across the entire dataset.

Finally, teams sometimes fail to validate that their formatted data actually matches what the model expects. They format examples by hand or with custom scripts, submit the training job, and assume it worked. But they never inspect the tokenized output to confirm that role tags are encoded correctly and turn boundaries are in the right places. When training fails, they debug the content without checking the format. Always validate your formatted data before training, especially when using custom formatting scripts or working with new model families.

## Practical Formatting Workflow for 2026

The robust approach to formatting in 2026 starts with choosing the correct format family for your task. Single-turn tasks use instruction-response format. Multi-turn conversational tasks use chat template format. Domain adaptation without task structure uses completion format. Make this decision before you create any training examples.

Once you have chosen the format, create a template example that defines the exact structure every example will follow. Write out the system message text if you are using one. Define what goes in the user message and in what order. Define what goes in the assistant message and in what format. This template becomes your specification. Every example you create must match it.

When creating training data, use structured data formats like JSON or JSONL where each example is a structured object with a messages array and each message has a role and content field. Do not create training data as plain text files with custom delimiters. Use structured formats that platforms can parse and validate. OpenAI's API expects JSONL with message objects. Anthropic's API expects similar structure. Hugging Face datasets use JSON or Parquet with message columns.

Validate your formatted data before submitting training jobs. Load a sample of examples and inspect the message structure. Confirm that role tags are consistent, that system messages are present where expected, that user and assistant messages alternate correctly in multi-turn examples. Use validation scripts that check for structural consistency: every example has the same roles in the same order, every assistant message contains content in the expected format, no user messages are empty.

Many platforms provide dataset validation tools as of 2026. OpenAI's fine-tuning API validates message structure when you upload files and returns errors if role tags are wrong or messages are malformed. Hugging Face's dataset library includes validators for chat template formats. Use these tools. They catch formatting errors before you waste compute on a training run that will fail.

When you submit training data to a platform API, the platform tokenizes your structured messages into the model-specific format automatically. You provide semantically structured messages, the platform applies the tokenizer and chat template. This separation of concerns means you focus on getting the semantic structure right and let the platform handle low-level encoding. But this only works if you submit properly structured data. Garbage in, garbage out applies to formatting as much as content.

If you are training on custom infrastructure or working with open-weight models, you must apply the chat template manually. Use the tokenizer's built-in chat template if available. Most modern tokenizers include a chat template method that converts structured messages into tokenized format. Do not write your own template logic unless you are absolutely certain you understand the model's expected format. Even small mistakes in special token placement break training.

## When Format Choice Affects Performance

Format choice directly affects what the model learns and how well it learns it. Using instruction-response format for a classification task is correct because classification is a single-turn task: you provide input, the model provides a category. Using chat template format for the same task adds unnecessary complexity and can degrade performance because you are teaching conversation structure when you only need single-turn response structure.

Conversely, using instruction-response format for a task that requires multi-turn interaction fails because the model never learns turn-taking behavior or how to maintain context across turns. A diagnostic agent trained only on single-turn instruction-response examples will treat every user message as a new conversation, losing all context from previous turns. You must use chat template format to teach conversational coherence.

Completion format teaches language and style but not task structure. If you fine-tune with completions, the model becomes better at generating domain-specific text but does not learn to follow instructions reliably. This is useful when combined with later instruction fine-tuning, but harmful if used alone for task-specific applications.

The format you choose also affects data efficiency. Instruction-response format is the most data-efficient for single-turn tasks because every example teaches a clear input-output mapping. Chat template format requires more examples to learn conversation dynamics because each multi-turn example contains more structure and variability. Completion format is the least data-efficient for task learning because the model must infer task structure from unstructured text rather than learning it from explicit examples.

In practice, most production fine-tuning in 2026 uses instruction-response format for single-turn tasks and chat template format for multi-turn tasks. Completion format is niche, used primarily for domain adaptation or continued pretraining. The industry has converged on structured instruction formats because they teach task behavior more reliably and more efficiently than unstructured completions.

Format is not a detail. It is the structure that makes learning possible. Choose the format that matches your task, apply it consistently across every example, validate that your formatted data matches what the model expects, and use platform tools to handle tokenizer-specific encoding. Get the format wrong and your training data becomes noise. Get it right and the model learns exactly what you intend.

The next question is how much data you need. Format tells the model what structure to learn. Dataset size determines whether you have enough examples to teach that structure reliably. We turn to dataset size requirements by task type and model size.
