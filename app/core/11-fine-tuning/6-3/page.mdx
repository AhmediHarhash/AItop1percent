# 6.3 — Medical Domain: Clinical Notes, Drug Interactions, and Safety Constraints

How do you fine-tune a model for a domain where a single omitted detail can delay life-saving treatment, where a misinterpreted symptom can lead to misdiagnosis, and where the model's failures result not in customer complaints but in patient harm? Medical domain adaptation operates under constraints that no other application faces: the overriding requirement that patient safety must never be compromised for functionality, efficiency, or user experience. Every decision you make—what data to train on, what tasks to automate, what evaluation standards to enforce—is filtered through a single question: if this model is wrong, will someone be hurt? The answer determines whether medical AI fine-tuning is engineering or malpractice.

The safety incident occurred in week nine. A cardiologist was reviewing an AI-generated note for a patient with atrial fibrillation being considered for anticoagulation. The note accurately documented the patient's symptoms, correctly identified the diagnosis, and appropriately suggested anticoagulation therapy. What it omitted was any mention of the patient's statement during the visit that he had experienced a gastrointestinal bleed six months prior. The physician had asked about bleeding history; the patient had disclosed it; the AI had heard it but not recognized its significance. The note recommended anticoagulation for a patient with recent major bleeding, a contraindication that would have been immediately obvious to any medical student.

The cardiologist caught the omission because she independently remembered the patient's statement and knew to look for bleeding history documentation. She immediately flagged the case to the hospital's safety team. A systematic review of 200 AI-generated notes found 17 instances where clinically significant information disclosed during visits was omitted from documentation, including medication allergies, prior adverse reactions, and contraindications for proposed treatments. The AI had learned what typical clinical notes contain but not what patient safety requires them to contain. The hospital terminated the pilot within 72 hours, and the healthcare AI company faced an FDA inquiry about whether their system constituted a medical device requiring premarket review.

Medical domain adaptation operates under the overriding constraint that patient safety must never be compromised for functionality, efficiency, or user experience. This constraint shapes every aspect of medical AI development: what tasks are appropriate for automation, what training data is acceptable, what evaluation standards must be met, and what deployment safeguards are required. Unlike legal AI where errors create liability exposure or financial AI where errors create economic loss, medical AI errors can directly harm patients. This distinction makes medical domain adaptation the most demanding and most regulated domain adaptation context.

## Patient Safety as Overriding Constraint

Patient safety is not a quality metric in medical AI; it is a categorical requirement that overrides all other considerations. A medical AI system that improves efficiency while introducing any safety risk is not acceptable. A system that achieves strong performance on average while creating even rare safety hazards is not acceptable. A system that works well for typical cases but fails dangerously on edge cases is not acceptable. The standard is not "better than average" or "good enough for most cases" but "safe enough that clinical harm is prevented even in atypical scenarios."

This standard creates fundamentally different tradeoffs than in other domains. In most applications, you accept some error rate as inevitable and manage it through downstream review or correction processes. In medical applications, certain error types are not acceptable at any rate because they create direct patient harm. A medication dosing error that suggests ten times the appropriate dose. A drug interaction warning that fails to flag a known dangerous interaction. A diagnostic support system that dismisses concerning symptoms as benign. These errors are not quality issues to be minimized; they are safety failures that must be prevented.

The safety constraint affects task selection. Some clinical tasks are too safety-critical for current AI reliability. Autonomous medication prescribing where the AI selects drugs and doses without physician review is not safe regardless of accuracy metrics because the consequences of error are too severe. Autonomous diagnostic decision-making where the AI determines diagnosis without physician verification is not safe because diagnostic errors lead directly to treatment errors. The appropriate role for AI in these contexts is decision support that assists physician judgment, not decision automation that replaces it.

Other clinical tasks have lower safety criticality and are more appropriate for AI assistance. Medical literature search and summarization. Clinical trial matching based on eligibility criteria. Administrative coding for billing. Patient communication and education. These tasks still require accuracy and quality, but errors do not create immediate patient harm. They are appropriate domains for AI systems that achieve high but imperfect reliability.

The distinction is not about how accurate the AI is but about what happens when it is wrong. An AI that achieves 95% accuracy on medication dosing suggestions is not safe for autonomous prescribing because the 5% error rate includes potentially fatal mistakes. The same AI might be perfectly appropriate for flagging dosing questions that physicians should review, because in that context the errors are caught by physician verification before reaching patients.

## Clinical Note Understanding and Generation

Clinical notes are the primary communication mechanism in healthcare, documenting patient encounters, supporting care continuity, justifying billing, and creating legal records. Understanding and generating clinical notes is a fundamental task for medical AI, but the challenge is not just extracting or producing information. Clinical notes must serve multiple functions simultaneously: they must document what the patient reported, what the clinician observed, what the clinician's assessment is, and what the plan forward involves. They must do this in a format that supports billing compliance, meets legal documentation requirements, and enables other clinicians to quickly extract decision-relevant information.

The structure of clinical notes reflects these multiple functions through standardized formats like SOAP notes: Subjective findings reported by the patient, Objective findings observed or measured by the clinician, Assessment of what these findings mean diagnostically, and Plan for treatment and follow-up. A medical AI fine-tuned on clinical notes must learn not just the SOAP format but what information belongs in each section and why these distinctions matter.

Subjective findings require faithful documentation of what patients report, not interpretation or summarization that might lose clinically significant details. When a patient describes chest pain, the exact characteristics matter: location, quality, duration, triggers, relieving factors, associated symptoms. An AI that summarizes "patient reports chest pain" has lost information essential for differential diagnosis. The model must learn to preserve detail in subjective documentation.

Objective findings require distinguishing observations from inferences. "Patient appears anxious" is an observation. "Patient is anxious due to work stress" is an inference that may not belong in the objective section. "Blood pressure 160 over 95" is an observation. "Blood pressure elevated due to medication non-adherence" mixes observation with inference. Teaching these distinctions requires training data annotated by clinicians who can identify where documentation inappropriately blends observation with interpretation.

Assessment requires clinical reasoning that connects findings to diagnoses through differential diagnosis thinking. The assessment is not just stating the most likely diagnosis but documenting the reasoning process: what findings support which diagnoses, what findings argue against alternatives, what uncertainties remain. A well-documented assessment allows other clinicians to understand and critique the diagnostic reasoning. An AI that generates assessments must learn to make reasoning explicit, not just state conclusions.

Plan documentation requires specificity about treatments, medications, follow-up, and contingencies. "Start metformin" is insufficient. "Start metformin 500mg twice daily with meals, titrate by 500mg weekly to target dose 2000mg daily as tolerated, monitor for GI side effects" is appropriate. The plan must be detailed enough that another clinician could execute it without guessing at intent. Teaching this specificity requires training data that exemplifies detailed plan documentation.

## Drug Interaction Detection and Medication Safety

Drug interaction detection is one of the most safety-critical applications of medical AI because drug interactions cause preventable patient harm at significant scale. Studies estimate that adverse drug events affect millions of patients annually, with drug interactions representing a substantial portion. An AI system that can reliably identify potential interactions before medications are prescribed could prevent substantial harm, but the reliability threshold must be extremely high because missing dangerous interactions creates direct patient risk.

The challenge is that drug interactions operate through multiple mechanisms and have varying clinical significance. Some interactions are pharmacokinetic: one drug affects how another drug is absorbed, metabolized, or eliminated, changing its effective dose. Some interactions are pharmacodynamic: drugs with similar or opposing effects combine to create excessive effect or reduced effect. Some interactions are indirect: drugs that affect common physiological pathways or compete for common binding sites.

The clinical significance of interactions ranges from irrelevant to life-threatening. Many interactions identified by drug databases are theoretically possible but clinically insignificant, occurring only at doses higher than therapeutic ranges or in patient populations not using the drugs. Other interactions are absolutely contraindicated, combinations that should never be prescribed together under any circumstances. A medical AI must distinguish between interactions that require awareness, interactions that require monitoring, interactions that require dose adjustment, and interactions that require avoiding the combination entirely.

Fine-tuning for drug interaction detection requires training data that captures interaction mechanisms, clinical significance, and management strategies. The data cannot be purely text from drug labels and interaction databases because these sources are often overly conservative, flagging theoretical interactions that practicing clinicians routinely override. The data must include real clinical decision-making about interactions: which interactions clinicians consider significant enough to modify prescribing, which they monitor without changing prescriptions, and which they ignore as clinically irrelevant.

This requires training data curated by clinical pharmacists and physicians who prescribe complex medication regimens. The curation must identify cases where interaction warnings were appropriately heeded and cases where they were appropriately overridden, teaching the model to distinguish between interactions that matter and interactions that are formally flagged but clinically unimportant. Without this clinical judgment in training data, the model will either over-warn, creating alert fatigue, or under-warn, missing dangerous interactions.

The evaluation requirement for drug interaction detection is zero tolerance for missed dangerous interactions. You can tolerate some over-warning for interactions of marginal significance, though excessive over-warning creates alert fatigue that causes clinicians to ignore all warnings. You cannot tolerate missing interactions that create serious patient harm. This asymmetric error tolerance means your evaluation must specifically test whether the model catches all known dangerous interactions, not just whether it achieves good average performance.

## Diagnostic Support and Differential Diagnosis

Diagnostic decision support is one of the most promising but most challenging applications of medical AI. Diagnosis is a reasoning process that combines pattern recognition with systematic evaluation of alternatives. A physician observes a patient's symptoms and signs, generates a list of possible diagnoses that could explain those findings, evaluates each possibility against the evidence, and arrives at the most likely diagnosis while acknowledging remaining uncertainty.

An AI system supporting this process cannot simply output the single most likely diagnosis. It must present differential diagnosis, the ranked list of possibilities with reasoning about what supports or contradicts each one. This is essential because diagnostic certainty is rarely absolute, because the most likely diagnosis is not always correct, and because considering alternatives prevents premature closure on an incorrect diagnosis. A diagnostic AI that outputs "pneumonia" without considering pulmonary embolism, heart failure, or other conditions that can present similarly is not providing decision support; it is encouraging diagnostic error.

Fine-tuning for diagnostic support requires training data that captures differential diagnosis reasoning, not just final diagnoses. This means clinical cases annotated with the full differential considered, the findings that supported each diagnosis, the findings that argued against each diagnosis, and the reasoning that led to the final assessment. Creating this training data is expensive because it requires expert physician annotation of substantial clinical detail.

The data must also include cases where the initial leading diagnosis was wrong and alternative diagnoses were ultimately correct, because learning from diagnostic errors is essential for teaching appropriate uncertainty. A training set containing only cases where the obvious diagnosis was correct teaches the model to be overconfident. Including diagnostically challenging cases where multiple diagnoses were plausible and where the correct diagnosis was not initially obvious teaches appropriate caution and thoroughness.

Evaluation for diagnostic support must assess both accuracy and appropriate uncertainty expression. A system that achieves high accuracy on typical cases but expresses false confidence on atypical cases is dangerous. A system that appropriately acknowledges uncertainty even when correct is safer than a system that is confident even when wrong. You need evaluation approaches that measure not just whether the correct diagnosis appears in the output but whether the reasoning and uncertainty communication are appropriate.

## HIPAA Compliance and De-identification Requirements

Medical AI training in the United States operates under HIPAA regulations that strictly limit use and disclosure of protected health information. Training a model on clinical data requires either patient authorization for each patient whose data is used, which is impractical at scale, or de-identification of the data to remove individually identifying information. De-identification is the standard approach, but it creates both technical challenges and risks of re-identification if not done properly.

HIPAA specifies two de-identification methods: the Safe Harbor method, which requires removing eighteen categories of identifiers, and the Expert Determination method, which requires a qualified expert to determine that re-identification risk is very small. Most medical AI training uses Safe Harbor de-identification, removing direct identifiers like names, dates, geographic information finer than state level, and other specified elements.

The challenge is that removing identifiers changes the clinical data in ways that can affect model training. Dates are clinically significant: the timing of symptom onset relative to medication changes, the interval between diagnoses, the age at first presentation. Removing specific dates and replacing them with relative time offsets preserves some temporal information but loses others. Geographic information matters for region-specific conditions, endemic diseases, and healthcare access patterns. Removing it eliminates features relevant to clinical reasoning.

More subtly, aggressive de-identification can remove or obscure rare conditions, unusual presentations, or unique patient characteristics that are individually identifying precisely because they are clinically distinctive. A patient with a rare genetic condition might be identifiable from that condition in a dataset, requiring the case to be excluded. But training on only common conditions teaches the model only about common cases, reducing its ability to recognize rare but important diagnoses.

You must balance privacy protection with training data quality. Use expert review to identify what can be safely retained while meeting de-identification requirements. Use synthetic data generation to create training examples for rare conditions or unusual presentations that cannot be safely included from real data. Use federated learning or other privacy-preserving techniques that allow learning from sensitive data without centralizing it. The goal is to achieve both strong privacy protection and sufficient clinical diversity for safe model performance.

## FDA Software as Medical Device Considerations

Medical AI systems may be regulated by the FDA as medical devices if they are intended for diagnosis, treatment, prevention, or mitigation of disease. The FDA's guidance on Software as a Medical Device distinguishes between systems that inform clinical decision-making, which generally receive lighter regulatory scrutiny, and systems that drive clinical decision-making, which may require full device premarket review.

A clinical documentation assistant that helps physicians write notes more efficiently is likely not a medical device because it informs documentation, not clinical decisions. A diagnostic support system that suggests possible diagnoses for physician consideration is borderline; it depends on how it is marketed and how much clinical reliance it encourages. An autonomous diagnostic system that determines diagnosis without physician review is clearly a medical device requiring premarket approval.

The distinction affects what evidence you need to deploy your system. Non-device software requires internal validation that it works as intended but not external regulatory approval. Device software requires clinical validation studies demonstrating safety and effectiveness, premarket submission to FDA, and ongoing post-market surveillance. The regulatory burden is substantial, often requiring years and millions of dollars for high-risk devices.

Most medical AI developers in the decision support space aim to stay outside device regulation by positioning their systems as informing rather than driving decisions and by requiring physician review of all outputs before clinical use. This is not just regulatory strategy; it is sound safety practice. Current AI reliability does not support fully autonomous clinical decision-making in most contexts. Physician review is not a regulatory workaround but a necessary safety control.

If your system does constitute a medical device, engage with FDA early in development. The agency offers pre-submission meetings where you can discuss your regulatory pathway before committing to full development. Understanding whether you need 510k clearance based on substantial equivalence to existing devices, de novo classification for novel device types, or PMA premarket approval for high-risk devices shapes your development timeline and evidence requirements.

## Working with Clinician Subject Matter Experts

Medical domain adaptation requires intensive collaboration with practicing clinicians who have the expertise to evaluate clinical appropriateness, identify safety issues, and articulate the tacit knowledge that makes expert clinical reasoning effective. These clinicians must represent the specialties and practice contexts your system addresses, must have sufficient seniority to recognize subtle clinical errors, and must be willing to invest significant time in training data curation and output evaluation.

The challenges in clinician engagement are similar to but more acute than in legal domain adaptation. Physicians are highly compensated, often exceeding $300 per hour for specialists. They have demanding clinical schedules and little spare capacity. The work you are asking them to do, annotating training data and reviewing AI outputs, is intellectually less engaging than practicing medicine. The value proposition must be compelling.

The most effective approach is engaging clinicians who will directly benefit from the system's success. Physicians at a health system building an internal AI tool have incentive to invest in its quality. Academic physicians who can publish research based on the AI development have incentive aligned with scholarly goals. Physician entrepreneurs building a medical AI product they plan to commercialize have strong financial incentive. External consultants brought in purely for annotation have the weakest alignment and typically produce the lowest quality contributions.

Even with aligned clinicians, you must structure the engagement to respect their expertise and time. Do not treat physicians as data labelers. Engage them in high-leverage activities: selecting challenging clinical cases that teach important distinctions, annotating complex scenarios that require expert judgment, evaluating system performance on difficult cases, and articulating the reasoning behind their clinical decisions. Use medical students, residents, or trained non-clinicians for lower-level annotation tasks under physician supervision.

The clinician engagement must also address interpersonal dynamics within medicine. Specialties have different practice patterns and sometimes different clinical opinions. A cardiologist and an emergency physician may have different approaches to the same clinical scenario, both appropriate for their contexts. Your training data and evaluation must acknowledge this variation rather than treating one approach as correct and others as wrong. Medicine is not algorithmic; clinical judgment involves weighing tradeoffs that different clinicians may weigh differently.

## Liability and Standard of Care Implications

Medical AI creates liability exposure for clinicians who use it, for health systems that deploy it, and potentially for developers who create it. A physician who relies on incorrect AI-generated information that leads to patient harm faces malpractice liability. A hospital that deploys an AI system that contributes to patient injuries faces institutional liability. A developer who makes unsupported claims about AI capabilities might face liability if those claims induce harmful reliance.

The standard of care question is particularly complex for medical AI. As AI systems become more prevalent, does the standard of care evolve to include using AI decision support for certain tasks? If a diagnostic AI system reliably catches rare diagnoses that physicians commonly miss, does failing to use that system fall below the standard of care? These questions are being litigated in early cases and will shape how medical AI affects malpractice law.

For now, the conservative approach is to position medical AI as augmenting rather than replacing physician judgment, to require physician review of all AI outputs before clinical action, and to document that the system is a tool supporting clinical decision-making rather than making decisions autonomously. This positioning manages liability risk while acknowledging current AI limitations.

From a developer perspective, you must be extremely careful about capability claims. Do not claim that your system achieves expert-level diagnostic accuracy unless you have rigorous evidence supporting that claim. Do not suggest that physicians can safely rely on your system without verification. Do not downplay limitations or rare failure modes. Overclaiming capabilities creates both regulatory risk and liability exposure if users rely on exaggerated claims.

## Evaluation Standards for Medical AI

Medical AI evaluation must demonstrate both technical performance and clinical safety. Technical performance means the system achieves strong accuracy, precision, and recall on relevant medical tasks measured against gold standard labels. Clinical safety means the system does not create patient harm through errors, omissions, or inappropriate outputs.

Standard machine learning metrics are necessary but not sufficient for medical AI evaluation. You need accuracy, F1 scores, and AUC on medical benchmarks. But you also need clinician evaluation of outputs on realistic clinical scenarios, safety testing on adversarial cases and edge conditions, and prospective evaluation in clinical settings before widespread deployment.

Clinician evaluation involves having physicians review AI outputs and assess whether they would be comfortable using those outputs in patient care. This is a higher bar than whether the outputs are technically correct. An output can be factually accurate while clinically inappropriate if it omits important caveats, if it is overconfident about uncertain matters, or if it uses terminology that might confuse rather than clarify.

Safety testing specifically targets scenarios where AI errors could cause harm. Drug interaction detection must be tested on all known dangerous interactions. Diagnostic support must be tested on conditions commonly misdiagnosed and on presentations that mimic more serious conditions. Medication dosing assistance must be tested on edge cases like renal impairment or drug-drug interactions that affect metabolism.

Prospective evaluation in clinical settings, even limited pilots, reveals issues invisible in retrospective testing. Clinicians interact with the AI in ways you did not anticipate, clinical workflows create contexts that affect how outputs are interpreted, and real patient variability stresses the system in ways synthetic test cases do not. You cannot fully evaluate medical AI without observing it in clinical use, but you must limit that evaluation to contexts where patient safety is protected through close monitoring and easy override.

## When Medical Domain Adaptation Is Justified

Medical domain adaptation through fine-tuning is justified for high-volume clinical tasks where general-purpose models fail to meet clinical safety and accuracy requirements, where the clinical value justifies the substantial investment in expert clinician involvement, and where you can achieve regulatory clarity about the system's status and requirements.

Medical domain adaptation is not justified for occasional medical tasks, for clinical contexts where you lack access to expert annotation and evaluation, for tasks where current AI reliability is insufficient for safe clinical use even with physician review, or for applications where regulatory uncertainty creates unacceptable risk.

The bar for medical AI is higher than for any other domain because patient safety is non-negotiable and because the regulatory environment is more demanding. You must be conservative about what tasks you attempt, rigorous about safety evaluation, and honest about limitations. The next subchapter examines financial domain adaptation, where regulatory requirements and economic consequences create a different but equally demanding set of constraints.
