# 2.1 — Why Training Data Quality Determines the Ceiling

In March 2025, a legal technology company spent four months fine-tuning Claude Opus 4 for contract review. They collected 18,000 training examples from their production contract database, hired three contract attorneys to label key clauses and risk indicators, ran 240 hours of training across two QLoRA iterations, and achieved 94% accuracy on their held-out test set. The team was confident. They deployed the model to production in May 2025, routing 30% of contract reviews through the fine-tuned model. Within three weeks, their general counsel flagged a problem: the model was missing non-standard indemnification clauses that appeared in custom agreements negotiated by enterprise clients. The test set had not included these cases because they represented only 2% of contracts. The training set had not included them either because the production database skewed toward standard agreements. The model had learned to identify common clause patterns with high accuracy, but it could not generalize to the rare, high-stakes variations that mattered most to the business. The root cause was not the model architecture, not the hyperparameters, and not the training infrastructure. It was the training data. The data did not cover the full distribution of real-world contracts, and no amount of tuning could fix that gap. The model's ceiling was set by the data's ceiling, and the data's ceiling was too low.

This is the fundamental law of fine-tuning: **data quality determines the performance ceiling**. Your model can never be better than the data you train it on. You can optimize hyperparameters, extend training time, apply regularization, use advanced techniques like DPO or RLHF, and still hit the ceiling if the training data is incomplete, noisy, biased, or unrepresentative. The model learns patterns from the data. If the patterns in the data are wrong, incomplete, or skewed, the model learns wrong, incomplete, or skewed behavior. There is no algorithmic trick that fixes bad data.

## The Data Quality Ceiling Concept

The data quality ceiling is the maximum performance your model can achieve given the characteristics of your training data. If your training data is 95% accurate, your model's accuracy ceiling is somewhere around 95%, and likely lower because models rarely reach perfect fit on training data and always lose some performance on unseen test data. If your training data covers 80% of real-world cases, your model's coverage ceiling is around 80%. If your training data contains systematic biases, your model will inherit those biases.

The ceiling exists because models learn by generalizing from examples. The model has no knowledge beyond what it observes in training data. If an edge case never appears in training, the model has no signal for how to handle it. The base model may have some general knowledge from pre-training that helps, but fine-tuning shifts the model's behavior toward the training distribution. If the training distribution does not represent production reality, the fine-tuned model will underperform on production cases.

Data quality has multiple dimensions. **Accuracy** is the degree to which labels and annotations are correct. If you are training a classification model and 10% of your labels are wrong, the model will learn incorrect mappings. **Coverage** is the degree to which the training data represents the full range of inputs the model will encounter in production. If your production data includes ten types of queries but your training data includes only seven, the model will struggle on the missing three. **Balance** is the degree to which different classes or scenarios are represented proportionally. If your task has a rare but critical class that makes up 1% of production cases, and that class is underrepresented in training data at 0.1%, the model will underperform on that class. **Consistency** is the degree to which similar cases are labeled similarly. If one annotator labels a case as positive and another labels an identical case as negative, the model receives conflicting signals.

Each dimension contributes to the ceiling. A dataset that is 98% accurate but covers only 60% of production cases will produce a model with a low ceiling due to coverage gaps. A dataset that is 90% accurate and covers 95% of cases but is highly imbalanced will produce a model with a low ceiling on minority classes. The ceiling is determined by the weakest dimension. You cannot compensate for low coverage with high accuracy. You cannot compensate for poor balance with more training data if the imbalance persists.

The legal technology company's training data had high accuracy — the contract attorneys labeled clauses correctly in the cases they reviewed — but low coverage. The rare enterprise contracts with non-standard clauses were missing. The model's ceiling was capped by the coverage gap, and no hyperparameter tuning would change that.

## Why More Data Does Not Fix Quality Problems

A common misconception is that more data fixes quality problems. If the training data has gaps, just collect more data. If the training data is noisy, just collect so much data that the noise averages out. This logic is wrong. More data helps only if the additional data improves quality along the relevant dimensions. Adding more low-quality data does not raise the ceiling — it reinforces the same problems at larger scale.

If your training data is biased toward common cases and underrepresents edge cases, collecting more data with the same collection method will give you more common cases and the same underrepresentation of edge cases. The model will become more confident in its handling of common cases, but the edge case problem remains. If your training data has inconsistent labels because different annotators have different interpretations of the labeling guidelines, collecting more data with the same annotators and guidelines will produce more inconsistent labels. The model will learn the inconsistency, not overcome it.

More data is valuable when it increases coverage, improves balance, or adds harder examples that challenge the model. If you identify that your training data is missing certain types of cases, and you specifically collect data to fill those gaps, more data helps. If you identify that a minority class is underrepresented, and you oversample or collect more examples of that class, more data helps. But indiscriminately adding data without addressing the quality problem wastes effort and may make the model worse by further entrenching poor patterns.

A customer support automation company fine-tuned a model for intent classification. Their initial training set had 5,000 examples and achieved 82% accuracy on the test set. They believed more data would improve performance, so they collected an additional 15,000 examples from the same production logs using the same sampling method. After retraining on 20,000 examples, accuracy improved to 84%. They added another 30,000 examples. Accuracy improved to 85%. Diminishing returns. The problem was that the production logs skewed heavily toward three common intents: billing questions, password resets, and order status inquiries. The rare intents — account closures, refund disputes, technical troubleshooting — remained underrepresented even at 50,000 examples. The model's ceiling was around 85% because the rare intents were learned poorly. Adding more data without addressing the imbalance did not help. When the team oversampled the rare intents and retrained on a balanced 10,000-example set, accuracy jumped to 91%.

## The Compounding Effect of Data Errors

Data errors compound. A small percentage of incorrect labels in training data does not degrade model performance by the same small percentage — it degrades performance more because errors create conflicting signals that confuse the model's learning process.

Suppose you are training a binary classifier and 5% of your positive examples are mislabeled as negative. The model sees positive cases labeled negative and learns that some features associated with positive cases should predict negative. When the model encounters a true positive case in production, it may predict negative because it learned from the mislabeled examples that those features indicate negative. The error rate in production is higher than 5% because the mislabeled examples corrupt the model's understanding of the decision boundary.

The compounding effect is worse when errors are systematic rather than random. Random errors — where mislabeling is evenly distributed across all cases — add noise but do not systematically bias the model. Systematic errors — where mislabeling occurs more frequently for certain types of cases — bias the model's behavior on those cases. If you are training a fraud detection model and your labeling process misses subtle fraud patterns, labeling them as non-fraud, the model will learn that those patterns are non-fraud. When it encounters similar patterns in production, it will classify them as non-fraud, even though they are fraud. The systematic error has taught the model the wrong pattern.

A healthcare technology company fine-tuned a model for clinical note summarization. The training data consisted of 12,000 clinical notes paired with summaries written by nurses. The notes were from an electronic health record system, and the summaries were created during discharge workflows. The team did not audit the summaries for quality — they assumed that summaries written by clinical staff were accurate. After deployment, physicians flagged that the model's summaries were omitting key medication changes documented in the notes. Investigation revealed that the training summaries had the same problem: nurses writing discharge summaries sometimes omitted recent medication changes because the discharge workflow focused on diagnosis and treatment plan, not ongoing medication management. The model had learned to replicate the summary style in the training data, which included the systematic omission of medication changes. The error rate was around 8% in training data and 9% in production — not a huge difference numerically, but clinically significant because omitted medication information can lead to adverse events.

The lesson is that even small error rates in training data can produce unacceptable failure rates in production if the errors are systematic and affect high-stakes cases. Data quality auditing must focus on finding and fixing systematic errors, not just reducing overall error rates.

## The Relationship Between Data Quality and Model Quality

The relationship between data quality and model quality is not linear. Improving data quality from 70% to 80% does not necessarily improve model performance by 10 percentage points. The relationship depends on which quality dimensions improve and how those dimensions interact with the model's learning dynamics.

Improving accuracy has diminishing returns. Going from 80% accurate labels to 90% accurate labels typically produces a larger performance gain than going from 90% to 95%, which produces a larger gain than going from 95% to 98%. At very high accuracy levels, the remaining errors are often hard cases that annotators disagree on, and the model may not learn much from correcting those errors because the cases are genuinely ambiguous.

Improving coverage has increasing returns up to a saturation point. Adding coverage for missing cases can produce large performance gains because the model was previously guessing on those cases. Once coverage reaches the point where the training data represents all major case types, further coverage improvements have smaller impact.

Improving balance has non-linear effects. Balancing a dataset that is 99% majority class and 1% minority class by oversampling the minority class can produce dramatic improvements in minority class performance because the model finally has enough signal to learn minority patterns. Balancing a dataset that is 60% majority and 40% minority may have smaller impact because the model already had reasonable signal for both classes.

The practical implication is that you should focus quality improvement efforts on the dimension that is currently limiting performance. If coverage is low, adding coverage produces the biggest gains. If coverage is high but accuracy is low, improving accuracy produces the biggest gains. If both coverage and accuracy are high but the dataset is imbalanced, balancing produces the biggest gains.

A fraud detection team analyzed their fine-tuned model's performance and found that it performed well on common fraud patterns but poorly on emerging fraud tactics. The issue was coverage — the training data was historical and did not include recent fraud tactics. They collected 500 examples of recent fraud cases and retrained. Performance on emerging tactics improved from 62% to 88%, a 26-point gain from 500 examples. They then invested in improving label accuracy for the full training set, correcting mislabeled cases. Performance improved from 88% to 91%, a 3-point gain from thousands of label corrections. The coverage fix produced much larger returns than the accuracy fix because coverage was the limiting factor.

## Data Quality as an Investment

Data quality is not free. Improving data quality requires time, money, and expertise. You must decide how much to invest in data quality based on the task's value, the acceptable failure rate, and the cost of errors. For high-stakes tasks where errors have significant consequences, data quality investment is non-negotiable. For low-stakes tasks where errors are tolerable, excessive data quality investment may not be justified.

The investment in data quality includes collection costs, labeling costs, review and auditing costs, tooling costs for quality checks, and iteration costs when you discover quality problems and must redo work. High-quality data typically costs more per example than low-quality data because it requires expert annotators, multiple reviewers, and rigorous quality control processes.

You must balance quality and quantity. Given a fixed budget, you can collect a large dataset with moderate quality or a smaller dataset with high quality. The right choice depends on the task. For tasks where the base model already has strong general capabilities and you are fine-tuning to adapt tone or format, a smaller high-quality dataset is usually better. For tasks where the base model lacks domain knowledge and you are fine-tuning to teach new concepts, a larger dataset with moderate quality may be better as long as the quality is above a minimum threshold.

A financial services company faced this tradeoff when fine-tuning a model for investment report generation. They could afford to create 2,000 high-quality examples with detailed expert review at 50 dollars per example, or 10,000 moderate-quality examples with basic review at 10 dollars per example. They chose the high-quality path, creating 2,000 examples with review by senior analysts. The model performed well, generating reports that met the firm's standards. They later experimented with the moderate-quality path, training a separate model on 10,000 examples with basic review. Performance was noticeably worse, with more factual errors and weaker narrative structure. The high-quality path was the right choice for this high-stakes task.

Data quality investment also includes monitoring and maintenance. Data quality degrades over time as production distributions shift, as new edge cases emerge, and as annotator drift occurs. You must invest in ongoing quality monitoring, periodic audits, and data refreshes to keep training data aligned with production reality.

The investment in data quality pays returns in model reliability, fewer production incidents, lower retraining costs, and faster iteration cycles. A model trained on high-quality data requires less debugging, fewer production fixes, and less frequent retraining. A model trained on low-quality data generates constant firefighting, user complaints, and emergency patches. The upfront investment in quality reduces long-term operational costs.

## Real Examples of Teams That Hit the Ceiling Because of Data Problems

A travel booking platform fine-tuned a model to generate personalized trip recommendations. The training data consisted of user search queries paired with the trips users ultimately booked. The model achieved strong performance on the test set and was deployed. Within two months, the product team noticed that the model rarely recommended international trips, even for users with search histories indicating interest in international travel. Investigation revealed that the training data was biased toward domestic trips because the platform's user base was 85% domestic travelers. The model learned the domestic bias. The team collected additional training data for international trips, oversampling international bookings, and retrained. Recommendation diversity improved.

A recruiting technology company fine-tuned a model to screen resumes for job matches. The training data consisted of historical resumes paired with hiring decisions made by recruiters. The model achieved 89% agreement with recruiter decisions on the test set. After deployment, candidates from non-traditional backgrounds — career changers, self-taught developers, international applicants — complained that the model was rejecting them despite strong qualifications. Analysis revealed that the historical hiring data reflected biases in past recruiter decisions: recruiters had historically favored candidates from traditional educational backgrounds. The model learned the historical bias. The team redesigned the training data to include resumes from diverse backgrounds with labels based on role requirements rather than historical hiring patterns. Bias metrics improved, and the model became more equitable.

A logistics company fine-tuned a model to predict delivery delays. The training data included weather conditions, traffic patterns, package weight, and delivery address. The model performed well in testing but underperformed in production during the winter holiday season. The training data had been collected during non-holiday periods and did not include the extreme volume spikes, weather disruptions, and staffing shortages that occur in December. The model's ceiling was capped by the lack of holiday data. The team collected data from the previous holiday season and retrained. Performance during the next holiday season improved significantly.

These examples share a common pattern: the training data did not represent the full production distribution, and the model could not generalize beyond the training distribution. The fix in every case was to improve the training data, not to tune the model.

## How to Think About the Ceiling Before You Start Training

Before you begin fine-tuning, you should estimate the ceiling imposed by your training data. This estimate guides decisions about whether to proceed with training or whether to invest more in data collection and quality improvement first.

The ceiling estimation process starts with a manual review of the training data. Sample 100 to 200 random examples from the training set and evaluate them against the production task. Ask: do these examples cover the types of cases I expect to see in production? Are the labels accurate? Are there systematic gaps or biases? If you find significant quality issues in the sample, the full dataset likely has the same issues at scale.

Next, compare the training data distribution to the production data distribution. If you have production logs, analyze the distribution of input types, query patterns, edge cases, and failure modes. Compare this distribution to the training data distribution. If the distributions are misaligned — for example, if production has 15% of cases in a category that makes up 2% of training data — you have a coverage problem that will cap the model's performance on that category.

You can also benchmark the ceiling by having human experts perform the task using only the information available in the training data. If experts achieve 92% accuracy on the task when working from training data examples, your model's ceiling is likely around 92% or lower. If experts struggle with certain cases because the training data is ambiguous or incomplete, the model will struggle too.

A content moderation team estimated the ceiling before fine-tuning a model to detect policy violations. They sampled 200 training examples and had three content moderators independently label them. Agreement among moderators was 87%, meaning 13% of cases were ambiguous or inconsistently labeled. The team concluded that the model's ceiling was around 87% because the model would receive conflicting signals on ambiguous cases. They decided to invest in clarifying the labeling guidelines and relabeling the dataset before training. After relabeling, inter-annotator agreement improved to 94%, raising the estimated ceiling. They proceeded with training and achieved 91% accuracy, close to the new ceiling.

If the ceiling estimate is too low for the task requirements, do not train yet. Invest in data quality improvement until the ceiling estimate meets the requirements. Training on low-ceiling data wastes compute and produces a model that cannot meet production needs.

## What Happens When You Ignore the Ceiling

When you ignore the data quality ceiling and proceed with training despite known data problems, you produce a model that underperforms in production, requires extensive post-deployment fixes, and erodes trust in fine-tuning as a technique.

The most common outcome is a model that performs well on the test set but poorly in production because the test set has the same coverage gaps and biases as the training set. The test set validates that the model learned the training distribution, but if the training distribution is not representative of production, test performance is misleading.

Another common outcome is a model that performs well on average but catastrophically fails on important edge cases. The model handles the 90% of cases that were well-represented in training data but mishandles the 10% of high-stakes cases that were missing or poorly represented. These edge case failures generate user complaints, escalations, and reputational damage.

A third outcome is a model that reflects and amplifies biases present in the training data. If the training data encodes historical biases, discriminatory patterns, or systemic inequities, the model learns those patterns and applies them at scale. This creates legal risk, regulatory scrutiny, and harm to users.

Ignoring the ceiling also wastes resources. You spend time and money training a model that cannot meet production requirements, then spend more time and money retraining on better data. The initial training effort is wasted. If you had audited the data quality first and fixed the problems before training, you would have trained once and deployed successfully.

The legal technology company that missed non-standard indemnification clauses spent four months training a model on incomplete data, deployed it, discovered the coverage gap, collected additional data, and retrained. The total timeline was eight months from start to successful deployment. If they had audited the training data for coverage gaps before the first training run, they could have collected the missing cases upfront and deployed in five months.

## The Ceiling as a Go/No-Go Decision

The data quality ceiling is not just a performance prediction — it is a go/no-go decision point. If the ceiling is below the minimum acceptable performance for the task, you should not proceed with fine-tuning. You should either improve the data quality until the ceiling meets requirements, or you should use a different approach.

For some tasks, the acceptable performance threshold is high. A medical diagnosis support tool may require 98% accuracy because false negatives can lead to missed diagnoses and patient harm. If your training data ceiling is 92%, fine-tuning is not viable. You must improve data quality or use a different solution.

For other tasks, the acceptable performance threshold is lower. A draft email generator may tolerate 80% quality because users will review and edit the drafts. If your training data ceiling is 85%, fine-tuning is viable.

The go/no-go decision also considers the cost of errors. For tasks where errors are low-cost and easily correctable, a lower ceiling may be acceptable. For tasks where errors are high-cost and difficult to detect, a higher ceiling is required.

A legal contract review system requires a high ceiling because missed clauses can lead to financial liability. An internal document summarization tool requires a lower ceiling because users can verify summaries against source documents. The fine-tuning decision depends on the ceiling relative to the task's requirements.

## Measuring the Ceiling During Development

You do not measure the ceiling once and forget about it. The ceiling changes as you improve the training data, as you collect more examples, and as you refine your labeling guidelines. You should measure the ceiling at multiple points during data collection and use those measurements to guide your quality improvement efforts.

The first ceiling measurement happens during the data collection planning phase. Before you collect any data, you estimate the ceiling based on the quality of your data sources, the expertise of your annotators, and the coverage of your sampling plan. This initial estimate is rough but useful for setting expectations and identifying obvious problems.

The second measurement happens after collecting a pilot dataset. You collect a small sample — 500 to 1,000 examples — and audit it thoroughly. You check label accuracy, coverage, balance, and consistency. You have experts independently label a subset and measure agreement. You train a small model on the pilot data and evaluate it on a separate test set. This pilot measurement gives you a concrete estimate of the ceiling based on real data. If the pilot ceiling is too low, you adjust your collection strategy before scaling up.

The third measurement happens during full-scale data collection. You audit samples of the growing dataset at regular intervals — every 2,000 examples, for instance. You track quality metrics over time and watch for degradation. Annotator fatigue, guideline drift, and edge case accumulation can lower quality as the dataset grows. Regular audits catch these problems early.

The fourth measurement happens after data collection is complete and before training begins. You conduct a final comprehensive audit of the full dataset. You check for systematic errors, coverage gaps, and quality outliers. You measure inter-annotator agreement on a holdout sample. You estimate the ceiling one last time and decide whether to proceed with training or invest in further data improvement.

A fraud detection team measured the ceiling at each stage. Their initial estimate was 88% based on historical data quality. Their pilot dataset of 800 examples achieved 85% on a test set, close to the estimate. During full-scale collection, they audited every 2,000 examples and found that quality was stable until the 10,000-example mark, when a new annotator joined and introduced inconsistent labels. They paused collection, retrained the annotator, and re-labeled the affected examples. The final audit of 15,000 examples estimated a ceiling of 89%. They proceeded with training and achieved 86% in production, within the expected range.

## The Iterative Ceiling Improvement Process

Raising the ceiling is an iterative process. You measure the ceiling, identify the limiting factor, address that factor, and measure again. Each iteration raises the ceiling incrementally until you reach a point where further improvement is not cost-effective or not feasible.

The first iteration typically focuses on accuracy. You audit labels, find errors, and correct them. You clarify ambiguous guidelines and relabel inconsistent cases. This iteration can raise the ceiling by five to ten percentage points if initial label quality was poor.

The second iteration typically focuses on coverage. You analyze the training data distribution, compare it to the production distribution, identify gaps, and collect additional examples to fill those gaps. This iteration raises the ceiling for underrepresented cases.

The third iteration typically focuses on balance. You measure class distribution, identify imbalances, and oversample or collect more examples for minority classes. This iteration raises the ceiling for minority class performance.

Subsequent iterations focus on progressively harder problems: handling ambiguous cases, improving consistency at edge cases, refining subtle distinctions, and removing annotation bias. Each iteration produces smaller gains, and you eventually hit diminishing returns where the cost of further improvement exceeds the value.

An insurance claims processing team went through five ceiling improvement iterations. Iteration one corrected labeling errors and raised the estimated ceiling from 81% to 87%. Iteration two added coverage for rare claim types and raised the ceiling to 90%. Iteration three balanced the dataset by oversampling denied claims and raised the ceiling to 92%. Iteration four refined guidelines for ambiguous cases and raised the ceiling to 93%. Iteration five attempted to improve handling of fraudulent claims but only raised the ceiling to 93.5%, so they stopped there and proceeded with training.

## The Cost of Ignoring Systematic Quality Issues

Systematic quality issues — errors that occur consistently for certain types of cases — are more damaging than random noise. A dataset with 5% random label errors may still produce a usable model. A dataset with 5% systematic errors in a critical category will produce a model that fails on that category.

Systematic errors are often invisible in aggregate metrics. If you measure overall label accuracy and see 94%, you might assume the data is high quality. But if 20% of rare cases have incorrect labels and those cases represent only 5% of the dataset, the aggregate accuracy hides the problem. The model will perform well overall but fail on the rare cases.

The solution is to measure quality not just in aggregate but by case type, by difficulty level, and by label class. Stratified quality audits reveal systematic errors that aggregate metrics miss. You sample examples from each category, each difficulty tier, and each label class, and measure accuracy within each stratum. If you find that one stratum has much lower quality than others, you have identified a systematic problem.

A hiring screening model trained on historical hiring data had aggregate label accuracy of 93%. Stratified audit revealed that labels for candidates with non-traditional backgrounds had only 78% accuracy because historical recruiters had inconsistent standards for those candidates. The systematic error meant the model performed poorly on non-traditional candidates despite high aggregate accuracy. The team relabeled the non-traditional candidate examples with clear standards and raised that stratum's accuracy to 91%. Model performance on non-traditional candidates improved significantly.

## Long-Term Ceiling Management

The ceiling is not static. As your product evolves, as your task requirements change, and as production data distribution shifts, the ceiling changes. You must monitor the ceiling over time and refresh the training data when the ceiling falls below acceptable levels.

Production distribution shift is the most common cause of ceiling degradation. Your training data represents the production distribution at the time of collection. Six months later, users are asking different questions, using different language, or encountering different edge cases. The training data no longer covers the current distribution, and the ceiling drops.

Task evolution is another cause. You add new product features, update policies, or change business rules. The training data reflects old features and old rules. The model cannot handle the new cases because they were not in the training set.

Regulatory changes can also lower the ceiling. A new regulation requires different handling of certain cases. The training data was collected before the regulation and reflects pre-regulation behavior. The model must be retrained on data that reflects the new regulation.

You manage long-term ceiling health by monitoring production performance, tracking distribution shift, auditing training data relevance, and scheduling periodic data refreshes. Some teams refresh training data quarterly, others annually, depending on how fast their domain evolves. Regulated teams may refresh data after every significant regulatory change.

A customer support fine-tuned model had a ceiling of 91% when deployed in January 2025. By October 2025, production performance had dropped to 84%. Investigation showed that the company had launched new features in March and updated return policies in June. The training data did not include these changes. The team collected 2,000 new examples covering the new features and updated policies, merged them with the original training data, and retrained. Performance recovered to 89%.

## The Ceiling as a Communication Tool

The ceiling concept is a powerful communication tool for managing stakeholder expectations. When Product asks why the model is only 87% accurate, you can explain that the training data ceiling is 88%, and the model is performing near the ceiling. The conversation shifts from "why is the model bad" to "how do we improve the data."

When Leadership asks if fine-tuning will solve a problem, you can estimate the ceiling based on available data and give an honest answer. If the ceiling is too low, you recommend a different solution or a data quality investment before fine-tuning. This prevents wasted effort on projects that cannot succeed due to data constraints.

When Legal or Compliance asks about model limitations, you can point to the ceiling and explain which cases the model handles well and which cases it struggles with based on training data coverage. This supports risk assessment and helps set appropriate human oversight requirements.

The ceiling also guides prioritization. If you have limited budget for data quality improvement, you prioritize improvements that raise the ceiling the most. If coverage is the limiting factor, you invest in expanding coverage. If accuracy is the limiting factor, you invest in better labeling. The ceiling measurement tells you where to invest.

## The Relationship Between Ceiling and Model Architecture

The ceiling is primarily determined by data quality, but model architecture and size also play a role. A small model may not have the capacity to learn complex patterns even if the training data is perfect. A large model may overfit on a small dataset and fail to generalize.

For most fine-tuning projects in 2026, using models like GPT-4.5, Claude Opus 4, or Llama 3, model capacity is not the limiting factor. These models have billions of parameters and strong pre-trained capabilities. The ceiling is almost always determined by data quality, not model capacity.

However, if you are fine-tuning a very small model — a few hundred million parameters — on a complex task, model capacity may limit performance even if the data is perfect. In that case, you face a joint optimization problem: improve data quality and choose a larger model.

The practical takeaway is that for modern large language models, you should assume data quality sets the ceiling and invest in data quality before worrying about model size or architecture. If you improve the data and still hit a performance wall, then consider whether a larger or different model would help.

The next chapter covers data collection strategies: how to source training data from internal logs, how to author data from scratch with domain experts, and how to use expert annotation to label model outputs at scale.
