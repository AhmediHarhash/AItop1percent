# 5.8 â€” Multi-GPU Training: Data Parallelism, FSDP, and DeepSpeed

In late 2025, an enterprise AI team at a financial services company provisioned a cluster of eight A100 GPUs to fine-tune a 13B parameter model for regulatory document analysis. The team had trained smaller models successfully on single GPUs and assumed that scaling to eight GPUs would simply require distributing the data across devices and letting PyTorch handle the rest. They enabled DataParallel mode, launched the training job, and watched GPU utilization hover between 35% and 50% while the job took fourteen hours to complete. A consultant reviewed the setup three days later and found that the entire 13B parameter model was being replicated on each of the eight GPUs, leaving only 30GB of the 80GB VRAM per device available for batches and optimizer states. Gradient synchronization was happening after every backward pass using all-reduce operations across eight devices, creating a communication bottleneck that stalled training while gradients were aggregated. The team had paid for eight GPUs but achieved only marginally better throughput than four GPUs would have delivered. The root cause was not a configuration error but a conceptual misunderstanding: multi-GPU training is not a single technique but a spectrum of strategies, each optimized for different memory and communication constraints, and choosing the wrong strategy for your workload wastes both time and money.

This chapter explains how to scale fine-tuning across multiple GPUs efficiently. You will learn when data parallelism suffices, when you need model parallelism through FSDP or DeepSpeed, how to configure each strategy for common model sizes, how to diagnose communication bottlenecks, and how to measure scaling efficiency to determine whether adding more GPUs actually improves throughput. By the end, you will know how to match the right parallelism strategy to your model size, memory budget, and cluster configuration.

## Data Parallelism: The Simplest Strategy for Models That Fit in Memory

Data parallelism is the default multi-GPU training strategy for models that fit entirely in the memory of a single GPU. The approach is conceptually simple: you replicate the full model on every GPU, split each batch across the GPUs so that each device processes a different subset of examples, compute gradients independently on each device, and then synchronize gradients across all devices before applying the optimizer step. PyTorch implements this pattern natively through DistributedDataParallel, and most fine-tuning libraries including Hugging Face Transformers and Axolotl default to data parallelism when you specify multiple GPUs.

Data parallelism works well for LoRA fine-tuning on models up to 13B parameters, where the base model plus adapter weights fit comfortably in 40GB to 60GB of VRAM per device. Training a 7B parameter model with LoRA on four A100 GPUs using data parallelism allows each GPU to hold a full copy of the model and process a batch size of 4 to 8 examples. Gradients are computed independently, synchronized using an all-reduce operation that averages gradients across devices, and the optimizer applies the averaged gradients to update weights. The wall-clock speedup is nearly linear for small clusters: four GPUs deliver 3.5x to 3.8x the throughput of one GPU, and eight GPUs deliver 6.5x to 7.2x the throughput, with the gap between ideal and actual speedup caused by gradient synchronization overhead.

The limitation of data parallelism is memory. If your model does not fit in the VRAM of a single GPU, you cannot replicate it across devices, and data parallelism fails. A 13B parameter model with full fine-tuning requires 120GB to 150GB of working memory, which exceeds the 80GB capacity of an A100. Attempting to use data parallelism in this scenario causes out-of-memory errors during the first forward pass. The solution is not to reduce batch size or enable gradient checkpointing, though both help at the margins. The solution is to shard the model across devices so that each GPU holds only a portion of the parameters, a technique called model parallelism.

Data parallelism also introduces a communication bottleneck when scaling to large clusters. Synchronizing gradients across sixteen or thirty-two GPUs requires all-reduce operations that transmit gigabytes of data across the network after every backward pass. For small models with millions of parameters, the gradient payload is manageable, and synchronization completes in tens of milliseconds. For large models with billions of parameters, the gradient payload reaches multiple gigabytes, and synchronization can take hundreds of milliseconds or even seconds, depending on network bandwidth and topology. When synchronization time exceeds computation time, adding more GPUs reduces throughput instead of increasing it, a failure mode called communication saturation.

The practical rule for data parallelism is this: use it when your model fits in single-GPU memory and when your cluster size is eight GPUs or fewer on a well-connected node with NVLink or InfiniBand. Beyond eight GPUs, or when memory constraints force you to shard the model, you need a more sophisticated parallelism strategy.

## FSDP: PyTorch-Native Model Sharding for Memory-Constrained Workloads

Fully Sharded Data Parallelism, introduced in PyTorch 1.11 and stabilized in PyTorch 2.0, extends data parallelism by sharding model parameters, gradients, and optimizer states across all available GPUs. Instead of replicating the full model on every device, FSDP partitions the model so that each GPU holds only a fraction of the parameters. During the forward pass, each GPU gathers the parameters it needs from other GPUs on demand, computes activations, and then discards the borrowed parameters to free memory. During the backward pass, the same gather-compute-discard pattern applies to gradients. This approach dramatically reduces per-GPU memory requirements, enabling full fine-tuning of models that would otherwise require prohibitively large clusters.

FSDP is the correct choice for full fine-tuning on models between 7B and 70B parameters when using four to sixteen GPUs. A 13B parameter full fine-tune that requires 140GB of working memory with standard data parallelism can be sharded across four A100 80GB GPUs using FSDP, with each GPU holding roughly 35GB of parameters, gradients, and optimizer states. The forward and backward passes involve frequent parameter gathering across devices, which introduces communication overhead, but the overhead is manageable on clusters with NVLink or InfiniBand interconnects. Training time increases by 15% to 30% compared to hypothetical data parallelism if you had infinite memory per device, but the alternative is not slower data parallelism; the alternative is an out-of-memory crash.

Configuring FSDP requires specifying a sharding strategy. The default strategy, FULL_SHARD, shards parameters, gradients, and optimizer states across all devices, maximizing memory savings. The SHARD_GRAD_OP strategy shards only gradients and optimizer states while replicating parameters, reducing communication at the cost of higher memory usage. The NO_SHARD strategy disables sharding entirely and falls back to standard data parallelism, useful for debugging or for models that fit in memory. The HYBRID_SHARD strategy combines sharding within nodes and replication across nodes, optimizing for multi-node clusters where inter-node bandwidth is lower than intra-node bandwidth.

FSDP also requires specifying a wrapping policy that determines how the model is partitioned across devices. The default policy wraps each transformer layer individually, so a 32-layer model is partitioned into 32 shards. This granularity balances memory savings against communication frequency: wrapping smaller submodules reduces memory per device but increases the number of gather operations, while wrapping larger submodules reduces communication but increases memory usage. For most transformer models, layer-level wrapping is optimal. Custom wrapping policies are needed only for non-standard architectures with irregular layer structures.

The failure mode with FSDP is enabling it for models that do not need it. A team fine-tuning a 7B parameter model with LoRA on eight A100 GPUs enables FSDP because it seems like the advanced option. Training time increases by 40% compared to standard data parallelism because FSDP introduces parameter gathering overhead that is unnecessary when the model already fits in memory. The rule is simple: use data parallelism when the model fits in single-GPU memory, use FSDP when memory constraints force sharding, and never enable FSDP for LoRA workloads on models under 13B parameters.

## DeepSpeed ZeRO: Staged Memory Optimization for Frontier-Scale Models

DeepSpeed, developed by Microsoft Research and widely adopted in enterprise and research environments, provides a more granular framework for memory optimization through its ZeRO optimizer. ZeRO stands for Zero Redundancy Optimizer, and it eliminates redundancy in parameter storage, gradient storage, and optimizer state storage across GPUs. DeepSpeed organizes these optimizations into three stages, each progressively reducing memory usage at the cost of increased communication.

ZeRO Stage 1 partitions optimizer states across GPUs but replicates parameters and gradients. In standard data parallelism, each GPU stores a full copy of the optimizer state, which for AdamW includes momentum and variance estimates for every parameter, effectively doubling the memory footprint of the model. ZeRO Stage 1 shards these states so that each GPU stores optimizer information only for the parameters it is responsible for updating. This reduces optimizer memory by a factor equal to the number of GPUs without introducing communication overhead during the forward or backward passes. ZeRO Stage 1 is a low-risk optimization that delivers 30% to 40% memory savings with minimal configuration effort.

ZeRO Stage 2 extends Stage 1 by also partitioning gradients across GPUs. During the backward pass, each GPU computes gradients for the full model but only retains the gradients corresponding to the parameters it will update, discarding the rest. This reduces gradient memory by a factor equal to the number of GPUs and delivers an additional 20% to 30% memory savings on top of Stage 1. The communication overhead is modest because gradient synchronization is required anyway for parameter updates; ZeRO Stage 2 simply defers aggregation and reduces the amount of data each GPU must store.

ZeRO Stage 3 partitions parameters themselves across GPUs, effectively replicating the behavior of FSDP. Each GPU stores only a fraction of the model parameters and gathers the parameters it needs from other GPUs during the forward and backward passes. ZeRO Stage 3 delivers the maximum memory savings, reducing per-GPU memory by a factor equal to the number of GPUs, but introduces significant communication overhead because parameters must be gathered and discarded repeatedly throughout training. For models that fit in memory with ZeRO Stage 2, Stage 3 is unnecessary and counterproductive. For models that exceed memory limits even with Stage 2, Stage 3 is the only option short of reducing model size or increasing cluster size.

DeepSpeed also provides offload capabilities that move optimizer states and gradients to CPU memory or NVMe storage when GPU memory is exhausted. Offloading reduces GPU memory usage but introduces transfer latency between CPU and GPU, slowing training by 2x to 5x depending on the offload strategy and hardware. Offloading is a last-resort optimization for teams that cannot afford larger clusters and are willing to trade training time for lower compute costs. For production fine-tuning, offloading is rarely justified; provisioning additional GPU memory is almost always more cost-effective than tolerating multi-hour slowdowns.

The strategic choice between FSDP and DeepSpeed ZeRO depends on your ecosystem and model scale. FSDP is PyTorch-native, requires minimal configuration, and integrates seamlessly with Hugging Face Transformers. DeepSpeed offers more granular control, supports offloading, and provides additional optimizations like gradient accumulation fusion and mixed-precision training enhancements. For most teams fine-tuning models between 7B and 70B parameters, FSDP is sufficient and easier to configure. For teams fine-tuning models above 70B parameters, training with custom architectures, or optimizing memory usage to the absolute limit, DeepSpeed offers capabilities that FSDP does not.

## Communication Overhead and Network Topology: NVLink, InfiniBand, and Ethernet

The performance of multi-GPU training is determined not only by GPU compute throughput but by the bandwidth and latency of the interconnect between GPUs. All-reduce operations during gradient synchronization and parameter gathering during FSDP or ZeRO Stage 3 transmit gigabytes of data across the network every few seconds, and when network bandwidth becomes the bottleneck, GPU utilization drops and training time increases.

NVLink is NVIDIA's proprietary high-speed interconnect for GPU-to-GPU communication within a single server. NVLink 3.0, used in A100 systems, provides 600 gigabytes per second of bidirectional bandwidth between GPUs. NVLink 4.0, used in H100 and H200 systems, doubles that to 900 gigabytes per second. NVLink operates at microsecond latencies and supports direct memory access between GPUs without involving the CPU. For clusters of four to eight GPUs on a single node, NVLink is the default interconnect, and communication overhead is negligible for most workloads. Training a 13B parameter model with FSDP on eight A100 GPUs connected via NVLink achieves 85% to 90% scaling efficiency, meaning eight GPUs deliver 6.8x to 7.2x the throughput of one GPU.

InfiniBand is a high-speed network fabric used to connect multiple servers in multi-node clusters. Modern InfiniBand systems provide 200 to 400 gigabits per second of bandwidth per link with latencies in the low microseconds. InfiniBand is standard in enterprise AI infrastructure and cloud HPC instances, and it is required for efficient multi-node training. A 32-GPU cluster spread across four eight-GPU nodes connected via InfiniBand can train a 70B parameter model with FSDP or ZeRO Stage 3 at 70% to 80% scaling efficiency. The efficiency is lower than single-node NVLink because inter-node communication is slower than intra-node communication, but it remains high enough to justify the additional hardware.

Ethernet is the fallback network for clusters without NVLink or InfiniBand. Standard 10-gigabit Ethernet provides one hundred times less bandwidth than NVLink, and even 100-gigabit Ethernet is ten times slower. Communication overhead dominates training time on Ethernet-connected clusters, and scaling efficiency drops to 40% to 60% for FSDP workloads and 20% to 40% for ZeRO Stage 3 workloads. Multi-GPU training on Ethernet is viable only for small models with minimal communication requirements or for data parallelism workloads where gradient payloads are small. For FSDP or ZeRO Stage 3, Ethernet is a non-starter; you must provision NVLink or InfiniBand infrastructure or accept that training will take three to five times longer than it should.

The practical rule is to match your parallelism strategy to your network topology. Data parallelism works on any network as long as gradient payloads are manageable. FSDP and ZeRO Stage 2 require NVLink for single-node clusters and InfiniBand for multi-node clusters. ZeRO Stage 3 requires InfiniBand even for single-node clusters when working with models above 70B parameters. If your infrastructure lacks the required network hardware, your options are to reduce cluster size, switch to a less communication-intensive parallelism strategy, or migrate to a cloud provider with proper interconnects.

## Diagnosing Communication Bottlenecks and Measuring Scaling Efficiency

The symptom of communication bottlenecks is low GPU utilization during training. You provision eight A100 GPUs, launch a fine-tuning job with FSDP enabled, and observe that GPUs spend 40% of their time idle while waiting for parameter gathering or gradient synchronization to complete. The job completes in six hours when ideal scaling would predict two hours, and you wasted four hours of eight-GPU time, equivalent to 32 GPU-hours, on communication overhead.

The diagnostic tool is profiling. PyTorch provides a built-in profiler that records GPU utilization, communication time, and computation time for each operation in the training loop. Running the profiler on a multi-GPU training job generates a trace file that visualizes where time is spent. A well-optimized FSDP training job shows 85% to 90% of time in computation and 10% to 15% in communication. A poorly optimized job shows 50% to 60% in computation and 40% to 50% in communication. The profiler pinpoints which operations are slow: all-reduce for data parallelism, all-gather for FSDP, or broadcast for parameter synchronization.

Scaling efficiency is the ratio of actual speedup to ideal speedup. If doubling the number of GPUs from four to eight increases throughput by 1.8x instead of 2x, scaling efficiency is 90%. Efficiency above 85% is excellent, efficiency between 70% and 85% is acceptable, and efficiency below 70% indicates that communication overhead or load imbalance is eating your performance gains. Measuring efficiency requires running the same workload on different cluster sizes and comparing throughput, measured in examples processed per second or tokens processed per second.

The fixes for communication bottlenecks depend on the root cause. If profiling shows that all-reduce operations dominate training time, you are synchronizing gradients too frequently. Increasing gradient accumulation steps reduces synchronization frequency, improving efficiency at the cost of higher memory usage for accumulated gradients. If profiling shows that all-gather operations dominate, you are sharding too aggressively. Switching from ZeRO Stage 3 to ZeRO Stage 2 or from FULL_SHARD to SHARD_GRAD_OP in FSDP reduces communication at the cost of higher memory usage. If profiling shows idle time during data loading, your data pipeline is the bottleneck, not communication. Increasing the number of data loader workers or switching to faster storage resolves the issue.

The strategic insight is that multi-GPU training is not a configuration you enable and forget. It is a system you tune iteratively by profiling, diagnosing bottlenecks, adjusting parallelism strategies, and measuring efficiency. Teams that treat multi-GPU training as a black box consistently achieve 50% to 60% efficiency and waste half their compute budget. Teams that profile systematically and optimize based on measurements achieve 80% to 90% efficiency and extract proportional value from every GPU they provision.

## Configuration Patterns for Common Model Sizes and Cluster Configurations

The correct multi-GPU strategy depends on model size, fine-tuning technique, and cluster configuration. For a 7B parameter model with LoRA on four A100 GPUs, use standard data parallelism with DistributedDataParallel. The model fits comfortably in 30GB per device, leaving ample headroom for batches and gradients. Expect 3.5x to 3.8x speedup over single-GPU training. For the same model with full fine-tuning, the memory requirement is 60GB to 70GB per device, which still fits on A100 80GB hardware. Use data parallelism and enable gradient checkpointing to stay within memory limits.

For a 13B parameter model with LoRA on eight A100 GPUs, use data parallelism. The model requires 45GB to 55GB per device, achievable with batch size tuning. Expect 6.5x to 7.2x speedup. For the same model with full fine-tuning, memory requirements exceed single-GPU capacity. Use FSDP with FULL_SHARD strategy across four A100 GPUs, achieving 35GB per device. Expect 3.2x to 3.5x speedup over single-GPU training if single-GPU training were possible, though the comparison is hypothetical because single-GPU training crashes.

For a 70B parameter model with QLoRA on one A100 80GB GPU, training is memory-constrained but viable. To scale across multiple GPUs for speed, use data parallelism across four A100 GPUs with QLoRA enabled on each device, achieving near-linear speedup because the memory footprint per device remains manageable. For the same model with standard LoRA, memory requirements reach 110GB to 130GB, exceeding single-GPU capacity. Use FSDP with FULL_SHARD across eight A100 GPUs or four H200 GPUs, achieving 15GB to 20GB per device on A100s or 30GB to 35GB per device on H200s.

For a 70B parameter model with full fine-tuning, memory requirements reach 600GB to 800GB, forcing you into a multi-node cluster. Use DeepSpeed ZeRO Stage 3 across 32 A100 GPUs distributed across four eight-GPU nodes with InfiniBand interconnects. Expect 70% to 75% scaling efficiency relative to ideal, achieving 22x to 24x throughput over hypothetical single-GPU training. Enable gradient checkpointing and mixed-precision training to reduce memory usage further. Monitor communication overhead closely and adjust sharding granularity if efficiency drops below 65%.

The general heuristic is to start with the simplest strategy that satisfies your memory constraints. Data parallelism is simplest, FSDP is next, and DeepSpeed ZeRO Stage 3 is the most complex. Use the simplest strategy that keeps memory usage below 75% of available VRAM per device, leaving headroom for variability in batch sizes and sequence lengths. If memory usage exceeds 90%, upgrade to the next sharding level or add more GPUs to the cluster.

Multi-GPU training is not a performance optimization for models that fit in single-GPU memory; it is a memory-scaling technique for models that do not. Data parallelism delivers near-linear speedup for LoRA fine-tuning on models up to 13B parameters as long as the cluster is small and well-connected. FSDP extends multi-GPU training to full fine-tuning workloads by sharding parameters, gradients, and optimizer states across devices, enabling models up to 70B parameters on clusters of four to sixteen GPUs. DeepSpeed ZeRO provides staged memory optimizations for frontier-scale models, with ZeRO Stage 1 and Stage 2 delivering easy wins and ZeRO Stage 3 reserved for models that exceed memory limits even with partial sharding. Communication overhead is the primary bottleneck in multi-GPU training, and scaling efficiency depends on network topology, with NVLink required for single-node clusters and InfiniBand required for multi-node clusters. Profiling, measuring scaling efficiency, and iteratively tuning parallelism strategies separates teams that achieve 85% efficiency from teams that waste half their compute budget on idle GPUs.

The final piece of the infrastructure puzzle is tracking experiments systematically so you can compare training runs, reproduce successful configurations, and diagnose failures after the fact.