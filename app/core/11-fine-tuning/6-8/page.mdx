# 6.8 â€” Expert Validation: Working with SMEs to Verify Domain-Adapted Output

In late 2024, a legal technology company fine-tuned a model to draft preliminary contract review memos for corporate M&A transactions. The team collected 3,000 example contracts and hired contract attorneys to write review memos identifying key risks and provisions. They fine-tuned GPT-4 on these examples, achieving 89 percent BLEU score overlap with the reference memos. The product team considered this successful and prepared to launch. Before launch, they ran a final validation round with three senior M&A partners at a law firm. The partners read 50 model-generated memos. Their feedback was harsh: the model frequently missed material risks, misinterpreted standard clauses, and used terminology incorrectly. One partner called the output "dangerously confident and subtly wrong." The team was confused. The BLEU scores were high. The memos looked good to non-experts. But the domain experts saw errors that automated metrics could not detect. The root cause was that the team had never involved senior domain experts in the training process. They had used junior attorneys for annotation, automated metrics for evaluation, and assumed good scores meant good quality. They learned that in high-stakes domains, expert validation is not optional. No automated metric can replace a subject matter expert's judgment of correctness.

## Why Automated Metrics Fail in Domain Tasks

Automated metrics measure surface similarity between generated outputs and reference outputs. BLEU measures n-gram overlap. ROUGE measures token recall. Semantic similarity metrics measure embedding cosine distance. These metrics work for tasks where surface form strongly correlates with correctness: translation, summarization, and simple Q&A. But they fail for domain tasks where correctness depends on deep domain knowledge.

A legal memo might have high BLEU overlap with a reference memo while being substantively wrong. The model might copy the reference memo's structure and phrasing but swap in incorrect legal standards. It might write "reasonable person standard" where the correct standard is "business judgment rule." The words are similar, so BLEU is high, but the legal conclusion is backwards. A human lawyer sees this immediately. An automated metric does not.

A medical diagnosis might have high semantic similarity to a reference diagnosis while being clinically dangerous. The model might recommend "amoxicillin for suspected bacterial pneumonia" when the reference recommended "azithromycin for suspected atypical pneumonia." The embeddings for these two sentences are close because they both involve antibiotics and pneumonia. But prescribing amoxicillin for atypical pneumonia is ineffective and delays correct treatment. A physician sees this immediately. A similarity metric does not.

A financial analysis might match the reference format perfectly while containing calculation errors or misapplied formulas. The model might write "enterprise value of 8.2 billion dollars" when the correct value is 2.8 billion dollars because it added debt instead of subtracting cash. The sentence structure matches the reference, so automated metrics look fine. But the number is wrong, and financial decisions based on that number will be wrong. An analyst sees this immediately. A metric does not.

Domain correctness is not about surface form. It is about applying domain knowledge correctly: interpreting evidence, following domain rules, respecting domain constraints, and reaching valid conclusions. Only domain experts can judge this. Automated metrics are useful for filtering obviously bad outputs, but they cannot confirm that outputs are correct. Expert validation is the only way to measure true domain quality.

## The Role of Subject Matter Experts in Fine-Tuning

Subject matter experts serve three roles in domain adaptation: they generate training data, they validate model outputs, and they diagnose failure modes. All three roles are critical. Skipping any of them produces models that look good on paper but fail in practice.

SMEs generate training data by creating high-quality examples of correct domain outputs. In legal fine-tuning, SMEs are senior attorneys who write contract review memos, legal research summaries, or case analysis. In medical fine-tuning, SMEs are physicians who write diagnostic assessments, treatment recommendations, or clinical notes. In financial fine-tuning, SMEs are analysts who write valuation reports, risk assessments, or investment memos. The quality of fine-tuning data is capped by the expertise of the people who create it. Junior experts produce junior-quality data. Senior experts produce senior-quality data. You cannot fine-tune a model to expert-level performance using non-expert training data.

SMEs validate model outputs by reviewing samples and judging correctness. After fine-tuning, you generate outputs on held-out test cases and ask SMEs to rate them. SMEs identify errors that automated metrics miss: incorrect application of domain rules, misinterpretation of evidence, unsupported conclusions, improper terminology, and omitted caveats. This validation tells you whether the model is actually ready for deployment or whether it needs more training.

SMEs diagnose failure modes by analyzing patterns in model errors. When the model makes mistakes, SMEs can often identify why. They notice that the model struggles with certain clause types, or that it confuses similar-sounding medical conditions, or that it misapplies formulas in specific scenarios. This diagnostic feedback informs the next training iteration: you collect more examples of the failure cases, refine your prompts, or adjust your evaluation criteria. Without SME diagnosis, you know the model is wrong, but you do not know how to fix it.

## Structuring SME Review for Efficiency

SME time is expensive and limited. Senior domain experts bill at 300 to 1,000 dollars per hour. You cannot ask them to review thousands of model outputs. You need structured review processes that extract maximum signal from minimal SME time.

The most efficient structure is staged review with filtering. First, use automated metrics to filter out obviously bad outputs. If an output has very low similarity to any reference example, flag it for SME review but do not waste SME time on outputs that are clearly broken. Second, sample outputs across the quality distribution. Include some high-scoring outputs, some medium-scoring outputs, and some low-scoring outputs. This gives SMEs a full picture of model behavior. Third, batch outputs into review sessions. Send SMEs 20 to 50 outputs at a time, not thousands. Ask them to review one batch per week. This fits into their schedule without overwhelming them.

Provide SMEs with clear review criteria. Do not just ask "is this correct." That is too vague. Instead, ask them to rate specific dimensions: factual accuracy, reasoning validity, completeness, terminology correctness, and appropriate caveats. Use a simple rating scale: correct, minor issues, major issues, or incorrect. Include a free-text field for explaining issues. This structured feedback is easier to aggregate and analyze than unstructured comments.

Provide SMEs with context when reviewing. Show them the input that prompted the model output. Show them the reference output if one exists. Show them any retrieved context the model used. SMEs cannot judge correctness in a vacuum. They need to see what the model was working with. A diagnosis that looks wrong in isolation might be reasonable given the input symptoms. A legal conclusion that seems incorrect might be correct under the specific jurisdiction provided in the input.

Calibrate SMEs before review begins. Have all SMEs review the same small set of outputs independently, then discuss their ratings. This reveals disagreements in interpretation and allows the team to clarify review standards. Some SMEs are harsher than others. Some focus on major errors, others on minor imprecision. Calibration aligns expectations and reduces noise in ratings.

Track SME agreement rates. If two SMEs review the same output and disagree on correctness, that is signal. High disagreement means either the SME instructions are unclear or the domain question is genuinely ambiguous. Investigate disagreements. Sometimes they reveal edge cases where domain best practices are unsettled. Sometimes they reveal that one SME misunderstood the task. Either way, disagreement is valuable feedback.

## Annotation Guidelines for Domain Experts

SMEs are experts in their domain, not in machine learning or annotation. You need to provide them with clear guidelines for creating training data and reviewing outputs. These guidelines translate domain expertise into ML-compatible labels.

For training data creation, specify the output format precisely. Show SMEs examples of well-formatted outputs. Explain what sections to include, what tone to use, what level of detail is appropriate, and what caveats to add. Domain experts naturally write in the style they use professionally, but professional style might not match the style you want the model to learn. If you want concise outputs, tell SMEs to write concisely. If you want outputs with confidence scores, tell SMEs to include confidence scores. Do not assume they will infer the desired format from a vague prompt.

For output review, define what "correct" means in your context. Correct can mean factually accurate, legally sound, clinically appropriate, financially valid, or compliant with regulations. It can also mean "acceptable for the intended use case even if not perfect." Clarify this. A contract review memo that would be unacceptable as final work product might be perfectly acceptable as a first draft for an attorney to refine. An investment analysis that would be unacceptable for publication might be acceptable as internal research. Define the quality bar clearly.

Provide examples of common error types. Show SMEs examples of outputs with factual errors, reasoning errors, formatting errors, and tone errors. Label each error type. This trains SMEs to recognize and label errors consistently. It also helps them understand what the model struggles with, so they can focus attention on those areas.

Explain how their feedback will be used. SMEs are more engaged when they understand the purpose. Tell them that their correctness ratings will be used to filter training data, that their error annotations will inform the next fine-tuning iteration, and that their diagnostic feedback will guide prompt engineering. This motivates careful review. SMEs who think their feedback disappears into a black box provide lower-quality feedback than SMEs who see their feedback drive improvements.

Allow SMEs to say "I don't know" or "ambiguous." Not every domain question has a clear answer. Some cases are edge cases where best practices are unclear. Some inputs are too vague to produce a definitive output. Some domains have legitimate disagreement among experts. Let SMEs flag these cases instead of forcing a binary correct or incorrect judgment. Ambiguous cases are valuable signal: they tell you where the model should express uncertainty instead of being confident.

## Managing SME Time and Cost

SME involvement is the most expensive part of domain fine-tuning. A single senior physician might cost 500 dollars per hour. Reviewing 1,000 outputs at two minutes per output costs 16,000 dollars. Creating 1,000 training examples at 15 minutes per example costs 125,000 dollars. These costs add up quickly, and they recur with every training iteration. You need strategies to manage SME cost without sacrificing quality.

The first strategy is to start small. Do not ask SMEs to create 10,000 training examples in the first iteration. Start with 100 examples. Fine-tune on those 100. Evaluate the model. If it learns the task, scale up. If it does not, diagnose why before investing in more data. Starting small reduces wasted SME time on data that turns out to be insufficient or misformatted.

The second strategy is to use SMEs for high-value tasks only. SMEs should create seed examples and validate final outputs. They should not do tasks that non-experts can do. Non-experts can collect raw inputs, format data, run evaluations, and generate model outputs. SMEs review the outputs for correctness. This division of labor minimizes expensive SME hours.

The third strategy is to use active learning to prioritize SME review. After fine-tuning, generate outputs on a large test set. Use model uncertainty or disagreement with automated metrics to rank outputs by review priority. Send SMEs the highest-priority outputs first. This ensures that limited SME time focuses on the cases where model behavior is most uncertain or most likely to be wrong.

The fourth strategy is to train junior SMEs and scale through them. Senior SMEs are expensive and scarce, but junior SMEs are cheaper and more available. Have senior SMEs create initial guidelines and review a calibration set. Then have junior SMEs do the bulk of annotation and review under senior supervision. Senior SMEs spot-check junior work to ensure quality. This leverages senior expertise without requiring senior time for every annotation.

The fifth strategy is to automate parts of the review workflow. Build tools that pre-flag potential errors: outputs with low confidence, outputs that contradict retrieved context, outputs with formatting issues, or outputs with rare or unknown terms. These flags help SMEs find errors faster. A well-designed review interface can double SME throughput, cutting costs in half.

The sixth strategy is to negotiate fractional or retainer arrangements with SMEs. Instead of hiring SMEs hourly, hire them for a fixed number of hours per month at a discounted rate. This gives you predictable costs and gives SMEs predictable income. It also builds long-term relationships: SMEs who work with you over multiple iterations understand your model's failure modes better and provide better feedback.

## Feedback Loops Between SME Review and Model Improvement

SME review is not a one-time validation step. It is a continuous feedback loop. SMEs review outputs, identify errors, you analyze error patterns, you collect more training data or refine prompts, you retrain, SMEs review new outputs, and the cycle repeats. The quality of domain-adapted models improves through iteration, not through a single perfect training run.

After each SME review round, aggregate the feedback. Count error types: how many factual errors, how many reasoning errors, how many formatting errors, how many terminology errors. Look for patterns. If 40 percent of errors involve misinterpreting a specific clause type, you need more training examples of that clause type. If 30 percent of errors involve incorrect confidence, you need to adjust your confidence calibration. If 20 percent of errors involve missing caveats, you need to add caveats to your system prompt or fine-tuning examples.

Use SME feedback to prioritize data collection. If SMEs identify a failure mode, collect examples that address it. If the model struggles with force majeure clauses, collect 50 more examples of force majeure analysis. If it struggles with differential diagnosis of chest pain, collect 50 more examples of chest pain cases. Targeted data collection addresses known weaknesses faster than random data collection.

Use SME feedback to refine evaluation criteria. If SMEs consistently flag errors that your automated metrics miss, add new metrics or adjust thresholds. If SMEs say that tone is critical but your metrics do not measure tone, add a tone metric. If SMEs say that caveats are essential but your metrics do not check for caveats, add a caveat detection metric. Align your automated metrics with SME priorities so that you can scale evaluation beyond what SMEs can review manually.

Use SME feedback to improve annotation guidelines. If different SMEs interpret the same output differently, clarify the guidelines. If SMEs make annotation mistakes, provide additional training. If SMEs request features in the review interface, build those features. The annotation process should improve over time just as the model improves.

Track model improvement across SME review rounds. After each retraining, have SMEs review a fixed held-out test set. Compare error rates across iterations. You should see steady improvement: fewer errors per output, fewer severe errors, and better handling of previously identified failure modes. If error rates plateau or increase, something is wrong with your training process. Investigate before continuing.

## When SME Validation Reveals Fundamental Issues

Sometimes SME review reveals that the model is not ready for the task at all. The error rate is too high, the errors are too severe, or the failure modes are too unpredictable. This is painful feedback, but it is valuable. It is better to learn this during development than after deployment.

When SMEs report high error rates, first check whether the task is well-defined. Vague task definitions produce inconsistent outputs that SMEs will rate as incorrect. If SMEs cannot agree on what correct outputs look like, the model has no chance. Refine the task definition, clarify the output format, and narrow the scope. A well-defined narrow task is trainable. A poorly defined broad task is not.

When SMEs report severe errors, check whether the base model has the necessary capabilities. Some domain tasks require reasoning abilities that current models lack. If the task requires multi-step logical inference, spatial reasoning, or deep domain knowledge that is not in the training data, fine-tuning will not create those capabilities. You might need a stronger base model, a different architecture, or a hybrid approach combining the model with retrieval or symbolic reasoning.

When SMEs report unpredictable failure modes, check whether the training data covers the input distribution. Models generalize well within the training distribution but poorly outside it. If your training data includes only simple cases and SMEs are testing on complex cases, the model will fail. Either expand the training data to cover complex cases or restrict the deployment scope to simple cases.

Sometimes the right decision is to pause fine-tuning and invest in better training data. If SMEs identify that your existing examples are low-quality, collecting more low-quality examples will not help. Stop, improve your annotation process, have senior SMEs create a smaller set of high-quality examples, and retrain. Quality over quantity.

Sometimes the right decision is to switch from fine-tuning to prompting. If the task is rare or highly variable, fine-tuning might not be cost-effective. Instead, use few-shot prompting with SME-written examples, retrieval-augmented generation with SME-curated knowledge bases, or structured prompts with SME-defined rules. These approaches give SMEs more control over model behavior without requiring large-scale annotation.

## Expert Validation as Quality Assurance

In high-stakes domains, SME validation is your quality assurance process. It is the only way to know whether the model is producing outputs that meet domain standards. Automated metrics tell you whether the model is improving relative to itself. SME validation tells you whether the model is good enough for real-world use.

Treat SME validation with the same rigor you treat software QA. Define clear test plans, track defect rates, maintain test sets, and require sign-off before deployment. Do not ship a domain-adapted model without SME approval. The consequences of shipping incorrect medical, legal, or financial outputs are severe: patient harm, legal liability, financial loss, and regulatory penalties. SME validation is your last line of defense against these outcomes.

Build long-term relationships with your SMEs. Domain adaptation is not a one-time project. Domains evolve, models drift, and new failure modes emerge. You will need SME involvement for initial training, ongoing validation, incident response, and periodic retraining. SMEs who understand your system and trust your team provide better feedback and faster turnaround. Invest in those relationships.

The next step is building evaluation suites that let you measure domain quality at scale, combining automated metrics with SME-validated benchmarks.

