# 4.13 — Fine-Tuning for Tool Discipline and Structured Outputs

Can a prompted language model reliably call twelve different tools with correct arguments, proper timing, and zero hallucinated tool names in a production customer support environment? In early 2025, a SaaS company believed the answer was yes. They deployed a GPT-5 agent with access to twelve tools: CRM lookup, billing API, product catalog search, ticket creation endpoint, and eight others. The agent was prompted to use tools when needed. In testing it worked beautifully. In production it failed catastrophically. The agent called tools at the wrong times, passed malformed arguments, hallucinated tool names that did not exist, and sometimes refused to call tools when it should have, instead generating plausible but wrong answers. After two weeks of escalating complaints, they pulled the agent offline. By mid-2025, they had fine-tuned a smaller model specifically on tool-calling examples, training it to recognize when to call tools, which tool to call, and how to format arguments. The fine-tuned model achieved 99.2 percent tool-calling accuracy compared to 73 percent for the prompted GPT-5 agent. The answer to the original question turned out to be no.

Tool-calling and structured output generation are two of the most valuable capabilities you can fine-tune into a model. Tool-calling enables agents to interact with external systems, APIs, databases, and services, extending the model beyond text generation into the real world. Structured output generation enables models to produce JSON, XML, database records, API requests, and other machine-readable formats that downstream systems can parse and act on. Both capabilities exist in frontier models through prompting and function-calling APIs, but prompting is fragile, inconsistent, and sensitive to phrasing, context length, and model updates. Fine-tuning instills these capabilities directly into model weights, making them robust, consistent, and fast.

This subchapter teaches you how to fine-tune models for tool discipline and structured outputs. You will learn why prompted tool use fails in production, how to construct training data for tool-calling, how to train models to emit valid structured outputs, what schema compliance means in fine-tuning, and how to measure tool-calling reliability post-training. This is not theoretical capability research; this is production engineering for agent systems that must work reliably at scale.

## Why Prompted Tool Use Is Unreliable

Frontier models like GPT-5, Claude Opus 4.5, and Gemini 2 support tool-calling through function-calling APIs. You define function signatures in the prompt, the model generates function calls as special tokens, and your application parses those tokens and executes the functions. This works well in demos and in narrow, controlled environments. It works poorly in production, especially when the number of tools is large, when tool signatures are complex, or when the model must decide between calling a tool and generating a direct answer.

The reliability problems fall into four categories. First, the model calls the wrong tool. When you provide ten tool signatures, the model sometimes selects the wrong one because tool names or descriptions are ambiguous, or because the input query could plausibly match multiple tools. Second, the model generates malformed arguments. Even when the correct tool is selected, the model sometimes omits required arguments, passes arguments in the wrong format, or hallucinates argument names that do not exist in the signature. Third, the model hallucinates tools that were never defined, generating function calls to nonexistent APIs because the tool name sounds plausible. Fourth, the model refuses to call a tool when it should, instead generating a text answer that may be wrong or incomplete.

These failures are not random. They correlate with context length, tool complexity, and phrasing. When the prompt is long and tool signatures are buried in the middle, the model is more likely to forget or misinterpret them. When tool descriptions are vague or overlapping, the model is more likely to choose the wrong one. When argument schemas are nested or contain optional fields, the model is more likely to emit malformed JSON. Prompting-based tool use is fundamentally a context-dependent, inference-time behavior, and it is subject to all the fragility of prompting: sensitivity to phrasing, degradation with context length, and inconsistency across model versions.

The customer support agent that opened this subchapter exhibited all four failure modes. It called the billing API when it should have called the CRM. It passed customer email addresses in the wrong argument field. It hallucinated a tool called search-product-inventory that did not exist. It answered billing questions directly instead of calling the billing API, and the answers were sometimes wrong because the model did not have access to real-time data. The team tried prompt engineering, few-shot examples, chain-of-thought prompting, and all the standard techniques. Reliability improved from 68% to 73%, but never exceeded 75%, which was unacceptable for a customer-facing agent.

Fine-tuning solves these problems by training the model explicitly on tool-calling examples, instilling tool discipline as learned behavior rather than prompted behavior. The fine-tuned model learns when to call tools, which tool to call, how to format arguments, and when to refuse tool calls that do not make sense. Tool-calling becomes a first-class model capability, not a fragile emergent behavior from prompting.

## Constructing Tool-Calling Training Data

Training a model to use tools reliably requires a large dataset of examples showing correct tool usage in varied contexts. The dataset must cover all tools in your system, all common input patterns, all argument variations, and all edge cases where the model might be tempted to hallucinate or refuse. Constructing this dataset is not trivial; it requires deliberate design, domain expertise, and often synthetic data generation.

The structure of a tool-calling training example is a three-part sequence: the user input, the tool call decision, and the tool execution result. The user input is the natural language query or instruction. The tool call decision is the model's output, formatted as a function call with the tool name and arguments. The tool execution result is the output from the tool, which the model then uses to generate the final response. During fine-tuning, you train the model on the full sequence, teaching it not just to call tools but to interpret tool results and incorporate them into responses.

A minimal example for a CRM lookup tool might look like this in prose: the user asks "What is the email for customer ID 47823?" The model generates a tool call to lookup-customer with argument customer-id set to 47823. The tool returns a record containing email address support at example dot com. The model then generates the final response: "The email for customer ID 47823 is support at example dot com." You create thousands of examples like this, covering every tool, every common query pattern, and every argument variation.

You collect training data from three sources. First, production logs, if you already have a prompted tool-using system in production. You extract examples where the prompted model called tools correctly, verify correctness manually or programmatically, and use those as training examples. Second, synthetic generation, where you write templates for tool usage patterns and generate variations programmatically. For example, you generate 500 variations of CRM lookups by customer ID, email, phone number, and account name. Third, expert annotation, where domain experts write examples of complex or rare tool usage that would not appear in logs or templates.

The critical design decision is coverage. Your training set must cover every tool, every argument type, every common input phrasing, and every edge case. If you train the model on CRM lookups by customer ID but never by email address, the model will not generalize to email-based lookups. If you never show examples of refusing tool calls when input is ambiguous, the model will call tools even when it should not. Coverage is more important than volume; 5,000 examples covering all tools and edge cases beats 20,000 examples concentrated on two popular tools.

You also include negative examples: inputs where the model should not call a tool. These are cases where the user asks a general question that does not require external data, or where the input is too ambiguous to determine the correct tool, or where the user explicitly asks for a hypothetical answer rather than real data. Training the model on negative examples teaches it restraint, preventing over-calling of tools in production.

## Training for Argument Formatting and Schema Compliance

One of the most common fine-tuning failures in tool-calling is training the model to call the right tool but generating malformed arguments that fail schema validation. The model outputs a function call, your application attempts to parse the arguments, and parsing fails because a required field is missing, a field is the wrong type, or the JSON structure is invalid. Fine-tuning for schema compliance is as important as fine-tuning for tool selection.

Schema compliance training requires examples that demonstrate correct argument structure in all its variations. If a tool accepts an optional field, you include examples both with and without that field. If a tool accepts a list of items, you include examples with zero, one, and many items. If a tool accepts nested objects, you include examples showing nested structure with all required and optional fields populated correctly. The training data becomes a specification of valid argument patterns, and the model learns to emit only valid patterns.

You enforce schema compliance during training by validating every training example against the tool's schema before including it in the dataset. If an example fails validation, you fix it or discard it. This ensures the model never sees invalid argument patterns during training, which is critical because models learn from every example, including mistakes. If you train on invalid examples, the model learns to generate invalid outputs.

A financial services company in late 2025 fine-tuned a model to call a transaction search API with six arguments, three required and three optional. Early in training, the model frequently omitted one of the required arguments, transaction-type, because in some examples the field was implied by context. The team added 1,200 examples explicitly showing transaction-type in varied contexts, and the omission rate dropped from 18% to 0.3%. They also added examples where optional arguments were omitted, teaching the model that omission is legal for optional fields but illegal for required fields. The resulting model had 99.7% schema compliance on held-out test cases.

Schema compliance also requires teaching the model correct type discipline. If an argument is defined as an integer, the model must not emit a string. If an argument is defined as a boolean, the model must emit true or false, not yes or no. If an argument is a timestamp, the model must emit a valid ISO 8601 string, not a natural language date. You train type discipline by including examples that exercise every argument type in your schema and by never including examples that violate type constraints.

## Fine-Tuning for Structured Output Reliability

Structured output generation is the sibling capability to tool-calling. Instead of calling an external tool, the model emits a structured data object—JSON, XML, a database record, a form submission—that a downstream system can parse and act on. Structured outputs are used for data extraction, form filling, workflow automation, and any task where the model's output must be machine-readable, not just human-readable.

Prompted models can generate structured outputs, but reliability is poor. The model sometimes omits required fields, emits malformed syntax, hallucinates field names, or wraps the structured output in natural language commentary that breaks parsing. Fine-tuning fixes these issues by training the model to emit valid structured outputs consistently, with no extraneous text, no syntax errors, and no schema violations.

The training data structure for structured outputs is similar to tool-calling: input, output, and optional post-output context. The input is the natural language instruction or source document. The output is the structured object, formatted exactly as it must appear in production. The post-output context, if included, might be a system response confirming successful parsing. You train the model to generate the structured object and nothing else, no commentary, no markdown fences, no explanatory text.

A common mistake is training the model on examples where the structured output is wrapped in markdown code fences, because the base model was trained to generate code blocks in that format. This works during training but fails in production when your parser expects raw JSON and receives markdown-wrapped JSON. The solution is to train on examples with no markdown fences, teaching the model to emit raw structured output. If your production system expects markdown fences, you include them in training examples, but you make that decision deliberately, not by accident.

You also train the model to handle optional fields correctly. If a field is optional and no value is available, the model should omit the field entirely or emit null, depending on your schema convention. If a field is required and no value is available, the model should emit a sentinel value like "unknown" or raise an error, depending on your error-handling design. You teach these behaviors by including examples that exercise all optionality patterns in your schema.

A healthcare data company in mid-2025 fine-tuned a model to extract patient information from clinical notes and emit structured JSON records. The schema had twelve required fields and eight optional fields. Early in training, the model frequently emitted optional fields with empty strings when no value was available, which broke downstream validation. The team added examples showing omission of optional fields when no data was present, and the error rate dropped from 22% to 1.1%. They also added examples showing the required field date-of-birth populated with a sentinel value "unknown" when the note did not contain a birthdate, preventing downstream crashes.

## Measuring Tool-Calling and Structured Output Reliability

Fine-tuning for tool discipline and structured outputs is not complete until you measure reliability on held-out test sets that resemble production traffic. The metrics you care about are different from standard fine-tuning metrics like loss or perplexity. You care about tool-call accuracy, argument validity, schema compliance, and parse success rate.

Tool-call accuracy is the percentage of examples where the model calls the correct tool. You measure this on a test set where the correct tool is known, either from human annotation or from production logs. A production-grade tool-calling model should achieve 95% to 99% tool-call accuracy, depending on the number of tools and the ambiguity of input queries. Below 95%, the model is not reliable enough to deploy without fallback logic.

Argument validity is the percentage of tool calls where the arguments pass schema validation. You measure this by programmatically validating every generated argument set against the tool's schema. A production-grade model should achieve 98% to 99.5% argument validity. Below 98%, you will see frequent runtime errors from malformed arguments, and you need more training data covering argument edge cases.

Schema compliance for structured outputs is the percentage of generated outputs that parse successfully and validate against the schema. You measure this by parsing every output and running schema validation. A production-grade model should achieve 97% to 99% schema compliance. Below 97%, downstream systems will experience frequent failures, and you need more training examples covering schema edge cases.

Parse success rate is the percentage of outputs that your production parser can successfully parse, which is stricter than schema compliance because production parsers often have quirks that standard validators do not catch. You measure this by running generated outputs through your actual production parser. A production-grade model should achieve 96% to 99% parse success. Below 96%, you will see production failures, and you need to align training data format exactly with production parser expectations.

You also measure refusal accuracy: the percentage of cases where the model correctly refuses to call a tool or emit a structured output because the input is ambiguous, incomplete, or inappropriate. You create a test set of refusal cases, run inference, and verify that the model generates an appropriate refusal message instead of hallucinating a tool call or structured output. A production-grade model should achieve 90% to 95% refusal accuracy. Below 90%, the model is too eager to call tools or emit outputs, which leads to low-confidence hallucinations.

## Tool-Calling Fine-Tuning in Multi-Tool Environments

Fine-tuning for tool discipline becomes significantly more complex when the number of tools is large. A system with two or three tools can achieve high reliability with a few hundred training examples per tool. A system with twenty or thirty tools requires thousands of examples per tool, careful attention to tool name collisions, and training strategies that prevent the model from favoring frequently-used tools over rarely-used tools.

In multi-tool environments, the most common failure mode is tool selection bias. The model learns to call frequently-used tools even when a rarely-used tool is correct, because the training data contains many examples of the frequent tool and few examples of the rare tool. The solution is balanced sampling: you oversample rare tools during training to ensure the model sees each tool with similar frequency, regardless of natural usage distribution.

Another failure mode in multi-tool environments is tool name confusion. If two tools have similar names or similar descriptions, the model sometimes selects the wrong one because the semantic difference is subtle. The solution is to include contrastive examples in training: pairs of inputs where one should call tool A and the other should call tool B, even though the inputs are similar. These contrastive examples teach the model the precise boundary between tools.

A logistics company in late 2025 fine-tuned a model to use 18 tools for shipment tracking, inventory management, carrier selection, and pricing. Early in training, the model frequently called the shipment-tracking tool when it should have called the shipment-status tool, because the tool names were similar and the use cases overlapped. The team added 600 contrastive examples explicitly distinguishing tracking (which returns location history) from status (which returns current state), and the confusion rate dropped from 14% to 2%. They also balanced training data so that each tool appeared in at least 800 examples, preventing the model from over-relying on the three most common tools.

In multi-tool environments, you also train the model to use tools in combination. Real-world tasks often require calling multiple tools in sequence: first calling a CRM tool to retrieve customer data, then calling a billing tool to retrieve account balance, then generating a response that synthesizes both results. You include multi-tool examples in training data, showing the full sequence of tool calls and the final synthesized response. This teaches the model to chain tools correctly and to incorporate results from multiple tools into coherent outputs.

## Fine-Tuning for Tool Error Handling and Fallback

Production tool-calling systems must handle tool errors gracefully. Tools fail: APIs return errors, databases are unreachable, rate limits are hit, schemas change. A robust fine-tuned model does not hallucinate when a tool fails; it acknowledges the failure, explains the issue to the user, and either retries, falls back to another tool, or exits gracefully.

You train error handling by including examples where tools return errors or null results, and the model generates an appropriate fallback response. For example, if a CRM lookup tool returns "customer not found," the model should generate "I could not find a customer with that ID" rather than hallucinating customer data. If a billing API returns a rate limit error, the model should generate "The billing system is temporarily unavailable, please try again in a few minutes" rather than pretending the call succeeded.

Error-handling training data is often neglected because production logs are biased toward success cases. You must synthetically generate error cases, or deliberately inject errors into production logs during data collection. A common pattern is to take successful tool-call examples, replace the tool result with an error message, and write the appropriate fallback response. You generate hundreds of error examples for each tool, covering all error types: not found, unauthorized, rate limited, timeout, malformed response, schema mismatch.

A travel booking platform in mid-2025 fine-tuned a model to call flight search APIs, hotel APIs, and car rental APIs. They included 1,200 error-handling examples in training data, covering API errors, timeouts, and empty result sets. The fine-tuned model learned to generate messages like "No flights are available for that route on that date" when the API returned an empty result, rather than hallucinating flight options. It also learned to suggest alternative dates or nearby airports when the primary search failed, a behavior they trained by including examples showing fallback search strategies.

## Iterative Improvement of Tool-Calling Models

Fine-tuning for tool discipline is not a one-time process. You deploy the model, monitor tool-calling behavior in production, identify failure cases, add those cases to training data, and retrain. This iterative loop is how tool-calling reliability improves from 95% to 99% over months of operation.

You monitor tool-calling reliability using the same metrics you measured during eval: tool-call accuracy, argument validity, schema compliance, parse success rate, and refusal accuracy. You log every tool call, every argument set, and every parsing outcome. You identify failure modes by clustering errors: which tools are called incorrectly most often, which arguments fail validation most often, which inputs trigger refusals when they should not.

You then generate training examples targeting the top failure modes. If the model frequently calls tool A when it should call tool B, you add contrastive examples distinguishing A from B. If the model frequently omits a specific argument, you add examples emphasizing that argument. If the model refuses to call tools in a specific input pattern, you add positive examples for that pattern. You retrain the model on the augmented dataset and redeploy. This cycle repeats every few weeks in high-activity systems.

A customer data platform in late 2025 ran this iterative loop for six months, retraining their tool-calling model every three weeks. At launch, tool-call accuracy was 94.2%. After six iterations, accuracy was 98.7%. Each iteration added 200 to 400 new training examples targeting the top five failure modes from production. The team tracked failure mode prevalence over time and watched each failure mode decrease as targeted examples were added, validating that the iterative process was working.

## When to Fine-Tune for Tool Discipline Versus Prompting

Not every tool-using system needs fine-tuning. If you have two tools, low usage volume, and tolerance for occasional errors, prompting is sufficient. Fine-tuning is justified when the number of tools is large, when usage volume is high, when reliability requirements are strict, or when inference cost is a concern and you want to use a smaller fine-tuned model instead of a large prompted model.

The decision threshold is approximately five tools. Below five tools, prompted GPT-5 or Claude Opus 4.5 achieves acceptable reliability with good prompt design. Above five tools, reliability degrades, and fine-tuning becomes cost-effective. At ten tools, fine-tuning is almost always necessary. At twenty tools, fine-tuning is mandatory for production deployment.

You also choose fine-tuning when you need full control over tool-calling behavior, which is critical in regulated domains. A prompted model's tool-calling behavior can change when the provider updates the base model, which is unacceptable in healthcare, finance, and legal applications. A fine-tuned model's behavior is stable across your deployment lifetime, because you control the weights and the inference environment.

Finally, you choose fine-tuning when inference cost matters. Prompted tool-calling with GPT-5 costs approximately $15 per million tokens. Fine-tuned tool-calling with a 7B model costs approximately $0.50 per million tokens, a 30x reduction. At high query volumes, fine-tuning pays for itself in weeks.

The future of AI systems is agentic: models that use tools, interact with APIs, and orchestrate multi-step workflows. Fine-tuning is what makes these systems reliable enough to deploy at scale, and tool discipline is the foundation of reliable agents.
