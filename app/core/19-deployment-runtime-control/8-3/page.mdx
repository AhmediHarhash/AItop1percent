# 8.3 â€” Artifact Registries: MLflow, Hugging Face Hub, Custom Solutions

Where do your models live? If the answer is "in someone's Google Drive" or "somewhere on S3" or "on the training server," you have a problem. Models that are not in a central, versioned, searchable registry are effectively unmanaged. You cannot answer basic questions like what models exist, which one is running in production, who created a specific model, or what data it was trained on. This is not a sustainable state for any team deploying models to production.

The artifact registry pattern solves this. A registry is a central catalog of versioned artifacts with associated metadata. It provides a single source of truth for all models. Every model that reaches staging or production must be registered. Models that are not registered do not get deployed. This discipline prevents the chaos of ad-hoc model management. The registry becomes the boundary between experimentation and production. Experiments can happen anywhere. Production deployments happen only from the registry.

The registry answers questions that are impossible to answer without it. What is the lineage of the model running in production? Query the registry by deployment identifier, get the model version, trace back to the training run. What models were trained on a specific dataset? Query by dataset version, get the list of models. What models use a deprecated base model? Query by base model identifier, get the list of models that need to be retrained. The registry is not just storage. It is an index that makes model governance possible.

## MLflow Model Registry

MLflow Model Registry is the most widely adopted open-source solution. It integrates with MLflow tracking, which logs experiments, parameters, and metrics during training. When a training run completes, the model can be registered in the MLflow Model Registry. The registry assigns it a version number, stores metadata like the source run ID and creation timestamp, and provides lifecycle management with stages like staging, production, and archived.

The strength of MLflow is integration. If you are already using MLflow for experiment tracking, adding the model registry is straightforward. The training script logs metrics and parameters to MLflow during training. At the end of training, it registers the model. The registry automatically links the model back to the training run, which links back to the code version, dataset version, and hyperparameters. You get lineage tracking with minimal additional code.

MLflow supports multiple storage backends. Models can be stored in local filesystems, S3, Azure Blob Storage, GCS, or HDFS. The registry itself is backed by a database, typically MySQL or PostgreSQL, which stores metadata. This separation of metadata and artifacts is important. The metadata database is small and queryable. The artifact storage is large and optimized for binary blobs. Queries run against the database. Downloads happen from the artifact store. This architecture scales better than trying to store everything in one system.

The limitations of MLflow become visible at scale. The model registry is designed for a single organization with a single MLflow deployment. If you have multiple teams with separate MLflow instances, you have multiple registries. Consolidating them is not straightforward. The permissions model is basic. You can restrict access at the registry level, but fine-grained controls like read-only access to certain models or role-based access by model type require external tooling. The UI is functional but not designed for large-scale browsing. If you have 500 models registered, finding the one you need requires knowing its name or using the API.

MLflow also does not natively handle multi-region deployments. If you deploy models in multiple regions, each region needs its own artifact storage. Keeping registries synchronized across regions requires custom logic. Some teams run a central registry and replicate artifacts to regional storage. Others run regional registries and implement cross-region search. Neither approach is seamless. MLflow was designed for centralized ML workflows, and distributed deployments require workarounds.

## Hugging Face Hub

Hugging Face Hub is the dominant platform for sharing and versioning transformer models. It was originally designed for open-source model distribution, but it now supports private repositories, enterprise deployments, and production workflows. The Hub provides git-based versioning for models, datasets, and even entire ML applications. Every model is a repository. Changes are tracked as commits. You can branch, tag, and roll back just like code.

The Hub's strength is standardization. If your models are transformers, the Hugging Face format is the de facto standard. You can push a model to the Hub from your training script with a few lines of code. You can pull it in production with a few lines of code. The format is consistent across frameworks. A model saved with PyTorch can be loaded in TensorFlow or JAX if it follows the Hugging Face conventions. This interoperability is valuable when teams use different tools for training and serving.

The Hub also emphasizes model cards. A model card is a standardized metadata document that describes what the model is, how it was trained, what it is intended for, what its limitations are, and what ethical considerations apply. Model cards are written in markdown and stored in the repository alongside the model. This makes documentation first-class. You cannot push a model to the Hub without creating a model card, which forces teams to document their models. In practice, this discipline improves model governance significantly.

The Hub's limitations are primarily around enterprise features. The permissions model is improving but still simpler than what large organizations need. You can make repositories private, grant access to specific users or organizations, and set up single sign-on, but you cannot easily implement custom approval workflows or audit trails. The Hub is optimized for open collaboration, not for compliance-heavy environments. Some enterprises run private Hub instances, but this requires infrastructure and maintenance that not all teams are equipped to handle.

The Hub is also optimized for models that fit the Hugging Face ecosystem. If your models are not transformers, the Hub is less useful. You can store arbitrary files, but you lose the tight integration with transformers libraries, the automatic model card generation, and the inference API. For computer vision models, speech models, or custom architectures, the Hub works but feels like a generic file store rather than a purpose-built registry.

## Weights and Biases Artifacts

Weights and Biases Artifacts extends W&B's experiment tracking platform with versioned artifact storage. An artifact is any file or directory that you want to version and track. Models are artifacts. Datasets are artifacts. Evaluation results are artifacts. The artifact API lets you log an artifact during a training run, link it to the run's metadata, and version it automatically. Later runs can reference artifacts by version, creating a lineage graph.

The strength of W&B Artifacts is lineage visualization. The platform automatically builds a graph showing how artifacts relate to each other. A model artifact links to the training run that produced it, which links to the dataset artifact it was trained on, which links to the preprocessing run that created the dataset. You can see the entire lineage in the UI. This makes it easy to answer questions like "which models were affected by a data quality issue in dataset version 3."

W&B Artifacts also supports artifact collections, which are groups of related artifacts. You might have a collection called production-models that contains all models currently deployed. You can query the collection to see what is running. You can add or remove models from the collection programmatically. This provides a lightweight registry on top of the artifact storage. The collection is just metadata, so updating it is fast and does not require moving large files.

The limitations of W&B Artifacts are similar to MLflow. It is designed for teams using W&B for experiment tracking. If you are not using W&B, adopting Artifacts alone is awkward. The artifact storage is tightly coupled to W&B runs, which means you need to log runs even if you just want to store models. The pricing model is based on storage and compute, which scales with artifact size. For teams with many large models, storage costs can become significant. Some teams use W&B for experiment tracking but store final models elsewhere to control costs.

## Custom Registries

Some teams build custom registries because off-the-shelf solutions do not fit their requirements. Common reasons include integration with existing internal systems, compliance requirements that demand on-premises storage, need for fine-grained access control, or workflows that do not map to existing registry models. Building a custom registry is significant work, but for large organizations with specific needs, it is sometimes the right choice.

A custom registry typically consists of a metadata database, an artifact store, an API for registration and retrieval, and a UI for browsing and search. The metadata database stores version numbers, training run identifiers, quality metrics, tags, and relationships between models. The artifact store holds the model files, usually in S3 or an equivalent object store. The API provides endpoints for uploading models, querying metadata, and downloading artifacts. The UI provides search, filtering, and visualization.

The advantage of a custom registry is control. You define the metadata schema. You define the versioning scheme. You integrate with your CI/CD pipelines, your monitoring systems, and your approval workflows. You implement the permissions model that matches your organization's policies. You optimize for your specific query patterns. If your organization has unique requirements, this control is valuable.

The cost is maintenance. A custom registry is a production system. It needs uptime, backups, disaster recovery, and oncall support. It needs ongoing development to add features, fix bugs, and adapt to changing requirements. For a team of five, building a custom registry is probably not worthwhile. For an organization with 200 ML engineers deploying 500 models, the investment might make sense.

## Cloud-Native Registries

Major cloud providers offer managed model registries as part of their ML platforms. AWS SageMaker Model Registry, Google Cloud Vertex AI Model Registry, and Azure Machine Learning Model Registry all provide similar capabilities. They integrate with the respective cloud's storage, IAM, and deployment services. If your infrastructure is already on one of these clouds, the native registry is often the easiest choice.

SageMaker Model Registry integrates with SageMaker training jobs and endpoints. A training job can automatically register a model upon completion. The registry stores metadata like approval status, model metrics, and deployment history. Models can be approved or rejected through the registry UI. Approved models can be deployed to SageMaker endpoints with one click. The integration is seamless if you are using SageMaker for training and deployment.

Vertex AI Model Registry provides similar functionality in GCP. It integrates with Vertex AI training and prediction services. Models are versioned automatically. Metadata includes lineage information that traces back to datasets and training jobs. The registry supports model evaluation, where you can compare versions side-by-side. It also supports deployment to Vertex AI endpoints or export to other serving platforms.

Azure ML Model Registry follows the same pattern. It integrates with Azure ML training and deployment. It supports model versioning, metadata tagging, and deployment tracking. It also supports responsible AI metadata, where you can attach fairness metrics, explainability reports, and data sheets to models. This is useful for organizations with strong governance requirements.

The limitation of cloud-native registries is lock-in. They work best when your entire ML pipeline is on the same cloud. If you train on AWS but deploy on-premises, SageMaker Model Registry becomes less useful. If you use multiple clouds, you need multiple registries or custom sync logic. The registries are also optimized for their respective cloud's ML services. If you use open-source frameworks and custom deployment infrastructure, the tight integration becomes less relevant.

## Registry Requirements

Regardless of which registry you choose, certain capabilities are non-negotiable for production use. Searchability is fundamental. You must be able to search models by name, version, training date, dataset, base model, quality metrics, tags, and any other metadata you track. Without search, the registry is just a file dump. Access control is also essential. Different teams and roles need different permissions. Data scientists might be able to register models but not deploy them. Production engineers might be able to deploy models but not delete them. Compliance teams might need read-only access to audit model usage.

Lineage tracking is critical for debugging and governance. Every model should link back to the training run, dataset, code version, and base model that produced it. When a model fails in production, you need to trace back to understand what inputs created it. When a dataset is found to have quality issues, you need to identify all models trained on it. Without lineage, these questions are unanswerable.

Metadata extensibility matters for evolving requirements. You cannot predict all the metadata you will need to track. The registry should allow custom fields. If you need to track regulatory approval status, cost per inference, or customer-specific customization flags, you should be able to add those fields without modifying the registry's core schema. Rigid schemas become obsolete as workflows evolve.

API access is necessary for automation. Deployment pipelines need to query the registry to find the latest approved model. Monitoring systems need to query the registry to link production behavior to model versions. Evaluation pipelines need to register new models and update their metadata. All of this must happen programmatically. A registry that only has a UI is not production-ready.

## Registry as Source of Truth

The most important discipline is treating the registry as the single source of truth. Models deployed to production must come from the registry, not from local filesystems, not from shared drives, not from training servers. This rule must be enforced technically, not just as policy. The deployment system should only accept model identifiers that exist in the registry. It should refuse to deploy models that are not registered.

This discipline prevents entire classes of problems. It prevents deploying an outdated model because someone still had it saved locally. It prevents deploying an experimental model that was never validated. It prevents losing track of what is running in production because deployments happened outside the system. It creates an audit trail where every deployment is linked to a registry entry, which is linked to a training run, which is linked to code and data.

The registry also becomes the checkpoint for promotion workflows. Models move from experimentation to staging to production by updating their registry metadata. A model is promoted to staging by setting its stage to staging in the registry. A model is promoted to production by setting its stage to production. The deployment system watches the registry and deploys models when their stage changes. This decouples model promotion decisions from deployment mechanics. Product teams can approve models. Platform teams can deploy them. The registry is the interface between the two.

The registry is not the final step in model management. It is the foundation. Once models are in a registry, you can build deployment automation, quality gates, cost tracking, and compliance reporting on top of it. Without a registry, all of those systems have no data to work with. The next step is standardizing how models are packaged so they can move reliably between the registry and production environments.

