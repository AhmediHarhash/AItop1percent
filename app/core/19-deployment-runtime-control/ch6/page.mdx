# Chapter 6 — Caching Strategies for AI Systems

Caching for LLMs is fundamentally different than caching for web content. A cached web page is identical every time you retrieve it. A cached LLM response might be wrong ten seconds later if the context changed, if the model was updated, if the user's state evolved, or if the response was non-deterministic to begin with. Traditional caching strategies assume deterministic outputs, stable keys, and safe invalidation policies. LLMs violate all three assumptions. You need semantic caching that matches similar inputs even when the text differs. You need invalidation policies that understand when outputs become stale. You need cache key design that captures context, not just the prompt. And you need to know when caching is actively harmful because freshness, safety, or personalization matters more than cost savings.

This chapter teaches caching strategies built for AI systems. You'll learn full response caching with request-response memoization, semantic caching that uses embeddings to match similar queries, KV-cache serving strategies that reuse computation across requests, and cache key design that balances specificity with hit rate. You'll learn invalidation policies based on TTL, event triggers, and manual overrides, cache infrastructure options from Redis to specialized AI caching layers, and when not to cache because the risk of stale responses outweighs the cost savings. By the end, you'll know how to design caching that cuts costs without silently serving outdated or incorrect answers.

---

- 6.1 — Why Caching Is Different for LLMs: Determinism, Context, and Cost
- 6.2 — Response Caching: Full Request-Response Memoization
- 6.3 — Semantic Caching: Embedding-Based Similarity Matching
- 6.4 — KV-Cache Serving Strategies: Reusing Computation Across Requests
- 6.5 — Cache Key Design: What Makes Two Requests The Same
- 6.6 — Cache Invalidation Policies: TTL, Event-Driven, and Manual
- 6.7 — Cache Hit Rate Optimization: Measuring and Improving
- 6.8 — Cache Infrastructure: Redis, Memcached, and Specialized Solutions
- 6.9 — Cache Warming and Precomputation Strategies
- 6.10 — When Not to Cache: Safety, Freshness, and Personalization Constraints

---

*A 60% cache hit rate sounds excellent until you realize the cached responses are outdated answers to questions about yesterday's data.*
