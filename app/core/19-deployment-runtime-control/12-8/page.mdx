# 12.8 — GitOps for AI: Argo CD, Flux, and Declarative Deployment

If it's not in git, it doesn't exist. GitOps makes your deployment state as auditable as your code. Every change to what runs in production is a git commit. Every rollback is a git revert. Every environment configuration is a file that can be reviewed, compared, and versioned. When production state drifts from what git declares, automated systems detect it and either reconcile the difference or alert the team. This discipline transforms deployment from a series of manual steps and scripts into a declarative system where the desired state is always visible, always auditable, and always recoverable.

GitOps is not new in 2026, but its application to AI systems is still maturing. Traditional GitOps handles code and configuration well. AI GitOps must also handle models, prompts, and evaluation artifacts—all of which are larger, change more frequently, and have dependencies that traditional infrastructure does not. The teams that succeed with AI GitOps do not try to force AI artifacts into traditional GitOps patterns. They adapt GitOps principles to AI constraints and build tooling that makes declarative deployment practical for models that weigh gigabytes and prompts that change daily.

## The GitOps Philosophy: Git as Single Source of Truth for Deployment State

GitOps centers on one principle: git is the single source of truth for what should be deployed. Every environment has a corresponding git repository or directory. The repository contains declarative specifications of what should run: which model version, which prompt template, which configuration values, which infrastructure settings. To change what runs in production, you change what is in git. To see what is running in production, you look at git. To roll back production, you revert a git commit.

This principle has powerful consequences. Deployment becomes deterministic. Given a git commit hash, you can reproduce exactly what was deployed. Deployment becomes auditable. Every change has a commit message, an author, a timestamp, and a code review. Deployment becomes recoverable. If production state is lost, you recreate it from git. If production state drifts, you detect it by comparing git to runtime.

Traditional deployment is imperative: a script runs commands that change production state. The script is the source of truth for what happens, but scripts can fail midway, can be run with different parameters, can be modified after the fact. You can log script execution, but logs do not capture intent. Logs show what commands ran. They do not show what state was intended.

GitOps deployment is declarative: git describes the desired state, and an automated system continuously reconciles actual state with desired state. If actual state matches desired state, the system does nothing. If actual state diverges, the system either corrects the divergence or alerts. The reconciliation loop runs continuously, ensuring that what git declares is what production runs.

## Declarative Deployment: Describing What Should Be Deployed, Not How

Declarative deployment specifies what the system should look like, not the steps to make it look that way. An imperative deployment script says: pull this model from the registry, stop the current inference service, load the new model, start the service, wait for health checks, mark the deployment successful. A declarative deployment specification says: the inference service should run model version 2.3.1 from this registry at this endpoint with these resource limits. The reconciliation system figures out what steps are needed to achieve that state.

The declarative approach is more robust. If a step fails midway, the reconciliation system retries. If production state drifts because someone manually changed something, the reconciliation system corrects it. If the desired state changes while deployment is in progress, the reconciliation system adapts. The deployment system is self-healing: it continuously works toward the declared state regardless of transient failures or manual interventions.

For AI systems, declarative deployment specifications include model artifacts, prompt templates, configuration files, and infrastructure requirements. A specification might declare: deploy model with this artifact hash from this registry, use prompt template version 3.7 with these parameters, configure inference with these timeout and retry settings, allocate these compute resources, enable these monitoring collectors. The specification does not say how to deploy these components. It says what the final state should be.

Writing declarative specifications requires discipline. The specification must be complete: it must describe everything that affects system behavior. The specification must be idempotent: applying it multiple times produces the same result as applying it once. The specification must be version-controlled: every change must be a commit with a clear description. The team must resist the temptation to make manual changes in production, because manual changes bypass git and break the GitOps contract.

## Argo CD: The Enterprise Standard for Kubernetes GitOps

Argo CD is the most widely adopted GitOps tool for Kubernetes in 2026. It watches git repositories for changes, compares the declared state in git with the actual state in Kubernetes clusters, and reconciles differences. When a new commit updates a deployment specification, Argo CD detects the change, validates it, and applies it to the cluster. When actual cluster state drifts from git, Argo CD either auto-corrects or alerts depending on configuration.

Argo CD's strength is its maturity and ecosystem. It integrates with Kubernetes-native primitives like Helm charts and Kustomize overlays. It provides a web UI that visualizes application state, deployment history, and synchronization status. It supports complex deployment patterns like canary deployments, blue-green deployments, and progressive rollouts. It has enterprise features like SSO integration, RBAC, and audit logging. For teams already running AI systems on Kubernetes, Argo CD is the natural choice for GitOps.

Using Argo CD for AI deployments requires adapting it to AI-specific needs. Models are large artifacts that do not belong in git repositories directly. Instead, git contains references to models in artifact registries. A deployment specification includes the model artifact hash and registry URL. When Argo CD reconciles, it pulls the model from the registry and deploys it. This keeps git lightweight while maintaining traceability.

Prompts and configuration files are small enough to store directly in git. A prompt template is a text file committed to the repository. A configuration file is a YAML or JSON file committed alongside the prompt. When these files change, Argo CD detects the change and redeploys the affected services. The git history becomes a complete record of every prompt change, making it easy to see what changed when and who changed it.

A healthcare AI company in mid-2025 adopted Argo CD for deploying their clinical decision support models. They structured their git repository with one directory per environment: dev, staging, production. Each directory contained Kubernetes manifests that declared which model version, which prompt template, and which configuration should run in that environment. To promote a model from staging to production, they copied the staging manifests to the production directory and committed the change. Argo CD detected the commit and deployed to production. The entire promotion was a git commit with a clear message and an automated deployment. Rollback was a git revert followed by Argo CD re-synchronization.

## Flux: The CNCF Alternative, Continuous Delivery the GitOps Way

Flux is the CNCF's GitOps solution, a set of Kubernetes controllers that keep clusters in sync with git repositories. Flux is more modular and Kubernetes-native than Argo CD, relying on custom resource definitions and controller patterns that align closely with Kubernetes design principles. Teams that prefer a more Kubernetes-centric approach or need deeper customization often choose Flux over Argo CD.

Flux operates through a toolkit of controllers: source-controller watches git repositories, kustomize-controller applies Kustomize overlays, helm-controller manages Helm releases, notification-controller sends alerts. These controllers run in the cluster and continuously reconcile desired state from git with actual state in the cluster. Flux does not require a central server or UI. It is fully distributed and declaratively configured through Kubernetes custom resources.

For AI deployments, Flux's modularity is an advantage when custom workflows are needed. A team might use source-controller to watch a git repository containing model specifications, write a custom controller that reconciles model artifacts from a registry into the cluster, and use notification-controller to alert when deployments succeed or fail. Flux's controller pattern makes it straightforward to extend GitOps to AI-specific artifacts.

Flux also integrates well with image automation. The image-reflector-controller watches container registries for new images, and the image-automation-controller updates git with new image tags. This pattern adapts to AI artifacts: a custom controller watches a model registry for new model versions, and when a new version passes staging validation, the controller updates git with the new version reference. Production deployments happen automatically when git changes, maintaining the GitOps principle that git is the source of truth.

A financial services company in late 2025 used Flux to manage multi-region AI deployments. Each region had its own Flux instance watching a shared git repository with region-specific overlays. When a new model was promoted to production, the team committed the new model reference to the shared repository. Flux instances in each region detected the change and deployed the model to their respective clusters. The deployment was synchronized across regions without manual coordination. Monitoring confirmed that all regions updated successfully, and regional dashboards showed consistent model versions across the globe.

## GitOps for AI Artifacts: Applying Declarative Deployment to Models and Prompts

AI artifacts differ from traditional infrastructure in size, mutability, and versioning. A Docker image is tens or hundreds of megabytes. A model is often several gigabytes or tens of gigabytes. A configuration file changes infrequently. A prompt changes weekly or daily. These differences require adapting GitOps patterns to AI constraints.

Models are referenced in git, not stored in git. A deployment specification includes a model artifact hash or version tag and a registry URL. The reconciliation system pulls the model from the registry when deploying. Git tracks which model version should be deployed, but the model bytes live in object storage or a model registry. This keeps git repositories small and fast while maintaining traceability.

Prompts are stored in git as text files or template files. They are small enough to commit directly and change frequently enough that git's version control is valuable. A prompt change is a commit that updates the prompt template file. The git history shows every prompt iteration, making it easy to see what changed when and revert to previous versions if needed. Prompt templates can use variable substitution for values that differ between environments, with environment-specific values stored in separate configuration files.

Configuration files for model inference settings, feature flags, and runtime parameters are also stored in git. These files declare thresholds, timeouts, retry policies, and other settings that affect model behavior. Changes to configuration are commits that can be reviewed, approved, and audited. The configuration history is as transparent as code history.

Evaluation artifacts like test datasets, golden outputs, and evaluation scripts are versioned in git or referenced from artifact storage. When a model is deployed, the corresponding evaluation artifacts are deployed alongside it so that regression tests can run in production environments. This ensures that production deployments are always accompanied by the tests that validate them.

## The Git-to-Cluster Reconciliation Loop: How Changes Flow from Git to Production

The reconciliation loop is the heart of GitOps. The loop continuously compares desired state in git with actual state in the cluster and takes action when they diverge. The loop runs on a schedule—typically every few minutes—or is triggered by git push events.

The loop begins by fetching the latest commit from the git repository. The GitOps controller reads the declarative specifications from git and parses them into a desired state model. The controller queries the cluster to determine actual state: which model version is running, which configuration is loaded, which resources are allocated.

The controller compares desired state to actual state. If they match, the controller does nothing and waits for the next reconciliation interval. If they differ, the controller determines what actions are needed to bring actual state into alignment with desired state. The controller executes those actions: pulling a new model from the registry, updating a ConfigMap with new prompt text, scaling resources to match desired limits.

After applying changes, the controller waits for health checks to pass. The new model must load successfully. The inference service must respond to test queries. The health endpoints must return OK. If health checks pass, the reconciliation is marked successful. If health checks fail, the reconciliation is marked failed, and the controller either retries or rolls back depending on configuration.

The entire reconciliation loop is logged. Each comparison, each action, each health check result is recorded. The logs provide an audit trail of what the GitOps system did, when, and why. When something goes wrong, the logs show which git commit triggered the change, what actions were taken, and where the deployment failed.

## Drift Detection: When Production State Doesn't Match Git

Drift occurs when actual production state diverges from the desired state declared in git. Drift is caused by manual changes: an engineer modifies a configuration value in production to debug an issue, an operator scales resources without updating git, a scheduled job modifies state in a way that git does not account for. Drift is dangerous because it breaks the GitOps contract: git is no longer the source of truth.

GitOps systems detect drift during reconciliation. The system compares actual state to desired state, finds a difference, and classifies it as drift. The system can respond in three ways: auto-heal by applying the desired state from git and overwriting the manual change, alert by notifying the team that drift was detected but not taking action, or ignore by marking certain resources as exempt from drift detection.

Auto-heal is the default for most GitOps systems. When drift is detected, the system corrects it automatically. An engineer who manually scales a deployment finds that the deployment scales back to the value declared in git within minutes. This enforces the discipline that changes must go through git. If you want to change production, you change git. Manual changes are rejected.

Alert is used for resources that should not be auto-healed or for environments where auto-healing is too aggressive. The system detects drift, logs it, sends an alert, and waits for human intervention. The team investigates the drift, determines whether it was intentional or accidental, and either updates git to match production or manually corrects production to match git.

Ignore is used for resources that are intentionally managed outside GitOps. Temporary debugging tools, operator-controlled scaling policies, or resources managed by other automation systems can be marked as exempt from drift detection. The system tracks these resources but does not attempt to reconcile them.

A retail company in early 2026 experienced a drift incident when an engineer manually updated a model configuration in production to mitigate a latency spike. The manual change worked, and the engineer intended to update git later. But before they could, the next reconciliation loop ran, detected the drift, and reverted the manual change. The latency spike returned. The incident taught the team that manual changes are ephemeral: either commit them to git immediately or accept that they will be reverted. The team updated their runbooks to include "commit change to git" as a mandatory step in any manual production change.

## Rollback via Git Revert: Reverting Deployment State by Reverting Commits

Rollback in GitOps is straightforward: revert the git commit that introduced the problematic change. The git revert creates a new commit that undoes the problematic commit. The GitOps reconciliation loop detects the new commit, recognizes that desired state has changed, and redeploys the previous configuration. Rollback is as simple and auditable as the original deployment.

This contrasts with imperative rollback, where the team must identify the previous version, find the deployment script or runbook, execute the rollback commands, and verify that rollback succeeded. Imperative rollback is error-prone because it requires the team to remember or reconstruct what the previous state was. GitOps rollback is reliable because the previous state is always visible in git history.

Rollback speed is determined by reconciliation loop frequency. If the loop runs every two minutes, rollback takes at most two minutes after the revert commit is pushed. If the loop runs every ten minutes, rollback takes up to ten minutes. Teams with aggressive rollback SLAs configure short reconciliation intervals or use webhook-triggered reconciliation to roll back immediately when a revert commit is pushed.

Some GitOps systems support declarative rollback policies. A policy might declare: if health checks fail after deployment, automatically revert to the previous commit. This policy turns rollback into an automated recovery mechanism rather than a manual intervention. The system detects failure, reverts git, and redeploys without human involvement.

A logistics company in mid-2025 deployed a new prompt that caused model output quality to degrade. Monitoring detected the degradation within five minutes. An engineer reverted the git commit that introduced the new prompt. Argo CD detected the revert within one minute and redeployed the previous prompt. Total time from detection to recovery was six minutes. The rollback was clean, auditable, and required no manual intervention beyond the git revert.

## GitOps and Feature Flags: Complementary Approaches to Deployment Control

GitOps controls what is deployed. Feature flags control what is enabled at runtime. The two are complementary. GitOps manages the deployment boundary: which model version runs, which prompt template is loaded, which configuration is active. Feature flags manage the runtime boundary: which features are enabled, which users see which behavior, which experiments are running.

A deployment might install a new model via GitOps but keep it disabled via feature flag. The model is present in production but not serving traffic. The team tests the model internally, monitors its behavior, and gradually enables it for users by updating the feature flag. If issues appear, the team disables the feature flag without redeploying. If the model needs to be removed entirely, the team reverts the git commit and redeploys.

Feature flags decouple deployment from enablement. Deployment can happen off-peak when risk is lower. Enablement can happen during business hours when the team is available to monitor. Deployment is binary: the model is either deployed or not. Enablement is gradual: the model serves five percent of traffic, then ten percent, then fifty percent. GitOps plus feature flags provides both deployment safety and rollout flexibility.

Some teams integrate feature flag state into their GitOps workflow. Feature flag configuration files are stored in git alongside deployment specifications. Changing a feature flag involves committing a configuration change to git and letting GitOps reconcile the change. This makes feature flag changes auditable and version-controlled, though it sacrifices the ability to toggle flags instantly without redeployment.

## Multi-Cluster GitOps: Managing Deployment Across Environments and Regions

Multi-cluster GitOps extends the GitOps pattern to multiple Kubernetes clusters representing different environments or regions. Each cluster has its own GitOps controller watching a git repository or directory. The repository structure reflects the multi-cluster topology: separate directories for each environment, shared base configurations with environment-specific overlays, or entirely separate repositories per cluster.

A common pattern is a monorepo with environment directories. The repository contains directories named dev, staging, and production. Each directory contains Kubernetes manifests declaring what should run in that environment. To promote a model from staging to production, the team copies or updates the production directory to match staging and commits the change. The production GitOps controller detects the commit and deploys.

Another pattern is overlay-based configuration using Kustomize or Helm. A base directory contains configurations shared across all environments. Environment-specific directories contain overlays that modify the base for each environment. The staging overlay might reduce resource limits, use synthetic data, and enable verbose logging. The production overlay might increase resource limits, use production data endpoints, and reduce logging verbosity. This pattern reduces duplication and makes environment differences explicit.

For multi-region deployments, the team creates a cluster per region, each with its own GitOps controller watching the same or similar git configurations. Regional differences are handled through overlays or region-specific configuration files. When a model is promoted globally, a single git commit updates all regions. Each region's GitOps controller detects the change and deploys independently. The team monitors all regions to ensure consistent deployment success.

A global SaaS company in late 2025 managed AI deployments across eight regions using Flux and a monorepo with regional overlays. Each region ran a Flux instance watching the same git repository. When a new model was promoted to production, the team updated the shared model reference in git. Flux instances in all eight regions detected the change within one minute and began deploying. Deployments completed across all regions within ten minutes. The team monitored a global dashboard that showed deployment status per region, and when all regions reported healthy, the promotion was considered complete.

The next step in deployment control is deployment triggers: defining what events should initiate deployment, how to handle concurrent deployments, and how to ensure that only validated changes reach production.

