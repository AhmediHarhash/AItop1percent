# 7.9 — Configuration Hot-Reload: Changing Behavior Without Restart

The incident started at 2:17 AM when the on-call engineer received an alert. A major customer was seeing response times over eight seconds, up from the usual 800 milliseconds. The root cause was clear within minutes: the system was routing their requests to Claude Opus 4.5 instead of Sonnet 4.5, and the larger model was overloaded. The fix was simple—change one line in the model routing configuration file. The problem was deployment. The standard deployment pipeline would take twelve minutes: build the container, push to registry, rolling restart across forty-eight pods. Twelve minutes of degraded service for every request from a customer paying $180,000 annually. Instead, the engineer opened the configuration management console, changed the model selection parameter for that customer's tenant ID, and clicked save. Three seconds later, every instance picked up the new configuration. Response times dropped back to 750 milliseconds. No restart, no deployment, no extended outage.

This is what hot-reload enables. Configuration changes take effect without service interruption. For AI systems where behavior is determined by dozens of parameters—model selection, temperature, timeout thresholds, retry logic, prompt versions, tool availability—the ability to change these parameters without restarting the service is not a convenience. It is a requirement for operating at production scale.

## The Hot-Reload Requirement

Traditional application configuration happens at startup. The service reads a configuration file or environment variables when it launches, stores those values in memory, and uses them until the process terminates. Changing configuration means deploying new code or restarting the service. For systems with infrequent configuration changes and generous maintenance windows, this model works. For AI systems in 2026, it does not.

AI systems require configuration changes multiple times per day. A customer reports that responses are too verbose, so you lower max tokens. A model provider raises rate limits, so you increase concurrency. A prompt performs poorly for a specific use case, so you swap in an alternative version. Quality metrics show drift for one customer segment, so you route them to a different model. Security identifies a tool that is being abused, so you disable it for certain user groups. Each of these changes affects live traffic. Each requires immediate effect. Waiting for a deployment cycle means minutes or hours of suboptimal behavior.

Hot-reload means the running service detects configuration changes and applies them without terminating existing processes. The service continues handling requests. In-flight operations complete normally. New operations pick up the updated configuration. From the outside, nothing restarts. From the inside, behavior shifts.

The engineering challenge is making this transition safe. Configuration is not passive data. It determines what code paths execute, what external services are called, what validation rules apply. A configuration change is a behavior change. The service must detect the change, validate it, coordinate across all running instances, and switch atomically without creating inconsistent states or partial updates. Get this wrong and half your instances run the old configuration while the other half run the new one, creating subtle failures that are difficult to diagnose.

## Configuration Sources and Polling

Hot-reload requires the service to check for configuration updates while running. The traditional configuration file on disk does not support this well. You can poll the file, but updating files across distributed systems is cumbersome and error-prone. Modern AI services pull configuration from external systems designed for dynamic updates.

The most common source is a configuration service: a centralized API that stores key-value pairs, supports versioning, and notifies services when values change. Examples include AWS AppConfig, Google Cloud Runtime Configurator, HashiCorp Consul, and etcd. The AI service connects to the configuration service at startup, loads the current configuration, and subscribes to updates. When configuration changes in the service, all subscribed instances receive notifications within seconds. The service validates the new configuration and, if valid, swaps it into active use.

Feature flag systems serve the same role but with richer semantics. LaunchDarkly, Split, Flagsmith, and similar platforms manage boolean flags, percentage rollouts, and targeting rules. An AI service might use feature flags to control whether a new prompt template is active, what percentage of traffic uses an experimental model, or which customer segments have access to beta tools. The flag platform handles rollout logic, gradual exposure, and emergency rollback. The AI service queries the flag state for each decision point.

Some teams store configuration in databases, typically a key-value table with tenant or user scope. The service polls the database every few seconds or subscribes to change notifications if the database supports them. This approach works well when configuration is tightly coupled to customer data and needs to be managed through the same admin interfaces that control accounts and permissions.

Cloud storage like S3 or Google Cloud Storage is another option, especially for larger configuration objects like prompt templates or few-shot example sets. The service polls the storage bucket at regular intervals, checks object metadata for changes, and downloads updated files when detected. This pattern supports configuration files too large to fit comfortably in a key-value store and allows non-engineers to update configuration by uploading files.

The trade-off across these approaches is latency versus complexity. Configuration services and feature flag platforms provide near-instant updates but add external dependencies and operational overhead. Polling databases or cloud storage introduces delay—typically five to thirty seconds—but integrates with existing infrastructure. For incident response scenarios where every second matters, the faster update path is worth the dependency. For planned changes during business hours, polling every ten seconds is sufficient.

## Reload Triggers and Consistency

Detecting configuration changes is only half the problem. The service must decide when to apply them. The naive approach is to apply changes immediately upon detection, but this creates consistency issues in distributed systems. If you have forty instances running across three regions and each instance polls configuration independently, they will detect the change at different times. For several seconds, some instances run the old configuration while others run the new one. If the configuration change affects routing logic or model selection, different users see different behavior based on which instance handled their request.

The solution is coordinated reload. When the configuration source changes, all instances receive a notification or detect the change on their next poll. Each instance validates the new configuration locally to ensure it is well-formed and safe to activate. If validation passes, the instance marks itself as ready to switch. Once all instances report ready—or after a timeout if some instances are slow—a coordinator signals all instances to activate the new configuration simultaneously. This coordination can be explicit, using a distributed lock or leader election, or implicit, using a version number or timestamp that all instances check before activating.

In practice, perfect synchronization across geographically distributed systems is impossible. There will always be a brief window where some instances are slightly ahead or behind. The goal is to minimize this window and ensure that no instance runs a configuration more than one version out of date. If the configuration service increments a version number with each change, instances can refuse to activate a configuration unless they have also received all previous versions. This prevents skipping updates and ensures that configuration evolves in a predictable sequence.

For AI systems where configuration changes affect prompt structure or tool availability, consistency matters. If one instance is using prompt version three and another is using version five, and version four introduced a new output format that downstream systems depend on, the version-three instance will produce responses that break downstream parsing. Coordinated reload prevents this. All instances move from version four to version five together, or they continue running version four until coordination completes.

## Validation Before Activation

Hot-reload is dangerous if invalid configuration reaches production. A typo in a model name, a malformed JSON schema, a temperature value outside valid bounds—any of these will cause runtime errors when the service tries to use the configuration. Unlike startup validation, where a bad configuration causes the service to refuse to start, hot-reload validation happens while the service is already running and serving traffic. If validation is weak, a bad configuration can crash the service or degrade behavior for all users.

Validation must happen before activation. When the service receives new configuration, it parses and checks it in isolation without affecting live traffic. The validation logic depends on what the configuration controls. For model selection, validation confirms that the specified model name exists in the provider's catalog and that the service has valid credentials for it. For prompt templates, validation checks that all placeholders match expected input fields and that the template produces well-formed text when rendered with sample data. For numeric parameters like temperature or timeout, validation ensures values fall within allowed ranges. For tool configurations, validation confirms that referenced functions or APIs are reachable and respond correctly to test inputs.

A fintech company with fifty AI-powered features across their platform implemented a three-stage validation process for configuration hot-reload. Stage one was schema validation: every configuration object had a JSON schema, and the system rejected any update that did not match. Stage two was semantic validation: the system checked that model names, API endpoints, and feature flags referenced real resources. Stage three was dry-run validation: the system executed a test query using the new configuration in a sandbox environment and verified that the response matched expected structure and quality. Only after passing all three stages did the configuration activate in production. This process caught 90 percent of configuration errors before they affected users. The remaining 10 percent were logic errors—configurations that were technically valid but produced undesirable behavior—which required rollback mechanisms covered later.

Validation should also include rate limit checks. If the new configuration routes traffic to a model with lower rate limits than the current configuration, activating it will immediately trigger throttling errors. The validation step should estimate the load the new configuration will generate and confirm that the target model or API can handle it. If not, the system should reject the change and alert the operator to provision additional capacity first.

## Graceful Transition and In-Flight Requests

When configuration changes, the service often has requests already in progress. These in-flight requests were initiated under the old configuration. Switching to the new configuration mid-request can cause failures if the two configurations are incompatible. For example, if the old configuration routed requests to Claude Sonnet 4.5 and the new configuration routes to GPT-5 Nano, an in-flight request that has already started streaming from Claude cannot suddenly switch to GPT mid-stream.

The graceful transition strategy is to let in-flight requests complete with the old configuration and apply the new configuration only to new requests. This requires the service to track which configuration version each request is using. When a request arrives, the service captures the current configuration version and associates it with that request's execution context. If configuration changes while the request is processing, the request continues using its original version. Only requests that arrive after the configuration change use the new version.

This approach introduces a transition period where two configuration versions are active simultaneously: the old version for in-flight requests and the new version for new requests. The service must maintain both configurations in memory until all requests using the old version complete. For short-lived requests measured in seconds, this is straightforward. For long-lived requests—such as agent sessions that last minutes or batch processing jobs that run for hours—this becomes more complex. The service must decide how long to keep the old configuration available. A common pattern is to set a maximum transition time, such as sixty seconds, after which the service forcibly switches all remaining requests to the new configuration and logs warnings for any that were still in progress.

Some configuration changes cannot be applied gracefully at the request level because they affect global state rather than per-request behavior. For example, changing the connection pool size for a model provider or adjusting the concurrency limit for tool calls affects all requests, not individual ones. In these cases, the service applies the change immediately, and in-flight requests experience the new behavior mid-execution. This is acceptable if the change is backward-compatible—such as increasing concurrency, which improves performance without breaking correctness. It is problematic if the change is incompatible—such as disabling a tool that some requests are actively using. For incompatible changes, the safe path is to wait until all in-flight requests complete before activating the new configuration, effectively draining traffic before the switch.

## Configuration Caching and Freshness

Hot-reload introduces a trade-off between configuration freshness and performance. Querying the configuration source on every request adds latency. For a service handling ten thousand requests per second, making ten thousand calls to the configuration service creates enormous load and adds milliseconds to every request. The solution is caching: the service loads configuration once, caches it in memory, and reuses it for many requests. Configuration changes are detected asynchronously via polling or push notifications, and the cache is updated when changes occur.

The caching strategy determines how quickly configuration changes take effect. If the service polls the configuration source every ten seconds, changes take up to ten seconds to propagate. If the configuration source pushes notifications and the service receives them within one second, changes propagate much faster. The right polling interval depends on how urgent configuration changes are. For routine changes like adjusting prompt templates or tweaking thresholds, ten to thirty seconds is acceptable. For incident response scenarios where you need to disable a failing model or reroute traffic immediately, one to three seconds is the target.

Caching also affects memory usage. If configuration includes large objects like full prompt templates or few-shot example sets, caching them in memory on every instance multiplies storage costs. A prompt template that is two kilobytes becomes eighty kilobytes when cached across forty instances. For large configuration objects, the service can cache only metadata or references and load the full objects lazily when needed. This reduces memory usage at the cost of occasional load latency when a new configuration object is first accessed.

Some teams implement multi-level caching: a fast in-memory cache for frequently accessed configuration and a slower remote cache or database for less common values. The service checks the in-memory cache first, and only queries the remote source if the value is missing or expired. This pattern works well when configuration has a long tail distribution: a small set of values is accessed constantly, and a large set is accessed rarely.

## Testing Hot-Reload

Testing configuration hot-reload is harder than testing static configuration because you must verify not only that the new configuration works, but that the transition from old to new happens correctly. The test suite must cover configuration detection, validation, coordination, activation, and rollback. Each of these stages has failure modes that only appear during live updates.

The first test category is detection latency: how quickly does the service detect configuration changes? The test updates configuration in the source system and measures how long before each service instance picks up the change. The target latency depends on your requirements, but anything over thirty seconds indicates a problem with polling intervals or notification delivery. If some instances detect changes much faster than others, the coordination mechanism may be broken.

The second category is validation correctness: does the service reject invalid configuration and accept valid configuration? The test suite injects deliberately malformed configuration—missing required fields, out-of-range values, references to nonexistent resources—and verifies that the service rejects it without affecting live traffic. It also injects valid configuration and confirms that the service activates it successfully.

The third category is consistency: do all instances activate the same configuration version at roughly the same time? The test updates configuration and sends a burst of test requests to multiple instances, checking that all responses reflect either the old configuration or the new configuration, but never a mix. If some responses show old behavior and others show new behavior for an extended period, the coordination mechanism is not working.

The fourth category is rollback: can the service revert to a previous configuration if the new one causes problems? The test activates a configuration change that intentionally degrades behavior—for example, routing to a slow model or setting an overly restrictive timeout—and then triggers a rollback. The test verifies that the service returns to the previous configuration and that quality metrics recover.

Testing hot-reload in production requires observability. The service must log every configuration change with timestamps, version numbers, and the affected parameters. It must emit metrics showing which configuration version each instance is running. During incidents, engineers need to see at a glance whether all instances are on the same version, which version was active when the problem started, and when the last configuration change occurred. Without this visibility, hot-reload becomes a black box that makes debugging harder rather than easier.

## Monitoring Configuration Changes

Configuration changes are deployment events. They alter system behavior just as code deployments do, and they should be monitored with the same rigor. Every configuration change must be logged with a full audit trail: who made the change, when, what values changed, and why. This log is essential for incident response. When something breaks, the first question is always "what changed?" If configuration changes are not logged, you are blind to a major category of changes.

The log should capture not just the final values but the delta: what was the old value, what is the new value. This makes it easy to understand the impact of a change without needing to cross-reference historical snapshots. For AI systems, the delta might show that the temperature parameter increased from 0.7 to 0.9, the max tokens decreased from 800 to 600, or the model selection switched from GPT-5 Mini to Claude Sonnet 4.5. Each of these changes has predictable effects on behavior, and seeing the delta immediately suggests what symptoms to look for.

Alerts should fire when configuration changes occur outside normal patterns. If configuration typically changes during business hours and suddenly changes at 3 AM, that is suspicious. If a parameter that has been stable for weeks suddenly changes multiple times in ten minutes, something is wrong. If a configuration change causes immediate degradation in quality metrics or error rates, the system should alert and consider auto-rollback.

Some teams implement automatic rollback based on metric thresholds. The system monitors key metrics like error rate, latency, and task success rate for the first five minutes after a configuration change. If any metric degrades beyond a threshold—for example, error rate increases by more than 20 percent or latency exceeds the 95th percentile by more than 50 percent—the system automatically reverts to the previous configuration and pages the on-call engineer. This safety net prevents configuration changes from causing prolonged outages.

Monitoring should also track configuration drift across instances. In a healthy system, all instances should be running the same configuration version within a few seconds of each other. If one instance is running version twelve while others are on version fifteen, something is preventing that instance from receiving updates. This could be a network partition, a stuck polling loop, or a crashed background thread. The monitoring system should detect this drift and alert before it causes user-visible problems.

Configuration changes should be correlated with quality metrics in dashboards. When you view a graph of response latency or success rate over time, configuration change events should appear as vertical markers on the timeline. This makes it immediately obvious when a metric change coincides with a configuration change. If latency spikes at 2:35 PM and a configuration change happened at 2:34 PM, you have a strong hypothesis for the root cause. Without this correlation, you waste time investigating unrelated code deployments or infrastructure issues.

With configuration hot-reload in place, the next question is what configuration to expose for dynamic control. For AI systems, the most powerful dynamic configuration is not numeric thresholds or timeout values, but the artifacts themselves: prompts, models, tools, and generation parameters. Feature flags for these artifacts turn behavior changes into operational actions, not engineering projects.

