# 9.2 — Prompt Version Control: Git, Registries, and Audit Trails

In November 2025, a healthcare AI company discovered their best-performing clinical documentation prompt was from three weeks earlier, but nobody had saved it. The current version was generating technically accurate notes, but physicians found them harder to read. Someone remembered that version 2.8 had perfect readability scores and high physician satisfaction. The team searched their Git history. The prompt wasn't there. It had lived in a Python file as a string literal, and when the developer refactored the code two weeks ago, they had rewritten the prompt from memory rather than copying it exactly. The original version was gone. The team spent four days trying to reconstruct what made version 2.8 work, running experiments based on physician feedback and old screenshots of the output. They never fully recovered the magic. The lesson was simple: if you don't version your prompts, you can't return to what worked.

Prompt version control is not optional. Every prompt change must be tracked, attributed, and recoverable. When a prompt improves, you need to know what changed so you can apply the lesson elsewhere. When a prompt degrades, you need to revert to the last known good version in seconds, not hours. When regulators or auditors ask why the model behaved a certain way on a specific date, you need to show them the exact prompt version that was live at that time. Version control gives you all of this, but only if you treat prompts as artifacts worth versioning in the first place.

## Git for Prompts

The simplest version control strategy is to store prompts in Git alongside or separate from your application code. Each prompt is a file. Edits are commits. Branches are experiments. Merges are deployments. This works because Git already solves the hard problems: distributed collaboration, conflict resolution, history tracking, and rollback. If your team knows Git, they can version prompts without learning new tools.

The file format matters. Plain text is best. A prompt stored as a text file with YAML or JSON structure is readable in diffs, searchable with grep, and compatible with every Git workflow. A prompt stored in a binary format or a database dump is opaque. When you compare two versions, you see "file changed" but not what changed. The whole point of version control is visibility into change history. Use a format that Git can diff.

The folder structure matters too. One common pattern is to organize prompts by domain or use case: customer-support/greeting.yaml, customer-support/escalation.yaml, legal-review/contract-analysis.yaml. Another pattern is to organize by identifier with version as part of the filename: prompts/cust-support-001/v1.yaml, prompts/cust-support-001/v2.yaml. The first pattern is easier to navigate when editing. The second pattern makes it explicit that each file is immutable once committed. Both work. Pick one and be consistent.

Git gives you branching, which is powerful for prompt experiments. A product manager proposes a new tone. They create a branch, edit the prompt, and push the branch. The CI system automatically evaluates the new prompt against the test suite and posts results as a comment on the pull request. The domain expert reviews the change, suggests tweaks, and the product manager iterates. When the prompt passes evaluation and review, the branch merges to main and the new version deploys. This workflow is familiar to engineering teams and can be taught to non-engineers with the right tooling.

The limitation of Git is that it's designed for engineers. Non-technical users struggle with branches, merge conflicts, and command-line workflows. If your product managers are comfortable with Git, use it directly. If they're not, build a UI on top of Git that abstracts away the complexity. The UI lets them edit prompts in a web form, click "propose change," and track the review process. Behind the scenes, the UI creates a branch, commits the change, and opens a pull request. The version control is still Git. The user experience is simplified.

## Monorepo vs Polyrepo

The decision is whether prompts live in the same Git repository as your application code or in a dedicated prompt repository. The monorepo approach keeps everything together. Developers see prompts and code in the same place. When a code change requires a prompt change, both happen in the same pull request. The dependency is explicit. Deployment is synchronized. The downside is coupling. Prompt changes trigger the same CI pipeline as code changes. If your CI runs for twenty minutes because it builds Docker images and runs integration tests, every prompt edit waits twenty minutes. The iteration speed advantage of prompt-as-code evaporates.

The polyrepo approach puts prompts in their own repository with their own CI pipeline. Prompt changes are independent. The CI pipeline for prompts runs evaluation suites, checks syntax, and validates metadata. It doesn't build Docker images or run application tests because those aren't relevant to prompts. A prompt change deploys in two minutes instead of twenty. The downside is coordination. When a code change depends on a new prompt version, you need to ensure the prompt deploys first. When a prompt change assumes new application behavior, you need to ensure the code deploys first. The dependencies are cross-repo, which means your deployment tooling needs to handle them explicitly.

The right choice depends on your team's deployment cadence. If you deploy code multiple times per day, a monorepo might work because the overhead is already amortized. If you deploy code once per week but want to iterate on prompts daily, a polyrepo is better. The iteration speed for prompts should not be limited by the deployment speed of code. If the two artifacts have different velocities, separate them.

A middle-ground approach is a monorepo with separate CI pipelines for different paths. Changes to the prompts directory trigger a fast prompt-only pipeline. Changes to application code trigger the full build pipeline. This gives you the coordination benefits of a monorepo with the speed benefits of separation. The downside is complexity: your CI configuration needs to detect which paths changed and route to the correct pipeline. The complexity is worth it if your team is large enough that coordination overhead justifies the tooling investment.

## Prompt Registries

A prompt registry is a centralized store for prompts with versioning, search, metadata, and access control. It's not a replacement for Git. It's a layer on top of Git or an alternative to Git for teams that need more than version control. A registry provides a UI for browsing prompts, an API for fetching them at runtime, and a database for indexing metadata like tags, owner, evaluation results, and deployment history.

The core capability is versioning. Every prompt in the registry has a unique identifier and an incrementing version number. Version 1 of prompt customer-support-greeting is distinct from version 2. Once a version is published, it's immutable. You can't edit version 2. You can only create version 3. Immutability is critical for audit trails and rollback. If version 2 is live in production and causes a problem, you can confidently roll back to version 1 knowing that version 1 is exactly what it was when you deployed it weeks ago. If versions were mutable, you couldn't be sure.

The search capability matters more than it sounds. As your prompt library grows, you'll have dozens or hundreds of prompts. A developer working on a new feature needs to find prompts related to "customer escalation." A product manager troubleshooting an issue needs to see all prompts deployed in the last week. A compliance officer needs to list all prompts that handle user data. Without search, people resort to asking in Slack or scrolling through directories. With search, they query the registry by tag, by owner, by deployment date, by evaluation score, or by free text in the prompt content.

Metadata turns prompts into queryable assets. Each prompt entry includes not just the template but also who created it, when, why, what evaluation criteria it's supposed to meet, what traffic it's currently serving, and what experiments are running on it. This metadata is what makes a registry more useful than a folder of text files. When you deploy a new version, the registry records the evaluation results, the approver, and the rollout schedule. When you look at a prompt six months later, you see its entire history without digging through Git logs.

Access control becomes important when multiple teams share a prompt registry. The customer support team should be able to edit customer support prompts but not legal review prompts. Product managers should be able to propose changes but not deploy them to production without approval. The registry enforces these rules. It checks permissions before allowing edits, tracks who approved what, and maintains an audit log of every access event. This is harder to enforce with raw Git unless you layer permissions tooling on top.

## Immutable Versions

The rule is simple: once a prompt version is published, it never changes. If version 3 needs a fix, you don't edit version 3. You publish version 4. Immutability seems inconvenient at first. It feels easier to just fix the typo in place. But immutability is what makes prompt systems safe and auditable.

The safety argument is about rollback. If your current prompt is causing problems, you roll back to the previous version. If the previous version is mutable, "rolling back" means hoping that the previous version is still what you think it is. Maybe someone fixed a typo in it yesterday. Maybe someone updated a variable name. You don't know, which means you don't know if the rollback will fix the problem or introduce new ones. Immutability guarantees that rolling back to version 2 gives you exactly what version 2 was when it passed evaluation and went live the first time.

The auditability argument is about compliance. When a regulator asks what prompt was live on March 15, you need to show them version 7, exactly as it was on March 15. If versions are mutable, you can't. You can show them a file named version 7, but you can't prove it hasn't been edited since March. Immutability gives you a cryptographic guarantee. Version 7's content hash is stored in the deployment log. You can prove that the content you're showing today matches the content that was live on March 15.

Immutability also forces clarity about what changed. When you publish version 4, you write a change description. "Fixed typo in line 12" or "Adjusted tone to be more formal" or "Added constraint to prevent financial advice." This description is part of the version metadata. Over time, the change descriptions become a narrative of how the prompt evolved and why. If versions were mutable, this narrative wouldn't exist. Changes would be silent edits scattered across time.

The cost of immutability is storage. Every version is a new record. A prompt that goes through fifty iterations has fifty versions in the registry. For text data, the cost is trivial. Prompts are measured in kilobytes, not gigabytes. A registry with ten thousand prompts, each with an average of twenty versions, stores around two million text records. On modern infrastructure, that fits in memory. The storage cost is negligible compared to the safety and auditability benefits.

## Branch Strategies

Git branches are how you experiment with prompts without affecting production. The strategy depends on your workflow. A common pattern is trunk-based development: the main branch is always production-ready, experiments happen in short-lived feature branches, and changes merge back to main as soon as they pass evaluation and review. This works well for teams that iterate quickly and have strong automated testing.

Another pattern is environment branches: you have a staging branch and a production branch. Prompt changes merge to staging first, get tested in the staging environment, and then merge to production. This gives you a full integration test before production deployment. The downside is slower iteration. Every prompt change goes through a two-step merge process. If your staging environment accurately reflects production and your evaluation suite is comprehensive, the safety benefit justifies the speed cost. If staging is unreliable or your eval suite already catches issues, the extra step is overhead.

Experiment branches are longer-lived. A product manager wants to test three different tones for a customer support prompt. They create three branches: experiment-formal-tone, experiment-casual-tone, experiment-empathetic-tone. Each branch has a different version of the prompt. The deployment system routes a slice of production traffic to each branch. After a week, the team compares metrics and merges the winner to main. The branches eventually get deleted, but they live as long as the experiment runs.

The branching strategy needs to match your deployment strategy. If your deployment system can route traffic based on Git branches, experiment branches are powerful. If your deployment system expects every production artifact to be on the main branch, you'll need to merge experiments to main before you can A/B test them, which feels backward. The tooling and the workflow need to align.

## Audit Requirements

Every industry with compliance requirements needs an audit trail for prompts. Healthcare needs to show what instructions the model received when it generated a clinical note. Finance needs to show what prompt was active when the model gave investment advice. Legal needs to show what prompt was used to analyze a contract. The audit trail must be tamper-proof, timestamped, and attributable.

The minimum audit trail records four things for every prompt change: who made the change, when, what changed, and why. The "who" is an authenticated user identity. The "when" is a UTC timestamp. The "what" is a diff between the old version and the new version. The "why" is a human-written explanation, like a commit message. These four pieces of information go into an append-only log that can't be edited or deleted without leaving evidence of tampering.

The deployment log is separate from the change log. The change log tracks edits to prompt versions. The deployment log tracks when each version went live, what percentage of traffic it served, and when it was rolled back if applicable. When you need to answer "what prompt was active for user session 123456 on March 15 at 14:32," you query the deployment log, find which prompt version was live at that timestamp, and then retrieve that version from the registry. The version is immutable, so you know it hasn't changed since it was deployed.

Regulators sometimes want more than just the prompt text. They want the evaluation results that justified the deployment. They want to know who approved it. They want to see the rollout schedule and the monitoring data that confirmed it was safe to expand beyond the initial five percent of traffic. A mature audit system stores all of this alongside the prompt version. When you retrieve version 7, you also retrieve its evaluation report, its approval record, its rollout schedule, and its performance metrics from the first 24 hours in production.

The storage format matters. Audit logs stored in a database can be altered if someone has database access. Audit logs written to append-only storage with cryptographic signatures are tamper-evident. If someone tries to change a log entry, the signature breaks and the tampering is detectable. For high-stakes applications, the extra infrastructure is worth it. For lower-stakes applications, a database with restricted access and regular backups is sufficient.

## Commit Messages for Prompts

Prompt commit messages serve the same purpose as code commit messages: they explain what changed and why. But the audience is different. Code commit messages are read by engineers. Prompt commit messages are read by engineers, product managers, domain experts, and sometimes auditors. The message needs to be clear to all of them.

A good prompt commit message has three parts. First, a one-line summary of the change: "Adjusted tone to be more empathetic in escalation scenarios." Second, a detailed explanation of what changed: "Replaced the phrase 'we understand your frustration' with 'I hear you, and I'm here to help.' Added examples showing how to acknowledge the user's emotion before offering a solution." Third, the justification: "User feedback indicated that the previous tone felt robotic. A/B test of the new version showed a 12 percent increase in satisfaction scores with no change in resolution time."

The justification is critical. Prompt changes aren't arbitrary. They're driven by data, feedback, or new requirements. The commit message should make it clear why the change was worth making. This helps future reviewers understand the evolution. When someone looks at the change history six months later, they should be able to follow the reasoning without asking around.

Commit messages also document evaluation. "This version passed regression evals with 94 percent accuracy on the golden set, up from 91 percent on the previous version. Edge case coverage improved from 78 percent to 84 percent." When the prompt causes a production issue later, you can look back at the commit message and see what was tested before deployment. If the issue involves a case that wasn't covered in the evals, the commit message helps you understand why the gap existed.

Bad commit messages are vague: "Updated prompt" or "Fixed issues" or "Per product feedback." These tell you nothing. Six months later, nobody remembers what issues were fixed or what the product feedback was. The commit is useless as documentation. Treat prompt commit messages the same way you treat code commit messages: as permanent documentation of intent.

## Comparing Prompt Versions

Diff tools for prompts need to show what changed in a way that's meaningful for prompt authors, not just for version control systems. A Git diff shows line-by-line changes, which works for code but is awkward for prose. A single sentence edit might show as deleting three lines and adding three lines when what you really want to see is "changed 'we understand' to 'I hear you' in sentence four."

Semantic diff tools for prompts highlight phrase-level changes. Instead of showing line deletions and additions, they show "replaced X with Y" or "added this sentence after paragraph two" or "removed this example from the few-shot section." These tools understand the structure of prompts. They know that a prompt has a system message, a user message template, a few-shot examples section, and output formatting instructions. When you compare versions, the diff shows changes within each section.

Visual diff tools make comparison even easier for non-technical users. Instead of showing text with plus and minus signs, they show the old version and the new version side by side with color highlighting on the changed parts. A product manager can glance at the diff and immediately see that the tone shifted from formal to casual without parsing Git syntax.

The comparison workflow is part of the review process. When someone proposes a prompt change, reviewers need to see the diff before approving. The diff should include not just the template text but also changes to metadata, variable schemas, and evaluation criteria. If the new version adds a variable, the diff shows it. If the new version changes the evaluation threshold from 90 percent to 95 percent, the diff shows it. A complete diff prevents surprises where a prompt looks similar but has hidden changes that affect behavior.

Comparing prompts also means comparing their outputs. Two prompt versions might have different text but produce similar results, or they might have similar text but produce different results. An output comparison tool runs both versions on the same test inputs and shows the differences in model responses. This is more informative than comparing templates because it shows the actual behavioral change. A reviewer can see that the new prompt produces shorter responses, or more detailed responses, or responses with a different emotional tone. The output comparison is what matters for approving a change.

The next question is what happens before a prompt deploys — how do you test that the new version doesn't break existing capabilities, introduce new errors, or degrade quality in ways your eval suite might miss.

