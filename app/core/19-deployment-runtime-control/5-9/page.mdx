# 5.9 — Routing Rule Configuration and Hot-Reload

The production dashboard showed the problem clearly. At 2:47 AM on a Tuesday in August 2025, a healthcare platform's primary provider started returning responses thirty seconds slower than normal. Median latency had been 380 milliseconds for weeks. Now it was spiking to 11 seconds, then 18, then failing entirely with timeout errors. The routing system kept sending traffic to the degraded provider because that's what the configuration said to do. The on-call engineer knew exactly what needed to happen — shift all traffic to the secondary provider immediately. The problem was that routing configuration lived in YAML files deployed with the application. Changing routing meant building a new container, pushing to the registry, and rolling out across eighteen pods. Estimated time to fix: twenty-five minutes. Actual time patients waited for broken responses: twenty-five minutes. The platform lost seventeen thousand dollars in SLA credits and damaged relationships with two hospital systems. The infrastructure worked perfectly. The deployment process worked perfectly. The constraint was that routing decisions were compiled into the deployment artifact instead of being runtime-configurable data.

## The Runtime Configuration Requirement

Production AI systems need routing rules that can change without service restarts. This is not a convenience feature. This is infrastructure reliability engineering. When a provider degrades, when costs spike unexpectedly, when a new model becomes available, when an A/B test needs to expand or contract — all of these scenarios require immediate routing changes. If your routing configuration requires deployment to modify, you have built a system that cannot respond to production reality at the speed production demands.

The principle sounds simple: move routing rules from code to configuration. The implementation requires careful architecture. You need somewhere to store configuration that multiple services can read. You need a mechanism for services to detect configuration changes and reload rules without dropping in-flight requests. You need validation to prevent bad configurations from reaching production. You need versioning so you can understand what changed and when. You need gradual rollout so you can test rule changes on a subset of traffic before applying them everywhere. This is not reading a config file at startup. This is building a runtime control plane for request routing.

## Configuration Storage Options

Where routing rules live determines how quickly they can change and how reliably services can read them. A configuration database — PostgreSQL, MySQL, DynamoDB — gives you durability, consistency, and query capability. Services poll the database every few seconds checking for configuration version changes. When the version increments, they fetch new rules and reload. Latency from configuration change to service adoption: five to fifteen seconds depending on poll interval. Cost: database reads every poll interval times number of service instances. For a hundred-instance deployment polling every ten seconds, that's ten reads per second, well within free tier for most managed databases.

A dedicated configuration service — Consul, etcd, ZooKeeper — provides stronger consistency guarantees and watch-based notification instead of polling. Services open a watch connection. When configuration changes, the config service pushes notifications to all watchers immediately. Latency from configuration change to service adoption: sub-second. Cost: persistent connections from every service instance to the config service. This scales well to thousands of instances but requires running and operating the config service infrastructure.

Feature flag systems — LaunchDarkly, Split, Statsig, Unleash — are purpose-built for runtime configuration changes. They provide not just storage but also gradual rollout, targeting rules, and experimentation frameworks. You can change routing rules for ten percent of traffic, for specific customer segments, for requests from particular regions. Latency from configuration change to adoption: typically sub-second via streaming connections. Cost: per-seat or per-request pricing depending on vendor. For routing configuration specifically, feature flags may be overengineered — you're paying for experimentation infrastructure when you primarily need fast reliable configuration distribution.

The simplest option for teams already using AWS: store routing rules in S3, publish change notifications via SNS. Services subscribe to the SNS topic and download the new configuration file from S3 when notified. Latency: two to five seconds. Cost: negligible. The trade-off is that you're building the validation and version management yourself rather than getting it from a config service or feature flag platform. For early-stage systems, S3 plus SNS is often the right starting point. For mature systems at scale, dedicated config infrastructure earns its operational cost.

## Rule Syntax and Structure

Routing rules need to be both human-readable and machine-parseable. YAML and JSON are the standard choices. YAML is easier for humans to write and maintain. JSON is easier for machines to validate and has fewer edge cases. A compromise: write rules in YAML, validate them by converting to JSON schema before publishing. The rule structure expresses the routing logic: if these conditions match, use this model, with these fallbacks, under these constraints.

A rule contains a segment definition, a primary model, a fallback list, and optional overrides. The segment definition specifies what traffic this rule applies to — which API endpoints, which customer tiers, which request characteristics. The primary model is where requests matching this segment route by default. The fallback list is the ordered sequence of models to try if the primary fails. Overrides allow temporary routing changes — reroute this specific customer to a different model, apply this rule only during specific hours, disable this model entirely for maintenance.

For a customer support system, a rule might say: for enterprise tier customers making requests to the ticket classification endpoint, route to GPT-5 as primary, fall back to Claude Opus 4.5 if GPT-5 fails, fall back to Claude Sonnet 4.5 if Opus fails, and return an error if all three fail. For free tier customers on the same endpoint, route to GPT-5-mini as primary, fall back to Claude Haiku 4.5, with no third fallback. The rule syntax makes this logic explicit and auditable.

Teams often start with flat rule files where each rule is independent. This works until you have fifty rules and realize that forty-five of them share the same fallback sequence. Then you need rule composition — define a fallback sequence once and reference it from multiple rules. Define a segment once and use it across multiple rules. The goal is to make common changes easy. If you need to swap your default fallback model across all segments, you should change one line, not fifty lines. Rule syntax should optimize for maintainability at scale.

## Version Control and Audit Trail

Every routing rule change must be versioned and logged. This is not optional governance overhead. This is operational necessity. When request patterns shift unexpectedly, when costs spike, when quality degrades — your first question is always "did we change routing recently?" If routing configuration changes are not versioned and auditable, you cannot answer this question reliably. You are debugging production with missing information.

Store routing rules in Git. Every change is a pull request with a descriptive commit message. The Git history becomes your audit trail. Who changed which rule, when, why. The pull request review becomes your approval process. No routing changes reach production without a second set of eyes reviewing the logic. Git also gives you free rollback — revert the commit, publish the previous version, routing returns to previous behavior. This is not theoretical infrastructure design. This is how you survive the incident where someone accidentally routes all traffic to the most expensive model and your daily inference bill jumps from four thousand dollars to sixty thousand dollars in six hours.

The configuration version should be embedded in the configuration itself as a monotonically increasing integer or a timestamp. When services read configuration, they compare the version to what they currently have loaded. If the version is newer, they reload. If the version is the same, they skip reload and save the CPU cost of parsing and validating rules. The version also appears in logs for every routing decision. When you're investigating why a particular request routed the way it did, the configuration version tells you exactly which rule set was active at that moment.

Some teams use semantic versioning for routing rules — major version for breaking changes like removing segments, minor version for new rules, patch version for adjustments to existing rules. This adds cognitive overhead for minimal benefit. A monotonic version number is simpler and sufficient. The Git commit hash can serve as the version if you want human-readable change tracking — just log the first eight characters of the commit that produced the configuration.

## Hot-Reload Mechanics

Hot-reload means replacing routing rules in a running service without restarting the service and without dropping in-flight requests. The naive implementation reads the new configuration file, parses it, replaces the in-memory rule set, and starts using the new rules for subsequent requests. This works until you have ten million requests per second and parsing configuration takes 400 milliseconds. During that 400 milliseconds, your routing service is blocked. Requests queue. Latency spikes. You've traded a slow deployment process for periodic latency spikes every time configuration changes.

The correct implementation is lock-free concurrent reload. The routing service maintains two copies of the rule set — the active set and the loading set. When a configuration change is detected, a background thread loads and parses the new configuration into the loading set. Request handling continues using the active set without interruption. Once the loading set is fully parsed and validated, the service atomically swaps pointers — the loading set becomes the active set, the old active set is marked for garbage collection after in-flight requests complete. Request routing is never blocked. The latency impact of configuration reload is zero.

For services that maintain routing state — like circuit breaker state or rate limit counters — the reload needs to transfer that state to the new rule set. If a circuit breaker was open under the old configuration, it should remain open under the new configuration unless the new configuration explicitly removes that model entirely. If a rate limiter had seen thirty requests in the current second, that count transfers to the new rule set. State transfer prevents configuration reload from accidentally clearing important runtime state.

Some teams implement phased reload — load the new configuration but don't activate it immediately. Let it sit in the loading set for thirty seconds while request handlers continue using the active set. Monitor for memory corruption, validation errors, unexpected rule parsing outcomes. If everything looks clean after thirty seconds, activate the new rules. If something looks wrong, discard the loading set and alert operators. This adds safety at the cost of thirty seconds of latency between configuration publish and activation.

## Validation Before Reload

Bad routing configuration has the same impact as bad code — it breaks production. But unlike code, configuration often lacks the same level of testing and validation before deployment. You need automated validation that prevents invalid configuration from reaching production services. The validation happens in two places: before configuration is published, and before configuration is activated by a service.

Pre-publish validation runs as part of your configuration deployment pipeline. When someone submits a pull request changing routing rules, CI runs a validation suite. Does the YAML parse correctly? Do all referenced models exist in your model inventory? Are segment definitions non-overlapping or properly prioritized? Do fallback chains avoid cycles? Are cost limits and latency thresholds within acceptable bounds? The validation suite fails the build if any check fails. This prevents invalid configuration from merging into the main branch and being published to production.

Pre-activation validation runs when a service loads new configuration. Even if pre-publish validation passed, runtime validation adds defense in depth. Services parse the configuration, validate rule syntax, check that referenced models are reachable, and simulate routing decisions for a sample of request patterns. If validation fails, the service discards the new configuration and continues using the existing rule set. It logs a validation failure alert so operators know that configuration could not be applied. This protects against scenarios like network partitions where a service might read a partially-written configuration file, or configuration corruption in transit.

Validation should include semantic checks, not just syntax checks. Syntax validation catches malformed YAML. Semantic validation catches rules that are technically parseable but operationally dangerous. A rule that routes all traffic to a model that costs twenty times more than your current primary — that's semantically questionable. A rule that has no fallback chain and will return errors if the primary model is unavailable — that's semantically risky. Semantic validation doesn't necessarily block configuration from being applied, but it generates warnings that require explicit acknowledgment. You can apply the expensive routing rule, but you have to confirm you understand the cost impact.

## Gradual Rule Rollout

Deploying a routing rule change to all traffic simultaneously is high-risk. If the rule contains an error, or if the rule's behavior differs from expectation, you've impacted one hundred percent of requests instantly. Gradual rollout applies the new rule to a small percentage of traffic first, monitors impact, and expands only if impact is acceptable. This is the same progressive delivery approach used for code deployments, applied to configuration changes.

The rollout mechanism requires a traffic splitting capability. When a service receives a request, it determines which configuration version to use for that request — the stable version or the canary version. Early in rollout, ninety-five percent of requests use the stable version, five percent use the canary. If canary metrics look good after ten minutes, expand to twenty percent canary. If canary error rates are higher than stable, halt rollout and investigate. If canary error rates match stable, continue expanding — fifty percent, then eighty percent, then one hundred percent. Total rollout time: thirty to sixty minutes for a typical rule change, depending on traffic volume and monitoring thresholds.

Traffic splitting can be random — assign each request randomly to stable or canary — or deterministic based on request characteristics. Deterministic splitting uses a hash of customer ID, session ID, or request ID to decide which version to use. This ensures that a particular customer sees consistent routing behavior throughout the rollout rather than flipping between stable and canary on every request. Deterministic splitting also makes debugging easier — if a customer reports an issue during rollout, you can check whether they were in the canary group.

Gradual rollout requires that your routing infrastructure supports multiple concurrent rule versions. Most teams implement this by tagging each rule set with a rollout stage — stable, canary — and maintaining both in memory. The request router checks the traffic split decision and selects the appropriate rule set. Once rollout completes and the canary version has been promoted to stable, the old stable version is discarded. The new stable version is now the only version in memory until the next rollout begins.

## Configuration Management Discipline

Hot-reloadable configuration is operationally powerful and operationally dangerous. You can fix routing issues in seconds. You can also break routing in seconds. The faster you can change configuration, the more important configuration management discipline becomes. Treat routing configuration with the same rigor you treat application code. Require reviews. Require tests. Require rollback plans.

A pattern that works well: routing configuration as infrastructure-as-code. Configuration files live in a Git repository. Changes go through pull requests with required reviewers. CI runs validation tests. Deployment uses a GitOps approach where the config service or S3 bucket automatically syncs with the main branch of the config repo. This combines version control, audit trail, automated validation, and deployment automation into one workflow. It also makes rollback trivial — revert the commit, push to main, config system syncs the old version.

Document each routing rule's purpose and rationale. A rule that says "route enterprise customers to GPT-5" is less useful than a rule that says "route enterprise customers to GPT-5 — required for contractual quality commitments, approved by product and finance teams on 2026-01-15, estimated cost impact plus twelve thousand dollars per month." When someone is reviewing a proposed change to that rule six months later, the context helps them understand impact. When someone is investigating a production issue, the rationale helps them understand intent.

Limit who can publish routing configuration changes. On small teams, all engineers might have access. On larger teams, restrict config publishing to infra and SRE teams, with product and engineering submitting config changes via pull requests that infra reviews and merges. This is not about trust. This is about blast radius. A developer who doesn't regularly work on routing infrastructure might not realize that a particular rule change will triple inference costs. An infra engineer who reviews routing changes daily will catch that before it ships.

The goal of hot-reloadable routing configuration is operational agility without operational chaos. You want to respond to production reality in seconds, not minutes or hours. You also want to prevent accidental misconfigurations from reaching production and causing expensive incidents. The architecture and the process work together to enable both.

Routing observability shows you where requests actually go and why, which becomes the foundation for optimizing routing strategy over time.
