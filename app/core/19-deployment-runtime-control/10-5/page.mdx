# 10.5 — Dark Launch: Enable Features for Zero Percent, Measure Cost and Latency

You can deploy code without enabling it. Dark launch lets you measure the infrastructure impact before any user sees the feature. The new model loads into memory, the new prompt template compiles, the new retrieval pipeline initializes—and then nothing happens. No user traffic reaches the new code. But you can see exactly what it would cost, how long it would take, and whether it would break under production load. When you do flip the feature flag to send real traffic, you already know the infrastructure will hold.

Dark launch is the discipline of deployment without exposure. It decouples the risk of deploying code from the risk of enabling functionality. Most teams conflate these two moments. They push new code and immediately route traffic to it, hoping everything works. Dark launch splits the process. First you deploy. Then you validate. Then you enable. If the validation reveals a problem, you redeploy without ever having affected a single user.

## The Dark Launch Concept: Deployed But Disabled

Dark launch means the code exists in production, but a feature flag keeps it dormant. The new model weights are loaded into the serving infrastructure. The new prompt template is registered in the template store. The new retrieval index is mounted and ready to query. But the routing layer sends zero percent of traffic to any of it. The old system continues handling all requests. The new system sits idle, consuming resources but producing no user-facing output.

This is not the same as staging or canary. Staging runs in a non-production environment with synthetic characteristics. Canary routes a small percentage of real traffic to new code. Dark launch routes zero percent of real traffic. It tests the new system's readiness without exposing users to its behavior. The distinction matters because you can measure infrastructure impact—latency, memory, CPU, cost—without worrying about output quality. If the infrastructure metrics are terrible, you never have to evaluate whether the responses are good.

The primary value is risk separation. Deploying code is a mechanical operation. Enabling a feature is a product decision. Dark launch lets engineering validate the mechanics before product evaluates the outcomes. If the deployment succeeds but infrastructure metrics are unacceptable, engineering can iterate without involving product, without writing incident reports, without explaining to users why the new feature was slow. The iteration happens in darkness. When the infrastructure is ready, the feature can be lit up with confidence.

## Measuring Without Exposure: Cost, Latency, and Error Rates

When a feature is dark, you can measure it by generating synthetic requests that mimic real traffic patterns. The synthetic requests are identical to production requests in structure and content, but they originate from the dark launch test harness rather than from users. The requests flow through the new code path. The system processes them, computes responses, logs metrics, and discards the output. No user ever sees the result. But you see the latency distribution, the token count, the memory footprint, and the error rate.

The metrics you collect during dark launch are the same metrics you will monitor during canary and full rollout. You want to know the median latency, the 95th percentile latency, and the 99th percentile latency. You want to know the average cost per request and the cost variability across different request types. You want to know whether the new system throws errors, runs out of memory, or times out under load. All of this can be measured with synthetic traffic before a single real user touches the system.

The synthetic traffic must be representative. If your production traffic is 60 percent short queries and 40 percent long documents, your dark launch traffic should match that distribution. If your production traffic includes multi-turn conversations, your synthetic traffic should too. If production requests include edge cases like empty inputs, malformed JSON, or unusually long prompts, inject those into the dark launch test set. The closer the synthetic traffic resembles real traffic, the more confident you can be that the dark launch metrics predict production behavior.

Dark launch is also when you discover whether the new system can handle the request rate. If your production system serves 200 requests per second, your dark launch should generate at least that much load. If the new code can only handle 80 requests per second before latency spikes, you learn that before enabling the feature. You can optimize, scale the infrastructure, or decide the feature is not ready. All of this happens while users continue using the old system without interruption.

## Synthetic Load for Dark Features: Generating Realistic Test Traffic

Generating synthetic load requires either replaying historical traffic or synthesizing new requests that match production patterns. Replay is the simplest approach. Capture a sample of real requests from the previous 24 hours, strip any personally identifiable information, and send those requests through the dark system. The requests are real user queries, so they reflect actual usage patterns. The responses are computed but discarded, so no user sees incorrect or unexpected output.

Replay has limits. If the new feature changes the input format or expects additional context that the old system did not require, historical requests may not exercise the new code correctly. In that case, you need synthesized traffic. Synthesized traffic is generated by a script or tool that understands the new input format and produces valid requests. The script can vary the request parameters to cover the full range of expected inputs. It can generate short and long prompts, simple and complex queries, common and edge-case scenarios.

The synthetic load should run continuously during the dark launch period. A one-time load test is useful, but it does not reveal how the system behaves over hours or days. Continuous load reveals memory leaks, cache warming effects, and performance degradation under sustained use. If latency starts at 200 milliseconds and climbs to 800 milliseconds after six hours, you have a problem. If memory usage grows linearly until the service crashes after 12 hours, you have a bigger problem. These issues do not appear in short load tests. They appear when the system runs long enough for the defect to accumulate.

The load does not need to match full production volume if your infrastructure is not yet scaled to handle it. You can run dark launch at ten percent of production volume and extrapolate the metrics. If ten percent volume costs 50 dollars per hour, full volume will cost 500 dollars per hour. If median latency at ten percent volume is 180 milliseconds, you can model what happens at 100 percent based on your scaling characteristics. The key is running enough load, for long enough, to surface issues that only appear under sustained use.

## Infrastructure Validation: Does the New Code Handle Production Load?

Dark launch answers a simple question: will this code survive production? Not "will users like it" or "is the output correct," but "will the infrastructure hold." The new model might produce perfect responses, but if it requires four GPUs per request and your infrastructure has two GPUs per node, the deployment will fail. Dark launch reveals that before you route traffic.

Infrastructure validation includes compute resource usage, memory footprint, disk I/O, network bandwidth, and dependency availability. The new model might load 40 gigabytes of weights into GPU memory, leaving no room for batch processing. The new retrieval pipeline might hammer the vector database with 50 queries per request, overwhelming the database connection pool. The new prompt template might generate 8,000-token outputs, doubling your token costs. All of these issues are detectable during dark launch with zero user impact.

Dark launch also validates cold start behavior. When the new code deploys, it starts cold. Caches are empty. Model weights are loading from disk. Connection pools are initializing. The first few requests are always slower than steady-state requests. Dark launch lets you warm the system before enabling it for users. You send synthetic traffic for 10 minutes, fill the caches, complete the initialization, and observe when latency stabilizes. Once the system is warm, you enable the feature flag and real traffic flows into a system that is already operating at steady-state performance.

The validation period should match your deployment frequency. If you deploy weekly, you can afford a 24-hour dark launch. If you deploy hourly, you need a 10-minute dark launch. The dark launch duration is long enough to detect issues but short enough to maintain development velocity. A dark launch that runs for a week before enabling a feature slows down iteration. A dark launch that runs for two minutes misses issues that take 10 minutes to appear. The right balance depends on your system's complexity and your tolerance for production incidents.

## Dark Launch for Models: Loading Weights and Warming Caches

When the new feature is a model change, dark launch tests whether the new model can be served at all. The model weights must be downloaded from storage, loaded into GPU memory, and initialized. The serving infrastructure must allocate the right GPU type, configure the batch size, and set the timeout parameters. If any of these steps fails, you learn during dark launch, not during canary.

Some models do not fit in the GPU memory configuration you have. A model that requires 80 gigabytes of memory will not run on a node with two 40-gigabyte GPUs unless you configure tensor parallelism correctly. Dark launch is when you discover that the tensor parallelism configuration is wrong, the model does not load, or the first request times out after 60 seconds. You fix the configuration, redeploy, and test again. Users never see a timeout.

Warming caches is especially important for models with large context windows. The first request that uses a 128,000-token context window takes much longer than subsequent requests because the key-value cache must be populated. Dark launch sends enough synthetic traffic to populate the cache, so when real traffic arrives, the cache is already warm. The first real user gets the same fast experience as the hundredth user.

Dark launch also tests model fallback behavior. If the new model is a fine-tuned version and the fine-tuned weights fail to load, does the system fall back to the base model, or does it return an error? If the fallback works, dark launch proves it. If it does not, you fix it before enabling the feature. Fallback logic that has never been tested in production is fallback logic that will fail when you need it most.

## Gradual Light-Up: From Dark to Canary to Full Rollout

Dark launch is not the final state. It is the first stage of a multi-stage rollout. After the dark launch validates infrastructure readiness, you move to canary. Canary routes one percent of real traffic to the new system. If the canary metrics match the dark launch metrics, you increase to five percent, then ten percent, then 50 percent, then 100 percent. Each stage is a validation gate. If metrics degrade at any stage, you halt the rollout and investigate.

The transition from dark to canary is the highest-risk moment. During dark launch, only synthetic traffic touched the new system. During canary, real users see the new behavior. If the new system produces incorrect outputs, lower-quality responses, or unexpected errors, real users are affected. The canary percentage should be small enough that the impact is contained but large enough that you collect statistically significant metrics. One percent of traffic at a system serving one million requests per day is 10,000 requests. That is enough to detect most problems.

The rollout speed depends on confidence. If the dark launch ran for 48 hours with perfect metrics, you can roll out faster. If the dark launch revealed memory issues that you patched at the last minute, roll out slower. If the new system is a minor prompt change, roll out faster. If the new system is a different model architecture, roll out slower. The rollout cadence is a function of risk, not a fixed schedule. High-risk changes earn slower rollouts. Low-risk changes can move from canary to full in minutes.

## Dark Launch Duration: How Long to Keep Features Dark

The right dark launch duration depends on what you are testing. If you are testing cold start behavior and cache warming, ten minutes is enough. If you are testing for memory leaks and performance degradation over time, 24 hours is better. If you are testing cost at scale, you need enough time to accumulate representative usage patterns across different times of day and different request types.

For most systems, a dark launch of four to 12 hours is sufficient. Four hours covers multiple request patterns and reveals issues that do not appear in the first few minutes. Twelve hours covers a full business cycle and captures both peak and off-peak behavior. If your system has weekly usage patterns, a longer dark launch is justified. If your system has uniform traffic, a shorter dark launch is fine.

The cost of keeping a feature dark is the cost of running the infrastructure without producing value. If the dark system consumes 100 dollars per day in compute costs, every day of dark launch costs 100 dollars. That cost is acceptable if it prevents a production incident that would cost 10,000 dollars in customer impact and engineering time. It is not acceptable if the feature is low-risk and the dark launch is just procedural overhead. Match the dark launch cost to the risk you are mitigating.

## Monitoring Dark Features: What to Watch When Nobody Is Using It Yet

During dark launch, you monitor the same metrics you will monitor during canary and full rollout: latency, error rate, resource usage, and cost. The difference is that all the metrics come from synthetic traffic. The monitoring dashboards should look identical to production dashboards. If the production dashboard shows median latency, 95th percentile latency, and error rate, the dark launch dashboard should show the same metrics. This makes it easy to compare dark launch performance to production performance and to spot deviations.

You also monitor the health of the synthetic traffic generator. If the generator stops sending requests, you learn nothing. If the generator sends requests at the wrong rate or with the wrong parameters, the metrics are misleading. The monitoring system should alert if the synthetic traffic rate drops below the expected threshold or if the generator crashes. A silent dark launch is a failed dark launch.

Resource usage metrics are especially important during dark launch. Memory usage should be stable over time. CPU usage should match the request rate. GPU utilization should match the model's compute requirements. If memory usage grows over time, you have a memory leak. If CPU usage is higher than expected, the new code is less efficient than you thought. If GPU utilization is low, the serving infrastructure is misconfigured and leaving performance on the table. All of these issues are fixable before canary.

## When Dark Launch Is Overkill: Simple Changes That Do Not Need Validation

Not every change needs a dark launch. If you are fixing a typo in a prompt template, deploying the fix and enabling it immediately is fine. If you are adjusting a feature flag configuration that does not change code paths, dark launch adds no value. If you are deploying a change that has been running in canary for a week and you are now moving from 50 percent to 100 percent, dark launch is unnecessary. The canary already validated the change.

Dark launch is valuable when the infrastructure impact is unknown. A new model with different resource requirements, a new retrieval pipeline with different latency characteristics, a new feature that introduces a new dependency—these changes justify dark launch. A change that affects code paths you have never tested under production load justifies dark launch. A change that could increase costs by an unknown amount justifies dark launch. If you already know the infrastructure impact from previous canary testing, skip the dark launch and go straight to expanding the canary.

The decision rule is simple. If deploying the change introduces uncertainty about infrastructure behavior, use dark launch. If deploying the change is mechanically identical to previous deployments and the only question is output quality, skip dark launch and go to canary. Dark launch is a tool for reducing infrastructure risk, not a mandatory step in every deployment. Use it when the risk justifies the cost.

Once dark launch proves the infrastructure can handle the new feature, the next step is staged rollout with validation gates—explicit checkpoints that must pass before advancing to the next percentage.
