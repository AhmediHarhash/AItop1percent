# 9.7 — Configuration Separation: Prompts, System Messages, and Parameters

A prompt is not one thing. It is system message, user template, parameters, and examples—each with different change frequencies and different owners. The system message defines how the model should behave: its role, its constraints, its tone. The user template structures the input: where the user's question goes, what context gets included, how to format multi-turn conversations. The generation parameters control the model's randomness, verbosity, and stopping conditions: temperature, max tokens, top-p, presence penalty. The few-shot examples demonstrate the kind of output you want. The tool definitions describe what functions the model can call and how to invoke them.

Treating all of these as a single monolithic prompt is a mistake. They change at different rates. The system message might be stable for months. The user template might change weekly as you iterate on input formatting. The generation parameters might be tuned daily during A/B tests. The few-shot examples might be updated hourly as you discover new edge cases. Bundling them together means every small change to one component requires redeploying the entire prompt, and that creates unnecessary risk, slows iteration, and makes rollback harder.

## The Configuration Decomposition: Breaking the Prompt into Separate Concerns

The first step in configuration separation is identifying the components. Not every application uses all components, but most use at least three: system message, user template, and generation parameters. Start by auditing your current prompts and asking: which parts of this prompt serve different purposes? Which parts are edited by different people? Which parts change at different frequencies? The answers define your component boundaries.

The system message is the instructions you give to the model about its identity, capabilities, and behavior. It might tell the model that it is a customer support assistant, that it should respond in a professional tone, that it should never discuss politics, that it should cite sources for factual claims, that it should ask clarifying questions when the user's intent is unclear. The system message is the most stable component. It encodes your product requirements and brand voice, and it should not change unless your product strategy changes.

The user template is the structure you use to present the user's input to the model. It might include the user's message, conversation history, retrieved context from a RAG system, user profile information, current date and time, and instructions about how to use that information. The user template changes more frequently than the system message because you iterate on what context to include, how to format it, and how to handle edge cases like very long messages or missing context.

The generation parameters control how the model samples from its output distribution. Temperature determines randomness: low temperature for deterministic, factual responses, high temperature for creative, varied outputs. Max tokens sets the upper limit on response length. Top-p controls diversity by limiting sampling to the most probable tokens. Presence penalty and frequency penalty discourage repetition. These parameters are tuned frequently during experiments and A/B tests, and they often vary by use case: one set of parameters for customer support, a different set for creative writing, another for code generation.

The few-shot examples are demonstrations of input-output pairs that show the model what good responses look like. If you want the model to always structure answers as numbered lists, you provide examples of numbered lists. If you want the model to extract structured data from text, you provide examples of extraction tasks with correct outputs. Few-shot examples are updated as you discover new patterns, new failure modes, or better ways to demonstrate the task. They change more often than the system message but less often than generation parameters.

The tool definitions describe external functions the model can invoke: search tools, calculation tools, database queries, API calls. Each tool has a name, description, and parameter schema. Tool definitions change when you add new tools, deprecate old ones, or modify tool behavior. They are versioned separately from the prompt because the tool infrastructure might change independently of the prompt content.

## System Messages: The Instructions That Define Model Behavior

The system message is where you encode your product's core requirements. It is the contract between your application and the model: here is what I need you to do, here are the boundaries you cannot cross, here is how to handle ambiguity. A well-designed system message is specific, unambiguous, and enforceable by the model.

Specificity in system messages means explicit instructions, not vague guidance. "Be helpful" is vague. "Answer user questions accurately, cite sources for factual claims, admit when you do not know something, and escalate to a human agent if the user is frustrated or requests it" is specific. The more specific your system message, the more consistently the model will follow it. Vague instructions are interpreted differently by the model across different inputs, leading to unpredictable behavior.

The system message should define the model's role and scope. If the model is a customer support assistant, the system message should say so explicitly: "You are a customer support assistant for Acme Corp. Your role is to help users resolve issues with their accounts, answer product questions, and escalate complex issues to human agents." This role definition sets expectations for the model's capabilities and limits. It prevents the model from attempting tasks outside its scope, like giving legal advice when it is a product support bot.

Behavioral constraints in the system message define what the model must not do. Never provide medical advice. Never discuss politics. Never share user data with other users. Never generate code that accesses file systems or networks without explicit user permission. These constraints are non-negotiable, and they should be stated as absolute rules, not suggestions. The model will occasionally violate these constraints despite clear instructions, so constraints must be enforced with additional safeguards—output filtering, red-teaming, human review—but the system message is the first line of defense.

Tone and style guidance belongs in the system message. If your brand voice is formal and professional, the system message should instruct the model to use formal language, avoid slang, and maintain a professional tone. If your brand voice is casual and friendly, the system message should encourage conversational language and warmth. Tone consistency matters for brand perception, and the system message is where you enforce it.

The system message is owned by product and trust-and-safety teams, not by individual prompt engineers. Changes to the system message require cross-functional review because they affect the entire product experience. A poorly chosen system message can cause the model to refuse legitimate requests, generate inappropriate content, or behave in ways that contradict your brand values. Treat system message changes with the same rigor as product requirement changes, because that is what they are.

## User Templates: The Structure for User Inputs

The user template defines how to format the user's message and any additional context before sending it to the model. A simple user template might be just the user's message: "the user message placeholder." A complex user template might include conversation history, user profile data, current date and time, retrieved documents, and instructions about how to use each piece of context.

Template variables are placeholders that get replaced with actual values at runtime. Common template variables include "the user message placeholder," "the conversation history placeholder," "the retrieved context placeholder," "the user name placeholder," and "the current date placeholder." The user template defines where these variables appear and how they are formatted. If the user's message is always preceded by "User:" and the model's response is always preceded by "Assistant:", the user template enforces that structure.

Context ordering in the user template affects model performance. If you place retrieved documents before the user's question, the model has context before seeing what the question is, and it might struggle to identify which parts of the context are relevant. If you place retrieved documents after the user's question, the model knows what it is looking for before it sees the context, and retrieval relevance improves. Experimentation is required to find the optimal ordering for your use case.

Handling long context is a user template concern. If the combined length of conversation history, retrieved documents, and the user's message exceeds the model's context window, the user template must define truncation or summarization strategies. Do you drop the oldest conversation turns? Do you summarize the conversation history? Do you truncate retrieved documents? The user template logic implements these decisions, and the decisions affect both model performance and user experience.

The user template is iterated by prompt engineers and product teams. Changes to the user template are frequent during the early stages of a product, less frequent as the product matures. User template changes are lower risk than system message changes because they affect how information is presented, not what the model is allowed to do. But they still require testing: a poorly formatted user template can confuse the model, degrade quality, or cause parsing errors.

## Generation Parameters: Temperature, Max Tokens, and Sampling Controls

Generation parameters control how the model generates text. They are numerical settings that affect output quality, diversity, length, and randomness. Tuning these parameters is part of prompt engineering, but they should be separated from the prompt text because they are tuned independently and at different cadences.

Temperature controls randomness. A temperature of 0.0 produces deterministic outputs: the model always picks the most probable token at each step, resulting in the same response for the same input every time. A temperature of 1.0 samples from the full probability distribution, producing varied and creative outputs. For factual tasks like answering support questions or extracting structured data, low temperature is preferred. For creative tasks like story generation or brainstorming, higher temperature is appropriate. Most systems use temperatures between 0.0 and 0.7, rarely above 1.0.

Max tokens sets the maximum number of tokens the model can generate in a single response. If the model reaches the max token limit, it stops mid-sentence, which creates a poor user experience. Setting max tokens too high wastes tokens and increases costs if the model generates unnecessarily long responses. The optimal max tokens value depends on your use case: 150 tokens for short answers, 500 tokens for medium-length explanations, 2000 tokens for long-form content. If users frequently hit the max token limit, increase it. If responses are consistently much shorter than the limit, decrease it to reduce costs.

Top-p, also called nucleus sampling, controls diversity by limiting the set of tokens the model can sample from. A top-p value of 0.9 means the model samples only from the smallest set of tokens whose cumulative probability is 90 percent or higher. This excludes low-probability tokens that are unlikely but possible, reducing the chance of nonsensical outputs while maintaining diversity. Top-p is often more effective than temperature for controlling output quality, and many practitioners use a combination: low temperature with moderate top-p for controlled but varied outputs.

Presence penalty and frequency penalty discourage the model from repeating itself. Presence penalty applies a fixed penalty to any token that has already appeared in the output, regardless of how many times it appeared. Frequency penalty applies a penalty proportional to how often the token has appeared. These penalties are useful for creative tasks where repetition is undesirable, but they can harm quality in factual tasks where repetition of key terms is necessary for clarity. Use presence and frequency penalties sparingly, starting with small values like 0.1 or 0.2, and increase only if repetition is a measurable problem.

Generation parameters are tuned through experimentation. You run A/B tests with different parameter values, measure the impact on quality, cost, and user engagement, and select the values that optimize your target metrics. Parameter tuning happens more frequently than prompt text changes, especially during the early stages of a product when you are still discovering what works. Separating parameters from prompt text allows you to iterate on parameters without touching the prompt, reducing deployment risk.

## Few-Shot Examples: Demonstrations That Guide Model Behavior

Few-shot examples are input-output pairs included in the prompt to demonstrate the task. If you want the model to extract structured data from unstructured text, you provide examples of unstructured text and the corresponding structured output. If you want the model to answer questions in a specific format, you provide examples of questions and correctly formatted answers. The model learns from these examples and applies the pattern to new inputs.

The number of examples matters. Zero-shot prompts provide no examples; the model relies entirely on the system message and user template. One-shot prompts provide a single example. Few-shot prompts provide two to five examples. Many-shot prompts provide dozens or hundreds of examples, though this is rare due to context length limits. For most tasks, two to four examples are sufficient. More examples improve consistency but consume context length and increase latency.

Example selection is critical. The examples should cover the range of inputs the model will encounter and demonstrate the correct output format, tone, and level of detail. If your task includes edge cases—unusual inputs, ambiguous questions, error conditions—include examples of those cases. If the model frequently makes a particular mistake, include an example that demonstrates the correct behavior for that case. Bad examples teach the model bad patterns, so example quality is more important than example quantity.

Example diversity prevents the model from overfitting to a narrow pattern. If all your examples are about the same topic, the model might learn topic-specific patterns instead of task-general patterns. If all your examples are short, the model might struggle with long inputs. If all your examples are simple, the model might fail on complex inputs. Vary your examples across topics, lengths, and difficulty levels to teach a robust pattern.

Few-shot examples are updated as you discover new failure modes. If users report that the model frequently mishandles a particular input type, add an example that demonstrates the correct handling. If evaluation reveals a quality gap on a specific task variant, add an example for that variant. Few-shot examples are living documentation: they evolve as your understanding of the task improves.

Few-shot examples are owned by prompt engineers and domain experts. Product teams define what good outputs look like, domain experts provide examples from their field, and prompt engineers structure the examples for maximum model performance. Changes to few-shot examples require validation: run the new examples through your eval suite to confirm they improve quality without harming other aspects of performance.

## Tool Definitions: What Tools Are Available and How to Use Them

Tool definitions describe the external functions the model can invoke during generation. Each tool has a name, a description, and a parameter schema. The name is a short identifier, like "search" or "calculate" or "get_weather." The description explains what the tool does and when to use it: "Search the knowledge base for relevant documents given a query." The parameter schema defines what inputs the tool expects: a query string, a filter object, a maximum result count.

Tool definitions follow a standard format, typically JSON Schema for parameters. The schema specifies parameter names, types, whether they are required or optional, and any constraints like minimum or maximum values. This schema is used by the model to generate tool invocations and by your application to validate that the model's tool calls are well-formed before executing them.

Tool descriptions must be clear and specific. The model uses the description to decide when to call the tool, so vague descriptions lead to incorrect tool use. "Search for information" is vague. "Search the internal knowledge base for documents relevant to the user's question. Use this tool when the user asks a factual question that requires up-to-date information not available in your training data" is specific. The description should explain both what the tool does and when it is appropriate to use it.

Tool definitions change when you add, remove, or modify tools. Adding a new tool requires adding its definition to the prompt and ensuring the model understands when to use it. Removing a tool requires removing its definition and updating any prompt instructions that reference it. Modifying a tool—changing its parameters, its behavior, or its output format—requires updating its definition and revalidating that the model uses it correctly.

Tool versioning is necessary when tools evolve. If you change a tool's parameter schema in a backward-incompatible way, prompts using the old schema will break. You can maintain multiple tool versions simultaneously, with different names or version suffixes, and migrate prompts to the new version gradually. Alternatively, you can make tool changes backward-compatible by adding new optional parameters instead of changing required ones.

Tool definitions are owned by the engineering team that maintains the tools, not by prompt engineers. This separation of ownership is important: the tool team understands the tool's capabilities and limitations, and they are responsible for documenting those in the tool definition. Prompt engineers consume the tool definitions and write prompts that use the tools correctly. When tool behavior changes, the tool team updates the definition, and prompt engineers update the prompts if necessary.

## Change Frequency Analysis: Which Components Change Often vs Rarely

Different prompt components have different change frequencies, and recognizing these differences allows you to optimize your deployment process for speed and safety. The system message is the most stable component, changing only when product requirements, compliance policies, or brand guidelines change. You might update the system message once per quarter or less. Because it changes rarely and has high impact, system message changes require thorough review, extensive testing, and careful rollout.

The user template changes more frequently, perhaps once per week or once per month. User template changes are driven by iterations on what context to include, how to format inputs, and how to handle edge cases. These changes are lower risk than system message changes because they affect presentation, not behavior. User template changes should still be tested, but the approval process can be lighter, and rollout can be faster.

Generation parameters are tuned continuously, especially during the early life of a product. You might run a new A/B test every few days, tweaking temperature, max tokens, or top-p to optimize for quality, cost, or engagement. Parameter changes are low risk if you stay within reasonable bounds—temperature between 0.0 and 1.0, max tokens within an order of magnitude of your baseline. Parameter tuning should be fast and iterative, with automated testing and immediate rollout for small changes.

Few-shot examples change frequently during the learning phase of a product, as you discover what patterns the model struggles with and what examples help. Once the product matures and the example set stabilizes, changes become rare. Few-shot example changes are medium risk: bad examples degrade quality, but the impact is usually localized to specific input patterns rather than affecting the entire system.

Tool definitions change when the tool infrastructure changes. If you add a new feature that requires a new tool, or if you deprecate an old tool, the tool definitions must be updated. Tool definition changes are medium-to-high risk because they can cause the model to invoke tools incorrectly, leading to errors, poor user experience, or security issues. Tool changes require coordination between the tool team and the prompt team, and they require thorough testing.

## Configuration Ownership: Who Controls Each Component

Each prompt component has a natural owner based on the expertise required to manage it. The system message is owned by product management and trust-and-safety teams. They define the product's behavior, tone, and constraints, and they are accountable for ensuring the model behaves in ways that align with company values and regulatory requirements. System message changes go through product review, legal review if necessary, and trust-and-safety review.

The user template is co-owned by prompt engineers and product teams. Prompt engineers design the template structure and optimize it for model performance. Product teams define what context to include based on user needs and product strategy. User template changes require agreement between these teams, but the approval process is lighter than for system messages.

Generation parameters are owned by prompt engineers and data scientists. They run experiments, analyze results, and tune parameters to optimize metrics. Parameter changes are technical decisions based on data, not product or policy decisions, so they do not require cross-functional approval. However, they should still be documented and logged, and major parameter changes should be validated with A/B tests.

Few-shot examples are owned by domain experts and prompt engineers. Domain experts provide examples of correct task execution, and prompt engineers format those examples for inclusion in the prompt. Example changes require validation but not formal approval, unless the examples touch on sensitive topics that require trust-and-safety review.

Tool definitions are owned by the engineering team that builds and maintains the tools. Prompt engineers consume tool definitions but do not create them. Tool definition changes require coordination between the tool team and the prompt team to ensure compatibility and correctness.

Clear ownership prevents conflicts and delays. If everyone owns the prompt, no one is accountable, and changes become slow and contentious. If each component has a clear owner, changes move quickly because the owner has the authority to make decisions without requiring consensus from the entire organization.

## Configuration Composition at Runtime: Assembling the Full Prompt from Components

At request time, the application assembles the full prompt by combining components: system message, user template with variables filled in, few-shot examples, tool definitions, and generation parameters. This composition happens in code, and the logic must ensure that all components are compatible and correctly ordered.

The composition order matters. Most models expect the system message first, followed by examples, followed by the user's actual input. If you place the user input before the examples, the model might not apply the examples correctly. If you place the system message after the user input, the model might ignore the system constraints. Follow the model's documented prompt structure, and test that your composition logic produces the expected order.

Variable substitution happens during composition. The user template contains placeholders like "the user message placeholder" or "the retrieved context placeholder." The application replaces these placeholders with actual values: the user's message, the retrieved documents, the conversation history. Variable substitution must be safe: it should escape any special characters that might break the prompt structure, and it should handle missing variables gracefully by either providing a default value or omitting the section.

Component versioning ensures compatibility. If your system message is at version 3, your user template is at version 7, and your few-shot examples are at version 2, the composition logic must ensure that these versions are compatible. Incompatibility might mean the user template references a tool that the current tool definitions do not include, or the examples assume a system message constraint that has been removed. Version compatibility checking catches these issues before they cause runtime errors.

Composition logic is testable. You can write unit tests that verify the composition function produces the expected prompt structure given known inputs. These tests catch regressions when you change composition logic or when you add new components. Composition bugs are insidious because they often manifest as subtle quality degradation rather than obvious errors, so automated testing is essential.

## Version Compatibility: Ensuring Components Work Together

Version compatibility is the guarantee that all prompt components active at the same time are designed to work together. Incompatibilities arise when one component references assumptions or features from another component that have changed or been removed. If your system message tells the model to cite sources, but your tool definitions do not include a citation tool, the model will fail to follow the system message, and users will notice.

Compatibility testing is automated. When you propose a new version of any component, the system runs compatibility checks against the current versions of all other components. These checks look for references to removed tools, assumptions about system constraints that no longer exist, and mismatches in expected output formats. If a compatibility issue is detected, the new version is rejected, or a warning is issued that requires manual review.

Backward compatibility is preferred when possible. If you need to change a tool's parameters, add new optional parameters instead of changing required parameters. If you need to change the user template, ensure the new template still works with existing system messages. Backward-compatible changes allow you to update components independently without requiring coordinated updates across all components.

Breaking changes require coordinated updates. If you must make a backward-incompatible change to a component, you need to update all dependent components at the same time. This coordination is complex and risky, so breaking changes should be rare and planned carefully. Document the breaking change, identify all affected components, update them in a staging environment, test the updated configuration, and deploy all changes together.

Version pinning for compatibility allows you to specify which versions of other components a given component is compatible with. The system message might specify that it requires user template version 5 or higher and tool definitions version 3 or higher. This metadata allows the system to enforce compatibility automatically and to warn engineers when they try to deploy incompatible component versions.

The goal of configuration separation is to make prompt changes fast, safe, and independent. By decomposing the monolithic prompt into components with separate ownership, change frequencies, and version control, you enable rapid iteration without sacrificing safety or coherence. The next step is implementing rollback mechanisms that allow instant reversion to previous configurations when problems arise.

