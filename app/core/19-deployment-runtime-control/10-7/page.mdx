# 10.7 — Progressive Delivery: Combining Canary, Flags, and Automation

Progressive delivery is not one technique. It is the combination of canary deployment, feature flags, and automated analysis—orchestrated to maximize safety while minimizing time to full rollout. The deployment advances in small increments. Each increment is validated automatically. If validation passes, the next increment begins. If validation fails, the deployment halts or rolls back. The entire process runs without human intervention, from one percent canary to 100 percent production, supervised by automated analysis and controlled by feature flags.

Progressive delivery is the production deployment equivalent of continuous integration. Continuous integration runs tests automatically on every commit. Progressive delivery runs validation automatically on every rollout stage. The developer commits code. The CI system builds, tests, and packages it. The progressive delivery system deploys to one percent, validates, deploys to five percent, validates, deploys to 25 percent, validates, and advances to full rollout if every stage passes. The developer's job is to write correct code. The progressive delivery system's job is to deploy it safely.

## The Progressive Delivery Philosophy: Small, Reversible Steps

The philosophy is simple. Large changes are risky. Small changes are safer. Irreversible changes are dangerous. Reversible changes are acceptable. Progressive delivery splits large changes into small increments and ensures every increment is reversible. Instead of deploying from zero to 100 percent in one step, deploy from zero to one percent, then to five percent, then to ten percent, then to 25 percent, then to 50 percent, then to 100 percent. Each step is small enough that if it fails, the blast radius is limited. Each step is reversible. If the five percent deployment fails, roll back to one percent. If the 25 percent deployment fails, roll back to ten percent.

Reversibility requires feature flags. The deployment does not change routing rules or replace binaries. It updates a configuration. The feature flag controls what percentage of traffic sees the new behavior. The old code and the new code both exist in production simultaneously. The flag routes traffic between them. If the new code fails, change the flag and all traffic returns to the old code. The rollback is instantaneous. No redeployment, no service restart, no downtime.

The small steps also reduce the cognitive load on the team. A deployment that goes from zero to 100 percent in one step requires the team to watch dashboards constantly, ready to react if something goes wrong. A progressive delivery deployment runs automatically. The team is notified when the deployment advances to each stage. They review the metrics. If the metrics are fine, they move on. If the metrics are concerning, they halt the deployment. The deployment does not require constant attention. It requires periodic review.

## Components: Canary Infrastructure, Feature Flags, Metrics, and Analysis

Progressive delivery requires four components working together. The first is canary infrastructure—the ability to route a percentage of traffic to the new system while the old system handles the rest. The second is a feature flag system that controls the routing percentage and can be updated instantly. The third is a metrics pipeline that collects latency, error rates, resource usage, and quality metrics in real time. The fourth is an automated analysis system that evaluates the metrics, compares them to thresholds, and decides whether to advance, halt, or roll back.

The canary infrastructure is typically implemented at the load balancer or service mesh layer. The load balancer receives a routing rule: send five percent of traffic to the new version and 95 percent to the old version. The service mesh enforces the rule at the network level. Every request is routed according to the percentage. The routing is stateless. Each request is independent. If the percentage changes, the next request reflects the new percentage. There is no session affinity, no gradual transition. The percentage is the percentage.

The feature flag system is the control plane. It stores the current rollout percentage for every feature. The canary infrastructure queries the feature flag system before routing each request. If the flag says five percent, the load balancer sends five percent of requests to the new version. If the flag updates to ten percent, the load balancer immediately sends ten percent of requests to the new version. The feature flag system is the single source of truth for rollout state. Engineers do not update load balancer configurations manually. They update the feature flag, and the infrastructure reads the flag.

The metrics pipeline collects data from both the old and new versions. Latency, error rate, throughput, and cost metrics are tagged with the version. The metrics pipeline aggregates the data and exposes it to the analysis system. The analysis system compares the new version's metrics to the old version's metrics. If the new version's error rate is higher, the analysis system flags it. If the new version's latency is significantly worse, the analysis system flags it. The flags trigger alerts or automatic rollback depending on the severity.

## Orchestration: How the Pieces Work Together

The orchestration layer ties the components together. It defines the rollout stages, the validation criteria for each stage, and the actions to take when validation passes or fails. A typical orchestration looks like this. Deploy the new version. Set the feature flag to one percent. Wait ten minutes. Collect metrics. Evaluate metrics against thresholds. If metrics pass, set the feature flag to five percent. Wait 30 minutes. Collect metrics. Evaluate metrics. If metrics pass, set the feature flag to ten percent. Repeat until the flag reaches 100 percent.

The orchestration layer also handles rollback. If the metrics fail at any stage, the orchestration layer sets the feature flag back to the previous percentage or to zero. If the five percent stage fails, the flag reverts to one percent. If one percent also shows problems, the flag reverts to zero and all traffic returns to the old version. The rollback is automatic. No human intervention required. The orchestration detects the failure, makes the decision, and executes the rollback.

The orchestration layer can also implement manual approval gates. After the canary reaches ten percent, pause and wait for a human to review the metrics and approve advancement. The human reviews a dashboard, examines the metrics, and clicks a button to continue or abort. If they approve, the orchestration resumes. If they abort, the orchestration rolls back. The manual gate is a break in the automation, but it provides a checkpoint for high-risk deployments where human judgment is necessary.

Orchestration tools like Argo Rollouts, Flagger, and Spinnaker provide pre-built orchestration logic. You define the stages, thresholds, and rollback rules in a configuration file. The tool executes the orchestration automatically. You do not write the rollout logic from scratch. You configure the tool and let it run. These tools integrate with Kubernetes, Istio, and other cloud-native infrastructure, making them easy to adopt if your deployment stack is already built on those platforms.

## Automated Rollout: Advancing Percentages When Metrics Pass

Automated rollout removes human latency from the deployment process. The orchestration system advances the rollout percentage automatically when validation passes. The developer deploys the new version, starts the progressive delivery process, and walks away. The system handles the rest. Hours later, the rollout is complete. The developer receives a notification that the deployment reached 100 percent. They review the metrics, confirm everything looks good, and close the deployment ticket.

The automation depends on well-calibrated thresholds. If the thresholds are too strict, legitimate deployments will fail validation and require manual intervention. If the thresholds are too lax, bad deployments will pass validation and reach production. Calibrating thresholds requires running several deployments manually, collecting metrics at each stage, and identifying the normal range of variation. The thresholds should allow for minor, expected variation but catch significant degradation.

The automated rollout should include a maximum duration for each stage. If the one percent canary runs for ten minutes and the metrics are still being collected, do not wait indefinitely. Set a maximum duration of 15 minutes. If the metrics are not ready by then, halt the deployment and alert the team. If the metrics pipeline is delayed or broken, the deployment should not advance blindly. The maximum duration is a safety net that prevents the automation from proceeding without data.

The automation should also include a maximum total rollout time. If the deployment has been running for 24 hours and is still at 50 percent, something is wrong. Either the metrics are failing repeatedly, or the orchestration is stuck. Set a maximum total rollout time of 48 hours. If the rollout does not complete within that window, halt it and alert the team. The automated rollout is designed to complete quickly. If it is taking an unusually long time, human investigation is required.

## Automated Rollback: Reverting When Metrics Fail

Automated rollback is the most valuable feature of progressive delivery. When the metrics fail validation at any stage, the orchestration system reverts the feature flag to the previous percentage or to zero. The rollback happens within seconds. No human intervention. No incident review. No manual redeployment. The system detects the failure, decides to roll back, updates the feature flag, and sends an alert. The bad deployment is contained before it affects more users.

The rollback criteria should be unambiguous. Error rate above one percent is a rollback. P95 latency above baseline plus 200 milliseconds is a rollback. Memory usage above 90 percent is a rollback. The criteria are defined before the deployment starts. If the criteria are met, the rollback is automatic. There is no discussion, no debate, no waiting for additional data. The criteria are the criteria. If the deployment fails them, it rolls back.

The rollback should be logged and auditable. Every automated rollback generates a record that includes the rollout stage, the metrics that triggered the rollback, the timestamp, and the feature flag state before and after the rollback. The team reviews the rollback log to understand why the deployment failed. If the failure was due to a bug in the new code, the team fixes the bug and redeploys. If the failure was due to overly strict thresholds, the team adjusts the thresholds and redeploys. The log provides the data needed to improve the next deployment.

Automated rollback does not mean the team ignores the deployment. The team still receives alerts when a rollback occurs. The alert includes enough context to triage the issue. The team decides whether to fix forward, redeploy, or investigate further. The automation handles the immediate response—reverting traffic to the safe version. The team handles the follow-up—determining the root cause and deciding the path forward.

## The Progressive Delivery Pipeline: Commit to Production

The progressive delivery pipeline starts with a commit and ends with the code running in production at 100 percent. The developer pushes code to the repository. The CI system runs tests, builds the artifact, and stores it in a registry. The deployment system deploys the artifact to production but does not enable it. The progressive delivery system begins the rollout. One percent, five percent, ten percent, 25 percent, 50 percent, 100 percent. Each stage validates automatically. If every stage passes, the code reaches full production without human intervention.

The pipeline is repeatable and predictable. Every deployment follows the same path. Every deployment uses the same validation criteria. Every deployment generates the same audit logs. The team does not reinvent the deployment process for each change. They follow the pipeline. The consistency reduces errors. The automation reduces toil. The validation reduces risk.

The pipeline also supports fast iteration. If a deployment fails at five percent, the team fixes the issue, commits the fix, and the pipeline runs again. The fix goes through the same stages. One percent, five percent, ten percent. The previous failure does not slow down the next deployment. The pipeline is stateless. Each deployment is independent. The team can deploy as often as they need without worrying about interference from previous deployments.

## Progressive Delivery for AI: Models and Prompts

Progressive delivery for AI systems is similar to progressive delivery for traditional software, but with additional considerations. Model deployments introduce variability that software deployments do not. Two requests to the same endpoint with the same input can produce different outputs due to sampling, temperature, or non-deterministic behavior. The metrics must account for this variability. Comparing the new model's outputs to the old model's outputs requires more than checking for exact matches. It requires semantic similarity, eval-based quality metrics, and user feedback.

Prompt changes also require careful validation. A small change to a prompt can have a large impact on output quality. Progressive delivery for prompts follows the same stages—one percent, five percent, ten percent—but the validation criteria include output quality metrics like hallucination rate, relevance, and tone consistency. The metrics are collected from automated evals running in production or from sampling and manual review. If the new prompt increases hallucination rate by more than two percentage points, the rollout halts or rolls back.

Model routing adds complexity. In a traditional software deployment, the old version and new version are separate binaries running on separate instances. In an AI deployment, the old model and new model might share the same serving infrastructure. The serving layer loads both models into memory and routes requests based on the feature flag. The infrastructure must support multi-model serving, or the rollout must use separate clusters for each model. Multi-model serving is more efficient but requires more sophisticated serving infrastructure.

## Tooling: Argo Rollouts, Flagger, Spinnaker, and Custom

Several open-source tools implement progressive delivery orchestration. Argo Rollouts integrates with Kubernetes and supports canary, blue-green, and rolling update strategies. It reads metrics from Prometheus or Datadog, evaluates thresholds, and updates the deployment automatically. Flagger works with Istio or Linkerd service meshes, using traffic routing rules to control rollout percentages. It supports automated rollback based on Prometheus metrics or custom webhooks. Spinnaker is a more heavyweight deployment platform that supports multi-cloud deployments, complex orchestration workflows, and integration with existing CI systems.

Each tool has strengths. Argo Rollouts is lightweight and easy to integrate if you are already using Kubernetes. Flagger is powerful if you use Istio and want fine-grained traffic control. Spinnaker is comprehensive if you need multi-cloud support and advanced orchestration. All three tools provide the core progressive delivery capabilities: percentage-based rollouts, automated validation, and automated rollback.

Custom orchestration is an option if the off-the-shelf tools do not fit your architecture. You can build a progressive delivery system using a feature flag service, a metrics pipeline, and a rollout controller that updates the feature flag based on metric thresholds. The custom system is more work to build and maintain, but it fits your exact requirements and integrates seamlessly with your existing infrastructure. Many companies that adopted progressive delivery early built custom systems because the tooling did not yet exist.

The choice of tooling depends on your infrastructure, team size, and deployment complexity. If you are on Kubernetes and have a straightforward deployment model, start with Argo Rollouts. If you need advanced orchestration, evaluate Spinnaker. If you have unique requirements or need tight integration with internal systems, consider custom orchestration. The tool matters less than the practice. The value is in the staged rollout, automated validation, and automated rollback—not in which tool you use to implement them.

## The Human Override: When Automation Should Pause

Automation is powerful, but it is not infallible. There are situations where the automated system should pause and ask for human input. If the metrics are borderline—error rate is 0.49 percent and the threshold is 0.5 percent—the automated system might advance the rollout, but a human reviewing the trend might notice that the error rate is climbing and decide to halt. If the metrics are passing but there are unusual patterns in the logs, a human might spot a problem that the automated thresholds missed.

The human override should be easy to invoke. A button in the deployment dashboard that says "pause rollout" or "halt and review." The engineer on call sees an alert, reviews the metrics, and decides the rollout should pause. They click the button. The orchestration stops advancing. The rollout stays at the current percentage until the engineer approves continuation or initiates rollback. The override does not require redeploying or reconfiguring. It is a simple control that puts a human in the loop when automation is not enough.

The override should also be logged. Every time a human pauses or resumes a rollout, the action is recorded with a timestamp, the engineer's identity, and a reason. The log provides accountability and helps the team understand why the rollout did not proceed automatically. If the same engineer pauses every deployment at the same stage, there might be a recurring issue that the automated thresholds are not catching. The log data informs future improvements to the thresholds and validation criteria.

Progressive delivery combines the best of automation and human judgment—automation handles the repetitive tasks, humans handle the edge cases—but the automation itself must be built, maintained, and improved over time.
