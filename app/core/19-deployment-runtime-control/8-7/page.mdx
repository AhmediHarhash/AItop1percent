# 8.7 — Dependency Tracking: When Model A Changes, What Breaks

When you update a base model, do you know which downstream systems will break? If not, you are playing roulette with production. Most teams discover dependencies the hard way. They upgrade a model to improve performance. The upgrade works perfectly for the primary use case. Then they get alerts from three other teams whose systems depend on that model in ways nobody documented. A customer support chatbot that relied on the model's ability to detect sarcasm stops working because the new model version interprets sarcasm literally. An invoice parsing pipeline that depended on specific JSON formatting breaks because the new model uses slightly different field names. A compliance monitoring system that relied on the model refusing certain requests starts flagging false positives because the new model has a less restrictive safety filter. None of these dependencies were known to the team that made the upgrade. The dependencies were implicit, undocumented, and discovered only when things broke.

Dependency tracking is the practice of explicitly recording what each model depends on and what depends on each model. The dependency graph is bidirectional. Upstream dependencies answer the question: what does this model need to function? Downstream dependents answer the question: what will break if this model changes? Without both directions, you are blind. You can see what a model consumes, but not what consumes it. Or you can see what uses a model, but not what it needs. Both perspectives are required for safe changes.

## The Dependency Graph Structure

The dependency graph is a directed graph where nodes represent artifacts—models, datasets, prompts, features, services—and edges represent dependencies. A model node has incoming edges from the things it depends on: the base model it was fine-tuned from, the dataset it was trained on, the code commit that defines its architecture. The same model node has outgoing edges to the things that depend on it: the services that call it, the features that use its outputs, the downstream models that consume its predictions as inputs. The graph is acyclic if your system is healthy. Circular dependencies between models are a design failure.

Each edge has metadata. The dependency type: is this a training dependency, a runtime dependency, or a deployment dependency? A training dependency means the model cannot be trained without the upstream artifact. A runtime dependency means the model cannot serve requests without the upstream artifact. A deployment dependency means the model cannot be deployed without the upstream artifact. The dependency strength: is this a hard dependency where the model fails completely if the upstream is unavailable, or a soft dependency where the model degrades gracefully? The version constraint: does this dependency require an exact version, a minimum version, a version range, or any version?

Building the graph requires instrumentation across every system that creates or uses models. When a training job runs, it emits dependency events: this model depends on dataset X version Y, this model depends on base model Z version W. When a service deploys a model, it emits a dependent event: this service depends on model M version V. When a prompt template is updated, it emits a dependency event: this prompt depends on model N version P. Each event is recorded in a central dependency store. The store builds and maintains the graph in near real-time.

## Tracking Upstream Dependencies

Upstream dependencies are what a model needs to exist and function. The most obvious upstream dependency is the base model for a fine-tuned model. If you fine-tune an adapter on top of GPT-5-mini-20260115, your adapter has a hard runtime dependency on that exact base model version. You cannot serve requests without it. If the base model becomes unavailable or deprecated, your adapter stops working.

Datasets are upstream dependencies for training. You cannot retrain the model without the dataset. But datasets also have a special property: they usually do not need to be pinned to exact versions for reproduction. If your dataset is "all customer support tickets from January 2026," you can reproduce the model as long as you have access to that data, even if it is stored under a different version identifier. But if your dataset is "customer support tickets from January 2026, filtered to remove personally identifiable information using filter script version 3.2," then you have a dependency on both the raw data and the specific filter script version. Without the filter script, you cannot recreate the exact training set.

Prompts are upstream dependencies for API-based models. If your model is GPT-5 accessed via API with a specific system prompt, the system prompt is an upstream dependency. Changes to the prompt change the model's behavior just as much as changing the model version itself. Prompts should be versioned in source control. Each model deployment should record which prompt version it uses. If you change the prompt, that is a model change from a dependency perspective.

Code commits are upstream dependencies for all models. The training code defines the model architecture, loss function, optimizer, and training loop. If you cannot access the exact code that trained the model, you cannot reproduce it. This dependency is usually implicit—engineers assume the code is in git and will always be available. But code repositories get migrated, branches get deleted, and commits get force-pushed over. Treat code commits as explicit dependencies. Record the commit hash as part of model metadata. Ensure that commit is protected from deletion. Some teams mirror critical commits to a separate archive repository to guard against accidental loss.

Preprocessing logic is an upstream dependency that teams often forget. A model trained on tokenized text depends on the tokenizer version. A model trained on resized images depends on the resizing algorithm and target dimensions. A model trained on normalized features depends on the normalization statistics computed from the training data. If the preprocessing code changes, the model's inputs change, and the model might produce nonsense. Preprocessing dependencies are subtle because they are often embedded in data pipelines rather than model training scripts. Make them explicit. Record the preprocessing code commit alongside the training code commit.

## Tracking Downstream Dependents

Downstream dependents are the systems that rely on a model. They break when the model changes. Tracking dependents is harder than tracking dependencies because the dependent systems are owned by different teams who may not realize they are depending on your model. A model trained by the fraud detection team might be used by the transaction processing team, the customer support team, and the risk analytics team. If the fraud detection team upgrades their model, do they know which other teams need to test their systems?

The only reliable way to track dependents is to require registration. Any system that calls a model must register itself as a dependent. Registration happens at deployment time or first API call. The dependent system provides its name, owner, contact information, and the model version it is using. The dependency store records the relationship. When the model owner wants to upgrade or deprecate a model version, they query the store to see which systems depend on it. They notify the dependent teams before making the change. They give those teams time to test and prepare.

Registration enforcement happens at the API layer. The model serving infrastructure rejects requests from unregistered callers. To call a model, you must first register your service and provide authentication credentials tied to that registration. This sounds bureaucratic. It is also the only approach that scales beyond a handful of teams. Without enforced registration, dependencies are discovered only when they break.

Some dependencies are indirect. Service A calls model B. Model B's predictions are stored in a database. Service C reads from that database and makes decisions based on the predictions. Service C depends on model B, but it does not call model B directly. The dependency is mediated through the database. Tracking indirect dependencies requires tracing data flow through the system. This is difficult. Most teams only track direct dependencies—services that call models via API. Indirect dependencies are discovered during incident response, not during normal operation. The mature approach is to instrument data pipelines to record data lineage. If model B's predictions flow into table T, and service C reads from table T, then service C is a dependent of model B. This lineage can be computed from logs and query patterns.

## Dependency Versioning and Constraints

Not all dependencies require exact version matches. Some dependencies are flexible. A service might work with any version of a model that meets a minimum accuracy threshold. Another service might require an exact version because it depends on specific output formatting that changes across versions. Dependency constraints express these requirements.

The simplest constraint is exact version pinning. Service X depends on model Y version 3.2.1. No other version is acceptable. If model Y version 3.2.1 is deprecated or unavailable, service X cannot function. Exact pinning provides stability but blocks upgrades. To upgrade, the dependent service must be modified, tested, and redeployed.

Version ranges are more flexible. Service X depends on model Y version 3.x, meaning any version in the 3.x series. Minor and patch updates are allowed. Major version changes are blocked. This works if the model owner follows semantic versioning and guarantees backward compatibility within a major version. But many model teams do not follow semantic versioning strictly. A minor version update might introduce breaking changes to output format or accuracy characteristics. Version ranges are safe only if the model owner commits to backward compatibility.

Minimum version constraints are common for security and compliance. Service X depends on model Y version 3.2.1 or higher. Versions below 3.2.1 are blocked because they have a known security issue or fail to meet regulatory requirements. The service will accept any version from 3.2.1 onward, assuming that newer versions maintain compatibility. This assumption is dangerous unless the model owner explicitly commits to it.

Constraint enforcement happens at deployment time and runtime. When a service tries to deploy with a dependency on model Y, the deployment system checks whether the specified version satisfies the constraints. If service X requires model Y version 3.2.1 exactly, but version 3.2.1 is marked as deprecated, the deployment fails. At runtime, the model serving infrastructure checks constraints on every request. If service X is registered as requiring version 3.2.1 but tries to call version 4.0.0, the request is rejected. This prevents accidental constraint violations.

## Breaking Change Detection

Not all model changes are breaking changes. Improving accuracy by two percentage points is usually safe. Reducing latency by fifty milliseconds is safe. But changing output format is a breaking change. Removing a capability is a breaking change. Shifting the probability distribution of predictions enough to affect downstream decision thresholds is a breaking change. The challenge is detecting these changes before deployment.

Breaking change detection requires comparing the new model version against the old version on a shared test set. The test set includes inputs that represent how downstream dependents use the model. Run both model versions on the test set. Compare outputs. If outputs match exactly, the change is non-breaking at least for those inputs. If outputs differ but the differences are minor—slightly different confidence scores, slightly different token choices—the change might be breaking depending on how dependents use the outputs. If outputs differ dramatically—different formats, different fields, different answer types—the change is definitely breaking.

The comparison must cover more than headline accuracy metrics. A model might improve overall accuracy while regressing on specific slices that a dependent cares about. A fraud detection model might improve overall precision but get worse at detecting a specific fraud type that a downstream compliance system monitors. Slice-based regression testing is critical. Each dependent should provide a set of test cases that represent their usage. When a model is updated, it is tested on every dependent's test cases. If any test case regresses, the dependent is notified before the change goes live.

Some breaking changes are not detectable from input-output comparisons. A model that increases latency from one hundred fifty milliseconds to four hundred milliseconds has not changed its outputs, but it might break a dependent that assumed sub-two-hundred-millisecond response times. A model that increases cost per request from two cents to five cents has not changed its behavior, but it might break a dependent's budget assumptions. Breaking change detection must include performance and cost metrics, not just functional correctness.

## Impact Analysis Before Changes

Before deploying a model change, you need to know its blast radius. Which services will be affected? How many users? What is the risk level of each dependent? Impact analysis is the process of walking the dependency graph to understand what will change and what might break.

The analysis starts with the model you want to change. Query the dependency store for all downstream dependents. For each dependent, retrieve its owner, risk tier, and traffic volume. Group dependents by risk. Critical dependents—those that affect revenue, compliance, or user safety—get manual review and testing before the change. Medium risk dependents get automated testing. Low risk dependents get notification but proceed without blocking. The decision framework is: high-risk dependents can block a model change until they explicitly approve. Medium and low risk dependents are informed but do not block.

Traffic volume matters because it determines the scale of potential impact. If a dependent processes ten requests per day, a breaking change is annoying but contained. If a dependent processes ten million requests per day, a breaking change is a major incident. The impact analysis should quantify exposure: model change M affects service A with two million daily requests, service B with fifty thousand daily requests, and service C with one hundred daily requests. Total exposure is approximately two point one million requests per day. If the change has even a one percent failure rate, that is twenty-one thousand failed requests per day. This quantification forces realistic risk assessment.

Some organizations require impact analysis reports for all model changes. The report lists every dependent, the risk tier, the test coverage, and the approval status. The report is reviewed by a change advisory board or automated policy system. Changes with low impact can proceed automatically. Changes with high impact require explicit sign-off from dependent teams. This process is bureaucratic but necessary in production systems with many teams and many models.

## Automated Dependency Updates

When a model is updated, should dependent services automatically upgrade, or should they stay pinned until they manually opt in? The answer depends on the dependency constraint and the risk tolerance of the dependent. Low-risk dependents with version range constraints can auto-upgrade. High-risk dependents with exact version constraints should never auto-upgrade.

Automated updates work best for patch releases that fix bugs or improve performance without changing behavior. If model Y version 3.2.1 has a latency issue and version 3.2.2 fixes it without changing outputs, dependents using version range 3.x can auto-upgrade to 3.2.2. The upgrade is transparent. No dependent action required. But the auto-upgrade must be accompanied by automated testing. Before the upgrade goes live, the dependency system runs regression tests for every auto-upgrading dependent. If any test fails, the upgrade is blocked for that dependent, and the owner is notified.

Major and minor version updates should not auto-upgrade by default. These updates carry higher risk of breaking changes. Dependents should explicitly opt in by updating their dependency constraint. The exception is security updates. If model Y version 3.2.1 has a critical security vulnerability and version 3.2.3 fixes it, dependents should be forced to upgrade even if they prefer to stay pinned. Security vulnerabilities create liability for the entire organization. The dependency system can enforce mandatory upgrades for security patches while requiring manual opt-in for feature updates.

The upgrade policy should be configurable per dependent. Some dependents want to stay on the bleeding edge and accept the risk of automatic updates. Others want stability and prefer manual control. The dependency system allows each dependent to declare their preference: auto-upgrade for patch releases, manual review for minor releases, explicit approval for major releases, forced upgrade for security fixes. The system enforces these policies and logs every upgrade decision for audit.

## Dependency Visualization

A dependency graph with hundreds of models and thousands of dependents is impossible to understand without visualization. Dependency visualization tools render the graph as an interactive diagram. Nodes represent models and services. Edges represent dependencies. Colors indicate risk tiers. Edge thickness indicates traffic volume. The visualization answers questions like: what is the blast radius of changing this model? Which models have the most dependents and are therefore the highest risk to change? Which services depend on deprecated models and need to migrate?

The visualization must support filtering and zooming. Show me only the dependencies for model X. Show me only the critical-tier dependents. Show me only the dependencies added in the last thirty days. Without filtering, the graph is too dense to read. With filtering, it becomes a navigation tool. Engineers use it to explore the system, understand relationships, and plan changes.

Some teams generate dependency graphs as part of every deployment. The CI pipeline produces a diagram showing what the deployment will affect. The diagram is included in the pull request review. Reviewers can see at a glance whether the change affects other teams. If the diagram shows no downstream dependents, the change is low risk. If it shows dependencies on five critical services, the change requires coordination and testing.

## The Blast Radius Question

Every change has a blast radius—the set of things that will be affected if the change goes wrong. For model changes, the blast radius is the transitive closure of all downstream dependents. Not just the services that call the model directly, but the services that depend on those services, and the services that depend on those. A failure in model M cascades to service A, which cascades to service B, which cascades to feature C. The blast radius is M, A, B, and C.

Calculating the blast radius requires traversing the dependency graph recursively. Start at the model you want to change. Find all direct dependents. For each direct dependent, find its dependents. Continue until you reach nodes with no outgoing edges. The result is the full set of potentially affected systems. The blast radius includes both expected dependencies—services that you know depend on the model—and transitive dependencies that are less obvious.

Some teams use blast radius as a deployment gate. If the blast radius exceeds a threshold—more than ten critical services, or more than one million requests per day—the change requires additional scrutiny. It might require a phased rollout, canary deployment, or extended testing period. Small blast radius changes can proceed quickly. Large blast radius changes require coordination.

Understanding how to track dependencies—building the graph, recording constraints, detecting breaking changes, analyzing impact, automating safe updates, visualizing relationships, and calculating blast radius—is what separates teams that can change models confidently from teams that fear every update. But dependency tracking is only useful if you enforce version constraints. Pinning versions, choosing when to allow ranges, and managing the trade-off between stability and progress is the next layer of control.
