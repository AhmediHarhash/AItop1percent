# 5.5 — Fallback Hierarchies: Primary, Secondary, Tertiary, Cached, Human

In April 2025, a patient scheduling platform went completely offline for four hours on a Monday morning. The root cause was straightforward: their primary AI provider experienced a regional outage. The impact was severe: seventeen thousand appointment booking requests failed, call center volume spiked by 340 percent, and the operations team spent the rest of the week manually rescheduling patients who had given up on the system. The company had invested heavily in prompt engineering, evaluation pipelines, and quality monitoring. They had tested edge cases, built comprehensive regression suites, and maintained detailed runbooks. But they had configured zero fallback options. When the primary provider returned a 503 error, their application had nowhere to route traffic. Every request failed immediately.

The engineering retrospective identified the failure mode within hours. The solution took six weeks to implement properly, and it required rethinking their entire runtime architecture. They built what production AI systems should have had from day one: a fallback hierarchy that degrades gracefully through multiple tiers before admitting defeat.

## The Five-Tier Fallback Pattern

A robust AI system maintains five distinct fallback tiers. The **primary tier** is your production model running on your preferred provider with your standard configuration. This is where 99 percent of requests should complete successfully under normal conditions. When the primary tier returns an error, exceeds latency thresholds, or shows degraded quality metrics, the system moves to the **secondary tier**: the same model on a different provider, or a different model with comparable capabilities on the same provider. The secondary tier handles maybe one percent of traffic during normal operations and absorbs the full load during primary outages.

When the secondary tier also fails or degrades, the system falls back to the **tertiary tier**: a faster, cheaper model that may sacrifice some quality but can handle the request. This tier prioritizes availability over perfection. If a GPT-5 request fails and the Claude Opus 4.5 fallback also fails, dropping to GPT-5-mini or Claude Haiku 4.5 keeps the user experience functional rather than completely broken. The tertiary tier might return a less nuanced response, skip some advanced reasoning steps, or handle fewer edge cases, but it returns something useful.

The **cached response tier** activates when all live model tiers fail. This tier serves previously generated responses for identical or highly similar queries. A scheduling assistant that has answered "How do I reschedule my appointment?" five hundred times today can serve that cached response when no models are available. The answer is not personalized to the specific user context, but it provides immediate value instead of an error message. Cached responses work best for high-frequency, low-variance queries where staleness is acceptable.

The **human escalation tier** is the final fallback. When no model tier can handle the request and no cached response applies, the system routes to a human operator with full context about the request, the failures encountered, and the user's history. This tier is expensive and slow, but it guarantees that every user gets a response. A patient trying to book an urgent appointment should reach a human scheduler, not a 503 error page.

## Defining Fallback Triggers

Each tier transition requires explicit trigger conditions. You cannot rely on intuition or manual intervention when a production system is failing in real time. The most common trigger is **error rate thresholds**. If the primary tier returns errors on more than five percent of requests over a sixty-second window, the system begins routing new requests to the secondary tier while retrying failed primary requests in the background. The threshold must be high enough to avoid triggering on random noise but low enough to catch real degradation before user impact becomes severe.

**Latency thresholds** trigger fallbacks before errors occur. If the primary tier starts returning responses in twelve seconds instead of the usual three seconds, quality may be fine but user experience is broken. A system can define a p95 latency threshold: if 95 percent of requests complete in under five seconds but the p95 suddenly spikes to fifteen seconds, route new traffic to the secondary tier immediately. Latency-triggered fallbacks prevent error-triggered fallbacks because you catch degradation earlier.

**Timeout conditions** handle the case where a provider simply stops responding. If a request has been in flight for thirty seconds with no response and no error, the system must assume failure and move to the next tier. The timeout value depends on your user experience requirements. A chatbot might timeout at ten seconds. A batch document processing system might wait two minutes. The key is that the timeout must be configured and enforced. Waiting indefinitely is the same as having no fallback at all.

**Quality thresholds** are harder to detect in real time but critically important. If the primary model starts returning responses that fail your quality checks—confidence scores below threshold, hallucination detectors firing, output format validation failing—the system should fallback even if latency and error rates look normal. This requires instrumentation that evaluates every response before serving it to users. Some teams run lightweight quality checks on 100 percent of responses and heavyweight checks on a sampled subset. Either way, quality-triggered fallbacks prevent serving degraded responses just because the provider is technically online.

## Quality Degradation at Each Tier

Fallback is not free. Each tier represents a deliberate trade-off between availability and quality. The primary tier delivers the quality level you designed for. The secondary tier should deliver comparable quality if it is a different provider running the same model class, or slightly degraded quality if it is a different model. A fallback from GPT-5 to Claude Opus 4.5 might preserve 95 percent of your quality metrics. A fallback from GPT-5 to GPT-5-mini might preserve 85 percent. You must measure these quality differences in advance, not discover them during an outage.

The tertiary tier sacrifices quality for availability. You are explicitly choosing to serve a less capable response rather than no response. This is the right choice for many use cases but the wrong choice for some. A legal contract review system should not fallback to a weaker model that might miss critical clauses. A casual FAQ chatbot can absolutely fallback to a faster model that handles 80 percent of questions well and apologizes for the rest. The decision depends on the user harm caused by degraded quality versus the user harm caused by complete unavailability.

Cached responses represent a different quality trade-off. The response quality might be high—it was generated by your primary model under normal conditions—but the response relevance degrades over time. A cached answer to "What are your holiday hours?" from November is perfect in December and useless in February. A cached answer to "How do I reset my password?" stays relevant for months. Teams that use cached fallbacks define TTLs (time-to-live) based on content type and maintain separate cache tiers for evergreen content versus time-sensitive content.

Human escalation preserves quality but destroys latency. A human expert can provide a better answer than any model for complex, ambiguous, or high-stakes queries. But the response time jumps from three seconds to three minutes or three hours depending on queue depth and agent availability. You must set user expectations correctly. A system that silently routes to human escalation and makes users wait three minutes without explanation creates terrible user experience. A system that says "Our AI is temporarily unavailable, connecting you to a specialist" and shows estimated wait time creates acceptable user experience.

## Cached Responses as Emergency Fallback

The cached response tier requires careful design. You cannot simply cache every response and hope for the best. The most effective approach is **semantic caching**: storing responses indexed by the semantic meaning of the query rather than the exact query text. When a user asks "How do I change my delivery address?" and your system has previously answered "How can I update my shipping address?", semantic caching recognizes the equivalence and serves the cached response. This requires embedding queries and comparing cosine similarity at request time, but the added latency is negligible compared to waiting for a model that is currently offline.

**Frequency-based caching** prioritizes common queries. If 20 percent of your queries account for 80 percent of your traffic, maintaining high-quality cached responses for those queries provides effective fallback coverage. A production system might monitor query distribution over a rolling seven-day window and automatically cache responses for any query pattern that appears more than one hundred times. The cache refresh happens daily during off-peak hours using your primary model while it is healthy.

**Confidence-gated caching** only caches responses that meet quality thresholds. If your primary model returns a response with confidence below 0.85, that response should not enter the cache even if it ultimately proved correct. Low-confidence responses represent edge cases where model behavior is less reliable, and serving them during fallback scenarios risks compounding quality problems. Only cache responses where the model demonstrated high confidence and, ideally, where the response also passed post-generation quality checks.

The cache must include **metadata about freshness and context**. Every cached entry should store the timestamp, the model version that generated it, the input context that was provided, and any user-specific parameters that might affect relevance. When serving a cached response, the system should validate that the cache entry is still appropriate for the current context. A cached response generated for a premium user with access to advanced features should not be served to a free-tier user who asks the same question.

## Human Escalation as the Final Tier

The human escalation tier requires operational infrastructure. You cannot simply route requests to a Slack channel and hope someone is available. Production human escalation means a **staffed queue** with defined SLAs, context handoff protocols, and response tracking. When a request escalates to human handling, the human operator receives the original query, the user's history, the models that were attempted and why they failed, and any partial responses or error messages. This context allows the human to provide a complete answer without asking the user to repeat themselves.

**Escalation routing rules** determine which humans receive which escalated requests. A technical support query should route to technical support staff. A billing query should route to billing specialists. A complex domain-specific question should route to subject matter experts. The routing logic must be defined in advance and tested regularly. Some teams maintain a 24/7 on-call rotation specifically for AI escalations. Others accept that escalations outside business hours will queue until morning. Either approach is defensible as long as user expectations are set correctly.

**Escalation volume monitoring** is critical for long-term system health. If your human escalation tier is handling 0.1 percent of requests, your fallback hierarchy is working. If escalation volume suddenly jumps to five percent, something is wrong with your models or your fallback configuration. Escalation volume is a trailing indicator of model health. A sudden spike in escalations three hours ago suggests model degradation that your other monitoring might have missed. Teams that track escalation volume by root cause—model error, quality failure, timeout, cache miss—can identify which tier is failing and fix it quickly.

The human escalation tier also serves as a **data collection mechanism**. Every escalated request represents a case where your models failed. Some of these failures are expected—truly novel queries, adversarial inputs, cases outside your training distribution. But many escalations reveal gaps in your model capabilities, missing training examples, or edge cases you never anticipated. Teams that systematically review escalated requests and feed representative examples back into their training data turn escalations into a continuous improvement loop.

## Fallback Testing: Chaos Engineering for AI

You do not know if your fallback hierarchy works until you test it under realistic failure conditions. The most effective approach is **controlled outage simulation**. During a scheduled maintenance window, deliberately disable your primary tier and route all traffic to the secondary tier. Measure latency, error rates, quality metrics, and user-visible behavior. If the secondary tier handles the load without degradation, the fallback is working. If errors spike or latency degrades, you have identified a capacity or configuration problem before a real outage forces you to discover it.

**Progressive tier testing** validates each fallback level independently. Disable the primary tier and verify secondary tier behavior. Disable both primary and secondary tiers and verify tertiary tier behavior. Disable all model tiers and verify cached response behavior. Each test should run long enough to detect issues that only appear under sustained load. A five-minute test might miss rate limiting problems that appear after thirty minutes. A thirty-minute test at full production traffic volume is more realistic.

**Latency injection testing** simulates degraded provider performance rather than complete outages. Artificially delay responses from your primary tier by adding ten seconds of latency. Does your system correctly timeout and fallback? Or does it wait patiently while user experience degrades? Latency injection reveals configuration bugs where timeout values are set too high or where fallback triggers are not sensitive enough to latency changes.

**Error injection testing** simulates partial failures. Configure your primary tier to return errors on 10 percent of requests. Does your system fallback for the failing requests while continuing to use the primary tier for the 90 percent that succeed? Or does it overreact and fallback everything, wasting capacity? The ideal behavior is per-request fallback: each request independently evaluates whether to fallback based on its own outcome, not based on aggregate statistics across all requests.

The testing schedule matters. Running fallback tests once during initial development is insufficient. Providers change their infrastructure, models get updated, rate limits change, network topology shifts. A fallback configuration that worked in March might fail in September. Production teams test fallback hierarchies at least quarterly and after every major configuration change. Some teams run lightweight fallback tests weekly as part of their continuous validation pipeline.

When a fallback test identifies a problem, the fix must happen before the next production outage. The patient scheduling platform that went offline for four hours in April 2025 had never tested their non-existent fallback configuration. The six weeks they spent building fallback infrastructure after the outage would have been better spent before the outage. The time to discover that your secondary tier cannot handle production load is during a controlled test, not during a Saturday afternoon when your primary provider is on fire and your CEO is asking why the product is down.

Multi-provider failover extends the fallback hierarchy across completely different providers, ensuring that no single vendor dependency can take down your entire system.

