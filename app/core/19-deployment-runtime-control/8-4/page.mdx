# 8.4 â€” Model Packaging Standards: ONNX, Safetensors, Hugging Face Format

A new team member joins the platform team. Their first task is to deploy an updated fraud detection model to production. They pull the model artifact from the registry. It is a PyTorch checkpoint saved with torch.save three months ago. They try to load it in the production environment. PyTorch throws an error. The model was saved with PyTorch 2.1. Production is running PyTorch 2.3. The serialization format changed between versions. The model will not load. The deployment is blocked until someone who knows the original training environment can figure out how to convert the model to a compatible format.

This problem is universal. Models are not just weights. They are serialized data structures that depend on the framework, the framework version, and sometimes the hardware architecture. A model saved in one environment might not load in another. A model saved with one serialization format might not be readable by a different serving framework. Model packaging is the discipline of ensuring that models can be reliably loaded and executed in any environment they might need to run in.

The packaging problem has two layers. The first is the serialization format: how are the weights and architecture stored on disk? The second is the container format: what additional files and metadata need to travel with the model? Both layers matter. A model with perfect weights but incompatible serialization is useless. A model with compatible serialization but missing metadata is risky. Production-grade model packaging addresses both layers with standards that prioritize safety, portability, and compatibility.

## Safetensors: The 2025-2026 Standard

Safetensors has become the default serialization format for production models in 2025 and 2026. It was designed to solve two problems with older formats: security vulnerabilities and loading speed. PyTorch's native format uses pickle, which is a general-purpose Python serialization protocol. Pickle can serialize arbitrary Python objects, including executable code. This makes it a security risk. A malicious actor can embed arbitrary code in a pickle file. When the file is loaded, the code executes. For models downloaded from the internet or transferred between untrusted systems, this is unacceptable.

Safetensors eliminates this risk by using a simpler, non-executable format. It serializes only tensors, not arbitrary objects. The format is a header that describes the tensor shapes and data types, followed by raw binary data for the weights. There is no code execution during loading. The format is also faster to load because it uses memory-mapped files. Large models can be loaded without reading the entire file into memory upfront. The weights are accessed on demand, which reduces startup time and memory usage.

The format is framework-agnostic. Safetensors files can be loaded in PyTorch, TensorFlow, JAX, or any other framework that implements the loading logic. This makes it easier to move models between frameworks. A model trained in PyTorch can be saved as safetensors and loaded in TensorFlow without manual conversion. The format does not encode framework-specific details. It just stores the tensor data and metadata.

Adoption has been rapid. Hugging Face made safetensors the default format for models on the Hub in 2024. Most popular model architectures now have safetensors support. Training frameworks like Axolotl and TRL save models as safetensors by default. Serving frameworks like vLLM and TGI load safetensors natively. If you are starting a new project in 2026, safetensors should be your default choice unless you have a specific reason to use something else.

The migration path from older formats is straightforward. Existing models saved as PyTorch checkpoints can be converted to safetensors with a single command using the safetensors library. The conversion reads the original checkpoint and writes a new file in safetensors format. The conversion is lossless. The weights are identical. The only difference is the serialization format. Teams that have large catalogs of legacy models typically convert them as part of a migration project. The conversion is automated, but it requires storage space for both the old and new formats during the transition.

## ONNX: Cross-Framework Interoperability

ONNX, Open Neural Network Exchange, is a format designed for cross-framework compatibility and runtime optimization. It was created by Microsoft and Facebook in 2017 and has since been adopted by most major ML frameworks. The idea is simple: instead of each framework having its own serialization format, they all export to a common intermediate representation. A model trained in PyTorch can be exported to ONNX and then loaded in TensorFlow, MXNet, or a specialized inference runtime like ONNX Runtime.

The primary use case for ONNX is deployment to optimized inference engines. ONNX Runtime is faster than native PyTorch or TensorFlow for inference on many model architectures. It applies graph optimizations, operator fusion, and hardware-specific acceleration that general-purpose training frameworks do not. For production deployments where latency matters, exporting to ONNX and serving with ONNX Runtime can reduce inference time by 30 to 50 percent compared to serving with the training framework.

ONNX also enables deployment to environments where the training framework is not available. Embedded devices, mobile applications, and some edge deployments cannot run full PyTorch or TensorFlow. They can run ONNX Runtime, which has a smaller footprint. Exporting to ONNX makes models portable to these environments. This portability is critical for applications like on-device speech recognition or real-time video analysis where models need to run outside of traditional server infrastructure.

The limitation of ONNX is operator coverage. ONNX defines a set of standard operators like convolution, matrix multiplication, and activation functions. If your model uses only standard operators, export to ONNX is straightforward. If your model uses custom operators or recent additions to PyTorch that are not yet in the ONNX standard, export might fail or require custom operator implementations. This is less of a problem in 2026 than it was in earlier years because operator coverage has improved, but it is still a consideration.

ONNX export also requires validation. The export process converts the model's computation graph to ONNX format. This conversion is not always perfect. Numerical precision can change slightly due to differences in how operators are implemented. For most models, the differences are negligible, but for models with tight quality constraints, you need to validate that the ONNX version produces outputs that are sufficiently close to the original. This validation is typically done by running both versions on a test set and comparing outputs.

## Hugging Face Format: The Transformer Standard

The Hugging Face format has become the de facto standard for transformer models. It is not a single serialization format but a set of conventions for organizing model files, configuration, and metadata. A Hugging Face model repository contains a config.json file that describes the model architecture, a tokenizer configuration, and model weights saved in either PyTorch, TensorFlow, or safetensors format. The repository can also include a model card, training logs, and example code.

The strength of the Hugging Face format is ecosystem integration. Models saved in this format can be loaded with the transformers library, which is the most widely used library for working with transformer models. The library handles all the boilerplate of loading configuration, initializing the model architecture, and loading weights. A model can be loaded in three lines of code. This standardization makes it easy to swap models. If you are using a Llama 4 model and want to try a Mistral Large 3 model, you change the model identifier. Everything else stays the same.

The format also supports model sharding for large models. A 70-billion-parameter model is too large to fit in a single file. The Hugging Face format splits it into multiple shard files, each a few gigabytes. The config.json file includes a map that describes which parameters are in which shard. The loading logic automatically loads all shards and reconstructs the full model. This sharding is transparent to the user. You load a sharded model the same way you load a single-file model.

The Hugging Face format is also designed for version control. The repository structure works with git and git-lfs. Changes to configuration or tokenizer files are versioned as text diffs. Changes to model weights are versioned as binary files through git-lfs. You can branch, tag, and roll back models just like code. This integration makes it easy to track model evolution over time.

The limitation is that the Hugging Face format is optimized for transformers. If your model is not a transformer, the format still works, but you lose some of the benefits. The transformers library will not automatically load your model. You need custom loading logic. The model card templates are designed for language models. Adapting them to other model types requires customization. For transformer models, the Hugging Face format is the best choice. For other architectures, it is one option among several.

## PyTorch Native Formats: Pickle Risks

PyTorch's native serialization uses pickle. This is convenient for experimentation because pickle can serialize almost anything. You can save the model, the optimizer state, the learning rate schedule, and any custom objects in a single file. When you load the file, everything is restored exactly as it was. This makes it easy to pause training, move to a different machine, and resume.

The problem is that pickle is unsafe for production. Pickle can execute arbitrary code during deserialization. If you load a malicious pickle file, it can run code that deletes files, exfiltrates data, or compromises the system. This is not a theoretical risk. There have been real incidents where malicious models were distributed as pickle files. For models that come from untrusted sources or are stored in shared environments, pickle is a security vulnerability.

PyTorch is moving away from pickle. Starting in PyTorch 2.3, the recommended format for saving models is safetensors or TorchScript. PyTorch still supports pickle for backward compatibility, but the documentation actively discourages its use for production deployments. If you have legacy models saved as pickle files, they should be converted to a safer format before deployment.

The conversion is usually straightforward. Load the model with torch.load, which handles the pickle deserialization. Then save it with the safetensors library or torch.jit.save for TorchScript. The new format is safer and often faster to load. The only reason to keep using pickle is if you need to save non-tensor objects like optimizer state, and even then, those objects should be saved separately from the model weights.

## vLLM and TGI Requirements

Modern serving frameworks have specific format requirements. vLLM, the most popular framework for serving large language models in 2026, prefers Hugging Face format models saved as safetensors. It can load models directly from the Hugging Face Hub or from local storage that follows the Hugging Face directory structure. The format is expected to include config.json, tokenizer files, and weights in safetensors format. If your model does not follow this structure, vLLM can still load it, but it requires custom configuration.

TGI, Text Generation Inference from Hugging Face, has similar requirements. It also expects Hugging Face format with safetensors. TGI adds support for tensor parallelism and quantization, but those features require models to be packaged in specific ways. For example, quantized models need additional metadata files that describe the quantization scheme. If those files are missing, TGI cannot apply quantization at runtime.

The implication is that if you plan to serve models with vLLM or TGI, you should package them in Hugging Face format with safetensors from the start. Trying to adapt other formats at deployment time is possible but adds friction. It is better to standardize on the format that your serving framework expects. This reduces deployment complexity and makes it easier to swap models or upgrade serving infrastructure.

Some teams maintain multiple packaging formats for the same model. They store the canonical version in safetensors for safety and compatibility. They also export to ONNX for deployments that use ONNX Runtime. They maintain a Hugging Face-format version for deployments that use vLLM. This redundancy increases storage costs but reduces deployment friction. Each deployment environment gets the format it expects without conversion at deploy time.

## Conversion Pipelines

Most organizations end up with models in multiple formats over time. Models trained years ago might be PyTorch pickle files. Models trained recently might be safetensors. Models intended for edge deployment might be ONNX. Standardizing on a single format for all new models is the right goal, but you still need to handle legacy formats. This requires conversion pipelines.

A conversion pipeline is a script or service that takes a model in one format and outputs it in another. The pipeline reads the source format, loads the model into memory, and saves it in the target format. For simple conversions like PyTorch to safetensors, this is a few lines of code. For complex conversions like PyTorch to ONNX, the pipeline might need to handle operator compatibility, validate numerical accuracy, and package additional metadata.

Conversion pipelines are often part of the deployment workflow. A model is trained and saved in the training framework's native format. The deployment pipeline converts it to the production format before deploying. This keeps the training and deployment formats decoupled. Training teams do not need to worry about production format requirements. Deployment teams do not need to support every training framework's serialization format. The conversion pipeline is the translation layer.

The pipeline should validate that the conversion was successful. This means running both the original and converted models on a test set and comparing outputs. If outputs diverge beyond a small threshold, the conversion failed. The failure should block deployment. Silent conversion failures are worse than no conversion because they result in models that load successfully but produce incorrect outputs.

## Versioning the Format

Model formats evolve. Safetensors might add new features. ONNX might add new operators. Hugging Face format conventions might change. When the format changes, models packaged in the old format might not load in systems that expect the new format. This creates a versioning problem on top of the model versioning problem.

The solution is to version the packaging format alongside the model version. A model's full identifier should include both its semantic version and its format version. For example, fraud-2.3.1-safetensors-0.4.0 indicates model version 2.3.1 packaged with safetensors version 0.4.0. If a deployment environment requires safetensors 0.4.0 or later, it can check the format version before loading the model.

Format versions also matter for backward compatibility. A serving system running safetensors 0.3.0 might not be able to load a model packaged with safetensors 0.4.0 if the new version introduced breaking changes. The system can detect this mismatch and refuse to load the model rather than failing silently. This fail-fast behavior prevents production incidents caused by format incompatibility.

Tracking format versions is additional complexity, but it is necessary in environments where models are packaged at different times with different tool versions. If your organization has a five-year history of model development, you likely have models packaged with dozens of different format versions. Without tracking those versions, you cannot reliably load old models. With tracking, you can maintain compatibility or explicitly mark old formats as unsupported.

## Security Scanning

Model files are data, but they can contain malicious content. Pickle files can contain code. Even safetensors files can contain data that exploits vulnerabilities in the loading code. Before deploying a model to production, especially a model from an external source, it should be scanned for security issues. This scanning is separate from functional validation. It checks whether the artifact itself is safe to load, not whether the model produces correct outputs.

Security scanning tools for models are still maturing in 2026, but basic practices are well-established. Do not load pickle files from untrusted sources. If you must load a pickle file, do it in a sandboxed environment where code execution is restricted. Prefer safetensors for any model that will be loaded in production. Even with safetensors, check the file size and structure before loading. A file that claims to be a 7-billion-parameter model but is 200 gigabytes is suspicious.

Some organizations implement automated scanning as part of the model registration pipeline. When a model is uploaded to the registry, a scanner checks the file format, verifies that the file size matches expectations for the claimed model size, and scans for known malicious patterns. If the scan fails, the model is not registered. This creates a security gate that prevents unsafe models from reaching production.

The scanning is not foolproof. New attack vectors are discovered regularly. But basic scanning catches most naive attacks and enforces the discipline of only using safe formats. Over time, as models become more widely distributed and supply chain attacks become more common, security scanning will become as standard for models as it is for software dependencies.

## The Packaging Contract

Model packaging is a contract between the team that produces the model and the team that deploys it. The contract specifies the format, the metadata, and the guarantees. If the producer team packages a model as safetensors with Hugging Face structure, the deployment team can rely on that structure. If the deployment team requires ONNX for performance, the producer team needs to deliver ONNX. The contract prevents surprises.

The contract also specifies what metadata must travel with the model. At minimum, this includes the model version, the format version, and the expected input and output shapes. More comprehensive contracts include quality metrics, known limitations, computational requirements, and license information. The richer the metadata, the easier it is for deployment teams to make informed decisions about whether and how to deploy the model.

Standardizing packaging formats across an organization is one of the highest-leverage investments in model operationalization. It eliminates an entire class of deployment failures. It makes models portable across teams and environments. It enables automation because deployment systems can rely on consistent structure. The next step is managing adapters and compositional models where multiple components need to be versioned and packaged together.

