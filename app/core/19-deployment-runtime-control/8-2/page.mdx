# 8.2 â€” Semantic Versioning for Models: Major, Minor, Patch for AI

Version numbers should communicate breaking changes. For code, a breaking change is a change that requires downstream systems to update their integration. For models, a breaking change is a quality regression or behavior shift that affects production use cases. If your versioning scheme does not communicate these changes clearly, you will deploy breaking changes without realizing it.

Semantic versioning was designed for software libraries. The convention is major.minor.patch. Increment the major version for breaking changes. Increment the minor version for backward-compatible additions. Increment the patch version for backward-compatible bug fixes. This convention works because software behavior is deterministic. A function that takes three arguments and returns a string will always take three arguments and return a string. If that contract changes, it is a breaking change. If new functionality is added without changing existing behavior, it is a minor version. If a bug is fixed without changing the API, it is a patch.

Models are probabilistic. They do not have fixed contracts. A model that takes text and returns a classification will always take text and return a classification, but what classification it returns for a given input might change between versions. Is that a breaking change? It depends on whether downstream systems depend on the old behavior. If the old model classified a specific input as spam and the new model classifies it as not spam, and your production system was built assuming the old classification, then yes, it is a breaking change. But you cannot detect this by looking at the model's signature. You have to evaluate its behavior.

## Adapting Semantic Versioning for Models

Semantic versioning for models requires redefining what major, minor, and patch mean in a probabilistic context. The goal is the same: communicate the risk of upgrading. A major version bump signals high risk. Downstream systems should not automatically upgrade. Manual validation is required. A minor version bump signals moderate risk. Automated evaluation might be sufficient. A patch version bump signals low risk. Automated deployment is reasonable.

The definitions that work in practice look like this. A major version change is any change that causes measurable quality degradation on critical metrics, significant behavior shifts on representative inputs, or changes in output format or schema. A minor version change is capability additions that improve quality metrics without degrading others, expansions in domain coverage that do not affect existing use cases, or model architecture changes that preserve behavior. A patch version change is retraining on the same data with no expected behavior change, infrastructure optimizations like quantization that preserve quality within tolerance, or fixes for rare edge cases that do not affect primary use cases.

These definitions are operational, not mathematical. They require testing. You cannot determine whether a model change is major or minor just by looking at the training code. You have to evaluate the model on production-relevant tasks and compare metrics to the previous version. If precision drops by more than 2 percent, that is a major version. If precision improves by 5 percent and recall stays flat, that is a minor version. If precision and recall both stay within 1 percent, that is a patch version. The thresholds are policy decisions, not universal constants, but they must be defined and enforced.

## What Constitutes a Major Version

A major version bump is required when the model's behavior changes in ways that could break downstream systems or degrade user experience. This happens when you change the training objective, switch to a different architecture, retrain on substantially different data, or make changes that cause quality regressions on critical metrics. The major version signals to downstream systems: do not assume this model behaves like the previous version. Validate thoroughly before deploying.

The most obvious trigger for a major version is a quality regression. If your fraud detection model's precision drops from 94 percent to 89 percent, that is a major version change. Downstream systems that were tuned for 94 percent precision will experience different outcomes. False positive rates will increase. User-facing behavior will change. This is a breaking change in the semantic versioning sense, even if the model's input and output schema are identical.

Behavior shifts also require a major version. If the model starts classifying inputs differently on representative examples, even if aggregate metrics stay similar, that is a major version. A content moderation model that previously flagged political speech as neutral but now flags it as potentially harmful has shifted behavior. Systems that rely on the old behavior, like auto-approval workflows, will break. The model's overall accuracy might not have changed, but its decision boundaries moved. That is a major version.

Output format changes are also major. If you add a new field to the model's output, that might seem like a minor change, but if downstream parsers do not expect that field, they might break. If you change the output from a single label to a ranked list of labels, that is definitely a major change. Downstream code that expects a single label will fail. Even renaming fields or changing the order of returned values can break integrations. These are contract changes, and contracts matter even for probabilistic systems.

## What Constitutes a Minor Version

A minor version bump is appropriate when the model improves without breaking existing use cases. This happens when you add new capabilities, improve quality metrics without degrading others, or expand domain coverage without affecting the primary task. The minor version signals: this model is better than the previous version, and it should not break anything, but validate before deploying to sensitive environments.

Quality improvements are the most common reason for a minor version. You retrain on more data, tune hyperparameters, or apply a new technique that improves precision from 94 percent to 96 percent without changing recall. Aggregate metrics improve. Behavior on existing test cases stays similar or improves. There is no obvious regression. This is a minor version. Downstream systems will see better outcomes, but they will not see unexpected failures.

Capability additions also justify a minor version. Your model previously handled English text. You retrain it to handle English and Spanish. Performance on English remains stable. Spanish is a new capability. Systems that only use English are unaffected. Systems that need Spanish can now use it. This is a backward-compatible addition, which is the definition of a minor version.

Architecture changes that preserve behavior can be minor versions. You switch from a dense model to a mixture-of-experts model. Inference latency improves. Quality metrics stay within tolerance. Output behavior is nearly identical on test cases. The internal architecture changed, but the external contract did not. This is a minor version. However, if the architecture change introduces any measurable behavior shift, it should be a major version. The burden of proof is on stability.

## What Constitutes a Patch Version

A patch version bump is for changes that are expected to be completely transparent to downstream systems. This happens when you retrain on the same data with the same process, apply infrastructure optimizations that do not affect outputs, or fix rare edge cases that do not change primary behavior. The patch version signals: this is essentially the same model, just refreshed or optimized.

Retraining on the same data is the most common patch version scenario. Your model was trained three months ago. The training data has not changed, but you retrain it with the same process to get a fresh set of weights. Evaluation shows metrics are within 0.5 percent of the previous version. Behavior on test cases is nearly identical. This is a patch version. The model is functionally the same, but the artifact is different because training has some inherent randomness.

Infrastructure optimizations also fit the patch definition. You quantize the model from 16-bit to 8-bit precision. Inference speed doubles. Quality metrics degrade by 0.3 percent, which is within your tolerance threshold. The model's behavior is effectively unchanged from a user perspective. This is a patch version. The artifact format changed, but the functionality did not.

Edge case fixes can be patches if they truly do not affect primary use cases. You discover that the model fails on inputs with more than 10,000 tokens due to a tokenization bug. You fix the bug and retrain. The fix only affects inputs longer than 10,000 tokens, which represent 0.01 percent of production traffic. Primary behavior is unchanged. This is a patch version. However, if the fix affects common inputs, it should be at least a minor version because behavior changed.

## The Challenge of Detecting Breaking Changes

The hard part is knowing which category a change falls into before you deploy it. Code breaking changes are syntactic. You can detect them with static analysis or compilation. Model breaking changes are semantic. You can only detect them by running the model on representative inputs and comparing outputs to the previous version. This requires infrastructure.

You need a regression test suite that covers production use cases. The test suite should include examples that represent the full distribution of production inputs, edge cases that matter to users, and adversarial examples that probe decision boundaries. You need to run both the old model and the new model on this test suite and compare outputs. If outputs diverge on more than a small threshold of examples, that is a behavior shift. If quality metrics degrade beyond your tolerance, that is a regression.

The comparison must be automated. Manual comparison does not scale. You cannot eyeball 10,000 examples and determine whether behavior changed. You need automated metrics that capture behavior similarity. Exact match rates tell you what percentage of outputs are identical. Semantic similarity metrics tell you whether outputs are approximately the same. Quality metrics tell you whether the new model is better or worse on dimensions you care about.

The problem is that small behavior changes are expected. Even retraining on the same data produces slightly different weights. Those weights produce slightly different outputs. A patch version does not mean bit-identical outputs. It means functionally equivalent outputs. Defining "functionally equivalent" is a policy decision. Is a 1 percent difference in outputs acceptable for a patch version? What about 5 percent? What if the outputs are different but both correct? These questions do not have universal answers, but your versioning policy must provide answers.

## Version Naming Conventions

The version number alone is not enough. You need additional identifiers that make versions distinguishable and traceable. A convention that works is major.minor.patch-descriptor. The descriptor includes information like the training date, the base model, and the purpose. For example, 2.3.1-20260115-llama4-fraud is version 2.3.1, trained on January 15, 2026, based on Llama 4 Scout, for fraud detection.

The training date makes versions chronologically sortable. If you have two models with the same major.minor.patch but different dates, the later one is the more recent artifact. This matters when checkpoints proliferate. The base model identifier is critical when you maintain multiple parallel versions built on different base models. If you have a version built on Claude Opus 4.5 and another built on Llama 4 Maverick, the version numbers alone do not tell you which is which. The descriptor does.

The purpose identifier helps when you have multiple models in production for different use cases. If you have fraud-2.3.1 and churn-1.4.0, you know which model is which without looking up metadata. This prevents deployment mistakes. Someone trying to deploy a fraud model will not accidentally grab the churn model because the version identifier makes the distinction obvious.

Some teams include the commit hash of the training code in the version string. This makes it easy to trace from a model version back to the exact code that produced it. Others include the dataset version. The descriptor can be arbitrarily detailed, but it should be readable. A version string that is 200 characters long is not usable. The key identifiers that enable traceability and prevent mistakes should be there. Everything else can be in metadata.

## Version Comparison and Upgrade Decisions

Semantic versioning enables automated upgrade decisions. If a new patch version is released, your deployment system can automatically promote it to staging and validate it. If validation passes, it can deploy to production without human approval. If a new minor version is released, the system can promote it to staging but require human review before production deployment. If a new major version is released, the system can notify the team but not promote it automatically.

This tiered approach reduces deployment friction for low-risk changes while maintaining control for high-risk changes. Patch versions deploy quickly because they are low risk. Minor versions deploy with lightweight review. Major versions deploy only after thorough validation. The version number itself encodes the risk level, so the deployment system can make decisions based on it.

Version comparison also matters for rollback decisions. If production is running version 2.3.1 and a quality issue is detected, should you roll back to 2.3.0, 2.2.5, or 1.9.4? The semantic version numbers provide guidance. If the issue was introduced in 2.3.0, rolling back to 2.2.5 is safer than rolling back to 2.3.0. If 2.3.1 was supposed to be a patch that fixed an issue in 2.3.0 but actually made it worse, rolling back to 2.3.0 might still be better than rolling back to 2.2.5 if 2.3.0 has important capability additions.

The version history creates a decision tree. Each version is a checkpoint with known properties. When something goes wrong, you can navigate the tree to find the last known good version. If you do not have semantic versioning, this navigation is guesswork. You are choosing between artifacts based on timestamps and vague memory of what changed when. Semantic versioning makes the decision tree explicit and navigable.

## Communicating Version Changes to Downstream Systems

Version numbers are metadata. They need to travel with the model through every environment and be accessible to downstream systems. When a model is deployed, the deployment system should record what version is running. When a downstream system makes a request to the model, the response should include the version number in the metadata. This allows downstream systems to detect version changes and adjust behavior if needed.

Some downstream systems are version-aware. They know that model 2.x.x has certain characteristics and model 3.x.x has different characteristics. They might route requests differently, apply different thresholds, or enable different features based on the model version. If the model version changes and downstream systems are not notified, they will continue using assumptions that are no longer valid. Version metadata prevents this.

Version changes should also trigger notifications. When a new major version is deployed, systems that depend on the model should receive an alert. The alert should include what changed, what the new version's quality metrics are, and what actions might be needed. When a minor version is deployed, a notification is still useful but less urgent. When a patch version is deployed, notification might be optional, or it might be logged for audit purposes without active alerts.

The versioning scheme is the contract between the model team and the systems that depend on the model. The contract says: we will tell you when breaking changes happen through major version bumps, we will tell you when improvements happen through minor version bumps, and we will tell you when operational refreshes happen through patch version bumps. If you enforce this contract, downstream teams can build automation and trust it. If you break this contract by shipping major changes as minor versions, downstream teams will stop trusting your version numbers and revert to manual validation for every change.

Semantic versioning is not just a numbering scheme. It is a communication protocol that makes model evolution legible to both humans and machines. The next step is building the infrastructure that stores, catalogs, and serves versioned model artifacts reliably.

