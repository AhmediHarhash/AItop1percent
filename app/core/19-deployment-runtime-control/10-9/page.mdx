# 10.9 — Deployment Scheduling: When to Deploy and When to Avoid

In November 2025, a logistics company deployed a new Claude Opus 4.5 routing model at 4:00 PM on the Friday before Thanksgiving. The deployment used a careful canary strategy and passed all automated checks. By 6:00 PM, the engineering team had gone home for the long weekend. At 9:00 AM Saturday, the model began hallucinating package locations for a specific category of international shipments—a failure mode that only manifested under Saturday morning traffic patterns. Customer support calls spiked. The on-call engineer could see the problem but lacked deployment permissions and context to roll back safely. The senior engineer who had pushed the deployment was unreachable, traveling with family. The model continued serving hallucinated data until Monday afternoon, affecting 23,000 shipments and costing the company $890,000 in customer credits and expedited re-routing. The technical failure was minor. The timing failure was catastrophic.

Deployment timing is not a soft concern or a scheduling preference. It is a hard operational constraint that determines whether you can respond to failures at all. A perfect deployment strategy executed at the wrong moment becomes a crisis waiting to happen. The Friday afternoon deployment is the canonical antipattern, but it is far from the only one. High-traffic periods, change freezes, holiday windows, product launches, time zone boundaries—every deployment exists in a temporal context that either enables or prevents effective response. The question is not just whether your deployment will succeed. The question is whether you can fix it quickly if it fails.

## The Friday Rule and the Response Window

The Friday rule is simple: do not deploy changes that you cannot monitor, debug, and roll back with full team availability. Friday afternoon deployments violate this rule because the team disperses before failure patterns have time to emerge. The problem is not that Friday deployments always fail. The problem is that when they do fail, the response capability drops to near zero just as the failure begins to propagate.

A deployment that completes at 3:00 PM Friday has approximately two hours of full team coverage. Most AI failures do not manifest immediately. They appear as traffic shifts, as edge cases accumulate, as caches expire and new data arrives. A model that looks healthy at 4:00 PM may degrade visibly by 7:00 PM, when only the on-call engineer remains available. That engineer faces the deployment alone—no teammates to consult, no deployment author to clarify intent, no leadership to authorize aggressive rollback. The decision to wait until Monday often feels safer than making a unilateral rollback decision, even when waiting guarantees greater user harm.

The same logic applies to any deployment that reduces response capacity. Deploying before team members leave for conferences, before long holiday weekends, before scheduled vacations—every scenario where the people who understand the change will not be available to respond to its failures. The deployment itself may be low risk. The timing makes the risk unmanageable. Teams that consistently deploy on Tuesday or Wednesday mornings preserve response windows. Teams that deploy late Friday create time bombs.

## High-Traffic Periods and Peak Load Deployment

Deploying during high-traffic periods trades deployment safety for operational necessity. Peak traffic reveals failure modes that testing cannot replicate, but it also amplifies the impact of those failures. A model that degrades slightly during off-peak hours might degrade catastrophically during peak hours when request volume is ten times higher and user tolerance is ten times lower.

The conservative approach is to never deploy during known peak periods—avoid Black Friday for retail systems, avoid tax season for financial systems, avoid evenings and weekends for consumer applications. This approach maximizes safety but creates deployment scarcity. If every peak period is off-limits, and every post-peak period must be reserved for cleanup and monitoring, deployment windows shrink to a few hours per week. Product velocity suffers.

The pragmatic approach is to tier deployments by risk and schedule accordingly. Low-risk changes—prompt wording adjustments, response format tweaks, configuration parameter updates—can deploy during moderate traffic periods because their failure modes are well-understood and rollback is fast. Medium-risk changes—new model versions, routing logic changes, retrieval updates—deploy during off-peak hours with extended monitoring before the next peak. High-risk changes—new architectures, major prompt rewrites, multi-layer coordinated deployments—deploy only during protected windows when traffic is low and the full team is available for at least six hours post-deployment.

A media company runs this tiering explicitly. Low-risk changes deploy anytime between 8:00 AM and 4:00 PM local time, avoiding only the exact peak traffic window from 6:00 PM to 10:00 PM. Medium-risk changes deploy Tuesday through Thursday mornings before 10:00 AM, ensuring eight hours of monitoring before the evening peak. High-risk changes deploy only on Tuesday mornings after a planning meeting, with the entire AI team committed to monitoring until 6:00 PM and on-call until midnight. The system is not perfect—urgent changes sometimes violate the schedule—but it prevents the casual Friday afternoon deployment that has no justification except convenience.

## Change Freeze Windows and Stability Guarantees

Change freeze windows are periods when no deployments occur, regardless of urgency or apparent safety. These windows exist to provide stability guarantees during moments when the cost of any failure—even a minor, quickly-resolved failure—exceeds the value of any improvement. The classic example is the retail holiday freeze: no AI model changes from mid-December through early January. The models running on December 15 are the models that will run through New Year's Day.

Change freezes frustrate engineers. A bug discovered on December 20 cannot be fixed until January 2, even if the fix is trivial and the bug is causing user complaints. A prompt improvement that would lift conversion rates by two percent cannot deploy until after the highest-traffic period of the year. The freeze feels like self-imposed stagnation. But the freeze exists because the alternative is worse. A deployment that goes wrong on December 23 affects millions of users during the most critical business period, with minimal team availability to respond. The two percent improvement is not worth the risk of a five percent degradation.

Not every business needs holiday freezes, but every business has periods when stability matters more than improvement. A tax preparation service freezes changes from April 1 through April 18. A university admissions system freezes from late March through mid-May. A payroll system freezes during the first week of each month. These freezes do not ban all technical work—they ban production deployments. Engineers still fix bugs, improve prompts, test new models—but the changes wait in staging until the freeze lifts.

The discipline required to maintain a freeze is non-trivial. Product managers request exceptions. Engineers argue that a particular change is too safe to wait. Leadership asks why a known improvement cannot deploy immediately. The freeze holds only if someone with authority refuses every exception. The moment the first exception is granted, the freeze becomes negotiable, and negotiable freezes collapse. A fintech company learned this in early 2025 when a "low-risk prompt update" deployed during a freeze, caused a five percent increase in false positive fraud flags, and blocked 14,000 legitimate transactions on the last day of the quarter. The change was indeed low-risk in isolation. But low-risk changes still fail, and during a freeze, any failure is unacceptable.

## Scheduled Deployments and Predictable Cadence

Scheduled deployments occur at the same time every week, creating predictability for both engineering teams and dependent systems. A healthcare company deploys every Tuesday at 9:00 AM. A logistics company deploys every Wednesday at 2:00 PM. A financial services company deploys twice per week, Tuesday and Thursday mornings. The schedule is public, visible to every team in the organization, and changes only under extraordinary circumstances.

The advantage of scheduled deployments is coordination. Dependent teams know when changes might occur and can plan monitoring accordingly. Customer support knows which days to expect potential issues and can staff accordingly. Product managers know when features will reach production and can plan launches around the deployment schedule. The deployment becomes a known event, not a surprise.

The disadvantage is inflexibility. A bug discovered on Wednesday cannot be fixed until the following Tuesday. A prompt improvement that would lift metrics immediately must wait four days. Urgent changes require breaking the schedule, which undermines the predictability the schedule was meant to provide. Teams that adopt scheduled deployments must also define an emergency deployment process for the inevitable cases when waiting until the next scheduled window is unacceptable.

A travel booking company balances scheduled and emergency deployments by defining clear criteria for emergency status. An emergency deployment is one where waiting until the next scheduled window would cause direct financial loss exceeding $50,000, user data exposure, or complete feature unavailability. Everything else waits. The criteria are ruthlessly enforced. A prompt degradation that drops user satisfaction scores by three percent is not an emergency—it waits until Tuesday. A model failure that prevents users from completing bookings is an emergency—it deploys immediately, regardless of schedule. The clarity prevents arguments. The team knows the rule and applies it without negotiation.

## Event-Aware Scheduling and External Coordination

Event-aware scheduling means avoiding deployments before known external events that increase risk or reduce response capacity. A model deployment scheduled for the afternoon before a major product launch is event-unaware. The product launch will drive a traffic spike, introduce new usage patterns, and consume the attention of every team in the company. A model failure during the launch will be catastrophic, and the team's ability to respond will be compromised because everyone is focused on the launch itself.

Event-aware scheduling pushes the deployment either before the event with sufficient time to validate stability, or after the event when attention and response capacity return. A consumer electronics company deploys no AI changes in the 72 hours before a new product announcement. The models that run during the announcement are the models that have been running stably for at least three days. If a change must deploy before the announcement—because the announcement itself depends on new AI capabilities—the change deploys at least a week in advance, with daily validation that it remains stable as traffic builds.

Marketing campaigns create similar constraints. A campaign that drives a traffic spike to a specific feature should not coincide with a deployment that changes that feature. Either the deployment happens before the campaign with time to validate, or it happens after the campaign completes. The coordination required is non-trivial. Marketing teams often finalize campaign timing days before launch. Engineering teams need advance notice to schedule deployments safely. A shared calendar where both teams publish significant events at least two weeks in advance is the minimum coordination infrastructure.

A subscription service uses a formal deployment coordination process. Every Monday, product, marketing, and engineering teams review the upcoming three weeks. Product announces planned feature launches. Marketing announces planned campaigns. Engineering announces planned deployments. Conflicts are identified and resolved. If a deployment and a campaign target the same week, either the deployment moves earlier, the campaign moves later, or both teams agree that the deployment is low-risk enough to coexist with the campaign. The meeting is tedious, but it prevents the collision between an AI model update and a million-dollar ad spend that happened twice in 2024 before the process existed.

## Time Zone Considerations and Global Deployment

Time zone considerations turn deployment scheduling from a local problem into a global coordination problem. A company with users in North America, Europe, and Asia has no universal off-peak period. When it is 3:00 AM in California, it is noon in Berlin and evening in Tokyo. A deployment scheduled for "low traffic" in one region is a deployment during peak traffic in another.

Global deployments require either compromise or replication. The compromise approach is to deploy during the lowest global traffic period, accepting that no time is perfect. For many consumer applications, early morning UTC—late night in the Americas, early morning in Europe, afternoon in Asia—represents the global traffic minimum. The deployment happens when the fewest users are active, even though users in every region are affected.

The replication approach is to deploy region by region, starting with the lowest-traffic region and progressing to the highest. A model update deploys first to Asia-Pacific at 2:00 AM local time, then to Europe at 3:00 AM local time, then to North America at 3:00 AM local time. Each region is monitored before the next region begins deployment. If the Asia-Pacific deployment fails, the Europe and North America deployments are cancelled. The approach requires regional infrastructure—separate model endpoints per region, regional traffic routing, regional rollback capability—but it prevents a single global deployment from affecting every user simultaneously.

A ride-sharing company uses regional deployment with strict validation gates. The first deployment happens in Australia and New Zealand, where traffic volume is smallest. If the deployment remains stable for four hours, it proceeds to Southeast Asia. After another four hours of stability, it proceeds to Europe. After another four hours, it proceeds to North America. The entire global rollout takes 18 hours from start to finish, but each region has an isolated failure boundary. A problem in Australia affects 200,000 users. A problem in North America would affect 12 million users. The 18-hour rollout is worth the risk reduction.

The time zone problem also affects response capacity. A deployment that completes at 5:00 PM in San Francisco completes at 1:00 AM in London and 9:00 AM in Tokyo. If the deployment requires monitoring by the team that built it, and that team is based in San Francisco, the deployment must complete early enough in the California day that the team can monitor for several hours before leaving for the evening. Deploying at 5:00 PM California time means the team goes home at 6:00 PM, leaving the deployment unmonitored through its highest-risk period. Deploying at 10:00 AM California time gives the team eight hours of monitoring before end-of-day.

## Emergency Deployments and Breaking the Rules

Emergency deployments violate every scheduling best practice because the alternative is worse. A model producing hallucinated medical advice cannot wait until Tuesday morning. A routing model directing users to incorrect financial data cannot wait until after the holiday freeze. A prompt leaking user information cannot wait until the next scheduled deployment window. The emergency is defined not by engineering convenience but by user harm that is occurring now and will continue until the deployment completes.

Emergency deployments require a different process than scheduled deployments. The criteria for emergency status must be explicit and ruthlessly enforced. User harm, data exposure, complete feature unavailability, direct financial loss exceeding a defined threshold—these are emergencies. Degraded metrics, slow response times, minor user complaints—these are not emergencies. They wait.

Once an emergency is declared, the deployment process accelerates but does not skip safety steps. Automated checks still run. Canary deployments still occur. Monitoring still happens. The difference is that human approval steps are expedited, response teams are assembled immediately, and rollback authority is pre-granted to the on-call engineer. A deployment that would normally take four hours from decision to completion takes 45 minutes, not because steps are skipped but because every step happens in parallel with maximum urgency.

A healthcare platform defines three emergency tiers. Tier 1 emergencies—patient safety risk, data exposure—trigger immediate all-hands response and deploy within 30 minutes. Tier 2 emergencies—complete feature unavailability, financial loss exceeding $100,000 per hour—deploy within two hours with senior engineer approval. Tier 3 emergencies—significant user impact but no safety or financial threshold met—follow an expedited process but still require normal approval and wait for the next business-hours window if discovered after hours. The tiers are written into the incident response runbook and enforced by the on-call rotation lead.

Emergency deployments that occur during normally restricted windows—Friday evenings, holiday freezes, product launch blackouts—must be followed by a post-incident review that validates the emergency criteria were met. If the review finds that the deployment was not actually an emergency, the process is revised to prevent future false emergencies. A false emergency deployment erodes trust in the scheduling rules and makes future rule violations more likely.

## The Deployment Calendar and Cross-Team Coordination

The deployment calendar is a shared view of when every team plans to deploy changes, when external events create deployment restrictions, and when windows are protected for stability. The calendar is not a suggestion—it is the authoritative source for deployment timing across the organization. Product teams consult it before planning launches. Marketing teams consult it before scheduling campaigns. Engineering teams publish deployment plans to it at least one week in advance.

The calendar must be visible, authoritative, and enforced. Visible means every stakeholder can view it without requesting access. Authoritative means when a deployment appears on the calendar, it will happen unless an emergency intervenes. Enforced means deployments that are not on the calendar do not happen, except under the explicit emergency criteria. A calendar that is visible but not authoritative becomes a wish list. A calendar that is authoritative but not enforced becomes ignored.

A financial services company uses a shared deployment calendar integrated with Slack and email notifications. When a team adds a deployment to the calendar, dependent teams receive automatic notifications. When a deployment is within 24 hours, a reminder goes to the deploying team, the on-call rotation, and the customer support lead. When a deployment completes, the calendar updates to show completion time and health status. The calendar is the single source of truth. Questions about "when is the next model update" or "why did this behavior change" are answered by consulting the calendar.

The calendar also tracks change freeze windows, high-traffic periods, and external events. The December holiday freeze appears on the calendar in June. Black Friday appears in July. Tax season appears in October. Product launches are added as soon as dates are confirmed. The engineering team can see months in advance when deployments will not be possible and can plan accordingly. The product team can see which weeks are already scheduled for deployments and can avoid planning launches that conflict.

The calendar is maintained by a designated owner—usually a senior engineer or engineering manager—who has authority to resolve conflicts and enforce the rules. When two teams want to deploy in the same window, the owner decides priority based on user impact, risk level, and business need. When a team requests an exception to a freeze, the owner evaluates whether the exception criteria are met. The owner is not popular, but the system requires someone who can say no.

Deployment timing is not a scheduling problem. It is a response-capacity problem disguised as a calendar. The question is not when you want to deploy—it is when you can afford to respond if the deployment fails, and whether the organization is structured to support that response with the attention, authority, and availability required to fix problems before they become disasters.

The coordination required to avoid multi-layer deployment collisions is the subject of the next subchapter, where the complexity of deploying models, prompts, and configurations together reveals why even perfect deployments fail when their dependencies are not synchronized.

