# 6.4 — KV-Cache Serving Strategies: Reusing Computation Across Requests

The KV-cache is where most inference computation goes. Reusing it across requests is the biggest optimization you can make. A typical LLM inference spends seventy to eighty-five percent of its time computing key-value pairs for attention layers. Every token in the input—system prompt, few-shot examples, conversation history—requires computing these pairs once per transformer layer. For a model with 32 layers processing a 500-token prompt, that is sixteen thousand key-value pair computations. If a hundred users send requests with the same 500-token system prompt, you compute those sixteen thousand pairs a hundred times, burning GPU cycles and money on redundant work.

KV-cache serving eliminates that redundancy. It stores the key-value pairs computed for shared prompt prefixes and reuses them across requests. The first request pays full computation cost. Every subsequent request with the same prefix reuses the cached KV pairs and only computes the suffix—the variable part of the prompt. A system prompt shared by ten thousand requests gets computed once and reused ten thousand times. The result is two to five times higher throughput and two to five times lower cost per request for workloads with shared prefixes.

## What the KV-Cache Is

The KV-cache is the internal state computed during transformer attention. For each token in the input, the model computes a key vector and a value vector at each layer. These vectors are used to calculate attention weights—how much each token should attend to every other token. The computation is expensive because it happens at every layer for every token. For a 7B parameter model with 32 layers, computing KV pairs for one token requires hundreds of millions of floating-point operations.

Once computed, the KV pairs are stored in GPU memory and reused within a single request. When generating token N, the model uses the KV pairs from tokens 1 through N minus 1 to compute attention. This avoids recomputing attention from scratch for every token, which would be impossibly slow. The KV-cache is what makes autoregressive generation feasible—without it, generating a 100-token response would require computing attention over the full sequence 100 times.

Traditional serving infrastructure discards the KV-cache after each request completes. The next request computes its KV pairs from scratch, even if it shares a system prompt with the previous request. This is simple to implement—each request is stateless—but wasteful. Modern serving frameworks preserve the KV-cache across requests and match new requests to cached prefixes. If a new request shares the first 300 tokens with a previous request, the framework loads the KV-cache for those 300 tokens and only computes the remaining tokens. The new request skips seventy percent of the computation.

The technical challenge is memory management. The KV-cache for a single request can consume several gigabytes of GPU memory depending on model size and prompt length. Storing KV-caches for thousands of requests is impossible. The serving framework must decide which KV-caches to keep, which to evict, and how to match incoming requests to cached prefixes. This is the core problem that prefix caching solves.

## Why KV-Cache Matters

KV-cache reuse has the largest performance impact of any inference optimization. It is not a marginal gain. It is a step-function improvement in throughput and cost. A system serving ten thousand requests per day with a shared 400-token system prompt saves thirty-two million KV-pair computations per day if it reuses the prefix cache. That translates to forty to sixty percent lower GPU utilization and forty to sixty percent lower cost per request.

The performance improvement grows with prefix length. A 100-token shared prefix saves ten percent of computation. A 1000-token shared prefix saves fifty percent or more. Systems with large system prompts, extensive few-shot examples, or retrieval-augmented generation—where retrieved documents are embedded in prompts—see the biggest gains. These systems often have prompts exceeding 2000 tokens, with only the final 100 to 300 tokens varying per request. KV-cache reuse makes them economically viable.

The second benefit is latency reduction. Reusing cached KV pairs eliminates the time spent computing them. For a 500-token prefix on a mid-size model, that saves fifty to two hundred milliseconds depending on hardware. The latency improvement is most noticeable for short responses—if the response is only 50 tokens, cutting fifty milliseconds from prompt processing is significant. For long responses, the relative impact is smaller but still measurable.

The third benefit is throughput increase. GPU utilization is the bottleneck for most inference workloads. If you can serve more requests per GPU by eliminating redundant computation, you need fewer GPUs to handle the same load. A system that handles one hundred requests per second per GPU without KV-cache reuse might handle two hundred requests per second with it. That halves your infrastructure cost. The throughput gain depends on how much of each request is shared prefix—the higher the prefix ratio, the greater the gain.

## Prefix Caching Fundamentals

Prefix caching stores the KV-cache for prompt prefixes that are shared across requests. A prefix is any contiguous sequence of tokens at the start of a prompt. If two requests begin with identical token sequences, they share a prefix. The serving framework computes the KV-cache for that prefix once, stores it in GPU memory, and reuses it for both requests.

The matching logic compares token sequences, not text. Two prompts with identical text but different tokenization do not share a prefix. This matters because tokenization depends on model vocabulary and tokenizer configuration. If you change tokenizers between requests, prefix matching fails even if text is identical. The safe approach is to tokenize prompts consistently and match based on token IDs.

The granularity of prefix matching varies by implementation. Some frameworks match only full system prompts—all or nothing. If the system prompt is 500 tokens, requests either match all 500 tokens or miss entirely. Other frameworks match at arbitrary token boundaries—if two prompts share the first 473 tokens, the cache stores those 473 tokens even if token 474 differs. The finer the granularity, the higher the cache hit rate, but the more complex the memory management.

The eviction policy determines which cached prefixes remain in memory when space is limited. LRU—least recently used—evicts the prefix that has gone longest without being reused. This works well for workloads with temporal locality, where recently used prefixes are likely to be reused soon. LFU—least frequently used—evicts the prefix with the fewest historical accesses. This works well for workloads with frequency skew, where some prefixes are accessed constantly and others rarely. Most serving frameworks default to LRU because it is simpler and performs adequately across diverse workloads.

The cache size limit is determined by available GPU memory. The KV-cache competes with model weights, activation memory, and output buffers for GPU RAM. On a GPU with 40GB memory, you might allocate 20GB to model weights, 10GB to activations, and 10GB to KV-cache. That 10GB holds dozens to hundreds of cached prefixes depending on prefix length and model size. When the cache fills, the framework evicts prefixes according to the eviction policy. The result is that only hot prefixes—frequently accessed ones—remain cached.

## vLLM Prefix Caching and PagedAttention

vLLM is the leading open-source serving framework for LLMs, and it pioneered practical prefix caching through a technique called PagedAttention. PagedAttention divides the KV-cache into fixed-size blocks—pages—and manages them like virtual memory in an operating system. Each page holds the KV pairs for a fixed number of tokens, typically 16 or 32. The framework stores pages in GPU memory and tracks which pages belong to which requests.

When a new request arrives, vLLM tokenizes the prompt and computes a hash of the token sequence. It checks whether any cached pages match the prefix. If the first 256 tokens of the prompt match a cached prefix, vLLM loads the KV-cache pages for those 256 tokens and computes only the remaining tokens. If multiple requests share the same prefix, they reference the same pages in memory—there is no duplication. One copy of the cached prefix serves thousands of requests.

The page-based representation is what makes this efficient. Instead of storing one monolithic KV-cache per request, vLLM stores a pool of pages that are shared across requests. A request is represented as a sequence of page references. If two requests share a prefix, their page sequences begin identically and diverge only where the prompts differ. This copy-on-write approach minimizes memory usage and maximizes sharing.

The eviction policy in vLLM is LRU at the page level. When GPU memory fills, vLLM evicts the least recently used pages. Because pages are shared, evicting a page affects every request that references it. Those requests must recompute the KV pairs for the evicted tokens. The framework prioritizes evicting pages that are referenced by few requests or by requests that have not been accessed recently. This keeps the hottest prefixes in cache and evicts the cold tail.

The performance gains are substantial. Benchmarks show that vLLM with prefix caching enabled achieves two to five times higher throughput than without it for workloads with shared system prompts. The gain depends on prefix length and traffic patterns. A workload where ninety percent of requests share a 1000-token system prompt sees close to five times improvement. A workload with diverse prompts and little sharing sees minimal improvement. The key is structuring your prompts to maximize prefix sharing.

## System Prompt Optimization

System prompts are the most cacheable part of most prompts. They are static or change infrequently. They are shared across all users. They are often long—300 to 1500 tokens—because they include instructions, guidelines, examples, and context. Optimizing system prompts to maximize prefix caching delivers the largest performance gains.

The first optimization is to front-load static content. Place the instructions, guidelines, and examples that never change at the beginning of the prompt. Place the variable content—user query, retrieval results, conversation history—at the end. This ensures the longest possible shared prefix. If your system prompt includes a dynamic date or session ID, move it to the end or remove it entirely. Every token of shared prefix increases cache hit rate.

The second optimization is to version system prompts explicitly. When you update the system prompt, increment a version identifier and treat it as a new prefix. This allows the cache to maintain multiple versions simultaneously during rollout. Old requests continue using the cached old version. New requests compute and cache the new version. After rollout completes, you can evict the old version. This eliminates cache thrashing during deployments.

The third optimization is to extract per-user customization into the suffix. Some systems personalize system prompts with user preferences, role information, or access levels. If this personalization appears at the start of the prompt, it destroys prefix sharing—every user gets a unique prefix. The solution is to place personalization at the end, after the shared instructions. The first 1000 tokens are identical across users and cached. The final 50 tokens are personalized and computed per request.

The fourth optimization is to canonicalize retrieval results. RAG systems embed retrieved documents into prompts. If document order varies between requests, prefix matching fails even if the same documents are retrieved. The solution is to sort documents by a consistent key—relevance score, document ID, or timestamp—before embedding them. This ensures that identical retrieval results produce identical prompt prefixes.

The measurable outcome is prefix hit rate—what percentage of tokens in incoming prompts match cached prefixes. A well-optimized system achieves seventy to ninety percent prefix hit rate, meaning seventy to ninety percent of tokens are served from KV-cache without recomputation. An unoptimized system might achieve ten to thirty percent. The difference is the design of the prompt structure, not the sophistication of the caching infrastructure.

## Multi-Tenant KV-Cache

Multi-tenant systems serve requests from many users or customers on shared infrastructure. Each user submits requests with their own queries, but the system prompt and few-shot examples are often identical. KV-cache reuse across tenants is what makes multi-tenant LLM serving economically viable. Without it, each tenant would require dedicated GPU capacity. With it, thousands of tenants share the same GPUs and benefit from shared KV-cache prefixes.

The isolation requirement is that one tenant cannot observe or infer information from another tenant's cached KV pairs. The KV-cache contains no sensitive information—it is a mathematical representation of token relationships—but you must ensure that caching does not leak query content or response data. The safe approach is to cache only the shared system prompt and few-shot examples, not the tenant-specific query or conversation history. The system prompt is identical across tenants by design, so sharing it is safe.

The performance benefit is dramatic. A multi-tenant system with one thousand tenants and a 800-token shared system prompt computes the KV-cache for that prompt once and reuses it one thousand times. The amortized cost per tenant is one-thousandth of the non-cached cost. This is what allows cloud LLM providers to serve millions of users on a finite number of GPUs. The KV-cache for common prefixes is shared across all users, and only the variable suffix is computed per request.

The operational challenge is cache eviction under load. When a burst of traffic arrives, the KV-cache fills with active request state. The framework must decide whether to evict shared prefixes—which serve many tenants—or tenant-specific suffixes—which serve one tenant. Evicting shared prefixes harms throughput for all tenants. Evicting suffixes harms latency for individual requests. The optimal policy prioritizes keeping shared prefixes in cache and evicting suffixes when memory is tight. Most serving frameworks implement this automatically.

## Cache Size Management

GPU memory is limited, and the KV-cache competes with other memory consumers. The model weights are fixed—they occupy a constant amount of memory. The activation memory grows with batch size and sequence length. The KV-cache grows with the number of concurrent requests and the length of their shared prefixes. When memory is exhausted, the framework must evict cached prefixes or reduce batch size. Both reduce throughput.

The trade-off is between KV-cache size and batch size. A larger KV-cache allows more prefixes to remain cached, increasing hit rate and reducing computation per request. A larger batch size allows more requests to be processed in parallel, increasing throughput. The optimal balance depends on workload characteristics. For workloads with high prefix sharing, allocating more memory to KV-cache is optimal. For workloads with low prefix sharing, allocating more memory to batch size is optimal.

The measurement you need is cache hit rate and eviction rate. Cache hit rate measures what percentage of tokens in incoming requests match cached prefixes. Eviction rate measures how often cached prefixes are evicted due to memory pressure. If eviction rate is high and cache hit rate is low, you are churning the cache—evicting prefixes before they can be reused. The solution is to reduce cache size and increase batch size, accepting lower caching benefit in exchange for higher parallelism.

If eviction rate is high and cache hit rate is also high, you are evicting valuable prefixes prematurely. The solution is to increase cache size by reducing batch size, reducing model precision—switching from FP16 to FP8 or INT8—or upgrading to GPUs with more memory. The choice depends on budget and latency tolerance. Reducing batch size increases latency for queued requests. Reducing precision can degrade output quality. Upgrading GPUs increases cost. You measure the trade-offs and choose the option that best fits your SLA and budget.

The dynamic approach adjusts cache size based on load. During low-traffic periods, the framework allocates most memory to KV-cache, maximizing hit rate. During high-traffic periods, it reduces cache size to allow larger batches, maximizing throughput. This adaptive allocation requires monitoring memory utilization and adjusting configuration in real time. Most production systems use static allocation because dynamic adjustment is complex and risks thrashing—frequent resizing that degrades performance.

## Measuring KV-Cache Hit Rates and Throughput Impact

Hit rate measures the percentage of tokens served from cached KV pairs. A hit rate of seventy percent means that seventy percent of prompt tokens are matched to cached prefixes and only thirty percent require fresh computation. Hit rate is the primary metric for evaluating prefix caching effectiveness. You measure it per request and aggregate it across all traffic.

The calculation is straightforward. For each request, count the number of tokens in the prompt. Count the number of tokens matched to cached prefixes. Divide matched tokens by total tokens to get per-request hit rate. Average across all requests to get system-wide hit rate. A system-wide hit rate above sixty percent indicates effective prefix caching. Below thirty percent indicates poor cache design or high prompt diversity.

Hit rate alone does not tell the full story. A high hit rate on short prompts saves less computation than a lower hit rate on long prompts. Weighted hit rate accounts for this by multiplying hit rate by prompt length. A request with a 500-token prompt and eighty percent hit rate contributes 400 cached tokens. A request with a 100-token prompt and ninety percent hit rate contributes 90 cached tokens. Weighted hit rate sums these contributions across all requests and divides by total tokens processed. This metric reflects actual computation saved.

Throughput impact measures how much prefix caching increases requests per second per GPU. You benchmark throughput with caching enabled and disabled, keeping all other variables constant. The ratio tells you the throughput multiplier. A system that handles 100 requests per second without caching and 250 requests per second with caching has a 2.5 times throughput multiplier. The multiplier depends on hit rate, prefix length, and model size. Larger models and longer prefixes see higher multipliers.

Latency impact measures how much prefix caching reduces time-to-first-token. TTFT is the latency from request arrival to first generated token. It includes prompt processing time. Prefix caching reduces prompt processing time by eliminating KV computation for cached tokens. A prompt with 1000 tokens and eighty percent hit rate computes only 200 tokens, cutting TTFT by sixty to seventy-five percent depending on model and hardware. You measure TTFT percentiles with and without caching to quantify the improvement.

The dashboard surfaces these metrics continuously. You track hit rate, weighted hit rate, throughput, and TTFT percentiles in real time. You alert when hit rate drops below baseline, indicating cache eviction or traffic shift. You alert when throughput drops despite stable hit rate, indicating memory pressure or infrastructure issues. The goal is to maintain high hit rate and high throughput simultaneously, and the monitoring system tells you when those goals are drifting.

KV-cache serving strategies are the most impactful optimization for LLM inference at scale. But even perfect KV-cache reuse cannot help if the cache keys themselves are poorly designed—and that is the final piece of the caching puzzle.
