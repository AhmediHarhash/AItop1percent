# 5.2 — Cost-Aware Routing: Cheapest Model That Meets Quality Bar

The right model is the cheapest one that meets your quality bar. Every other choice is waste. This principle is obvious when stated plainly, but most production systems violate it constantly. They route queries to expensive models that offer no marginal quality improvement for that specific input, or they route to cheap models that fail to meet minimum quality thresholds and force expensive downstream corrections. Cost-aware routing solves both problems by making model selection a function of input characteristics and measured quality, not a static configuration.

The core insight is that query complexity varies exponentially, but model pricing is linear within a tier. A simple factual query costs the same to process on Claude Opus 4.5 as a complex multi-turn reasoning task, even though the simple query could have been handled by Claude Haiku 4.5 at one-tenth the cost. Cost-aware routing identifies which queries can be served by cheaper models without quality degradation and routes them accordingly. The queries that genuinely need premium models still get them. The rest pay only for the capability they actually require.

## The Cost-Aware Routing Decision

Cost-aware routing requires three components: an input classifier that estimates query complexity, a quality bar that defines acceptable performance for each query type, and a routing table that maps complexity estimates to model tiers. The input classifier examines each incoming request and assigns it a complexity score. The quality bar specifies the minimum quality threshold that must be met. The routing table selects the cheapest model that is predicted to exceed the quality bar for that complexity level.

Input classification is not a binary decision. You are not simply categorizing queries as "simple" or "complex." You are estimating where a query falls on a continuous spectrum of difficulty, then bucketing it into tiers that correspond to available model options. A five-tier system might classify queries as trivial, simple, moderate, complex, or expert-level. A three-tier system might use simple, standard, and complex. The number of tiers depends on how many distinct model options you have available and how granular your cost-quality tradeoffs need to be.

The routing decision happens before the model sees the query. You cannot afford to send every request to multiple models and pick the best result — that would multiply your costs instead of reducing them. The classifier must make a prediction based on observable input features: token count, detected task type, user tier, query structure, historical patterns for similar requests. These features are fast to compute and provide enough signal to make routing decisions that hold up under real traffic.

## Input Classification Signals

Token count is the simplest and most reliable signal for initial classification. A query with fifty tokens is unlikely to require deep reasoning. A query with five thousand tokens of context almost certainly does. The correlation is not perfect — a fifty-token query asking for a formal proof of a mathematical theorem might be more complex than a five-thousand-token query asking for a summary of a legal document — but token count alone provides enough signal to avoid egregious misrouting.

Detected task type adds a second dimension. You can build lightweight classifiers that recognize common patterns: code generation requests, creative writing prompts, factual lookups, multi-turn conversations, reasoning-heavy tasks. These classifiers do not need to be perfect. They need to be fast, and they need to be right often enough that routing decisions improve on average. A simple keyword-based classifier that looks for phrases like "write a function," "explain why," or "summarize the following" can achieve seventy to eighty percent accuracy with minimal latency overhead.

User tier is a business signal, not a technical one, but it affects cost tolerance. A free-tier user might receive a response from a smaller model even if the query complexity suggests a larger model would perform better. An enterprise customer paying ten thousand dollars per month expects premium quality on every interaction. User tier acts as a floor on model selection. You might route a simple query from a free user to GPT-5-nano, but you would never route a simple query from an enterprise user to anything less than GPT-5-mini or Claude Sonnet 4.5.

Historical quality for similar queries provides ground truth for routing decisions. If you have processed ten thousand queries that match a particular complexity profile, and ninety-five percent of them achieved acceptable quality when routed to Claude Haiku 4.5, you can confidently route future queries with that profile to Haiku. If only sixty percent met the quality bar, you need to route to a higher-tier model. This feedback loop is what makes cost-aware routing improve over time. You are not guessing which model will work. You are using measured data from production traffic.

## Model Tiers and Capability Profiles

Model tiers map to pricing and capability. In 2026, the major providers offer clear tier structures. OpenAI has GPT-5-nano, GPT-5-mini, GPT-5, and GPT-5.1. Anthropic has Claude Haiku 4.5, Claude Sonnet 4.5, and Claude Opus 4.5. Google has Gemini 3 Flash and Gemini 3 Pro. Each tier represents a different balance of cost, latency, and capability. The smallest models are fast and cheap. The largest models are slow and expensive. Your job is to route each query to the smallest model that meets your quality bar.

The cost difference between tiers is significant. Claude Haiku 4.5 costs roughly one-tenth of Claude Opus 4.5 per million tokens. GPT-5-mini costs roughly one-fifth of GPT-5. If sixty percent of your queries can be served by the smaller model without quality loss, you reduce your total inference cost by forty to fifty percent. If you route ninety percent of queries to smaller models and only use the premium tier for genuinely complex requests, cost savings reach sixty to seventy percent.

Capability profiles define what each model tier is good at. Smaller models excel at straightforward tasks: summarization, factual retrieval, pattern matching, simple Q&A. They struggle with multi-step reasoning, ambiguous instructions, tasks requiring deep domain knowledge, and queries where the correct answer is not obvious from the input. Larger models handle all of this plus edge cases, creative reasoning, and tasks where the user's intent must be inferred from limited context. Cost-aware routing is the process of matching task characteristics to model strengths.

## Quality Bar Enforcement

The quality bar is not a single threshold. It is a set of thresholds that vary by query type, user tier, and business context. A simple factual query might require ninety-five percent accuracy — the model should retrieve and present the correct information with minimal error. A creative writing request might allow more latitude — the model should produce coherent, engaging text, but there is no single "correct" answer. A high-stakes query from an enterprise customer might require ninety-nine percent accuracy and additional verification before the response is returned.

Quality bar enforcement requires measurement. You cannot route based on predicted quality unless you measure actual quality for routed queries and adjust your routing logic when predictions are wrong. This means instrumenting your system to capture quality metrics by model tier and query type. You need to know, for every routing decision, whether the chosen model met your quality bar. If it did, that routing decision was correct. If it did not, you either need to reclassify that query type or route it to a higher-tier model in the future.

Measurement happens through multiple channels. Automated evals provide fast feedback on a sample of queries. Human review provides ground truth for high-value interactions. User feedback signals quality failures in real time. Production metrics like task success rate, user satisfaction scores, and downstream correction rates all contribute to quality measurement. The key is connecting these signals back to routing decisions so you can tune your classifier and routing table based on real outcomes.

## The Feedback Loop

Cost-aware routing improves over time through a feedback loop. You start with a simple classifier — maybe just token count and a few heuristics. You route queries based on this classifier and measure quality by tier. You discover that queries with fewer than one hundred tokens and no code-related keywords achieve ninety-seven percent quality on Claude Haiku 4.5. You update your routing table to send all queries matching that profile to Haiku. You discover that queries mentioning "Python" or "JavaScript" have a seventy-two percent quality rate on Haiku but a ninety-four percent rate on Claude Sonnet 4.5. You update the classifier to detect code-related keywords and route those queries to Sonnet.

This process continues indefinitely. Every week, you analyze routing decisions and quality outcomes. You identify patterns where your classifier is miscategorizing queries. You identify model tiers that are underperforming for certain query types. You adjust classification logic, routing rules, and quality thresholds. The system becomes more accurate as it processes more traffic. A routing system that starts with seventy-five percent cost savings in month one might reach eighty-five percent cost savings by month six as the classifier learns from real data.

The feedback loop also adjusts to changes in model capabilities and pricing. When a new model is released, you A/B test it against your existing routing decisions. If the new model offers better quality at the same cost or the same quality at lower cost, you update your routing table to prefer it. If a provider changes pricing, you recompute optimal routing decisions based on the new cost structure. Cost-aware routing is not a set-it-and-forget-it system. It is a continuous optimization process that adapts to both your traffic patterns and the external model landscape.

## Cost Savings Examples

In mid-2025, a SaaS company providing an AI writing assistant implemented cost-aware routing after their monthly inference bill reached ninety-three thousand dollars. They analyzed their traffic and found that forty-two percent of queries were simple completions — the user provided a sentence fragment, and the model completed it with one or two sentences. These queries were routed to GPT-5, which cost three dollars per million input tokens and fifteen dollars per million output tokens. They tested these queries on GPT-5-mini, which cost sixty cents per million input tokens and one dollar and eighty cents per million output tokens. Quality was indistinguishable.

By routing simple completions to GPT-5-mini, they saved approximately twenty-eight thousand dollars per month. They then analyzed their moderate-complexity queries — longer completions, rephrasing requests, style adjustments. Twenty-three percent of queries fell into this category. They tested routing these to Claude Sonnet 4.5 instead of Claude Opus 4.5 and achieved equivalent quality at one-third the cost. This saved an additional fifteen thousand dollars per month. They continued to route their remaining thirty-five percent of queries — creative long-form content, complex editing requests, research-heavy writing — to premium models. Total cost savings: forty-three thousand dollars per month, a forty-six percent reduction, with no measurable quality degradation.

A healthcare company providing a clinical decision support tool implemented cost-aware routing with quality safeguards. They categorized queries by clinical complexity. Simple queries — medication dosing for standard cases, basic symptom lookups, common procedure protocols — were routed to Gemini 3 Flash. Moderate queries — drug interaction checks, differential diagnosis support, protocol selection for non-standard cases — were routed to Claude Sonnet 4.5. Complex queries — rare conditions, multi-morbidity cases, scenarios requiring deep reasoning about conflicting evidence — were routed to Claude Opus 4.5 with an additional verification step.

They achieved fifty-eight percent cost savings, but more importantly, they improved quality metrics for complex cases. By reserving their premium model capacity for genuinely difficult queries, they reduced latency for those queries and increased the likelihood that they received thorough, well-reasoned responses. Their clinicians reported higher confidence in the system's recommendations. The cost savings funded additional human review for high-stakes decisions, which further improved safety and user trust.

## The Cost-Quality Frontier

Cost-aware routing is not purely about minimizing cost. It is about operating on the efficient frontier of the cost-quality tradeoff space. Every model tier represents a point on this frontier: more cost buys more quality, less cost accepts lower quality. Your job is to ensure that for every query, you are on the frontier — not spending more than necessary for the quality you need, and not accepting quality below your threshold to save money.

This means rejecting two failure modes. The first failure mode is over-provisioning: routing a query to a model that is more expensive than necessary for the required quality. This is waste. The second failure mode is under-provisioning: routing a query to a model that cannot meet your quality bar, forcing downstream corrections that cost more than using the right model in the first place. Both failures move you off the efficient frontier. Cost-aware routing keeps you on it by matching capability to need.

Operating on the efficient frontier requires continuous measurement and adjustment. Model capabilities change. Pricing changes. Your quality requirements change. Traffic patterns change. The routing decisions that were optimal last month might be suboptimal this month. The feedback loop ensures you stay on the frontier by constantly testing whether your current routing rules are still the best available options.

The next subchapter covers latency-aware routing — the strategy of selecting the fastest model that meets your quality bar when response time matters more than raw capability.

