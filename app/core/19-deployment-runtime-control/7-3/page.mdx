# 7.3 — Percentage Rollout Flags: Gradual Traffic Exposure

You have a new prompt template. It performed well in testing, beat the baseline in your evaluation suite, and looks great in manual review. But you do not want to hit one hundred percent of production users on day one. One evaluation suite cannot capture every edge case that real traffic will surface. You need gradual exposure: route a small percentage of traffic to the new prompt, monitor quality metrics, increase the percentage if metrics hold, and be ready to roll back instantly if they do not. Percentage rollout flags give you this capability. They let you control exactly how much traffic sees a change and adjust that percentage at runtime without deploying anything.

## The Percentage Rollout Pattern

A percentage rollout flag does not just turn a feature on or off. It turns it on for a specified percentage of requests and off for the remainder. At ten percent rollout, one out of every ten users gets the feature enabled. At fifty percent, half do. At one hundred percent, everyone does. You start low, prove the change works, and gradually increase until you reach full adoption.

The typical rollout schedule looks like this: one percent for the first day, five percent for the next two days, twenty-five percent for a week, fifty percent for another week, and then one hundred percent. These numbers are not magic — they reflect a balance between moving quickly and gathering enough data to detect problems. At one percent, you are testing the change with a small population that is large enough to surface common issues but small enough that a failure does not affect many users. At five percent, you are confirming that the change works across more traffic patterns. At twenty-five and fifty percent, you are proving it scales. At one hundred percent, you are done.

The schedule is not automatic. You do not set a flag to "gradually increase to one hundred percent over three weeks" and walk away. You increase the percentage manually after reviewing metrics at each level. You roll out to five percent and wait. You watch error rates, quality scores, latency, user feedback, and any other signal that indicates how the change is performing. If metrics are stable or improved, you increase to the next level. If metrics degrade, you stop and investigate. If metrics degrade badly, you roll back to zero percent immediately.

This manual control is critical. Automatic rollouts sound efficient, but they remove the human judgment that catches problems before they become disasters. A metric might degrade slightly at five percent — not enough to trigger an automatic rollback threshold, but enough to make you concerned. Human review catches this. Automatic rollout would proceed to twenty-five percent and make the problem five times worse before anyone noticed.

## Sticky Assignment: Consistent User Experience

A percentage rollout must give the same user the same flag value across requests. If a user sees the new feature on their first request but not their second, they will be confused. If a user gets the new prompt on one query and the old prompt on the next, they will notice inconsistent behavior. Sticky assignment ensures that once a user is bucketed into the rollout, they stay in that bucket.

The standard technique is to hash the user identifier. Take the user ID, hash it using a consistent hash function, take the result modulo one hundred, and compare it to the rollout percentage. If the hash mod one hundred is less than the rollout percentage, the user is in. Otherwise, they are out. This gives you stable assignment: the same user ID always produces the same hash value, so the user always gets the same flag result as long as the rollout percentage does not change.

When you increase the rollout percentage from ten percent to twenty-five percent, the users who were already in the rollout stay in. The hash values do not change — only the threshold changes. A user whose hash mod one hundred equals twelve was in the rollout at ten percent and remains in at twenty-five percent. A user whose hash mod equals fifteen was not in at ten percent but is now in at twenty-five percent. No user moves from in to out when you increase the percentage. This forward-only movement prevents users from experiencing the change and then losing it, which would be jarring.

Rolling back creates a different behavior. If you roll back from twenty-five percent to ten percent, users with hash values between ten and twenty-five move from in to out. They had the change, and now they do not. This is usually fine during a rollback because you are responding to a problem. The alternative — keeping them in the bad experience — is worse. But it is worth knowing that rollback is not invisible to users who were in the expanded rollout range.

## Bucketing Strategies: What to Hash

User ID is the most common choice for percentage rollout hashing. Every user has a unique ID, and hashing it gives you random distribution across the hash space. Users are randomly assigned to buckets in a stable way. This works well for user-facing features where you want each user to have a consistent experience.

But user ID bucketing does not work for every use case. If you are rolling out a change to an unauthenticated API where there are no user IDs, you need a different bucketing key. Session ID works if you have session tracking. Request ID works if you are okay with the same client getting different flag values on different requests — sometimes useful for backend changes where consistency across requests does not matter.

Anonymous user bucketing is tricky. If you assign a random bucket on the first request and store it in a cookie, you get stable bucketing for that user's future requests. But if the user clears their cookies, they get rebucketed randomly. They might move from the rollout group to the control group or vice versa. This is usually acceptable because cookie clearing is infrequent and the user does not notice the change. The alternative — using IP address as a bucketing key — is worse because IP addresses are not unique identifiers and can represent many users behind a NAT or corporate proxy.

For multi-tenant systems, you sometimes want to bucket by tenant ID rather than user ID. This keeps all users within a tenant on the same flag value. If you are rolling out a new model configuration, you might want Tenant A to be fully on the new config and Tenant B to be fully on the old config rather than splitting individual users within each tenant. This makes debugging easier — if Tenant A reports an issue, you know they are all seeing the new config, so the issue is definitely related to the change. With user-level bucketing, issues are harder to attribute because users in the same tenant see different behaviors.

## Ramp Schedules: Manual vs Automated Increases

Most teams use manual ramp schedules. An engineer or product manager reviews metrics at the current rollout level, decides whether to increase, updates the flag configuration to the new percentage, and monitors the results. The decision to ramp is deliberate and informed by data. This is the safest approach and the one that catches the most problems before they scale.

Automated ramp schedules are possible but risky. You configure the flag system to automatically increase the rollout percentage on a schedule: one percent on day one, five percent on day two, twenty-five percent on day five. The system executes the schedule without human intervention. This sounds convenient, but it removes the safety check. If metrics degrade at five percent, the system still ramps to twenty-five percent unless you have automated rollback logic sophisticated enough to catch the problem.

Some teams use semi-automated ramps: the system proposes the next increase based on a schedule, but a human must approve it. This keeps the efficiency of automation while preserving human oversight. The flag system sends a notification: "Rollout of new-prompt-template has been at five percent for forty-eight hours. Metrics are stable. Approve ramp to twenty-five percent?" An engineer reviews, checks metrics, and clicks approve. The rollout advances. If metrics are not stable, the engineer delays the ramp or rolls back.

The healthcare company used manual ramps for all AI changes. They found that automated ramps created a false sense of security. Someone would configure a rollout, assume the automation would handle it, and stop monitoring. Then a problem would occur at twenty-five percent that could have been caught at five percent if someone had been watching. Manual ramps forced someone to look at metrics before each increase, which caught issues before they scaled.

## Rollout Monitoring: Metrics at Each Tier

Percentage rollouts only work if you can measure whether the change is performing well at each rollout level. You need metrics that compare the rollout cohort to the control cohort. The rollout cohort is the percentage of users seeing the change. The control cohort is everyone else. If the metrics are similar, the change is safe. If the rollout cohort metrics are better, the change is an improvement. If they are worse, you stop the rollout and investigate.

The critical metrics depend on what you are rolling out. For a prompt change, you watch quality scores, user satisfaction, task success rates, and latency. For a model change, you watch the same plus cost per request. For a feature gate, you watch feature usage, error rates, and user engagement. Whatever you are changing, you need metrics that tell you whether the change is having the intended effect and whether it is causing any negative side effects.

Latency is always on the list. Even if the change is not supposed to affect latency, you measure it anyway because unexpected latency increases are a common side effect of AI changes. A new prompt might cause the model to generate longer outputs, increasing time to completion. A new model might have different throughput characteristics. A new feature might add processing steps. Latency degradation can kill user experience even if quality improves, so you watch it at every rollout level.

Statistical significance matters when comparing cohorts. At one percent rollout, your sample size is small. If you see a five percent increase in error rates, that could be noise. You need enough requests at each rollout level to detect real differences from random variance. This is why you do not jump from one percent to one hundred percent. The intermediate levels give you larger sample sizes that let you detect real effects with confidence.

You also watch for rare catastrophic failures. A change might work fine for ninety-nine percent of the rollout cohort but fail completely for one percent. If the failure is silent — the model produces wrong answers but does not throw errors — you will not see it in aggregate error rate metrics. You need quality metrics that catch this. Manual spot-checking is valuable here. Look at a random sample of outputs from the rollout cohort at each level. If you see concerning patterns, investigate even if aggregate metrics look fine.

## Automatic Rollback: Reverting When Metrics Degrade

Some teams implement automatic rollback logic that reverts a rollout to zero percent if key metrics cross failure thresholds. If error rates exceed a certain level, if latency degrades beyond a threshold, or if quality scores drop below a floor, the system automatically sets the rollout percentage to zero. This limits the blast radius of a bad change.

Automatic rollback requires clear thresholds and reliable metrics. You cannot use a metric that is noisy or slow to update. If your quality score takes six hours to compute, automatic rollback based on quality score is useless — the rollout could reach fifty percent before the metric updates and triggers the rollback. You need metrics that update in near real-time and have low variance.

The thresholds must be tuned carefully. Set them too sensitive, and you get false positive rollbacks. A temporary spike in latency due to unrelated infrastructure issues triggers a rollback even though the change is fine. Set them too loose, and you do not catch real problems until they have affected many users. Most teams start with loose thresholds and tighten them over time as they learn what normal variance looks like.

Automatic rollback is a backstop, not a replacement for monitoring. You should still watch metrics manually at each rollout level and decide whether to proceed. Automatic rollback catches disasters that happen outside of your active monitoring window — overnight, during weekends, when no one is watching the dashboard. It does not replace the judgment of looking at metrics and deciding whether the change is working as intended.

## The Soak Period: Holding at Each Level

A soak period is the time you spend at each rollout percentage before increasing to the next level. At five percent, you might soak for two days. At twenty-five percent, a week. At fifty percent, another week. The soak gives you time to accumulate enough data to make confident decisions and to surface issues that do not appear immediately.

Some problems are immediate: error rates spike, latency degrades, users report obvious bugs. These show up within hours of a rollout. Other problems are delayed: a prompt change causes rare edge case failures that only appear in certain contexts, a model change degrades quality for a specific user segment that makes up three percent of traffic. These take days or weeks to surface in sufficient volume to detect.

The soak period must be long enough to surface delayed problems but short enough to maintain momentum. A one-day soak at five percent is too short — you will not catch issues that occur in only one percent of requests. A one-month soak is too long — the rollout drags on forever, and you lose the ability to iterate quickly. Most teams soak for two days at low percentages, a week at mid percentages, and a week or more at high percentages.

Weekend traffic is different from weekday traffic. If you ramp to twenty-five percent on Friday afternoon, you are rolling out to weekend traffic, which may not represent your normal user population. Weekday morning traffic is different from evening traffic. Business hours traffic is different from off-hours traffic. You want your soak period to cover a full traffic cycle so you see the change perform across all traffic patterns before you ramp further.

The healthcare company learned this when they rolled out a new summarization prompt on Friday at 3 PM. They ramped to twenty-five percent and saw stable metrics Friday afternoon and Saturday. On Monday morning, error rates spiked. The prompt failed on a specific type of discharge summary that was common on Mondays when weekend hospitalizations were processed but rare on Fridays. They had not soaked through a full week, so they missed the Monday traffic pattern. They rolled back and extended their soak periods to cover at least one full week at each level to ensure they saw all traffic patterns.

## Percentage Rollouts for AI: What Is Different

Rolling out a code change is different from rolling out an AI behavior change. With code, the change is deterministic. If the code works for ten percent of users, it will work for one hundred percent. With AI, the change is probabilistic. A prompt might work well for the ten percent of traffic that saw it but fail for edge cases in the other ninety percent that you never tested.

This means you cannot assume that success at ten percent guarantees success at one hundred percent. You must monitor at every level. You must soak long enough to accumulate diverse traffic. You must watch for cohort effects where the change works for one user segment but fails for another. Percentage rollouts for AI are not just about scaling — they are about discovering how the change performs across the full distribution of real-world inputs.

Another difference is the cost of rollback. Rolling back a code change is simple: revert the flag, and behavior returns to the previous state. Rolling back an AI change is also simple mechanically, but the user experience can be confusing. If a user had ten conversations with the new prompt and suddenly the prompt changes back, they might notice. Their mental model of how the system behaves has shifted, and now it shifts back. This is usually fine — users adapt — but it is worth knowing that AI rollbacks are more visible than code rollbacks.

Percentage rollouts are powerful, but they apply the same change to a percentage of all users — sometimes you need more precise control, routing specific users or tenants to specific behaviors rather than randomly sampling from the entire population.

