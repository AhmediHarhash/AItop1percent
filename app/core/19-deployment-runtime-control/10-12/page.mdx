# 10.12 — The Deployment Checklist: Pre-Flight, In-Flight, Post-Flight

Pilots use checklists because they work. The same discipline applies to AI deployments. A checklist catches the mistakes that excitement and fatigue cause. The engineer who has deployed the same model update ten times successfully is the engineer most likely to skip a verification step on the eleventh deployment, assuming that familiarity equals safety. The checklist does not trust experience. It does not trust memory. It trusts only verification. Every item is checked, every time, regardless of how routine the deployment feels.

The deployment checklist is not a formality. It is a forcing function that prevents the classes of failures that occur not because the system is complex but because a human forgot to check something simple. The model was tested in staging but not validated against the production data schema. The canary threshold was configured but not tested. The rollback procedure was documented but not rehearsed. The stakeholders were notified about the deployment window but not given the expected completion time. Each of these failures is trivial to prevent and catastrophic when missed. The checklist prevents them by making the invisible visible and the implicit explicit.

## The Checklist Philosophy and Cognitive Load

The checklist philosophy is that critical tasks should not rely on human memory under stress. During a deployment, the engineer is monitoring metrics, responding to alerts, coordinating with stakeholders, and deciding whether to advance or roll back. Cognitive load is high. The engineer's working memory is full. Asking that engineer to also remember whether they verified the rollback automation, checked the alerting configuration, and confirmed stakeholder notification is asking for failure.

The checklist externalizes memory. Instead of remembering what to check, the engineer follows the list. Instead of trusting that nothing was forgotten, the engineer verifies that every item was completed. The checklist does not reduce the engineer's skill or judgment—it frees the engineer to apply skill and judgment to the hard problems by eliminating the need to remember the easy problems.

Checklists work best when they are short, specific, and actionable. A checklist with 50 items is ignored because it takes too long to complete. A checklist with vague items like "ensure system is ready" is useless because readiness is not verifiable. A checklist with specific, binary items—"Verify canary threshold is set to 0.5 percent error rate," "Confirm rollback automation is enabled," "Send deployment start notification to Slack channel"—is used because every item can be checked quickly and definitively.

A healthcare platform uses a three-phase checklist: pre-flight, in-flight, and post-flight. Each phase has five to ten items. Pre-flight verifies that the deployment is ready to start. In-flight verifies that monitoring and response are active during the deployment. Post-flight verifies that the deployment completed successfully and no issues were missed. The total time to complete all three checklists is approximately 15 minutes across a multi-hour deployment. The 15 minutes prevent failures that would cost hours to debug and fix.

## Pre-Flight Checklist and Deployment Readiness

The pre-flight checklist is completed before the deployment starts. It verifies that the environment is ready, the change has been validated, the monitoring is configured, and the team is prepared. The deployment does not begin until every pre-flight item is checked.

The first category of pre-flight items is environment readiness. Verify that staging tests passed. Verify that the new model version is deployed to staging and has been running for at least 24 hours. Verify that the production deployment environment has sufficient capacity to handle the new model. Verify that all dependent services are healthy. Verify that no other deployments are in progress that could conflict. Each of these items is binary. Either staging tests passed or they did not. Either the staging model has been running for 24 hours or it has not. There is no ambiguity.

A fintech company requires that staging tests not only pass but that they pass for three consecutive days before a production deployment is permitted. A single day of passing tests is insufficient because it might represent a lucky day where edge cases did not appear. Three consecutive days of passing tests provide higher confidence that the model is stable. The pre-flight checklist includes this requirement explicitly: "Verify staging tests passed for three consecutive days." If the requirement is not met, the deployment is postponed.

The second category is change validation. Verify that the change has been code-reviewed. Verify that the change has been approved by the relevant stakeholders. Verify that the change is documented in the deployment runbook. Verify that the change has been tested under load. These items ensure that the change itself is ready, not just the environment.

The third category is monitoring and alerting. Verify that health check thresholds are configured. Verify that alerting is enabled for the deployment. Verify that the deployment dashboard is accessible and showing current metrics. Verify that the on-call engineer has been notified and is available. These items ensure that the team can detect and respond to issues during the deployment.

The fourth category is rollback readiness. Verify that the rollback procedure is documented. Verify that rollback automation is enabled and has been tested in staging. Verify that the previous version is still available and can be restored. Verify that the rollback triggers are configured correctly. A deployment without a tested rollback procedure is a deployment that cannot be undone safely.

The fifth category is communication. Verify that stakeholders have been notified of the deployment window. Verify that customer support has been briefed on what is changing. Verify that the deployment start time has been posted to the shared calendar and the Slack channel. Verify that the estimated completion time is documented. Communication failures do not prevent deployment, but they create confusion and unnecessary status questions.

## In-Flight Checklist and Active Monitoring

The in-flight checklist is completed periodically during the deployment, typically at each stage transition—canary start, canary complete, gradual rollout start, gradual rollout milestones. The checklist verifies that monitoring is active, metrics are healthy, and the team is responding to issues.

The first in-flight item is metric verification. Check that error rate is below the threshold. Check that latency is within acceptable range. Check that user-reported issues have not spiked. Check that cost per query is within expected bounds. These checks are redundant with automated alerting, but redundancy is the point. Automated systems fail. An alert that should fire might not fire due to a configuration error. The manual check catches what automation misses.

A logistics company requires that in-flight checks include not just metric values but metric trends. It is not enough to verify that error rate is currently 0.3 percent when the threshold is 0.5 percent. The check must also verify whether error rate is increasing, stable, or decreasing. An error rate increasing from 0.2 percent to 0.3 percent over ten minutes is concerning even though it is below the threshold. An error rate steady at 0.3 percent for 30 minutes is less concerning. The in-flight checklist includes: "Verify error rate is below threshold AND not increasing."

The second in-flight item is alert status. Check whether any alerts have fired since the last check. If alerts fired, verify that they were acknowledged and investigated. Verify that resolved alerts actually resolved due to transient issues and not due to alerting system failures. An alert that appears to resolve automatically but actually stopped firing because the alerting system lost connectivity is a false resolution.

The third in-flight item is deployment progress. Verify that the deployment is advancing on schedule. If the canary was expected to complete in 30 minutes and it has been 45 minutes, investigate why. If the gradual rollout was expected to reach 50 percent by now and it is still at 25 percent, investigate why. Delays are signals. Even if metrics are healthy, a deployment that is not advancing on schedule may be encountering subtle issues.

The fourth in-flight item is communication. Verify that stakeholders have been updated on deployment progress. If the deployment was expected to complete by 2:00 PM and it is now 2:30 PM with 50 percent remaining, stakeholders must be notified of the delay. Silence during delays creates anxiety. Updates create confidence.

The fifth in-flight item is rollback readiness. Verify that rollback is still possible. Verify that rollback automation has not been disabled. Verify that the on-call engineer is still available. These checks are paranoid, but paranoia is appropriate during deployments. A deployment that reaches 90 percent complete and then discovers that rollback is no longer possible due to a configuration change is a deployment in crisis.

## Post-Flight Checklist and Completion Verification

The post-flight checklist is completed after the deployment finishes, whether the deployment succeeded, rolled back, or was aborted. It verifies that the system is in a known state, that monitoring remains active, and that the deployment's outcome is documented.

The first post-flight item is metric stability. Verify that error rate, latency, and other key metrics remain stable for at least 30 minutes after the deployment completes. A deployment that passes canary and gradual rollout but degrades ten minutes after reaching 100 percent is not a successful deployment. The post-flight checklist catches this by requiring a stability window after deployment completion before the deployment is marked as done.

A subscription service extends the stability window to two hours. The deployment completes at 2:00 PM, but the post-flight checklist is not completed until 4:00 PM, after verifying that metrics remained stable throughout the post-deployment window. This requirement caught two deployments in 2025 that looked successful initially but degraded after 60 to 90 minutes due to cache expiration and data staleness issues that only appeared after the caches refilled with new data.

The second post-flight item is deployment outcome documentation. Record whether the deployment succeeded, rolled back, or was aborted. Record how long each stage took. Record whether any alerts fired and what they were. Record any manual interventions that occurred. This documentation is the input for post-deployment reviews and the basis for refining future deployment processes.

The third post-flight item is alerting re-enablement. If any alerts were suppressed during the deployment, verify that they are re-enabled. A temporary alert suppression during canary that is never lifted becomes a permanent blind spot. The post-flight checklist makes re-enablement explicit.

The fourth post-flight item is stakeholder notification. Notify stakeholders that the deployment is complete. Include the outcome, the completion time, and any known issues. If the deployment rolled back, include the reason and the plan for the next attempt. Stakeholder notification closes the communication loop that the pre-flight checklist opened.

The fifth post-flight item is cleanup. Verify that staging environments are returned to baseline state. Verify that temporary deployment infrastructure is shut down. Verify that deployment logs are archived. Cleanup is often skipped because the deployment feels done once it reaches production, but skipping cleanup creates clutter that complicates future deployments.

## Checklist Automation and Tool Integration

Checklist automation means using tools to verify items automatically rather than relying on human verification. Automated checks are faster, more reliable, and eliminate the temptation to skip items when under time pressure. Not every checklist item can be automated—"Verify stakeholders have been briefed" requires human judgment—but many can.

Pre-flight automation includes checks like: query staging for test results and verify they passed, query deployment system for staging uptime and verify it exceeds 24 hours, query monitoring system for dependent service health and verify all services are healthy, query deployment calendar for conflicting deployments and verify none exist. Each of these queries returns a binary result that is displayed on the checklist. The engineer sees "Staging tests passed: Yes" without having to manually check test results.

In-flight automation includes checks like: query metrics database for current error rate and compare to threshold, query alerting system for active alerts and count them, query deployment system for current rollout percentage and compare to expected progress. These checks run automatically at each stage transition and populate the in-flight checklist with current data. The engineer verifies the data but does not need to retrieve it manually.

Post-flight automation includes checks like: query metrics database for stability over the last 30 minutes and verify no breaches occurred, query alerting system for suppressed alerts and list any that remain suppressed, query deployment system for total deployment time and record it. These checks ensure that post-flight verification happens consistently and nothing is forgotten.

A media company built a deployment checklist tool integrated with their CI/CD system. The tool displays the checklist as a web interface. Automated items populate automatically with green checkmarks if they pass and red X marks if they fail. Manual items display as checkboxes that the engineer marks as complete. The tool enforces ordering—in-flight checks cannot be completed until pre-flight checks are done, post-flight checks cannot be completed until in-flight checks are done. The tool logs every checklist completion with timestamps and the engineer's identity. The log is the authoritative record of deployment verification.

## Checklist Ownership and Sign-Off Authority

Checklist ownership means designating who is responsible for completing the checklist and who has authority to approve exceptions. In most cases, the engineer performing the deployment completes the checklist. But for high-risk deployments—new models, multi-layer changes, deployments during high-traffic periods—a second engineer or engineering manager may be required to review and sign off on the checklist before the deployment proceeds.

Sign-off authority is explicit. The pre-flight checklist includes a field for the deploying engineer's name and a field for the approving engineer's name. Both names are required before the deployment can start. This is not bureaucracy—it is a forcing function to ensure that a second set of eyes reviewed the deployment plan and confirmed that it is ready.

A financial services company requires dual sign-off for any deployment that changes models, changes routing logic, or deploys during high-traffic windows. The deploying engineer completes the pre-flight checklist. A senior engineer or engineering manager reviews the checklist, asks clarifying questions, and signs off. The approval is logged. If the deployment fails, the post-incident review examines whether the checklist was completed correctly and whether the sign-off process should have caught issues that were missed.

Checklist ownership also means accountability. If a deployment fails due to a missed checklist item, the engineer who completed the checklist is responsible for explaining why the item was missed and how to prevent the same failure in future deployments. This is not punitive—it is corrective. The goal is to improve the process, not to blame individuals. But accountability requires knowing who completed the checklist and whether they followed the process.

## Checklist Evolution and Continuous Improvement

Checklist evolution means updating the checklist based on incidents, near-misses, and lessons learned. A deployment that fails because the rollback automation was not tested leads to adding "Test rollback automation in staging" to the pre-flight checklist. A deployment that succeeds but causes confusion because stakeholders were not notified of the completion time leads to adding "Document estimated completion time in deployment notification" to the pre-flight checklist.

The checklist is not static. It is a living document that reflects the team's accumulated experience. Every post-incident review asks: "Would a checklist item have prevented this incident?" If the answer is yes, the item is added. If the answer is no, the incident reveals a gap in process that may require something more than a checklist—a new monitoring metric, a new automated gate, a new approval step.

A healthcare platform reviews the deployment checklist quarterly. The review team examines all deployments from the previous quarter, identifies which checklist items were most frequently skipped or marked as not applicable, and decides whether those items should be removed, revised, or kept as-is. The review also examines all incidents from the previous quarter and identifies whether any would have been prevented by additional checklist items. The checklist grows and shrinks organically as the deployment process matures.

Checklist evolution also means removing items that no longer add value. A checklist that grows indefinitely becomes burdensome and is ignored. If an item has been on the checklist for a year and has never caught an issue, and if the risk it is meant to address has been eliminated by process improvements elsewhere, the item can be removed. The checklist should reflect current risks, not historical risks that no longer apply.

## The Stop-the-Line Culture and Checklist Authority

The stop-the-line culture means that anyone can halt a deployment if a checklist item fails or if they observe something concerning that is not on the checklist. The checklist is not the only source of deployment authority—it is the minimum bar. If an engineer sees an issue that the checklist did not catch, they have authority to stop the deployment even if every checklist item passed.

A logistics company adopted a stop-the-line policy after a 2024 deployment proceeded despite multiple engineers expressing concern about elevated latency during canary. The latency was below the automated threshold, so the deployment advanced. The latency continued to increase, eventually breaching the threshold during gradual rollout and triggering a rollback. The post-incident review found that three engineers had noticed the latency trend and mentioned it in Slack but did not escalate forcefully because the checklist had been completed and the automated checks were passing. The new policy explicitly grants any engineer authority to pause or stop a deployment if they believe it is unsafe, regardless of checklist status.

Stop-the-line authority requires psychological safety. Engineers must feel comfortable stopping a deployment without fear of being overruled or criticized for being overly cautious. A culture where stopping a deployment requires justifying the decision to senior leadership is a culture where engineers will hesitate to stop deployments, and that hesitation will eventually lead to a deployment failure that should have been prevented.

Checklist authority means that the checklist itself is authoritative. If a pre-flight item fails, the deployment does not proceed, period. There is no "it's probably fine, let's deploy anyway." There is no "we can fix it during the deployment." The checklist items are requirements, not suggestions. If a requirement cannot be met, the deployment is postponed until it can be met.

## Checklist Versus Runbook and When to Use Each

A checklist is not a runbook. A runbook provides step-by-step instructions for how to perform a deployment. A checklist provides verification steps to ensure the deployment is ready, monitored, and completed correctly. Runbooks tell you what to do. Checklists tell you what to verify.

A deployment that is routine and well-understood may not need a runbook—the deploying engineer knows the steps. But it still needs a checklist to ensure nothing is forgotten. A deployment that is complex or infrequent needs both a runbook and a checklist. The runbook guides the engineer through the steps. The checklist verifies that each step was completed correctly.

A subscription service maintains separate runbooks and checklists. The runbook for deploying a new model version includes 30 steps: update the model configuration, deploy to staging, run staging tests, deploy to production canary, monitor canary, advance to gradual rollout, monitor each rollout milestone, complete deployment. The checklist for the same deployment includes 15 items spread across pre-flight, in-flight, and post-flight phases. The runbook is procedural. The checklist is verification. Both are necessary.

Checklists are also used for emergency deployments where runbooks may not exist. An emergency rollback does not follow a pre-planned runbook because the specific failure scenario was not anticipated. But the rollback still requires verification: verify that the previous version is available, verify that rollback automation is enabled, verify that stakeholders are notified, verify that rollback completes successfully, verify that metrics return to baseline. The checklist provides structure even when the procedure is improvised.

## Chapter Summary and Deployment as Disciplined Practice

Deployment is not an art. It is a discipline. The discipline is built from checklists, dashboards, automated gates, monitoring, communication, and coordination. The discipline does not eliminate risk—it manages risk by making failures detectable, containable, and recoverable. A deployment culture that treats every deployment as a routine event is a culture that will eventually suffer a catastrophic failure that should have been prevented.

The deployment patterns covered in this chapter—gradual rollouts, canary deployments, feature flags, shadow deployments, deployment scheduling, multi-layer coordination, dashboards, and checklists—are not optional enhancements for mature teams. They are the minimum infrastructure required for safe AI deployments at any scale. A team that deploys monthly without canary testing is taking the same risk as a team that deploys daily without canary testing. The risk is not a function of deployment frequency—it is a function of deployment discipline.

The teams that deploy confidently are the teams that have built this discipline into their process and their culture. They deploy during protected windows with full team availability. They canary every change and monitor it carefully before advancing. They coordinate multi-layer changes with explicit dependency management. They provide real-time visibility through dashboards that every stakeholder can access. They verify every deployment with checklists that catch what automation misses. The confidence comes not from experience alone but from infrastructure that turns experience into repeatable process.

Deployment discipline enables velocity. The team that can deploy safely is the team that can deploy frequently. The team that can deploy frequently is the team that can respond to user needs, fix issues quickly, and iterate on improvements without fear. The discipline is not a constraint on velocity—it is the foundation that makes velocity sustainable.

The next frontier of deployment safety is not better rollout strategies or more sophisticated monitoring. It is better rollback mechanics—the ability to undo a deployment quickly, completely, and safely when something goes wrong, the topic of Chapter 11.

