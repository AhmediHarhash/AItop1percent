# 12.3 — Integration with Evaluation Gates: Section 18 Handoff

The pipeline doesn't decide what's ready for production. The eval system does. The pipeline just enforces the eval system's verdict. This separation of concerns is not a minor architectural detail. It is the foundational principle that makes continuous deployment of AI systems trustworthy. Your pipeline orchestrates artifacts, runs validation checks, and handles rollout mechanics. Your evaluation system measures quality, maintains baselines, defines thresholds, and renders pass-fail judgments. The integration between these two systems—the handoff from evaluation to deployment—is where quality governance becomes automated reality.

A mid-sized healthcare company learned this lesson expensively in early 2025. They had built an evaluation system that tracked 23 quality metrics across their clinical note summarization model. They had defined thresholds for each metric. They had a process where ML engineers reviewed eval results before deploying. What they did not have was integration. Evaluation happened in one system. Deployment happened in another. The connection was a Slack message and a manual approval click. The manual step failed when an engineer on-call at midnight approved a deployment without reviewing the eval results—results that showed a 7 percent drop in medical term accuracy, below their defined threshold. The model deployed, generated summaries with incorrect medication names for 19 hours before a physician escalated, and cost the company $420,000 in emergency review and regulatory reporting. The evaluation system had caught the problem. The deployment system ignored it.

## Evaluation as a Deployment Prerequisite

Evaluation must be a hard prerequisite for deployment, enforced automatically by the pipeline, not by human memory or discipline. No model deploys without evaluation passing. No prompt deploys without evaluation passing. No configuration change that affects model behavior deploys without evaluation validating the impact. This enforcement transforms evaluation from a quality monitoring tool into a deployment gate that physically blocks bad artifacts from reaching production.

The integration means your deployment pipeline calls your evaluation system as a required stage. When a model completes training, the pipeline does not immediately promote it to production. The pipeline submits the model to the evaluation system, waits for evaluation to complete, retrieves the evaluation results, compares metrics against defined thresholds, and only proceeds with deployment if all thresholds are met. If any metric falls below threshold, the pipeline blocks deployment and notifies the responsible team. The human is informed of the decision, not asked to make it.

This automatic enforcement prevents the midnight approval mistake. The healthcare company rebuilt their integration so the deployment pipeline could not proceed without a passing evaluation result. The pipeline called the evaluation API, received a response with metrics and a pass-fail verdict, and checked the verdict before allowing deployment. An engineer could still see the pending deployment and the evaluation results, but they could not override the gate. The deployment either met quality standards or it did not deploy. Human judgment moved from "should we deploy?" to "why did evaluation fail and what do we fix?"

## The Eval System Integration: API Contract

The integration between deployment pipeline and evaluation system requires a clear API contract. The deployment pipeline must know how to request evaluation, how to check evaluation status, how to retrieve results, and how to interpret those results. The evaluation system must expose endpoints for these operations and return responses in a format the pipeline can parse.

The API contract typically includes four operations. **Submit evaluation request**: the pipeline sends the artifact to evaluate—a model ID, a prompt version, a configuration hash—along with metadata identifying the evaluation suite to run and the deployment target. The evaluation system returns a job ID. **Check evaluation status**: the pipeline polls the evaluation system with the job ID to determine if evaluation is complete, still running, or failed. **Retrieve evaluation results**: once complete, the pipeline fetches the full evaluation results—metrics, scores, pass-fail verdict, details on which examples passed or failed. **Query evaluation history**: the pipeline can retrieve historical evaluation results for comparison, allowing it to enforce policies like "new model must not degrade any metric by more than 2 percent compared to current production model."

The healthcare company's evaluation API exposed these operations as REST endpoints with JSON responses. Their deployment pipeline, implemented in GitHub Actions, called the evaluation API using curl commands with proper authentication. The pipeline waited up to 60 minutes for evaluation to complete, polling status every 30 seconds. If evaluation did not complete within 60 minutes, the pipeline failed the deployment and alerted the ML team that evaluation was taking unusually long—a signal that evaluation infrastructure might be overloaded or broken.

## Gate Pass Criteria: What Evaluation Results Allow Deployment

The pipeline must translate evaluation results into a binary deployment decision: proceed or block. This translation requires defining gate pass criteria—the specific conditions under which evaluation results permit deployment. Criteria must be explicit, measurable, and enforced automatically.

Common gate pass criteria include absolute thresholds on individual metrics, relative thresholds compared to baseline, no regressions on critical dimensions, and manual approval for borderline cases. Absolute thresholds might specify that accuracy must exceed 94 percent, precision must exceed 92 percent, and recall must exceed 89 percent. Relative thresholds might specify that no metric can drop more than 2 percentage points compared to the current production model. Critical dimension protection might specify that even if overall accuracy is acceptable, deployment blocks if accuracy on protected demographic groups drops below threshold. Manual approval might be required if metrics are within 1 percentage point of threshold—close enough that human judgment should review the trade-offs.

The healthcare company defined gate pass criteria as a YAML configuration file versioned alongside their codebase. The criteria specified absolute minimums for 23 metrics, maximum allowable regression for 12 high-priority metrics, and zero tolerance for regressions on medical term accuracy and medication name recognition. The deployment pipeline parsed this configuration, compared evaluation results against each criterion, and blocked deployment if any criterion failed. The criteria file was code—changes required review and approval just like application code changes.

## Gate Fail Handling: Blocking Deployment and Notifying Stakeholders

When evaluation fails to meet gate criteria, the pipeline must block deployment and communicate why. Blocking is the easy part—simply do not proceed to the deployment stage. Communication is harder. The pipeline must notify the right stakeholders with enough detail to understand the failure and enough context to decide what to do next.

Notification should include which metrics failed, by how much, which evaluation examples contributed to the failure, and links to full evaluation results for deeper investigation. The notification should route to the team responsible for fixing the issue—ML team for model evaluation failures, Product team for prompt evaluation failures, Platform team for configuration evaluation failures. The notification should include enough context that the recipient understands urgency—is this a critical regression that requires immediate attention, or a marginal miss that can be addressed in the next iteration?

The healthcare company's pipeline sent notification through three channels simultaneously when evaluation failed. A Slack message to the team channel with a summary of failed metrics and a link to the evaluation dashboard. An email to the on-call ML engineer with full evaluation details attached. A status update in their issue tracker creating a ticket assigned to the ML lead describing the deployment that was blocked and why. This multi-channel notification ensured someone saw the failure regardless of which communication tool they were actively monitoring.

## Evaluation Timing: When to Run Evals in the Pipeline

Evaluation timing determines how early you catch quality issues and how much time you invest in evaluating artifacts that will never deploy. The trade-off is between fast feedback and wasted evaluation cost. Running evaluation very early—immediately after every code commit—provides fast feedback but wastes evaluation runs on experiments that never reach production. Running evaluation very late—only when deploying to production—saves evaluation cost but delays feedback and blocks deployments at the worst possible moment.

The optimal strategy is tiered evaluation with increasing depth at each stage. Run fast lightweight evaluation on every pull request—a small representative eval set that catches catastrophic failures. Run medium-depth evaluation on staging deployments—a larger eval set that validates quality is acceptable. Run full comprehensive evaluation before production promotion—the complete eval suite with all dimensions measured. This tiering provides fast feedback for iteration while ensuring production deployments have the highest confidence.

The healthcare company implemented three evaluation tiers. **PR evaluation** ran on every pull request that changed prompts or model code, using 150 evaluation examples and completing in under 5 minutes. This caught syntax errors, obvious regressions, and gave developers immediate feedback. **Staging evaluation** ran when artifacts deployed to the staging environment, using 1,500 examples and completing in under 30 minutes. This validated that the artifact was viable for production consideration. **Production evaluation** ran before production promotion, using the full 8,000 example eval set and completing in 2 hours. Only artifacts that passed production evaluation could reach production. The tiering allowed developers to iterate quickly while ensuring production quality was never compromised by insufficient evaluation.

## Evaluation Caching: Avoiding Redundant Evaluation Runs

If the model has not changed and the prompt has not changed and the evaluation suite has not changed, re-running evaluation produces the same results. Evaluation caching exploits this determinism to avoid redundant work. When the pipeline requests evaluation, the evaluation system checks whether it has recent cached results for the exact same artifact and eval suite version. If yes, the system returns cached results immediately. If no, it runs evaluation and caches the results for future requests.

Caching is safe when artifacts are immutable and eval suites are versioned. A model with ID "model-2026-02-08-v3" and hash "a1b2c3d4" evaluated against eval suite version "v12" produces deterministic results assuming evaluation uses a fixed random seed for sampling. Caching that result and reusing it when the same model is evaluated against the same eval suite version avoids wasting compute on redundant evaluation.

The healthcare company implemented evaluation caching with a 30-day cache lifetime. When the deployment pipeline requested evaluation, the evaluation system checked a cache keyed by model ID, model hash, prompt version, eval suite version, and configuration hash. If a cache entry existed and was less than 30 days old, the system returned cached results in under 5 seconds. If no cache entry existed, evaluation ran fully and the result was cached. This caching reduced evaluation cost by 60 percent because many deployments were configuration-only changes or rollbacks to previously evaluated models.

## Partial Evaluation: Fast Checks for PRs, Full Evaluation for Release

Pull request evaluation must be fast enough to provide feedback within minutes. Full production evaluation might take hours. Partial evaluation allows you to run a representative subset of your eval suite for fast feedback during development, then run the complete eval suite before production deployment. The subset must be carefully chosen to catch most quality issues while completing quickly.

The representative subset should include diverse examples covering major input patterns, edge cases, and historically problematic scenarios. It should include critical dimensions that must never regress. It should be large enough to provide statistical confidence that passing the subset strongly predicts passing the full eval, but small enough to complete in under 10 minutes. 150 to 500 examples typically strikes this balance.

The healthcare company's partial eval set included 180 examples: 120 representing common clinical note patterns across major specialties, 30 edge cases from historical failures, 20 demographic diversity examples ensuring fairness across patient populations, and 10 adversarial examples designed to catch overfitting or prompt injection vulnerabilities. These 180 examples took 4 minutes to evaluate and caught 87 percent of issues that full evaluation would catch. The remaining 13 percent of issues were subtle and only appeared in full evaluation, but developers got fast feedback on the vast majority of quality problems without waiting for 2-hour eval runs.

## Cross-Reference to Section 18 on Regression Testing

The deployment pipeline's evaluation gates rely entirely on the infrastructure and strategy covered in Section 18. Section 18 defines how to build eval suites, how to version them, how to compute metrics, how to define thresholds, how to detect regressions, and how to maintain ground truth datasets. The deployment pipeline takes Section 18's outputs—evaluation results with pass-fail verdicts—and enforces them as deployment prerequisites. If Section 18 is weak, your deployment gates provide false security. A deployment pipeline with strong enforcement of weak evaluation is just automated deployment of unvalidated artifacts.

The integration between Section 18's evaluation infrastructure and Section 19's deployment pipeline requires coordination. Evaluation suite changes must be versioned and reviewed because they change what gets deployed. Threshold changes must be approved at a policy level because they change your quality bar. Ground truth updates must be validated because they redefine correctness. The deployment pipeline does not make these decisions—it enforces them. But enforcement requires that Section 18's evaluation system is production-grade infrastructure, not an afterthought.

The healthcare company treated their evaluation system as tier-one infrastructure with the same reliability standards as production APIs. Evaluation system downtime blocked all deployments—a forcing function to keep evaluation reliability high. Evaluation suite changes went through the same review process as code changes. Threshold changes required VP approval because they changed organizational quality commitments. This rigor made the deployment gate trustworthy. Teams knew that passing evaluation meant quality met standards, not that they got lucky with evaluation timing.

## The Eval-Deployment Contract: Clear Interface Between Systems

The contract between evaluation system and deployment pipeline must be explicit, documented, and stable. The pipeline expects certain guarantees from evaluation. The evaluation system expects certain inputs from the pipeline. Violating these expectations breaks deployment.

Pipeline expectations include: evaluation API is available with defined uptime SLA, evaluation completes within a maximum time bound, evaluation results include all required metrics, pass-fail verdict is unambiguous, evaluation results are immutable once returned. Evaluation system expectations include: artifacts submitted for evaluation are immutable and identified by stable IDs, eval suite version is specified explicitly, artifact metadata includes all fields needed for evaluation, the pipeline respects rate limits on evaluation API calls.

The healthcare company documented this contract in a formal interface specification that both systems adhered to. When the evaluation team wanted to add a new metric, they updated the interface spec and gave the deployment team two weeks to update their pipeline to expect the new metric. When the deployment team wanted to trigger evaluation for configuration-only changes, they updated the interface spec to define how configurations would be identified and what evaluation metadata they required. The contract prevented uncoordinated changes that would break the integration.

## Why This Integration is Non-Negotiable

The integration between evaluation and deployment is the enforcement point for every quality decision your organization makes. You can have world-class evaluation infrastructure, deeply considered quality metrics, and rigorously defined thresholds. If the deployment pipeline does not enforce them, they are suggestions. Suggestions get ignored under deadline pressure, during incidents, and when the on-call engineer is tired at midnight. Automated enforcement makes quality non-negotiable.

The healthcare company's $420,000 incident happened because enforcement was manual. A human had to look at evaluation results and decide whether to proceed. Humans make mistakes. Humans take shortcuts. Humans rationalize exceptions. Automated gates do not. The gate checks the verdict. If evaluation passed, deployment proceeds. If evaluation failed, deployment blocks. No exceptions. No judgment calls. No midnight mistakes. This is the only way to make quality thresholds meaningful.

The evaluation gate is only as strong as the artifacts it validates. Those artifacts—model weights, prompts, configurations—must have integrity and provenance, proving they are what they claim to be and were produced through authorized processes. That integrity foundation is where we turn next.

