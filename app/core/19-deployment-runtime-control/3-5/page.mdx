# 3.5 â€” Graceful Degradation Under Overload: What to Sacrifice First

When traffic exceeds your capacity, something will degrade. The only question is whether you control what breaks or let the system decide for you. The wrong choice turns a temporary spike into a permanent reputation problem.

In October 2025, a customer support AI serving 400,000 users hit capacity during a product launch. The engineering team had planned for scale, but not for what happens when scale arrives faster than infrastructure. Traffic spiked to 340 percent of normal within twelve minutes. The system had three options: sacrifice quality by routing to a smaller model, sacrifice latency by extending queue times, or sacrifice availability by rejecting requests. The team had never documented which to choose. The default behavior was to queue everything indefinitely. After nine minutes, the median response time hit 47 seconds. Users assumed the service was broken and refreshed repeatedly, amplifying the problem. The system crashed entirely. When they rebuilt with explicit degradation rules, the next spike was handled without user complaints. The decision was made in advance: paid users get longer queues, free users get a smaller model, enterprise users never degrade.

Graceful degradation is not about preventing overload. It is about choosing what you lose when overload is inevitable. The choice reveals what you value. Every system makes this choice, either consciously with a strategy or unconsciously with chaos.

## The Degradation Hierarchy: What Can You Actually Sacrifice

You have four dimensions you can degrade: quality, latency, availability, and features. Each has different user impact, different cost, and different engineering complexity.

**Quality degradation** means the response is worse but still arrives. You route to a smaller model, reduce output length, simplify the prompt, skip expensive quality checks. The user gets an answer, just not the best one. The advantage is that availability stays high and latency stays predictable. The disadvantage is that some users notice the quality drop, especially power users who know what good looks like. Quality degradation works best when the quality gap is small and the user task is forgiving. A chatbot giving slightly less detailed answers during a spike is fine. A legal contract reviewer skipping validation checks is not.

**Latency degradation** means the response is the same quality but takes longer. You extend queue times, increase timeouts, accept higher P95 and P99. The user waits, but they get the full-quality result. The advantage is that quality never drops, which matters for tasks where correctness is critical. The disadvantage is that users interpret long waits as system failure and abandon requests or retry, making the problem worse. Latency degradation works best when you communicate the delay explicitly. A status message saying "High demand, estimated wait 12 seconds" keeps users engaged. Silence for 45 seconds loses them.

**Availability degradation** means some requests are rejected entirely. You return an error, serve a cached response, or queue the request for later processing with a callback. The user gets nothing immediately. The advantage is that the users who do get served receive full quality at normal latency. The disadvantage is that rejected users are frustrated, and if you reject the wrong users, they churn. Availability degradation works best when you can prioritize ruthlessly. Reject anonymous users before authenticated ones. Reject free tier before paid. Reject retries before first attempts.

**Feature degradation** means the core response works but secondary features are disabled. You turn off streaming and return the full response at once, skip per-request guardrails, disable expensive post-processing like citation linking or entity recognition. The user gets the answer but loses polish. The advantage is that you free up capacity without dropping quality of the core output. The disadvantage is that some features are actually critical in disguise, and disabling them creates safety or compliance risk. Feature degradation works best when you have clear separation between essential and nice-to-have, and when users do not depend on the disabled feature for their workflow.

The hierarchy is not universal. It depends on what your users value. For a coding assistant, quality degradation is death. Developers know when the suggestions are worse, and they stop using the tool immediately. For a conversational chatbot, latency degradation is death. Users expect instant replies, and a ten-second delay feels broken. For a research tool, availability degradation is acceptable if you communicate clearly. The user would rather queue and get the full result than receive a degraded answer immediately.

## Quality Degradation Strategies: Smaller Models and Shorter Outputs

Quality degradation works by reducing the computational cost per request without rejecting the request. The most effective lever is model routing. When load exceeds 85 percent capacity, start routing new requests to a smaller, faster model. If your primary model is Claude Opus 4.5, degrade to Claude Sonnet 4.5. If your primary model is GPT-5, degrade to GPT-5-mini. The smaller model costs half as much per token and runs twice as fast, effectively quadrupling capacity. The quality drop is noticeable to experts but acceptable to most users during a temporary spike.

You can also reduce the maximum output tokens. If your system normally allows 2000 token responses, cap degraded requests at 800 tokens. This reduces generation time and memory pressure. For most use cases, 800 tokens is sufficient. You lose the ability to generate long-form content, but short-form answers still work. Some tasks cannot tolerate this truncation. Document summarization, code generation, and creative writing all need full-length outputs. For these, quality degradation is not viable.

Prompt simplification is a third lever. If your production prompt includes multiple examples, complex instructions, and chain-of-thought reasoning, strip it down to the minimal version during overload. The shorter prompt reduces input token cost and speeds up prefill. The output quality drops slightly, but not as much as switching models. The challenge is that you need a pre-validated simplified prompt. You cannot generate it dynamically during an incident. Prepare it in advance, test it, and keep it ready to deploy.

Skipping expensive features is the fourth lever. If you run a safety classifier on every output, disable it during overload. If you run entity extraction or citation linking as post-processing, disable it. If you run a quality LLM-as-judge check on every response, disable it. Each of these features adds latency and cost. Disabling them buys capacity. The risk is that you disable something that prevents a catastrophic failure. If your safety classifier is the only thing stopping the model from generating harmful content, you cannot disable it, even during overload. If your citation linking is nice-to-have polish, disable it freely.

The degradation needs to be transparent. If you route a user to a smaller model, consider telling them. A message like "Due to high demand, responses may be shorter than usual" sets expectations. If you say nothing, users assume the quality drop is permanent and lose trust. Transparency does not mean apologizing endlessly. It means one clear message at the start, then normal behavior.

## Latency Degradation Strategies: Longer Queues and Extended Timeouts

Latency degradation works by making users wait longer without changing what they receive. The simplest strategy is to increase queue depth limits. If your normal maximum queue depth is 50 requests, increase it to 200 during overload. Requests queue longer but eventually get served at full quality. The system throughput does not change, but the system absorbs bursts without rejecting requests.

The danger is that long queues create a doom spiral. If the median wait time exceeds 15 seconds, users assume the request failed and retry. Each retry adds another request to the queue, increasing wait times further. By the time the queue drains, you have served the same logical request three times, wasting capacity and making the problem worse. To prevent this, you need explicit communication. If a request enters a queue longer than 10 seconds, return an immediate message with estimated wait time. The user knows the system is working and waits instead of retrying.

You can also extend request timeouts. If your normal request timeout is 30 seconds, extend it to 90 seconds during overload. This allows slower requests to complete instead of timing out and getting retried. The cost is that very slow requests consume worker slots for longer, reducing effective throughput. This only makes sense if your P99 latency is close to your timeout threshold. If your P99 is 8 seconds and your timeout is 30 seconds, extending the timeout does nothing useful.

Accepting higher P95 and P99 is another lever. Under normal load, you might auto-scale when P95 exceeds 3 seconds. During overload, increase that threshold to 6 seconds before scaling. This allows the existing capacity to run hotter before triggering expensive scale-out. The trade-off is that the worst-case user experience degrades. Most users see no change, but the slowest 5 percent wait twice as long. If your application is latency-sensitive, this is unacceptable. If your application is quality-sensitive, this is fine.

The latency degradation strategy only works if users trust that the system will eventually respond. If your system has a history of timing out, users do not wait. They retry immediately or leave. If your system has a history of reliability, users tolerate 15-second waits without complaint. Trust is built over months and lost in minutes. If you use latency degradation, you must be confident that requests complete reliably once queued.

## Availability Degradation Strategies: Rejecting Requests with Priority

Availability degradation is the most aggressive strategy. You refuse to serve some requests to protect capacity for others. This sounds harsh, but it is often the right choice. Rejecting 20 percent of requests to keep the other 80 percent fast is better than slowing everyone to 40-second response times.

The question is who gets rejected. The wrong answer is random. If you reject requests randomly, you reject your highest-value users as often as your lowest-value users. You lose the customers you most want to keep. The right answer is priority-based rejection. Define priority tiers in advance. Enterprise customers are never rejected. Paid customers are rejected only after all other strategies are exhausted. Free tier users are rejected first. Anonymous users are rejected before authenticated users. Retry attempts are rejected before first attempts.

Implementation requires tagging every request with priority metadata at ingress. The load balancer or gateway checks current load. If load exceeds 90 percent capacity, requests below priority tier 2 return HTTP 429 with a Retry-After header. If load exceeds 95 percent, requests below tier 3 are rejected. If load exceeds 98 percent, only tier 1 requests are served. The thresholds and tier definitions are policy decisions, not technical ones. The technical system enforces whatever policy you define.

Returning cached responses is a softer form of rejection. If a user asks the same question twice within ten minutes, return the cached first response instead of generating a new one. The user gets an instant answer, the system serves zero new compute. This works well for FAQ-style questions and poorly for creative or session-dependent tasks. The cache must be request-scoped, not user-scoped. Two users asking the same question can share a cached response. The same user asking a follow-up question in conversation cannot.

Serving static fallbacks is another option. If the system is overloaded, return a pre-written fallback message: "Our AI is currently at capacity. Please try again in a few minutes, or visit our help documentation." This is only acceptable if the user's task can tolerate delay. For urgent customer support, a static fallback is worse than a 20-second queue. For exploratory research, a static fallback is fine.

Queueing with callback is the most sophisticated form of availability degradation. If the system is overloaded, return an HTTP 202 Accepted with a callback URL. The request is queued for asynchronous processing. When the result is ready, the system POSTs to the callback URL or stores the result for later retrieval. This works well for batch-style workloads and poorly for interactive chat. The user experience is fundamentally different. Some products can adopt this model easily. Others cannot.

## Feature Degradation Strategies: Disabling Non-Essentials

Feature degradation frees up capacity by disabling expensive secondary features while keeping the core response intact. The most effective lever is disabling streaming. Streaming responses require holding a connection open for the full generation time, consuming a worker slot for 5 to 15 seconds. Batch responses allow the worker to start the generation, then release the slot while the model generates, then return the full response at once. This increases worker throughput by 30 to 50 percent during overload. The trade-off is user experience. Streaming feels faster because the user sees progress. Batch responses feel slower because the user waits in silence, then receives everything at once. For latency-sensitive applications, disabling streaming is painful. For quality-sensitive applications, it is painless.

Disabling per-request guardrails is another option. If you run a prompt injection classifier on every input and a toxicity classifier on every output, each request incurs two extra LLM calls. Disabling these during overload doubles throughput. The risk is obvious: you lose safety. This is only acceptable if your base model is already safe enough without guardrails, or if the risk window is short enough that you can manually review flagged sessions afterward. Most production systems cannot disable guardrails safely. If you cannot, do not list this as a degradation option.

Disabling expensive post-processing is safer. If you run entity recognition on every output to highlight key terms, disable it. If you run citation linking to turn references into hyperlinks, disable it. If you run a quality check LLM-as-judge that scores response relevance and re-generates on low scores, disable it. Each of these adds latency and cost. Each is nice-to-have, not must-have. Disabling them during overload is low-risk.

Feature degradation requires feature flags. Every feature that can be disabled must have a runtime toggle. The toggle must be fast to flip, ideally via configuration change without deployment. During an incident, you do not have time to deploy code. You need to disable the feature in under 60 seconds. If your feature flags require a deploy, they are too slow for incident response.

The decision of which features to degrade should be documented in advance. A runbook that lists every feature, its cost, its risk of disabling, and the load threshold at which to disable it. During an incident, the on-call engineer follows the runbook instead of making judgment calls under pressure. The runbook is reviewed quarterly and updated as features change.

## Priority-Based Degradation: Treating Different Users Differently

Degradation without priority is indiscriminate. Degradation with priority is strategic. The principle is simple: protect your most valuable users at the expense of your least valuable users. The implementation is harder than it sounds.

For paid users, maintain quality and extend latency. They are paying for the best experience. If the system is overloaded, queue their requests longer before degrading. If you must degrade, degrade latency first, quality last. A paid user who waits 12 seconds for a high-quality response is annoyed. A paid user who gets a low-quality response after 3 seconds is angry.

For free users, degrade quality or reject. They are not paying, so their experience can be sacrificed to protect capacity. If the system is overloaded, route free-tier requests to a smaller model immediately. If overload continues, reject free-tier requests entirely. Some free users churn. That is acceptable. The alternative is that paid users churn because the service is slow, which is not acceptable.

For enterprise users, never degrade. Enterprise contracts include SLAs. Violating an SLA has legal and financial consequences. If your system cannot serve enterprise traffic during overload, you have under-provisioned. Enterprise users expect reserved capacity. Either pre-allocate dedicated resources for enterprise traffic or give enterprise requests absolute priority in the queue. If the system must choose between rejecting 1000 free users and degrading 1 enterprise user, reject the free users.

The priority system must be enforced at ingress, not at the worker. If priority checks happen after requests are queued, low-priority requests consume queue slots, preventing high-priority requests from entering. The gateway must check priority, check current load, and make the degradation decision before the request enters the system.

## Communication During Degradation: What Users Need to Know

When the system degrades, users need to know three things: that degradation is happening, what it means for them, and how long it will last. Silence creates panic. Transparency creates patience.

The status message should be specific. "Due to high demand, responses may take up to 15 seconds" is better than "experiencing high load." The first sets an expectation. The second creates uncertainty. If you route to a smaller model, say so: "Due to high demand, we are using a faster model. Responses may be less detailed than usual." The user knows what to expect and can decide whether to proceed or wait.

The status page must be updated in real time. If your public status page says "All systems operational" while users experience 30-second response times, you lose credibility permanently. The status page should reflect degradation honestly: "Elevated response times due to high traffic. We are scaling capacity and expect normal performance within 20 minutes." This tells users that you know, you are acting, and there is an end in sight.

Internal alerting must be clear about what is degraded and why. An alert that says "High load" is useless. An alert that says "Load at 94 percent, free-tier requests rejected, paid users experiencing P95 latency of 8 seconds, enterprise unaffected" tells the on-call engineer exactly what is happening and whether escalation is needed.

Communication after degradation ends is also critical. Users who experienced degradation should receive an explanation and, if appropriate, a credit or apology. "We experienced high traffic yesterday between 2pm and 3pm Pacific. Response times were elevated. We have added capacity to prevent this in the future. If you were affected, we have credited your account with an extra day of service." This turns a negative experience into a demonstration of accountability.

## Recovery from Degradation: Avoiding Oscillation

When load drops, you must restore normal behavior carefully. The instinct is to flip everything back immediately. The danger is oscillation. Load drops to 80 percent. You restore full quality. Load spikes back to 95 percent because the higher quality attracts more usage. You degrade again. Load drops. You restore. The system oscillates between degraded and normal every few minutes, creating a worse experience than staying degraded.

The solution is hysteresis. Degrade when load exceeds 90 percent. Restore when load drops below 70 percent. The 20-percent gap prevents oscillation. The system stays degraded slightly longer than necessary, but the user experience is stable.

Restoration should be gradual. If you routed 40 percent of traffic to a smaller model during overload, do not route it all back to the larger model at once. Route 10 percent back, wait two minutes, check load, route another 10 percent. Gradual restoration prevents a load spike from the restored traffic.

After restoration, verify quality. If you disabled features during degradation, check that re-enabling them does not introduce errors. If you extended timeouts, check that resetting them does not cause new timeouts. The post-incident review includes a quality check, not just a performance check.

Graceful degradation is not a sign of system weakness. It is a sign of system maturity. Systems that never degrade are systems that never face real load. Systems that degrade catastrophically are systems that were not designed for reality. Systems that degrade gracefully are systems that acknowledge limits and choose intelligently what to sacrifice. The users notice the honesty. They trust you more, not less.

