# 5.3 — Latency-Aware Routing: Fastest Model for This Request

A user sits waiting for a response. Every hundred milliseconds of additional latency increases the probability they will abandon the interaction. At two hundred milliseconds, the system feels instant. At five hundred milliseconds, it feels responsive. At one second, the user notices the delay. At two seconds, they start wondering if something is wrong. At three seconds, a meaningful percentage of users close the tab or navigate away. Latency is not an engineering detail. It is a user experience variable that directly impacts engagement, satisfaction, and retention.

When latency matters more than raw capability — real-time chat, voice interactions, interactive features where the user expects immediate feedback — you need latency-aware routing. This means selecting the fastest model that can meet your quality bar for a given request, even if a slower model would produce a marginally better response. The trade-off is deliberate: you are sacrificing the top one or two percent of quality for a fifty to seventy percent reduction in response time. For latency-sensitive use cases, this trade-off is correct. A response that arrives in three hundred milliseconds and is ninety-five percent as good as the optimal response is far more valuable than the optimal response that arrives in one thousand milliseconds.

## When Latency Matters More Than Capability

Latency-aware routing is not appropriate for all use cases. If a user submits a research query and expects a thorough, well-reasoned answer, they will wait two seconds for a high-quality response. They will not accept a fast, shallow response. If a user is drafting a legal contract and asks for clause recommendations, they care far more about accuracy than speed. But for conversational interfaces, real-time collaboration tools, and interactive applications where the model is part of a tight feedback loop, latency dominates quality in determining user satisfaction.

Voice interfaces are the clearest example. A voice assistant that takes one and a half seconds to respond feels sluggish and unnatural. A voice assistant that responds in three hundred milliseconds feels like a real conversation. The quality of the response matters, but if the response is delayed, the user experience degrades regardless of content quality. In voice interactions, latency thresholds are measured in hundreds of milliseconds, not seconds. Every model selection decision must account for this constraint.

Real-time chat is another domain where latency-aware routing is essential. When a user sends a message in a chat interface, they expect to see the model's response begin streaming within half a second. If the first token does not arrive for two seconds, the interface feels broken. The user might send the message again, thinking the first one did not go through. They might switch to another task, breaking their focus. The psychological experience of waiting for a chatbot is different from waiting for a search engine. In chat, speed signals responsiveness, attentiveness, and competence. A slow chatbot feels dumb, even if its responses are technically accurate.

## Latency Routing Signals

Latency-aware routing requires real-time data about model performance. You need to know the current latency distribution for every model you might route to: P50, P95, and P99 latency for the past minute, the past five minutes, and the past hour. You need to know queue depth for each model endpoint. You need to know whether a model is currently experiencing elevated latency due to infrastructure issues, rate limiting, or regional outages. This data must be collected continuously and made available to the routing layer with minimal overhead.

P50 latency tells you the typical case. If a model's P50 latency is three hundred milliseconds, half of requests complete in less than three hundred milliseconds, and half take longer. This is useful for understanding baseline performance, but it does not tell you about the tail. P95 latency tells you what the worst five percent of requests experience. If P95 is one thousand milliseconds, one in twenty users waits a full second or more. P99 latency tells you about the truly bad cases. If P99 is three seconds, one in a hundred users experiences an unacceptable delay. For latency-sensitive applications, P95 and P99 matter more than P50.

Queue depth is a leading indicator of latency degradation. If a model's request queue is growing, latency will increase for future requests even if current latency is acceptable. Routing new requests to a model with a growing queue is a mistake. You should route to an alternative model with lower queue depth, even if that model is slightly slower under normal conditions. The goal is to avoid latency spikes, not to optimize for the absolute fastest model when all conditions are ideal.

Request priority adds a business dimension to latency routing. A free-tier user might tolerate higher latency than a paying customer. A background batch processing job can accept multi-second latency. A user interacting with your interface in real time cannot. Priority can be explicit — set by the user or determined by their account tier — or implicit — inferred from the interaction context. High-priority requests get routed to the fastest available model. Low-priority requests get routed to models with higher queue depth or elevated latency, preserving fast-path capacity for users who need it.

## Real-Time Latency Monitoring and Routing Decisions

Latency-aware routing decisions happen on a per-request basis, using the most recent latency data available. You cannot route based on last week's latency metrics. Model performance changes throughout the day as load varies, as infrastructure scales up or down, and as providers make backend changes. A model that had two hundred millisecond P50 latency this morning might have six hundred millisecond P50 latency this afternoon. If you route based on stale data, you will send requests to models that are currently slow.

The routing layer must consume real-time latency metrics from your observability infrastructure. Every completed request updates the latency distribution for the model that served it. The routing layer maintains a sliding window of recent latency data — typically the last one to five minutes — and uses this data to predict latency for the next request. The prediction is probabilistic, not deterministic. You cannot guarantee that a request routed to a model with three hundred millisecond P50 latency will complete in three hundred milliseconds. You can only say that it is likely to complete in that timeframe based on recent performance.

When multiple models meet your quality bar, latency-aware routing selects the one with the lowest predicted latency. If Claude Sonnet 4.5 and GPT-5-mini both produce acceptable responses for a given query type, and Sonnet's P50 is two hundred and fifty milliseconds while GPT-5-mini's P50 is four hundred milliseconds, you route to Sonnet. If Sonnet's queue depth suddenly increases and its P95 latency climbs to eight hundred milliseconds, you switch to GPT-5-mini. The routing decision is dynamic, responding to real-time conditions rather than static configuration.

## Balancing Latency with Quality

Latency-aware routing introduces a trade-off: faster models are often less capable than slower models. Claude Haiku 4.5 responds faster than Claude Opus 4.5, but it cannot handle the same level of reasoning complexity. GPT-5-mini responds faster than GPT-5, but it struggles with tasks that require deep domain knowledge or multi-step reasoning. The routing decision is not simply "pick the fastest model." It is "pick the fastest model that meets your quality bar."

This requires defining quality thresholds by use case. For a simple factual query, Claude Haiku 4.5's quality might be indistinguishable from Claude Opus 4.5, so you always route to Haiku when latency matters. For a complex reasoning task, Haiku's quality might fall below your threshold, so you route to Sonnet or Opus even though latency will be higher. The quality bar is a floor. You will not go below it to gain latency. But within the set of models that meet the quality bar, you always prefer the fastest option.

In practice, this means maintaining a quality-latency profile for each model and query type. You measure quality and latency for every routing decision and build a dataset that maps input characteristics to model performance. Over time, you learn which models provide acceptable quality at low latency for which types of queries. This profile guides routing decisions. A query that matches a profile where Claude Haiku 4.5 achieves ninety-five percent quality and two hundred millisecond latency gets routed to Haiku. A query that matches a profile where Haiku achieves seventy-eight percent quality gets routed to a higher-tier model.

## Dynamic Latency Routing

The most sophisticated latency-aware routing systems adjust dynamically based on real-time conditions. If your primary model is experiencing elevated latency — P95 jumps from four hundred milliseconds to one thousand milliseconds — the routing layer automatically shifts traffic to an alternative model. This shift might involve routing to a different model tier, a different provider, or a different regional deployment of the same model. The goal is to maintain acceptable latency for users even when individual models experience performance degradation.

Dynamic routing requires fallback logic. Your primary routing decision might be "send this query to Claude Sonnet 4.5 in US-East." But if Sonnet's latency exceeds your threshold, the fallback might be "send it to GPT-5-mini in US-West" or "send it to Claude Haiku 4.5 in US-East." The fallback model might be slightly lower quality, but it is fast enough to meet your latency SLO. Users experience consistent response times even when backend conditions change.

Health checks and circuit breakers prevent latency-aware routing from sending traffic to models that are completely unavailable or experiencing severe degradation. If a model's error rate exceeds a threshold, or if multiple consecutive requests time out, the routing layer opens a circuit breaker and stops sending traffic to that model until health checks indicate it has recovered. This prevents cascading failures where every request to a degraded model times out, consuming resources and creating a backlog that delays all subsequent requests.

## P50, P95, P99 Latency SLOs and Routing Enforcement

Latency SLOs define acceptable response times for different user interactions. A typical set of SLOs might specify: P50 latency under three hundred milliseconds, P95 under six hundred milliseconds, P99 under one thousand milliseconds. These SLOs guide routing decisions. Any model that consistently exceeds your P95 SLO should be removed from the routing pool for latency-sensitive queries, even if its quality is excellent. You are not meeting user expectations if five percent of users wait longer than your threshold.

Routing enforcement means making routing decisions that honor SLOs in real time. If all available models are currently exceeding your P95 SLO due to high load, you have limited options. You can queue requests until capacity becomes available, but this further degrades latency. You can shed load by rejecting low-priority requests. You can route to models outside your preferred set — perhaps using a smaller model with lower quality but better latency. You can scale up additional capacity if your infrastructure allows it. The correct choice depends on your business priorities, but latency-aware routing gives you the visibility and control to make the choice explicitly.

SLO violations trigger alerts and investigations. If your P95 latency SLO is six hundred milliseconds and your monitoring shows P95 latency at seven hundred and fifty milliseconds for the past hour, something is wrong. You might have underprovisioned capacity. You might have a misconfigured routing rule sending too much traffic to slow models. You might have a provider experiencing an outage. Latency-aware routing is not a replacement for capacity planning and incident response, but it helps you detect problems faster and mitigate their impact on users through intelligent traffic distribution.

## The User Experience of Speed

Users do not think about latency in milliseconds. They think about whether your system feels responsive, slow, or broken. The difference between three hundred milliseconds and six hundred milliseconds might seem small on a dashboard, but users perceive it as the difference between instant and delayed. The difference between six hundred milliseconds and one thousand two hundred milliseconds is the difference between acceptable and frustrating. Latency-aware routing exists to keep your system on the right side of these perceptual boundaries.

This is why latency-aware routing often sacrifices quality for speed, even when cost is not a concern. A response that is ninety-four percent as good as the optimal response but arrives in two hundred and eighty milliseconds creates a better user experience than the optimal response that arrives in nine hundred milliseconds. The user does not know they received a slightly lower-quality response. They do know that your system responded quickly. Speed creates a perception of competence and reliability that quality alone cannot achieve.

The next subchapter covers quality-aware routing — the strategy of selecting the best model for a specific input type when quality is the primary optimization target.

