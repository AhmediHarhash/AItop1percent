# 12.2 — Pipeline Architecture: From Commit to Production

An AI deployment pipeline is not a linear sequence. It is a directed graph with multiple artifact types flowing through different validation paths. Traditional software has one pipeline: code changes trigger build, test, package, deploy. AI systems require orchestrating four parallel pipelines—code, models, prompts, and configurations—each with its own validation requirements, each capable of blocking the others, each potentially deployable independently. The architecture challenge is not making any single pipeline work. The challenge is making them work together without creating gridlock where every change waits on every other change, and without creating gaps where changes slip through validation because no pipeline considered them its responsibility.

The moment you treat AI deployment as a single linear pipeline, you lose. You couple artifacts that should be independent. You serialize changes that could be parallel. You force model retraining to wait on code reviews. You force prompt updates to wait on model validation. You create a deployment bottleneck that slows every team touching your AI system. The legal tech company that lost $170,000 on a bad deployment started with a single linear pipeline. They ended with four orchestrated pipelines that could each deploy independently while respecting cross-artifact dependencies. The architectural shift from linear to graph-based is foundational.

## The Four Pipeline Paths

Your deployment system must model four distinct artifact types, each flowing through its own path. The **code path** handles application logic, API definitions, orchestration code, and infrastructure definitions. This is your traditional CI/CD pipeline unchanged. Developers commit to version control, the pipeline builds containers, runs unit tests, runs integration tests, packages artifacts, and deploys to staging then production. Code changes happen frequently—multiple times per day—and the deployment velocity should be high.

The **model path** handles trained model weights, quantized variants, and model metadata. Model changes happen less frequently—weekly, monthly, or only when performance degrades. Training produces a candidate model. The pipeline validates the training run completed successfully, runs the evaluation suite against the model, checks evaluation metrics against thresholds, publishes the model to a registry with provenance metadata, and makes it available for deployment. Model deployments are heavier—large artifacts, longer download times, potential memory pressure during swaps—so they require more careful orchestration than code deploys.

The **prompt path** handles prompt templates, few-shot examples, system instructions, and response formatting rules. Prompts change frequently—sometimes multiple times per day as Product iterates on tone and output structure. Prompt changes are lightweight—text files—but have outsized impact on model behavior. The pipeline validates prompt syntax, runs the evaluation suite with the new prompt against the current model, checks metrics, and promotes the prompt to production if quality holds. Prompt deployments are fast—no container rebuilds, no model swaps—but they require evaluation gates just as strict as model deployments.

The **configuration path** handles routing rules, temperature settings, max token limits, retry policies, fallback behavior, and provider selection logic. Configuration changes happen at variable frequency—sometimes multiple times per week during optimization sprints, sometimes once per month during stable periods. The pipeline validates configuration syntax, checks compatibility with deployed code and models, runs smoke tests to ensure configurations do not cause crashes, and deploys configurations with phased rollout. Configuration changes can break systems just as thoroughly as bad code, so they require validation even though they feel lightweight.

## The Pipeline DAG: Dependencies and Triggers

These four paths are not independent. They have dependencies. A new prompt requires evaluation against the currently deployed model. A new model requires evaluation with the currently deployed prompt. A new configuration might specify a model version that must exist in the registry. A new code deploy might expect specific model metadata fields that older models do not provide. Your pipeline architecture must model these dependencies explicitly and enforce them automatically.

The dependency graph is a directed acyclic graph where nodes are pipeline stages and edges are dependencies. The prompt evaluation stage depends on the model registry containing a valid model. The model promotion stage depends on the evaluation suite completing successfully. The configuration validation stage depends on the code deployment defining the expected configuration schema. The final deployment stage depends on all artifact paths completing and passing their gates.

Triggers determine what kicks off each path. Code commits trigger the code path. Model training completion triggers the model path. Prompt file changes trigger the prompt path. Configuration file changes trigger the configuration path. But triggers can also be manual—a model path kicked off by a platform team deciding to retrain, a prompt path kicked off by Product deciding to test a tone change. The pipeline must support both automated triggers from version control and manual triggers from authorized users.

The legal tech company's pipeline architecture used GitHub Actions for the code path, a custom orchestration system for the model path built on Argo Workflows, and a shared evaluation service that both model and prompt paths called. Configuration changes went through the code path because configurations lived in the same repository as application code. The four paths converged at a deployment gate that verified all artifacts were compatible before promoting to production. A code change could deploy independently if it did not require model or prompt changes. A prompt change could deploy independently if evaluation passed. A model change required coordination with configuration because model routing rules were configuration-driven.

## Artifact Versioning and Compatibility

Every artifact must be versioned immutably. Code gets versioned by Git commit SHA. Models get versioned by training run ID and a semantic version number. Prompts get versioned by commit SHA and a prompt version number that increments on every functional change. Configurations get versioned by commit SHA. Versions are immutable—once created, never modified. This immutability allows rollback and provides an audit trail.

Compatibility rules define which artifact versions can work together. A model trained with a specific tokenizer requires prompts that use the same tokenization rules. A code version that expects model metadata with a "training_date" field cannot deploy with a model that does not provide that field. A configuration that routes to "gpt-5-mini" requires that model to exist in your provider setup. Your pipeline must validate compatibility before deploying any combination of artifacts.

Compatibility checking happens at multiple stages. When a prompt changes, the pipeline checks whether the prompt is compatible with all currently deployed models. When a model changes, the pipeline checks whether the model is compatible with all currently deployed prompts and configurations. When code changes, the pipeline checks whether the new code is compatible with deployed model metadata schemas. Incompatibilities block deployment and surface errors describing what must change to resolve the incompatibility.

The legal tech company encoded compatibility rules as a versioned schema file that defined which model versions supported which prompt template versions. When Product updated a prompt to use a new field that only existed in models v3.0 and later, the pipeline blocked deployment until all regions were running model v3.0 or the prompt was rolled back. This prevented the partial deployment where some regions got the new prompt with old models and silently failed to use the new field.

## Pipeline Failure Handling and Notification

Every stage in every pipeline path can fail. Code fails to compile. Tests fail to pass. Evaluation suites fail to meet thresholds. Artifact uploads fail due to network issues. Compatibility checks fail due to version mismatches. Your pipeline must handle failures gracefully, notify the right people, and prevent bad artifacts from progressing.

Failure handling has three components: detection, notification, and blocking. Detection means every pipeline stage returns a clear success or failure signal with detailed error information. Notification means failures immediately alert the responsible team—code failures alert the developer who committed, model failures alert the ML team, prompt failures alert Product, configuration failures alert Platform. Blocking means failed stages prevent downstream stages from executing and prevent deployment from proceeding.

Notification routing must be context-aware. A code test failure during a developer's pull request should notify only that developer. A model evaluation failure during production promotion should notify the entire ML team and the on-call engineer. A prompt evaluation failure during a Product experiment should notify the product manager running the experiment but not page anyone at 2am. The pipeline needs different notification rules for different contexts—PR validation versus staging promotion versus production deployment.

The legal tech company configured their pipeline to fail fast and notify loudly. Any evaluation failure blocked deployment and sent a Slack message to the team channel with a link to the full evaluation results. Any compatibility failure sent a detailed error message explaining which artifacts were incompatible and what needed to change. Any model registry upload failure paged the on-call platform engineer because it indicated infrastructure issues that could block future deployments. Failures were not silent. Every failure was surfaced immediately to someone who could act on it.

## Orchestration and Concurrency Control

Multiple teams may trigger pipeline paths simultaneously. Engineering deploys code while ML promotes a new model while Product tests a prompt change. Your orchestration system must handle concurrent pipeline runs without race conditions, without conflicting deployments, and without one team's work overwriting another's.

Concurrency control requires locks and serialization at the deployment boundary. Multiple code paths can run in parallel—different developers testing different branches. Multiple prompt experiments can run in parallel in isolated environments. But only one deployment to production can proceed at a time. The final deployment stage acquires a lock, deploys all artifact changes that passed validation, releases the lock. This serialization prevents two deployments from stomping on each other.

The orchestration system must also handle partial failures gracefully. If code deploys successfully but model deployment fails mid-rollout, the system must decide: roll back code to maintain consistency, or leave code deployed and retry model deployment. The choice depends on whether the code is backward-compatible with the old model. Your orchestration logic must encode these rollback and retry policies explicitly.

The legal tech company used Kubernetes for container orchestration and a custom deployment controller that enforced serialization at the production deployment stage. The controller maintained a queue of pending deployments. Each deployment in the queue specified which artifacts were changing—code, model, prompt, configuration. The controller validated that all artifact changes passed their gates, acquired a cluster-wide lock, applied the changes in dependency order—configurations first, then models, then code, then prompts—waited for health checks, released the lock. If any step failed, the controller rolled back the entire deployment atomically.

## Pipeline Visualization and Observability

Teams need visibility into pipeline state. Which artifact versions are in which stage? Which paths are blocked on failures? How long has a model been waiting for evaluation to complete? Which deployments are queued for production? Pipeline observability is essential for debugging failures and understanding deployment velocity.

Your pipeline system should expose a dashboard showing all active pipeline runs, their current stage, their status, and their blocking dependencies. Developers should see their code moving through build, test, package stages. ML engineers should see models moving through validation, evaluation, registry publication. Product managers should see prompts moving through syntax validation, evaluation, promotion. Platform engineers should see the full dependency graph and identify bottlenecks.

The legal tech company built a custom dashboard that visualized their four pipeline paths as parallel swimlanes. Each artifact change appeared as a node moving through stages. Dependencies between paths appeared as edges connecting nodes. Failed stages appeared in red with links to error logs. The dashboard showed estimated time to completion for each stage based on historical durations. Teams used the dashboard to answer questions like "why is my prompt stuck in evaluation?" and discover that evaluation was waiting on model deployment to complete because the prompt evaluation needed to run against the newly deployed model.

## The Deployment Gate: Where All Paths Converge

The final stage of your pipeline architecture is the deployment gate—the point where all artifact paths converge and a single decision determines whether the combined set of artifacts deploys to production. The deployment gate validates that all artifacts have passed their individual gates, that they are mutually compatible, that their combined evaluation meets thresholds, and that no blocking issues exist.

The deployment gate is not just a final check. It is the enforcement point for your deployment policy. Your policy might require that at least two engineers approve code changes. That model evaluation metrics meet thresholds. That prompt changes are reviewed by Product. That configuration changes are reviewed by Platform. The deployment gate checks that all policy requirements are met before allowing deployment.

The legal tech company's deployment gate checked seven conditions: code passed tests and had two approvals, model passed evaluation and was signed by the ML lead, prompt passed evaluation and was approved by Product, configuration passed validation and was approved by Platform, all artifacts were mutually compatible, combined evaluation of the full system met production thresholds, no active incidents or maintenance windows were blocking deployments. If any condition failed, the gate blocked and surfaced the failure reason. Only when all seven conditions passed did the deployment proceed.

## Pipeline Evolution and Maintenance

Your pipeline architecture is not static. As your AI system evolves, your pipeline must evolve. You add new model types. You add new artifact types. You add new validation stages. You add new deployment targets. The pipeline must accommodate these changes without requiring a full rebuild every time.

Pipeline evolution requires modularity. Each pipeline path should be a self-contained module with a clear interface. Adding a new artifact type means adding a new module that implements the same interface—validation, evaluation, promotion, deployment. Changing how evaluation works means updating the evaluation module without touching deployment orchestration. This modularity prevents pipeline complexity from growing unboundedly as your system scales.

The legal tech company started with two pipeline paths—code and model. They added the prompt path six months later when Product wanted faster iteration on outputs. They added the configuration path nine months later when Platform needed to adjust routing rules independently of code deploys. Each addition was a new module plugged into the existing orchestration framework. The core deployment gate logic never changed. The modularity allowed them to grow their pipeline architecture incrementally without destabilizing existing deployments.

Your pipeline architecture determines your deployment velocity, your quality gates' effectiveness, and your team's ability to iterate safely. A monolithic linear pipeline creates bottlenecks and tight coupling. A modular graph-based pipeline enables parallel iteration and independent artifact evolution. The legal tech company's shift from linear to graph-based pipeline architecture doubled their deployment frequency while halving their production incidents. The architecture made safe continuous deployment of AI systems possible.

The architecture is useless without enforcement. The deployment gate's power comes from its integration with the evaluation system—the subject of the next chapter, where we examine how evaluation results flow into deployment decisions and what happens when evaluation says no.

