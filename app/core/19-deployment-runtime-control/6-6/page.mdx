# 6.6 — Cache Invalidation Policies: TTL, Event-Driven, and Manual

In July 2025, a financial services chatbot cached model responses to reduce inference costs during high-volume trading hours. The cache used a 48-hour time-to-live. The team updated their system prompt to include new regulatory disclosures required by a policy change. They deployed the new prompt at 2 PM on a Tuesday. By 4 PM, they started receiving complaints from compliance monitors who were testing the system. Users were still receiving responses generated with the old prompt — responses missing the required disclosures. The cache was serving stale responses. The team didn't have an invalidation mechanism beyond waiting for TTL expiry. They had two choices: wait 48 hours for the cache to naturally expire, or manually flush the entire cache and absorb the cost spike as it rebuilt. They flushed the cache. The incident cost $52,000 in emergency inference spending and created a compliance exposure that took weeks to fully document. The problem wasn't the cache. It was the lack of a policy for invalidating cache entries when upstream dependencies changed.

Cache invalidation is one of the hardest problems in computer science for a reason. Knowing when cached data is no longer valid requires understanding the dependencies between cached responses and the artifacts that generated them. If you invalidate too aggressively, you waste the cache — it's always empty. If you invalidate too conservatively, you serve stale responses that harm users and create compliance risk. The right policy depends on your system's tolerance for staleness, the frequency of changes to prompts and models, and the operational sophistication of your team.

## Time-Based Expiration and Its Limitations

Time-to-live is the simplest invalidation policy. Every cache entry has an expiration timestamp. After that time, the entry is considered stale and removed from the cache. The next request computes a fresh response and caches it with a new expiration. TTL requires no coordination, no dependency tracking, no operational complexity. You set a duration and the cache self-manages.

The challenge is choosing the right TTL. Too short and you invalidate entries that are still correct, reducing hit rates and wasting the cache. Too long and you serve stale responses when something changes. The right TTL depends on content type. For reference information that rarely changes — documentation, policy explanations, general knowledge queries — a TTL of 24 hours or even a week might be appropriate. For time-sensitive content — stock prices, breaking news, real-time availability — a TTL of minutes or even seconds is necessary. For conversational applications where context is unique per conversation, TTL might be measured in hours, just long enough to serve repeat queries within a single session.

Mixed TTL strategies apply different expiration times to different cache entry types. High-confidence responses to simple factual queries get long TTLs. Responses involving complex reasoning or time-sensitive data get short TTLs. Responses that include user-specific information get very short TTLs or no caching at all. This requires cache key design that allows you to identify entry type. A cache key that includes content category — factual, analytical, personalized — enables category-specific TTL policies.

TTL alone is never sufficient for systems that evolve. If you update your prompt, TTL doesn't know. If you upgrade your model, TTL doesn't know. If you change your retrieval index in a RAG system, TTL doesn't know. Cached entries generated under old configurations continue serving until they expire naturally. This creates correctness windows — periods during which the cache is a source of wrong answers. For low-risk applications, this might be acceptable. For regulated industries, it's not. TTL is a baseline policy, not a complete solution.

## Event-Driven Invalidation for Artifact Changes

Event-driven invalidation ties cache lifetime to the artifacts that generated cached responses. When a prompt changes, invalidate all entries generated with the old prompt. When a model is upgraded, invalidate all entries generated with the old model. When a retrieval index is updated, invalidate all entries that referenced documents from the old index. This requires dependency tracking — the system must know which cache entries depend on which artifacts.

The simplest implementation is artifact versioning in cache keys. Include the prompt version, model version, and index version in every cache key. When you update a prompt, increment its version number. All new requests will use the new version in their cache keys and will miss the cache, computing fresh responses. Old cache entries with the old version sit unused and eventually age out. This approach is safe — no risk of serving stale responses — but it effectively invalidates the entire cache on every artifact change, even if most cached responses would still be correct under the new artifact.

Selective invalidation is more efficient but more complex. Maintain a mapping from artifact identifiers to the cache keys that depend on them. When prompt version 7 generates a response, record that this cache key depends on prompt version 7. When you deploy prompt version 8, query the mapping to find all cache keys associated with version 7 and delete them. This requires infrastructure: a secondary index mapping artifacts to cache keys, an invalidation service that processes artifact change events and deletes dependent entries. The payoff is targeted invalidation — you only clear the cache entries that are actually affected by the change.

Cascade invalidation handles transitive dependencies. In a RAG system, a cached response might depend on a prompt, a model, and a set of retrieved documents. If the document index is updated, the cached response is potentially stale. If the prompt is updated, the cached response is definitely stale. Different dependencies have different invalidation rules. Prompt changes require full invalidation. Document index changes might require partial invalidation — only entries that referenced documents that changed. Model changes might require eval-based invalidation — compare outputs from old and new models, and only invalidate if outputs differ significantly. This level of sophistication requires deep system instrumentation but minimizes unnecessary cache loss.

## Dependency Tracking Infrastructure

To invalidate based on events, you need to know what depends on what. This requires recording dependencies at cache write time and querying them at invalidation time. The infrastructure challenge is doing this at scale without slowing down the serving path.

At write time, the serving layer must identify all artifacts that contributed to the response. For a simple prompt-model system, that's the prompt identifier and the model identifier. For a RAG system, add the document IDs that were retrieved and included in context. For a multi-stage agent system, include all prompts, models, and tools that were invoked. Serialize these dependencies and store them alongside the cached response. The cache entry becomes a tuple: the response, the cache key, and the dependency set.

At invalidation time, the system must find all cache entries that depend on a given artifact. This is an inverse lookup: given a prompt ID, find all cache entries that list that prompt ID in their dependencies. If your cache is a simple key-value store, this requires scanning all entries — prohibitively expensive at scale. The solution is a secondary index: a mapping from artifact ID to the set of cache keys that depend on it. When you write a cache entry, you also update this index. When you invalidate, you query the index to get the list of affected keys, then delete them.

Distributed cache stores complicate dependency tracking. If your cache is Redis or Memcached spread across multiple nodes, maintaining a consistent secondary index is challenging. One approach is to centralize the index in a separate service — a metadata store that tracks dependencies even though the actual cached responses live in distributed storage. Another approach is to encode dependencies in cache key patterns and use pattern-based invalidation. Cache keys that start with "prompt-v7-" depend on prompt version 7. To invalidate, delete all keys matching "prompt-v7-*". This works if your cache system supports prefix scans or pattern-based deletion.

The trade-off is complexity versus precision. Simple TTL requires no dependency tracking. Artifact versioning in cache keys requires no secondary index but invalidates more than necessary. Full dependency tracking with a secondary index provides precise invalidation but adds operational overhead. Choose based on your invalidation frequency and the cost of over-invalidation. If you update prompts once a week, over-invalidation is cheap. If you update prompts ten times a day, precision matters.

## Manual Invalidation Workflows

Sometimes you need human judgment to decide what to invalidate. A content policy change might require invalidating only cache entries related to a specific topic. A discovered bug in a retrieval component might require invalidating entries that used that component. A legal or compliance issue might require immediate deletion of all cached responses containing certain information. Manual invalidation workflows give operators control.

The basic workflow is a cache invalidation API: an authenticated endpoint that accepts a cache key or a pattern and deletes matching entries. Engineering uses this API during deployments to clear caches after artifact updates. Trust and safety uses it to remove cached responses that violate policy. Compliance uses it to purge cached data when required by regulation. The API is simple, but access control is critical — the ability to delete cache entries is the ability to cause a cost spike and latency spike by forcing all requests to compute fresh responses. Restrict access and log all invalidation requests.

Pattern-based invalidation allows operators to specify a cache key prefix, substring, or regex pattern. All entries matching the pattern are deleted. This is useful when you can't enumerate every affected cache key individually. If you know that all cache entries related to a specific domain topic include the topic ID in the cache key, you can invalidate by topic. If you version your prompts and include prompt version in cache keys, you can invalidate by version. Pattern matching requires support from your cache infrastructure — some systems provide it natively, others require you to scan keys and filter.

Scheduled invalidation runs at fixed intervals. Every night at 2 AM, clear all cache entries older than a certain threshold. Every Sunday, clear cache entries related to time-sensitive content. This is a middle ground between pure TTL and pure event-driven invalidation. The schedule is predictable, which makes capacity planning easier — you know cache hit rates will drop every night at 2 AM and recover by 4 AM. It also ensures that even if event-driven invalidation misses something, the scheduled purge will eventually clean it up.

Dry-run invalidation previews the impact of an invalidation request without actually deleting anything. An operator requests invalidation of a pattern. The system scans the cache, identifies all matching entries, and reports how many would be deleted and what the estimated cost impact would be. The operator reviews the dry-run results and either proceeds or cancels. This prevents accidental over-invalidation — discovering after the fact that a pattern matched far more entries than intended.

## Staged Invalidation to Prevent Thundering Herd

Invalidating a large portion of your cache at once creates a thundering herd problem. Thousands or millions of requests suddenly miss the cache simultaneously and hit your inference backend. If your backend can't handle the spike, requests queue, latency spikes, and timeouts cascade. Staged invalidation spreads the impact over time, allowing the cache to refill gradually without overwhelming the backend.

The simplest staged approach is randomized TTL jitter. Instead of setting TTL to exactly one hour, set it to one hour plus or minus five minutes randomly. Entries cached at the same time will expire at slightly different times. If you invalidate by updating artifact versions, you can achieve the same effect by gradually rolling out the new version. For the first ten minutes, one percent of traffic uses the new version and misses the cache. For the next ten minutes, ten percent. By the time you're at one hundred percent, the cache is partially warm.

Probabilistic invalidation deletes only a percentage of matching entries at once. When you deploy a new prompt, you might invalidate twenty percent of old cache entries immediately, another twenty percent five minutes later, another twenty percent five minutes after that, and so on. This ensures that at any given moment, eighty percent of the cache is still intact. The downside is that for a period, you're serving a mix of old and new responses, which creates inconsistency. This is acceptable if staleness is low-risk. It's not acceptable for compliance or correctness-critical changes.

Rate-limited invalidation deletes cache entries at a controlled pace. Instead of deleting all matching entries immediately, the invalidation service deletes them at a rate your backend can tolerate — say, 1,000 deletions per second. This guarantees that the thundering herd is bounded. The trade-off is time. If you have ten million cache entries to invalidate and you delete 1,000 per second, full invalidation takes nearly three hours. During that window, some users get old responses and some get new responses. The duration is predictable, which allows you to communicate expectations and plan operational coverage.

Cache prewarming before invalidation is the gold standard. Before you delete old cache entries, you proactively generate and cache responses under the new configuration. Take a sample of recent traffic, replay those queries against the new prompt or model, and write the results to cache using the new cache keys. Once the new cache is sufficiently warm, switch traffic to use the new keys and delete the old ones. This approach eliminates the thundering herd entirely — there's no cold start because the cache is already populated. It requires infrastructure to replay traffic and the ability to compute responses before serving them to users, but the operational smoothness is worth it for high-volume systems.

## Versioned Cache Keys as Implicit Invalidation

Instead of explicitly deleting cache entries, you can make them unreachable by changing the cache keys. This is what happens when you include artifact versions in keys. Old entries sit in the cache, but no request will ever ask for them because all requests use the new version in their keys. The old entries eventually age out through TTL or LRU eviction.

This approach eliminates the need for an explicit invalidation service. You never delete anything. You just change the keys. Deploy a new prompt version, and all new requests generate new cache keys that miss the existing cache and populate fresh entries. The old entries are orphaned. Over time, they expire. This is operationally simple — no invalidation API, no secondary index, no dependency tracking. The infrastructure is just a cache with TTL or LRU eviction.

The downside is memory waste. Old cache entries consume space until they expire. If you update prompts frequently, you accumulate orphaned entries. A cache that was fifty gigabytes of useful data becomes one hundred gigabytes of useful data plus fifty gigabytes of orphaned data waiting to expire. If cache memory is expensive or scarce, this waste matters. You can mitigate it with aggressive TTL — short expiration times ensure orphaned entries don't linger long. You can also periodically scan for orphaned entries and delete them manually, but at that point you're reintroducing the complexity of explicit invalidation.

Versioned keys also make rollback safe. If you deploy a new prompt and discover it's broken, you can instantly roll back by reverting to the old version. All requests immediately start using the old cache keys again and hitting the old cache entries, which are still present because you never deleted them. This is one of the strongest arguments for versioned keys — they make deployments safer and rollback instant.

## Monitoring Staleness and Invalidation Effectiveness

You can't know if your invalidation policy is working unless you measure staleness. Staleness is the degree to which cached responses differ from fresh responses. High staleness means your cache is serving wrong or outdated information. Low staleness means your invalidation policy is keeping the cache current.

The direct measurement approach is to periodically re-compute responses for cached queries and compare them to the cached responses. Sample one percent of cache hits. For each sampled hit, also compute a fresh response and compare it to the cached response using your eval suite. If the cached response and the fresh response have similar quality scores, the cache is not stale. If the fresh response scores significantly higher, the cached response is stale and should have been invalidated. This measurement gives you a staleness rate: the percentage of cache hits that are serving responses meaningfully worse than what a fresh computation would provide.

Indirect measurement tracks artifact update events and cache invalidation lag. When you deploy a new prompt at 2 PM, how long does it take before one hundred percent of cache hits are using the new prompt? If your invalidation is instant, the lag is zero. If you rely on TTL, the lag is equal to your TTL duration. If you use staged invalidation, the lag is the duration of your staging process. Track this lag over time. If lag is increasing, it means your cache is serving stale responses for longer periods, indicating a problem with invalidation.

Cache invalidation rate is another useful metric. How many cache entries are you invalidating per day, and how does that compare to cache write rate? If you're writing one million entries per day and invalidating ten thousand, your invalidation is sparse — most entries expire via TTL rather than explicit invalidation. If you're invalidating one million entries per day and writing one million, you're churning the cache heavily, which suggests either frequent artifact changes or over-invalidation. Neither rate is inherently wrong, but tracking the ratio over time shows whether your system is becoming more or less dynamic.

User-reported staleness is a lagging indicator but a critical one. If users are complaining that the system is giving outdated information, your invalidation policy is failing. Track complaints that mention "old," "outdated," "incorrect," or specific references to outdated facts. Correlate these complaints with cache hit logs to see if the problematic responses were served from cache. If you find a pattern, it indicates that your invalidation is missing cases it should be catching.

## The Correctness-Availability Trade-Off

Aggressive invalidation increases correctness but decreases cache availability. If you invalidate the entire cache every time you deploy a prompt change, you guarantee users never see stale responses, but you also guarantee they experience a latency and cost spike after every deployment. Conservative invalidation increases cache availability but risks serving stale responses. The right balance depends on your application's risk profile.

For compliance-sensitive applications, correctness dominates. Serving a response that violates regulatory disclosure requirements is unacceptable even if it only happens to one percent of users. Invalidate aggressively. Flush the cache on every artifact change. Accept the cost and latency spikes as the price of compliance. For consumer applications where staleness is low-impact, availability might dominate. It's acceptable if five percent of users see yesterday's response as long as the system stays fast and cheap. Use long TTLs and infrequent invalidation.

Hybrid policies balance both. Use aggressive invalidation for high-risk content and conservative invalidation for low-risk content. Tag cache entries by risk level. High-risk entries — anything involving financial advice, medical information, legal guidance, personally identifiable data — get short TTLs and event-driven invalidation. Low-risk entries — general FAQs, branding content, casual conversation — get long TTLs and infrequent invalidation. This requires content classification at cache write time, but it allows you to tune invalidation to the risk profile of each cached response.

Once your invalidation policy is sound, the next priority is maximizing the value you extract from the cache — improving hit rates so more requests benefit from caching rather than computing fresh responses every time.

