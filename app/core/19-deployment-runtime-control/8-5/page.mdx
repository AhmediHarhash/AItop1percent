# 8.5 — Base Model Plus Adapter Composition: LoRA Version Management

Fine-tuning is not about training one model. It is about managing the combination of a base model and an adapter—and that combination is harder to version than either alone. Most teams treat a fine-tuned model as a single artifact. They save it, version it, deploy it, and consider the job done. But when you use adapter-based fine-tuning methods like LoRA, QLoRA, or IA3, your production model is actually two separate pieces: a base model and a lightweight adapter that modifies its behavior. Those two pieces have independent lifecycles, independent version histories, and independent failure modes. If you version them as a single unit, you lose the flexibility that made adapters valuable in the first place. If you version them separately without tracking their relationship, you lose the ability to reproduce what you deployed.

The composition pattern is simple on the surface: base model plus adapter equals deployed model. The base model provides the foundational capabilities—language understanding, reasoning, world knowledge. The adapter provides task-specific behavior—how to format a legal contract, how to answer customer support questions, how to extract entities from medical records. At inference time, the adapter weights are applied to the base model's attention layers, modifying its outputs without changing the base model itself. This design is elegant and efficient. A base model might be sixty gigabytes. An adapter might be two hundred megabytes. You can store dozens of adapters for the price of one full fine-tuned model. You can switch between adapters in seconds. You can share a single base model across multiple tasks and multiple teams.

But that elegance creates a versioning challenge. When you deploy a model, which version of the base model are you using? Which version of the adapter? If you upgrade the base model, does your adapter still work? If you retrain your adapter, which base model did you train it against? If something breaks in production, can you roll back just the adapter, or do you need to roll back the base model too? These questions have no good answers if your versioning system treats composition as an afterthought.

## The Composition Identifier

Every deployed model in an adapter-based system needs a composition identifier—a version string that captures both the base model and the adapter as a single addressable unit. The simplest form is a tuple: base model version plus adapter version. In practice, this might look like base:gpt-5-mini-20260115 plus adapter:legal-contract-v3.2 equals deployment:legal-contract-20260201. The deployment identifier is what your serving infrastructure uses. It points to a specific base model and a specific adapter. If you need to reproduce that exact behavior, you need both pieces. If you want to upgrade just the adapter, you change only that part of the tuple and create a new deployment identifier.

Some teams use hash-based identifiers instead of semantic versions. The base model gets a content hash. The adapter gets a content hash. The composition gets a combined hash. This approach guarantees uniqueness and makes it impossible to accidentally deploy mismatched components. But it sacrifices human readability. An identifier like base:a3f9d2 plus adapter:7c4e1b does not tell you anything about what the model does or when it was trained. Semantic versioning is slower to implement but easier to debug. Hash-based versioning is faster to automate but harder to reason about in an incident. Most production systems use semantic versioning for base models and adapters, then generate a combined hash for the deployment identifier. You get readability during development and uniqueness during deployment.

The critical rule is consistency. Every model serving request must log which composition identifier it used. Every evaluation run must record which composition it tested. Every rollback must specify which composition to return to. If your logs say "we deployed legal-contract-v3" but do not say which base model it was composed with, you cannot reproduce the behavior when something goes wrong.

## Base Model Updates and Adapter Compatibility

Base models do not stand still. Model providers release updated versions every few months. GPT-5-mini gets a patch that improves reasoning on arithmetic. Claude Sonnet 4.5 gets a security update that changes how it handles certain prompt patterns. Llama 4 Scout gets a fine-tuning-friendly variant with better gradient stability. Each of these updates changes the base model slightly. Sometimes the changes are invisible to adapters. Sometimes they break everything.

The compatibility question is: if you trained your adapter against base model version A, will it still work correctly when applied to base model version B? The answer depends on what changed between A and B. If the model architecture stayed identical and only the weights changed slightly, your adapter will probably work. Its performance might shift by a few percentage points, but it will not catastrophically fail. If the model architecture changed—different attention head configurations, different layer normalization, different tokenization—your adapter will fail to load or produce nonsense outputs.

Most teams discover compatibility problems the hard way. They upgrade their base model to get a performance boost or a cost reduction. They assume their existing adapters will keep working. They deploy to production. Accuracy drops by thirty percent overnight. The root cause: the new base model uses a slightly different tokenization scheme, so the adapter's learned weights are now misaligned with token boundaries. The fix requires retraining every adapter against the new base model. That process takes weeks. The rollback is immediate and embarrassing.

The mature approach is to maintain a compatibility matrix. This is a simple table that records which adapter versions are validated against which base model versions. When you train an adapter, you record which base model you used. When you upgrade a base model, you run a compatibility test suite against all existing adapters. Adapters that pass get marked as compatible. Adapters that fail get marked as requiring retraining. Adapters that produce degraded but acceptable performance get marked as provisional. Your deployment system enforces the matrix. You cannot deploy an adapter with a base model unless the matrix says they are compatible.

Building the compatibility matrix is not a one-time effort. Every time a new base model version arrives, you need to test every adapter. Every time you train a new adapter, you need to test it against every base model version you plan to support. This sounds expensive. It is less expensive than discovering incompatibility in production.

## Adapter Proliferation and Runtime Selection

Adapter-based architectures are efficient until you have fifty adapters. Then they become a versioning nightmare. Each adapter serves a different task: legal contract generation, customer support triage, medical entity extraction, code review summarization, invoice parsing. Each adapter has its own version history. Each adapter was trained against a specific base model version. Each adapter has different accuracy requirements and different rollback policies. Your serving infrastructure needs to route incoming requests to the correct adapter, compose it with the correct base model, and return a response—all in under two hundred milliseconds.

The selection problem is which adapter to use for a given request. The naive approach is manual routing: the client specifies the adapter in the request metadata. This works when you have five adapters and a small engineering team. It fails when you have fifty adapters and product teams who do not know which adapter corresponds to which task. A better approach is task-based routing: the request specifies a task identifier, and the serving system maintains a task-to-adapter mapping. The mapping is versioned separately from the adapters themselves. You can update the mapping to point to a new adapter without changing client code.

The versioning complexity multiplies with every adapter you add. If you have fifty adapters and each adapter has been retrained five times, you are managing two hundred fifty adapter versions. If you support three different base model versions, you are managing potential compatibility relationships between seven hundred fifty combinations. Not all of those combinations are valid or tested. Your versioning system needs to know which ones are safe to deploy and which ones are untested or explicitly incompatible.

Some teams adopt a pruning policy to control proliferation. Old adapter versions are archived after ninety days unless they are still in active use. Adapters that have been superseded by retraining are deprecated unless a team requests continued support. This reduces the surface area, but it creates risk. If an old adapter is still running in a forgotten internal tool and you delete it, that tool breaks. The safer approach is soft deprecation: mark old versions as deprecated, log warnings when they are used, but keep them available until usage drops to zero.

## Adapter Retraining Triggers

Knowing when to retrain an adapter is as important as knowing how to version it. Adapters degrade for predictable reasons. The base model gets updated and the adapter's learned behavior is no longer aligned. The task distribution shifts and the adapter's training data no longer represents what users need. New regulations or policies require changes to output formatting or content filtering. A competitor launches a feature that requires improving your adapter's capabilities.

Each of these triggers requires a decision: retrain the adapter or create a new one. Retraining preserves the version lineage. You train a new version of the same adapter, test it, and deploy it as a replacement. Creating a new adapter starts a new version lineage. You might do this if the task has changed so much that the old adapter is no longer relevant. The versioning system needs to capture both paths. An adapter version history might look like this: legal-contract-v1 trained in January, legal-contract-v2 retrained in March after base model upgrade, legal-contract-v3 retrained in June with expanded training data, legal-contract-v2 still deployed in a legacy system that cannot upgrade yet.

The retraining decision is a cost-benefit trade-off. Retraining takes time, compute, and data labeling effort. If the base model upgrade improves accuracy by two percent but retraining costs fifteen thousand dollars, you might skip it. If the base model upgrade changes tokenization and breaks the adapter completely, you have no choice. Your versioning system should track why each adapter version was created: base model upgrade, data refresh, task expansion, bug fix, regulatory requirement. This metadata helps future teams understand the lineage and decide whether to continue the lineage or start fresh.

## Composition Testing and Validation

Testing a composed model is different from testing a base model or an adapter in isolation. The base model might pass all its benchmarks. The adapter might pass all its task-specific evaluations. But when you combine them, new failure modes appear. The base model's updated reasoning capabilities might conflict with the adapter's learned shortcuts. The adapter might amplify a bias that was subtle in the base model but becomes obvious in composition. The combined model might produce outputs that are technically correct but violate formatting expectations.

Every composition needs its own evaluation run before deployment. You cannot assume that because base model version X and adapter version Y both passed their tests, the combination will work. The test suite for a composition includes the adapter's task-specific metrics, a sample of the base model's general capabilities, and interaction tests that check for conflicts. Interaction tests are the hardest to design but the most valuable. They probe whether the adapter interferes with base model capabilities that your users depend on but are not explicitly part of the adapter's task. A legal contract adapter should not break the base model's ability to answer questions in Spanish. A customer support adapter should not break the base model's code generation capabilities if users sometimes paste code into support requests.

When a composition fails validation, you need to determine which component is at fault. Did the base model change in a way that breaks the adapter? Did the adapter's retraining introduce a bug? Did the combination expose a latent issue that neither component had in isolation? The debugging process requires comparing the failed composition against known-good compositions. If base:v1 plus adapter:v3 worked, but base:v2 plus adapter:v3 fails, the base model is the likely culprit. If base:v2 plus adapter:v2 worked, but base:v2 plus adapter:v3 fails, the adapter is the likely culprit. If both base:v1 plus adapter:v3 and base:v2 plus adapter:v3 fail, but base:v1 plus adapter:v2 worked, you might have a compatibility issue that only appears with specific version combinations.

This requires systematic composition testing. Every new base model version gets tested with all current adapter versions. Every new adapter version gets tested with all supported base model versions. The results populate your compatibility matrix. The matrix is not a static document. It is a living database that gets updated every time you train or deploy.

## Multi-Adapter Composition

Some systems compose multiple adapters with a single base model. You might apply a task-specific adapter to handle contract generation, then apply a second adapter to enforce tone and style guidelines, then apply a third adapter to remove sensitive information. Each adapter modifies the base model in a different way. The order matters. Apply tone before task-specific behavior and you get different outputs than applying task-specific behavior before tone.

Multi-adapter composition creates a versioning problem that is exponentially more complex than single-adapter composition. Now you are not just versioning base plus adapter. You are versioning base plus adapter-A plus adapter-B plus adapter-C plus the order in which they are applied. If you have three adapters and each has five versions, you have one hundred twenty-five possible combinations for each base model version. Not all combinations are valid. Not all are tested. Your versioning system needs to capture which combinations are approved for production and which are experimental.

The identifier format for multi-adapter composition typically uses a stack notation: base:gpt-5-mini-20260115 plus adapters in order: task:legal-v3, tone:formal-v2, safety:pii-redact-v1. The combined identifier becomes something like legal-formal-safe-20260201. The system must enforce that the order is preserved. Deploying task:legal-v3 plus tone:formal-v2 is not the same as deploying tone:formal-v2 plus task:legal-v3.

Testing multi-adapter compositions is expensive. You cannot test every combination. Instead, you define a set of supported stacks and test those explicitly. Unsupported stacks are blocked at deployment time. If a team wants to try a new adapter combination, they must add it to the supported set, run the full test suite, and get approval. This slows down experimentation but prevents untested compositions from reaching production.

## Version Pinning and Drift Prevention

Pinning is the practice of locking a deployment to specific versions instead of using version ranges or "latest" tags. In an adapter-based system, pinning means specifying the exact base model version and the exact adapter version for every deployment. No wildcards. No automatic upgrades. If you deploy base:gpt-5-mini-20260115 plus adapter:legal-v3.2, that deployment stays on those versions until you explicitly change it.

Pinning prevents drift. Without pinning, your production model might silently change when the base model provider releases an update. Your adapter stays the same, but the base model underneath it shifts. Behavior changes. Accuracy drops or improves in ways you did not predict. Users notice. You do not know what changed because your logs do not capture the base model version—they just say "gpt-5-mini" without the date stamp.

The cost of pinning is manual upgrade effort. When a new base model version arrives, you need to decide whether to upgrade, test the new composition, and deploy. You cannot rely on automatic updates to keep you current. But that manual effort is the point. Every upgrade is intentional. Every composition change is tested. Every deployment is reproducible.

Some systems use version ranges for non-critical deployments and strict pinning for critical ones. An internal tool that generates meeting summaries might use adapter:summary-v3.x with base:gpt-5-mini-latest. The latest minor version updates are acceptable because the risk is low. A customer-facing legal contract generator uses adapter:legal-v3.2.1 with base:gpt-5-mini-20260115. No automatic updates. Every change is deliberate.

The versioning system enforces pinning policies. You define which deployments require strict pinning and which allow version ranges. The system blocks deployments that violate the policy. If a critical deployment tries to use a version range, the deployment fails with an error. This prevents accidental configuration drift.

## Adapter Lifecycle and Archival

Adapters have lifecycles just like models. They are created, tested, deployed, run in production for months or years, get superseded by newer versions, get deprecated, and eventually get archived. The versioning system needs to track every stage. An adapter in the created state is not yet approved for production. An adapter in the deployed state is running live traffic. An adapter in the deprecated state is still available but should not be used for new deployments. An adapter in the archived state is stored for historical reference but cannot be deployed.

The lifecycle transitions are recorded as versioning events. When legal-v3.2 moves from testing to deployed, that transition is logged with a timestamp, the person who approved it, and the deployment identifier. When legal-v3.1 moves from deployed to deprecated because legal-v3.2 replaced it, that transition is logged too. When legal-v2.0 moves from deprecated to archived after six months with no usage, that final transition is logged. The audit trail lets you reconstruct the entire history of which adapters were in production at any point in time.

Archival does not mean deletion. Archived adapters are stored in long-term cheap storage. They are not loaded into memory. They are not available for new deployments. But they are preserved for compliance and debugging. If a customer from two years ago surfaces a complaint about a contract that was generated in 2024, you need to be able to reconstruct the exact model that generated it. That might mean retrieving an archived adapter, loading the base model version from that era, and running an inference with the same inputs. Without archival, this is impossible.

The versioning system maintains a map from deployment dates to adapter versions. For every day your system was in production, you can query which adapters were active. This map is critical for compliance. If a regulator asks what version of your model was running on a specific date, you need to answer immediately and correctly. The map is also critical for root cause analysis. If you discover a systemic error in old outputs, you need to know which adapter version was responsible and which other outputs might be affected.

Understanding how to version the composition of base models and adapters—tracking compatibility, managing proliferation, testing combinations, enforcing pinning, and archiving lifecycle stages—is essential for any production system that uses adapter-based fine-tuning. But composition is only one part of the versioning story. Every model, whether composed or monolithic, carries metadata about how it was created. That metadata—training data version, hyperparameters, code commit, lineage—is what makes reproduction possible when something goes wrong, and tracking it systematically is the subject of the next challenge.
