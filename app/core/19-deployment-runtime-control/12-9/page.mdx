# 12.9 — Deployment Triggers: Automated vs Manual vs Scheduled

It's 3:17 AM Pacific Time when the deployment kicks off. Traffic is at its daily low — fewer than 200 requests per minute across all endpoints. The merge to main happened six hours earlier, but the scheduled deployment window doesn't open until after midnight. The CI/CD pipeline runs through its stages: build, test, eval verification, canary preparation. Twenty-three minutes later, the new prompt template is live in production. Nobody is watching. The on-call engineer is asleep. The dashboard shows green across all metrics for the first hour. By 4:30 AM, latency has climbed from 340 milliseconds to 890 milliseconds. The new template added three extra reasoning steps that weren't caught in staging because staging traffic patterns don't match production's international distribution. When the European morning rush hits at 6 AM London time, timeout rates spike to 12 percent. The first alert fires at 6:47 AM Pacific — three and a half hours after deployment. The on-call engineer wakes up, investigates for 20 minutes, and initiates rollback. Total customer impact: 94,000 requests with degraded experience. The deployment itself was flawless. The trigger timing was catastrophic.

Deployment triggers determine when changes move to production. The choice between automated, manual, and scheduled triggers is not a philosophical preference — it's a risk-versus-velocity trade-off that depends on the type of change, the maturity of your testing infrastructure, and the consequences of getting it wrong. Every trigger type has failure modes. The art is matching trigger type to change characteristics so that the deployment happens when the system is most prepared to handle problems.

## The Three Trigger Archetypes

Automated triggers deploy on every merge to the main branch. The moment CI passes and eval gates clear, the change moves to production without human intervention. This is the fastest path from code to customer, and it only works when you trust your automated checks completely. If your eval suite catches 95 percent of regressions but deploys run ten times per day, you're shipping a regression every other day. Automated triggers demand extreme confidence in your testing infrastructure.

Manual triggers require a human to click the deploy button. Someone reviews the change, checks the timing, confirms that the system is ready, and initiates deployment. This adds latency — sometimes minutes, sometimes hours if the person responsible is in meetings or asleep. But it also adds judgment. A human can look at production metrics, check for ongoing incidents, verify that customer support isn't handling an unusual spike, and decide that now is not the time. Manual triggers are slower but safer when your automated checks don't cover every failure mode.

Scheduled triggers deploy at specific times: every Tuesday at 10 AM, every night at 2 AM, the first Monday of each month. The timing is predictable. Teams can prepare. Customer support knows when to expect issues. Monitoring is staffed. But scheduled triggers also batch changes. If you deploy once per week, every deployment contains seven days of commits. When something breaks, isolating the culprit is harder. Scheduled triggers trade deployment frequency for predictability.

## Choosing Trigger Type by Change Characteristics

Not all changes deserve the same trigger discipline. A spelling fix in a user-facing string is not the same risk as a new RAG retrieval strategy. The trigger type should match the blast radius and the observability you have into potential failures.

Use automated triggers for changes with narrow, well-understood impact. Configuration adjustments that affect latency by milliseconds. Prompt tweaks that alter phrasing but not logic. Model version upgrades where you've run comprehensive eval comparisons and seen no regressions. These changes are safe to ship continuously because your eval suite already confirmed safety, and production monitoring will catch outliers within minutes. Automated triggers maximize velocity when the risk is contained.

Use manual triggers for changes with broad, hard-to-test impact. New agent reasoning paths that interact with external APIs. Major prompt architecture overhauls that change the entire conversation flow. Introduction of a new model family you haven't used in production before. These changes might pass eval but still surface unexpected behavior under production traffic patterns. A human trigger allows someone to watch the deployment closely, monitor for anomalies in real time, and initiate rollback at the first sign of trouble. Manual triggers add human judgment when automated checks can't cover the full risk surface.

Use scheduled triggers for changes that require coordination across teams or systems. Database schema migrations that must happen before the model deployment. Feature launches that depend on marketing, customer support, and engineering being ready simultaneously. Large batches of changes accumulated over a sprint that you want to test together in production. Scheduled triggers provide coordination points when multiple moving parts must align.

## Trigger Conditions and Blocking Logic

A trigger is not just a button or a clock. It's a set of conditions that must all evaluate to true before deployment proceeds. Trigger conditions act as runtime gates — checks that happen at trigger time, not at build time. They verify that the system is in a safe state to receive changes.

Common trigger conditions include eval gate passage, which confirms that the new version passed all required quality thresholds. Production health checks, which verify that the current production environment is stable — no ongoing incidents, no elevated error rates, no unusual traffic spikes. Approval requirements, which mandate that a specific person or role has reviewed and signed off on the deployment. Time-based conditions, which prevent deployments outside of approved windows. Dependency readiness, which confirms that any prerequisite deployments have already completed.

Blocking conditions are the inverse — they prevent triggers from firing even if all positive conditions are met. An active incident blocks deployment. A production error rate above threshold blocks deployment. A recent rollback within the past hour blocks deployment to prevent thrashing. A failed dependency health check blocks deployment. A manual hold flag set by an engineer investigating an issue blocks deployment. Blocking conditions act as circuit breakers — they stop deployments when the system is too fragile to handle more change.

The sophistication of your trigger logic correlates with deployment safety. Simple trigger logic — deploy on merge if tests pass — works when changes are low-risk and monitoring is fast. Complex trigger logic — deploy on merge if tests pass and no incidents in the past six hours and production latency below 400 milliseconds and approval from two reviewers — works when changes are high-risk and the cost of a bad deployment is severe. The trap is adding so many conditions that deployments become rare. If your trigger conditions are so strict that you deploy once per week, you've built a scheduled trigger system with automated trigger complexity.

## Trigger Chaining and Cascading Deployments

Some deployments trigger other deployments. A model upgrade in the embeddings service requires redeployment of the RAG pipeline that consumes those embeddings. A prompt template change in the classification layer requires redeployment of the downstream routing logic that depends on classification outputs. Trigger chaining automates these cascades.

Chaining works through dependency graphs. Service A declares that it depends on Service B. When Service B deploys, the CI/CD system checks for dependent services and queues their deployments automatically. The dependent deployments wait for the parent deployment to complete and pass health checks before proceeding. If the parent deployment fails or rolls back, the dependent deployments are canceled. This ensures that changes propagate in the correct order without manual coordination.

The risk in trigger chaining is cascade failures. If Service B deploys a breaking change and Service A automatically deploys afterward, you've now broken two services instead of one. Chaining amplifies both velocity and blast radius. The mitigation is to treat chained deployments as lower-priority canaries. Deploy the parent service to a small percentage of traffic. If it succeeds, deploy dependents to the same small percentage. Expand gradually. This way, a cascading failure affects only the canary segment, not full production.

## Time-Based Trigger Strategy

When you deploy matters as much as what you deploy. A deployment during peak traffic exposes problems quickly but affects the most users. A deployment during off-peak hours minimizes blast radius but delays problem detection. The optimal timing depends on your monitoring speed and rollback latency.

Deploy during business hours if your monitoring is fast and your rollback is automated. Business hours mean engineers are online, customer support is staffed, and leadership is available if escalation is needed. You detect problems in minutes, not hours. You can coordinate rollback and mitigation without waking anyone up. The trade-off is that more users are affected if something goes wrong. Business hours deployments are high-visibility, high-support events.

Deploy during off-peak hours if your monitoring is slow or your rollback requires human intervention. Off-peak means fewer users affected during the initial rollout. You have time to detect issues, investigate root causes, and decide on mitigation before the morning traffic surge. The trade-off is that if something breaks and you don't notice until the next morning, the system has been degraded for hours. Off-peak deployments are low-visibility, high-latency events.

The best teams deploy during business hours with automated monitoring and automated rollback. This combines fast detection with minimal blast radius. The deployment happens when people are watching, but the people don't need to do anything unless automation fails. This is the mature state — where deployment timing is chosen for observability, not for fear.

## Trigger Observability and Audit Trails

Every deployment trigger should generate an audit record. Who or what initiated the deployment. What conditions were evaluated. Which conditions passed and which were skipped. What time the trigger fired. What version is being deployed. What version is being replaced. This audit trail is essential for post-incident analysis and compliance requirements.

Trigger observability includes real-time visibility into pending deployments. A dashboard that shows which deployments are queued, which are in progress, which are waiting on conditions, and which are blocked. Engineers should be able to see at a glance what's about to deploy and why. This prevents surprise deployments — situations where someone discovers a deployment in progress and has no context for why it's happening.

Manual trigger observability includes notification systems. When a deployment is ready to trigger but waiting for manual approval, notify the responsible person. When a scheduled deployment window is about to open, notify the team. When an automated deployment is blocked by a condition, notify the team so they can investigate why the system is not in a deployable state. Observability without notification leads to missed deployments and stale changes sitting in the queue.

The deployment trigger is the most critical decision point in your CI/CD pipeline. It's where velocity meets safety, where automation meets judgment, and where the change you've built finally reaches customers. Get the trigger wrong and even perfect code ships at the wrong time to the wrong audience. Get it right and deployments become routine, predictable, and safe.

Triggers themselves are just decision points. The real challenge is knowing whether the deployment succeeded or failed after the trigger fires. That requires pipeline observability — tracking deployment health from trigger to completion and beyond.

