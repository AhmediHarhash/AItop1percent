# 12.5 â€” Artifact Promotion: Only After Gate Pass, From Registry Only

The most dangerous artifact is the one that skips the registry. Every shortcut becomes a production incident. The engineer who deploys directly from their build output bypasses every safety mechanism the organization built. The team that promotes without passing gates discovers their failures in production instead of in testing. The system that pulls artifacts from multiple sources has no way to audit what actually ran when something goes wrong. In 2026, the discipline of artifact promotion separates mature AI operations from teams that are one bad deploy away from a crisis.

Artifact promotion is not just moving files between environments. It is the enforcement of a contract: this specific artifact, identified by this exact hash, passed these specific gates, was approved by these specific people, and is now authorized to run in this environment. Every deviation from this contract is a risk that production will eventually collect.

## The Promotion Pattern: Artifacts Move Through Environments Only After Passing Gates

An artifact begins its life in a build pipeline. A model is fine-tuned and exported. A prompt template is versioned and packaged. A configuration file is validated and wrapped with metadata. At this moment, the artifact is unproven. It has not been tested. It has not been reviewed. It exists but it is not yet trustworthy.

The artifact is pushed to a registry with a unique identifier: a content hash, a semantic version, a build number. The registry becomes the single source of truth for what artifacts exist and what state they are in. The artifact is tagged with metadata: who built it, when, from which commit, with what parameters. This metadata is permanent and immutable.

Before the artifact can move to the first testing environment, it must pass gates. The gates vary by artifact type and organization risk tolerance, but the pattern is universal. A model must pass basic inference tests: does it load, does it respond to standard inputs, does it meet minimum quality thresholds on a holdout set. A prompt must pass schema validation: does it contain required fields, does it respect token limits, does it avoid known problematic patterns. A configuration file must pass safety checks: does it specify valid endpoints, does it stay within resource limits, does it avoid dangerous flag combinations.

Each gate that passes updates the artifact's metadata in the registry. The artifact accumulates a record: passed schema validation at this timestamp, passed quality threshold at this timestamp, passed security scan at this timestamp. When all required gates pass, the artifact is tagged as ready for the next environment. Only then can promotion occur.

## Registry as Single Source: Production Pulls Only From Registry, Never From Build Output

A financial services company in early 2025 ran a deployment pipeline where engineers could deploy models either from the registry or directly from their local build output for speed. The registry path took twenty minutes because it included scanning and validation. The direct path took three minutes. For low-risk changes, engineers used the direct path with manager approval. For six months this worked fine. Then an engineer deployed what they believed was a tested model directly to production. The model they deployed was one commit behind the model they tested. The untested commit introduced a regression that caused the model to fail on a specific class of inputs that represented eight percent of production traffic. The issue was discovered when customer complaints spiked. The incident took four hours to resolve because the team had no authoritative record of which artifact was actually running in production. The model hash in the deployment logs did not exist in the registry. The engineer had built the model locally, deployed it, and then deleted the build directory. The team eventually had to roll back to the previous registry artifact and rebuild the intended model from scratch.

The lesson: production must pull only from the registry, never from build output, never from local artifacts, never from alternative sources. The registry is the single source of truth. If an artifact is not in the registry, it does not exist. If production is not pulling from the registry, the organization has no authoritative record of what is running.

This discipline requires enforcement at multiple layers. The deployment pipeline must be configured to reject any artifact that does not come from the registry. The registry URL must be the only allowed source in production deployment configurations. The CI/CD system must validate that the artifact hash exists in the registry before initiating deployment. The infrastructure must be locked down so that engineers cannot bypass the pipeline and deploy directly.

The registry also provides the foundation for rollback. When something goes wrong in production, the team does not scramble to find the previous version or rebuild it from git history. They look at the registry, identify the last known good artifact, and redeploy it. The registry preserves all promoted artifacts with their full metadata, making rollback a matter of selecting a previous version rather than reconstructing history.

## Promotion Criteria: What Must Be True Before an Artifact Can Move to the Next Environment

Promotion criteria are not suggestions. They are hard requirements that must be met before an artifact can advance. The criteria vary by environment and artifact type, but they share a common property: they are objective, measurable, and automatically enforced.

For moving from build to development environment, the criteria are minimal. The artifact must exist in the registry, must have valid metadata, and must pass basic structural validation. A model must load without error. A prompt must parse without syntax errors. A configuration must contain required fields. These gates catch the most obvious failures before they consume testing resources.

For moving from development to staging, the criteria become substantive. The artifact must pass functional tests: does it produce outputs for standard inputs, does it handle edge cases, does it maintain acceptable latency and resource usage. The artifact must pass quality gates: does it meet minimum accuracy thresholds on the validation set, does it avoid known failure modes, does it maintain parity with the baseline on critical metrics. The artifact must pass security scans: does it avoid injection vulnerabilities, does it respect access controls, does it comply with data handling policies.

For moving from staging to production, the criteria become comprehensive. The artifact must pass all previous gates plus production-readiness checks. It must complete a full regression suite that covers both new functionality and existing behavior. It must pass load testing that simulates production traffic patterns. It must pass a manual review by a designated approver who verifies that the change is understood and the risk is acceptable. It must have a rollback plan documented and tested. It must have monitoring configured so that issues are detected immediately after deployment.

Each criterion is implemented as an automated check that passes or fails with a clear reason. The promotion pipeline blocks until all criteria pass. There is no override button that bypasses gates without leaving an audit trail. When a gate fails, the artifact does not advance. When all gates pass, the artifact is tagged as eligible for promotion, but promotion does not happen automatically. It waits for an explicit promotion trigger.

## Manual vs Automated Promotion: When Each Makes Sense

Automated promotion moves artifacts through environments without human intervention once gates pass. Manual promotion requires a human to review gate results and explicitly approve the move. The choice depends on risk tolerance, change frequency, and organizational maturity.

Automated promotion makes sense for low-risk changes in mature systems with comprehensive gates. A prompt wording change that passes all quality gates and has no downstream dependencies can be promoted automatically from development to staging. A configuration change that adjusts a cache timeout can move through environments automatically if regression tests confirm no impact. A model retrained on new data with the same architecture and hyperparameters can advance automatically if quality metrics remain within expected ranges.

Manual promotion makes sense for high-risk changes, novel deployments, and organizations still building confidence in their gates. A new model architecture that has never been deployed before requires human review before moving to staging. A prompt change that affects a customer-facing feature requires manual approval before production. A configuration change that modifies rate limiting or circuit breaker thresholds requires human judgment about operational impact.

The industry trend in 2026 is toward automated promotion with human checkpoints at critical boundaries. Development to staging is often fully automated: if all gates pass, the artifact promotes overnight. Staging to production is often semi-automated: gates run automatically, but a human must review results and click approve before promotion executes. This balances speed with safety. The team moves fast within safe boundaries and slows down at boundaries where mistakes are expensive.

Some organizations implement time-based promotion: artifacts that pass all gates can be promoted automatically if they have been stable in the current environment for a minimum duration. A model that passes all gates and runs in staging for twenty-four hours without incident can be promoted to production automatically. This pattern reduces promotion bottlenecks while ensuring that artifacts get real-world validation before advancing.

## Promotion Audit Trail: Recording Every Promotion Decision

Every promotion creates an immutable audit record. The record captures who initiated the promotion, when, which artifact was promoted, which environment it was promoted to, which gates passed, which approvals were obtained, and what the system state was at the time of promotion. This record is stored outside the artifact itself, typically in a deployment database or audit log system.

The audit trail serves multiple purposes. It provides accountability: when something goes wrong, the team knows exactly which artifact was promoted, who approved it, and what checks passed. It provides traceability: compliance teams can prove which version of a model was deployed when and who authorized it. It provides forensics: when investigating an incident, the team can correlate system behavior with specific deployments.

A healthcare AI company in mid-2025 faced a regulatory audit that required proving which version of their clinical decision support model was deployed during a specific three-week period eighteen months earlier. The team had the artifact in the registry, but they did not have a detailed promotion audit trail. They could prove which version was built when, but they could not prove when it was promoted to production or who approved it. The deployment logs showed timestamps but not approval records. The team eventually reconstructed the timeline from git commits, Slack messages, and engineer interviews, but the audit dragged on for two months longer than necessary. After the audit, the team implemented comprehensive promotion audit trails that captured every promotion decision with full context.

The audit trail must be tamper-proof. It cannot be edited after the fact, even by administrators. It is typically stored in an append-only log or a write-once storage system. The trail is indexed by artifact, environment, timestamp, and approver, making it easy to query for compliance or incident response.

## Promotion Rollback: Demoting an Artifact That Shouldn't Have Been Promoted

Promotion is not always one-way. Sometimes an artifact that was promoted to staging or production needs to be demoted. The artifact passes gates, gets approved, deploys successfully, and then reveals a problem that gates did not catch. The team discovers the issue hours or days later and needs to roll back.

Promotion rollback is not the same as deployment rollback. Deployment rollback swaps the running artifact for a previous version. Promotion rollback revokes the artifact's authorization to run in the environment and removes its promotion status in the registry. The artifact remains in the registry with full history, but it is no longer marked as approved for that environment.

A promotion rollback typically follows this sequence. The team identifies the problem and confirms it is caused by the newly promoted artifact. They execute a deployment rollback to restore the previous artifact in the affected environment. They update the registry to revoke the problematic artifact's promotion status. They add metadata explaining why the promotion was revoked. They update gates or review processes to prevent similar issues from passing in the future.

Promotion rollback is less common than deployment rollback because promotion happens less frequently and involves more gates. But when it happens, it is critical to handle it cleanly. The problematic artifact must be clearly marked in the registry so that it cannot be accidentally re-promoted. The rollback must be documented so that future teams understand why the artifact was demoted. The audit trail must capture the demotion decision with the same rigor as the original promotion.

## Cross-Environment Consistency: Ensuring the Same Artifact Runs Everywhere

One of the hardest problems in multi-environment deployment is drift. The model running in staging is version 2.3.1, but the model running in production is version 2.2.8, and the model running in development is version 2.4.0-alpha. The team intends to promote 2.3.1 to production, but they are testing features against 2.4.0-alpha, and production is lagging behind because a promotion got delayed.

Cross-environment consistency means the same artifact version runs in all environments, or the differences are explicitly tracked and understood. The goal is not to force every environment to run the exact same version at all times. Development environments naturally run ahead because they test new features. Production environments naturally lag behind because they prioritize stability. But the drift must be visible, intentional, and bounded.

A logistics company in late 2025 ran into a subtle consistency issue. Their staging environment ran a model that had been promoted two weeks earlier. Their production environment ran a model that had been promoted four weeks earlier. Both were officially "current" because production had not been scheduled for an upgrade yet. A new feature was tested in staging, passed all gates, and was approved for production promotion. The feature depended on a model behavior that existed in the staging model but not in the production model. The team did not catch this dependency because their tests assumed the models were the same version. When the feature deployed to production, it failed because the underlying model did not support the required behavior. The team had to emergency-promote the staging model to production to fix the feature, which introduced risk because that model promotion had not been planned or reviewed.

The fix was environment versioning visibility. The deployment dashboard showed which artifact version was running in each environment and highlighted any drift beyond acceptable thresholds. The team set a policy that staging and production could differ by at most one promoted artifact version. If staging got too far ahead, production was scheduled for a catch-up promotion. If production needed to stay on an older version for stability, staging was held back from advancing further. This discipline ensured that features tested in staging would work in production because the environments were version-compatible.

## Promotion Velocity: Balancing Speed with Safety

Promotion velocity is the rate at which artifacts move through environments. High velocity means artifacts advance quickly from build to production, often within hours or days. Low velocity means artifacts spend extended time in each environment, often weeks. The right velocity depends on the system's maturity, the risk of changes, and the cost of delays.

In 2026, mature AI teams aim for high promotion velocity with high confidence. They achieve this through comprehensive gates, automated testing, and staged rollouts that catch issues early. An artifact that passes all gates in development can be promoted to staging within an hour. An artifact that passes all gates in staging and runs stably for twenty-four hours can be promoted to production the next business day. This velocity is sustainable because the gates are trustworthy and the rollback process is fast.

Less mature teams often have low promotion velocity not because they are cautious but because they lack confidence in their gates. An artifact sits in staging for two weeks not because it is being tested but because no one is sure what to test or when it is safe to promote. The promotion decision becomes a judgment call made by senior engineers who are too busy to review promptly. The low velocity does not improve safety; it just slows down the team.

The fix is to make promotion velocity a measured metric. Track how long artifacts spend in each environment and why. If artifacts are sitting in staging waiting for manual review, automate more of the review or dedicate reviewer capacity. If artifacts are sitting because gates take too long to run, optimize gate execution or run gates in parallel. If artifacts are sitting because teams fear promoting, add monitoring and rollback automation to reduce the cost of mistakes.

Promotion velocity should increase over time as the team builds confidence in their gates and their ability to detect and fix issues quickly. A team that takes three weeks to promote an artifact to production in their first quarter should aim for one week by their third quarter and one day by their fifth quarter. This acceleration is not recklessness; it is maturity.

## Anti-Patterns: Direct Deployment, Skipping Environments, Emergency Bypasses That Become Routine

The most common anti-pattern is direct deployment: building an artifact and deploying it to production without pushing it to the registry or passing through gates. Engineers justify this as a time-saver for trivial changes. It works fine until a trivial change turns out not to be trivial. The team loses traceability, loses audit trail, and loses the ability to roll back cleanly.

Another common anti-pattern is skipping environments. The team promotes an artifact from development directly to production, bypassing staging because the change is urgent or because staging is broken or because no one wants to wait. Skipping environments eliminates the safety net that staging provides. Staging is where integration issues are caught, where load testing happens, where manual review occurs in a production-like environment without production consequences. Every artifact that skips staging is a risk that production will catch what staging should have.

The emergency bypass is the most insidious anti-pattern. The organization builds a process for emergency deployments that bypasses gates, skips approvals, and deploys directly to production. The process is intended for genuine emergencies: a critical security patch, a rollback to fix an outage, a hotfix for a high-severity bug. But once the emergency process exists, it gets used for non-emergencies. A feature that missed a deadline becomes an emergency. A promotion that is delayed by approvals becomes an emergency. Within six months, ten percent of promotions are going through the emergency process. The process that was supposed to save the organization in a crisis becomes the process that creates crises.

The fix is to make the emergency process painful enough that it is only used for real emergencies. Require multiple senior approvals. Require a written justification that is reviewed post-incident. Require a follow-up action item to understand why the normal process could not accommodate the change. Track emergency bypass usage as a metric and review it quarterly. If more than two percent of promotions are going through emergency bypass, the normal process is too slow or too bureaucratic, and it needs to be fixed.

The principle underlying all of these anti-patterns is the same: shortcuts that save time in the short term create risk in the long term. The mature organization builds processes that are fast enough that shortcuts are not tempting. When the normal promotion path takes one day and the shortcut takes one hour, the shortcut is not worth the risk.

The next step in deployment control is approval workflows: defining which changes require human judgment before they can be promoted to production, and building workflows that provide oversight without creating bottlenecks.

