# 1.10 — Deployment Ownership: Who Deploys, Who Approves, Who Reverts

The ownership question sounds bureaucratic until a deployment goes wrong at 3am and nobody knows who has the authority to roll it back. A payments company learned this in January 2026 when a fraud detection model started flagging legitimate transactions at 4x the normal rate. The on-call engineer identified the problem within eight minutes. Reverting the model would have taken 90 seconds. But the engineer didn't have deploy permissions — only senior engineers did, and the deployer who had pushed the change was asleep. The on-call engineer paged the deployer. No response. Paged the backup. No response. Escalated to the engineering manager. The manager was in a different timezone and took 20 minutes to see the page.

By the time someone with deploy permissions reverted the model, 41 minutes had passed. In those 41 minutes, the system blocked $180,000 in legitimate transactions, generated 220 support tickets, and triggered manual review for 340 customer accounts. The technical problem was trivial. The organizational problem — unclear ownership, restricted permissions, no documented escalation path — turned a 90-second fix into a 41-minute disaster.

Who deploys, who approves, and who reverts are not technical questions. They are organizational design questions that determine whether your deployment process enables velocity or creates bottlenecks, whether it distributes risk appropriately or concentrates it dangerously, and whether it can respond to incidents at the speed incidents actually happen.

## The Ownership Question: Who is Responsible When Deployment Goes Wrong

Ownership in deployment systems has two dimensions: operational ownership — who has the ability and authority to execute deployments — and accountability ownership — who is responsible when a deployment causes damage. Most teams conflate these, which creates either dangerous concentration of power or dangerous diffusion of responsibility.

Operational ownership is about capability and access. Who can trigger a deployment? Who can approve a change? Who can revert a bad deploy? These are permission questions, and they have direct consequences for how fast you can ship and how fast you can recover from failures.

Accountability ownership is about responsibility. If a deployment goes wrong, who answers for it? Who leads the incident response? Who owns the post-mortem? Who decides whether the deployment process itself needs to change? These are organizational authority questions, and they determine whether your deployment process improves over time or repeats the same failures.

The healthiest deployment organizations separate operational ownership from accountability ownership, but align them clearly. Engineers have operational ownership — they can deploy, they can revert, they can respond to incidents. But a designated role, usually a platform team or a release engineering function, has accountability ownership — they own the deployment infrastructure, they define the standards, they audit compliance, they drive process improvements after incidents.

The worst deployment organizations do the opposite: they concentrate operational ownership in a small group, creating bottlenecks, while diffusing accountability ownership so that nobody is responsible for the deployment process itself. The payments company had this backwards. Only senior engineers could deploy, which meant on-call responders couldn't revert bad changes quickly. But nobody owned the deployment process — it had evolved organically over two years with no single team responsible for improving it. The result was slow deployments and slow incident response, with no mechanism to fix either problem.

## Model 1: Developer-Owned Deployment

In developer-owned deployment, the engineers who write the code also deploy it to production. This is the model that most early-stage startups use, and it has significant advantages when your team is small and your risk is low.

The primary advantage is speed. There is no handoff, no approval queue, no waiting for a platform team to execute your deploy. You write a prompt change, run your eval suite, merge to main, and the change deploys automatically via CI/CD. Time from code complete to production: minutes. For a team iterating quickly on a new AI feature, this velocity is essential.

The secondary advantage is context preservation. The person who made the change is the person who deploys it, which means they understand exactly what changed and can respond intelligently if something goes wrong. There is no translation loss, no "I deployed what you told me to deploy" finger-pointing. If the deploy breaks, the deployer knows why and can fix it immediately.

But developer-owned deployment has two major risks. The first is lack of separation of duties. In regulated industries — finance, healthcare, critical infrastructure — you often cannot have the same person write code and deploy it to production. Regulatory frameworks require independent review and approval. Developer-owned deployment is simply not compliant in those contexts.

The second risk is inconsistency. When every engineer owns their own deployments, you get as many deployment processes as you have engineers. One engineer deploys at 2pm after running evals. Another deploys at 9am without evals because they're confident. Another deploys on Friday evening because that's when they finished the work. The lack of standardization creates risk concentration: the quality of your deployment process depends on which engineer happens to be deploying that day.

Developer-owned deployment works well for small teams — fewer than 20 engineers — working on low-to-medium-risk AI systems where regulatory compliance is not a constraint. It works especially well when combined with strong automated guardrails: if your CI/CD pipeline enforces eval gates, rollback readiness checks, and deployment windows, then developer ownership is safe because the infrastructure prevents dangerous deployments regardless of who triggers them.

A developer tools company runs developer-owned deployment for their code generation AI. Any engineer can merge to main, which triggers an automated deployment pipeline. The pipeline runs a 15-minute eval suite, checks for rollback capability, verifies that the deploy is happening outside of peak traffic hours, and requires a manual confirmation step before pushing to production. The engineer owns the deployment, but the pipeline enforces the standards. The result is fast iteration — the team ships model updates twice per week — with low incident rates, because bad deployments get blocked by automation rather than by human review.

## Model 2: Platform Team Deployment

In platform team deployment, a dedicated team owns the deployment infrastructure and executes all production deployments. Engineers who build features request deployments, but they do not execute them. This is the model that most large enterprises use, and it has significant advantages when your risk is high and your compliance requirements are strict.

The primary advantage is separation of duties. The person who writes the code is not the person who deploys it, which satisfies regulatory requirements in finance, healthcare, and other regulated industries. The platform team can audit changes, verify that required approvals are in place, and enforce deployment standards consistently across all teams.

The secondary advantage is operational expertise. The platform team becomes expert at deployment. They know the infrastructure, they know the failure modes, they know how to respond when things go wrong. They maintain runbooks, they train on-call responders, they improve the deployment process over time. This expertise concentration reduces incident frequency and improves incident response.

But platform team deployment has one massive risk: it creates a bottleneck. If every deployment must go through a central team, deployment frequency is limited by that team's capacity. A healthcare AI company with 80 engineers building features and a 5-person platform team executing deployments found that deployment requests took an average of 4 days from submission to production. The platform team was executing 3-4 deployments per day and was completely saturated. Feature teams were batching changes to reduce deployment frequency, which increased the blast radius of each deployment and made rollbacks harder. The velocity cost was brutal.

Platform team deployment also creates context loss. The platform engineer executing the deployment did not write the code, did not run the evals, and may not fully understand what changed. If something goes wrong, they have to pull in the feature team engineer to diagnose it, which adds latency to incident response. The payments company that couldn't revert quickly had this problem: the platform team could deploy, but they didn't understand the fraud model well enough to know whether a revert was safe without consulting the model team.

Platform team deployment works well for large organizations — hundreds of engineers — working on high-risk AI systems in regulated industries where compliance requires separation of duties. It works especially well when the platform team focuses on building self-service tooling rather than manually executing deployments. If the platform team builds a deployment pipeline that feature teams can use independently, you get the compliance benefits of platform ownership without the bottleneck of centralized execution.

A financial services company runs platform team deployment for their credit decisioning AI. The platform team owns the deployment infrastructure and maintains the audit logs required for regulatory compliance. But they don't execute deployments manually. Instead, they built a self-service deployment portal where feature teams can request deployments, which go into a queue. The platform team reviews the request — verifying that evals passed, that required approvals are in place, that the change is documented — and then approves it. Once approved, the deployment executes automatically via the pipeline. The platform team maintains control and auditability without becoming a bottleneck, because the actual deployment execution is automated.

## Model 3: Hybrid with Guardrails

Hybrid deployment with guardrails is the model that most successful AI teams converge toward. Engineers own and execute their own deployments, but they deploy through platform-provided infrastructure that enforces standards automatically. The platform team owns the deployment pipeline, not the individual deployments.

This model combines the velocity of developer-owned deployment with the consistency and safety of platform team deployment. Engineers can deploy whenever they want, as often as they want, without waiting for approvals or queues. But they cannot deploy code that fails evals, cannot deploy without rollback capability, cannot deploy during restricted windows, and cannot deploy without creating an audit trail. The guardrails enforce those constraints automatically.

A SaaS company runs hybrid deployment for their customer support AI. Engineers deploy by merging to main, which triggers a deployment pipeline owned by the platform team. The pipeline enforces four guardrails: eval suite must pass with no regressions, deployment must include rollback instructions, deployment cannot happen during the 9am-11am peak support window, and deployment must create a structured log entry with deployer identity, change description, and eval results. If any guardrail fails, the deployment is blocked. If all guardrails pass, the deployment proceeds automatically.

The platform team owns the pipeline infrastructure. They define the guardrails, maintain the eval framework, monitor deployment health, and improve the process based on incident learnings. But they do not approve individual deployments and they do not execute them. That ownership stays with feature teams. The result is high velocity — the company deploys model or prompt updates 20-30 times per week — with low incident rates, because the guardrails catch dangerous changes before they reach production.

Hybrid with guardrails works well for most teams. It scales from 10 engineers to 500 engineers because the platform team's workload does not increase linearly with deployment frequency. It satisfies most regulatory requirements because the guardrails enforce separation of duties — the deployer does not control the standards that their deployment must meet. And it preserves context because the person who made the change is the person who deploys it and the person who responds if it breaks.

The critical requirement for hybrid deployment is strong guardrails. If your automated gates are weak — if they can't actually detect bad deployments — then engineer-owned deployment is just developer-owned deployment with extra steps. The guardrails must enforce real standards: eval quality, rollback readiness, deployment timing, audit logging. If they do, you get velocity and safety. If they don't, you get velocity and incidents.

## Approval Workflows: Risk-Based Delegation

Not all AI changes carry the same risk. A prompt wording change carries far less risk than a model architecture change. A configuration tweak to sampling temperature carries less risk than a swap from Claude Opus 4.5 to Llama 4 Maverick. Your approval workflow should reflect this risk stratification, with lightweight approvals for low-risk changes and heavyweight approvals for high-risk changes.

Low-risk changes — prompt wording, temperature adjustments, minor feature flag changes — should require no human approval if they pass automated eval gates. The eval suite is the approval. If the change improves or maintains eval performance, it ships. If it regresses, it gets blocked. This keeps velocity high for the 80% of changes that are low risk.

Medium-risk changes — new prompt structures, new tools added to an agent, changes to retrieval logic — should require approval from one other engineer, typically the tech lead for that area. The approval is not a rubber stamp. The approver reviews the eval results, checks that the change is well-scoped, and verifies that rollback is straightforward. This adds a small latency tax but catches mistakes that automated evals miss.

High-risk changes — model version upgrades, architecture changes, new fine-tuned models, changes to systems serving regulated use cases — should require approval from multiple stakeholders: engineering lead, product lead, and in some cases legal or compliance. These changes go through a formal review process with documented decision rationale. The approval latency is higher, but the risk justifies it.

A healthcare AI company stratifies approvals by risk tier. Tier 1 changes — prompt wording, minor config — require zero approvals, just passing evals. Tier 2 changes — new prompts, new tools — require one engineering approval. Tier 3 changes — model upgrades, architecture changes — require engineering, product, and clinical approvals. Tier 4 changes — anything that affects diagnostic or treatment recommendations — require all of the above plus a third-party clinical review. The result is that most changes ship in hours, while the highest-risk changes take days or weeks. The latency is proportional to the risk.

The key is making risk classification objective and automatable. If engineers have to manually decide whether their change is Tier 1 or Tier 3, they will always choose the lower tier to avoid approval overhead. Instead, classification should be based on detectable criteria: which files changed, which models are affected, which evals ran, which users are impacted. The deployment pipeline reads the changeset and classifies it automatically, then enforces the appropriate approval workflow.

## Revert Authority: Who Can Roll Back and When

Revert authority is more important than deploy authority, because reverts happen during incidents when every second matters. The payments company incident happened because only senior engineers could revert, but no senior engineer was available when the incident occurred. The correct model is: anyone on-call can revert anything, no approval required.

This sounds dangerous until you realize that the alternative — requiring approval to revert during an active incident — is far more dangerous. If your fraud detection model is blocking legitimate transactions, waiting 20 minutes for a senior engineer to approve the revert is not caution, it is negligence. The on-call responder must have unilateral authority to roll back to the last known-good state.

Pre-authorized revert is the mechanism that makes this safe. When a deployment happens, the system automatically tags the previous deployment as the rollback target and grants the on-call rotation permission to revert to it without approval. The on-call engineer does not need to understand the change, does not need to diagnose the root cause, does not need to consult anyone. They just need to recognize that the system is broken and execute the revert.

A fintech company implements pre-authorized revert for all AI deployments. Every deployment creates a rollback target and updates the on-call runbook with a single command: "If fraud detection is misbehaving, run rollback-fraud-model and page the team." The on-call engineer does not need to be an expert in fraud detection. They need to recognize the symptoms — alerts firing, support tickets spiking — and execute the revert. The revert takes 90 seconds and requires zero approvals. The post-incident investigation can happen after the system is stable.

The objection to unrestricted revert authority is: what if the on-call engineer reverts a good deployment by mistake? The answer is: that is less costly than leaving a bad deployment running while you wait for approval. A false revert costs you some deployment velocity and some engineer time to re-deploy. A delayed revert costs you revenue, trust, and regulatory exposure. The asymmetry is clear.

You do need rollback observability. If an on-call engineer reverts a deployment, the team needs to know immediately. The revert should trigger alerts, create an incident ticket, and post to a dedicated Slack channel. This ensures that the team can investigate why the revert happened and whether it was justified. Most reverts are justified — the on-call engineer saw real degradation. Occasionally a revert is a false alarm — the alert was noisy, the degradation was transient. Either way, the incident response process handles it after the fact.

## Break Glass Procedures for Emergency Response

Sometimes the normal deployment and revert processes are too slow for the severity of the incident. A content moderation AI deployed an update that started approving violating content at scale. The standard revert process required running a 5-minute safety eval before the rollback could execute — a reasonable safeguard under normal circumstances, but unacceptable when harmful content is getting published every second.

Break glass procedures provide a bypass for true emergencies. The on-call engineer can trigger an immediate rollback that skips all gates, skips all evals, and reverts to the last known-good state instantly. This is dangerous — you are deploying without verification — but in a true emergency, returning to a known-good state is always safer than staying in a known-bad state.

Break glass is not a routine operation. A well-run team should trigger break glass maybe once or twice per year. It is reserved for situations where the normal incident response process is too slow relative to the damage being caused. If you are using break glass every month, your normal deployment process is broken and needs fixing.

A social media company implements break glass with strong auditability. Any on-call engineer can trigger it, but doing so immediately pages the entire engineering leadership team, creates a Sev-1 incident ticket, and starts recording all actions for post-incident review. The break glass command is intentionally named to be unmistakable: emergency-rollback-skip-all-gates. The on-call engineer has to type the full command, which creates a moment of deliberation. If they execute it, everyone knows an emergency is happening and response mobilizes immediately.

## On-Call Responsibilities for AI Systems

On-call rotations for AI systems are different from on-call for traditional services because AI degradation is often subtle and delayed. A web service either responds or it doesn't. An AI system can respond perfectly fine while giving increasingly wrong answers. The on-call responsibilities for AI must include active monitoring for correctness, not just availability.

The minimum on-call responsibility is: respond to alerts, execute runbooks, escalate when necessary. For AI systems, add: review eval dashboards hourly, spot-check recent outputs, monitor user feedback channels. The on-call engineer is not expected to diagnose subtle model drift, but they are expected to notice when precision drops 10 percentage points or when user complaints spike 3x.

A customer support AI team rotates on-call across all engineers, not just senior ones. The on-call responsibilities are documented in a runbook: check eval dashboard every hour, read the last 20 support tickets handled by the AI, execute the revert command if precision drops below 85% or if you see three hallucinations in 20 tickets. If you revert, page the model team. If you see degradation but it doesn't meet revert criteria, open an incident ticket and monitor closely. The on-call engineer does not need to fix the model, but they need to recognize when it is broken and take the defined action.

On-call rotations for AI systems should include everyone who deploys to that system. If you deployed it, you are on-call for it. This creates accountability and ensures that engineers understand the operational consequences of their changes. It also distributes operational knowledge across the team instead of concentrating it in a few senior engineers who become bottlenecks.

## Escalation Paths When the Deployer is Unavailable

The payments company incident happened because there was no documented escalation path when the deployer was unavailable. The on-call engineer knew something was wrong but didn't know who to page or how to get revert authority. By the time they figured it out, 41 minutes had passed.

Every deployment should have a documented escalation path. If the deployer is unavailable, who gets paged? If that person is unavailable, who is next? If nobody in the chain is available, what is the fallback? The answers should be in the runbook and updated automatically when deployments happen.

A logistics AI company solves this with deployment metadata. Every deployment records the deployer, the deployer's backup, and the team lead. When an incident is detected, the on-call engineer pages the deployer first. If no response in 3 minutes, the system automatically pages the backup. If no response in another 3 minutes, it pages the team lead. If no response in another 3 minutes, it pages the VP of Engineering and triggers break glass revert authority for the on-call engineer. The escalation is automatic and time-bound. Nobody has to decide who to page or when to escalate. The system handles it.

The fallback for complete escalation failure is always: grant the on-call engineer full revert authority. If nobody who understands the deployment is available, the on-call engineer must be empowered to make the safest available choice, which is reverting to the last known-good state. This is not ideal — you would prefer the deployer to be involved — but it is far better than leaving a broken system running while you wait for someone to wake up.

## Documentation Requirements for Deployments

Every deployment must create a structured record that answers: what changed, who deployed it, why, what evals were run, what the results were, and how to revert it. This is not optional bureaucracy. It is the minimum information needed to respond to incidents and to satisfy regulatory requirements.

The documentation should be automatically generated by the deployment pipeline, not manually written by engineers. Manual documentation gets skipped under time pressure. Automated documentation happens every time, whether the engineer wants it to or not.

A fintech platform generates deployment records automatically. Every deployment creates a JSON log entry with: timestamp, deployer identity, commit SHA, changed files, eval suite results, approval chain, rollback command, and a link to the pull request. The log entry is written to an append-only audit database, posted to a Slack channel, and indexed for search. If an incident happens three months later, the team can search for deployments around that time and reconstruct exactly what changed.

The documentation standard in 2026 is higher than it was even two years ago, driven by the EU AI Act and similar regulations. You must be able to produce complete deployment records for any production AI system on demand. If you cannot, you are out of compliance. The only way to guarantee compliance is to make documentation automatic and immutable.

---

Deployment ownership is not a theoretical exercise. It determines whether your team can deploy quickly, respond to incidents effectively, and operate under regulatory scrutiny. The best ownership model for your team depends on your size, your risk profile, and your compliance requirements — but every model must answer clearly who deploys, who approves, who reverts, and who is accountable when things go wrong. Next, we examine the 2026 deployment stack — the tools, patterns, and expectations that define how modern AI systems get deployed to production.
