# 7.7 — Flag Lifecycle Management: Creation, Rollout, Cleanup

In late 2025, a logistics company discovered they had 847 feature flags in production. Nobody knew which ones were still used. Most were created months or years earlier for features that had long since shipped. Some controlled experiments that had concluded. Some gated features that had been rolled out to 100 percent of users but never removed from code. Every flag check added a small amount of latency and a small amount of risk, because deprecated flags might have bugs or interact with new code in unexpected ways. The team spent six weeks auditing the entire flag inventory, removing 623 flags that were no longer needed, and documenting the remaining 224. The effort cost $140,000 in engineering time. The root cause was not technical — it was organizational. The team had a clear process for creating flags but no process for removing them.

Feature flags are temporary by design. A flag exists to control a rollout, test a variant, or decouple deployment from release. Once the rollout is complete, the test is concluded, or the deployment risk has passed, the flag should be removed. The code should be simplified to the winning variant. The flag configuration should be deleted from the flag service. The flag check should be removed from the codebase. When this doesn't happen, flags accumulate. Technical debt grows. Cognitive load increases. The system becomes harder to reason about because every code path has conditional logic that might or might not still matter.

## Technical Debt Accumulation: Flags That Outlive Their Purpose

A flag created in March to test a new recommendation algorithm. By June, the test is complete, the new algorithm won, and it's rolled out to all users. But the flag is still in the code, still being evaluated on every request, still configured in the flag service. Nobody removed it because removing flags wasn't part of the team's workflow. A flag created in April to gate a risky new payment flow during initial launch. By July, the flow is stable and running for all users. The flag is still there, now permanently set to true, doing nothing but adding a branch in the code.

The cost of an unused flag is small per flag but scales linearly with the number of flags. Every flag check adds a few microseconds of evaluation time. Every flag configuration adds a few bytes to the SDK's cached ruleset. Every flag in the codebase adds a branch that engineers need to understand when reading the code. Ten flags are manageable. One hundred flags are annoying. Five hundred flags are a maintenance disaster.

Worse, old flags can interact with new code in unexpected ways. A flag that controlled behavior in version 1.5 of your model now interacts with version 3.2, which uses different inputs and produces different outputs. The flag logic no longer makes sense in the new context, but the code still checks it. A flag that gated a feature in your monolithic application now runs in a microservice that doesn't have access to the user attributes the flag depends on. The flag evaluation fails silently, and the feature behaves incorrectly.

A SaaS platform in early 2026 had a flag from 2024 that controlled whether to use a legacy data format or a new one. By 2026, the legacy format was no longer supported anywhere in the system. But the flag was still being checked in three different services. One service evaluated the flag and then did nothing with the result. Another service evaluated the flag and always passed the same variant to downstream logic because the code had been refactored to remove support for the other variant. A third service evaluated the flag, and if it returned the legacy variant, threw an error because the legacy code path had been deleted. None of this was intentional. It happened because removing the flag wasn't prioritized when the migration finished.

## Flag Creation Process: Naming, Documentation, Ownership

The lifecycle of a flag starts with its creation. Most teams have no formal process for this. Someone needs a flag, they create it in the flag service, they add a check in the code, and they move on. The flag name is often generic — new-model-test, experiment-42, enable-feature-x. The flag has no description, no documented purpose, no owner, and no expiration date. Six months later, nobody remembers why the flag exists or when it should be removed.

A better creation process requires three things. First, a clear naming convention. Flags should be named according to what they control and when they were created. Naming schemes vary, but a common pattern is scope-purpose-date. Example: payments-new-flow-2026-03. This tells you the flag is in the payments domain, it controls a new flow, and it was created in March 2026. Another pattern is purpose-jira-ticket. Example: experiment-fraud-detection-RISK-1847. This ties the flag to a specific project ticket where context is documented.

Second, required documentation at creation time. Most flag platforms allow adding a description when creating a flag. This should be mandatory. The description should explain what the flag controls, why it exists, what the variants represent, and what criteria determine when the flag should be removed. "This flag gates the new fraud detection model. Variant A is the current model. Variant B is the new model. Remove this flag when the new model is rolled out to 100 percent and has been stable for two weeks." This takes 30 seconds to write and saves hours of confusion later.

Third, assigned ownership. Every flag should have an owner — a person or team responsible for monitoring the flag, making rollout decisions, and ultimately removing it when it's no longer needed. The owner should be recorded in the flag service and visible in the dashboard. When a flag has been enabled for all users for more than 30 days, the flag service should notify the owner and ask whether the flag can be removed. Without ownership, flags become orphaned.

A healthcare AI platform in mid-2025 implemented a flag creation checklist. Before a feature flag could be created, the engineer had to fill out a form specifying the flag's purpose, expected lifespan, rollout plan, success criteria, and owner. The form was integrated into their flag service UI and submission was required before the flag was created. The initial reaction was friction — engineers didn't want to spend five minutes documenting a flag. But after three months, the team had 60 percent fewer orphaned flags compared to the prior quarter. When flags came up for review, the documentation made it obvious whether the flag was still needed or ready for cleanup.

## Rollout Phases: From Development to Full Launch

A flag's lifecycle has distinct phases. It starts in development, where engineers use it to test new code locally. It moves to staging, where it's tested in a production-like environment. It progresses to production with a limited rollout — maybe one percent of users, or a specific test cohort. If the rollout succeeds, it expands — five percent, ten percent, 25 percent, 50 percent. Eventually it reaches 100 percent. At that point, the flag is no longer controlling a gradual rollout. It's just a constant.

Each phase has different monitoring requirements. In development, you just need the flag to work. In staging, you need to verify that the flag evaluation logic is correct and that the variants behave as expected. In limited production rollout, you need active monitoring — metrics dashboards, alerts for regressions, logs of flag evaluations. In full rollout, you need confirmation that the new behavior is stable before you can remove the flag.

The mistake most teams make is treating 100 percent rollout as the end state. The flag stays at 100 percent indefinitely. The correct end state is flag removal. Once a flag has been at 100 percent for a soak period — one week, two weeks, 30 days depending on risk tolerance — and no issues have surfaced, the flag should be marked for removal. The winning variant should be promoted to the default code path. The flag check should be deleted. The flag configuration should be archived.

A fintech platform in late 2025 automated this process with a scheduled job that scanned all flags in production. If a flag had been at 100 percent rollout for more than 14 days and had been evaluated more than 10,000 times without errors, the job created a Jira ticket assigned to the flag owner with a checklist for removal. The ticket included links to the flag's configuration, the code locations where it was checked, and the commit history for the feature it controlled. This turned flag cleanup from something that required manual tracking into something that happened automatically as part of the normal ticket workflow.

## Flag Completion: When a Flag Is No Longer Needed

A flag is complete when it no longer serves its purpose. For a rollout flag, that's when the feature is fully launched and stable. For an experiment flag, that's when the experiment has concluded and a winner has been chosen. For a kill switch flag, completion is trickier — the flag exists to disable a feature if something goes wrong, so it might never be "done." Kill switches have indefinite lifespans and should be maintained as long as the feature exists.

Differentiating between temporary and permanent flags at creation time helps. Temporary flags have a clear completion condition. Permanent flags, like kill switches, operational overrides, and emergency controls, are marked as long-lived and excluded from automatic cleanup suggestions. This prevents the flag service from nagging you to remove a kill switch that you genuinely need.

A video streaming platform in early 2026 had two categories of flags — rollout flags and operational flags. Rollout flags were tagged with an expected removal date. Operational flags were tagged as permanent. Every flag was assigned to one category at creation. The flag dashboard showed rollout flags sorted by how long they'd been at full rollout. Flags that had been at 100 percent for more than 30 days appeared in red. This made it obvious which flags were overdue for cleanup. Operational flags had their own dashboard section and were reviewed quarterly rather than continuously.

The completion criteria for a flag should be documented when the flag is created. "Remove when the new model has been at 100 percent for two weeks with no increase in error rate." "Remove when experiment reaches statistical significance and winning variant is deployed." "Remove when the beta period ends and the feature is available to all users." These criteria turn flag cleanup from a judgment call into a checklist.

## Cleanup Discipline: Removing Flags After Rollout

Removing a flag is a three-step process. First, remove the flag check from the code. Replace the conditional branch with the winning variant. If the flag was enabling a new feature, remove the control path and keep only the new path. If the flag was an experiment, remove the losing variants and keep only the winner. This step requires a code change, code review, and deployment.

Second, verify that the flag is no longer referenced anywhere in the codebase. Search for the flag name. Check that no other services or scripts depend on it. This catches cases where the flag was checked in multiple places, or where downstream systems log the flag value, or where documentation or configuration files reference it.

Third, delete the flag configuration from the flag service. Most flag platforms support archiving flags rather than deleting them outright. Archiving preserves the flag's history and configuration while removing it from active use. This is safer than hard deletion because you can restore the flag if you realize it's still needed. After a retention period — 90 days, six months — archived flags can be permanently deleted.

The danger is stopping after step one. You remove the flag check from the code but leave the flag configuration in the service. Now the flag is orphaned. It still appears in the dashboard. It still consumes storage. If someone searches for it, they see it and assume it's active. They might even re-enable it, not realizing the code no longer checks it.

A B2B collaboration platform in mid-2025 had a policy: flag removal required a pull request that touched both the codebase and the flag configuration. The pull request description included a link to the flag's dashboard, a screenshot of the final rollout state, and confirmation that the flag had been archived in the flag service. Reviewers verified all three before approving. This made flag cleanup a single atomic operation rather than two separate tasks that might not both get done.

## Stale Flag Detection: Automated Identification

Flag services can help identify stale flags automatically. A flag that hasn't changed state in six months is likely stale. A flag that's been at 100 percent rollout for 60 days is a cleanup candidate. A flag that hasn't been evaluated in the last 30 days might be dead code. These heuristics aren't perfect — a stable flag that's working as intended will also meet these criteria — but they surface flags that deserve review.

LaunchDarkly, Statsig, and most other platforms provide stale flag reports. LaunchDarkly's "code references" feature scans your Git repositories and identifies flags that are no longer referenced in code. This catches the case where the flag configuration still exists but the code check has been removed. Statsig tracks flag evaluation frequency and flags entries that haven't been evaluated recently.

A logistics platform in late 2025 set up a monthly flag hygiene review. The engineering lead exported a stale flag report from LaunchDarkly, filtered for flags older than 90 days that hadn't changed state in 60 days, and assigned each one to the original creator or the current team responsible for that area. The assigned engineer had two weeks to either document why the flag was still needed or remove it. Flags that weren't addressed after two reminders were disabled in production to see if anything broke. If nothing broke after 48 hours, the flag was removed. This reduced their active flag count by 40 percent over six months.

## Flag Auditing: Who, Why, When

Flag platforms maintain audit logs that record every change to a flag. Who enabled it. Who changed the rollout percentage. Who added a targeting rule. Who archived it. These logs are essential for understanding flag history and debugging issues.

Audit logs answer questions like: Why is this flag enabled for user X? Because someone added a targeting rule three weeks ago. Who decided to roll this feature back? The on-call engineer at 2am when error rates spiked. When was this flag last modified? Sixteen months ago, which explains why nobody remembers what it does.

Audit logs also provide accountability. If a flag change causes an outage, the audit log shows who made the change and when. This isn't about blame — it's about understanding what happened and ensuring the team learns from it. Some platforms integrate audit logs with incident management systems so that flag changes during an incident are automatically included in the post-incident report.

A healthcare AI platform in early 2026 had compliance requirements around feature changes. Every change to production behavior needed to be documented and traceable. They configured their flag service to send audit log events to their compliance data warehouse. When a flag changed state, the event included who made the change, what the change was, why the change was made — pulled from a mandatory comment field — and what approval process was followed. This made flags part of their regulatory audit trail rather than a gap in their change control system.

## The Flag Hygiene Ritual: Regular Reviews

Flag hygiene doesn't happen automatically. It requires discipline. The most effective pattern is a recurring ritual — weekly, biweekly, or monthly depending on your flag creation rate — where the team reviews the current flag inventory and makes cleanup decisions.

The ritual has a simple agenda. First, review new flags created since the last session. Verify that each has proper documentation, an owner, and an expected lifespan. Second, review flags at 100 percent rollout for more than the soak period. Decide which are ready for removal. Third, review flags that have been in limited rollout for longer than expected. Decide whether to expand the rollout, roll back, or conclude the test. Fourth, review operational flags to verify they're still needed and still working.

A SaaS platform in mid-2025 held a 30-minute flag review every two weeks. The meeting was attended by the engineering lead, the product manager, and one engineer from each team. They went through the stale flag report, made decisions on the spot, and created tickets for any cleanup work. The meeting was short enough that people didn't resent it and frequent enough that flags didn't accumulate. Over a year, they kept their active flag count stable at around 40 despite launching dozens of new features.

The alternative to regular reviews is reactive cleanup — only addressing flags when they become a problem. This leads to periodic flag debt crises where the team has to stop feature work and spend a week or two auditing and removing flags. Regular reviews prevent the crisis by keeping the flag inventory manageable continuously.

## Organizational Discipline: Making Cleanup Part of the Process

Flag cleanup fails when it's treated as optional or as something to do "when we have time." It succeeds when it's part of the standard workflow. If your definition of done for a feature includes removing the flag after rollout, it will happen. If it doesn't, it won't.

Some teams make flag cleanup part of the acceptance criteria for the original feature. The story isn't complete until the flag is removed. This works well for short-lived flags where the rollout completes within the same sprint or quarter. For longer-lived flags, a separate cleanup story is created when the rollout begins, scheduled for the expected completion date.

Other teams tie flag cleanup to their release process. Before a major release, they review all flags related to features in that release and remove any that are no longer needed. This ensures that each release not only adds features but also simplifies the codebase by removing the scaffolding from previous releases.

A fintech platform in late 2025 added flag cleanup to their quarterly planning process. Each team identified flags they owned that were candidates for removal in the coming quarter. Those flags were added to the team's backlog and prioritized alongside feature work. Leadership tracked flag cleanup as a health metric — teams that consistently cleaned up flags were recognized as having good engineering hygiene. This created social incentives around a task that otherwise had no immediate benefit.

Flags are powerful tools for controlling risk and managing deployments, but they're designed to be temporary. Without a deliberate lifecycle management process, they accumulate into technical debt that makes your system harder to understand and slower to change. The next step is using analytics to understand whether flags are helping or hurting before you decide whether to expand the rollout or clean up the experiment.
