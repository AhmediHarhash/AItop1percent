# 3.11 â€” Cross-Region Routing and Data Residency Constraints

Routing users to the nearest region sounds simple until you add legal constraints. A European healthcare company deployed their diagnostic AI in both eu-west-1 and us-east-1 in mid-2025. Their routing logic sent users to the lowest-latency region. On August 3rd, 2025, eu-west-1 had a partial outage. Traffic automatically failed over to us-east-1. Patient data from Germany was processed in Virginia. The company discovered the issue during a compliance audit four weeks later. The regulatory investigation is ongoing. The fine will be substantial. Data residency is not a performance optimization. It is a legal requirement that overrides every other routing consideration.

## The Competing Objectives of Global Routing

Your routing system has four objectives. Minimize latency to provide good user experience. Balance load to prevent any single region from becoming a bottleneck. Respect data residency to comply with regulations. Maintain availability by routing around failures. These objectives conflict.

Latency-based routing sends every user to the geographically nearest region. A user in Paris connects to eu-west-1. A user in Tokyo connects to ap-northeast-1. This minimizes network latency and provides the fastest response time. But if eu-west-1 is at ninety percent capacity and us-east-1 is at twenty percent capacity, latency-based routing overloads the European region while American capacity sits idle.

Load-based routing sends users to the least loaded region regardless of geography. This maximizes capacity utilization and prevents any region from becoming a bottleneck. But it increases latency for users routed far from their location. A user in London routed to us-west-2 because it is less loaded experiences three hundred milliseconds of additional network latency. For latency-sensitive workloads, this is unacceptable.

Data residency requirements override both latency and load. If a user's data must stay in the EU, they cannot be routed to a US region even if the US region is faster and less loaded. Data residency creates hard boundaries. A system that respects residency requirements cannot use all available capacity. EU capacity might be exhausted while US capacity is idle, but you cannot shift EU users to US infrastructure.

The routing system must balance all four objectives with data residency as a hard constraint. Within the set of legally permissible regions, optimize for latency and load. When a permissible region fails, route to another permissible region or reject the request. Never violate residency requirements to improve latency or availability.

## Latency-Based Routing in Practice

Latency-based routing measures network distance, not geographic distance. A user in Seattle is geographically closer to us-west-2 than us-east-1, but if their ISP has poor peering with us-west-2 and excellent peering with us-east-1, actual latency to us-east-1 might be lower. Do not assume geography equals latency.

Real-time latency measurement is more accurate than static mapping. Your routing layer pings each region from the user's location and selects the lowest-latency region. This accounts for transient network conditions, routing changes, and peering differences. The cost is that every request requires a latency probe before routing. For high-throughput systems, this adds unacceptable overhead.

The practical approach is probabilistic latency mapping with periodic updates. Your routing service maintains a latency map from major geographic regions to your deployment regions. A user in Western Europe is statistically likely to have lowest latency to eu-west-1, second-lowest to eu-central-1, third-lowest to us-east-1. Route based on this map. Update the map every five minutes using aggregate latency measurements from real traffic. This provides good latency without per-request probing.

Latency-based routing must account for region health. If the nearest region is degraded, route to the second-nearest. Define degradation thresholds. If a region's P95 latency exceeds its baseline by fifty percent, consider it degraded. Route new traffic elsewhere while existing traffic drains. This prevents users from being routed into a slow region just because it is geographically close.

## Load-Based Routing and Capacity Awareness

Load-based routing requires real-time capacity metrics from every region. Each region reports its current queue depth, active request count, and available capacity. The routing layer selects the region with the most available capacity. This prevents hotspots and distributes traffic evenly.

The challenge is metric propagation delay. If metrics are updated every thirty seconds, the routing layer operates on stale data. A region that was idle thirty seconds ago might be overloaded now. The routing layer sees it as low-load and sends more traffic, making the overload worse. Use a shorter update interval. Five to ten seconds is usually sufficient. Faster updates increase overhead. Slower updates cause routing lag.

Overflow routing is a hybrid approach. Route users to the nearest region by default. If that region exceeds a capacity threshold, overflow to the next-nearest region. If the second region is also at capacity, overflow to the third. This preserves latency for most users while preventing any region from being overwhelmed. Define overflow thresholds conservatively. Start overflowing at seventy percent capacity, not ninety-five percent. By the time a region hits ninety-five percent, it is already degraded.

Capacity-aware routing must consider GPU heterogeneity. If us-east-1 runs on H100 GPUs and processes requests in one second, while ap-southeast-1 runs on A100 GPUs and processes requests in two seconds, a queue depth of ten in us-east-1 is equivalent to a queue depth of five in ap-southeast-1. Normalize capacity metrics by processing speed. Report available throughput, not just queue depth.

## Data Residency Requirements by Jurisdiction

GDPR requires that personal data of EU residents be processed within the EU unless specific safeguards are in place. The safeguards are complex and not universally reliable. The safest approach is to process EU user data only in EU regions. This means eu-west-1, eu-central-1, and eu-north-1 are permissible. us-east-1, us-west-2, and ap-southeast-1 are not.

China's data sovereignty laws require that data collected in China stay in China. If you serve Chinese users, you must process their requests in a China region. Data cannot leave Chinese borders. This is a hard constraint with no exceptions. If you do not have infrastructure in China, you cannot serve Chinese users in compliance with Chinese law.

Healthcare data under HIPAA has geographic constraints for certain covered entities. Federal healthcare data may be subject to FedRAMP requirements that mandate US-based processing. ITAR-controlled data must stay in the United States. Financial data under PCI-DSS has no explicit geographic requirement but often has contractual restrictions on where data is processed.

The regulatory landscape is complex and jurisdiction-specific. Your routing logic must encode these rules. Maintain a mapping from user location to permissible regions. A user in France maps to EU regions only. A user in California maps to US regions, EU regions, and others where US data processing is permitted. A user in China maps to China regions only.

## Implementing Data Residency Routing

User location detection is the first challenge. IP-based geolocation is eighty to ninety percent accurate for country-level detection. It is less accurate for city-level detection and fails entirely for VPN users. If a user's IP address suggests they are in Germany but they are actually in the US using a German VPN exit node, you will route them to an EU region unnecessarily. This adds latency but does not violate compliance. The error is conservative.

User-provided location is more accurate but less trustworthy. If your application asks users to declare their location, they might lie to get better performance. A US user might claim to be in the EU to access EU-region infrastructure if it is faster. Do not rely solely on user declarations. Combine IP geolocation with user-provided location. If they conflict, choose the more restrictive option.

Request routing rules encode residency constraints. Define rules like: "If user is in EU, route to EU regions only. If all EU regions are unhealthy, reject the request." Or: "If user is in US, prefer US regions but allow EU regions as fallback." Or: "If user is in China, route to China regions only. If no China region is available, reject." These rules are configuration, not code. Store them in a policy engine that can be updated without deploying new software.

Enforcement at the edge ensures compliance. Your edge routing layer, whether it is a CDN, a global load balancer, or a reverse proxy, applies residency rules before forwarding requests to regions. If a request violates residency policy, the edge rejects it with HTTP 451 Unavailable For Legal Reasons. This ensures that even if your application logic has a bug, the edge enforces the hard boundary.

## Cross-Region Failover Under Residency Constraints

When an EU region fails, can EU users fail over to a US region? The legal answer is: it depends. If you have Standard Contractual Clauses in place and the US region meets GDPR adequacy requirements, maybe. If you do not, no. The safest approach is to assume that residency boundaries are hard. If all EU regions are down, EU users cannot be served. Reject their requests rather than route them to a non-compliant region.

Partial region failure complicates this decision. If eu-west-1 is at fifty percent capacity due to infrastructure issues, do you overflow EU users to us-east-1? No. You overflow to eu-central-1 or eu-north-1. Only if all EU regions are fully unavailable do you consider rejecting requests. The principle is: fail within the residency boundary before failing across it.

Graceful degradation within a residency zone is always preferable to violating residency. If EU capacity is exhausted, serve EU users with higher latency, lower throughput, or reduced functionality rather than routing them to the US. Return 429 or 503 with Retry-After headers. Queue requests and process them slowly. Do anything except send EU personal data to a non-EU region without legal justification.

Some teams implement compliance overrides for emergencies. A senior engineer can manually authorize cross-boundary routing for a limited time during a major incident. This decision is logged, requires approval, and triggers a compliance review. This is a last resort for situations where total unavailability is worse than a temporary compliance gap. Most organizations do not allow this. The legal risk is too high.

## Metadata vs Payload in Residency Decisions

What constitutes personal data under GDPR is a legal question, not a technical one. The user's prompt is almost certainly personal data. The model's response is almost certainly personal data. The user's IP address is personal data. The session ID might be personal data. The timestamp might not be.

Telemetry and logging often contain personal data. If you log the first fifty characters of every prompt for debugging, you are logging personal data. That log must be stored in a compliant region. If you export latency metrics that include user IDs, those metrics contain personal data. If you export aggregate metrics with no user-level information, they probably do not.

The conservative approach is to treat everything associated with a user request as subject to residency requirements. Process it in the appropriate region. Store it in the appropriate region. Do not replicate it to other regions unless you have legal justification. This is over-inclusive but safe. The cost is that you cannot centralize logging or telemetry. Each region's data stays in that region. Analysis requires querying all regions and aggregating results.

The nuanced approach is to classify data by sensitivity. Prompts and responses are high sensitivity and must respect residency. Aggregate metrics are low sensitivity and can be centralized. Session metadata is medium sensitivity and requires case-by-case evaluation. Work with your legal team to define these classifications. Encode them in your data handling policies. Audit regularly to ensure compliance.

## Configuration Management for Routing Rules

Routing rules are configuration, not code. You must be able to update them without deploying new software. A new regulation takes effect. A new region comes online. A new customer contract requires special routing. All of these require routing rule changes.

Store routing rules in a globally replicated configuration service. DynamoDB, Cosmos DB, or Consul work well. The edge routing layer reads rules from this service. Updates propagate within seconds. Version your rules. Every change is a new version with a timestamp and author. Rollback is trivial. If a rule change causes problems, revert to the previous version instantly.

Test routing rules in staging before applying them in production. Your staging environment should mirror production's multi-region topology. Deploy a rule change to staging. Run synthetic traffic through all routing paths. Verify that EU users stay in EU, US users route correctly, and edge cases behave as expected. Only after passing staging tests do you apply the rule to production.

Hot-reload routing rules without restarting services. Your routing layer polls the configuration service every ten seconds. If rules have changed, it loads the new rules and applies them to new requests. In-flight requests use the old rules. This allows instant rule updates with no downtime. Implement rule validation before loading. If a new rule is malformed, reject it and log an error. Do not load broken rules into production.

## Auditing and Compliance Verification

Log every routing decision with sufficient context to prove compliance. Include the user's detected location, the set of eligible regions, the residency rules applied, the selected region, and the timestamp. If a regulator asks, "How do you know you never sent EU user data to the US?" you produce logs showing that EU users were always routed to EU regions.

Audit logs must themselves respect residency. An audit log entry for an EU user must be stored in the EU. This creates a bootstrapping problem: where do you store logs about routing decisions? The answer is to store them in the same region as the routed request. If a request is routed to eu-west-1, the routing log entry is stored in eu-west-1. If a request is rejected, the log entry is stored in the region where the rejection occurred, which is usually the edge.

Compliance monitoring continuously validates routing correctness. Run a background job that samples routing logs and verifies that residency rules were followed. If any violations are detected, alert immediately. Investigate whether the violation was a logging error, a configuration error, or an actual compliance failure. If it is a compliance failure, notify legal and determine reporting obligations.

Periodic compliance reviews involve manual inspection. Every quarter, a compliance engineer reviews routing rules, samples logs, and verifies that the system behaves as intended. This catches issues that automated monitoring misses. It also provides documentation for audits and regulatory inquiries.

## The Practical Routing Decision Tree

When a request arrives, your routing system follows a decision tree. First, determine the user's location. Use IP geolocation. If the application provides location, use that. If they conflict, use the more restrictive. Second, determine the set of legally permissible regions. Apply residency rules. If the user is in the EU, permissible regions are EU regions only. Third, within permissible regions, determine which are healthy. Exclude regions failing health checks. Fourth, within healthy permissible regions, select based on latency or load. If optimizing for latency, choose the nearest. If optimizing for load, choose the least loaded. If the selected region is available, route there. If no permissible region is available, reject the request with 503 and an explanation.

This decision tree ensures compliance first, availability second, and performance third. You never sacrifice compliance for performance. You sometimes sacrifice performance for availability. You always log the decision with full context.

The next step is synthesizing all of this into a scaling playbook that teams can follow as they grow from prototype to enterprise scale.
