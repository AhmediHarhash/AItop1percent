# 8.10 — The Model Registry Pattern: Single Source of Truth

The model registry is not a nice-to-have. It is the foundation of reliable model deployment. Without it, you are guessing what is running in production. With it, you have a verifiable source of truth for every model version, every deployment, every promotion, and every access decision. The registry is where models are published, reviewed, approved, promoted across environments, and ultimately deployed. It is the central control point that makes version pinning possible, deprecation enforceable, and audit trails complete. A team without a model registry is managing deployments through a combination of file systems, shared drives, Slack messages, and institutional memory. That approach works until the person who remembers where the production model lives leaves the company.

A financial services company learned this during a routine deployment. The team needed to deploy a model update to production. They retrieved what they believed was the latest approved model from a shared S3 bucket. The deployment succeeded. Two days later, an internal audit discovered that the deployed model was not the version that had passed compliance review. It was a similar version from a parallel experiment that had been uploaded to the same bucket with a slightly different filename. The two versions were functionally identical for most inputs but diverged on edge cases that compliance testing had specifically checked. The company had deployed an unreviewed model to production and processed 19,000 transactions with it before discovery. The incident cost 11 days of remediation work, reprocessing of affected transactions, and a formal regulatory notification. The root cause was the absence of a system that enforced which models were approved for production. They built a model registry in the following quarter. Every model is now published to the registry, reviewed, approved, and promoted through controlled gates before deployment. The registry enforces that only approved versions can be deployed. Accidental deployment of unapproved models is now structurally impossible.

## The Registry as Single Source of Truth

The single source of truth principle means that all model deployments pull from the registry, never from local files, never from ad hoc storage locations, never from a colleague's laptop. If a model is not in the registry, it cannot be deployed. If a model is in the registry but not approved for production, it cannot be deployed to production. The registry is the only place where the question "what models exist and where can they be deployed" has an authoritative answer.

This centralization eliminates the most common source of deployment errors: local artifacts that diverge from the approved versions. A developer trains a model, saves it locally, tests it, and deploys it from their laptop. That deployment works. Three months later, the developer is on leave and the model needs to be redeployed. Nobody else on the team has the file. The original training data is gone. The model configuration has been lost. The deployment cannot be reproduced. This failure mode disappears when all deployments pull from the registry. The model gets published to the registry after training. The deployment pulls from the registry. If the developer leaves, the model is still in the registry. If the laptop dies, the model is still in the registry. The registry is the persistent source of truth that outlives individual contributors.

The registry also eliminates version confusion. Without a registry, a team might have multiple files named "production_model_v3.h5" in different locations. Are they the same file? Different files with the same name? The same model trained twice? There is no way to know without comparing file hashes, and nobody does that consistently. With a registry, every version has a unique identifier. There is only one artifact with identifier "model-v3.2.1-prod-20260115." If someone references that identifier, there is no ambiguity about which artifact they mean. The registry enforces uniqueness and provides unambiguous references.

The single source of truth pattern requires that the registry is always available. If the registry is down, deployments cannot proceed. This makes the registry a critical dependency. It needs high availability, disaster recovery, and robust monitoring. The registry is infrastructure, not a side project. Teams that treat it as a side project experience registry outages that block all deployments. Teams that treat it as infrastructure provision redundancy, failover, and backups that ensure the registry is more reliable than any individual model deployment.

## Registry Architecture: The Four-Layer Model

A model registry is not a single component. It is an architecture with four layers: storage, metadata, API, and access control. Each layer serves a specific purpose. Together they provide the capabilities that make the registry a reliable foundation for deployment.

The storage layer holds the actual model artifacts — the weights, the tokenizer files, the configuration files, the associated prompt templates. This layer needs to support large files, provide high durability, and allow efficient retrieval. Object storage systems like S3, Google Cloud Storage, or Azure Blob Storage are the typical choice. The storage layer does not enforce business logic. It is a dumb storage system that persists bytes. The intelligence lives in the layers above. The storage layer needs to support versioning at the storage level so that even if a file is accidentally overwritten in the registry, it can be recovered from storage history. The storage layer also needs encryption at rest and in transit. Model weights are intellectual property and sometimes contain sensitive information learned from training data.

The metadata layer tracks information about each model version. This includes the version identifier, the publication date, the author, the training configuration, the evaluation metrics, the approval status, the deprecation status, and the relationships to other versions. The metadata layer is typically a relational database or a document store. It needs to support fast queries because every deployment queries the metadata layer to resolve version identifiers and check approval status. The metadata layer is also where the registry records access logs — every time a model is retrieved, the registry logs who retrieved it, when, and from where. These access logs are essential for audit trails and usage analytics.

The API layer provides programmatic access to the registry. The API includes endpoints for publishing models, querying metadata, retrieving models, updating approval status, and querying access logs. The API is what deployment systems, CI/CD pipelines, and developer tools interact with. The API enforces business logic. When a deployment requests a model, the API checks the metadata layer to ensure the model exists, is approved for the target environment, and is not deprecated. The API also enforces rate limits, authentication, and authorization. The API is the registry's interface to the rest of the organization.

The access control layer determines who can publish models, who can approve them, who can promote them between environments, and who can deploy them. Access control is role-based. A data scientist can publish models to the development registry but cannot promote them to production. A platform engineer can deploy models but cannot approve them. A compliance officer can approve models but cannot publish or deploy them. The separation of roles prevents any single person from publishing and deploying an unapproved model. This separation is especially important in regulated industries where independent review is a compliance requirement.

The four layers need to be consistent with each other. If the storage layer contains a model that the metadata layer does not know about, something is broken. If the metadata layer records a model as approved but the access control layer does not allow deployment, something is misconfigured. The registry needs internal consistency checks that detect and alert on these inconsistencies. A nightly job that compares storage contents to metadata entries and flags discrepancies is common. The consistency checks catch operational errors before they become incidents.

## Registry Workflows: Publish, Review, Approve, Promote, Deploy

The registry is not just storage. It is a workflow engine that enforces the process from model training to production deployment. The workflow has five stages, each with clear entry and exit criteria. The workflow prevents shortcuts that bypass review and approval.

The first stage is publish. After a model is trained, the data scientist publishes it to the registry. Publishing includes uploading the model artifacts to the storage layer and creating a metadata entry with the version identifier, training metrics, and evaluation results. The published model is now in the development registry. It is visible to the team but not approved for deployment. Publishing is a gate. It forces the data scientist to include metadata and evaluation results. If key information is missing, the publish operation fails. This ensures that every model in the registry has a minimum level of documentation.

The second stage is review. A peer or a more senior data scientist reviews the published model. They inspect the training configuration, the evaluation results, and the model behavior on test cases. They verify that the model meets quality standards and does not have obvious problems. The review can result in approval to proceed, requests for changes, or rejection. This stage catches mistakes early. A model that would fail in production gets caught during review instead of during deployment. The review is documented. The reviewer's identity and their decision are recorded in the metadata layer. This creates accountability.

The third stage is approve. After review, a designated approver marks the model as approved for a specific environment. Approval for development is automatic after review. Approval for staging requires that the model passed development testing. Approval for production requires that the model passed staging testing and meets any additional compliance requirements. The approver is typically a technical lead, a product manager, or a compliance officer, depending on the organization. The approval is an explicit action. It is not automatic. This forces a conscious decision point where someone with authority says "yes, this model is ready for the next environment."

The fourth stage is promote. Promotion moves the model from one environment registry to the next. Promotion does not copy the model. The same artifact in storage becomes visible in the next environment's registry. The metadata is updated to indicate that the model is now available in the new environment. Promotion is unidirectional. Models move from development to staging to production, never in reverse. This ensures that every model in production has been through the full pipeline. Promotion is logged. The metadata records when the promotion occurred, who performed it, and which approval gate it satisfied.

The fifth stage is deploy. A deployment system retrieves the model from the registry and deploys it to the target environment. The deployment references the model by its immutable version identifier. The registry verifies that the model is approved for the target environment before allowing retrieval. If the model is not approved, the retrieval fails with an error that explains which approval is missing. This enforcement prevents accidental deployment of unapproved models. The deployment is logged. The registry records that the model was deployed, when, where, and by whom. This log is the audit trail.

The workflow is enforced by the registry. There is no way to skip stages. You cannot promote a model to production without approving it in staging. You cannot deploy a model that has not been promoted. The registry is not just documentation. It is enforcement.

## Promotion Between Environments and Environment Isolation

Environment isolation is a core registry capability. A model that is published to the development registry is not visible in the production registry until it has been explicitly promoted. This isolation prevents accidents. A developer cannot accidentally deploy a development model to production because the production deployment system cannot see development models. The registry partitions the model namespace by environment.

The isolation is implemented through environment-specific views in the metadata layer. The storage layer contains all model artifacts. The metadata layer contains one entry per artifact with tags indicating which environments can see it. When a deployment system queries the production registry, the query is filtered to only return models tagged as production-approved. When a developer queries the development registry, the query returns all models tagged as development or higher. The filtering is automatic and enforced at the API layer. There is no way to bypass it without elevated privileges.

Promotion is the mechanism for moving models across environments. Promotion is a deliberate action that requires approval. The promotion workflow queries the metadata layer to verify that all prerequisites are met — the model passed the required tests, it was approved by the required reviewers, it has been in the previous environment for the required minimum duration. If any prerequisite is missing, promotion fails. If all prerequisites are met, promotion succeeds and the metadata is updated. The model is now visible in the new environment.

Some organizations use separate physical registries for each environment. The development registry is in a development AWS account. The production registry is in a production AWS account. The accounts are isolated for security and compliance reasons. In this architecture, promotion involves copying the artifact from one storage backend to another. The copy is bit-for-bit identical. After copying, the hash is recomputed and verified to match the original. This ensures that the promoted artifact is exactly the same as the source artifact. The two-registry architecture is more complex than the single-registry architecture but provides stronger isolation guarantees.

Environment isolation also applies to configuration and dependencies. A model might depend on a specific version of a prompt template or a specific configuration file. Those dependencies are also versioned and registered. When a model is promoted, its dependencies are promoted with it. The registry ensures that the model and all its dependencies are available in the target environment before allowing deployment. This prevents deployment failures caused by missing dependencies.

## Registry Integration with CI/CD Pipelines

The registry is most effective when it is integrated into CI/CD pipelines so that model publication, testing, promotion, and deployment happen automatically as part of a continuous delivery workflow. The pipeline removes manual steps, reduces errors, and accelerates the path from training to production.

The CI/CD integration starts with automated publishing. When a training job completes, the pipeline automatically publishes the trained model to the registry. The publication includes the model artifacts, the training configuration, the evaluation metrics, and links to the training logs. The automation ensures that every trained model gets registered. There is no risk of losing a model because someone forgot to save it. The automation also enforces metadata standards. If the training job did not produce required metadata, the publication fails, and the training job is marked as incomplete. This enforces discipline in the training process.

After publication, the pipeline triggers automated testing. The registry notifies the CI/CD system that a new model is available. The CI/CD system deploys the model to a testing environment and runs a test suite. The test results are recorded in the registry metadata. If tests pass, the pipeline advances the model to the review stage. If tests fail, the pipeline marks the model as failed and notifies the data scientist. The automated testing catches regressions and quality issues before any human looks at the model. This saves reviewer time and prevents bad models from advancing.

The pipeline also automates promotion. After a model is approved for staging, the pipeline deploys it to the staging environment and runs a staging test suite. If the staging tests pass and the model has been in staging for the required minimum duration, the pipeline automatically promotes the model to the production-ready state. The final production deployment is typically manual or requires an explicit approval, but the pipeline handles everything up to that point. This reduces the time from training to production-ready from days to hours.

The registry API provides webhooks and event streams that the CI/CD system subscribes to. When a model is published, an event is emitted. When a model is approved, another event is emitted. The CI/CD system listens for these events and triggers appropriate actions. This event-driven architecture decouples the registry from the CI/CD system. The registry does not need to know about every CI/CD tool. It just emits events. The CI/CD tool subscribes to the events it cares about.

## Registry Integration with Serving Infrastructure

The serving infrastructure — the systems that run models in production — also integrates with the registry. When a model needs to be deployed, the serving infrastructure queries the registry to retrieve the model artifacts. The retrieval is authenticated and authorized. The serving infrastructure provides credentials that prove it is allowed to deploy models to the target environment. The registry checks those credentials and returns the model only if authorization succeeds.

The retrieval process includes validation. The serving infrastructure requests a model by version identifier. The registry returns the model along with a cryptographic hash. The serving infrastructure recomputes the hash of the downloaded artifact and compares it to the registry-provided hash. If the hashes match, the artifact was not corrupted during download. If they do not match, the retrieval is considered failed, and the deployment is aborted. This hash verification ensures that the deployed model is exactly the artifact that was registered.

The serving infrastructure also queries the registry for metadata. Before deploying a model, the serving system checks the deprecation status. If the model is deprecated, the serving system logs a warning or refuses deployment depending on the deprecation stage. The serving system also checks approval status. If the model is not approved for the target environment, deployment fails. These checks prevent configuration errors from bypassing governance. Even if a deployment configuration references a deprecated or unapproved model, the serving infrastructure will not deploy it.

The registry provides APIs for runtime status updates. When a model is deployed, the serving infrastructure notifies the registry. The registry records the deployment in the metadata layer. This creates a real-time view of which models are deployed where. The registry can answer questions like "which environments are running version 2.4.7" or "how many active deployments are using deprecated versions" by querying the deployment status metadata. This visibility is essential for deprecation planning and incident response.

The serving infrastructure can also use the registry for dynamic model loading. Instead of baking a model into a container image, the serving infrastructure starts with a generic runtime and loads the model from the registry at startup. The model version to load is specified in a configuration file or environment variable. This allows changing the deployed model version without rebuilding or redeploying containers. The dynamic loading pattern is especially useful in environments that use auto-scaling. New instances start quickly because they do not need to download large container images. They download the model artifacts from the registry at startup.

## Registry Access Control and Role-Based Permissions

Access control is what prevents unauthorized publication, promotion, or deployment of models. The registry enforces role-based access control where every action requires appropriate permissions. The roles are designed to enforce separation of duties. No single person can publish, approve, and deploy a model without involving others. This separation is a control against both malicious actions and accidental mistakes.

The typical role structure includes data scientists who can publish models to development, peer reviewers who can approve models for staging, technical leads who can promote models to production-ready status, and platform engineers who can deploy models to production. The exact roles vary by organization, but the principle is consistent: the person who creates a model is not the same person who approves it for production, and the person who approves it is not the same person who deploys it. This three-party control is standard in regulated industries and good practice everywhere else.

Access control is enforced at the API layer. Every API request includes authentication credentials. The API verifies the credentials and checks whether the requester has permission to perform the requested action. If permission is missing, the request is denied with an error message explaining which permission is required and how to request it. The access control checks are logged. Every denied access attempt is recorded. Repeated denied attempts trigger alerts that might indicate a misconfiguration or an attempted security breach.

The registry also supports audit roles. An auditor can query the registry to see which models are deployed, who approved them, when they were promoted, and who deployed them. The auditor cannot modify anything. They have read-only access to all metadata and logs. This role is essential for compliance and incident investigation. During an incident, the auditor role lets the incident response team see exactly what was deployed without worrying that their investigation will accidentally change system state.

Access control also applies to model retrieval. A deployment system must authenticate before retrieving a model. The authentication proves that the requester is authorized to deploy to the target environment. This prevents a compromised development environment from retrieving production models. Even if an attacker gains access to a development deployment system, they cannot retrieve production models because the development credentials do not grant access to production artifacts. The isolation limits the blast radius of security incidents.

## Registry Backup and Disaster Recovery

The registry is a critical system. If the registry is lost, you lose the ability to deploy models, the record of what is deployed, the audit trail of approvals, and the metadata needed for deprecation and compliance. Registry backup and disaster recovery are not optional. They are requirements.

The backup strategy includes both the storage layer and the metadata layer. The storage layer backup is straightforward. Model artifacts are stored in object storage with versioning enabled. The object storage provider maintains multiple copies across availability zones. Point-in-time snapshots are taken daily and retained for 30 days. If an artifact is accidentally deleted, it can be recovered from storage versioning. If the entire storage bucket is lost, it can be restored from the most recent snapshot. The recovery point objective for the storage layer is 24 hours. The recovery time objective is four hours.

The metadata layer backup is more complex because it includes relational data with foreign keys and constraints. The metadata database is backed up using continuous replication to a standby instance in a different availability zone. If the primary database fails, the standby is promoted automatically. The failover is typically complete within five minutes. The metadata database also takes nightly full backups that are stored in separate geographic regions. If both the primary and standby databases are lost, the database can be restored from the nightly backup. The recovery point objective is 24 hours. The recovery time objective is two hours.

The disaster recovery plan is tested quarterly. The test involves simulating a complete loss of the primary registry, failing over to the disaster recovery environment, and verifying that deployments can continue from the disaster recovery registry. The test identifies gaps in the recovery process and validates that documentation is current. A test that succeeds proves that disaster recovery works. A test that fails is valuable because it finds problems before a real disaster occurs. The disaster recovery test is treated as a high-priority event. It involves the full platform team and is scheduled weeks in advance.

The backup and disaster recovery strategy also includes configuration. The registry configuration — access control policies, environment definitions, promotion workflows — is stored as code in version control. If the registry needs to be rebuilt from scratch, the configuration can be restored from version control and applied to a new registry instance. This configuration-as-code approach ensures that the registry's operational behavior can be reproduced even if the infrastructure is completely lost.

## Registry Observability and Usage Analytics

The registry is not a black box. It exposes metrics, logs, and analytics that provide visibility into how models are being published, promoted, and deployed. This observability is essential for identifying problems, planning capacity, and informing deprecation decisions.

The registry exposes real-time metrics on publication rate, retrieval rate, approval latency, promotion success rate, and error rates. These metrics are collected by the monitoring system and displayed in dashboards. A sudden increase in error rate might indicate a misconfiguration or a service degradation. A drop in publication rate might indicate that training pipelines are failing. The metrics provide early warning of operational problems before they escalate into incidents.

The registry also produces detailed logs. Every action — publication, approval, promotion, retrieval, access denial — is logged with a timestamp, the identity of the requester, the action taken, and the result. These logs are retained for at least one year and are searchable. During an incident, the logs provide a detailed timeline of what happened. During an audit, the logs provide evidence of who did what and when. The logs are tamper-proof. They are written to append-only storage that cannot be modified after writing. This ensures that the logs are trustworthy evidence.

The registry provides usage analytics that answer questions like "which models are most frequently deployed," "which environments have the most active deployments," "how long does it take for a model to go from development to production," and "which teams are publishing the most models." These analytics inform resource allocation, process improvement, and strategic planning. A product team that sees that their models take twice as long to reach production as other teams can investigate bottlenecks in their approval process. A platform team that sees that staging deployments are increasing rapidly can plan for additional staging capacity.

The analytics also support deprecation planning. The registry tracks access patterns for every model version. If a version has not been accessed in 90 days, it is a candidate for deprecation. If a version is accessed daily, it is still in use and should not be deprecated. The access analytics remove guesswork from deprecation decisions. You know which versions are active and which are dormant.

## The Registry as Organizational Foundation

The model registry is infrastructure, but it is also a policy enforcement mechanism, an audit system, and a collaboration platform. It is where the organization's governance decisions are made concrete. The registry embeds your approval workflows, your environment boundaries, your access control policies, and your deprecation timelines into a system that enforces them automatically. Without the registry, those policies are documentation that people might follow. With the registry, those policies are code that the system enforces.

The registry also changes how teams think about models. Before the registry, models are files. After the registry, models are versioned artifacts with lifecycles, metadata, and governance. The shift from files to artifacts is a maturity transition. It is the difference between ad hoc model management and engineering discipline. Teams that make this transition experience fewer incidents, faster deployments, and better compliance posture.

Building a model registry is not trivial. It requires infrastructure, tooling, process design, and organizational buy-in. The investment is substantial. The return is greater. The registry eliminates entire categories of operational risk and makes reliable model deployment possible. It is the foundation on which every other deployment practice depends.

This chapter covered the full lifecycle of model artifacts from versioning through deprecation to the registry pattern that makes it all work. The principles apply not just to models but to every deployable artifact in an AI system — prompts, configurations, policies, and business rules. The next chapter extends these concepts to prompt and configuration deployment, where the artifacts are smaller but the need for version control and governance is just as critical.

