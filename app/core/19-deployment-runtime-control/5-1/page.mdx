# 5.1 — The Routing Problem: Why Static Model Assignment Fails

Most teams hardcode their model selection and then spend months trying to understand why costs explode or quality degrades. They pick Claude Opus 4.5 as their default model because it performs well in testing, deploy it to production, and watch their inference bill climb to six figures per month. Or they choose GPT-5-mini to control costs and then field user complaints about response quality. The problem is not the models they chose. The problem is the idea that any single model should handle every request.

Static model assignment — always routing to the same model regardless of query complexity, user tier, or task type — is the default approach because it is simple to implement. Your routing logic is a single line: send every request to model X. No classification logic, no decision trees, no complexity. But this simplicity comes at a cost that compounds every hour your system runs in production. You are either over-provisioning capability for simple queries or under-serving users who need more sophisticated reasoning. There is no middle ground when you only have one option.

## Why Traffic Patterns Destroy Static Assignment

Production traffic is not uniform. A customer service chatbot receives thousands of requests per hour, but the distribution of complexity is exponential. Seventy percent of queries are simple lookups that could be answered by retrieving a knowledge base article and summarizing it in two sentences. Twenty percent require moderate reasoning — comparing options, explaining a process, handling a multi-turn conversation with context. Ten percent involve complex problem-solving — diagnosing an unusual issue, handling an edge case not covered by documentation, reasoning through ambiguous requirements.

When you route all of this traffic to Claude Opus 4.5, you are spending premium pricing on queries that Claude Haiku 4.5 could handle identically. The user asking "what are your business hours" does not need the same model capability as the user asking "I have a unique tax situation involving foreign income and deferred compensation — how does your product handle this?" But with static assignment, both queries cost you the same. You are paying for a capability ceiling that most requests never approach.

The inverse problem is equally destructive. A team deploys GPT-5-mini as their default model to control costs. For seventy percent of queries, it performs well. For the remaining thirty percent, it produces responses that are technically correct but lack depth, miss nuance, or fail to handle the reasoning complexity the user expected. Users notice. Some submit the same query multiple times hoping for a better response. Some abandon the product. Some escalate to human support, which costs more than the model inference you were trying to save. Static assignment to a lower-capability model does not save money if it increases support load or user churn.

## The Dimensionality Problem

Query complexity is not the only variable that matters. User tier affects cost tolerance. A free-tier user might receive a response from a smaller model, while an enterprise customer paying thousands of dollars per month expects premium quality on every interaction. Task type affects optimal model selection. A query asking for Python code generation might perform better on a model with strong code training, while a creative writing request might benefit from a different capability profile. Input length affects token costs and latency. A query with ten thousand tokens of context costs dramatically more to process than a fifty-token question.

Time of day, traffic load, and current model availability all influence what "optimal" means for a given request. During peak hours, your primary model might be rate-limited or experiencing elevated latency. Routing every request to that model creates a queue that degrades user experience across the board. During off-peak hours, you have excess capacity and can afford to use higher-capability models more liberally. Static assignment ignores all of this context. It makes the same decision at three in the morning with zero queue depth that it makes at two in the afternoon with five hundred requests waiting.

Geographic distribution adds another dimension. A user in North America might be routed to a model deployment in a US region with low latency and high availability. A user in Europe might be better served by a European deployment, even if it means using a slightly different model version. A user in a region with limited model availability might need to fall back to a different model tier entirely. Static assignment based on model name alone cannot account for deployment topology, regional pricing differences, or local regulatory requirements.

## The Over-Provisioning Tax

When you default to the most capable model for all traffic, you are paying a continuous tax on every simple query. In early 2025, a fintech company launched a conversational assistant for account inquiries using GPT-5 as their default model. The product worked well. Users were happy. The executive team reviewed the first month's invoice and discovered they had spent forty-seven thousand dollars on model inference for a product with twelve thousand active users. The average query cost them three dollars and ninety cents. That math did not work.

The team ran an analysis on their query logs. They sampled five thousand queries and manually categorized them by complexity. Sixty-two percent were simple factual lookups that required no reasoning beyond retrieving and summarizing information. Twenty-four percent involved moderate reasoning — comparing two account options, explaining a fee structure, walking through a multi-step process. Fourteen percent required complex reasoning — analyzing spending patterns across multiple accounts, providing personalized financial advice, handling unusual scenarios not covered by standard documentation.

They tested the simple queries against GPT-5-mini and Claude Haiku 4.5. Quality was indistinguishable from GPT-5 for this category. They tested the moderate reasoning queries against GPT-5 and Claude Sonnet 4.5. Quality remained high on the smaller models. Only the complex reasoning queries required the full capability of GPT-5 or Claude Opus 4.5. By continuing to route all traffic to GPT-5, they were overspending by roughly thirty-one thousand dollars per month. The capability they paid for was used fourteen percent of the time. The rest was waste.

## The Under-Serving Trap

The opposite failure mode is equally common. A legal tech startup building a contract analysis tool chose Llama 4 Scout as their default model to control costs during their beta phase. For straightforward contracts — employment agreements, NDAs, simple service contracts — the model performed adequately. For complex commercial agreements involving multiple parties, conditional obligations, and nested clauses, the model struggled. It missed edge cases. It failed to identify ambiguities that a human lawyer would flag immediately. It produced summaries that were accurate at a surface level but lacked the depth that legal professionals needed.

Users noticed. The product gained a reputation for being useful for simple documents but unreliable for real work. Churn among professional users was high. The team eventually upgraded to Claude Opus 4.5 for all queries, which solved the quality problem but tripled their inference costs. The revenue they generated from retained users justified the cost, but the damage to their brand during the beta period was permanent. Some users who left during the low-quality phase never came back, even after the product improved.

The lesson was not that they should have used Claude Opus 4.5 from the start. The lesson was that static assignment forced them to choose between cost and quality when the correct answer was dynamic routing. Simple contracts could use Llama 4 Scout. Complex contracts needed Claude Opus 4.5. The model selection should have depended on the input, not on a global configuration setting.

## Why Static Assignment Persists

Despite these problems, static model assignment remains the default for most teams because it is easy. You configure your inference client with a model name and an API key, and every request goes to the same endpoint. No classification logic, no decision trees, no runtime complexity. Your system is stateless with respect to model selection. You can reason about behavior, debug issues, and estimate costs without worrying about routing logic.

This simplicity is valuable in the early stages of product development. When you are testing an idea, validating a use case, or running a proof of concept, static assignment lets you focus on product logic rather than infrastructure. But the moment you reach production scale — thousands of queries per day, paying users, cost scrutiny from finance — the simplicity becomes a liability. You are leaving money on the table or delivering suboptimal quality, and the gap widens every day.

## The Signal That Static Assignment Is Failing

The clearest signal that you need dynamic routing is variance in your quality metrics by query type. If you measure quality across all queries and see an aggregate score of eighty-five percent, you might conclude your system is performing well. But if you segment by query complexity, you often find that simple queries score ninety-seven percent, moderate queries score eighty-eight percent, and complex queries score sixty-three percent. Your aggregate metric hides the fact that you are failing a significant minority of users.

The second signal is cost analysis showing that your average query cost is far higher than the median. This suggests that most queries are cheap to serve, but you are paying a uniform price across all traffic. If your average cost per query is two dollars and forty cents, but your median is seventy cents, you are likely over-provisioning for the majority of requests. A few high-complexity queries are driving up the average, but every query is paying the premium price.

The third signal is user feedback that your system is "inconsistent." Users report that sometimes responses are excellent, sometimes they are mediocre, and they cannot predict which they will get. This often means your static model choice works well for some query types and poorly for others. The user who asks a simple factual question gets a fast, high-quality response. The user who asks a complex reasoning question gets a response that feels incomplete. Your model is not inconsistent — your routing strategy is failing to match capability to need.

## The Routing Decision as Architecture

Dynamic routing is not a feature you bolt on later. It is a foundational architectural decision that affects how you design your inference pipeline, how you instrument your system, and how you measure success. The moment you accept that different queries require different models, you must build the infrastructure to classify inputs, maintain model capability profiles, track quality by routing decision, and adjust thresholds in response to real-world performance.

This infrastructure is not optional at scale. Every production AI system that serves diverse traffic eventually builds some form of dynamic routing, whether they call it that or not. The only question is whether you build it intentionally as part of your initial architecture or retrofit it later after months of cost overruns and quality complaints. The teams that build it early gain a compounding advantage in both cost efficiency and user satisfaction. The teams that defer it pay the over-provisioning tax every day until they prioritize the work.

The next subchapter covers cost-aware routing — the strategy of using the cheapest model that meets your quality bar for each query.

