# 12.13 — The CI/CD Maturity Model: From Scripts to Platform

CI/CD maturity is not about tools. It's about how much you trust your pipeline to make decisions without human intervention. A mature pipeline deploys code to production, monitors for problems, and reverts automatically if something goes wrong — all without waking anyone up. An immature pipeline requires a human to watch every stage, approve every transition, and manually verify that production is healthy after deployment. The difference is not technical capability. Most organizations have the tools to automate deployment. The difference is confidence. Mature organizations trust their automation because they've invested in the testing, observability, and rollback mechanisms that make automation safe. Immature organizations don't trust automation because they haven't built those foundations. They're stuck in manual processes not because they lack tools, but because they lack the infrastructure that makes automation reliable.

The path from immature to mature CI/CD is a ladder. You can't skip rungs. You can't automate deployment before you automate testing. You can't automate rollback before you have production observability. Each level builds on the previous level. Attempting to jump ahead — automating deployment without automated testing, for example — creates brittle systems that fail in unpredictable ways. The maturity model is a progression, not a menu. You climb step by step.

## Level 1: Manual Deployments with Some Scripts

At Level 1, deployments are human-driven events. An engineer writes code, manually runs tests on their laptop, manually builds artifacts, manually copies them to production servers, and manually verifies that the deployment worked. Some steps might use scripts — a build script, a test runner, a deployment script — but the overall process is orchestrated by a human. The engineer decides when to deploy, what to deploy, and whether the deployment succeeded.

Level 1 organizations deploy infrequently because deployments are expensive. Each deployment requires dedicated engineering time, careful coordination, and manual verification. Deployments happen weekly, biweekly, or monthly. Engineers batch changes to minimize deployment overhead. When something breaks, the engineer who deployed it investigates, fixes it, and redeploys manually.

The failure mode at Level 1 is inconsistency. Different engineers follow different processes. One engineer runs all tests before deploying. Another skips tests because they're slow. One engineer verifies latency after deployment. Another assumes everything is fine if no errors appear. This inconsistency leads to unpredictable deployment outcomes. Sometimes deployments work. Sometimes they don't. Nobody knows why.

Level 1 is acceptable for very small teams — two or three engineers working on a low-stakes internal tool. It's unacceptable for anything customer-facing or anything involving more than five engineers. The variability and manual overhead don't scale.

## Level 2: Automated Builds, Manual Deployments

At Level 2, the build and test process is automated. Engineers commit code to version control. A CI system detects the commit, checks out the code, runs tests, builds artifacts, and reports success or failure. If tests pass, the CI system produces a deployable artifact — a container image, a binary, a package. But deployment to production is still manual. An engineer triggers deployment by clicking a button, running a command, or following a runbook.

Level 2 organizations deploy more frequently than Level 1 because the build process is no longer manual labor. But deployment frequency is still limited by human availability. If deployment requires an engineer to watch for 30 minutes, deployments happen a few times per week during business hours.

The benefit of Level 2 is consistency in builds. Every build runs the same tests in the same environment. Flaky laptop environments are eliminated. The build artifact is reproducible. The failure mode is deployment drift. The artifact is consistent, but how it's deployed varies. One engineer deploys to region A first. Another deploys to all regions simultaneously. One engineer monitors latency. Another monitors error rate. Deployment outcomes are more predictable than Level 1, but still inconsistent.

Level 2 is the baseline for any professional software organization. If you're still at Level 1, move to Level 2 immediately. Automate builds and tests first. Worry about deployment automation later.

## Level 3: Automated Deployment to Staging, Manual Promotion to Production

At Level 3, deployments to non-production environments are fully automated. Code that passes CI is automatically deployed to a staging environment. Staging is a production-like environment where changes are tested under realistic conditions. Engineers verify that staging behaves correctly. If staging looks good, an engineer manually promotes the change to production.

Level 3 organizations deploy to staging continuously — every merged commit reaches staging within minutes. But production deployments are still gated by human judgment. An engineer reviews staging metrics, confirms that functionality is correct, and clicks the "promote to production" button. Production deployments happen daily or multiple times per day, but always with human oversight.

The benefit of Level 3 is that most deployment mechanics are automated. The staging deployment exercises the same infrastructure, the same artifact promotion, the same health checks that will later run in production. When promotion to production happens, the only new element is the production environment itself. The failure mode is staging-production skew. Staging doesn't perfectly replicate production traffic, production data characteristics, or production dependencies. Changes that work in staging sometimes fail in production because the environments aren't identical.

Level 3 is where most mature engineering teams operate as of 2026. It balances automation with safety. Staging deployment automation provides fast feedback. Manual promotion to production provides judgment. Level 3 is stable and predictable. But it's not the final state.

## Level 4: Automated Deployment with Manual Approval Gates

At Level 4, deployment to production is automated, but it pauses at approval gates. Code that passes CI and staging verification enters a deployment queue. A human approver reviews the change, checks production health, and approves or rejects the deployment. If approved, the deployment proceeds automatically. If rejected, the deployment is canceled and the change goes back to the team for revision.

Level 4 organizations deploy to production multiple times per day with minimal manual effort. The approval step is lightweight — reviewing metadata, checking dashboards, confirming that timing is appropriate. The actual deployment mechanics are fully automated. The approver doesn't run commands or click through infrastructure. They just approve.

The benefit of Level 4 is that human judgment is preserved without requiring human execution. The approver acts as a final safety check. They can block a deployment if production is already unstable, if a major incident is ongoing, or if the change looks risky. But they don't need deep technical knowledge of the deployment process. They're making a go/no-go decision, not orchestrating infrastructure.

The failure mode is approval bottlenecks. If only one person can approve production deployments and that person is unavailable, deployments stall. If approvals require deep review and take 30 minutes each, deployment frequency drops. The approval gate must be fast — under five minutes — or it defeats the purpose of automation. Some Level 4 organizations implement time-based auto-approval: if no human responds within ten minutes, the deployment proceeds automatically. This removes the bottleneck while preserving the option for human intervention.

Level 4 is appropriate for high-risk systems where human judgment provides value but manual deployment execution is too slow. Financial systems, healthcare systems, and regulated industries often operate at Level 4 because regulatory or risk frameworks require human oversight even when automation is technically feasible.

## Level 5: Fully Automated Deployment with Automated Rollback

At Level 5, deployment to production is fully automated with no manual gates. Code that passes CI, passes staging verification, and meets all deployment criteria deploys to production automatically. Post-deployment health checks monitor the deployment. If health checks pass, the deployment is confirmed. If health checks fail, the deployment rolls back automatically. Humans are notified of deployment outcomes but do not intervene unless automation fails.

Level 5 organizations deploy tens or hundreds of times per day. Deployment is continuous. Engineers merge code and it reaches production within 20 minutes without human touch. Deployment is invisible infrastructure — it just works. Engineers focus on writing code and defining quality criteria. The pipeline handles everything else.

The benefit of Level 5 is maximum velocity with maximum safety. Deployments are fast because there's no manual gate. Deployments are safe because automated rollback catches problems before they escalate. The feedback loop is tight — engineers see production impact of their changes within minutes. This enables rapid iteration.

The failure mode is over-reliance on automation. If automated health checks miss a subtle degradation mode — like user confusion that doesn't manifest in latency or error rate — the deployment stays live even though it's causing harm. If automated rollback has a bug and fails to revert a bad deployment, the system is in an undefined state with no human monitoring. Level 5 requires extremely high confidence in your testing, observability, and rollback infrastructure. You must have proof that automation works, not just belief.

Level 5 is the target for high-performing engineering organizations. Companies like Google, Meta, and Amazon operate large parts of their infrastructure at Level 5. But reaching Level 5 takes years of investment in testing, observability, and deployment infrastructure. Don't aim for Level 5 on day one. Climb the ladder.

## Assessing Your Current Maturity Level

To assess your current level, ask these questions. Are builds automated? If no, you're at Level 1. If yes, are deployments to any environment automated? If no, you're at Level 2. If yes, are deployments to production automated? If no, you're at Level 3. If yes, do production deployments require manual approval? If yes, you're at Level 4. If no, are rollbacks automated based on health check failures? If yes, you're at Level 5. If no, you're between Level 4 and Level 5.

Most organizations are not at a single level uniformly. Some services are at Level 5 while others are at Level 3. Low-risk services move to higher maturity faster. High-risk services stay at lower maturity longer. This is fine. Maturity is not binary. The goal is not to force every service to Level 5. The goal is to provide the infrastructure that enables teams to reach the maturity level appropriate for their risk profile.

Track maturity per service, per team, and per deployment type. A model deployment might be at Level 3 while a configuration deployment is at Level 5. Configuration changes are lower risk, so higher automation is justified. Model changes are higher risk, so manual gates provide value. This nuance is lost if you assess maturity at the organization level only.

## Climbing the Maturity Ladder

To move from Level 1 to Level 2, automate builds and tests first. Set up a CI system. Write automated tests. Ensure every commit triggers a build. This is foundational work. It's not glamorous, but it's necessary. Without automated testing, you can't trust automated deployment.

To move from Level 2 to Level 3, automate deployment to a non-production environment. Create a staging environment. Configure the CI system to deploy passing builds to staging automatically. Monitor staging. Verify that automated deployment works reliably before attempting production automation.

To move from Level 3 to Level 4, automate production deployment with a manual approval gate. Build a deployment dashboard. Add an approval button. Ensure that clicking approve triggers deployment without further human action. Train multiple people to approve so availability isn't a bottleneck. Measure time-to-approval. If approval takes longer than ten minutes on average, simplify the approval criteria.

To move from Level 4 to Level 5, automate post-deployment health checks and rollback. Define health check criteria. Implement automated rollback triggered by health check failures. Test rollback in staging. Run health checks in shadow mode in production — evaluate but don't trigger rollback — to calibrate thresholds. When you have high confidence that health checks catch real problems and rollback works reliably, enable automated rollback in production. Start with low-risk services. Expand gradually.

Each transition takes months, not days. Level 1 to Level 2 might take two months. Level 2 to Level 3 might take four months. Level 3 to Level 4 might take six months. Level 4 to Level 5 might take a year. These timelines are for a single service. Scaling across an organization takes longer. This is a multi-year journey.

## Maturity Traps and Anti-Patterns

The first maturity trap is automating too early. If your tests are flaky, automating deployment doesn't make deployment safer — it makes unreliable tests block every deployment. If your rollback mechanism is untested, enabling automated rollback might make incidents worse. Automation amplifies your existing capabilities. If those capabilities are weak, automation amplifies weakness.

The second maturity trap is under-automating when capable. Some organizations have reliable tests, comprehensive observability, and proven rollback mechanisms but still require manual approval for every production deployment. This is risk aversion masquerading as prudence. If you have the infrastructure to automate safely, automate. Manual gates that don't add value just slow you down.

The third maturity trap is inconsistent maturity across teams. If Team A operates at Level 5 and Team B operates at Level 2, cross-team dependencies become friction points. Team A expects continuous deployment. Team B expects weekly deployment windows. When A depends on B, A's velocity is constrained by B's maturity. Inconsistent maturity creates organizational drag. Invest in raising the floor, not just the ceiling.

The fourth maturity trap is measuring maturity by level number rather than outcomes. Reaching Level 5 is not the goal. Deploying safely and frequently is the goal. If Level 4 enables you to deploy ten times per day with zero incidents, that's success. If Level 5 enables you to deploy 100 times per day but incidents increase, that's failure. Maturity is a means, not an end.

## AI-Specific Maturity Considerations

AI systems add complexity to the maturity model. Traditional software CI/CD focuses on code correctness — does the code compile, do tests pass, do APIs return expected responses. AI CI/CD adds quality evaluation — does the model produce correct outputs, are responses safe, does the prompt architecture generalize. Evaluating quality is slower and less deterministic than evaluating correctness. This affects maturity timelines.

Multi-artifact deployments are more complex. Deploying an AI system might require deploying a model artifact, a prompt template, a retrieval index, and a configuration file. These artifacts have dependencies. The prompt template must match the model's expected format. The retrieval index must match the prompt's context structure. Coordinating multi-artifact deployment is harder than deploying a single binary. This pushes AI systems toward lower maturity levels — Level 3 or Level 4 — where human oversight catches coordination failures.

Eval integration is an AI-specific maturity requirement. At Level 2, you're automating tests. At Level 3, you're automating evals in staging. At Level 4, you're blocking production deployment if evals regress. At Level 5, you're running live traffic evals post-deployment and rolling back on quality degradation. Traditional CI/CD doesn't have an equivalent to eval gates. This is a new discipline that AI teams must build.

AI maturity also considers data lineage. Did the training data change? Did the eval dataset change? Is the model trained on data that reflects current production distribution? These questions don't exist in traditional software. An AI CI/CD pipeline at Level 4 or Level 5 must track data versions, detect data drift, and block deployment when data staleness exceeds thresholds. This is infrastructure that most AI teams have not yet built.

## The Platform Investment Required for High Maturity

Reaching Level 4 or Level 5 requires platform investment. You can't reach high maturity with ad-hoc scripts. You need a deployment platform — a system that orchestrates builds, manages artifacts, executes deployments, runs health checks, and triggers rollbacks. Building this platform takes engineering effort. For a team of ten engineers, expect to allocate one engineer full-time to platform work. For a team of 50 engineers, expect three to five engineers. For a team of 200 engineers, expect 10 to 15 engineers dedicated to deployment infrastructure.

The platform must support observability, policy enforcement, and self-service. Observability means every deployment is tracked, every health check result is logged, and every rollback is auditable. Policy enforcement means the platform prevents unsafe deployments — blocking deployments during blackout windows, enforcing eval gate passage, requiring approval for high-risk changes. Self-service means teams can deploy without platform team intervention. They configure their pipelines, define their health checks, and trigger deployments through a UI or API. The platform team builds the infrastructure. Application teams use it.

Platform investment is hard to justify when you're small. A ten-person startup should not build a custom deployment platform. Use off-the-shelf tools — GitHub Actions, GitLab CI, CircleCI, Jenkins. As you grow, invest in customization. By 50 engineers, you need platform investment. By 200 engineers, you need a platform team. The maturity ceiling is determined by platform investment.

## Section Synthesis: Deployment as Discipline

This chapter has covered CI/CD for AI systems — the stages, the artifacts, the gates, the triggers, the observability, the rollback integration, the multi-team coordination, and the maturity progression. Deployment is not a one-time event. It's a discipline. A deployment system is infrastructure that requires design, implementation, testing, and maintenance just like any other production system.

The infrastructure that enables safe velocity is not accidental. It's built deliberately over years. High-performing teams deploy continuously because they invested in testing, observability, and rollback. Low-performing teams deploy infrequently because they didn't. The difference is not risk tolerance. It's infrastructure maturity.

If you deploy once per month, you're not being careful. You're being slow because you don't trust your deployment process. If you deploy ten times per day, you're not being reckless. You're being fast because you built the infrastructure that makes speed safe. Velocity and safety are not trade-offs. They're both outcomes of the same thing: a mature deployment system.

Build that system. Start at Level 1. Climb to Level 2. Reach Level 3. Push toward Level 4. Aim for Level 5. The journey takes years. Start now.

