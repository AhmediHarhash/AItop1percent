# 5.11 — The Routing Decision Framework: Building Your Routing Strategy

Routing strategy is a business decision, not a technical one. It encodes your priorities. When you decide which requests route to expensive capable models and which route to cheap fast models, you are deciding how to allocate limited resources across competing demands. When you decide how aggressively to fail over to fallback models, you are deciding your tolerance for increased cost versus decreased availability. When you decide whether to prioritize latency or quality for particular customer segments, you are deciding which customers matter most to your business. Technical implementation follows from these business decisions. The framework for building routing strategy starts with clarifying priorities, understanding your model inventory, segmenting your traffic, defining routing rules, implementing fallbacks, and building observability for continuous optimization. Get this framework right and routing becomes a source of competitive advantage. Get it wrong and routing becomes a source of uncontrolled cost, degraded quality, and angry customers.

## Step 1: Define Your Optimization Priorities and Their Weights

Every routing system optimizes for something. The question is whether you optimize deliberately or accidentally. Deliberate optimization starts with naming your priorities and assigning relative weights. The three primary priorities for most AI systems are cost, latency, and quality. Your routing strategy will make trade-offs between these three. You need to know which trade-offs are acceptable and which are not.

For a customer support chatbot, quality might be the top priority because poor answers damage customer relationships. Latency is second because users expect fast responses. Cost is third because the value of good support exceeds the cost of inference. The priority ordering is quality first, latency second, cost third. This means routing rules should prefer higher-quality models even if they cost more, should avoid models with high tail latency even if they're cheaper on average, and should tolerate higher costs in exchange for better outcomes.

For a content moderation system scanning user-generated posts at scale, cost might be the top priority because the system processes millions of posts per day and small per-request cost differences compound massively. Quality is second because moderation accuracy matters. Latency is third because moderation can happen in the background — a two-second latency is fine, a 200-millisecond latency adds no value. The priority ordering is cost first, quality second, latency third. Routing rules should prefer the cheapest model that meets the minimum quality threshold, should batch requests to optimize throughput over latency, and should avoid premium models unless no cheaper alternative meets the quality bar.

For a legal research assistant used by attorneys billing five hundred dollars per hour, latency might be the top priority because every second of wait time is expensive. Quality is second because legal accuracy is non-negotiable. Cost is third because the customer's time is worth far more than inference cost. The priority ordering is latency first, quality second, cost third. Routing rules should prefer the fastest models that meet quality requirements, should use parallel routing and take the first acceptable response, and should tolerate high per-request costs to save user time.

These priorities are not universal. They are specific to your product, your users, and your business model. A routing strategy optimized for one set of priorities will fail under a different set. The framework begins with writing down your priorities explicitly, getting agreement from product and business stakeholders, and using those priorities to guide every routing decision.

## Step 2: Map Your Model Inventory and Their Profiles

You cannot route intelligently without understanding the characteristics of every model in your fleet. Build a model inventory that documents cost, latency, quality, rate limits, and acceptable use cases for each model. This inventory is your routing decision reference. When you're writing a routing rule, you look at the inventory to see which models meet your requirements.

For each model, document the cost per million tokens, the typical latency at the 50th and 95th percentiles, the quality level based on your evals, and the rate limit expressed as requests per minute or tokens per minute. For GPT-5, you might document: cost is eight dollars per million input tokens, median latency is 420 milliseconds, 95th percentile latency is 850 milliseconds, quality score on your eval suite is 94 percent, rate limit is 500,000 tokens per minute. For Claude Haiku 4.5: cost is 40 cents per million input tokens, median latency is 180 milliseconds, 95th percentile latency is 320 milliseconds, quality score is 87 percent, rate limit is 1.5 million tokens per minute.

The inventory also notes which models are suitable for which tasks. GPT-5 is suitable for complex reasoning, multi-turn conversations, code generation, and high-stakes decision support. GPT-5-mini is suitable for classification, simple question answering, low-stakes chat, and high-volume batch processing. Claude Opus 4.5 is suitable for long-context tasks, document analysis, and nuanced reasoning. Claude Haiku 4.5 is suitable for fast responses, high-throughput scenarios, and cost-sensitive applications. These task suitability notes prevent you from routing requests to models that technically could handle the task but would do so poorly or inefficiently.

The model inventory is a living document. When a new model is released, you benchmark it on your eval suite, document its cost and latency characteristics, and add it to the inventory. When a model's pricing changes, you update the inventory. When your evals reveal that a model's quality has improved or degraded, you update the quality score. The inventory reflects current reality, not assumptions from six months ago.

Some teams maintain model inventory as a structured data file — a JSON or YAML document with fields for each model attribute. This lets routing systems read the inventory programmatically. Other teams maintain it as a wiki page or spreadsheet. Programmatic access is better for automation, but even a spreadsheet is valuable if it's kept up to date. The format matters less than the discipline of maintaining accurate data.

## Step 3: Classify Your Traffic into Routing Segments

Not all requests are the same. Some come from paying customers, others from free users. Some require high accuracy, others tolerate lower quality. Some are latency-sensitive, others are batch jobs that can wait. Routing strategy begins by segmenting traffic into categories with similar requirements, then defining routing rules for each segment.

A segment is defined by request characteristics you can detect at routing time: which API endpoint was called, what the customer's pricing tier is, what the request size is, what the user's historical behavior is, what the time of day is. For a documentation assistant, you might define segments by customer tier: enterprise customers who pay for premium service, small business customers on standard plans, free-tier users on community plans. For a code generation tool, you might segment by task complexity: simple autocomplete requests, full function generation requests, complex refactoring requests.

Segments should be mutually exclusive and collectively exhaustive. Every request should match exactly one segment. If segments overlap, routing logic becomes ambiguous — a request matches two segments with conflicting rules, and the system has to pick one arbitrarily. If segments don't cover all traffic, some requests won't match any rule and will route unpredictably. The segment definitions need to be crisp and complete.

For some systems, segments are hierarchical. At the top level, segment by customer tier. Within each tier, segment by request type. Within each request type, segment by request size or complexity. Hierarchical segments let you define default rules at high levels and override them at lower levels. Enterprise customers use GPT-5 by default, except for simple classification tasks which use GPT-5-mini. Free users use GPT-5-mini by default, except for complex reasoning tasks which use Claude Sonnet 4.5 because Sonnet offers better cost-efficiency than GPT-5 for the free tier's quality requirements.

The segmentation should align with your business model and your users' expectations. If you promise enterprise customers that they get "premium AI models," the routing rules for the enterprise segment need to reflect that promise. If you position the free tier as "limited but functional," the routing rules for the free segment need to deliver functional results while keeping cost sustainable. Segment definitions are where business commitments translate into routing logic.

## Step 4: Define Routing Rules for Each Segment

A routing rule specifies the primary model for a segment, the conditions under which that model should be used, and any constraints or overrides that apply. The rule is written in the configuration format your routing system uses — typically YAML or JSON — and stored in version-controlled configuration files. Rules should be explicit and auditable. Anyone reading the rule should understand what it does and why.

For the enterprise segment, a rule might specify: primary model is GPT-5, use this model for all requests unless the request explicitly opts into a different model via an API parameter, log all routing decisions for this segment for compliance purposes, apply no cost limits because enterprise customers are billed based on usage. The rule makes enterprise routing behavior explicit and ensures it aligns with enterprise customer expectations.

For the free-tier segment, a rule might specify: primary model is GPT-5-mini, use this model unless the request involves long context or complex reasoning, in which case route to Claude Haiku 4.5, apply a per-user rate limit of 50 requests per day to prevent abuse, return a quota-exceeded error if the user exceeds the limit. The rule encodes the free tier's trade-offs — cheap sustainable service with quality and rate constraints.

Rules can include conditional logic. If the request is larger than eight thousand tokens, route to Claude Opus 4.5 because it handles long context better than GPT-5. If the request's language is not English, route to GPT-5 because its multilingual performance is stronger. If the request comes from a specific geographic region, route to a model deployed in a nearby region to reduce latency. Conditional logic lets routing rules adapt to request characteristics dynamically.

Rules can also include time-based logic. During peak hours when rate limits are more likely to be hit, prefer models with higher rate limits or distribute traffic more evenly across models. During off-peak hours, consolidate traffic onto fewer models to reduce operational complexity. On weekends when usage is lower, run experiments with new models that you wouldn't test during high-traffic weekdays. Time-based routing optimizes for the reality that traffic patterns change predictably.

Writing routing rules is part logic, part judgment. The logic comes from your priorities, your model inventory, and your segment definitions. The judgment comes from understanding your users, your product, and your business. A technically optimal rule that routes based purely on cost might produce poor user experiences. A user-experience-optimal rule that ignores cost might be financially unsustainable. The best routing rules balance multiple objectives and make trade-offs explicit.

## Step 5: Implement Fallback Hierarchies

Every primary model will fail occasionally. Fallback hierarchies ensure that when the primary fails, requests route to a secondary model instead of returning errors to users. The fallback hierarchy is an ordered list of models to try in sequence. When the primary model fails, try the first fallback. If the first fallback fails, try the second fallback. If all fallbacks fail, return an error.

Fallback hierarchies should balance reliability and cost. The first fallback should be a model with similar quality to the primary, so users don't experience a quality drop. The second fallback can be a cheaper model with slightly lower quality — better to degrade quality slightly than to fail entirely. The third fallback, if needed, might be an even cheaper model with lower quality. The hierarchy trades off quality for availability as you move down the list.

For enterprise customers, the fallback hierarchy might be: GPT-5 as primary, Claude Opus 4.5 as first fallback, GPT-5-mini as second fallback. This ensures that even if two providers fail, enterprise customers still get some response. The third-tier fallback delivers lower quality than the primary, but the routing rule documents that this is acceptable under failure conditions — the alternative is no service at all.

For free-tier users, the fallback hierarchy might be: GPT-5-mini as primary, Claude Haiku 4.5 as first fallback, Llama 4 Scout as second fallback. The hierarchy uses progressively cheaper models because cost control matters more for free-tier traffic. The trade-off is that if the primary and first fallback both fail, the second fallback might produce lower-quality responses.

Fallback hierarchies require cross-provider redundancy. If your primary and all fallbacks are from the same provider, a provider-wide outage takes down your entire system. Effective fallback hierarchies mix providers. Primary from OpenAI, first fallback from Anthropic, second fallback from a self-hosted open-source model. This ensures that a single provider's failure doesn't cascade into total service unavailability.

Some routing systems implement probabilistic fallbacks. Instead of always using the primary and falling back only on failure, route ninety percent of traffic to the primary and ten percent to the fallback by default. This keeps the fallback model warm and ensures that you detect fallback model issues during normal operations, not during an incident when you suddenly need it. The cost is slightly higher because you're using the fallback even when the primary is healthy, but the reliability benefit is significant.

## Step 6: Build Observability for Continuous Optimization

Routing rules are not static. They evolve as your traffic patterns change, as new models become available, as your priorities shift, as you learn from production data. Observability is what makes evolution possible. You deploy a rule, observe how it performs, identify improvements, deploy an updated rule. This cycle repeats continuously.

The observability layer tracks where traffic is routing, whether fallbacks are triggering, what the cost and latency distributions are, whether experiments are showing meaningful differences, and whether any routing anomalies appear. These metrics feed into regular reviews where the team evaluates routing performance and decides whether to adjust rules. The review might be weekly for early-stage systems with rapidly changing traffic, or monthly for mature systems with stable patterns.

During the review, compare actual routing behavior to intended behavior. If the rule says seventy percent of traffic should go to GPT-5-mini and thirty percent to Claude Haiku 4.5, actual traffic distribution should match within a few percentage points. If actual distribution is eighty-twenty, investigate why. Maybe one model is faster and handles more requests per second. Maybe segment matching logic is incorrect and more requests are matching the GPT-5-mini segment than expected. Find the cause and decide whether to adjust the rule or accept the deviation.

Evaluate whether routing is meeting your objectives. If the objective was to reduce cost by twenty percent while maintaining quality, check whether cost decreased and whether quality metrics remained stable. If cost decreased by twenty-five percent but quality dropped by five percentage points, the rule overshot on cost and undershot on quality. Adjust the rule to shift more traffic back to higher-quality models.

Look for opportunities to optimize further. If observability shows that a certain segment's traffic almost never triggers fallbacks, maybe the fallback hierarchy is over-engineered. Simplify it. If another segment's traffic triggers fallbacks five percent of the time, maybe the primary model is undersized for that segment's needs. Upgrade the primary or add capacity. Observability turns routing from guesswork into data-driven optimization.

## Common Routing Patterns

Certain routing patterns appear repeatedly across production systems. Recognizing these patterns helps you design routing rules faster and avoid reinventing solutions to common problems. The three most common patterns are tiered routing, segment routing, and priority routing.

Tiered routing assigns models to customer tiers and routes based on the customer's tier. Free users get cheap models. Paid users get mid-tier models. Enterprise users get premium models. This pattern is simple to implement and aligns routing directly with business model. The challenge is handling edge cases — what happens when a free user's request is too complex for the cheap model? Either fail gracefully or allow an occasional upgrade to a better model with clear messaging about the limitation.

Segment routing classifies requests by task type and routes each type to the best model for that task. Simple classification requests go to fast cheap models. Complex reasoning requests go to capable expensive models. Document analysis requests go to long-context models. Segment routing optimizes for task-model fit rather than customer tier. This works well for products where users perform diverse tasks and different tasks have different requirements.

Priority routing assigns priority levels to requests and routes high-priority requests to premium models with guaranteed low latency, while low-priority requests go to cheaper models with variable latency. Medical diagnosis requests are high-priority. Content summarization requests are low-priority. Priority routing is common in systems where some requests are business-critical and others are nice-to-have. The routing rule can throttle low-priority requests during peak load to preserve capacity for high-priority requests.

These patterns can be combined. A system might use tiered routing at the top level — enterprise customers get better models than free users — and segment routing within each tier — enterprise users doing complex tasks get GPT-5, enterprise users doing simple tasks get GPT-5-mini. The combination gives you both business model alignment and task optimization.

## The Iterative Approach: Start Simple, Add Complexity Based on Data

The temptation when building a routing system is to design the perfect multi-tier, multi-segment, multi-fallback architecture on day one. Resist this temptation. Start with the simplest routing strategy that meets your immediate needs. Deploy it. Observe how it performs. Identify the biggest gap between actual behavior and desired behavior. Add one piece of complexity to address that gap. Repeat.

On day one, your routing strategy might be: all requests go to GPT-5-mini. This is simple, predictable, and cheap. After a week, observability shows that fifteen percent of requests produce poor-quality responses because GPT-5-mini lacks the capability for complex queries. You add a rule: requests with more than two thousand tokens or requests marked as high-complexity route to GPT-5 instead. This addresses the quality gap without over-engineering.

After another two weeks, observability shows that cost is higher than budget because more requests are hitting the high-complexity rule than expected. You add a rule: free-tier users always use GPT-5-mini regardless of complexity, paid users use the complexity-based routing. This brings cost under control. After another month, you notice that GPT-5 occasionally hits rate limits during peak hours. You add a fallback: if GPT-5 is unavailable, fall back to Claude Opus 4.5. Each change addresses a specific observed problem rather than a hypothetical future issue.

This iterative approach prevents over-engineering and keeps the routing system understandable. A routing configuration with three rules is easy to reason about. A routing configuration with thirty rules is a maintenance nightmare. Add rules only when observability shows that the current rules are insufficient. Remove rules when observability shows they're no longer needed. The routing system should be exactly as complex as necessary and no more complex.

## When Routing Becomes Too Complex: Signs You Need to Simplify

Routing complexity is not free. Every additional rule adds cognitive overhead for the team, increases the chance of rule conflicts or unintended interactions, and makes the system harder to debug. Complexity is worth it when it solves real problems. Complexity is waste when it solves hypothetical problems or when simpler solutions exist.

Signs that routing has become too complex: the team cannot explain the routing logic without referring to documentation. New engineers take more than a day to understand how routing works. Routing rules conflict and the conflict resolution logic is unclear. The routing configuration file exceeds five hundred lines. More than twenty percent of routing decisions require examining three or more rules to determine the outcome. Any of these signs indicate that simplification is needed.

Simplify by consolidating overlapping rules. If three rules all specify similar conditions and similar outcomes, combine them into one rule with slightly broader conditions. Simplify by removing rules that have negligible impact. If a rule handles less than one percent of traffic and that traffic could be handled by another rule with minimal degradation, delete the rule. Simplify by raising the level of abstraction. Instead of defining rules per endpoint, define rules per customer tier. Instead of defining rules per task, define rules per task category.

The goal is not to have the fewest possible rules. The goal is to have the clearest, most maintainable set of rules that achieves your routing objectives. Sometimes that means ten rules. Sometimes that means thirty rules. The number matters less than the understandability. If an engineer can look at the rules and predict where a given request will route without running the code, the complexity is manageable. If they cannot, the complexity needs to be reduced.

## Chapter Summary: Routing as a Core Competency

Request routing is not a minor infrastructure detail. It is a core competency for AI systems at scale. Routing decisions determine cost, quality, latency, and availability. Poor routing strategy leads to blown budgets, degraded user experiences, and system outages. Thoughtful routing strategy leads to cost efficiency, consistent quality, and high reliability. The difference between a team that treats routing as an afterthought and a team that treats it as a strategic capability is measured in hundreds of thousands of dollars per year and in user satisfaction scores.

The routing decision framework gives you a systematic process for building routing strategy. Define priorities. Map your model inventory. Segment your traffic. Write routing rules for each segment. Implement fallback hierarchies. Build observability for continuous optimization. Iterate based on data. This framework works whether you're a two-person startup routing a hundred requests per day or a hundred-person company routing ten million requests per day. The scale changes. The principles do not.

Routing is not a one-time decision. It is an ongoing discipline. Models change. Costs change. Traffic patterns change. User expectations change. Your routing strategy evolves in response. The teams that succeed are the teams that treat routing as a living system that requires regular care, regular evaluation, and regular improvement. The teams that fail are the teams that set routing rules once and never revisit them. Six months later, their rules no longer match reality, and they're burning money or delivering poor quality without realizing it.

Caching is the next control layer — storing and reusing responses to avoid redundant inference work entirely, which becomes the most powerful cost optimization technique available.
