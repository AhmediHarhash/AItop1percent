# 10.2 â€” Canary Deployment Mechanics: 1 Percent to 5 Percent to 25 Percent to 100 Percent

You cannot know how a change will behave in production until you run it in production. Canary deployment limits the blast radius of that uncertainty. The principle is simple: deploy the new version to a small percentage of traffic first, monitor for problems, and if everything looks good, gradually increase the percentage until all traffic runs on the new version. If problems appear, you abort and roll back before most users are affected. The name comes from the canary in the coal mine, the bird that warned miners of toxic gas before it reached lethal concentrations. Your canary deployment warns you of production issues before they reach your full user base.

The standard progression is 1 percent, 5 percent, 25 percent, 100 percent. These numbers are not arbitrary. One percent is small enough that if the deployment is catastrophically broken, only one in a hundred requests fails. Five percent is large enough to surface issues that occur in one-in-fifty scenarios. Twenty-five percent represents substantial production load, enough to reveal performance problems that do not appear at low traffic volumes. One hundred percent is full rollout. Some teams add intermediate steps: 1, 2, 5, 10, 25, 50, 100. Others skip straight from 5 to 50 if they have high confidence. The key is incremental exposure with defined checkpoints.

## Traffic Splitting Mechanisms

To run a canary deployment, you need infrastructure that can route a percentage of traffic to the new version and the rest to the old version. The simplest mechanism is random selection. For each incoming request, the load balancer generates a random number between 0 and 100. If the number is less than the canary percentage, the request goes to the canary. Otherwise, it goes to the stable version. This works and is easy to implement, but it has a weakness: user experience is inconsistent. A user might hit the canary on their first request, the stable version on their second request, and the canary again on their third. If the canary behaves differently, this creates confusion.

The more sophisticated approach is sticky routing based on user identity or session. The routing layer hashes the user ID or session token and uses the hash to decide whether that user is in the canary population. Once a user is assigned to the canary, they stay in the canary for all subsequent requests. This provides a consistent experience. If the canary has a bug, a user either sees the bug on every request or never sees it. They do not flip back and forth.

For AI systems, sticky routing is usually preferable. Model outputs can vary between versions even when both versions are working correctly. If a user asks a question, gets an answer from the canary model, then asks a follow-up and gets routed to the stable model, the follow-up answer might not align with the first answer. The user notices the inconsistency and loses trust. With sticky routing, the user gets all answers from the same model version, and the conversation flows naturally.

Sticky routing introduces a complication: what happens when you increase the canary percentage? If 1 percent of users are in the canary and you increase to 5 percent, you need to add 4 percent more users to the canary population. The existing 1 percent should stay in the canary for consistency. You do not want to randomly reassign users mid-deployment. Most routing systems handle this by expanding the hash range that maps to the canary. Users already in the canary stay there, and additional users are added until the total reaches the target percentage.

## Choosing the Canary Population

Not all traffic is equally valuable for canary testing. Ideally, your canary population represents the full diversity of production use cases. If your AI application serves both free-tier users and enterprise customers, you want both in the canary. If it handles both simple queries and complex multi-turn conversations, you want both. If it processes requests in English, Spanish, and Mandarin, you want all three languages.

Some teams deliberately bias the canary population toward lower-risk users. They send internal employees to the canary first, then free-tier users, then paying customers, then high-value enterprise accounts. This staged approach reduces the impact of bugs. If the canary breaks, you catch it before it reaches your most important users. The downside is that internal employees and free-tier users may not exercise the system the same way enterprise customers do. You might miss bugs that only appear under enterprise usage patterns.

Other teams bias the canary toward higher-risk traffic. They send complex requests to the canary because those are the scenarios most likely to reveal problems. They send traffic from regions with strict compliance requirements because those are the deployments where mistakes are most costly. This approach finds problems faster but increases the blast radius if something goes wrong.

The most common strategy is random sampling from the full production population. One percent of all users, chosen randomly, go to the canary. This ensures that the canary population is representative without introducing bias. The trade-off is that you cannot control which users are affected if the canary fails. A high-value customer might be in that 1 percent, and if the canary is broken, they experience the failure.

## Monitoring Canary Metrics

During the canary deployment, you monitor the same metrics you would monitor in any production deployment: error rate, latency, throughput, resource utilization. The difference is that you compare canary metrics to stable version metrics continuously. If the canary's error rate is higher than the stable version's error rate by a statistically significant margin, something is wrong. If the canary's 95th percentile latency is 200 milliseconds higher than the stable version's, something is wrong.

For AI systems, you also monitor model-specific metrics. You compare output quality metrics: refusal rate, hallucination rate, policy violation rate. You compare output characteristics: average response length, token count, reasoning step count. If the canary model is refusing 8 percent of requests and the stable model is refusing 3 percent, you investigate. If the canary's responses are 40 percent longer on average, you investigate. These differences might be acceptable if you deliberately changed the model to be more cautious or more verbose, but if you did not expect the change, it signals a problem.

You also monitor user-facing signals. If your application collects thumbs-up and thumbs-down feedback, you compare feedback rates between canary and stable. If the canary has a thumbs-down rate 2x higher than stable, the model is producing outputs users dislike. If your application tracks task success, you compare success rates. If users in the canary abandon sessions 20 percent more often than users on stable, something about the canary experience is driving them away.

The key is automated comparison. You cannot manually watch dashboards for hours during every deployment. You need alerting that triggers when canary metrics deviate from stable metrics by more than a threshold. If canary error rate exceeds stable error rate by 2 percentage points, alert. If canary latency exceeds stable latency by 100 milliseconds, alert. These thresholds depend on your system's baseline metrics and your tolerance for risk.

## Advancement Criteria

At each canary percentage, you hold for a defined soak period. At 1 percent, you might hold for 30 minutes. At 5 percent, you might hold for an hour. At 25 percent, you might hold for two hours. The soak period gives you time to observe metrics and detect issues that do not appear immediately. Some bugs only manifest after the system has been running for a while, after caches warm up or after specific traffic patterns arrive.

To advance from one percentage to the next, you verify that canary metrics meet success criteria. Error rate must be within acceptable bounds. Latency must be within acceptable bounds. Model quality metrics must be within acceptable bounds. If any metric violates a threshold, the deployment pauses. A human reviews the data and decides whether the violation is acceptable or whether the deployment should be aborted.

Some teams automate the advancement decision. If canary metrics pass all thresholds for the duration of the soak period, the system automatically increases the traffic percentage to the next level. This works well for low-risk deployments and for teams with high confidence in their metrics and thresholds. The risk is that automated systems can miss issues that a human would catch. If your canary's error rate is technically within thresholds but the errors are all concentrated in one critical use case, an automated system might advance the deployment while a human would stop it.

Other teams require manual approval at each stage. After the soak period, an engineer reviews the metrics, looks at sample outputs, checks for anomalies, and approves advancing to the next percentage. This is slower but safer. The trade-off is that it requires an engineer to be available and paying attention during the entire deployment, which can take hours.

## Abort Criteria

If the canary exhibits unacceptable behavior, the deployment aborts. Traffic percentage does not increase further, and the percentage already on the canary rolls back to the stable version. Abort criteria are typically the inverse of advancement criteria. If error rate exceeds stable by more than a threshold, abort. If latency exceeds stable by more than a threshold, abort. If model quality metrics degrade beyond acceptable bounds, abort.

Some abort criteria are absolute rather than relative. If canary error rate exceeds 5 percent, abort, regardless of what the stable version's error rate is. If canary latency exceeds 2000 milliseconds at the 99th percentile, abort. These absolute thresholds catch catastrophic failures even if the stable version also has high error rates or latencies.

For AI systems, you also define qualitative abort criteria. If a human reviewer testing the canary model observes that it is generating harmful content, you abort. If you discover that the canary is violating a policy that the stable version respects, you abort. These criteria cannot be fully automated because they require judgment about whether an output is acceptable.

Aborting a canary deployment is less disruptive than rolling back a full deployment. If 5 percent of traffic is on the canary and you abort, only 5 percent of users experience the rollback. The other 95 percent never saw the new version. You lose the work you put into the deployment, but you avoid widespread impact.

## Soak Time and Statistical Confidence

The duration of each soak period depends on your traffic volume and the types of issues you want to detect. If your system handles 10,000 requests per minute and you deploy a canary at 1 percent, the canary receives 100 requests per minute. After 30 minutes, it has processed 3,000 requests. If your typical error rate is 0.1 percent, you expect 3 errors in those 3,000 requests. If you see 10 errors, the error rate is 3x higher than expected, which is probably a problem. If you see 5 errors, it is 1.5x higher, which might be statistical noise or might be a real issue.

Low traffic volumes require longer soak times to achieve statistical confidence. If your system handles 100 requests per minute and your canary is at 1 percent, the canary sees 1 request per minute. After 30 minutes, it has seen 30 requests. You cannot draw meaningful conclusions from 30 requests. You need to increase the soak time to several hours or increase the canary percentage to get enough sample size.

For AI systems, some failure modes are rare. A model might hallucinate a harmful instruction once in every 10,000 requests. If your canary is at 1 percent and runs for an hour, it might not encounter the specific input that triggers the hallucination. You would advance the canary to 5 percent, then 25 percent, then 100 percent without ever seeing the issue. Then, when the deployment is complete, a user hits the rare input and the model fails. This is an inherent limitation of canary deployments: you can only detect issues that appear frequently enough to show up in your sample size during your soak time.

To mitigate this, some teams run extended canaries. Instead of advancing to 100 percent after a few hours, they hold at 25 percent for days or even weeks. This gives the canary time to encounter rare inputs. The trade-off is deployment velocity. If every deployment requires a week-long canary, you can only deploy once a week. For fast-moving teams, this is unacceptable. The compromise is to use canary deployment for high-risk changes and faster deployment patterns for low-risk changes.

## Automated Canary Analysis

Manually monitoring canary metrics for hours is tedious and error-prone. Automated canary analysis tools compare canary and stable metrics, compute statistical significance, and make advancement or abort decisions based on rules. These tools integrate with your monitoring infrastructure. They query metrics for both canary and stable populations, calculate differences, and trigger alerts or actions when thresholds are crossed.

The simplest automated analysis is threshold-based. If canary error rate exceeds stable error rate by more than X, abort. If canary latency exceeds stable latency by more than Y milliseconds, abort. These thresholds are fixed and defined before the deployment starts. The advantage is simplicity. The disadvantage is that fixed thresholds do not account for baseline variability. If your stable version's error rate fluctuates between 0.5 percent and 1.5 percent throughout the day, a fixed threshold of 1 percentage point might trigger false alerts when canary is at 1.4 percent and stable happens to be at 0.6 percent at that moment.

More sophisticated systems use statistical tests. They compute confidence intervals around the difference between canary and stable metrics. If the 95 percent confidence interval for the difference excludes zero, the difference is statistically significant and the system takes action. This approach accounts for baseline variability and reduces false positives. The disadvantage is complexity. Statistical tests require larger sample sizes, which means longer soak times or higher traffic volumes.

Some platforms offer out-of-the-box canary analysis. Cloud providers like AWS, GCP, and Azure have deployment tools that can automatically monitor canary metrics, compare them to stable, and roll back if anomalies are detected. These tools work well for standard metrics like error rate and latency. They work less well for AI-specific metrics like hallucination rate or policy violation rate unless you instrument those metrics in a way the platform understands.

## Canary for Model Changes

When the deployment involves changing a model rather than changing application code, canary deployment takes on additional nuance. Model changes affect output behavior in ways that code changes usually do not. A code bug produces an error or incorrect result. A model change produces a different result that might be better, worse, or just different. Deciding whether a different result is acceptable requires human judgment, not just metrics.

You can automate some of this judgment by using eval suites. During the canary, you run your eval suite against both canary and stable models on the same set of inputs. You compare pass rates across dimensions: accuracy, hallucination, policy compliance, tone, refusal rate. If the canary model's pass rate on any dimension drops below a threshold, you investigate. This works well for catching regressions but does not catch every issue. Eval suites are never comprehensive. They miss edge cases.

You also compare user feedback. If users are rating canary outputs lower than stable outputs, the canary model is producing something users dislike. This signal is delayed. Users do not rate every output, and they do not rate immediately. You might need hours or days of data to see a statistically significant difference in feedback rates. For deployments where you want to advance from 1 percent to 5 percent in 30 minutes, user feedback is too slow to be actionable at early stages.

Another approach is to route challenging or high-risk requests to human review during the canary. If the canary model produces an output that you want to verify before showing it to a user, you send it to a human reviewer first. The reviewer approves or rejects the output. If the rejection rate is high, you abort the canary. This is expensive and does not scale to high traffic volumes, but it provides high confidence for critical deployments.

Rolling deployment offers yet another approach for systems where canary's incremental exposure is too slow or where you need updates to propagate across many instances without maintaining multiple traffic populations.

