# 6.3 — Semantic Caching: Embedding-Based Similarity Matching

In early 2025, a customer support team at a retail company deployed exact-match response caching and achieved a three percent hit rate. They analyzed the logs and discovered that users asked the same thirty questions in over two hundred different phrasings. "How do I return an item" appeared as "how can I send something back," "what is your return policy," "I need to return a product," and fifty other variations. The responses were nearly identical. The cache was useless because it demanded exact string matches. The team needed semantic caching—matching queries by meaning, not by text—and when they deployed it, hit rate jumped to thirty-eight percent.

Semantic caching is the answer to query diversity. Users do not speak like SQL queries with consistent syntax. They paraphrase, abbreviate, and vary phrasing based on context, mood, and typing speed. Exact-match caching treats "how do I reset my password" and "how can I reset my password" as different queries. Semantic caching recognizes them as identical and serves the same cached response. The trade-off is complexity—you need embedding models, vector search infrastructure, and similarity threshold tuning—and risk—you can serve the wrong cached response if similarity matching fails.

## The Semantic Caching Concept

Semantic caching generates a vector embedding for each query, then searches for cached responses with similar embeddings. The embedding is a dense vector—typically 384 to 1536 dimensions—that represents the semantic meaning of the text. Queries with similar meanings produce similar embeddings. You measure similarity using cosine distance or Euclidean distance. If the distance is below a threshold, you consider the queries equivalent and return the cached response. If the distance exceeds the threshold, you treat it as a cache miss and generate a new response.

The advantage is that semantic caching captures variations that exact-match caching misses. "What is your refund policy" and "how do refunds work" have completely different text but nearly identical embeddings. Semantic caching serves the same response to both. This increases hit rate substantially for domains where users ask repetitive questions in diverse language—customer support, FAQ systems, documentation search.

The disadvantage is false positives. Semantic similarity is imperfect. "How do I delete my account" and "How do I access my account" are semantically similar—both involve account operations—but require different responses. If your similarity threshold is too loose, you serve the deletion instructions to someone trying to log in. The result is user confusion, support escalations, and loss of trust. False positives are the failure mode of semantic caching, and tuning the threshold is the hardest operational challenge.

The second disadvantage is latency. Generating an embedding adds five to fifty milliseconds depending on embedding model size and infrastructure. Searching for similar embeddings in a vector database adds another five to twenty milliseconds. Total latency for a semantic cache hit is twenty to eighty milliseconds, compared to two to eight milliseconds for exact-match caching. This is still faster than full inference—which takes hundreds of milliseconds—but the latency overhead is measurable and impacts user experience on fast queries.

## Embedding Generation for Queries

The embedding model converts query text into a vector. The vector encodes semantic meaning—words with similar meanings are close in vector space, words with different meanings are far apart. The quality of the embedding determines the quality of semantic caching. A poor embedding model produces embeddings where unrelated queries appear similar and related queries appear different. A strong embedding model accurately clusters semantically equivalent queries.

The choice of embedding model matters. Small models like all-MiniLM-L6-v2 generate 384-dimensional embeddings and run in under ten milliseconds on CPU. They work well for short queries—single sentences, questions—and are the default for cost-sensitive deployments. Larger models like OpenAI text-embedding-3-large generate 1536-dimensional embeddings with higher accuracy but require twenty to fifty milliseconds and cost more per query. The trade-off is accuracy versus latency and cost.

Domain-specific models outperform general-purpose models if you are caching within a narrow domain. A model fine-tuned on medical text produces better embeddings for healthcare queries than a general-purpose model. A model fine-tuned on legal text produces better embeddings for contract questions. If your queries are concentrated in a specific domain and you have training data, fine-tuning an embedding model improves cache hit rate and reduces false positives. Most teams start with general-purpose models and fine-tune only if hit rate or false positive rate is unacceptable.

The embedding generation happens at cache write and cache lookup. When you generate a response and store it in cache, you also generate an embedding for the query and store it alongside the response. When a new query arrives, you generate an embedding for it and search the cache for similar embeddings. Both operations must use the same embedding model and the same normalization pipeline. If you switch embedding models, you must regenerate embeddings for all cached entries or risk incorrect similarity scores.

Normalization before embedding improves consistency. You lowercase the query, strip punctuation that does not affect meaning, remove leading and trailing whitespace, and normalize Unicode. The goal is to ensure that "How do I reset my password?" and "how do i reset my password" produce identical embeddings. Most embedding models are trained on clean text and produce better embeddings when input is normalized. The risk of over-normalization still applies—removing punctuation changes meaning in some cases—so normalize conservatively.

## Similarity Thresholds

The similarity threshold determines when two embeddings are close enough to share a cached response. Set the threshold too high—requiring very similar embeddings—and you get few cache hits because minor phrasing differences produce embeddings just outside the threshold. Set the threshold too low—allowing distant embeddings—and you get false positives where semantically different queries retrieve the same cached response.

Cosine similarity ranges from negative one to one, with one meaning identical vectors and zero meaning orthogonal vectors. Typical thresholds for semantic caching fall between 0.85 and 0.95. A threshold of 0.90 means you return a cached response if cosine similarity is 0.90 or higher. Below 0.90, you treat it as a cache miss. The exact value depends on embedding model quality, query diversity, and false positive tolerance.

Euclidean distance is an alternative metric. It measures the straight-line distance between two vectors. Lower distances indicate more similar embeddings. Typical thresholds range from 0.3 to 0.8 depending on embedding dimensionality and normalization. Cosine similarity is more common because it is invariant to vector magnitude, making it more robust to embedding scale differences.

Threshold tuning requires data. You cannot set an optimal threshold without measuring false positive rate and hit rate across a range of values. The process is to log every semantic cache hit along with the query, the cached query it matched, and the similarity score. You also log user behavior after the cache hit—did they rephrase the query, did they rate the response as unhelpful, did they escalate to a human agent? Queries followed by rephrasing are likely false positives. You count false positives and cache hits for each threshold value, then choose the threshold that maximizes hit rate while keeping false positive rate below your tolerance—typically one to three percent of cache hits.

The challenge is that optimal thresholds vary by query type. FAQ queries tolerate looser thresholds because responses are designed to cover multiple phrasings. Troubleshooting queries need tighter thresholds because small phrasing differences indicate different problems. A single global threshold is a compromise that underperforms for both. The mature approach classifies queries by type—FAQ, troubleshooting, account management, general knowledge—and sets per-type thresholds. This requires classification logic, which adds complexity but improves both hit rate and correctness.

## The False Positive Problem

False positives are the risk that makes semantic caching dangerous. A false positive occurs when the cache returns a response that does not answer the user's actual question. The user asked A, the cache returned the response for B, and the user is now confused or misled. In low-stakes domains—general knowledge, casual conversation—false positives are annoying. In high-stakes domains—healthcare advice, financial transactions, legal guidance—false positives are dangerous.

The causes of false positives are embedding limitations and threshold misconfiguration. Embedding models compress language into fixed-size vectors, losing nuance. "I want to close my account" and "I don't want to close my account" have similar embeddings because they share most words and discuss the same topic, but they express opposite intents. A loose similarity threshold treats them as equivalent and serves the wrong response. The result is a user trying to keep their account open receiving instructions for account closure.

Detection requires monitoring user behavior after cache hits. You track whether users immediately rephrase queries, whether they provide negative feedback, whether they abandon the session, or whether they escalate to a human agent. Clusters of cache hits followed by these behaviors indicate false positives. You log the query, the matched cached query, and the similarity score for every suspected false positive. You review these logs weekly or daily depending on traffic volume.

Mitigation has three layers. First, raise the similarity threshold for query types with high false positive rates. If account management queries show five percent false positive rate, increase the threshold from 0.90 to 0.93 for that category. Second, add negative examples to exclude known bad matches. If you discover that "close my account" is frequently matched incorrectly to "access my account," you blacklist that pair—even if similarity is high, you force a cache miss. Third, run a secondary validation check. Before serving a cached response, you verify that the response actually answers the query using a lightweight relevance model. This adds latency but eliminates most false positives.

The hardest false positives are the ones users never report. A user receives an irrelevant cached response, assumes the system is incompetent, and leaves. They do not file a bug report. They do not rephrase the query. They simply stop using your product. These silent false positives are invisible in logs unless you track session abandonment rate after cache hits. If users abandon sessions more frequently after semantic cache hits than after fresh responses, you have undetected false positives.

## Vector Search Infrastructure

Semantic caching requires a vector database to store embeddings and search for similar ones. The database must support high-throughput approximate nearest neighbor search—finding the closest embedding to a query embedding in milliseconds, even with millions of cached entries. Exact nearest neighbor search is too slow for production. You need approximate algorithms like HNSW, IVF, or ScaNN that trade slight accuracy loss for massive speed gains.

Pinecone is a managed vector database designed for this use case. You insert embeddings with metadata—the query text, the cached response, the timestamp—and query for the nearest neighbors to a new embedding. Pinecone handles indexing, scaling, and query optimization. The latency is ten to thirty milliseconds for most queries. The cost scales with index size and query volume, making it viable for systems with thousands to millions of cached entries.

Weaviate is an open-source alternative with similar capabilities. You deploy it on your own infrastructure, giving you control over cost and data residency. The trade-off is operational overhead—you manage the deployment, scaling, and monitoring. Weaviate supports multiple distance metrics, hybrid search combining vector and keyword matching, and complex filtering on metadata. It is the choice for teams that need flexibility or have regulatory requirements preventing managed services.

Qdrant is another open-source vector database optimized for performance. It provides single-digit millisecond query latency for smaller indexes—under one million vectors—and scales horizontally for larger ones. Qdrant supports on-disk storage for cost efficiency and in-memory storage for maximum speed. The API is straightforward—insert vectors, search by similarity, filter by metadata. The operational complexity is lower than Weaviate but higher than Pinecone.

pgvector is a PostgreSQL extension that adds vector search to an existing Postgres database. If you already run Postgres, pgvector lets you store embeddings alongside your relational data without adding a new database. The performance is acceptable for small to medium indexes—under one hundred thousand vectors—but degrades for larger scales. The advantage is simplicity—no new infrastructure, no data synchronization between databases. The disadvantage is limited scalability and slower query times compared to purpose-built vector databases.

The choice depends on scale, cost tolerance, and operational capacity. Pinecone is the fastest path to production for most teams. Weaviate or Qdrant is better for teams with scaling ambitions or regulatory constraints. pgvector is best for small-scale deployments or proof-of-concept work. All of them support the core operation: given a query embedding, find the cached embeddings with similarity above a threshold, and return the associated responses.

## Performance Considerations

Semantic caching adds latency at every step. Generating the query embedding takes five to fifty milliseconds depending on embedding model size. Searching the vector database takes five to thirty milliseconds depending on index size and infrastructure. Total latency for a semantic cache hit is twenty to eighty milliseconds. This is significantly slower than exact-match caching, which takes two to eight milliseconds.

The trade-off is whether the increased hit rate justifies the increased latency. If exact-match caching achieves five percent hit rate and semantic caching achieves thirty percent hit rate, you are trading faster cache lookups for more cache hits. The net effect on user-perceived latency depends on the ratio of cache hits to misses and the latency difference between cache hit and full inference. For most systems, the hit rate improvement outweighs the latency penalty because full inference takes hundreds of milliseconds—much longer than the semantic cache overhead.

The embedding generation latency can be reduced by caching embeddings themselves. When a user submits a query, you generate the embedding and search the vector database. If you find a match, you serve the cached response. If you miss, you generate a fresh response, store it, and also store the query embedding for future lookups. But if the same user submits the same query again later, you do not regenerate the embedding—you retrieve it from an exact-match cache keyed on the query text. This two-layer caching strategy eliminates redundant embedding generation for repeated queries.

The vector search latency can be reduced by sharding the index. If you have ten million cached entries, searching a single index is slow. Sharding splits the index into multiple partitions—perhaps by query category, by time range, or by hash. Each shard contains fewer entries and supports faster search. The trade-off is that you must search multiple shards if you do not know which shard contains the relevant cached entries, which can increase latency if done serially. The solution is to search shards in parallel or use metadata to route queries to specific shards.

The infrastructure cost is higher than exact-match caching. Vector databases require more memory and compute than key-value stores. Embeddings are large—384 to 1536 dimensions per entry—and indexes require additional overhead for fast search. A vector database storing one million embeddings consumes several gigabytes of memory. The cost is worthwhile if semantic caching increases hit rate enough to offset infrastructure spend through inference cost savings. Most teams find that semantic caching pays for itself within weeks if hit rate improves by ten percentage points or more.

## Hybrid Approaches

The most effective production systems use hybrid caching: exact-match first, semantic fallback second. When a query arrives, you compute the cache key and check the exact-match cache. If you find an entry, you return it immediately with two to eight milliseconds latency. If you miss, you generate an embedding and search the vector database for semantically similar queries. If you find a match above the similarity threshold, you return the cached response with twenty to eighty milliseconds latency. If you miss again, you perform full inference and store the result in both caches.

The advantage is that you get the speed of exact-match caching for repeated queries and the coverage of semantic caching for paraphrased queries. The hottest queries—the ones repeated thousands of times per day in identical form—hit the exact-match cache and return in single-digit milliseconds. The common queries phrased differently hit the semantic cache and return in tens of milliseconds. The long-tail queries miss both caches and trigger inference.

The implementation is straightforward. Both caches use the same TTL and invalidation logic. When you generate a fresh response, you store it in both the exact-match cache—keyed on query text—and the semantic cache—keyed on query embedding. When you invalidate cache due to system prompt change or model update, you flush both caches. The operational complexity is higher because you maintain two cache systems, but the performance and hit rate gains are significant.

The hybrid approach also supports cache warm-up more effectively. You can pre-populate the exact-match cache with common queries and their responses, giving instant hits for known frequent queries. The semantic cache populates organically as traffic arrives, capturing paraphrased versions. Over time, the semantic cache becomes a long-term store of all historical queries, while the exact-match cache remains a fast short-term store of recent hot queries.

## Quality Monitoring

Monitoring semantic cache quality requires tracking false positive rate, false negative rate, and hit rate by query category. False positive rate measures how often the cache serves an incorrect response. False negative rate measures how often the cache misses queries that should have hit—queries that are semantically similar to cached entries but fell below the similarity threshold. Hit rate measures what percentage of queries are served from semantic cache.

False positive detection relies on user feedback signals. You log every semantic cache hit along with the similarity score. You track whether users rephrase queries, provide negative ratings, or escalate to human support after cache hits. High rates of these behaviors indicate false positives. You compute false positive rate as the percentage of cache hits followed by negative feedback. The acceptable rate depends on domain—one percent for high-stakes applications, three to five percent for low-stakes ones.

False negative detection requires manual review. You sample queries that missed the semantic cache and compare them to cached entries. If you find queries that are semantically equivalent but missed due to a too-strict threshold, those are false negatives. The review process is labor-intensive—requires human judgment to assess semantic equivalence—but essential for threshold tuning. You perform it weekly or monthly depending on traffic volume and system maturity.

Hit rate by query category reveals which types of queries benefit most from semantic caching. FAQ queries typically show the highest hit rates because users ask the same small set of questions in many phrasings. Troubleshooting queries show lower hit rates because problems are diverse and context-specific. General knowledge queries fall in between. You optimize caching strategy per category—tighten thresholds for low-hit-rate categories to reduce false positives, loosen thresholds for high-hit-rate categories to capture more paraphrasing.

The monitoring dashboard surfaces these metrics in real time. You track hit rate, false positive rate, and latency percentiles continuously. You alert when false positive rate exceeds threshold, indicating a need for threshold adjustment or negative example addition. You alert when hit rate drops significantly, indicating traffic shift or cache invalidation. The goal is to maintain high hit rate with low false positive rate, and the monitoring system tells you when those goals are drifting.

Semantic caching bridges the gap between exact-match caching and the diversity of human language. But even semantic caching cannot capture the biggest optimization in LLM inference—reusing the KV-cache across requests to eliminate redundant computation entirely.
