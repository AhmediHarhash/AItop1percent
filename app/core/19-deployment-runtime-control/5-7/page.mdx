# 5.7 — Request Splitting for A/B Tests and Experiments

How do you know the new prompt is better if you never test it in production? You do not. Offline evaluation tells you that the new prompt scores higher on your eval suite, but eval suites are approximations. They capture known failure modes, representative scenarios, and historical patterns. They do not capture the full distribution of production queries, the edge cases you have not imagined yet, or the interactions between your prompt and real user behavior. The only way to know if a change improves the real system is to run it against real traffic and measure real outcomes.

Request splitting is the infrastructure that makes production experimentation safe. You route a controlled percentage of production traffic to a variant—a new prompt, a different model, an experimental routing rule—while the rest of traffic continues using the baseline. You measure quality, latency, cost, and user behavior for both populations. If the variant performs better, you increase its traffic percentage gradually until it becomes the new baseline. If the variant performs worse, you end the experiment immediately and all traffic returns to the proven baseline. This is how production AI systems improve continuously without taking unnecessary risks.

## Request Splitting Infrastructure

A system with request splitting capability maintains **variant configurations** for each component that might be experimented on. Your prompt management system does not just store the production prompt. It stores the production prompt plus any experimental variants currently being tested. Each variant has a name, a traffic allocation percentage, and metadata about what is being tested and why. When a request arrives, the routing layer decides which variant to use based on traffic splitting rules, applies the appropriate configuration, and tracks which variant handled the request.

The splitting decision happens **before the request is processed**. The system cannot evaluate the request with the baseline prompt, decide it was good, and then evaluate it again with the variant prompt. That would be cost-prohibitive and would not reflect real user experience. Instead, the system commits to a variant immediately and processes the request exactly once. This means traffic splitting must be **deterministic per user or session**. If a user's first query goes to the variant, their subsequent queries in the same session should also go to the variant. Mixing variants within a single user session creates inconsistent experience and pollutes experimental results.

**User bucketing** is the most common approach. Each user is assigned to a bucket based on a hash of their user ID. If you are running an experiment with five percent traffic to the variant, buckets zero through four go to the variant and buckets five through ninety-nine go to the baseline. The bucketing assignment is stable: the same user always goes to the same bucket, ensuring consistency across their session and across multiple visits. Some systems recalculate buckets daily to prevent long-term bias from sticky assignments. Others maintain bucket assignments indefinitely for repeatability.

**Session bucketing** handles anonymous users or scenarios where user IDs are not available. Each session is assigned to a bucket based on a hash of the session ID. Anonymous users browsing an FAQ bot get consistent experience within a session but may be bucketed differently in their next session. Session bucketing provides less statistical power than user bucketing because the same human may appear in both experiment and control groups across different sessions, but it is the only option for systems without persistent user identity.

The routing layer must **track variant assignments** and pass them through the entire request lifecycle. When a request is logged, the log entry includes which variant was used. When metrics are collected, they are tagged with the variant. When quality checks run, they record the variant. When results are returned to the user, the response metadata includes the variant so that downstream systems can analyze behavior by variant. Without consistent variant tagging, experimental results are meaningless because you cannot attribute outcomes to variants.

## A/B Test Infrastructure

An A/B test compares two variants: the baseline and a single experimental variant. The baseline is labeled A, the variant is labeled B. The test measures whether B is better than A on one or more metrics. The infrastructure must support defining metrics, collecting measurements, analyzing statistical significance, and deciding when to end the test.

**Metric definition** happens before the experiment starts. You cannot decide after the experiment ends which metrics matter. That introduces bias. The most common metrics are quality scores from your eval pipeline, user engagement signals like click-through rates or task completion, latency measurements, and cost per request. Each metric has a directionality: higher is better or lower is better. Some teams define a **primary metric** that determines success and several **secondary metrics** that provide additional context. The primary metric might be task success rate. Secondary metrics might include latency, user satisfaction ratings, and cost.

**Sample size calculation** determines how long the experiment must run. If you allocate five percent of traffic to the variant and your system handles one million requests per day, the variant receives fifty thousand requests per day. If you need one hundred thousand samples to reach statistical significance, the experiment runs for two days. If you need one million samples, the experiment runs for twenty days. Sample size depends on the expected effect size, the baseline variance, and the confidence level you require. Larger effect sizes reach significance faster. Higher variance requires more samples. Higher confidence requires more samples.

The system must **collect metrics continuously** during the experiment. Every request to the variant is evaluated, scored, and logged. Every request to the baseline is evaluated, scored, and logged. At any moment, you should be able to query the current state: how many samples have been collected, what are the mean and variance for each metric in each group, and what is the current significance level. Some teams run statistical analysis in real time and update dashboards every minute. Others batch analysis and update daily.

**Statistical significance testing** determines whether observed differences are real or random noise. The most common approach is a t-test comparing the means of the two groups. If the p-value is below 0.05, the difference is statistically significant at the 95 percent confidence level. If the p-value is below 0.01, the difference is significant at the 99 percent confidence level. Some teams require both statistical significance and **practical significance**: the improvement must be large enough to matter. A prompt variant that improves quality by 0.1 percent might be statistically significant with enough samples but too small to justify the operational complexity of maintaining two prompt variants.

**Guardrail metrics** protect against regressions. Even if the variant improves the primary metric, if it degrades a critical secondary metric, the experiment should end immediately. A prompt that increases task success rate by five percent but doubles latency is not acceptable. A model that improves quality but increases cost by 80 percent may not be economically viable. Guardrail metrics have hard thresholds: if latency exceeds four seconds, if cost per request exceeds ten cents, if any safety classifier fires at more than 0.1 percent rate, end the experiment immediately.

## Experiment Isolation and Contamination Prevention

Experiments must be **isolated from each other**. If you run two experiments simultaneously—one testing a new prompt and one testing a new model—you cannot allocate overlapping traffic. If the same user receives both the new prompt and the new model, you cannot determine which change caused the observed outcome. The bucketing system must ensure that experiments either use completely separate traffic or are explicitly nested.

**Nested experiments** are possible when testing changes at different layers. You might run an experiment on prompt variants at 10 percent traffic and a separate experiment on model routing rules at 20 percent traffic. The 10 percent prompt experiment traffic is subdivided: half uses baseline routing, half uses variant routing. This allows testing both changes simultaneously, but analysis becomes more complex because you must account for interactions between the two experiments.

**Holdout groups** provide long-term comparison. A holdout group is a small percentage of traffic—usually one to five percent—that never receives experimental variants. The holdout always gets the baseline configuration as it was at the start of a measurement period. Over months, you run dozens of experiments, each improving some metric. But did the cumulative effect of all experiments actually improve the system? The holdout group provides ground truth. If the holdout group's outcomes are similar to the current production group's outcomes despite dozens of experiments, your experiments are not actually moving metrics. This is a sobering signal that prevents teams from chasing noise.

**Experiment spillover** happens when variant behavior affects baseline behavior. If you are testing a new model on customer support queries and the new model escalates to human operators at a higher rate, those human operators become busier. If human operator busyness affects their response time, the baseline group may experience slower human escalation even though they are not using the new model. Spillover is rare but can completely invalidate experimental results. The only robust defense is detecting it through instrumentation and ending experiments where spillover is observed.

Some teams maintain **experiment registries** that track all active experiments, their traffic allocations, their metrics, and their expected end dates. The registry prevents accidental overlaps, provides visibility into current experimental load, and ensures that experiments are evaluated and concluded rather than running indefinitely. An experiment that runs for six months without reaching significance or being terminated suggests poor experimental design or lack of follow-through.

## Gradual Rollout via Traffic Splitting

Once an experiment demonstrates that the variant is better than the baseline, the variant becomes the new candidate for full rollout. But immediately switching 100 percent of traffic from baseline to variant is risky. Even though the experiment showed positive results on a small sample, edge cases might only appear at higher traffic volumes. The safe approach is **gradual rollout**: incrementally increase variant traffic while monitoring for regressions.

A typical rollout schedule might go: five percent for two days, 10 percent for two days, 25 percent for one day, 50 percent for one day, 100 percent. At each stage, quality metrics, latency, error rates, and user engagement are monitored. If any metric degrades beyond acceptable thresholds, the rollout pauses or rolls back. The incremental approach ensures that even if something goes wrong, only a fraction of users are affected.

**Automated rollout systems** advance traffic percentages automatically based on metric thresholds. If the variant maintains quality within two percent of baseline, latency within 10 percent of baseline, and error rates below 0.5 percent, the system increases traffic allocation automatically every few hours. If any threshold is violated, automated rollout stops and alerts fire. Manual intervention is required to investigate the issue before rollout continues. This pattern allows fast rollout when things go well and immediate halt when things go wrong.

**Rollback capability** is mandatory. If the variant at 50 percent traffic suddenly starts producing low-quality responses, the system must be able to roll back to baseline immediately. Rollback is just traffic splitting in reverse: change the variant allocation from 50 percent to zero percent. Requests that are in-flight may complete with the variant, but all new requests use the baseline. Rollback must be a single command or button press, not a 30-minute process involving configuration file edits and service restarts.

Some teams implement **automatic rollback triggers** that monitor key metrics during rollout and roll back automatically if thresholds are violated. If error rate exceeds one percent, roll back. If p95 latency exceeds six seconds, roll back. If the safety classifier fires at higher than 0.2 percent rate, roll back. Automatic rollback prevents scenarios where an engineer notices a problem but is in a meeting and cannot respond immediately. The system protects itself.

## Holdout Groups for Long-Term Validation

A **permanent holdout** is a small segment of traffic that never receives experiments. The holdout always uses the baseline configuration as it existed at a fixed point in time—often the beginning of a quarter or year. Over months, production configuration evolves through dozens of experiments. The holdout provides a comparison point: how much better is the current system than the system three months ago?

Holdout groups are particularly valuable for detecting **metric gaming**. If your team is incentivized to improve task success rate, they will run experiments that improve task success rate. But are those improvements real, or are they optimizing for the metric in ways that do not improve actual user value? If the holdout group shows that task success rate has improved by 15 percent over three months but user retention has not changed, the metric improvements are hollow. Holdouts keep teams honest.

**Holdout size** is a trade-off. A larger holdout provides more statistical power for long-term comparison but diverts more traffic away from current production configuration. Most teams use a one to three percent holdout. This is large enough to collect meaningful data over weeks but small enough that withholding the latest improvements from three percent of users is acceptable.

Holdouts must be **truly isolated**. If the holdout group is supposed to use the baseline configuration from January but accidentally receives a prompt update in March, the holdout is contaminated and long-term comparison is invalid. Configuration management must explicitly mark holdout groups and prevent them from receiving any experimental changes. Some teams treat holdout configuration as **immutable infrastructure** that can only be changed through a deliberate, audited process.

## Statistical Significance and Sample Sizes

Statistical significance is the probability that the observed difference between variant and baseline is not due to random chance. A p-value of 0.05 means there is a five percent probability that the observed difference is random noise and a 95 percent probability that the difference is real. A p-value of 0.01 means one percent chance of random noise, 99 percent chance of real difference.

Sample size determines how quickly you reach significance. If the variant improves quality by 10 percent, you need relatively few samples to detect that improvement. If the variant improves quality by 0.5 percent, you need thousands or tens of thousands of samples. The formula for required sample size involves the baseline mean, the baseline standard deviation, the expected effect size, and the desired confidence level. Online calculators and statistical libraries can compute this, but the key insight is that **smaller effect sizes require exponentially more samples**.

**Variance matters as much as mean**. If your baseline quality metric has a mean of 0.85 and a standard deviation of 0.15, individual requests vary widely in quality. Detecting a small improvement in mean quality requires many samples because the variance overwhelms the signal. If you can reduce variance—by stratifying analysis by query type or by using more stable quality metrics—you reach significance faster with fewer samples.

Some teams use **sequential testing** rather than fixed-horizon testing. Fixed-horizon testing decides in advance that the experiment will run for 10 days and then evaluate significance. Sequential testing continuously evaluates significance and stops the experiment as soon as significance is reached. Sequential testing reaches conclusions faster when effects are large but requires more sophisticated statistical methods to avoid false positives from peeking at results too often.

**Confidence intervals** provide more information than p-values. A p-value of 0.03 tells you the result is statistically significant. A confidence interval tells you the plausible range of the true effect size. If the variant improved quality with a mean improvement of 3 percent and a 95 percent confidence interval of 1.5 percent to 4.5 percent, you know the improvement is real and you know its likely magnitude. Confidence intervals prevent over-interpreting noisy results.

## Ending Experiments and Declaring Winners

An experiment should end when one of three conditions is met: the variant is clearly better, the variant is clearly worse, or the experiment has run long enough without reaching significance. The first two conditions are straightforward. The third requires discipline. If an experiment has run for four weeks and has not reached significance, continuing for another four weeks is unlikely to change the outcome. End the experiment, analyze why significance was not reached, and design a better experiment.

**Experiment conclusions** must be documented and shared. When an experiment ends, the team should produce a summary: what was tested, what metrics were measured, what the results were, what decision was made, and what was learned. This documentation serves future experiments. If someone proposes testing a similar variant two years later, the historical record shows that it was already tested and what happened. Documentation also builds organizational knowledge about what kinds of changes improve the system and what kinds do not.

Some teams maintain **experiment post-mortems** for experiments that failed to improve metrics or that caused regressions. These post-mortems are not blame-oriented. They analyze what assumptions were wrong, what signals were missed, and what the team learned. An experiment that makes quality worse is valuable if the team learns from it. An experiment that makes quality worse and teaches nothing is waste.

**Winning variants** must be cleaned up and integrated into production configuration. An experiment that succeeds by routing 20 percent of traffic to a new prompt variant should not continue routing 20 percent indefinitely. The new prompt should become the baseline, the old prompt should be deprecated, and traffic splitting should end. Configuration cruft accumulates when experiments are declared successful but never fully rolled out. Over time, the system is routing traffic to six different prompt variants based on historical experiments, and nobody remembers why.

Context-based routing extends traffic splitting from random assignment to intelligent assignment based on request characteristics, user attributes, or business logic.

