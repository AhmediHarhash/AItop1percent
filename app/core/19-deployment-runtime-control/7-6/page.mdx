# 7.6 — Flag Evaluation Engines: LaunchDarkly, Statsig, Unleash, Custom

You don't need a third-party flag service. You can build feature flags yourself in a weekend — a database table of flag states, a cache layer, an API to check flags, and some UI to toggle them. It works. Thousands of teams have done it. But you probably should use a third-party service anyway, because the weekend prototype becomes a maintenance burden within six months, and the feature flag platform you build in-house will eventually need rule targeting, gradual rollouts, analytics integration, multi-environment support, SDKs for every language, audit logs, and a dozen other capabilities that the commercial services have already built. The build versus buy decision for feature flags is the same as for any infrastructure component — build it yourself only if it's genuinely core to your differentiation, or if the commercial options don't meet your requirements. For most teams, feature flags are essential but not differentiating. That makes them a strong candidate for buying rather than building.

The feature flag market in 2026 is mature. LaunchDarkly dominates the enterprise segment. Statsig has strong traction in product-led companies that care about experimentation. Unleash offers an open-source alternative for teams that want self-hosted control. PostHog bundles feature flags with product analytics. CloudBees, Split, Optimizely, and a dozen smaller players compete for specific niches. They all solve the same core problem — evaluating flags quickly, safely, and with fine-grained control — but they differ in pricing, integration patterns, analytics capabilities, and the trade-offs they make between power and simplicity.

## LaunchDarkly: The Enterprise Standard

LaunchDarkly became the default choice for enterprise feature flag management because they focused on reliability, scale, and support for complex organizations from day one. They offer server-side SDKs for every major language, client-side SDKs for web and mobile, edge SDKs for Cloudflare Workers and similar platforms, and OpenFeature support for standardized flag evaluation. Their targeting rules support user attributes, percentage rollouts, and complex boolean logic. Their streaming architecture pushes flag updates to SDKs in near real-time. Their analytics show flag exposure and performance impact. Their audit logs track every flag change and who made it.

The LaunchDarkly evaluation model uses local SDKs that cache flag rules in memory. When your application starts, the SDK connects to LaunchDarkly's streaming service and downloads the current flag ruleset. It maintains a persistent connection and receives updates whenever a flag changes. Flag evaluation happens locally in-process with microsecond latency. There's no network call per evaluation. This makes LaunchDarkly fast enough for high-throughput request paths where remote flag lookups would add unacceptable latency.

The trade-off is complexity at large scale. If you run a thousand service instances, each one maintains its own streaming connection to LaunchDarkly. Flag updates propagate to all instances within a few seconds, but not instantly. There's a brief window where different instances might evaluate the same flag differently. For most use cases, this is fine. For use cases that require strict consistency — financial transactions, medical decisions — you need to be aware that eventual consistency is the model.

A healthcare AI platform in early 2026 used LaunchDarkly to control rollout of new diagnostic features to clinicians. They had 200 backend services, 2,000 instances at peak load, and strict requirements around feature access. They configured LaunchDarkly with targeting rules that checked clinician license status, training completion, and facility accreditation level. Flag evaluation happened in-process with no added latency. The challenge came when they needed to instantly revoke a feature from specific users after discovering a data quality issue. LaunchDarkly's streaming updates took 3 to 7 seconds to propagate to all instances. For most flags this was acceptable, but for urgent revocations, they built a secondary real-time revocation system that wrote directly to a shared cache. LaunchDarkly handled the normal rollout and targeting. The custom system handled emergency overrides.

LaunchDarkly's pricing is based on monthly active users and seats. A seat is a team member with dashboard access. Monthly active users are unique users who encounter feature flags in a billing period. For a B2B platform with ten thousand business users and a small engineering team, the cost is manageable. For a consumer product with ten million monthly actives, LaunchDarkly can cost tens of thousands per month. The pricing model favors enterprise B2B over high-volume consumer use cases.

## Statsig: Experimentation and Product Analytics

Statsig entered the market with a different thesis. They argued that feature flags, A/B testing, and product analytics should be tightly integrated, because the questions product teams ask — did this feature help, should we ship it, which variant is better — require all three. Statsig provides feature gates, which are their term for feature flags, dynamic configs for runtime configuration, and experiments for A/B testing, all with built-in statistical analysis and metric tracking.

The Statsig evaluation model is similar to LaunchDarkly's — SDKs cache rules locally and evaluate in-process. But Statsig puts more emphasis on the experimentation workflow. When you create a feature gate, Statsig prompts you to define primary and guardrail metrics. When you roll out a gate, Statsig automatically tracks those metrics for users in each variant. The analytics dashboard shows exposure counts, conversion rates, and statistical significance. If the feature improves the primary metric without degrading guardrails, Statsig recommends shipping it. If it degrades guardrails, Statsig recommends rolling back.

This integration is powerful for product teams that think in terms of experiments rather than deployments. A SaaS platform in mid-2025 used Statsig to test a new onboarding flow. They created a feature gate with two variants — control and new onboarding. They defined activation rate and day-seven retention as primary metrics. They ran the experiment for two weeks with ten percent of new signups. Statsig's dashboard showed that the new onboarding increased activation by 8 percent with 95 percent confidence but decreased day-seven retention by 3 percent with 88 percent confidence. The net effect was unclear. The product team decided to iterate on the onboarding flow rather than shipping the original variant. Without integrated analytics, they would have shipped based on the activation improvement and not discovered the retention degradation until much later.

Statsig's pricing is based on events tracked, not users. An event is a flag evaluation, a metric log, or an experiment exposure. For a product that logs metrics aggressively, this can get expensive. For a product that uses flags sparingly, it's cheaper than LaunchDarkly. The free tier is generous enough for small teams and prototypes. The enterprise tier includes dedicated support and custom contracts.

Statsig's SDKs are less mature than LaunchDarkly's in some languages. As of early 2026, their Python and JavaScript SDKs are solid, but some newer languages have community-maintained SDKs rather than official ones. For teams using a mainstream stack, this isn't an issue. For teams on less common platforms, it's a consideration.

## Unleash: Open-Source and Self-Hosted Control

Unleash is open-source feature flag software that you deploy and operate yourself. You run the Unleash server in your own infrastructure, you control the database, you manage the SDKs. This gives you full control over data residency, latency, and customization at the cost of operational responsibility. Unleash is a good fit for teams that have strong operations expertise, that have regulatory or security requirements that make third-party services difficult, or that want to avoid vendor lock-in.

The Unleash architecture separates the control plane — the API and UI for managing flags — from the evaluation plane — the SDKs that evaluate flags in your application. The control plane is a Node.js application backed by PostgreSQL. You deploy it like any other web service. The SDKs fetch flag configurations from the control plane and cache them locally. Evaluation happens in-process, just like LaunchDarkly and Statsig. You can run Unleash in a single region or replicate it globally depending on your requirements.

The trade-off is that you're responsible for uptime, scaling, backups, security patching, and monitoring. If the Unleash control plane goes down, your SDKs continue to evaluate flags from cache, but you can't change flag states until it's back up. If the Unleash database is lost, you lose your flag history and configuration. If you need to scale to a new region, you need to deploy and configure Unleash there yourself. Commercial services handle all of this. Self-hosted Unleash means you handle it.

A European fintech platform in late 2025 chose Unleash because they had strict data residency requirements. Customer data could not leave the EU, and feature flag evaluation contexts included sensitive user attributes. They deployed Unleash in their primary EU datacenter and replicated it to a secondary EU datacenter for redundancy. They integrated Unleash's audit logs into their compliance monitoring system. They assigned two engineers to maintain the Unleash infrastructure. The total cost — infrastructure plus engineering time — was lower than LaunchDarkly's enterprise pricing for their user volume. But it required ongoing operational attention that a commercial service would have abstracted away.

Unleash also offers a hosted version called Unleash Cloud, which combines the open-source software with managed hosting. This gives you the Unleash feature set without the operational burden. The pricing is similar to other commercial services. For teams that like Unleash's approach but don't want to self-host, this is a reasonable middle ground.

## PostHog: Feature Flags as Part of Product Analytics

PostHog is an open-source product analytics platform that added feature flags as one of several integrated capabilities. PostHog's feature flags use the same user identification and event tracking that powers their analytics, session replay, and cohort analysis. The integration is seamless — you identify a user once, and that identity is available for flag evaluation, analytics segmentation, and session replay filtering.

PostHog's flag evaluation architecture is similar to others — SDKs cache flag rules and evaluate locally. The difference is that PostHog also captures every flag evaluation as an event in their analytics pipeline. This means you can answer questions like "what percentage of users who saw variant A completed the onboarding flow" without manually logging metrics. The flag evaluation and the metric tracking are the same action.

The downside is that PostHog's feature flag capabilities are less mature than dedicated flag platforms. Targeting rules are simpler. Multi-variate experiments are supported but less polished. Real-time streaming updates are not available — flag changes take up to 30 seconds to propagate. For teams that already use PostHog for analytics and want basic feature flag functionality, it's a convenient addition. For teams that need sophisticated flag targeting and real-time control, dedicated platforms are stronger.

A startup building a B2B collaboration tool in early 2026 used PostHog for both analytics and feature flags. They had a small team, limited budget, and wanted to minimize the number of vendors. PostHog's combined platform let them test new features, measure impact, and watch session replays of users interacting with those features, all from one dashboard. As they grew, they found PostHog's flag capabilities limiting — they wanted gradual rollouts with dynamic percentage adjustments, which PostHog supported, but also wanted conditional rules based on computed attributes, which PostHog did not. They eventually migrated feature flags to Statsig while keeping PostHog for analytics. The migration took two weeks and required refactoring flag checks across their codebase.

## CloudBees, Split, Optimizely, and Other Players

CloudBees acquired Rollout in 2019 and rebranded it as CloudBees Feature Management. It's positioned as part of CloudBees' broader CI/CD platform. If you're already a CloudBees customer for Jenkins or their software delivery platform, their feature flag offering integrates naturally. If you're not, there's limited reason to choose CloudBees over LaunchDarkly or Statsig unless you're committed to the broader ecosystem.

Split emphasizes the experimentation side of feature flags. Their platform treats every flag as a potential experiment and integrates statistical analysis deeply into the workflow. They're similar to Statsig in positioning but arrived earlier and have stronger penetration in traditional enterprises. Split's pricing is higher than Statsig's for comparable usage. Their SDKs are mature and their reliability is strong. For teams with budget and a focus on rigorous experimentation, Split is a credible alternative to Statsig.

Optimizely started as an A/B testing platform and added feature flags as a natural extension. Their feature flagging capabilities are solid, but the product still feels anchored in the experimentation world rather than the deployment control world. Optimizely is a good fit if experimentation is your primary use case and feature flags are secondary. If feature flags are your primary need, other platforms are more focused.

Smaller players include GrowthBook, an open-source A/B testing and feature flagging platform; Flagsmith, another open-source option similar to Unleash; DevCycle, a newer entrant focused on developer experience; and ConfigCat, which emphasizes simplicity and low pricing. All of these work. None have the maturity, scale, or ecosystem of the top three platforms. They're worth considering if you have specific requirements that the major platforms don't meet, or if you want to avoid the market leaders for strategic reasons.

## Custom Flag Implementations: When and Why

Some teams build their own feature flag systems. This makes sense in a few scenarios. First, when your requirements are genuinely unusual. You need flag evaluation to happen at the database query level, not the application level. You need sub-millisecond flag propagation across ten thousand instances. You need flag rules that depend on proprietary data sources that can't be exposed to third-party services. Second, when you're large enough that the cost of a commercial service exceeds the cost of building and maintaining your own. A consumer app with 100 million monthly actives might pay hundreds of thousands per year for LaunchDarkly but could build an equivalent system for less. Third, when you have strong opinions about architecture and the commercial services don't fit your model.

The cost of building your own is not in the initial implementation. A competent team can build basic feature flags in a week. The cost is in the feature accumulation over time. You start with simple boolean flags. Users ask for percentage rollouts. Then targeting rules. Then multi-variate flags. Then gradual rollout schedules. Then analytics integration. Then audit logs. Then role-based access control for who can change flags. Then SDKs for every language your company uses. Then real-time streaming updates. Then geographic distribution. Then disaster recovery. Two years later you have a team of three engineers maintaining an internal feature flag platform.

A large social media platform in 2025 built their own feature flag system because LaunchDarkly's pricing for their user base — over 500 million monthly actives — was untenable. They built a distributed flag service backed by a globally replicated key-value store. Flag evaluation happened in-process using a local SDK that fetched flag configurations every 30 seconds. They built a web UI for flag management and integrated it into their internal deployment tooling. They staffed a team of four engineers to maintain it. Total cost was lower than LaunchDarkly. But when they wanted to add experimentation capabilities similar to Statsig, they realized they were now building an experimentation platform, not just a flag system. They spent another six months integrating statistical analysis and metric tracking. Eventually they had a system that met their needs, but it consumed significantly more engineering effort than they initially expected.

If you're considering building your own, ask these questions: Do we have requirements that commercial services cannot meet? Is the cost of a commercial service greater than the cost of building and maintaining our own, including the ongoing feature requests we'll face? Do we have the engineering capacity to treat this as a multi-year investment rather than a one-time project? If the answers are yes, build. If the answers are mixed, buy and customize. If the answers are no, buy and use it as-is.

## Evaluation Performance and SDK Design

Feature flag evaluation performance depends on SDK architecture. Server-side SDKs generally cache flag rules in memory and evaluate them synchronously in the request path. This makes evaluation fast — microseconds for simple flags, low milliseconds for complex conditional rules. Client-side SDKs have different constraints. They can't hold the full flag ruleset because it might contain sensitive targeting logic. They need to either evaluate flags server-side and pass the results to the client, or use a simplified client-side evaluation model that fetches only the flags relevant to the current user.

The LaunchDarkly client-side SDK, for example, fetches a user-specific flag evaluation at initialization and subscribes to updates via streaming. The client never sees the full ruleset or targeting logic. This protects sensitive targeting criteria but adds initialization latency. Mobile apps using LaunchDarkly need to wait for the SDK to initialize before they can evaluate flags. This takes 100 to 300 milliseconds on a fast connection, longer on slow networks. For critical features, this means the app needs to handle the case where flag values are not yet available.

Statsig's client-side SDK uses a similar model. Unleash offers both server-side evaluation with a client-side proxy and direct client-side evaluation depending on your security requirements. The performance trade-offs are the same across platforms — server-side evaluation is faster and more secure, client-side evaluation avoids server round trips but exposes more information to the client.

Streaming versus polling is another trade-off. Streaming SDKs maintain a persistent connection to the flag service and receive updates in near real-time. This is efficient at scale and provides fast propagation of flag changes. Polling SDKs fetch flag configurations on a schedule — every 30 seconds, every minute. Polling is simpler to implement and works through restrictive firewalls, but flag changes propagate more slowly. Most commercial platforms default to streaming with polling as a fallback.

## Multi-Environment Support: Dev, Staging, Production

Every feature flag platform supports multiple environments. You define flags once and configure different values per environment. A flag might be enabled for all users in development, enabled for ten percent of users in staging, and disabled in production. This prevents the need to manage separate flag definitions for each environment while allowing different rollout states.

The challenge is keeping environments in sync. You create a new flag in development and forget to configure it in staging. A teammate tests in staging and the flag evaluates to its default value, which is the wrong state. Or you delete a flag in production but forget to delete it in development. Now development has a flag that production doesn't, and someone writes code that depends on it. Multi-environment support solves the configuration problem but not the coordination problem.

Some platforms provide workflows to promote flag configurations from one environment to the next. You configure a flag in development, test it, then promote the configuration to staging, then to production. The flag's targeting rules and rollout percentages move together. This reduces the chance of misconfiguration. Other platforms treat each environment as fully independent and rely on documentation and discipline to keep them aligned.

A B2B SaaS platform in mid-2025 used LaunchDarkly with four environments — development, staging, pre-production, and production. They required that every flag be configured in all four environments before code using that flag could merge to the main branch. This prevented the scenario where a flag existed in code but not in an environment. They enforced this with a pre-merge CI check that called LaunchDarkly's API and verified the flag existed in all environments. If the flag was missing, the merge was blocked. This added friction to the development process but eliminated a class of integration bugs.

## Integration Patterns and SDK Initialization

Feature flag SDKs need to initialize before they can evaluate flags. Initialization involves connecting to the flag service, fetching the current ruleset, and caching it in memory. This takes time. Applications need to decide whether to block startup until the SDK is ready, or proceed with default flag values and update once initialization completes.

Blocking startup ensures that all flag evaluations use current state, but it makes your application dependent on the flag service being reachable at startup. If the service is down or slow, your application won't start. Non-blocking startup avoids this dependency but means the first few requests might evaluate flags using default values rather than the configured values. For critical flags, this could cause user-visible issues.

Most teams use non-blocking initialization with fallback defaults that represent the safest state. If a flag controls whether to use a new expensive model, the default is the old cheap model. If a flag controls whether to enable a risky feature, the default is disabled. This way, if the SDK fails to initialize, the application still works, just in a conservative mode.

A logistics platform in late 2025 used Unleash with blocking SDK initialization. They wanted to ensure that all requests evaluated flags with correct state. This worked fine until a network partition isolated their application instances from the Unleash server. The instances couldn't initialize the SDK, so they wouldn't start. The entire service went down. They fixed this by switching to non-blocking initialization with cached flag state persisted to disk. On startup, the SDK loaded the last known flag state from disk and used it until it could fetch updates from the Unleash server. This made the service resilient to Unleash outages while still using reasonably current flag state.

Feature flag platforms vary in maturity, cost, and integration complexity, but they all solve the same fundamental problem — giving you runtime control over feature behavior without redeploying code. The next step is managing the lifecycle of these flags so they don't accumulate into technical debt that no one understands.
