# 5.10 — Routing Observability: Tracking Where Requests Go and Why

You cannot optimize routing you cannot observe. Most teams fly blind. They build sophisticated routing systems with cost optimization rules, quality tiers, fallback hierarchies, and A/B experiments — then they have no visibility into whether those systems work as intended. When costs spike, they don't know which routing rules caused the spike. When quality degrades, they don't know if requests routed to the wrong model. When fallbacks trigger, they don't know if it was a transient blip or a sustained provider issue. Routing without observability is infrastructure gambling. You're making expensive decisions based on assumptions rather than data. The fix is to log every routing decision with enough context to understand why it happened, then aggregate those logs into metrics and dashboards that show routing behavior in real-time.

## What to Log for Every Routing Decision

Every request that enters your routing system should produce a log entry that records the routing outcome. At minimum, this log contains the timestamp, the request identifier, the selected model, the reason the model was selected, and the list of alternative models that were considered but not chosen. Optional but valuable: the estimated cost of the selected model, the customer tier or segment the request belonged to, any feature flags or experiment assignments that influenced routing, and the latency of the routing decision itself.

The reason field is critical. It's not enough to know that a request routed to GPT-5-mini. You need to know why. Was it because the request matched the free-tier segment rule? Was it because the primary model failed health checks and the router fell back to the secondary? Was it because an A/B experiment assigned this request to the cost-optimized variant? The reason makes logs actionable. When you're investigating why costs are higher than expected, you can filter logs to "reason: fallback triggered" and see if you're falling back more than planned. When you're debugging an A/B experiment, you can filter to "reason: experiment assignment" and verify traffic split percentages.

Structured logging is non-negotiable. Do not log "Routed request 12345 to GPT-5-mini because of segment rule." Do log a JSON object with fields for request ID, model, reason, segment, alternatives, cost, and latency. Structured logs can be parsed by log aggregation systems. Unstructured text logs require regex parsing, which breaks when log format changes. Structured logs feed directly into metrics dashboards without transformation.

Some teams worry about log volume. Logging every routing decision at ten thousand requests per second produces ten thousand log lines per second. That's 864 million logs per day. Storage cost for a year of logs at this volume: around two thousand dollars in S3 with lifecycle policies that move to infrequent access tier after thirty days. The value of being able to query routing history for any request over the past year is worth far more than two thousand dollars. If log volume is a genuine concern, sample logs — log every routing decision with full detail for one percent of traffic, and log only critical events like fallbacks and errors for the remaining ninety-nine percent. Sampled logs still give you traffic distribution and cost patterns. Full logs for one percent of traffic still let you debug individual request routing.

## Routing Decision Metrics

Raw logs are forensic tools for debugging specific requests. Metrics are operational tools for understanding system behavior. Routing metrics answer questions like: what percentage of traffic went to each model in the last hour? How often are fallbacks triggering? What's the cost distribution across customer segments? Are A/B experiment splits matching target ratios? Metrics come from aggregating routing logs, typically using a time-series database or a log aggregation service's built-in metrics functionality.

The most important routing metric is traffic distribution by model. A simple bar chart showing request count or percentage for each model in your fleet over the last hour, day, or week. This metric immediately shows routing anomalies. If your routing rules say seventy percent of traffic should go to GPT-5-mini and thirty percent to Claude Haiku 4.5, but the actual distribution is ninety-ten, something is wrong. Maybe a fallback rule is triggering more than expected. Maybe a segment rule changed and shifted traffic. Maybe one model is faster and handling more requests before others get tried. The distribution metric surfaces these issues.

Fallback trigger rate is the percentage of requests that did not route to their assigned primary model and used a fallback instead. Under normal operations, fallback rate should be near zero — maybe 0.1 to 0.5 percent for transient network issues or rate limit overruns. If fallback rate jumps to five percent or fifteen percent, a primary model is having sustained issues. This metric should trigger alerts. A two-minute alert delay at five percent fallback rate is reasonable — enough time to filter transient spikes, fast enough to catch real provider degradation.

Cost metrics aggregate estimated routing cost across all requests. Total estimated cost per hour. Cost per model. Cost per customer segment. Cost per API endpoint. These metrics let you validate that routing optimization is working. If you implemented a rule to route low-priority traffic to cheaper models, cost per request for that segment should decrease. If cost per request increases after a routing change, either your cost estimates are wrong or the routing rule isn't behaving as intended. Cost metrics also feed into real-time budgeting — if you've allocated ten thousand dollars per day for inference and you're on track to spend fifteen thousand today, routing observability tells you which segments or models are driving the overage.

Experiment assignment metrics verify A/B test integrity. If you're running an experiment that should assign fifty percent of traffic to variant A and fifty percent to variant B, actual assignment rates should match within a few percentage points. If you see sixty-forty or seventy-thirty splits, your traffic splitting logic has a bug or your traffic patterns are non-uniform in a way that biases assignment. Experiment metrics also track key outcomes by variant — average latency, error rate, estimated cost per request. These metrics tell you which routing variant wins the experiment.

## Dashboards for Routing Visibility

Metrics become actionable when displayed in dashboards that update in real-time. A routing dashboard should be visible on large monitors in the engineering area or embedded in the team Slack channel. The dashboard shows the current state of routing: which models are handling what percentage of traffic, what the fallback rate is, what the cost burn rate is, what A/B experiments are running and how they're performing. When something breaks, the dashboard shows it immediately.

The primary view is traffic distribution over time. A stacked area chart with one layer per model. The chart shows the last four hours or the last twenty-four hours. Normal operation looks stable — the same few models handling consistent percentages of traffic. An incident looks like sudden shifts — one model drops from forty percent to zero percent, another model jumps from thirty percent to seventy percent as fallbacks activate. Operators glance at this chart and know instantly whether routing is behaving normally.

Fallback rate over time is a line chart with a red threshold line at one percent. When the fallback rate crosses one percent, the chart turns red. This is your primary routing health indicator. A spike in fallbacks might correlate with a provider's known incident. Or it might reveal an incident you didn't know about yet. The dashboard is often the first place teams learn that a provider is degrading.

Cost burn rate is a real-time counter showing estimated spending per hour extrapolated to per day and per month. If your routing rules are performing as expected, the burn rate should be predictable. If it suddenly jumps, the dashboard shows which models or segments are driving the increase. Cost burn rate matters most for teams operating under strict inference budgets. For a company that allocated five hundred thousand dollars per year for inference, a jump from sixty dollars per hour to one hundred fifty dollars per hour represents a budget overrun of nearly eight hundred thousand dollars annually. The dashboard surfaces this immediately instead of letting the team discover it when the monthly bill arrives.

Experiment performance panels show side-by-side metrics for active A/B tests. Variant A versus variant B: traffic count, average latency, error rate, cost per request. If you're testing whether routing lower-priority requests to a cheaper model degrades quality, the experiment panel shows error rate and latency for both variants in real-time. If the cheaper model has double the error rate, you stop the experiment. If error rates match and cost per request is forty percent lower, you promote the cheaper routing rule.

## Alerting on Routing Anomalies

Dashboards require human attention. Alerts bring attention when humans are not watching. Routing anomalies that require alerts: fallback rate above threshold, traffic distribution shifted unexpectedly, cost burn rate exceeds budget, specific high-value customer routed to wrong tier, experiment assignment ratio deviates from target. Each alert should trigger a page or Slack notification with enough context to start investigation without having to open dashboards.

Fallback rate alerts trigger when the percentage of requests using fallback models exceeds one percent for more than two minutes. The alert message includes which model is being fallen back from, what the current fallback rate is, and what the normal rate is. This helps the on-call engineer prioritize. A fallback rate of two percent might be a minor provider slowdown. A fallback rate of thirty percent means the primary provider is completely down. The severity should scale with the rate.

Traffic distribution alerts detect when the percentage of traffic handled by a particular model deviates significantly from the expected range. If GPT-5-mini normally handles sixty to seventy percent of traffic but suddenly drops to twenty percent, something shifted routing behavior. This might be intentional — someone deployed a new routing rule — or unintentional — a bug in segment matching. The alert fires so the team can verify the change was intentional and expected.

Cost burn rate alerts fire when spending per hour exceeds the budgeted threshold. For a team with a budget of ten thousand dollars per day, the threshold might be five hundred dollars per hour. If actual burn rate hits six hundred dollars per hour for fifteen minutes, alert. The alert includes which models and which segments are driving cost. Often this surfaces mistakes like accidentally routing all traffic to the most expensive model, or a high-volume customer switching to a feature tier that uses premium models.

Customer-specific routing alerts trigger when a particular customer or segment gets routed in a way that violates their contract or expectation. An enterprise customer who pays for premium model access should never be routed to a low-cost model. If a routing rule accidentally sends an enterprise request to a cheap model, this alert fires immediately. These alerts typically require integrating routing logs with customer metadata so the routing system knows which customers have which tier commitments.

## Debug Traces: Why This Specific Request

Metrics and dashboards show aggregate behavior. Sometimes you need to understand one specific request. A customer reports poor quality. A support engineer needs to know which model handled that request, whether it was the intended model or a fallback, what the prompt was, what the response was, what the routing decision logic was. This requires request-level debug traces.

A debug trace is a detailed log of everything that happened during the lifecycle of a specific request. The request arrives, routing system evaluates segment rules in order, determines the primary model, checks model health, routes to primary, primary responds, response is returned. Each step is logged with timing. The trace shows not just which model was selected but which rules were evaluated and why they matched or didn't match. It shows which models were considered as fallbacks and why they weren't used. It shows the complete context that led to the routing decision.

Most production systems cannot afford to produce debug traces for every request — the log volume would be enormous. Instead, debug tracing is on-demand. A support engineer provides a request ID or a customer ID and a time range, and the system searches logs for traces matching that criteria. Or debug tracing is enabled for specific customers or sessions via a feature flag. The customer reports an issue, support enables tracing for that customer for the next hour, the customer reproduces the issue, support retrieves the trace. The trace shows exactly what the routing system did for that request.

Some routing systems implement a trace ID header. The client includes a trace ID with the request. The routing system logs all decisions associated with that trace ID with extra detail. The client can then query logs using that trace ID to see the routing path. This is the distributed tracing pattern applied to routing decisions. It works well when clients can control trace IDs and when your logging infrastructure supports searching by custom fields.

## A/B Test Observability

A/B experiments in routing systems require their own observability layer. You need to know which requests were assigned to which experiment variant. You need to know whether variant performance differs meaningfully. You need to know whether the experiment is ready to conclude or needs more data. Experiment observability is the feedback loop that makes experimentation scientifically valid rather than guesswork.

Experiment assignment logs record which variant each request was assigned to. These logs must be joined with outcome logs — did the request succeed, what was the latency, what was the cost, what was the user's downstream behavior. This requires consistent request IDs across logs. The routing decision log says "request A was assigned to experiment X variant B." The response log says "request A completed with latency 450 milliseconds and cost 0.003 dollars." The application log says "request A resulted in a user action Z." Joining these logs across systems lets you compute experiment metrics: average latency by variant, success rate by variant, user conversion rate by variant, cost by variant.

Experiment dashboards track statistical significance in real-time. Variant A has mean latency 420 milliseconds with standard deviation 80 milliseconds across five thousand requests. Variant B has mean latency 390 milliseconds with standard deviation 75 milliseconds across five thousand requests. The dashboard computes a p-value and confidence interval. If the difference is statistically significant with ninety-five percent confidence, the dashboard shows "Variant B is faster, high confidence." If not, it shows "Inconclusive, need more data." This prevents teams from calling experiments too early based on noisy data.

Experiment observability also monitors for bias. Is variant assignment balanced across customer segments, regions, time of day? If one variant is getting all the high-value customers and another is getting all the low-value customers, the experiment results are biased. The assignment logic should produce balanced splits. Observability verifies that it actually does.

## Using Observability to Optimize Routing Rules

Routing observability is not passive monitoring. It's active optimization. You deploy a routing rule based on assumptions. Observability tells you whether those assumptions were correct. If they were wrong, you adjust the rule. Over time, the routing system becomes more efficient, more cost-effective, and more reliable because observability creates a feedback loop for continuous improvement.

A team might assume that ninety percent of their traffic can be handled by a small fast model, with only ten percent needing a larger capable model. They implement a routing rule that uses request metadata to predict which requests need the large model. Observability shows that the rule is routing fifteen percent of traffic to the large model, not ten percent. Investigating the logs reveals that the prediction criteria are too conservative — some requests are routed to the large model unnecessarily. The team tightens the criteria. Observability shows traffic to the large model drops to eleven percent. Cost per request decreases by eight percent. The optimization was only possible because observability made the over-routing visible.

Another team implements a fallback hierarchy: primary model is GPT-5-mini, fallback is Claude Haiku 4.5, second fallback is Llama 4 Scout. Observability shows that the second fallback is triggered three times per day, always during peak load hours when both GPT-5-mini and Haiku hit rate limits. The team realizes they need a fourth fallback or they need to increase rate limits. They add a fourth fallback to Gemini 3 Flash. Observability shows that the second fallback rate drops and the fourth fallback triggers occasionally. The routing system now handles peak load without returning errors.

Cost observability reveals which customer segments are most expensive to serve. A legal research platform finds that enterprise customers use premium models for ninety percent of queries, costing twelve cents per query, while free-tier users route to cheap models, costing one cent per query. The product team uses this data to restructure pricing — enterprise tier now includes an inference usage cap, with overage charges for excessive usage. The change reduces surprise costs and aligns pricing with actual infrastructure expense.

Observability is the difference between routing rules that decay over time because they no longer match reality and routing rules that improve over time because you can see where they fail and fix them. Routing is not a deploy-and-forget system. It's a control system that adapts to production conditions. Observability is the sensor layer that makes adaptation possible.

The routing decision framework synthesizes all of this — priorities, model inventory, traffic segments, rules, fallbacks, and observability — into a systematic approach for building routing strategy.
