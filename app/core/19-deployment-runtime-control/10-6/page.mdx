# 10.6 — Staged Rollout with Validation Gates

In March 2025, a fintech company deployed a new fraud detection model using canary deployment. They routed five percent of traffic to the new model, watched the dashboards for 30 minutes, saw latency and error rates within acceptable ranges, and pushed the deployment to 100 percent. Two hours later, customer support started receiving complaints. The new model was flagging legitimate transactions as fraudulent at twice the rate of the old model. The operations team scrambled to roll back. The incident affected 40,000 transactions and cost the company 180,000 dollars in reversed charges and customer compensation. The canary metrics did not catch the problem because the metrics only measured latency and error rates, not output quality.

The root cause was not the canary deployment itself. The root cause was the absence of validation gates. The team had a deployment process but no decision framework. They watched metrics but did not define what "success" meant. When the basic infrastructure metrics looked acceptable, they assumed everything was fine and proceeded to full rollout. There was no gate that required precision and recall to match baseline performance. There was no gate that required manual review of flagged transactions. There was no gate that required sign-off from the fraud operations team. The deployment advanced because no gate blocked it, not because any gate approved it.

## The Validation Gate Concept: Explicit Checkpoints That Must Pass

A validation gate is a condition that must be met before a deployment can advance to the next stage. The condition is defined before the deployment begins. The deployment pauses at the gate. The condition is evaluated. If the condition is met, the gate opens and the deployment continues. If the condition is not met, the gate remains closed and the deployment halts. The gate is not a suggestion. It is a barrier. The deployment cannot proceed without passing it.

Gates can be automated or manual. An automated gate evaluates metrics and opens when thresholds are met. A manual gate requires a human to review data and make a decision. A time-based gate opens after a fixed duration, regardless of metrics. Most rollouts use a combination of all three. Automated gates handle the mechanical checks. Manual gates handle the judgment calls. Time-based gates ensure the system runs long enough to surface issues that take time to appear.

The value of gates is clarity. Without gates, the decision to advance is implicit. Someone looks at a dashboard, decides it looks okay, and clicks the button to increase the rollout percentage. The decision is subjective, informal, and undocumented. With gates, the decision is explicit. The gate criteria are written down. The evaluation is recorded. If the deployment fails, you can review the gate criteria and determine whether they were too lax or whether the failure was unpredictable. Gates turn deployment from an art into a process.

## Gate Types: Automated, Manual, and Time-Based

Automated gates are conditions that a system can evaluate without human input. The most common automated gate is a metric threshold. The canary error rate must be less than 0.5 percent. The canary p95 latency must be less than 600 milliseconds. The canary cost per request must be less than 1.2 times the baseline cost. The system collects these metrics, compares them to the thresholds, and opens the gate if all conditions are met. If any condition fails, the gate remains closed and an alert is sent to the on-call engineer.

Manual gates require human judgment. A product manager reviews a sample of outputs and approves the quality. A domain expert examines edge cases and confirms the new system handles them correctly. A legal or compliance reviewer verifies that the new feature meets regulatory requirements. Manual gates are slower than automated gates, but they catch issues that automated metrics miss. The fintech fraud detection model had acceptable latency and error rates, but the precision was terrible. An automated gate would not have caught it. A manual gate where the fraud operations team reviewed a sample of flagged transactions would have.

Time-based gates enforce a minimum duration at each stage. The canary must run for at least four hours before advancing to ten percent. The ten percent rollout must run for at least 12 hours before advancing to 50 percent. Time-based gates are the simplest to implement and the hardest to justify skipping. They ensure the system runs long enough for patterns to emerge. A memory leak that takes six hours to manifest will not be detected by a 30-minute canary. A time-based gate that requires four hours of canary runtime catches it. Time-based gates are not sufficient on their own—metrics can look fine for four hours and then degrade—but they are a necessary floor.

## Multi-Stage Rollout: Canary, Rings, and Validation Between Stages

A staged rollout divides the user population into segments and deploys to each segment sequentially. The most common pattern is canary followed by ring-based expansion. Canary is the first stage, routing one to five percent of traffic to the new system. If the canary passes its gates, the rollout advances to the first ring. Rings are predefined user segments, ordered by risk tolerance. Internal employees are Ring 0. Beta users who opted into early access are Ring 1. General users in a low-risk geography are Ring 2. All users are Ring 3.

Each ring has its own gates. Ring 0 might have a manual gate where the product team reviews outputs and approves advancement. Ring 1 might have automated gates for latency and error rate, plus a time-based gate requiring 24 hours of runtime. Ring 2 might have automated gates for quality metrics like precision and recall, plus a manual gate where customer support reviews tickets. Ring 3 has no additional gates—if the rollout reached Ring 3, all previous gates have passed.

The ring boundaries depend on your user base and risk profile. A B2B SaaS company might define rings by customer size: internal testing, then small customers, then mid-market, then enterprise. A consumer app might define rings by geography: internal, then a low-risk region like New Zealand, then a larger region like Europe, then the United States. A healthcare app might define rings by feature criticality: non-clinical features first, then diagnostic support features, then treatment recommendation features. The principle is the same. Deploy to the lowest-risk segment first, validate, then expand.

## Ring-Based Deployment: Internal, Beta, and General Availability

Ring 0 is internal users. Engineers, product managers, customer support, and other employees use the new system before any external user sees it. Internal users are the most tolerant of bugs and the fastest to report them. If the new system breaks, the impact is contained to people who understand that they are testing new functionality. Internal users also provide qualitative feedback that metrics cannot capture. An engineer who uses the system daily will notice subtle quality degradation that would not trigger an automated alert but would frustrate external users over time.

Ring 1 is beta users. These are external users who have opted into early access programs, who have agreed to use experimental features, or who are part of a pilot program. Beta users are more tolerant of issues than general users, but they are still real users with real expectations. If the new system fails for beta users, the damage is limited. The user population is small, the users are expecting potential issues, and the company has a direct relationship with them. Beta users are also more likely to provide detailed feedback, making them valuable for identifying edge cases and unexpected usage patterns.

Ring 2 is general availability for a subset of users. This might be users in a specific geography, users who signed up after a certain date, or users selected randomly. The subset is large enough to represent the full user base but small enough that a failure does not affect the majority. If the new system has a critical bug that passed all previous gates, the blast radius is limited. The company can roll back, fix the issue, and redeploy without most users ever knowing there was a problem.

Ring 3 is full general availability. All users, all traffic, all the time. By the time the rollout reaches Ring 3, the new system has been validated at every previous stage. The confidence level is high. The rollout to Ring 3 might still be gradual—going from 50 percent to 75 percent to 100 percent over a few hours—but the gates are fewer and the risk is lower. If the rollout has passed all the gates to reach Ring 3, the probability of a major incident is low.

## Gate Criteria: What Must Be True to Advance

The criteria for each gate depend on what you are testing. For infrastructure gates, the criteria are metric thresholds. Error rate below 0.5 percent. P95 latency below baseline plus 50 milliseconds. Memory usage below 80 percent of available capacity. Cost per request below baseline plus 20 percent. These criteria are measurable, objective, and automatable. The system collects the metrics, evaluates the thresholds, and opens or closes the gate.

For quality gates, the criteria are more complex. Precision and recall must match baseline within five percentage points. Hallucination rate must be below two percent. User satisfaction score must be above 4.0 out of 5. These criteria require running evals, collecting user feedback, or conducting manual reviews. They are slower to evaluate than infrastructure metrics, but they are more predictive of user impact. A system can have perfect infrastructure metrics and terrible output quality. Quality gates catch that.

For compliance gates, the criteria are often binary. The new system must pass a security review. The new system must comply with GDPR data retention policies. The new system must not log personally identifiable information. These gates are typically manual, requiring review by a subject matter expert. They are time-consuming, but they are non-negotiable. A deployment that fails a compliance gate does not proceed, regardless of how good the infrastructure and quality metrics are.

The gate criteria should be stricter for higher-risk changes and more permissive for lower-risk changes. A new model architecture that has never been used in production should have tight thresholds and multiple manual gates. A prompt change that has been tested extensively in staging can have looser thresholds and fewer manual gates. The criteria adapt to the risk. High-risk changes earn high barriers. Low-risk changes move faster.

## Gate Failures: What Happens When a Gate Does Not Pass

When a gate fails, the deployment halts. The rollout percentage does not increase. An alert is sent to the responsible team. The team investigates the failure, determines the root cause, and decides on a path forward. The options are roll back, fix forward, or wait and reevaluate.

Roll back is the safest option. The deployment is reverted to the previous version. All traffic returns to the old system. The new system is removed from production. The team fixes the issue in development, redeploys, and starts the rollout process again. Roll back is appropriate when the gate failure indicates a fundamental problem—a bug, a configuration error, a resource constraint. If the new system cannot pass the gate, it should not be in production.

Fix forward is faster but riskier. The team deploys a patch to fix the issue without rolling back. The patch is deployed to the canary, tested, and if it resolves the gate failure, the rollout continues. Fix forward is appropriate when the issue is minor and the fix is straightforward. If the canary error rate is 0.6 percent and the gate threshold is 0.5 percent, the team might adjust a timeout parameter, redeploy, and watch the error rate drop. If the issue is more complex or the root cause is unclear, roll back is safer.

Wait and reevaluate is appropriate for time-sensitive gates or for situations where the metrics are noisy. If the gate requires four hours of canary runtime and the deployment has only been running for three hours, the gate has not failed—it has not yet been evaluated. If the canary error rate is spiking due to a transient issue with a downstream dependency, waiting 15 minutes and reevaluating might reveal that the spike was temporary. Waiting is not the same as ignoring the gate. It is giving the system time to stabilize before making a decision.

## Automated Gating: Using Metrics and Evals to Open Gates Automatically

Automated gates remove human latency from the deployment process. The system collects metrics, evaluates gate criteria, and advances the rollout without human intervention. An engineer deploys the new system at 9 AM, sets the gate criteria, and walks away. The system runs the canary for four hours, evaluates the metrics, opens the gate, and advances to ten percent. It runs at ten percent for 12 hours, evaluates the metrics, opens the gate, and advances to 50 percent. By the next morning, the rollout is at 100 percent. The engineer reviews the deployment history, confirms all gates passed, and closes the deployment ticket.

Automated gates require robust metric collection and alerting. The metrics must be accurate, timely, and comprehensive. If latency metrics are delayed by ten minutes, the automated gate might open based on stale data and miss a latency spike. If the metrics pipeline drops data during a network partition, the automated gate might evaluate incomplete data and make the wrong decision. The metrics infrastructure must be as reliable as the deployment system itself. An automated gate is only as good as the data it evaluates.

Automated gates also require well-calibrated thresholds. If the thresholds are too strict, legitimate deployments will fail gates and require manual intervention, defeating the purpose of automation. If the thresholds are too lax, bad deployments will pass gates and reach production. Calibrating thresholds requires historical data. Run several rollouts manually, record the metrics at each stage, and identify the normal range. Set the gate thresholds just outside the normal range, allowing for minor variation but catching significant deviations.

The safety net for automated gates is alerting. Even if the gate opens automatically, the system should alert the responsible team. The alert includes the gate that passed, the metrics that were evaluated, and the rollout stage that was advanced to. If the engineer sees the alert and notices something suspicious in the metrics, they can halt the rollout manually. Automated gates make deployments faster, but they do not remove human oversight. They shift oversight from active decision-making to passive monitoring.

## Human-in-the-Loop Gates: When Manual Approval Is Required

Some decisions cannot be automated. A product manager reviewing output quality, a domain expert evaluating edge cases, a compliance officer verifying regulatory adherence—these require human judgment. Human-in-the-loop gates pause the deployment and wait for a human to approve or reject advancement.

The human-in-the-loop gate presents the reviewer with the data they need to make a decision. For a product manager reviewing output quality, the gate includes a sample of 50 to 100 outputs from the canary, selected to represent common queries, edge cases, and failure modes. The product manager reviews the outputs, compares them to the old system's outputs, and approves advancement if the quality is acceptable. The review takes 20 minutes. The deployment waits.

For a compliance gate, the reviewer might evaluate the system's behavior against a checklist of regulatory requirements. Does the system log only the data specified in the data retention policy? Does the system anonymize personally identifiable information before logging? Does the system respect user consent preferences? The reviewer checks each item, documents the findings, and approves or rejects the deployment. The review takes two hours. The deployment waits.

Human-in-the-loop gates are slower than automated gates, but they catch issues that automated systems miss. The fraud detection model that flagged legitimate transactions would have been caught by a human reviewer examining the flagged transactions. The automated metrics showed low latency and low error rates, but a human would have seen that the flagged transactions were obviously legitimate—a purchase at the user's regular grocery store, a mortgage payment to a known lender. Human judgment is expensive, but it is irreplaceable for evaluating correctness and appropriateness.

## Gate Timeout: How Long to Wait Before Making a Decision

A time-based gate has a fixed duration. The canary must run for four hours. After four hours, the gate evaluates the metrics. If the metrics pass, the gate opens. If the metrics fail, the gate closes and the deployment halts. The timeout is explicit and non-negotiable. The deployment does not advance before the timeout expires, even if the metrics are perfect after 30 minutes.

The timeout duration depends on what you are testing. For cold start and initialization issues, 10 to 15 minutes is enough. For sustained performance under load, four to six hours is better. For memory leaks and resource exhaustion, 12 to 24 hours is necessary. For usage pattern changes that only appear during peak hours, the timeout should span at least one peak period. If your system has daily peaks at noon and 6 PM, the timeout should be at least 12 hours to capture both peaks.

The timeout also protects against prematurely advancing a deployment based on noisy or incomplete data. If the canary has only been running for ten minutes, the metrics are not yet stable. The first few minutes include cold start latency, cache warming, and connection pool initialization. The metrics do not reflect steady-state performance. A gate with a ten-minute timeout ensures the system has stabilized before evaluating whether to advance.

Timeouts can be combined with metric gates. The canary must run for at least four hours, and the error rate must be below 0.5 percent. Both conditions must be met. If the error rate is below 0.5 percent after 30 minutes, the gate does not open until four hours have passed. If four hours have passed but the error rate is 0.7 percent, the gate does not open until the error rate drops. The timeout is a floor, and the metric threshold is a ceiling. Both must be satisfied.

Staged rollout with validation gates ensures that deployments advance only when they are ready, but orchestrating the stages, gates, and metrics manually is tedious and error-prone—progressive delivery automates the entire process.
