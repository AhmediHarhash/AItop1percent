# 5.8 — Context-Based Routing: Routing by User, Tenant, or Input Characteristics

The same query from different users may need different models. A premium subscriber asking for financial analysis deserves GPT-5 with extended reasoning time. A free-tier user asking the same question can receive a response from Claude Haiku 4.5 in two seconds. Both users get their question answered. The premium user gets deeper analysis with more comprehensive reasoning. The free user gets a faster, more concise answer. This is not treating users poorly. This is aligning service level with value exchange.

Context-based routing makes routing decisions based on information available at request time. That information might be user tier, tenant identity, query characteristics, geographic location, or time of day. The routing layer evaluates context, applies routing rules, and selects the appropriate model, prompt variant, or provider configuration. The result is a system that optimizes for different objectives simultaneously: premium experience for high-value users, cost efficiency for high-volume use cases, latency optimization for time-sensitive queries, compliance adherence for regulated tenants.

## User-Based Routing: Premium Users Get Premium Models

User-based routing is the simplest form of context-based routing. Each user has an associated tier: free, basic, premium, enterprise. The routing layer checks the user's tier and applies the corresponding model configuration. Premium users route to GPT-5 or Claude Opus 4.5. Basic users route to GPT-5-mini or Claude Sonnet 4.5. Free users route to Claude Haiku 4.5 or Gemini 3 Flash. Each tier receives responses appropriate to their subscription level.

The routing decision happens **before the model is called**. The system does not generate responses with multiple models and choose which one to serve. That would be wasteful. Instead, the system commits to a model based on user tier and generates one response. This means user tier must be available at routing time, not after the model call completes. Some systems pass user tier as a request header. Others look it up from a database before routing. Either way, routing time must be low—under ten milliseconds—to avoid adding noticeable latency.

**Tier-specific prompts** often accompany tier-specific models. A premium user's prompt might include instructions to provide detailed explanations, cite sources, and consider multiple perspectives. A free user's prompt might instruct the model to be concise, focus on the most likely answer, and skip nuance. The same underlying question receives different treatment based on tier. This maximizes value delivered per dollar spent while ensuring that every tier receives a functional experience.

Some systems use **graduated routing** where user tier determines not just model choice but also context window size, temperature settings, or reasoning budget. A premium user's request might allow the model to use extended reasoning, consuming more tokens but producing higher-quality output. A free user's request restricts reasoning tokens to keep cost low. The model architecture is the same, but the configuration parameters shift based on user value.

**Usage-based tier transitions** provide dynamic routing. A user who starts on the free tier but demonstrates high engagement—making fifty queries per week—might automatically graduate to basic tier routing after one month. A premium user who has not logged in for six months might drop to basic tier routing until they re-engage. Usage-based transitions align routing cost with user value dynamically rather than relying on static subscription tiers.

## Tenant-Based Routing: Enterprise Customers Get Dedicated Capacity

Tenant-based routing handles multi-tenant systems where different customers have different contracts, compliance requirements, or quality expectations. A SaaS platform serving two hundred enterprise customers cannot treat all customers identically. Customer A has a contract specifying 99.9 percent uptime and sub-three-second latency. Customer B operates in a regulated industry and requires all inference to happen in a specific geographic region. Customer C negotiated a contract with cost-plus pricing and wants the most expensive, highest-quality model for every request.

The routing layer maintains a **tenant configuration table** that maps tenant identifiers to routing rules. When a request arrives, the system extracts the tenant ID—usually from authentication context—looks up that tenant's routing configuration, and applies it. Customer A's requests route to a high-availability model cluster with strict latency SLAs. Customer B's requests route to models running in EU-Central region only. Customer C's requests route to GPT-5 with extended context and maximum reasoning budget.

**Tenant-specific rate limits** prevent one customer from impacting others. If Customer D sends a sudden spike of ten thousand requests per minute, the routing layer enforces their contracted rate limit and rejects excess requests. This prevents Customer D's spike from exhausting shared capacity and degrading experience for Customers A, B, and C. Tenant-level rate limiting is more granular than account-level rate limiting and critical for SaaS systems that must guarantee isolation.

Some enterprise customers require **dedicated model instances** rather than shared capacity. These customers pay significantly more but receive guarantees that no other tenant's traffic uses their allocated capacity. The routing layer maintains a mapping of tenant IDs to dedicated endpoints. When Customer E's requests arrive, they route exclusively to Customer E's dedicated vLLM cluster. No traffic from other tenants ever touches that cluster. This provides complete isolation for customers with strict security or compliance requirements.

**Tenant-specific prompt repositories** handle cases where different customers need different behavior from the same underlying feature. A generic document summarization feature might use a standard prompt for most tenants. But Customer F, a legal services firm, requires summaries that preserve specific legal terminology and cite clause numbers. Customer F's tenant configuration specifies a custom prompt variant optimized for legal documents. The routing layer loads the appropriate prompt based on tenant ID and applies it to Customer F's requests.

## Input Characteristic Routing: Different Queries Need Different Models

Not all queries are created equal. A simple FAQ question requires different infrastructure than a complex multi-document analysis task. Input characteristic routing inspects the request content and routes based on query complexity, length, language, domain, or other observable attributes.

**Query length routing** is the most straightforward. If the input text is under 500 tokens, route to a fast, small model like Claude Haiku 4.5. If the input is between 500 and 5,000 tokens, route to Claude Sonnet 4.5. If the input exceeds 5,000 tokens, route to GPT-5 or Claude Opus 4.5 with extended context window. Long-context queries require models with large context windows and the infrastructure to handle them efficiently. Short queries do not justify the cost.

**Language detection routing** handles multilingual systems. If the input is in English, route to the standard model configuration. If the input is in Spanish, French, German, or another widely-supported language, route to the same model but with language-specific prompts. If the input is in a lower-resource language, route to a multilingual model like GPT-5 or Gemini 3 that handles diverse languages more reliably. Language detection happens before routing, using lightweight language identification libraries that add negligible latency.

**Domain classification routing** sends specialized queries to specialized models. A system that handles both medical queries and financial queries might route medical inputs to a model fine-tuned on medical data and financial inputs to a different model fine-tuned on financial data. Domain classification can use lightweight classifiers—a small BERT-based model trained to detect query domain—or keyword matching for high-confidence cases. Misclassification risk is real, so many teams only apply domain routing when confidence exceeds 0.9 and fall back to a generalist model otherwise.

**Complexity estimation routing** is harder but valuable. Some queries are inherently harder than others. "What time do you close?" is trivial. "Compare the long-term investment implications of these three portfolio strategies given my risk tolerance and tax situation" is not. Complexity estimation can use heuristics—query length, number of constraints, presence of comparison keywords—or lightweight models trained to predict query difficulty. High-complexity queries route to more capable models. Low-complexity queries route to faster, cheaper models.

**Intent-based routing** sends different user intents to different model configurations. A customer support system might classify queries as informational, transactional, or complaint. Informational queries route to a standard FAQ model. Transactional queries route to a model with tool-calling capabilities so it can execute actions like updating subscriptions. Complaint queries route to the highest-quality model with instructions to be empathetic and thorough, because complaint handling has outsize impact on customer retention.

## Geographic Routing: Latency Optimization and Data Residency

Geographic routing selects model endpoints based on the user's physical location. A user in Tokyo routes to an Asia-Pacific model endpoint. A user in London routes to a European endpoint. A user in New York routes to a US-East endpoint. The goal is latency reduction: routing to the nearest endpoint minimizes network round-trip time and improves user-perceived responsiveness.

**Latency-based routing** requires measuring actual latency to each available endpoint. The routing layer periodically probes endpoints from various geographic locations and records response times. When a request arrives from Tokyo, the routing layer knows that the Asia-Pacific endpoint responds in 120 milliseconds, the US-East endpoint responds in 280 milliseconds, and the European endpoint responds in 310 milliseconds. The request routes to Asia-Pacific. Latency tables are updated every few minutes based on continuous probing.

**Data residency routing** enforces regulatory requirements. GDPR requires that EU citizen data processed by AI models must remain within the EU unless specific transfer mechanisms are in place. A system serving European customers must route their requests to models running on EU-based infrastructure. The routing layer checks the user's location—based on IP geolocation or account registration country—and enforces data residency rules. European users always route to EU endpoints regardless of latency considerations.

Some regions have **limited model availability**. Not every AI provider offers endpoints in every region. If your system requires a model that is only available in US-East and EU-Central, users in Asia-Pacific must route to one of those regions and accept higher latency. Geographic routing rules must account for model availability: prefer the nearest endpoint where the required model is available, fall back to more distant endpoints if necessary.

**Cross-region failover** is a special case of geographic routing. If the Asia-Pacific endpoint goes down, requests from Tokyo should failover to the next-nearest healthy endpoint, even if that endpoint is in a different region. Cross-region failover increases latency but preserves availability. The routing layer monitors endpoint health per region and automatically adjusts routing when regional outages occur.

## Segment-Specific Quality Requirements

Certain user segments or query types have higher quality requirements than others. A healthcare query where the user is asking about medication interactions must receive the highest-quality model available, even if that user is on a free tier. A casual FAQ query from a premium user can use a mid-tier model because the query does not justify premium compute. Segment-specific routing overrides tier-based routing when safety, compliance, or business impact demands it.

**Safety-sensitive routing** applies to queries that could cause harm if answered incorrectly. Medical questions, legal advice, financial planning, and child safety queries all fall into this category. The routing layer classifies incoming queries for safety sensitivity and routes high-risk queries to the most reliable models regardless of user tier. A free-tier user asking "Can I take these two medications together?" receives the same model quality as a premium user because incorrect medical advice can cause serious harm.

**Compliance-sensitive routing** handles queries that involve regulated data or regulated domains. A banking application processing loan applications must route all loan-related queries through models that meet financial services compliance requirements. Those models may run on specific infrastructure, in specific regions, with specific logging and audit trails. The routing layer enforces compliance rules by inspecting query content and applying mandatory routing paths.

**High-value transaction routing** ensures that queries tied to revenue or high-stakes decisions receive premium treatment. An e-commerce platform processing a query from a user who has ten thousand dollars worth of items in their cart should route that query to the best available model. The cost of losing that transaction far exceeds the incremental cost of using GPT-5 instead of GPT-5-mini. Revenue-based routing requires context about the user's current session: cart value, purchase history, predicted lifetime value.

Some teams implement **dynamic quality budgets** where the system allocates a per-user or per-session quality budget and adjusts routing to stay within that budget while maximizing quality for high-impact requests. A user might have a budget of one hundred quality-weighted requests per month. Simple queries consume one point and route to fast models. Complex queries consume five points and route to premium models. If the user exhausts their budget mid-month, subsequent queries route to cheaper models. This approach balances cost control with user experience.

## Building the Routing Context: What Information Is Available

Context-based routing only works if the necessary context is available at routing time. The routing layer must answer: who is this user? What tier are they on? What tenant do they belong to? What does the query contain? Where is the user located? All of this information must be available within the few milliseconds allocated to routing decisions.

**Authentication context** provides user identity, tier, tenant ID, and entitlements. Most systems perform authentication before routing, so this information is available as structured data attached to the request. The routing layer reads user ID from an authentication token, looks up tier from a user database or cache, and applies routing rules. If authentication is slow, it becomes the bottleneck. Teams optimize authentication by caching user tier and tenant information in-memory with short TTLs—30 seconds to five minutes—so that repeated requests from the same user avoid database lookups.

**Request metadata** includes query length, detected language, estimated complexity, and inferred domain. Some of this metadata can be extracted from the raw request with negligible latency. Query length is trivial to compute. Language detection libraries like langdetect or fastText run in single-digit milliseconds. More complex analysis—domain classification, intent detection, complexity estimation—requires lightweight models that add 20 to 50 milliseconds. The routing layer must balance the value of richer context against the latency cost of extracting it.

**Geographic context** comes from IP geolocation or user account settings. IP geolocation is fast—under five milliseconds with in-memory GeoIP databases—but not perfectly accurate. User account settings are more accurate but only available for authenticated users. The routing layer uses the best available signal: account-based location for authenticated users, IP-based location for anonymous users.

**Session context** tracks the user's recent behavior within the current session. If the user has already asked three high-complexity questions in the past five minutes, subsequent questions might route to a faster model to manage cost. If the user is interacting with a high-value feature—configuring a deployment, finalizing a purchase—routing prioritizes quality over cost. Session context requires maintaining per-session state, either in a distributed cache or in client-side tokens.

## Context Extraction and Routing Decision Speed

The entire routing process—extracting context, evaluating rules, selecting a model, and initiating the request—must complete in under 50 milliseconds. Anything slower becomes user-visible latency. A routing layer that takes 200 milliseconds to make decisions adds 200 milliseconds to every request. Over millions of requests per day, that delay costs real money in compute time and real user satisfaction in perceived slowness.

**Caching is critical**. User tier information, tenant configuration, and geographic routing tables must be cached in-memory. Database lookups are too slow for the hot path. The routing layer loads configuration into memory at startup and refreshes periodically. Changes to tenant configuration or routing rules propagate to all routing instances within one minute. This delayed propagation is acceptable because routing rules change infrequently.

**Lazy evaluation** skips unnecessary work. If user tier is sufficient to make a routing decision, the system does not bother running language detection or domain classification. If the query is under 100 tokens and from a free-tier user, route to Claude Haiku 4.5 immediately without further analysis. Lazy evaluation means that simple cases incur minimal overhead and complex cases incur overhead only when necessary.

**Precomputed routing tables** accelerate decisions. Instead of evaluating a complex rule tree for every request, the system precomputes common routing paths and stores them in a lookup table. User tier plus tenant ID maps to a routing configuration. The routing layer hashes those two values and looks up the result. Hash table lookups are sub-microsecond. Rule evaluation can take milliseconds. Precomputed tables work when the routing logic is deterministic and the number of unique routing paths is bounded.

Some teams implement **routing layers as separate services** that run on dedicated infrastructure optimized for low latency. The routing service maintains all context caches, runs all lightweight classifiers, and returns routing decisions in under 20 milliseconds. Application servers send a routing request, receive a routing decision, and execute it. This separation allows independent scaling: routing infrastructure scales based on request rate, model infrastructure scales based on token throughput.

## Routing Rule Configuration and Evolution

Context-based routing rules evolve over time. A system that starts with simple tier-based routing adds geographic routing when it expands internationally, adds domain-based routing when it handles more diverse queries, and adds safety-sensitive routing after a high-profile mistake. The routing layer must support evolving rule sets without requiring code changes.

**Declarative routing configuration** defines rules as data rather than code. A YAML or JSON file specifies: if user tier is premium and query length exceeds 1000 tokens, route to GPT-5 with extended context. If tenant is Customer B, route to EU-Central region only. If safety classifier confidence exceeds 0.8, route to GPT-5 regardless of tier. The routing layer loads this configuration and interprets it. Engineers add new rules by editing the configuration file, not by deploying new code.

**Rule priority and conflict resolution** handle cases where multiple rules apply. If a premium user from the EU asks a safety-sensitive question, three rules might apply: premium tier routing, EU data residency routing, and safety-sensitive routing. The system needs a priority order. Most teams define rule categories with explicit precedence: compliance rules override quality rules, quality rules override cost rules. Within a category, more specific rules override general rules.

**A/B testing routing rules** applies the same experimental methodology to routing logic that Section 5.7 described for prompt variants. If you want to test whether routing long queries to Claude Opus 4.5 improves quality compared to routing them to GPT-5, split traffic and measure outcomes. A small percentage of long queries route to Claude Opus, the rest route to GPT-5, and the experiment measures quality, latency, and cost for both groups. Winning routing rules graduate from experiment to production configuration.

The routing layer is the control plane for production AI systems. It determines which users get which models, which queries receive premium treatment, which requests failover during outages, and which experiments run on which traffic. Every decision the routing layer makes affects cost, quality, and user experience. Getting routing right is not optional. It is the foundation of reliable, cost-effective, user-aligned AI infrastructure.

