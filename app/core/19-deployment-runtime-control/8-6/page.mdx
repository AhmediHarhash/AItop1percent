# 8.6 — Metadata Versioning: Tracking Provenance and Dependencies

In late 2025, a financial services company faced an urgent problem. Six months earlier, they had deployed a fraud detection model that performed exceptionally well—precision at ninety-four percent, recall at eighty-nine percent, false positive rate low enough to keep customer support teams happy. Then something changed. A competitor launched a new attack pattern. The old model's performance dropped. The team decided to retrain, but they wanted to start from the same foundation that produced the successful version. They opened the model registry. The model file was there. But nobody had documented which training data was used. Nobody had recorded which hyperparameters. Nobody had saved the training code commit hash. The model weights existed, but the knowledge of how to recreate them did not. The team spent three weeks reverse-engineering their own model, trying different dataset versions and hyperparameter combinations until they approximated the original performance. They eventually succeeded, but those three weeks cost the company real fraud losses and engineering time that should have been spent improving the model, not reconstructing it.

The provenance problem is simple to state: you need to know exactly what produced a model. Which dataset. Which base model. Which training code. Which hyperparameters. Which random seed. Which hardware configuration. Without that knowledge, you cannot reproduce the model. You cannot debug it effectively. You cannot explain to regulators or auditors why it behaves the way it does. You cannot confidently retrain it when requirements change. A model without provenance is a black box inside a black box. You know what it does today, but you do not know why, and you cannot reconstruct it tomorrow.

Metadata versioning is the practice of systematically recording and versioning the information that describes how a model was created. The metadata is as important as the model weights themselves. In some regulatory contexts, it is more important. A model that performs well but has no documented lineage is not deployable in healthcare or finance. A model that performs adequately but has complete provenance is defensible in an audit. Metadata versioning turns model creation from an ad-hoc process into a reproducible engineering practice.

## Essential Metadata Fields

Every model needs a core set of metadata fields captured at training time. The minimum viable set includes training data version, training code commit, base model version, hyperparameters, training duration, final validation metrics, and the person or system that initiated training. This is not optional documentation. This is the reproducibility contract. If any of these fields are missing, reproduction becomes guesswork.

Training data version identifies the exact dataset used for training. If your dataset is versioned in a data versioning system, this is a content hash or semantic version like customer-support-v2024-11-15. If your dataset is stored in a data warehouse, this is a query fingerprint or snapshot timestamp. The key requirement is uniqueness. Two different datasets must produce two different identifiers. The same dataset accessed at different times must produce the same identifier if the content is identical. Without this, you cannot determine whether a performance difference between two models is due to code changes or data changes.

Training code commit is the git SHA or equivalent identifier for the exact code that ran the training job. Not the branch name. Not "latest." The specific commit. Training code includes the training loop, data preprocessing logic, model architecture definitions, and hyperparameter configuration. If any of these change, the commit hash changes. When you need to retrain a model, you check out the exact commit that produced the original version. You do not hope that the current main branch still works the same way. You know it works because you are running the exact same code.

Base model version identifies which pretrained model you started from. If you are fine-tuning gpt-5-mini, you need to record not just "gpt-5-mini" but the specific version: gpt-5-mini-20260115. If you are fine-tuning Claude Sonnet 4.5, you need to record the API version and model identifier. If you are fine-tuning an open-source model like Llama 4 Scout, you need to record the Hugging Face commit hash or the download URL with content hash. Base models change over time. Providers release updates. Bugs get fixed. Tokenization schemes evolve. A model trained against January's base model might not reproduce if you use March's base model, even if they have the same name.

Hyperparameters include learning rate, batch size, number of epochs, warmup steps, weight decay, optimizer type, gradient clipping threshold, and any other configuration that affects training dynamics. These values must be recorded exactly as they were used. Not the default values from your config file. The actual values, including any that were overridden at runtime. Some teams serialize the entire hyperparameter object as JSON. Others record each field individually. Either approach works as long as the data is complete and structured for querying.

Training duration and hardware configuration matter for reproducibility. A model trained for six hours on eight A100 GPUs might converge differently than the same model trained for twelve hours on four GPUs, even with identical hyperparameters. Gradient accumulation, distributed training strategies, and hardware-specific floating point behavior all introduce subtle variations. You do not need to record every detail of the hardware, but you do need enough information to reproduce the training environment if exact reproduction is required. Cloud instance type, GPU count, and framework version are usually sufficient.

Final validation metrics provide the baseline for comparison. When you retrain a model, you want to know whether the new version is better or worse than the original. Without recorded metrics, you cannot make that comparison. The metadata should include not just the headline number—precision at ninety-four percent—but the full evaluation suite. Precision, recall, F1, AUC, per-class breakdowns, latency percentiles, cost per inference. The more complete the recorded metrics, the easier it is to detect regressions or improvements.

The creator field records who or what initiated training. A human username if training was manually triggered. A service account if training was automated. A pull request ID if training was triggered by CI. This field is critical for accountability and debugging. When a model behaves unexpectedly, you need to know who to ask. When a model violates policy, you need to know who approved its creation.

## Capture Automation

Manual metadata recording fails. Engineers forget. Fields get skipped. Values get recorded incorrectly. The only reliable approach is automated capture at training time. Your training infrastructure must collect metadata as a side effect of running a training job, not as an optional manual step afterward.

The automation happens at the orchestration layer. When a training job starts, the orchestrator captures the current git commit, queries the data versioning system for the dataset identifier, reads the hyperparameter config, records the instance type and GPU count, and writes all of this to a metadata store before the first training step runs. When the training job completes, the orchestrator appends the final metrics, training duration, and any error logs. The metadata is immutably linked to the model artifact. You cannot create a model without creating its metadata.

Some teams embed metadata directly in the model file. PyTorch models can store arbitrary key-value pairs in the model checkpoint. TensorFlow SavedModels support metadata fields. Hugging Face models include a config.json that can be extended with custom fields. Embedding metadata in the model file ensures they stay together. You cannot accidentally lose the metadata by moving the model to a different storage location. But embedding has downsides. Model files become larger. Querying metadata requires loading the model file or at least parsing its headers. Updating metadata after model creation is difficult or impossible.

The more scalable approach is external metadata storage with strong linking. Each model gets a unique identifier—a content hash or UUID. The metadata is stored in a database keyed by that identifier. The model file includes only the identifier. When you load the model, you query the metadata store to retrieve its provenance. This design allows fast metadata queries without loading model files. It supports metadata updates if you discover errors in the original recording. It enables efficient search across all models based on metadata filters: show me all models trained on dataset X, or all models trained with learning rate less than 0.0001.

The linking must be robust. If the model identifier is wrong or missing, the metadata is orphaned. If the metadata store becomes unavailable, you lose provenance for all models. Production systems use both approaches: essential metadata is embedded in the model file for resilience, and extended metadata is stored in a database for query performance. If the database fails, you can still retrieve basic provenance from the model file. If you need to search across thousands of models, you query the database instead of downloading and parsing every model file.

## Metadata Schema and Versioning

The metadata schema itself needs versioning. As your system matures, you will discover new fields that should have been recorded from the beginning. You will change how certain fields are structured. You will deprecate fields that turned out to be useless. If every model uses a different metadata schema, querying becomes impossible.

The schema should have a version number. Metadata written in 2024 might use schema v1. Metadata written in 2026 might use schema v3. Each schema version defines required fields, optional fields, field types, and validation rules. When you introduce schema v3, you do not delete the v1 and v2 readers. You maintain backward compatibility. Old models retain their original metadata in the original schema. New models use the new schema. Your query layer understands all schema versions and translates them to a common internal representation.

Schema evolution requires careful planning. Adding a new optional field is safe. Adding a new required field breaks compatibility unless you provide a default value for old records. Changing the type or format of an existing field requires a migration. Migrations are expensive. If you have ten thousand models with metadata in schema v1 and you need to migrate them to schema v3, you need to reprocess every metadata record. Some fields can be migrated automatically. Others require manual annotation or recalculation. The safest approach is to design the schema conservatively from the start and add fields incrementally as optional extensions.

## Lineage Tracking

Metadata describes a single model. Lineage describes the chain of artifacts that led to that model. A fine-tuned model has lineage back to its base model. A model trained on a filtered dataset has lineage back to the raw dataset and the filtering code. A model that was retrained from a checkpoint has lineage back to the original checkpoint. Lineage is the graph of dependencies that explains how a model came to exist.

Lineage tracking answers questions like: which models were trained from base model X? Which models used dataset Y? If I discover a bug in preprocessing code version Z, which models are affected? Lineage is harder to capture than simple metadata because it requires tracking relationships across multiple systems. The dataset versioning system knows about datasets. The model registry knows about models. The code repository knows about commits. Lineage ties them together.

The simplest lineage representation is a directed acyclic graph where nodes are artifacts—datasets, models, code commits—and edges represent dependencies. A model node has an edge to the dataset it was trained on, an edge to the base model it was fine-tuned from, and an edge to the code commit that defined its training logic. Traversing the graph backward from a model reveals its full provenance. Traversing forward from a dataset reveals all models that depend on it.

Building the lineage graph requires instrumentation at every step of the pipeline. When a dataset is created, the data processing job records which raw data sources it used. When a model is trained, the training job records which dataset, base model, and code commit it used. When a model is deployed, the deployment system records which model version it used. Each of these steps emits lineage events to a central lineage tracker. The tracker builds and maintains the graph.

Querying lineage is as important as recording it. If you discover that a dataset contained mislabeled examples, you need to find every model trained on that dataset. The query is: find all descendants of dataset:customer-support-v2024-09-10 where node type is model. If you need to reproduce a production incident from three months ago, you need to find the exact model version that was running. The query is: find all models deployed to service:fraud-detection between timestamp T1 and timestamp T2. Lineage queries become complex fast. Graph databases are a natural fit, but they require expertise to operate. Many teams use relational databases with recursive queries or adjacency list representations. The choice matters less than the discipline of recording edges consistently.

## Reproducibility Verification

Recording metadata is necessary but not sufficient. You also need to verify that the metadata is correct—that it actually enables reproduction. Reproducibility verification is the practice of periodically taking an old model, retrieving its metadata, and attempting to recreate it from scratch. If the recreated model matches the original within acceptable tolerances, the metadata is valid. If it does not, the metadata is incomplete or incorrect.

The verification process starts with selecting a model for testing. Pick a model that is at least a few weeks old but still relevant. Retrieve its metadata from the store. Check out the recorded code commit. Download the recorded dataset version. Spin up the recorded hardware configuration. Run training with the recorded hyperparameters. Compare the resulting model against the original. The comparison cannot be exact—floating point nondeterminism and hardware variations guarantee some difference. But the metrics should be close. If the original model had precision at ninety-four percent and the reproduced model has precision at ninety-three point nine percent, the metadata is probably correct. If the reproduced model has precision at seventy-eight percent, something is missing.

Common reproduction failures include missing dataset versions, incorrect base model identifiers, undocumented preprocessing steps, and random seeds that were not recorded. Each failure reveals a gap in your metadata schema. Fix the schema. Add the missing field. Rerun the verification. Iterate until reproduction succeeds. Once verification succeeds for one model, add reproducibility testing to your continuous integration pipeline. Every week, pick a random model from the registry and verify it can be reproduced. If verification fails, alert the team. Investigate and fix the root cause before more models accumulate incorrect metadata.

## Metadata for API-Based Models

Most of the discussion so far assumes you trained the model yourself. But many production systems use API-based models—GPT-5, Claude Opus 4.5, Gemini 3 Pro—where you did not train the model and cannot access its weights. You can still track metadata, but the fields are different.

For API-based models, the essential metadata includes API provider, model identifier, API version, request parameters, and prompt template version. The model identifier must be specific. Not just "gpt-5" but "gpt-5-20260201" or the equivalent versioned endpoint. Providers update models over time. If you record only the model family name, you lose the ability to reproduce behavior when the provider releases a new version. API version matters because providers sometimes introduce breaking changes or behavior shifts between API versions. Request parameters include temperature, top-p, max tokens, frequency penalty, presence penalty, and any other settings that affect output distribution.

Prompt template version is critical. If you are using a model via API, the prompt is your primary control surface. A change to the prompt can shift behavior as much as a change to the model itself. Your prompts should be versioned in the same system you use for code. When you send a request to an API model, record which prompt template version you used. If you discover a quality issue three weeks later, you need to know whether it was caused by a prompt change, a model change, or a data change.

You cannot reproduce API-based model behavior with the same confidence as self-trained models because you do not control the model. But you can at least detect when behavior changes. Log the model version and prompt version for every request. Monitor output distributions over time. If the distribution shifts, check whether the model version changed. If it did, you know the shift is external. If it did not, the shift is likely in your prompt or data.

Some teams record sample outputs as part of the metadata. For each model version and prompt version combination, they run a fixed set of test inputs and save the outputs. When they suspect behavior has changed, they rerun the same inputs and compare outputs. If the outputs differ, behavior has changed. This approach does not explain why, but it provides definitive evidence that a change occurred.

Understanding how to capture, structure, version, and verify the metadata that describes model provenance—from training data to hyperparameters to lineage graphs—is what makes reproduction possible when things go wrong and audits possible when regulators come calling. But metadata about a single model is only part of the picture. Models exist in systems, and systems have dependencies. When one model changes, other parts of the system can break. Tracking those dependencies and understanding the blast radius of a change is the next layer of control.
