# 2.10 — Serverless GPU Platforms: Modal, Replicate, Together AI, Anyscale

Most teams think infrastructure is binary — you either use managed APIs or you run your own GPUs. The reality in 2026 is far more interesting. Serverless GPU platforms occupy the middle ground between fully managed and fully self-hosted, offering model control without infrastructure burden. You deploy your code or your custom model weights, the platform handles GPU provisioning, scaling, and billing. You pay per second of compute, not per instance hour. When traffic drops, your cost drops to zero. When traffic spikes, capacity scales automatically. This model is transforming how teams serve custom models, but each platform has different trade-offs that most teams don't understand until they're locked in.

## What Serverless GPU Actually Means

Traditional GPU infrastructure requires you to provision instances, configure them, deploy your model, and pay for uptime whether you're serving traffic or not. **Serverless GPU flips this model**: you define your inference function as code or container, the platform runs it on-demand, and you pay only for execution time. When a request arrives, the platform allocates a GPU, loads your model, runs inference, and returns the result. When requests stop, the GPU is released. You're billed per second of GPU time, not per hour of instance reservation.

The economics are compelling for workloads with variable traffic. A traditional A100 instance costs around $3 per hour on AWS, which is $2,160 per month if you run it continuously. If your traffic is bursty — active for 6 hours per day, idle for 18 — you're paying $1,620 per month for nothing. Serverless GPU charges you for those 6 hours of usage and nothing for the idle time. Your monthly cost drops to around $540. The savings scale with usage sparsity. The more bursty your traffic, the better serverless economics look.

But this comes with **cold start latency**. When your function hasn't run recently, the platform needs to allocate a GPU, pull your container or model weights, load the model into GPU memory, and then run inference. This cold start can take 5-30 seconds depending on model size and platform. For user-facing applications, that latency is unacceptable. For batch processing, background jobs, or development workflows, it's fine. The key architectural question is whether you can tolerate cold starts or need warm capacity.

Platforms optimize cold starts in different ways. **Container caching** keeps your environment warm for minutes or hours after the last request. **Model weight caching** stores your model weights on fast storage attached to the GPU pool. **Warming strategies** let you keep a minimum number of instances always-on to eliminate cold starts entirely, at the cost of paying for idle capacity. The most sophisticated teams use serverless for batch workloads and reserved capacity for latency-sensitive user-facing requests.

## Modal: Python-Native Developer Experience

Modal is built for Python developers who want GPU access without thinking about infrastructure. You write a Python function, decorate it with `@app.function(gpu="A100")`, and deploy with `modal deploy`. The platform handles everything else — provisioning, scaling, networking, logging. For teams coming from web development who think in functions and decorators, Modal feels like a natural extension of their existing workflow.

The **developer experience** is Modal's primary advantage. You don't write Dockerfiles unless you want to. You don't configure Kubernetes. You don't set up load balancers. You write Python code that looks like any other Python code, and Modal makes it run on GPUs. The feedback loop is fast — you can deploy a new model, test it, iterate, and redeploy in minutes. For experimentation and prototyping, this velocity is unmatched.

Modal's **container caching** is sophisticated. When you deploy a function, Modal builds a container image with your dependencies and caches it. On subsequent invocations, the cached container starts in seconds, not minutes. The platform also supports **volume mounts** for model weights. You upload your 15GB fine-tuned Llama 4 model to a Modal volume once, then mount that volume in your function. Every subsequent cold start skips the weight download and loads directly from the cached volume. This reduces cold starts from 30 seconds to under 10 seconds for large models.

The **billing model** is per-second compute with no idle charges. If your function runs for 2.3 seconds on an A100, you pay for 2.3 seconds. If it doesn't run at all, you pay nothing. This makes Modal extremely cost-effective for intermittent workloads — daily batch jobs, CI/CD eval pipelines, development environments, or low-traffic production APIs. The break-even point compared to reserved instances is typically around 20-30% utilization. If your GPU is idle more than 70% of the time, serverless is cheaper.

The limitations show up at scale. **Cold start latency** is still 5-15 seconds for large models, which rules out user-facing applications that need sub-second response times. **Regional availability** is limited compared to major clouds — Modal runs in a few AWS and GCP regions, not globally. **Vendor lock-in** is real — Modal's decorator-based API is specific to Modal. Porting to another platform means rewriting deployment code. **Cost at sustained high load** can exceed reserved instances. If you're running an A100 at 80% utilization 24/7, a reserved instance is cheaper than per-second billing.

A developer tools company used Modal for their daily evaluation pipeline. Every night at 2am, they ran 5,000 test cases through three different model versions to detect regressions. The job took 90 minutes on an A100. With a reserved instance, they'd pay $2,160 per month for an A100 that was idle 23 hours per day. With Modal, they paid for 90 minutes of compute per day, around $150 per month. The cold start latency didn't matter because the job was asynchronous. The per-second billing saved them $2,000 per month with zero operational overhead.

## Replicate: Model-Centric Deployment

Replicate is built around the **Cog** packaging format. You define your model as a Cog container — a standardized format that specifies model weights, dependencies, and inference interface. You push the container to Replicate, and it becomes an API endpoint. The platform handles serving, scaling, and billing. For teams that want to share models or deploy models from the open-source community, Replicate is the fastest path to production.

Replicate's **model marketplace** is its differentiating feature. Thousands of pre-packaged models are available as instant APIs — Stable Diffusion, Whisper, Llama, SAM, ControlNet. You don't need to set up infrastructure or download weights. You call an API endpoint and get results. For prototyping or integrating commodity models, this is the fastest option. You can go from idea to working prototype in an afternoon.

The **Cog packaging standard** makes models portable. A Cog container runs identically on Replicate, on your laptop, or on any other platform that supports Docker. This portability reduces lock-in compared to platform-specific APIs like Modal's decorators. If you decide to move off Replicate, your Cog container is 80% of the migration work already done. You just need a different serving layer.

Replicate's billing is **per-prediction** rather than per-second. For image generation, you might pay $0.02 per image. For text inference, you pay per token generated. This makes pricing predictable for small-scale use cases, but it can become expensive at volume. The pricing structure is optimized for occasional use and prototyping, not for high-throughput production systems.

The limitations are similar to Modal's. **Cold start latency** is 5-20 seconds depending on model size. **Sustained high-volume workloads** often cost more on Replicate than self-hosting. **Customization** is limited to what Cog supports — you can't deeply optimize inference or integrate complex preprocessing pipelines. Replicate is best for deploying standard models quickly, not for building highly optimized custom serving systems.

A design tool startup used Replicate to add AI image generation to their product. They integrated Stable Diffusion XL via Replicate's API in two days, with zero infrastructure work. The per-image pricing was $0.015, which translated to around $3,000 per month at their usage volume. When they compared this to self-hosting, they calculated it would take eight months of engineering time to build equivalent infrastructure and three months to break even on costs. For their stage and scale, Replicate was the obvious choice. They deferred infrastructure complexity until growth demanded it.

## Together AI: Inference-First Platform

Together AI focuses on **inference at scale** for both open-source and custom models. The platform provides API access to dozens of popular models — Llama 4, Mistral Large 3, Qwen, DeepSeek — with competitive pricing and high throughput. You can also upload your own fine-tuned model weights and serve them through the same infrastructure. For teams that need custom model serving without managing GPUs, Together AI offers a middle ground between fully managed APIs and full self-hosting.

The **inference API** is designed for simplicity. You send a standard OpenAI-compatible request to Together's endpoint, specifying the model name. The platform routes to the appropriate model, runs inference, and returns results. The API compatibility layer makes it easy to switch between Together-hosted models and OpenAI or Anthropic APIs by changing the base URL. This portability is valuable for teams that want to avoid vendor lock-in.

Together AI supports **fine-tuning as a service**. You upload your dataset, select a base model, and Together handles the training. Once training completes, the fine-tuned model is automatically deployed to their inference infrastructure. This integrated workflow — fine-tune then deploy on the same platform — eliminates the complexity of moving model weights between training and serving environments. For teams without ML infrastructure expertise, this end-to-end platform reduces the barrier to custom models.

The **pricing model** is per-token, similar to OpenAI or Anthropic, but typically 30-60% cheaper for open-source models. Llama 4 inference on Together might cost $0.20 per million tokens, compared to $1.50 per million tokens for GPT-5-mini on OpenAI. For high-volume workloads using open-source models, this pricing difference can save tens of thousands of dollars per month. The trade-off is that you're responsible for prompt engineering and model selection — Together doesn't provide the same level of model quality guarantees as OpenAI or Anthropic.

The limitations are mostly about **flexibility**. You can't control the serving infrastructure. You can't optimize batching or quantization. You can't run inference on-premise. Together AI is a managed service that abstracts infrastructure entirely, which is convenient but limiting for teams that need deep control. It's best for teams that want to use open-source models without self-hosting and are willing to trade control for simplicity.

A legal tech company used Together AI to serve their fine-tuned Llama 4 model for contract clause extraction. They fine-tuned on Together's platform using 50,000 labeled examples, then deployed the model to Together's inference API. The combined cost for fine-tuning and serving was around $4,000 per month at their usage volume. Self-hosting would have required hiring a platform engineer, procuring GPUs, and building deployment tooling — at least six months of work and $200,000 in labor cost. Together AI let them go from dataset to production in three weeks with a small team.

## Anyscale: Ray Ecosystem for Complex Pipelines

Anyscale is built on **Ray**, the distributed computing framework. While Modal and Replicate focus on simple function deployment, Anyscale supports complex multi-stage pipelines — preprocessing, inference, post-processing, ensemble models, multi-turn agents. If your serving logic involves orchestrating multiple models, transforming data between stages, or managing stateful workflows, Anyscale provides primitives that the other platforms lack.

The **Ray integration** means you can use Ray's task and actor APIs to build sophisticated serving logic. You might have one task that preprocesses input, one actor that batches requests for efficient GPU utilization, one task per model in an ensemble, and one task that aggregates results. Anyscale handles distributing these tasks across a cluster, scaling capacity up and down, and managing dependencies. For teams already using Ray for training or batch inference, deploying to Anyscale is a natural extension.

Anyscale supports both **training and inference** on the same platform. You can fine-tune a model using Ray Train, then deploy it using Ray Serve, without leaving Anyscale's infrastructure. This integrated workflow is valuable for teams that iterate frequently between training and deployment. The same platform, same APIs, same billing — no friction moving from experimentation to production.

The **enterprise features** are Anyscale's differentiator. Multi-tenancy, private clusters, VPC integration, SSO, audit logging, cost allocation by team — these are critical for large organizations but missing from the other serverless platforms. Anyscale is the only serverless GPU platform designed for enterprises that need governance and compliance.

The limitations are **complexity and cost**. Anyscale has a steeper learning curve than Modal or Replicate. You need to understand Ray's programming model, which is more complex than decorating a Python function. The pricing is also higher — Anyscale is positioned as an enterprise platform, not a developer tool. For small teams or simple use cases, Modal or Replicate are easier and cheaper. For large organizations with complex pipelines and governance requirements, Anyscale is often the only serverless option that meets their needs.

A financial services firm used Anyscale to deploy a multi-model fraud detection pipeline. The system preprocessed transactions with one model, scored them with three different risk models, and aggregated results with a final ensemble model. The entire pipeline was built in Ray Serve and deployed on Anyscale's infrastructure. The platform handled scaling each model independently based on load, which would have been operationally complex to build on Kubernetes. The enterprise features — VPC integration, audit logs, SOC 2 compliance — were requirements they couldn't meet with the other serverless platforms.

## When to Use Serverless GPU: The Decision Map

Serverless GPU makes sense for **variable traffic with significant idle periods**. If your workload runs for a few hours per day and is idle the rest of the time, serverless saves money compared to reserved instances. If your traffic spikes unpredictably and you need to scale from zero to 100 GPUs and back to zero, serverless provides that elasticity without over-provisioning.

**Development and testing environments** are ideal for serverless. Your CI/CD pipeline runs eval suites on every commit. Developers spin up model instances to test changes. These workloads are intermittent and latency-tolerant. Serverless gives you on-demand GPU access without paying for idle capacity. The cost savings can be 70-90% compared to keeping dedicated GPUs online for development.

**Low-volume production** is another strong fit. If you're serving 10,000 requests per day with minutes of idle time between bursts, serverless is cheaper than a dedicated instance. The cold start latency might be acceptable if your application can tolerate 5-10 second delays on the first request after idle periods. The subsequent requests benefit from warm containers and have latency comparable to dedicated infrastructure.

Serverless does not work well for **user-facing applications that need sub-second latency**. Cold starts kill the experience. Even with container caching, the first request after an idle period takes seconds. For interactive applications — chatbots, real-time assistants, live content generation — you need warm capacity. Serverless platforms support keeping minimum instances warm, but at that point you're paying for reserved capacity and losing the serverless cost advantage.

Serverless also struggles with **sustained high throughput**. If you're running GPUs at 70% utilization or higher, the per-second billing becomes more expensive than reserved instances. The break-even point is typically 20-30% utilization. Above that, reserved instances or self-hosted infrastructure is cheaper. The teams that win with serverless are the ones who use it for the workloads it's designed for — bursty, intermittent, latency-tolerant — and use dedicated capacity for everything else.

## Cost Modeling: When Serverless Beats Dedicated

The cost comparison starts with **utilization**. A reserved A100 on AWS costs around $3 per hour, or $2,160 per month. Serverless GPU on Modal costs around $3 per hour of compute time, but you only pay for actual execution. If your workload uses the GPU 30% of the time, you pay $648 per month on serverless versus $2,160 on reserved. The savings are $1,512 per month, or 70%.

But you need to account for **cold start cost**. If your workload is called 1,000 times per day and each call takes 10 seconds, but 100 of those calls incur 10-second cold starts, you're paying for 20,000 seconds per day instead of 10,000. Cold starts double your compute cost. The optimization is to minimize cold starts through container caching, model preloading, and traffic patterns that keep containers warm.

The **break-even calculation** depends on your specific traffic pattern. For a workload that runs 8 hours per day at full utilization, reserved instances are cheaper. For a workload that runs 2 hours per day with gaps, serverless is cheaper. The crossover point is around 25-30% sustained utilization. Below that, serverless wins. Above that, dedicated wins. The critical insight is that most teams don't measure utilization and make decisions based on peak load instead of average load.

The migration path is **gradual**. Start with serverless for batch jobs and development environments where cold starts don't matter. Measure actual utilization and cost. If the math works, expand to low-volume production endpoints. Reserve dedicated capacity for high-volume, latency-sensitive workloads. The optimal architecture is usually hybrid — serverless for 80% of endpoints that have variable traffic, dedicated for the 20% that drive most of the load.

By 2026, serverless GPU platforms have matured from experimental tools to production infrastructure. Modal dominates for developer velocity. Replicate leads for model marketplaces and fast prototyping. Together AI wins for cost-effective open-source model serving. Anyscale serves enterprises with complex pipelines. The teams that succeed understand which platform fits which workload, and they build abstraction layers that let them move between platforms as their needs evolve.

---

The next subchapter presents a systematic framework for choosing serving infrastructure based on volume, latency, cost, and customization requirements.
