# 4.11 — Dynamic Rate Limiting: Adjusting Limits Based on System Load

Fixed rate limits are simple to implement and simple to understand. Every user gets 100 requests per minute. The limit never changes. This simplicity is also a weakness. When the system is quiet at 3 AM with 10 percent GPU utilization, the limit of 100 requests per minute is artificially conservative. The system has capacity for 500 requests per minute, but users are artificially constrained. When the system is overloaded at 2 PM with 95 percent GPU utilization and requests queuing for 30 seconds, the limit of 100 requests per minute is dangerously generous. The system cannot handle the load, but users keep sending requests. Fixed limits waste capacity during quiet times and fail to protect the system during peak times. Dynamic rate limiting solves both problems by adjusting limits in real time based on system load. The limits expand when capacity is available and contract when the system is under stress. This maximizes utilization without sacrificing stability.

Dynamic rate limiting is the practice of modifying rate limits automatically based on current system conditions. The system monitors load signals like queue depth, latency, GPU utilization, and error rate. When load is low, limits increase. When load is high, limits decrease. This creates a feedback loop. High load triggers limit reductions, which reduce incoming requests, which reduce load. Low load triggers limit increases, which invite more requests, which increase utilization. The system self-regulates, keeping load in a safe zone while maximizing throughput.

## Why Dynamic Limits

Fixed limits force a choice between two bad outcomes. Set the limit too high and the system overloads during peak usage. Requests queue for minutes. Latency spikes. Error rates climb. Users experience degraded service. The system may crash entirely under sustained overload. Set the limit too low and the system underutilizes capacity during off-peak hours. Users experience artificial scarcity. They wait unnecessarily. The platform leaves revenue on the table. Capacity that could serve requests sits idle.

Dynamic limits eliminate this trade-off. During quiet hours, limits increase. Users who would have been rate-limited under fixed limits can now issue requests. They experience better service. The platform generates more revenue. Spare capacity is monetized instead of wasted. During peak hours, limits decrease. Users who would have overloaded the system under fixed limits are now throttled. The system remains stable. Latency stays acceptable. Error rates stay low. The few users who are rate-limited experience a predictable error instead of an overloaded, unresponsive system.

Dynamic limits also adapt to unexpected events. A sudden traffic spike from a viral post, a coordinated attack, or a misconfigured client can overwhelm fixed limits. The system has no defense. With dynamic limits, the spike triggers an immediate limit reduction. The system sheds load before it becomes overloaded. This automatic adaptation prevents outages that would require manual intervention with fixed limits.

## Load Signals for Adjustment

The system needs measurable signals to decide when to adjust limits. Four signals are most useful. First, queue depth. The number of requests waiting for execution. A healthy system has a shallow queue, measured in single-digit requests. A stressed system has a deep queue, measured in hundreds or thousands of requests. When queue depth exceeds a threshold, lower limits. When queue depth falls below a threshold, raise limits. Queue depth is a direct measure of system capacity. It responds quickly to changes in load.

Second, latency percentiles. Specifically, p95 and p99 latency. These percentiles capture the experience of the slowest requests, which are the first to degrade under load. A healthy system maintains p95 latency below 500 milliseconds and p99 latency below 1,000 milliseconds. A stressed system sees p95 latency climb to 2 seconds and p99 latency climb to 5 seconds. When latency percentiles exceed thresholds, lower limits. When they fall comfortably below thresholds, raise limits. Latency is a user-facing signal. High latency means users are experiencing slow service, which justifies limiting new requests.

Third, GPU utilization. For GPU-bound workloads, utilization is a direct measure of capacity. A GPU at 60 percent utilization has room for more requests. A GPU at 98 percent utilization is saturated. When utilization exceeds 90 percent, lower limits. When utilization falls below 70 percent, raise limits. GPU utilization is a resource signal. It tells you whether the hardware can handle more load.

Fourth, error rate. The percentage of requests that fail. A healthy system has an error rate below 0.5 percent. A stressed system has an error rate above 2 percent. Errors indicate that the system is dropping requests, timing out, or otherwise failing to serve users. When error rate spikes, lower limits immediately. When error rate returns to baseline, raise limits cautiously. Error rate is a failure signal. It indicates that the system is beyond its safe capacity.

These four signals together provide a comprehensive view of system health. The system calculates a composite health score from all four signals. If any signal is in the danger zone, health is low. If all signals are in the safe zone, health is high. Limits adjust based on the health score.

## Dynamic Adjustment Strategies

The system has three strategies for adjusting limits. First, multiplicative adjustment. Scale the current limit by a factor. If health is high, multiply the limit by 1.2, increasing it by 20 percent. If health is low, multiply the limit by 0.8, decreasing it by 20 percent. Multiplicative adjustment is proportional. A limit of 100 adjusts to 120 or 80. A limit of 1,000 adjusts to 1,200 or 800. The adjustment magnitude scales with the current limit. This strategy is stable and predictable.

Second, additive adjustment. Add or subtract a fixed number from the current limit. If health is high, add 10. If health is low, subtract 10. Additive adjustment is absolute. A limit of 100 adjusts to 110 or 90. A limit of 1,000 adjusts to 1,010 or 990. The adjustment magnitude is constant regardless of the current limit. This strategy is simple but can cause problems. A fixed adjustment of 10 is 10 percent of a limit of 100 but only 1 percent of a limit of 1,000. The adjustment impact varies wildly.

Third, threshold-based adjustment. Define discrete health levels and corresponding limit levels. Health level 1 corresponds to limit level 1. Health level 2 corresponds to limit level 2. When health changes levels, the limit jumps to the new level. For example, health levels might be: critical (queue depth greater than 1,000), stressed (queue depth between 500 and 1,000), moderate (queue depth between 100 and 500), healthy (queue depth less than 100). Corresponding limit levels might be: 50 requests per minute, 100 requests per minute, 150 requests per minute, 200 requests per minute. This strategy creates clear, discrete steps. It is easy to reason about but can cause abrupt changes.

Multiplicative adjustment is the most common and most stable strategy. It adjusts proportionally, avoids abrupt jumps, and works across a wide range of limit values. Most production systems use multiplicative adjustment with damping to prevent oscillation. Damping means limiting the rate of change. Instead of adjusting by 20 percent every 10 seconds, adjust by 5 percent every 30 seconds. Slower adjustments reduce the risk of overreaction.

## Raising Limits During Quiet Hours

When the system is quiet, limits should increase. More capacity is available. Better user experience is possible. The system should aggressively expand limits during off-peak hours to maximize utilization. But expansion must be controlled. Raising limits too quickly can cause instability. A sudden flood of requests can overwhelm the system before limits can react. Expansion should be gradual and monitored.

Start by identifying quiet hours. Analyze historical load data. Most systems have predictable quiet periods. Weeknights from midnight to 6 AM. Weekends. Holidays. During these periods, load is consistently low. The system can preemptively raise limits at the start of quiet hours and preemptively lower them at the end. This predictive adjustment is smoother than reactive adjustment. The system adjusts before load changes, not after.

During quiet hours, the system monitors load signals continuously. If load remains low, limits increase incrementally. Every 60 seconds, if queue depth is less than 10, latency is below 300 milliseconds, and GPU utilization is below 60 percent, raise the limit by 10 percent. This gradual expansion continues until either load increases or the limit reaches a predefined ceiling. The ceiling prevents runaway expansion. Even during the quietest hour, the limit does not exceed 500 requests per minute for a user who normally has a 100-request-per-minute limit. The ceiling is a safety mechanism.

Higher limits during quiet hours improve user experience for users who work off-peak. Developers in different time zones, batch jobs scheduled for overnight, users who prefer working late. These users benefit from expanded capacity. They experience fewer rate limit errors. Their requests complete faster. The platform generates more revenue from the same infrastructure. This is a win for everyone.

## Lowering Limits During Peak Hours

When the system is stressed, limits must decrease. The system must shed load to protect stability. Lowering limits is more dangerous than raising them. Lowering limits degrades user experience. Users who were successfully issuing requests suddenly cannot. This creates frustration. The system must lower limits decisively but not excessively. The goal is to reduce load to a safe level, not to block all requests.

The system detects stress via load signals. When queue depth exceeds 500, p95 latency exceeds 1,000 milliseconds, GPU utilization exceeds 90 percent, or error rate exceeds 1 percent, the system is under stress. The system immediately lowers limits. The first reduction is large, typically 30 percent. This aggressive reduction sheds load quickly. Gradual reductions are too slow. The system would continue to degrade while limits slowly contracted.

After the initial reduction, the system monitors load. If load remains high, reduce limits again by 20 percent. If load falls to acceptable levels, hold the limit steady. Do not immediately raise the limit. Wait for sustained evidence that the system has stabilized. Premature expansion causes oscillation. The system lowers limits, load drops, the system raises limits, load spikes again, the system lowers limits again. This oscillation is worse than holding limits low. Users experience unpredictable behavior.

When lowering limits, prioritize paid users over free users. Paid users have an expectation of service. Free users do not. If the system must shed load, shed free-tier load first. Lower free-tier limits by 50 percent and paid-tier limits by 20 percent. This ensures that paying customers experience minimal impact while still reducing overall load. Tiered limit reductions are a business decision, not just a technical decision.

## Per-Tier Dynamic Adjustment

Not all users should experience the same dynamic adjustments. Premium users paid for guaranteed capacity. Free users did not. When limits adjust dynamically, the adjustment magnitude should vary by tier. Premium users experience smaller adjustments. Free users experience larger adjustments. This tiering protects revenue and ensures that paying customers receive the service they paid for.

For example, during peak load, premium limits might decrease by 10 percent while free limits decrease by 50 percent. During off-peak hours, premium limits might increase by 20 percent while free limits increase by 100 percent. The premium user's limit fluctuates between 90 and 120 requests per minute. The free user's limit fluctuates between 25 and 100 requests per minute. Both users benefit from dynamic limits, but the free user experiences more volatility.

This tiering requires careful communication. Free users must understand that their limits are dynamic and may decrease during peak usage. The terms of service should state this explicitly. Premium users must understand that their limits are more stable but not entirely fixed. Even premium users experience modest adjustments during extreme load. Transparency prevents surprise and frustration.

## Implementation Patterns

Dynamic rate limiting requires centralized coordination. A single controller monitors load signals and calculates new limits. The controller updates a configuration service with the new limits. All API servers poll the configuration service every 10 to 30 seconds and apply the new limits. This pull model is simple and resilient. If the controller fails, API servers continue to enforce the last-known limits. The system degrades to fixed rate limiting, which is acceptable.

Alternatively, the controller can push updates to API servers. When limits change, the controller sends a message to all API servers via a message queue or pub-sub system. API servers update limits immediately upon receiving the message. This push model is faster but more complex. If the message queue fails, API servers may not receive updates. The system must handle this failure mode gracefully, falling back to the last-known limits.

The controller itself must be resilient. It should run as multiple replicas for redundancy. If one replica fails, others continue. The replicas must coordinate to avoid conflicting limit updates. Typically, only one replica is active at a time, with others in standby. The active replica makes decisions. If it fails, a standby takes over. This active-standby pattern is simpler than active-active coordination.

## Safety Guardrails

Dynamic limits need safety guardrails to prevent pathological behavior. First, a minimum limit floor. Limits should never fall below a certain value, even under extreme load. If the normal limit is 100 requests per minute, the floor might be 20 requests per minute. The system never reduces limits below 20. This ensures that users retain some capacity even during the worst conditions. A limit of zero is effectively a service outage. The floor prevents that.

Second, a maximum limit ceiling. Limits should never exceed a certain value, even during the quietest hours. If the normal limit is 100 requests per minute, the ceiling might be 500 requests per minute. The system never increases limits above 500. This prevents runaway expansion. Even if load is zero, the system does not grant unlimited capacity. The ceiling protects against bugs in the controller logic and against sudden load spikes that could overwhelm an overly generous limit.

Third, rate-of-change limits. Limits should not change too quickly. A limit should not jump from 100 to 200 in one adjustment. Rapid changes cause instability. The rate-of-change limit might be 20 percent per minute. If the current limit is 100, the next adjustment can bring it to at most 120 or at least 80. Even if the controller calculates that the optimal limit is 300, the system increases gradually from 100 to 120 to 144 to 173 to 207, taking several minutes to reach the target. Gradual changes are safer than abrupt changes.

Fourth, human override capability. Operators must be able to override dynamic limits manually. During an incident, the operator may need to force limits to a specific value to stabilize the system. The override should be simple and fast. A command-line tool or dashboard button that sets limits immediately. The override takes precedence over the controller's automatic adjustments. When the incident is resolved, the operator disables the override and returns control to the automatic system.

## Observability

Dynamic rate limiting is invisible to users until it affects them. Observability is essential to understand how limits are changing and why. The system must track effective limits over time. A time-series graph of the limit value for each user tier. The graph shows how limits fluctuate throughout the day. Operators can see that limits rise at night and fall during peak hours. This visibility confirms that dynamic limiting is working as designed.

The system must correlate limit changes with load metrics. A dashboard that shows queue depth, latency, GPU utilization, error rate, and current limits on the same graph. When queue depth spikes, limits should drop. When latency falls, limits should rise. If limits change without corresponding load changes, something is wrong. The controller logic may be buggy. The load signals may be incorrect. Correlation is the diagnostic tool.

The system must alert on unexpected adjustments. If limits drop by 50 percent in one minute, something unusual is happening. The system may be under attack. A misconfigured client may be flooding requests. A code bug may have introduced a performance regression. The alert triggers investigation. Operators can determine whether the limit reduction was appropriate or whether the system needs manual intervention.

The system must log all limit adjustments. Each adjustment includes the timestamp, the old limit, the new limit, the reason for adjustment, and the load signals that triggered it. These logs are searchable. When a user complains about rate limiting, the operator can查看 the logs to see exactly when the limit was reduced and why. This transparency builds trust. The user sees that the limit reduction was not arbitrary but was a response to measurable system stress.

Dynamic rate limiting is more complex than fixed limiting. The complexity is justified by the benefits: better utilization during quiet times, better protection during peak times, and automatic adaptation to changing conditions. Teams that implement dynamic limiting successfully see higher throughput, more stable latency, and fewer support tickets about rate limiting. The system uses its capacity efficiently and protects itself from overload without constant manual tuning.

