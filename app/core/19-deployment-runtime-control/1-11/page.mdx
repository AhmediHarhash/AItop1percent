# 1.11 — The 2026 AI Deployment Stack: Tools, Patterns, and Expectations

Most teams think the hard part is training the model. They are wrong. The hard part is serving it reliably at scale, updating it safely, routing traffic intelligently, and recovering gracefully when it breaks. The 2026 AI deployment stack has evolved into a set of well-understood layers, each solving a specific piece of the deployment problem. If you are building production AI in 2026 and your stack does not include these layers, you are either over-engineering or under-engineering — and both are expensive mistakes.

The deployment stack is not monolithic. It is a set of composable layers that you assemble based on your scale, your risk profile, and your team's capabilities. A 10-person startup serving 5,000 users needs a different stack than a 500-person company serving 50 million users. But both need the same fundamental layers: serving, orchestration, gateway, configuration, CI/CD, and observability. The difference is how sophisticated each layer needs to be.

## Serving Layer: vLLM, TGI, TensorRT-LLM, and When to Use Each

The serving layer is where your model turns into an API. In 2026, there are four dominant serving frameworks, each optimized for different constraints. Choosing the wrong one costs you latency, throughput, or infrastructure spend. Choosing the right one makes everything downstream easier.

vLLM is the default choice for high-throughput inference with large language models. It implements PagedAttention, which dramatically reduces memory waste during batching, and continuous batching, which keeps GPUs saturated by processing requests as they arrive rather than waiting for fixed batch boundaries. A SaaS company serving 200,000 requests per day switched from a naive PyTorch serving setup to vLLM and saw throughput increase by 3x on the same hardware. vLLM works with most Hugging Face models out of the box, supports distributed inference across multiple GPUs, and integrates cleanly with OpenAI-compatible APIs. If you are serving LLMs at scale and don't have a specific reason to use something else, start with vLLM.

TGI — Text Generation Inference from Hugging Face — is the choice when you need maximum compatibility with Hugging Face ecosystem models and want production-ready serving without extensive configuration. TGI handles tokenization, batching, and quantization automatically, supports streaming responses, and includes built-in safeguards like token limits and request timeouts. It is slightly less performant than vLLM for pure throughput, but it is easier to configure and maintain. A developer tools company uses TGI for their code generation API because they frequently swap between different Hugging Face models — CodeLlama, StarCoder, DeepSeek Coder — and TGI makes model swapping a one-line config change. If your deployment cadence involves frequent model experimentation, TGI's ease of use justifies the small performance trade-off.

TensorRT-LLM is the choice when you are serving on NVIDIA GPUs and need maximum performance. It compiles models into optimized TensorRT engines that exploit every available hardware optimization — kernel fusion, quantization, flash attention, tensor parallelism. A fintech company serving fraud detection inference at sub-20ms latency uses TensorRT-LLM because the compilation overhead is worth it for the latency reduction. But TensorRT-LLM has a steep learning curve, requires model-specific optimization, and ties you to NVIDIA hardware. Only choose it if latency is your primary constraint and you have the engineering capacity to maintain the complexity.

Triton Inference Server is the choice when you need to serve multiple model types — not just LLMs but also embedding models, rerankers, classifiers, traditional ML models — from a single infrastructure layer. Triton supports TensorRT, PyTorch, ONNX, and TensorFlow backends, handles dynamic batching across all of them, and provides a unified API. An e-commerce company uses Triton to serve a hybrid AI stack: a Llama 4 Maverick-based product recommendation model, a BERT-based search embedding model, and a gradient-boosted-tree fraud classifier. All three models run in the same Triton deployment, sharing GPU memory and batching infrastructure. If your AI system is not purely LLM-based, Triton gives you a single serving layer instead of maintaining three separate ones.

Ray Serve is the choice when you need custom orchestration that the standard serving frameworks don't support — multi-step inference pipelines, dynamic model composition, complex routing logic. Ray Serve lets you define arbitrary Python functions as servable endpoints and handles the distribution, scaling, and lifecycle management. A content moderation platform uses Ray Serve to orchestrate a three-stage pipeline: first-stage LLM classifies content risk, second-stage embedding model checks similarity to known violating content, third-stage LLM generates an explanation. The three models run on different hardware with different scaling policies, and Ray Serve handles the coordination. But Ray Serve requires more operational expertise than the standard frameworks. Only choose it if your serving requirements are complex enough to justify the overhead.

The pattern in 2026: use vLLM for standard LLM serving, use TGI if you need Hugging Face ecosystem integration, use TensorRT-LLM if latency is critical and you have the expertise, use Triton if you are serving heterogeneous models, use Ray Serve if your orchestration logic is custom. Most teams use vLLM.

## Orchestration Layer: Kubernetes, GPU Scheduling, and Autoscaling

The orchestration layer determines how your serving containers run, scale, and fail over. In 2026, Kubernetes has won this layer for any team operating at meaningful scale. The question is not whether to use Kubernetes, but how to configure it for GPU workloads, which have different constraints than CPU workloads.

GPU-aware scheduling is the first requirement. Standard Kubernetes treats GPUs as opaque resources — you request one GPU, you get one GPU. But not all GPUs are equivalent. An H100 has 3x the memory and 4x the throughput of an A100. A model that fits on an H100 might not fit on an A100. Your scheduler needs to understand GPU types and allocate workloads to appropriate hardware.

The NVIDIA GPU Operator and the node selector pattern solve this. You label your Kubernetes nodes with GPU type — nvidia.com/gpu.product equals H100 or nvidia.com/gpu.product equals A100 — and your deployment manifests specify required GPU types via node selectors. When you deploy a large model that requires H100 memory, Kubernetes schedules it only on H100 nodes. When you deploy a smaller model that runs fine on A100, Kubernetes can schedule it on either, preferring cheaper hardware. A cloud AI platform runs this pattern with four GPU types in their cluster. Large models go to H100 nodes, medium models go to A100 nodes, small models go to T4 nodes. The scheduler handles placement automatically based on node labels.

GPU bin packing is the second requirement. GPUs are expensive, and running them at 40% utilization is waste. If you have a model that uses 30GB of an 80GB H100, you should be able to colocate other workloads on the same GPU to fill the remaining 50GB. Kubernetes does not do this by default — it treats GPUs as non-divisible resources. You need either fractional GPU support via the NVIDIA MIG feature, which partitions a single GPU into isolated slices, or you need multi-tenancy at the serving layer, where vLLM or TGI runs multiple models on the same GPU and handles isolation in software.

A SaaS company runs multi-tenant vLLM to achieve GPU bin packing. They deploy one vLLM instance per GPU, and that instance serves 3-5 different fine-tuned models that share the base model weights. The models are LoRA adapters that add minimal memory overhead, so five models fit comfortably in the memory budget of one H100. GPU utilization increased from 35% to 78% after implementing multi-tenancy, which reduced their infrastructure spend by $18,000 per month.

Autoscaling for AI workloads is the third requirement. Standard Kubernetes HPA — Horizontal Pod Autoscaler — scales based on CPU or memory utilization, but AI workloads are bottlenecked by GPU, not CPU. You need to scale based on GPU utilization or, better, based on queue depth and time-to-first-token.

Queue-depth-based autoscaling works like this: your serving layer exposes a metric for pending requests. When the queue depth exceeds a threshold — say 20 pending requests — Kubernetes spins up additional serving pods. When the queue drains, Kubernetes scales back down. This is more responsive than utilization-based scaling because it reacts to demand directly rather than waiting for GPUs to saturate.

A customer support AI platform uses queue-depth autoscaling with a 15-request threshold. Under normal load, they run 3 vLLM pods. When support volume spikes — which happens predictably during business hours and unpredictably during product incidents — the queue fills, autoscaling triggers, and they scale to 12 pods within 90 seconds. When the spike subsides, they scale back down. The autoscaling eliminates the need to overprovision for peak load, which saves $40,000 per month in GPU costs.

Time-to-first-token autoscaling is even better. Instead of scaling based on queue depth, you scale based on user-experienced latency. If TTFT exceeds your SLA — say 800ms — you scale up. If TTFT is well below your SLA, you scale down. This directly optimizes for user experience rather than for infrastructure metrics.

Node pools are the fourth requirement. You do not want to run batch inference jobs on the same nodes as real-time serving, because batch jobs spike GPU utilization and starve serving requests. You create separate node pools — one for real-time serving, one for batch jobs, one for training — and use Kubernetes taints and tolerations to enforce isolation. Serving workloads only run on serving nodes, batch workloads only run on batch nodes. This prevents resource contention and makes capacity planning straightforward.

## Gateway Layer: API Proxying, Routing, and Rate Limiting

The gateway layer sits between your users and your serving layer. It handles routing, rate limiting, quota enforcement, caching, and observability. In 2026, most teams use either a commercial gateway like Helicone or Portkey, or they build a custom gateway for routing logic that commercial tools don't support.

Helicone and Portkey are the dominant commercial gateways. They are OpenAI-compatible proxies that you point your API calls at, and they handle logging, caching, rate limiting, and cost tracking automatically. A developer tools company uses Helicone to log all LLM requests without modifying their application code. They changed their OpenAI base URL from api.openai.com to their Helicone endpoint, and every request now gets logged to Helicone's dashboard with full request and response bodies, token counts, latency, and cost. They use the logs for debugging, for cost analysis, and for building eval datasets. The integration took 10 minutes. The value is ongoing.

Portkey adds advanced routing on top of logging. You can define fallback chains — try GPT-5 first, fall back to Claude Opus 4.5 if GPT-5 is rate-limited, fall back to Llama 4 Maverick if both are unavailable. You can define load balancing — split traffic 50/50 between two models to compare performance. You can define conditional routing — use GPT-5 for complex queries, use GPT-5-mini for simple ones. A fintech platform uses Portkey to route 80% of their support queries to a fine-tuned Llama 4 model and 20% to GPT-5 as a quality baseline. The routing logic is defined in Portkey's config, not in application code, which makes it easy to adjust without redeploying the application.

Custom gateways are necessary when your routing logic is too complex for commercial tools. A healthcare AI company built a custom gateway that routes clinical queries based on patient risk score, query complexity, and model confidence. High-risk patients always go to the most expensive, most accurate model. Low-risk patients with simple queries go to the cheapest model. Medium-risk patients go to a medium-capability model, but if the model's confidence is below a threshold, the query gets escalated to the high-capability model. This multi-dimensional routing logic is business-critical and too specific to delegate to a commercial tool. They built the gateway in Go, deployed it on Kubernetes in front of their vLLM serving layer, and it handles 300,000 requests per day with sub-5ms overhead.

Rate limiting and quota enforcement are essential gateway functions. You do not want a single user or a single customer to consume all your inference capacity and starve everyone else. The gateway enforces per-user, per-customer, and per-endpoint rate limits. A B2B AI platform enforces tiered rate limits: free tier gets 100 requests per day, pro tier gets 10,000 requests per day, enterprise tier gets unlimited requests but with burst limits to prevent runaway usage. The gateway tracks request counts per customer and returns HTTP 429 errors when limits are exceeded. This prevents abuse and ensures fair resource allocation.

Caching is the underutilized gateway function. Many AI queries are repeated — users ask the same questions, search for the same documents, generate the same summaries. If you cache responses at the gateway layer, you can serve repeated queries from cache without hitting the model, which reduces latency to near-zero and reduces inference cost to zero. A developer docs platform caches LLM responses at the gateway with a 24-hour TTL. 40% of their queries are cache hits, which saves them $8,000 per month in inference costs and reduces average latency from 600ms to 80ms for cached queries.

## Configuration Layer: Feature Flags, Prompt Registries, Model Registries

The configuration layer controls what gets deployed without requiring code changes. You change a feature flag, a prompt, or a model version, and the serving layer picks up the change dynamically. This is essential for safe deployments because it decouples code deploys from model deploys.

Feature flag systems like LaunchDarkly and Statsig are the standard for controlling rollouts. You wrap new AI features behind feature flags, deploy the code to production with the flag off, then gradually enable the flag for increasing percentages of traffic. If something goes wrong, you turn the flag off — no code deploy, no rollback, just a config change that takes effect in seconds.

A SaaS company uses LaunchDarkly to control all AI feature rollouts. They deployed a new GPT-5-powered email composer, but initially enabled it for 0% of users. Then 1%. Then 5%. Then 10%. They monitored eval metrics, user engagement, and error rates at each step. At 10%, they noticed that long emails were getting truncated. They paused the rollout, fixed the truncation bug, and resumed. By the time they reached 100%, they had caught and fixed three issues that would have caused major user complaints if they had shipped to everyone at once. The feature flag gave them control and visibility that a traditional code deploy would not have provided.

Prompt registries are the configuration layer for prompt management. Instead of hardcoding prompts in application code, you store them in a registry — LangSmith, a custom database, or even a Git repo — and reference them by ID. When you want to change a prompt, you update the registry, and the serving layer picks up the new version without redeploying code.

A customer support platform uses a custom prompt registry backed by PostgreSQL. Each prompt has a version number, an owner, and a deployment status — draft, staging, or production. Engineers edit prompts in a web UI, deploy them to staging for testing, then promote them to production with one click. The serving layer queries the registry every 60 seconds to check for updated prompts. When a new prompt version is promoted, it goes live across all serving pods within 60 seconds without any code deploy. This makes prompt iteration fast and safe.

Model registries are the configuration layer for model management. MLflow and Hugging Face Hub are the standard options. You upload a model to the registry with metadata — eval results, training config, dataset version — and tag it with a deployment stage: staging, production, archived. The serving layer pulls models from the registry based on the production tag. When you want to deploy a new model, you change the tag, and the serving layer picks it up.

An e-commerce company uses MLflow to manage model deployments. They train new recommendation models weekly and upload them to MLflow with full eval results. The current production model is tagged mlflow.production. When a new model beats the production model on evals, an engineer reviews the results, changes the production tag to point at the new model, and the serving layer picks it up on the next restart. The registry provides a full audit trail — every model version, every tag change, every deployment event is logged. This satisfies their compliance requirements and makes rollback trivial: just change the tag back to the previous model.

## CI/CD Layer: Pipelines, GitOps, and Eval Gates

The CI/CD layer automates the path from code commit to production deploy. In 2026, GitHub Actions and GitLab CI dominate for pipeline execution, and Argo CD and Flux dominate for GitOps-based deployment. The key difference from traditional CI/CD is the integration of eval gates — automated quality checks that block bad deployments before they reach production.

A standard AI deployment pipeline in 2026 looks like this: engineer merges a pull request to main, CI runs unit tests and linters, CI triggers the eval suite against the updated prompts or model, eval results are analyzed automatically, if evals pass then the deployment manifest is updated in the GitOps repo, Argo CD detects the manifest change and applies it to the Kubernetes cluster, the new version rolls out via a canary deployment, production metrics are monitored for 10 minutes, if metrics are stable the canary becomes the primary, if metrics degrade the deployment is auto-reverted.

This pipeline is automated end-to-end, but it includes multiple human override points. If evals fail, the pipeline blocks and posts results to Slack. An engineer reviews the failure, decides whether it's a real regression or a noisy eval, and either fixes the code or overrides the gate with justification. If production metrics degrade during canary, the pipeline auto-reverts, but an engineer can also trigger a manual revert at any point.

A fintech platform runs this exact pipeline for their fraud detection model. Eval gates check precision, recall, false positive rate, and fairness metrics. If any metric regresses by more than 2 percentage points, the pipeline blocks. If all metrics are stable, the deployment proceeds to a 10% canary. The canary runs for 15 minutes while the pipeline monitors production fraud catch rate and false positive rate. If both are within tolerance, the canary expands to 100%. If either metric degrades, the pipeline auto-reverts and pages the on-call engineer. The entire process from merge to full production takes 45 minutes and requires zero manual intervention if everything is healthy.

GitOps — managing deployments via Git commits to a config repo — is the 2026 standard for declarative infrastructure. Your Kubernetes manifests live in a Git repo. You change a manifest, commit it, and Argo CD or Flux applies the change to your cluster. This gives you version control, audit trails, and easy rollback — just revert the Git commit and Argo CD rolls back the deployment.

A healthcare AI company uses GitOps for all production deployments. Their deployment repo contains Kubernetes manifests for every AI service. When they want to deploy a new model version, they update the manifest with the new image tag and commit it. Argo CD detects the change within 30 seconds and applies it. Every deployment is a Git commit, which means every deployment is auditable, every deployment is reviewable via pull request, and every deployment is revertable via git revert. This satisfies their SOC 2 and HIPAA audit requirements without any custom tooling.

## Observability Layer: AI-Specific and Infrastructure Monitoring

The observability layer tells you whether your deployments are healthy. In 2026, you need two types of observability: AI-specific monitoring that tracks model behavior, and infrastructure monitoring that tracks system health. Both are non-negotiable.

AI-specific monitoring tools like Langfuse, Arize Phoenix, and Weights & Biases track request-level data: what input was sent, what output was returned, what the latency was, what the token count was, whether the output passed quality checks. This telemetry is essential for debugging model behavior and for building eval datasets from production traffic.

A legal tech company uses Arize Phoenix to monitor their contract analysis AI. Every request — contract text in, extracted clauses out — gets logged to Phoenix with full input, output, latency, and token count. They use Phoenix to spot-check outputs, to identify queries that took longer than expected, and to find examples of bad outputs that they can add to their eval suite. Phoenix also tracks output distributions over time, which lets them detect drift — if the distribution of extracted clause types shifts, it might indicate that the model is degrading or that user behavior is changing.

Infrastructure monitoring tools like Datadog, New Relic, and Prometheus track system-level metrics: GPU utilization, request throughput, error rates, latency percentiles. This telemetry is essential for capacity planning and for incident response.

A SaaS platform uses Datadog to monitor their vLLM serving infrastructure. They track GPU memory usage, requests per second, P50/P95/P99 latency, error rate, and queue depth. They set alerts: if P95 latency exceeds 2 seconds, page on-call. If error rate exceeds 1%, page on-call. If GPU memory usage exceeds 90%, trigger autoscaling. The alerts fire before users notice degradation, which gives the team time to respond proactively instead of reactively.

The integration of AI-specific and infrastructure monitoring is where 2026 observability has advanced beyond 2024. You correlate model behavior metrics with infrastructure metrics to diagnose issues faster. If P95 latency spikes, is it because the model is generating longer responses, or because the GPU is saturated, or because the network is slow? You can only answer that question if you have both types of telemetry in the same dashboard.

## What Enterprises Expect in 2026

Enterprise buyers in 2026 have well-defined expectations for AI deployment infrastructure. If you are selling AI to enterprises, your deployment stack must meet these expectations or you will not pass procurement.

Sub-minute rollback is table stakes. Enterprises will not accept AI systems that take 10 minutes to revert when something goes wrong. You must be able to demonstrate that you can rollback to a previous model version in under 60 seconds, and you must have tested this capability in production.

Full audit trails are mandatory. Enterprises need to know who deployed what, when, why, what evals were run, what the results were, and how to revert it. The audit trail must be immutable — stored in append-only logs or a tamper-evident database — and it must be searchable. If an auditor asks for all deployments in the last six months, you must be able to produce a complete record in minutes.

Multi-region deployment is expected for any enterprise-scale service. Enterprises do not accept single-region deployments because a regional outage means total unavailability. You must be able to serve traffic from at least two regions, with automatic failover if one region goes down. A financial services AI vendor runs active-active in US-East and US-West, with traffic routed to the nearest region via GeoDNS. If one region fails, all traffic automatically routes to the other region within 30 seconds.

Zero-downtime updates are expected. Enterprises will not accept maintenance windows where the AI system is unavailable. You must be able to deploy new models, new code, and new infrastructure changes without any user-visible downtime. This requires canary deployments, rolling updates, or blue-green deployments — all of which are standard Kubernetes patterns in 2026.

The enterprise expectation gap is where many early-stage AI companies fail. They build a great model, they achieve good accuracy, but their deployment infrastructure is immature. They cannot rollback quickly, they do not have audit trails, they run in a single region, they require downtime for updates. Enterprise procurement teams reject them on operational maturity grounds, regardless of model quality. The lesson: deployment infrastructure is not a nice-to-have. It is a requirement for enterprise sales.

---

The 2026 AI deployment stack is well-understood, well-tooled, and well-documented. You do not need to invent new patterns. You need to assemble the right layers for your scale and your risk profile — serving, orchestration, gateway, configuration, CI/CD, observability — and operate them with discipline. The teams that do this well deploy confidently, recover quickly, and meet enterprise expectations. The teams that skip layers or under-invest in tooling spend their time fighting fires instead of building product. Next, we examine how to build the business case for deployment infrastructure investment — how to quantify the ROI, present it to leadership, and get the resources to build the stack your system needs.
