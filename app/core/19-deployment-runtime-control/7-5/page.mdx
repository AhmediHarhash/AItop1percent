# 7.5 — Conditional Flags: Context-Dependent Feature Activation

What if the same user should see different behavior depending on what they're doing? A premium subscriber might need the fast model for simple queries but the reasoning model for complex analysis. A mobile user might need compressed responses while a desktop user gets full detail. A query in Spanish might route to a multilingual model while English uses the standard one. That's where conditional flags come in — feature activation that evaluates not just who the user is, but what they're trying to do, where they are, and what the system knows about the request.

Static user-level flags are clean and predictable. You check if user 47392 is in the test group, and the answer is always yes or always no. Conditional flags are messier and more powerful. They evaluate rules based on attributes of the current request — the query type, the detected language, the input length, the time of day, the device type, the session history. The same user might hit different flag states on consecutive requests. This creates complexity. It also creates precision.

## What Makes a Flag Conditional

A conditional flag evaluates a rule before deciding which variant to serve. The rule can be as simple as checking a single attribute — is the detected language French — or as complex as combining multiple conditions with boolean logic. User is premium AND query length exceeds 500 characters AND confidence score from intent classifier is above 85 percent. User is in Europe OR request includes GDPR-flagged terms. Device type is mobile AND time of day is during peak hours.

The flag system receives context attributes with every evaluation request. These might come from the authentication layer, from preprocessing steps earlier in the request pipeline, from the user's session state, or from real-time classifiers that run before the flag check. The more attributes available, the more precise the targeting becomes. The more attributes required, the slower the evaluation and the more brittle the system becomes if any attribute is missing.

Teams build conditional flags when user identity alone is insufficient. A fintech platform in mid-2025 wanted to test a new fraud detection model but only on high-risk transactions. They couldn't just enable it for ten percent of users — they needed it active for specific transaction patterns. They built a conditional flag that checked transaction amount, merchant category, user account age, and time since last transaction. The flag evaluated to true when any two of four risk signals were present. This let them target exactly the scenarios where the new model mattered most.

## Context Attributes That Drive Decisions

The attributes you can condition on depend on what information is available at flag evaluation time. Some attributes are static per request. Device type, detected language, geographic region, authentication level. These are known at the start of the request and don't change. Other attributes are dynamic and require computation. Intent classification scores, content safety predictions, semantic similarity to previous queries, estimated response latency.

Static attributes are cheap to evaluate. You already parsed the user agent string, you already extracted the auth token. Conditional rules that use only static attributes add microseconds to the request path. Dynamic attributes require work. Running a lightweight intent classifier to decide whether to enable the advanced reasoning model takes 15 to 40 milliseconds. Running a toxicity detector to decide whether to enable stricter content filters takes another 20 milliseconds. Every dynamic attribute adds latency before you even call the main model.

Query type is one of the most useful attributes. A customer support platform in early 2026 ran three different prompt strategies depending on whether the query was a product question, a troubleshooting request, or a complaint. They used a small classifier to predict query type at request time, then used a conditional flag to route to the appropriate prompt variant. Classification accuracy was 91 percent. When the classifier was confident — score above 80 percent — the conditional flag routed to the specialized prompt. When the classifier was uncertain, the flag fell back to the general-purpose prompt. This reduced average resolution time by 18 percent for correctly classified queries and avoided degrading the experience for ambiguous cases.

Input length is another common condition. Short queries below 50 characters might use a fast model. Medium queries between 50 and 300 characters use the standard model. Long queries above 300 characters use the large context model. You don't need a classifier for this — you just count tokens. But you do need a rule engine that can express these ranges cleanly and evaluate them quickly.

## Rule Composition and Boolean Logic

Simple conditional flags check one attribute. Is the language French? Is the user premium? Is the input length above 200 tokens? Complex conditional flags combine multiple conditions with AND, OR, and NOT operators. User is premium AND query is complex. User is in Europe OR query contains regulated terms. User is authenticated AND NOT flagged for abuse.

The more complex the rule, the harder it is to reason about. A rule with three AND conditions is straightforward — all three must be true. A rule with three OR conditions is also clear — any one must be true. A rule that mixes AND and OR requires careful parentheses and clear operator precedence. Enable the feature if user is premium AND either query is complex OR input length exceeds 500 tokens. Does that mean premium is required and then either condition suffices? Or does it mean premium users with complex queries, or anyone with long input? The logic is ambiguous without explicit grouping.

Most flag platforms provide a rule builder UI that lets product teams construct these conditions without writing code. LaunchDarkly's rule builder uses a visual interface where you add conditions, choose operators, and see the resulting logic. Statsig uses a similar approach. These interfaces help, but they don't eliminate the fundamental complexity. A rule with six conditions and mixed operators is hard to understand no matter how you build it.

A healthcare AI platform in late 2025 used conditional flags to control access to experimental diagnostic features. The rule was: user is a licensed clinician AND user has completed training module AND either patient case is flagged high-priority OR user has explicitly requested experimental mode. The rule had four conditions with mixed logic. It was technically correct but hard for the product team to verify. They eventually decomposed it into two separate checks — a user eligibility check and a case eligibility check — and combined them in application code. The logic became more explicit and easier to test.

## AI-Specific Conditions: Confidence and Classification

AI systems generate signals that are perfect candidates for conditional flag evaluation. Confidence scores from intent classifiers, toxicity predictions from content safety models, semantic similarity scores from retrieval systems, estimated query difficulty from lightweight analyzers. These signals can drive feature decisions in ways that static user attributes cannot.

Confidence-based flags are particularly powerful. You have a new model that performs better on complex queries but is more expensive. You don't want to route all traffic to it — only the traffic where the new model's strengths matter. You run a lightweight complexity analyzer at request time. It produces a score from zero to one. If the score is above 0.7, route to the new model. If below, use the standard model. The flag evaluation checks the complexity score and makes the routing decision.

This pattern appears everywhere. A legal research platform used confidence scores from their citation relevance model to decide whether to show experimental citation features. If the model was confident — score above 0.85 — the feature appeared. If uncertain, it didn't. Users saw the feature exactly when it was most likely to be helpful. A translation service used language detection confidence to decide whether to enable dialect-specific handling. High confidence in detecting Castilian Spanish triggered Castilian-specific prompts. Low confidence fell back to general Spanish handling.

The risk with confidence-based flags is that the confidence score itself might be unreliable. A poorly calibrated classifier might report 90 percent confidence when its actual accuracy is 70 percent. If you condition a flag on that score, you're routing traffic based on a false signal. Calibrating confidence scores is its own discipline — comparing predicted confidence to actual accuracy and adjusting the score distribution accordingly. Without calibration, confidence-based flags can make decisions that feel data-driven but are actually just following a noisy signal.

## Dynamic Threshold Flags: Adjusting Cutoffs in Real Time

Some conditional flags don't check a fixed rule. They check a rule with a parameter that can be adjusted at runtime. Enable the feature when quality score is above X. Route to the fast model when latency budget is below Y milliseconds. Show the experimental UI when user satisfaction score is above Z. The threshold — X, Y, Z — is itself a configuration value that can change without redeploying code.

This pattern is common in cost-quality tradeoffs. You want to use the expensive model only when quality matters most, but your definition of "matters most" evolves as you learn. You start with a complexity threshold of 0.8. After a week, you see that 0.75 captures most of the value with 20 percent more coverage. You lower the threshold. Another week passes, costs are higher than expected, you raise it to 0.78. The code never changes — just the configuration value that the flag evaluation checks against.

An e-commerce platform in early 2026 used dynamic thresholds to control which product descriptions got rewritten by their generative model. They had a quality predictor that scored existing descriptions from zero to one. Descriptions below the threshold got rewritten. Descriptions above it were left alone. The threshold started at 0.6. They raised it to 0.7 after seeing too many unnecessary rewrites. They lowered it to 0.65 after missing opportunities to improve borderline descriptions. The flag check was always the same — is the quality score below the threshold — but the threshold itself moved based on business feedback.

Dynamic thresholds require careful monitoring. If you adjust a threshold and quality drops, you need to see it immediately and roll back. If you adjust a threshold and costs spike, same thing. The threshold is a control surface, and like any control surface, it can be moved in the wrong direction. Treat threshold changes like code deployments — measure before and after, watch for regressions, be ready to revert.

## Performance Impact of Complex Conditions

Every conditional flag adds evaluation overhead to the request path. A simple check — is user ID in this set — takes microseconds. A complex rule with five conditions, three of which require looking up attributes from external systems, can take tens of milliseconds. If you have ten conditional flags in the request path, and each takes 20 milliseconds to evaluate, you've added 200 milliseconds of latency before you even started the actual work.

The cost comes from two places. First, gathering the context attributes. If your flag needs to know the user's subscription tier, and that requires a database lookup, you just added a round trip. If your flag needs to know the detected language, and that requires running a lightweight classifier, you just added inference latency. Second, evaluating the rule logic. Boolean expressions are fast, but if the rule engine is inefficient or the rule is deeply nested, evaluation can take longer than expected.

Most flag platforms optimize for this by using local evaluation with SDKs that cache flag rules. Instead of making a network call to a flag service on every request, the SDK downloads the rule set once, caches it in memory, and evaluates rules locally. This reduces network latency but doesn't eliminate the cost of complex rule evaluation or the cost of gathering attributes.

A media streaming platform in mid-2025 had conditional flags that checked user subscription level, geographic region, device type, and current content category. Gathering those attributes required three service calls — one to the user service, one to the geo service, one to the content metadata service. They were making those calls sequentially, adding 60 to 80 milliseconds per request. They refactored to make the calls in parallel and pre-fetch common attributes at session start. Latency dropped to 15 milliseconds. The lesson: conditional flags don't inherently slow you down, but careless attribute fetching does.

## Testing Conditional Logic: Ensuring Rules Behave as Expected

Conditional flags introduce logic bugs. If your rule says enable the feature when user is premium AND query is complex, but the intent classifier is broken and always returns "complex," the flag will activate for all premium users even when it shouldn't. If your rule says enable when confidence is above 0.8 OR user is in experiment group, but you accidentally wrote AND instead of OR, the flag will under-activate and you won't know why.

Testing conditional flags requires two layers. First, unit tests for the rule logic itself. Given this set of attributes, does the rule evaluate correctly? You mock the context attributes and assert that the flag evaluates to true or false as expected. Most flag platforms provide testing utilities for this. LaunchDarkly lets you test rules in their UI by supplying mock context. Statsig has a rule simulator. If you built a custom flag system, you need to build these test utilities yourself.

Second, integration tests that verify the attributes are populated correctly. It's not enough to test that the rule works given correct attributes. You need to verify that the attributes reaching the rule are actually correct. If your flag depends on detected language, you test that the language detector is running, that its output is being passed to the flag evaluation, and that edge cases — no language detected, mixed-language input — are handled gracefully.

A logistics platform in late 2025 had a conditional flag that was supposed to enable a new routing algorithm for high-priority shipments. The rule was straightforward — if shipment priority is high, enable the flag. They tested the rule logic in isolation and it worked. They deployed to production and the new algorithm never activated. Turns out the attribute "shipment priority" wasn't being populated in the flag evaluation context. The rule was correct, the integration was broken. They caught it only after users complained that high-priority shipments weren't being handled correctly.

## Edge Cases: Missing Context and Ambiguity

What happens when the context attributes your flag depends on are missing or ambiguous? The user's subscription level field is null. The language detector returns no result. The intent classifier times out. The conditional flag needs to make a decision anyway.

The default behavior matters enormously. Some flag platforms default to the control variant when evaluation fails. This is the safe choice — if something goes wrong, fall back to known-good behavior. Other platforms default to the last successfully evaluated state. This is riskier but avoids unnecessary feature toggling when transient errors occur. Still others let you specify a fallback value per rule.

Missing attributes are common in real systems. A user who just signed up might not have a subscription level assigned yet. A query in a rare language might return no language code from the detector. A first-time visitor might have no session history. Your conditional flag rules need to handle these cases explicitly. You can require that certain attributes must be present, and fail evaluation if they're not. Or you can treat missing attributes as false for AND conditions and ignore them for OR conditions.

Ambiguity is harder. The language detector returns English with 60 percent confidence and Spanish with 40 percent confidence. Is that English, Spanish, both, or neither? Your flag rule says enable for Spanish queries. Does this query qualify? You need a policy. One approach: use the highest-confidence prediction above a threshold — say 70 percent. If no prediction exceeds the threshold, treat the attribute as missing. Another approach: use the highest-confidence prediction regardless of threshold, accepting that some decisions will be based on weak signals. Neither is perfect.

A customer support platform in early 2026 had a conditional flag that routed queries to specialized agents based on detected issue category. The category classifier was accurate 88 percent of the time when confident, but 15 percent of queries produced no confident prediction. The flag rule initially treated those queries as "general" and routed them to the default agent pool. After a few weeks, the team realized that many of these ambiguous queries were actually complex edge cases that needed senior agents. They changed the rule to route low-confidence classifications to senior agents instead of general agents. Resolution time for edge cases dropped by 30 percent.

## Flag Evaluation Performance at Scale

At low request volumes, conditional flag evaluation overhead doesn't matter. At high volumes, every millisecond counts. If you're handling 50,000 requests per second and each request evaluates five conditional flags, you're performing 250,000 flag evaluations per second. If each evaluation takes 2 milliseconds, that's 500 CPU cores just for flag evaluation.

The optimization strategies are the same as for any hot path code. First, minimize attribute fetching. Fetch attributes once per request and reuse them across multiple flag evaluations. Don't make the same database lookup ten times because ten different flags need the same attribute. Second, cache aggressively. If a flag rule hasn't changed in five minutes, you can evaluate it from cached state. Most flag SDKs do this automatically. Third, short-circuit evaluation. If a flag rule is user ID is in set X OR user ID is in set Y OR user ID is in set Z, and the user is in set X, don't bother checking sets Y and Z.

Fourth, batch flag evaluations when possible. Some flag platforms support evaluating multiple flags in a single call, reducing the per-flag overhead. If you're using a remote flag service rather than local evaluation, batching can turn ten network calls into one. Fifth, profile and measure. You think your conditional flag evaluation is cheap, but you don't actually know until you measure it under load.

A fintech platform in mid-2025 had six conditional flags in their API gateway that routed requests to different backend services. At 20,000 requests per second, flag evaluation was consuming 12 percent of gateway CPU time. They profiled and found that three of the flags were checking attributes that required Redis lookups. They moved those attributes into the authentication token payload, eliminating the lookups. CPU usage for flag evaluation dropped to 3 percent.

Conditional flags are powerful because they let you make fine-grained decisions based on real-time context, but that power comes with performance overhead and complexity that static flags avoid. The next challenge is choosing the platform that will evaluate these flags millions of times per day without breaking your latency budget or your infrastructure.
