# 2.11 — Serving Infrastructure Selection: The Decision Framework

Why do most fine-tuning projects never reach production? Because the team builds a great model and then realizes they have no viable path to serve it. They spent eight weeks on dataset curation and training. They achieved 94% accuracy on their eval suite. They demoed it to leadership and got approval to launch. Then they discover that serving the model costs $40,000 per month on managed APIs, their security team won't approve sending data to third parties, and they don't have anyone on staff who knows how to run GPU infrastructure. The project stalls. The model never serves a production request. The failure wasn't technical — it was architectural. They made serving decisions last instead of first.

Serving infrastructure selection is a strategic decision that should happen before you fine-tune, before you build evals, before you commit to a model architecture. The right infrastructure choice depends on traffic volume, latency requirements, cost constraints, privacy needs, and team capabilities. Get this decision right and your system scales smoothly from prototype to production. Get it wrong and you're locked into an infrastructure tier that can't support your requirements.

## The Five Decision Factors: What Actually Matters

The first factor is **traffic volume**, measured in requests per day or tokens per day. Volume determines cost structure. At low volume, per-request pricing is fine. At high volume, it's prohibitively expensive. The break-even points are around 10,000 requests per day for simple endpoints and 1 million tokens per day for language models. Below those thresholds, managed APIs are usually cheaper. Above them, you need to consider alternatives.

But volume alone doesn't tell the full story. **Traffic pattern** matters as much as total volume. A system that processes 100,000 requests per day spread evenly over 24 hours has different infrastructure needs than a system that processes the same 100,000 requests in a 2-hour batch job. Steady traffic benefits from dedicated capacity. Bursty traffic benefits from serverless or autoscaling. Measuring peak-to-average ratio tells you whether you need elasticity or sustained capacity.

**Latency requirements** are the third factor. If your application needs to respond in under 500 milliseconds, you need warm capacity — dedicated GPUs or permanently warm serverless instances. Cold starts, which can take 5-30 seconds, are unacceptable. If your application can tolerate 2-5 second response times, serverless with occasional cold starts is viable. If you're running batch jobs where latency doesn't matter, you have maximum flexibility. Latency requirements eliminate entire classes of infrastructure before you consider cost.

**Cost constraints** determine how much you can spend per request. A consumer-facing application with thin margins might have a budget of $0.001 per request. An enterprise SaaS product with high contract values might tolerate $0.10 per request. The cost constraint sets a ceiling on infrastructure choices. Managed APIs that charge $1.50 per million tokens are viable for enterprise use cases but not for consumer applications at scale. Self-hosted infrastructure that costs $10,000 per month in fixed overhead is viable at 10 million requests per month but not at 100,000.

**Team expertise** is the constraint that breaks most infrastructure projects. Self-hosting requires knowledge of GPU architectures, inference frameworks, Kubernetes, observability, and on-call operations. If you don't have those skills on your team, self-hosting will take 3-6 months longer than planned and cost twice as much. Managed APIs require minimal expertise — you call an endpoint and handle responses. Serverless platforms are in between — you need to understand deployment and monitoring, but not GPU-level operations. The infrastructure choice must match your team's capabilities, or you need to hire before you can execute.

## The Decision Tree: A Systematic Selection Process

The decision starts with a single question: **What is your daily traffic volume?** If you're processing under 10,000 requests per day or under 1 million tokens per day, start with managed APIs. The math is straightforward. At 1 million tokens per day with a model like GPT-5-mini at $1.50 per million tokens, your monthly cost is $45. Building any kind of self-hosted infrastructure costs more than that in engineering time, even if the GPU itself were free. Managed APIs are the rational choice for low volume.

Between 10,000 and 100,000 requests per day, or between 1 million and 10 million tokens per day, the decision depends on other factors. This is the **evaluation zone** where multiple infrastructure tiers are viable. Managed APIs still work but start to feel expensive. Serverless GPU becomes cost-competitive. Light self-hosting on a single GPU is an option. The next question determines which tier wins.

The second question is: **Do you need to serve a custom model?** If the answer is no — if you're using GPT-5, Claude Opus 4.5, or another frontier model available via API — managed APIs remain the best choice even at moderate volume. The API gives you automatic updates, multi-region availability, and operational simplicity that you can't replicate. If the answer is yes — you've fine-tuned Llama 4 on your data, or you're running a domain-specific model that isn't available via API — you need infrastructure to host your weights. Managed APIs can't help you. The choice narrows to serverless or self-hosted.

The third question is: **What are your latency requirements?** If you need P99 latency under 500 milliseconds, you need warm capacity. Serverless with cold starts won't cut it. You need either dedicated GPU instances that are always-on or serverless instances configured with minimum warm capacity, which eliminates most of the cost savings. If you can tolerate 2-5 second latency with occasional spikes to 10 seconds, serverless is viable. If you're running batch workloads where latency is irrelevant, serverless is optimal.

The fourth question is: **What are your data privacy requirements?** If your data can be sent to third-party APIs under your compliance framework, managed APIs remain an option. If your data must stay within your VPC or on-premise, you must self-host. Healthcare companies processing patient data under HIPAA, financial institutions handling PII under GDPR, and government agencies with classified data typically cannot use managed APIs. Privacy requirements force self-hosting regardless of cost or complexity.

The fifth question is: **What is your team's infrastructure expertise?** If you have engineers who have built production ML infrastructure before, self-hosting is feasible. If you don't, self-hosting is a 6-12 month project that will cost more than you think. Serverless platforms require less expertise — you need to understand container deployment and monitoring, but not GPU operations. Managed APIs require minimal expertise. Match infrastructure to team capability, or accept that you're building a new capability from scratch.

A health tech startup went through this decision tree for their clinical note summarization feature. Daily volume: 500,000 tokens per day. Custom model: yes, fine-tuned Llama 4 on medical notes. Latency: batch processing, latency irrelevant. Privacy: HIPAA-compliant, data cannot leave their infrastructure. Team expertise: two engineers, no ML infrastructure experience. The decision tree pointed to self-hosting as mandatory due to privacy requirements. But team expertise was the blocker. They hired a platform engineer with ML infrastructure experience and committed to a six-month buildout. The alternative — not launching the feature at all — was worse than accepting the timeline and cost.

## Cost Modeling: The Break-Even Analysis

Every infrastructure decision comes down to break-even math. **Managed APIs cost per token.** GPT-5 is around $3 per million tokens for input and $15 per million tokens for output. Claude Opus 4.5 is similar. Llama 4 on Together AI is $0.20 per million tokens. If you're processing 10 million tokens per day of output, you're paying $150 per day or $4,500 per month. That's your baseline.

**Serverless GPU costs per second of compute.** An A100 on Modal costs around $3 per hour, billed per second. If your model processes 1,000 tokens per second and you're serving 10 million tokens per day, that's 10,000 seconds of compute, or 2.8 hours per day. Your monthly cost is around $250. Serverless is 18 times cheaper than the managed API for this workload.

**Self-hosted infrastructure has fixed and variable costs.** An A100 instance on AWS reserved for a year costs around $1.50 per hour, or $1,080 per month. Add platform engineering labor at $150,000 per year fully-loaded, which is $12,500 per month. Your all-in cost for a single self-hosted GPU is around $13,580 per month. This cost is fixed whether you serve 1 million tokens or 100 million tokens.

The break-even calculation compares these costs at your usage level. At 10 million tokens per day, managed APIs cost $4,500 per month, serverless costs $250 per month, and self-hosted costs $13,580 per month. Serverless wins. At 100 million tokens per day, managed APIs cost $45,000 per month, serverless costs $2,500 per month, and self-hosted still costs $13,580 per month. Self-hosted wins. The crossover point is around 40 million tokens per day, where self-hosted and serverless cost about the same. Below that, serverless is cheaper. Above that, self-hosted is cheaper.

But the real calculation includes **time to value**. Serverless can be deployed in days. Self-hosted takes months. If serverless costs $2,500 per month and self-hosted costs $13,580 per month but takes six months to build, you'll spend $15,000 on serverless during those six months while building self-hosted. The total cost to self-host is $13,580 times six months for the platform engineer, plus $15,000 in serverless costs, which is $96,480. To amortize that investment, you need to run self-hosted long enough that the monthly savings cover the upfront cost. If self-hosted saves you $11,000 per month compared to serverless, the payback period is nine months. You need to run self-hosted for at least 15 months total to justify the investment.

This is why most teams should start with serverless or managed APIs and migrate to self-hosted only when the math clearly works. The upfront cost of building infrastructure is high. The break-even period is long. Unless you're certain your usage will remain high for years, the ROI case is weak.

## Hybrid Architecture Patterns: The Best of All Tiers

The most sophisticated systems don't pick one infrastructure tier. They use multiple tiers strategically. **Managed APIs for standard models, self-hosted for custom models** is the most common hybrid pattern. You call GPT-5 via OpenAI's API for general-purpose summarization. You serve your fine-tuned Llama 4 model on your own infrastructure for domain-specific extraction. Each workload runs on the infrastructure that makes sense for its requirements.

**Self-hosted primary, managed fallback** is the resilience pattern. Your primary traffic routes to self-hosted GPUs. When capacity is exhausted or GPUs fail, traffic automatically fails over to a managed API. This gives you the cost savings of self-hosting with the reliability of managed APIs. The implementation requires routing logic that detects capacity exhaustion or failures and redirects traffic. You also need to ensure that the fallback model is compatible with your primary model — similar enough that users don't notice degradation when failover happens.

**Different tiers for different application tiers** is the segmentation pattern. Your free tier users hit managed APIs with rate limits. Your paid tier users hit serverless infrastructure with higher capacity. Your enterprise customers get dedicated self-hosted instances with SLA guarantees. Each customer segment gets infrastructure that matches the economics and expectations of that tier. The complexity is in routing, billing, and observability across multiple backends.

A SaaS company serving AI-powered contract analysis used all three patterns. They called Claude Opus 4.5 via API for general summarization, which handled 60% of requests. They self-hosted a fine-tuned Llama 4 model for contract clause extraction, which handled 35% of requests. They used serverless GPU on Modal for experimental features that had low traffic, which handled 5% of requests. Managed was the easiest to operate. Self-hosted was the cheapest per request for high-volume workloads. Serverless enabled fast iteration on new features without infrastructure overhead. The hybrid architecture was more complex than a single-tier system, but it optimized for cost, capability, and agility simultaneously.

## Migration Planning: Building for Portability

The most important architecture decision is not which infrastructure to choose today — it's how to avoid being locked into that choice forever. **Build abstraction layers from day one.** Wrap every model call in an interface that can route to multiple backends. The interface should support OpenAI, Anthropic, self-hosted, and serverless endpoints behind the same function signature. Your application code calls `generate_completion(prompt, model)` and the routing layer decides whether to send that request to OpenAI, your self-hosted vLLM cluster, or Modal.

The abstraction layer handles **request formatting** differences. OpenAI expects requests in one format. Anthropic expects a slightly different format. Your self-hosted vLLM cluster might expect yet another format. The abstraction layer translates your application's internal request format to the backend's expected format. This translation layer is 200 lines of code that saves you from rewriting every callsite when you change providers.

The abstraction layer also provides **unified observability**. Every request, regardless of backend, logs the same metrics: model name, token counts, latency, cost, error type. Your dashboards show cost and performance across all backends in a single view. Without this visibility, you can't make rational decisions about which workloads to migrate. You're guessing based on intuition instead of measuring based on data.

**Gradual traffic shift** is how you de-risk migration. When you decide to move from managed APIs to self-hosted, you don't flip a switch and route 100% of traffic to the new infrastructure. You route 5% of traffic to self-hosted and 95% to managed. You monitor error rates, latency distribution, and cost. If everything looks good, you increase to 10%, then 20%, then 50%. At each step, you verify that the new infrastructure performs as expected. If it doesn't, you roll back to the previous percentage in seconds.

The implementation is simple: a weighted routing rule in your abstraction layer. `if random() < 0.05: route_to_self_hosted() else: route_to_managed()`. You adjust the threshold over days or weeks as confidence grows. The teams that succeed at migration do it gradually with continuous monitoring. The teams that fail try to migrate everything in one deployment and discover problems when users are already impacted.

**Rollback capability** must be instant. If self-hosted infrastructure fails, your system should automatically route 100% of traffic back to managed APIs within seconds. If a new model version degrades quality, you should be able to route traffic back to the old version without manual intervention. Rollback is not a nice-to-have. It's the difference between an incident that lasts two minutes and an incident that lasts two hours.

## Red Flags: Mistakes That Kill Infrastructure Projects

The first red flag is **building abstraction after choosing infrastructure**. Teams that commit to OpenAI or self-hosted without an abstraction layer find themselves locked in. Every prompt, every error handler, every retry policy is coupled to that specific backend. When they want to switch, they're rewriting thousands of lines of code. The fix is to build the abstraction layer first, before committing to any infrastructure. The abstraction layer takes one week to build. Migration without it takes one month.

The second red flag is **over-engineering for current scale**. Teams with 10,000 requests per day build Kubernetes clusters with autoscaling, multi-region deployments, and complex monitoring. They spend six months building infrastructure that won't pay off for two years. The fix is to start simple and add complexity only when scale demands it. Managed APIs are the simplest starting point. Serverless is the next step. Self-hosted Kubernetes is the final step, only when you're at millions of requests per day.

The third red flag is **under-engineering for growth**. Teams build infrastructure that works today but can't scale. They run a single GPU with no redundancy, no autoscaling, no rollback capability. When traffic doubles, the system falls over. When the GPU fails, the system goes down. The fix is to build for one order of magnitude above your current scale. If you're at 100,000 requests per day, build for 1 million. You don't need to pay for 1 million worth of capacity, but your architecture should support it without a rewrite.

The fourth red flag is **choosing infrastructure based on what's interesting, not what's rational**. Engineers want to build self-hosted Kubernetes clusters because it's technically interesting. Leadership wants to self-host to feel like they own their infrastructure. Neither reason is rational. The rational choice is the infrastructure that minimizes total cost — including engineering time, operational burden, and opportunity cost of not working on product. For most teams, that means managed APIs for longer than they expect.

## The Recommendation: Start Simple, Evolve Deliberately

The recommendation is this: **start with managed APIs**. They are the simplest, fastest, and most reliable option. They let you focus on product, not infrastructure. They cost more per request than self-hosted at scale, but they cost nothing in engineering time. For the first six months of any AI product, managed APIs are almost certainly the right choice.

When you hit 1 million tokens per day, or when you need to serve a custom model, **add serverless GPU**. Modal, Replicate, Together AI, or Anyscale give you model control without infrastructure burden. You deploy your code or weights, the platform handles the rest. Serverless is the middle ground that works for 80% of teams who outgrow managed APIs.

Consider **self-hosted infrastructure** only when three conditions are met: you're processing over 10 million tokens per day, you have platform engineering expertise on the team, and the break-even math clearly works. Self-hosting is not a strategic advantage. It's a cost optimization that comes with operational burden. The teams that succeed at self-hosting are the ones who wait until they must do it, then commit fully to building it well.

Build **abstraction layers and observability from day one**. These are the investments that let you change infrastructure as your needs evolve. An abstraction layer takes one week to build and saves months during migration. Unified observability costs nothing and gives you the data to make rational infrastructure decisions.

By 2026, the pattern is clear. The teams that win start simple, measure everything, and evolve infrastructure only when scale and requirements demand it. They don't build for the scale they want to be at. They build for the scale they're at, with the flexibility to evolve. Managed APIs are the starting line. Serverless is the first evolution. Self-hosted is the final form, reached only when necessary. This is not a linear path. It's a decision tree that you re-evaluate every quarter as your product, your scale, and your team capabilities change.

---

The next chapter covers scaling and load management — autoscaling strategies, batching, caching, and traffic shaping for production AI systems.
