# Chapter 4 — Rate Limiting, Quotas, and Cost Control

Rate limiting for AI is fundamentally different from rate limiting for web APIs. Traditional rate limits count requests. A hundred API calls per minute. A thousand requests per day. That model assumes uniform cost per request. AI systems break this assumption completely. A single request can generate 128,000 tokens and cost forty dollars. Another request generates 50 tokens and costs three cents. Request counts mean nothing. Token-based rate limiting is the infrastructure primitive that separates functional AI systems from cost disasters.

This chapter covers token metering, per-tenant quota enforcement, priority tier architecture, streaming cutoffs, agent loop caps, and cost-aware request rejection. You'll learn how to design rate limiting that accounts for variable request cost, how to enforce quotas in real-time without blocking legitimate traffic, how to prevent runaway agent loops from draining budgets, and when to reject a request before it even starts. By the end, you'll understand how to build the cost control layer that keeps AI systems economically viable at scale.

---

- 4.1 — Why Token-Based Rate Limiting Matters for LLMs
- 4.2 — Request-Based vs Token-Based Rate Limits: The Critical Difference
- 4.3 — Token Metering as Infrastructure: Real-Time Budget Enforcement
- 4.4 — Per-Tenant Quota Allocation Strategies
- 4.5 — Priority Tier Enforcement: Premium vs Free Tier Traffic
- 4.6 — Burst Handling and Token Bucket Algorithms
- 4.7 — Streaming Cutoffs and Mid-Generation Cancellation
- 4.8 — Hard Token Caps for Agents: Preventing Runaway Loops
- 4.9 — Cost-Aware Request Rejection: When to Say No
- 4.10 — Rate Limit Communication: Headers, Errors, and User Experience
- 4.11 — Dynamic Rate Limiting: Adjusting Limits Based on System Load
- 4.12 — Rate Limiting for Agents and Multi-Step Workflows

---

*The difference between a profitable AI product and a money-losing AI product is often just a well-designed rate limiting layer.*
