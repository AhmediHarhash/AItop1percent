# 6.2 — Mapping Tasks to Evaluation Strategies

In early 2024, a financial services company built an AI assistant that handled multiple tasks: extracting transaction amounts from receipts, classifying expense categories, generating justification summaries for auditors, and answering questions about company expense policy. They built a single evaluation harness that checked everything the same way: comparing outputs to reference examples with exact string matching. The results were bizarre. The extraction tasks showed 98 percent accuracy. The classification tasks showed 87 percent accuracy. The generation tasks showed 12 percent accuracy.

The problem was not the system. The problem was that they were evaluating every task the same way, even though different tasks require fundamentally different evaluation approaches. Extraction tasks have clear right answers that can be checked automatically. Generation tasks require human judgment about quality dimensions that string matching cannot capture. They had built the wrong evaluation strategy for half their system.

This is what happens when you do not map tasks to evaluation strategies during problem framing. You build evaluation systems that measure the wrong things or measure the right things the wrong way.

## Why Task Type Determines Eval Strategy

Each task type in your taxonomy implies a specific evaluation approach because each type has different success conditions and different ways to verify correctness.

Generation tasks produce open-ended text where there is no single correct answer. You cannot check these with exact matching. You need human judgment or LLM-as-judge with detailed rubrics covering quality dimensions like accuracy, relevance, tone, completeness, and safety. Generation eval is expensive—it requires trained raters or sophisticated automated systems—but there is no shortcut.

Extraction tasks pull specific information from larger inputs. These have clear right answers. Did you extract the correct phone number? The correct date? The correct entity name? You can automate this with precision and recall metrics: how many of the extractions were correct, and how many of the correct answers did you find? Extraction eval is cheap and fast once you have labeled test data.

Classification tasks assign inputs to predefined categories. These also have clear right answers and can be evaluated automatically. You use accuracy for overall performance, confusion matrices to see which categories are confused with which, and per-class metrics like precision and recall to identify where the system struggles. Classification eval is straightforward but requires sufficient test examples per class.

Transformation tasks convert inputs from one format to another—summarization, translation, rephrasing, format conversion. These need semantic equivalence checks: does the output preserve the meaning and key information of the input while meeting the transformation objective? This often requires a combination of automated metrics and human judgment. A summary should be shorter and capture key points; you can check length automatically but need humans to judge completeness.

Reasoning tasks require the system to work through multi-step logic to reach a conclusion. For these, you need process evaluation, not just output evaluation. It is not enough to check if the final answer is correct; you need to verify that the reasoning steps are valid. This requires chain-of-thought evaluation, where you check intermediate steps against expert-validated reasoning paths.

The mapping is not arbitrary. It is based on what correctness means for each task type and how you can verify it.

## Building the Mapping Table

During problem framing, you should create a mapping table for every task in your registry. For each task, document four things: the primary evaluation strategy, the secondary evaluation strategy, the required ground truth format, and the key metrics.

The primary evaluation strategy is your main method for checking correctness. For an extraction task extracting invoice numbers, the primary strategy might be "automated exact match against labeled ground truth." For a generation task writing customer emails, the primary strategy might be "human rating on 5-point scale for helpfulness, accuracy, and tone."

The secondary evaluation strategy is your backup or supplementary check. For the extraction task, a secondary strategy might be "manual review of failures to identify systematic errors." For the generation task, it might be "automated checks for prohibited language and required elements like greeting and signature."

The required ground truth format specifies what kind of reference data you need for evaluation. The extraction task needs a test set of documents with correct extractions labeled. The generation task needs either reference examples of high-quality emails or a detailed rubric trained raters can apply consistently.

The key metrics are how you will quantify performance. The extraction task uses precision, recall, and F1 score. The generation task uses mean rating scores per dimension, percentage of responses above threshold, and inter-rater agreement.

Here is what this looks like in practice. For Task A: "Extract structured data from unstructured medical records." Primary strategy: automated comparison to ground truth labels. Secondary strategy: manual review of edge cases with clinical expert. Ground truth format: annotated medical records with entities labeled and boundaries marked. Key metrics: entity-level precision, recall, F1, boundary accuracy.

For Task B: "Generate patient-friendly explanations of lab results." Primary strategy: expert rating on accuracy, clarity, and appropriate level of detail. Secondary strategy: automated safety checks for prohibited statements like definitive diagnoses. Ground truth format: rubric with scoring criteria and anchored examples. Key metrics: mean ratings per dimension, percentage rated adequate or better, inter-rater agreement above 0.7.

This mapping table becomes the blueprint for building your evaluation system. The eval team knows exactly what they need to build for each task.

## Why This Mapping Must Happen at Framing Time

Some teams think: "We will figure out evaluation strategy once we see how the system performs." This is backwards. The evaluation strategy shapes what you build and how you measure it.

If you discover during implementation that a task requires expensive human evaluation, that changes your resource planning. You need a labeling budget. You need to hire and train raters. You need time for rating cycles. If you did not know this during framing, you might have promised a timeline you cannot meet.

If you discover that a task cannot be evaluated automatically, that changes your iteration speed. Automated eval lets you test changes in hours. Human eval takes days or weeks. Your development velocity depends on knowing this up front.

If you discover that a task needs a specific ground truth format you do not have, that is a blocking dependency. You cannot evaluate without ground truth. If you did not identify this need during framing, you might start building only to discover you are missing critical data.

The mapping reveals resource requirements before you commit to timelines. If 60 percent of your tasks require human evaluation, you know you need a labeling team, a rating platform, and time for multiple rating rounds. If 80 percent of your tasks can be evaluated automatically, you know you need eval infrastructure, test data pipelines, and continuous monitoring, but you can iterate much faster.

This directly shapes your evaluation operating model. A system with mostly automated eval can use continuous evaluation—running tests on every code change. A system with mostly human eval needs batched evaluation—collecting changes and rating them periodically. Knowing this during framing prevents you from designing an evaluation process that does not match your task mix.

## The Resource Reality Check

Let me show you what the mapping reveals about cost and timeline.

A customer support automation project had twelve tasks in their registry. During framing, they mapped each task to an evaluation strategy. Four tasks were extraction and classification: these could be evaluated automatically with precision, recall, and accuracy metrics. Two tasks were transformation: these needed semantic equivalence checks, which could be partially automated with embedding similarity but required human spot-checks. Six tasks were generation: these needed full human evaluation with detailed rubrics.

The mapping immediately revealed a resource problem. The six generation tasks accounted for half the system's functionality. Each would need 500 test cases evaluated by two independent raters. That is 6,000 total ratings. At five minutes per rating, that is 500 hours of rating time, which translates to three months of calendar time if you have two full-time raters.

The team had originally planned to ship in four months. The evaluation timeline alone consumed three-quarters of that. They had to make a choice: delay the launch, reduce scope, or find a hybrid approach using LLM-as-judge for initial filtering and human evaluation for validation.

They chose the hybrid approach. For the generation tasks, they used GPT-4 with detailed rubrics to rate all test cases, then had humans validate a 20 percent sample and all cases where the LLM ratings were uncertain. This reduced human rating time by 70 percent while maintaining quality control.

They could only make that decision because they mapped tasks to evaluation strategies during framing. If they had discovered the rating bottleneck during implementation, they would have either shipped with inadequate evaluation or blown their timeline.

## The Automation Opportunity

The mapping table also reveals where you can save time and money through automation.

Any task that can be evaluated with automated metrics should be evaluated that way. It is faster, cheaper, and more consistent than human evaluation. The mapping tells you which tasks qualify.

Extraction tasks: automated. Classification tasks: automated. Transformation tasks with clear criteria: partially automated. Reasoning tasks with deterministic logic: partially automated. Generation tasks: rarely automated unless you are checking narrow criteria like format compliance or prohibited content.

For the tasks that can be automated, the mapping specifies what infrastructure you need. Do you need a test dataset that is versioned and updated as the product evolves? Do you need a continuous evaluation pipeline that runs tests on every commit? Do you need dashboards that track metrics over time and alert when performance degrades?

For tasks that require human evaluation, the mapping specifies what labeling infrastructure you need. Do you need a rating platform? A rater training program? Quality control processes to catch bad ratings? A process for resolving disagreements between raters?

Knowing this during framing lets you provision resources before you need them. The team that discovers they need a rating platform after they have already built the system loses weeks setting it up. The team that knows they need it from the mapping table can start building it in parallel with system development.

## Evaluation Strategy for Multi-Step Tasks

Some tasks in your registry are not atomic—they involve multiple steps where the output of one step feeds into the next. These need composite evaluation strategies.

Consider a task like "Answer a complex customer question by retrieving relevant policy documents, extracting pertinent clauses, and generating a natural language explanation." That is actually three tasks: retrieval, extraction, generation. Each needs its own evaluation strategy, and you also need to evaluate the end-to-end flow.

The mapping table should break this down. Task 1: Retrieve relevant documents. Primary strategy: precision and recall at different cutoff levels. Task 2: Extract pertinent clauses from retrieved documents. Primary strategy: exact match against ground truth extractions. Task 3: Generate explanation from extracted clauses. Primary strategy: human rating on accuracy and clarity. Task 4: End-to-end question answering. Primary strategy: human rating on final answer quality, plus automated checks for citation accuracy.

This decomposition reveals that you can partially isolate failures. If the end-to-end quality is low, you can check whether retrieval failed, extraction failed, or generation failed. This makes debugging tractable.

It also reveals dependencies. The generation task can only be as good as the extraction task. If extraction misses key clauses, generation cannot include them. Your evaluation strategy needs to account for error propagation through the pipeline.

## What the Mapping Tells You About Risk

The evaluation strategy mapping also serves as a risk assessment tool. Tasks that require human evaluation are riskier than tasks with automated evaluation because human eval is slower, more expensive, and harder to scale.

High-risk combinations: high-frequency tasks that require human evaluation. If a task runs ten thousand times per day and you need humans to check quality, you cannot evaluate every instance. You need sampling strategies, which means you will miss some failures.

High-risk combinations: high-impact tasks where automated evaluation is unreliable. If a task could harm users and you cannot confidently evaluate it automatically, you need robust human evaluation, which is expensive and slow.

Low-risk combinations: low-frequency tasks that can be evaluated automatically. You can check every instance with zero human cost.

The mapping lets you see these risk patterns before you build. You can make informed decisions about where to invest in evaluation infrastructure, where to accept sampling risk, and where to redesign tasks to make them easier to evaluate.

## Using the Mapping in Practice

Once you have completed the mapping table, it becomes a planning tool for the evaluation team. They know exactly what to build for each task. They know what ground truth they need. They know what metrics matter. They know what is automated and what requires humans.

The mapping also becomes a communication tool for leadership. When you report that evaluation will take three months and require two full-time raters, you can point to the mapping table and show exactly why. Six generation tasks, each requiring human judgment, with specific quality criteria that automated tools cannot measure. Not a vague estimate, but a concrete resource requirement derived from the task structure.

The mapping evolves as the project progresses. You might discover that a task you thought required human evaluation can actually be automated with a good rubric and LLM-as-judge. You might discover that a task you thought could be automated actually has too many edge cases for automated metrics to capture. Update the mapping table as you learn.

Finally, the mapping table feeds forward into the ground truth planning process covered in the next section. Once you know what evaluation strategy each task requires, you know what ground truth format each task needs. That is the next piece of the handoff.
