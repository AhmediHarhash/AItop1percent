# 4.9 â€” The Success Criteria Review: Getting Sign-Off Before Building

In March 2025, a Series B fintech startup building a credit underwriting AI spent fourteen weeks developing a model that no one would deploy. The team had documented success criteria in a shared Google Doc: 92% approval accuracy, sub-200ms inference latency, and "fair outcomes across protected classes." Engineering built exactly what was specified. The model hit 93.4% accuracy and averaged 140ms response time. But when Legal reviewed the fairness testing methodology two weeks before launch, they refused sign-off. The criteria document said "fair outcomes" but never defined the metric, the comparison baseline, or the threshold. Engineering had used demographic parity at 5% tolerance. Legal required equalized odds at 2% tolerance, a metric the model couldn't hit without retraining on rebalanced data. Compliance had never been in the criteria review meeting because Product thought "fairness" was just a data science problem. The company delayed launch by eleven weeks, burned $340,000 in rework, and lost their first enterprise customer to a competitor. The VP of Product left three months later. The root cause wasn't a bad model or unclear writing. It was treating success criteria as a draft document instead of a signed contract between stakeholders.

## Why Success Criteria Need Formal Review

Success criteria are not documentation. They are a binding agreement between everyone who can kill your project. Product can kill it for missing business metrics. Engineering can kill it for violating performance budgets. Legal can kill it for compliance gaps. Domain experts can kill it for producing nonsense outputs. QA can kill it for untestable acceptance conditions. If any of these groups sees the criteria for the first time after you've built the system, you have built the wrong system. The purpose of a formal review is to surface disagreements when they cost zero dollars to resolve instead of when they cost six figures in rework. A good criteria review meeting is contentious, specific, and ends with written sign-off from every stakeholder who has veto power over deployment. A bad criteria review is a calendar placeholder where people nod along to slides and submit "looks good" in Slack. You need the former.

The review serves three functions that no other process step provides. First, it forces precision on vague words. "Accurate" means nothing until you specify the metric, the test set, the baseline, and the threshold. "Fast" means nothing until you specify the percentile, the load condition, and the measurement point. A review meeting is where Product says "accurate predictions" and Engineering says "accurate compared to what" and you don't leave the room until you have a number and a test procedure. Second, the review surfaces hidden constraints. Legal might require audit logs that add 40ms to every request. Domain experts might require explanations that double your inference cost. These constraints change what's buildable, and you need to know them before sprint planning, not during QA. Third, the review creates accountability. When everyone signs a document that says "we will deploy if and only if the system achieves X, Y, and Z," no one can later add a secret criterion W or claim they never agreed to the tradeoffs. The sign-off is a forcing function for honesty.

## Who Must Be in the Room

The criteria review meeting requires every role that can veto deployment. If someone can say "we're not shipping this" after the system is built, they must be in the criteria review before you write the first line of code. Missing a stakeholder is the most common reason criteria reviews fail. The core attendees are Product, Engineering, Domain Experts, Legal, and QA. Product owns the business metrics and user impact thresholds. Engineering owns the performance and cost budgets. Domain experts own the correctness and safety definitions. Legal owns compliance and risk. QA owns testability and acceptance procedures. If your product touches regulated domains, add Compliance as a separate role from Legal. If you're building internal tooling, add representatives from the teams who will use the system. If you're replacing a human workflow, bring the humans who currently do the work.

Product and Engineering are never sufficient by themselves. The credit underwriting startup failed because Legal wasn't in the room. A healthcare AI company spent nine weeks rebuilding a triage model because the clinical domain expert who reviewed criteria had never worked in an emergency department and didn't know that ED nurses need predictions in under 10 seconds, not under 60 seconds. A logistics company's route optimization system passed all success criteria but drivers refused to use it because the criteria never specified "routes must avoid left turns across traffic during peak hours," a constraint every driver knew but no one from the warehouse team thought to mention. The stakeholder list is not "who understands AI" or "who requested the feature." It's "who can prevent deployment and who will be blamed if something breaks."

Domain experts are not optional. If you're building medical AI, you need a practicing clinician, not a data scientist with a biology degree. If you're building legal AI, you need a lawyer who practices in the relevant jurisdiction, not a paralegal. If you're building content moderation AI, you need a trust and safety specialist who has reviewed millions of violations, not a policy intern. The domain expert's job is to translate real-world correctness into measurable criteria. They are the ones who know that a radiology model must never miss a pneumothorax even if it means 100 false positives, or that a contract review model must flag every indemnification clause even if it means flagging some standard warranty language. Engineering and Product will optimize for aggregate metrics. Domain experts enforce the "never do this" constraints that prevent catastrophic failures on edge cases.

## The Review Meeting Format

A successful criteria review meeting has three phases: presentation, debate, and decision. The presentation phase is not a readout of the document. Everyone has read the document before the meeting. The presentation is a 10-minute summary of the three hardest tradeoffs and the three biggest risks. Product presents the business context and the user impact thresholds. Engineering presents the performance and cost constraints. The criteria owner, usually a PM or tech lead, presents the proposed success metrics and asks for the specific questions the team needs to resolve. If you're spending more than 15 minutes on presentation, you're wasting time. The point is to get to debate.

The debate phase is where you test every word that means two different things to two different people. Someone reads each criterion aloud. Someone else explains how they would test it. If two people describe different tests, the criterion is ambiguous and must be rewritten in the meeting. You are looking for gaps, conflicts, and hidden assumptions. Gaps are things the criteria don't measure that someone cares about. Conflicts are criteria that can't all be true at the same time. Hidden assumptions are things the criteria assume but don't state, like "the input data will be formatted correctly" or "users will provide feedback within 24 hours." When you find a gap, you either add a criterion or you explicitly document that the gap is out of scope and get agreement from the person who raised it. When you find a conflict, you either relax one threshold or you declare the project infeasible and stop. When you find a hidden assumption, you either validate it with data or you turn it into a testable criterion.

The decision phase is binary. Either the group agrees on every criterion and signs off, or they don't and the meeting ends with assigned action items to resolve the open questions. There is no "we mostly agree" sign-off. Partial agreement is no agreement. If Legal says "I need to check whether 95% accuracy meets our regulatory threshold," the review is incomplete and you schedule a follow-up. If the domain expert says "I think 200ms is fine but I want to test it with five users first," you do the user test before sign-off, not after. The output of a successful review meeting is a version-controlled document with a signature block showing name, role, date, and explicit statement of agreement. Digital signatures are fine. Email confirmations are fine. Verbal "sounds good" is not fine. The signature means "I agree this is the right system to build and I commit to supporting deployment if these criteria are met."

## Common Failure Modes in Reviews

The most common failure mode is rubber-stamping. Rubber-stamping happens when stakeholders show up but don't engage. They don't read the document before the meeting. They don't ask questions. They say "looks good to me" because they're busy and the meeting is cutting into their sprint planning time. Rubber-stamping is worse than skipping the review entirely because it creates false confidence. The team thinks they have alignment, but they have attendance records. You prevent rubber-stamping by making the review meeting adversarial by design. The facilitator's job is to ask "Legal, what's the compliance risk if we miss this threshold?" and "Engineering, what breaks if we require this latency?" and "Domain expert, describe the worst-case failure if this metric is wrong." If someone gives a generic answer, you push. If someone says "I trust the team," you ask them to explain the criterion in their own words. Agreement is cheap. Informed agreement is expensive. You need the expensive version.

The second failure mode is bikeshedding, where the meeting spends 40 minutes debating the name of a metric and five minutes on the threshold that actually matters. Bikeshedding happens when criteria are presented without prioritization. Every criterion looks equally important, so people focus on the ones they understand instead of the ones that carry risk. You prevent bikeshedding by tagging every criterion with a priority: must-have, performance target, or aspirational. Must-have criteria are binary pass-fail requirements. If you miss them, you don't deploy. Performance targets are thresholds you expect to hit but might negotiate under specific conditions. Aspirational criteria are nice-to-haves that don't block deployment. The review meeting focuses on must-haves first, performance targets second, and aspirational criteria only if there's time. If someone tries to debate the wording of an aspirational criterion, you table it.

The third failure mode is missing stakeholders. You think you have everyone, but you don't. The criteria review happens without Security, and then three days before launch Security requires pen-testing that adds two weeks to the timeline. The review happens without the Data Engineering team, and then you discover the input pipeline can't deliver data at the required freshness SLA. The review happens without Finance, and then Finance says your projected inference cost violates the unit economics. You prevent missing stakeholders by doing a pre-review checklist. For each criterion, ask "who can block deployment if this is wrong?" and "who gets fired if this breaks in production?" Those people must be in the review. If you're unsure whether a role is a stakeholder, invite them and let them decline. False negatives are catastrophic. False positives are one wasted hour.

## Handling Disagreements Between Stakeholders

Disagreements in criteria reviews are good. They surface real conflicts before those conflicts become rework. The question is not whether stakeholders will disagree but how you resolve disagreement without diluting the criteria into uselessness. There are three types of disagreement: metric choice, threshold calibration, and priority conflict. Each requires a different resolution strategy.

Metric choice disagreements happen when two stakeholders want to measure the same thing differently. Product wants approval rate because that's what the business dashboard tracks. Engineering wants precision-recall because that's what the model outputs. Legal wants false negative rate on protected classes because that's what the auditor will check. These are not the same metric, and you can't optimize for all of them simultaneously. The resolution is not compromise. The resolution is decision rights. Someone has to own the final call, usually Product for business metrics, Legal for compliance metrics, and Engineering for performance metrics. The non-owners can add constraints. Legal can say "I don't care about your approval rate as long as false negative rate on protected classes is below 2%." Engineering can say "I don't care about precision-recall as long as inference latency stays under 200ms." But one person owns the primary success metric, and their metric is the one the team optimizes for.

Threshold calibration disagreements happen when stakeholders agree on the metric but not the number. Engineering says 95% accuracy is achievable. Product says 98% is required for user trust. Domain experts say 92% is the human baseline and anything above that is acceptable. You resolve calibration disagreements with data and with tiering. Data means you look at the human baseline, the competitor baseline, and the cost curve for improving the metric. If going from 95% to 98% accuracy requires doubling your training data budget and delaying launch by two months, that's a trade-off Product can evaluate against business impact. Tiering means you set a launch threshold and a stretch threshold. You deploy at 95%, but you commit to improving to 98% within six months post-launch. The criteria document specifies both thresholds and the timeline for the stretch goal. If Product refuses to launch below 98% and Engineering can't deliver 98% in the required timeline, the project is not feasible and you stop.

Priority conflicts happen when criteria are mutually exclusive. Legal requires explanations for every prediction. Engineering says explanations add 300ms to response time. The performance budget is 200ms. You cannot satisfy both criteria. The resolution is escalation. The criteria review meeting is not the place to make existential trade-offs between compliance and performance. That decision belongs to the executive team. Your job in the review meeting is to document the conflict clearly and escalate it with a recommendation. The recommendation includes the cost of each option, the risk of each option, and your proposed path. If the executives decide compliance wins, you relax the latency budget or cut scope. If the executives decide performance wins, you document the legal risk and get sign-off from Legal that they understand the tradeoff. What you never do is paper over the conflict with vague language like "we'll do our best to minimize latency while providing explanations." That is not a criterion. That is a recipe for rework.

## The Sign-Off Artifact

The sign-off artifact is a version-controlled document that records who agreed to what and when. It is not the criteria document itself. It is a signature page appended to the criteria document, showing the name, role, date, and statement of agreement for each stakeholder. The statement of agreement is explicit: "I agree that the success criteria in version 2.3 of this document, dated April 15, 2026, define the system we should build. I commit to supporting deployment if these criteria are met, and I understand that requests for additional criteria after sign-off require re-opening this review." The signature page is not editable after sign-off. If you need to change the criteria, you version the document, update the criteria, and hold a new review meeting with new signatures.

The sign-off artifact commits the team to two things. First, it commits each stakeholder to deploying the system if the criteria are met. Product cannot later say "the model is too conservative" if it hits the specified precision threshold. Legal cannot later add a new compliance requirement that wasn't in the signed criteria. Engineering cannot later claim the performance budget was unrealistic if they signed off on it. The sign-off is a one-way door. You can always decide not to deploy a system that meets criteria, but you cannot block deployment of a system that meets criteria unless you can demonstrate a new risk that was not foreseeable at the time of sign-off. Second, it commits the team to not building the system if the criteria are unachievable. If Engineering signs off on a 50ms latency requirement and later discovers that the required model architecture can't run under 80ms, they must escalate immediately. The sign-off is not a promise to try hard. It's a commitment that the criteria are feasible.

The artifact includes a change log that records every modification to the criteria and the reason for the change. If Product tightens the accuracy threshold from 90% to 95% after the initial draft, the change log shows the date, the old value, the new value, and the justification. If Legal adds a new criterion for audit logging, the change log shows who requested it and why. The change log prevents revisionist history. Six months into development, someone will claim "we always said we needed 95% accuracy" when the original criteria said 90%. The change log proves what was agreed when. It also prevents criteria drift. If the criteria document has 20 changes between draft and sign-off, something is wrong. Either the initial criteria were poorly researched, or stakeholders are treating the document as a wishlist instead of a contract. A well-run criteria process has three to five revisions before sign-off, not thirty.

## How Sign-Off Prevents Scope Creep

Scope creep in AI projects usually appears as criteria creep. The signed criteria say 90% accuracy. The model achieves 91%. Then Product says "could we get to 93% if we add two more features to the input?" Engineering says yes, it's possible, and now you're optimizing for a threshold no one agreed to. Or the criteria say "classify support tickets into five categories." The model works. Then a VP sees a demo and says "this is great, could it also extract the customer's sentiment and urgency?" Now you're building a different system. Sign-off prevents scope creep by making criteria changes visible and costly. If Product wants to raise the accuracy threshold, they must reopen the criteria review, document the justification, and get new sign-off from all stakeholders. That process takes time and political capital. It forces Product to justify why 93% is worth delaying launch by four weeks. Most of the time, the answer is it's not worth it, and the team ships at 91%.

The sign-off also prevents post-hoc rationalization. Post-hoc rationalization happens when a system fails to meet criteria and someone invents a reason why the criterion was wrong. The model achieves 87% accuracy instead of 90%, and Product says "actually, 87% is fine because our users don't care about accuracy, they care about speed." If that were true, the criterion should have been speed, not accuracy. The sign-off forces honest evaluation. Either the system meets the criteria or it doesn't. If it doesn't, you have three options: fix the system, relax the criterion with a new review and sign-off, or cancel the project. What you cannot do is pretend the criterion never mattered. The signature page has Product's name on it, committing to 90% accuracy as a deployment gate. If Product now claims 87% is acceptable, they are either admitting the original criterion was wrong or admitting they want to ship a system that doesn't meet requirements. Either admission requires explaining to stakeholders why their time in the criteria review was wasted.

Sign-off also protects Engineering from feature requests disguised as bug fixes. A user complains that the sentiment classifier labels "this product is not terrible" as neutral instead of positive. Product files a ticket saying "the model is wrong, fix it." Engineering looks at the criteria. The criteria specify 85% agreement with human labels on a specific test set. The model achieves 87%. The user's example is not a bug. It's a disagreement about what "positive sentiment" means, and that disagreement should have been resolved in the criteria review by specifying the edge cases and the labeling guidelines. Engineering can point to the sign-off and say "this behavior is within spec, if you want different behavior we need to reopen criteria and retrain the model." Without sign-off, Engineering has no defense against infinite nitpicking.

## When to Re-Open a Signed-Off Criteria Document

You re-open a signed-off criteria document in exactly three situations: when you discover a new risk that was not foreseeable at sign-off, when a regulatory or business requirement changes externally, or when the project becomes infeasible under the current criteria. New risk means something you could not have known during the original review. A competitor launches a feature that changes user expectations. A security researcher publishes an attack that applies to your model architecture. A production incident at another company reveals a failure mode you didn't consider. These are valid reasons to re-open. "We didn't think about this carefully enough" is not a valid reason. That's a failure of the original review process, not new information.

External changes are rare but legitimate. The EU AI Act adds your use case to the high-risk category, and now you need criteria for human oversight and audit trails that weren't required when you started. A key customer changes their SLA from 500ms to 200ms response time, and that customer represents 40% of your revenue. Your cloud provider deprecates the GPU instance type you planned to use, and the replacement instance has different cost and performance characteristics. These changes are outside the team's control and they materially affect what's buildable. You re-open the criteria, update the relevant thresholds or add new criteria, and hold a new review meeting with sign-off. The change log documents the external forcing function.

Infeasibility is the hardest case. Engineering signed off on criteria they believed were achievable, but halfway through development they discover those criteria are mutually exclusive or require technology that doesn't exist. The model can hit 95% accuracy or 100ms latency, but not both. The explanation method that Legal requires doesn't work with the model architecture that Engineering chose. Infeasibility requires escalation, not quiet re-scoping. Engineering must present data showing why the criteria are unachievable, propose alternative criteria with trade-offs, and get executive approval to re-open. If the infeasibility was foreseeable at the time of sign-off, that's an Engineering failure and Engineering owns the consequences. If the infeasibility emerged from new information or incorrect assumptions that everyone shared, you re-open without blame.

When you re-open criteria, you do not edit the existing document. You create a new version, mark the previous version as superseded, and document the reason for reopening in the change log. The new version goes through the full review and sign-off process. Stakeholders are not obligated to agree to relaxed criteria. If Product signed off on 95% accuracy and Engineering now says only 90% is feasible, Product can decide the project is no longer worth building and cancel it. That is a legitimate outcome. The criteria review is not a negotiation where you lowball your initial offer and then meet in the middle. It's a commitment, and if you can't meet the commitment, the other party has no obligation to accept less.

## The 2026 Context: EU AI Act Documentation Requirements

As of February 2025, the EU AI Act requires documented and verifiable success criteria for any AI system classified as high-risk. High-risk systems include those used for credit scoring, employment decisions, law enforcement, critical infrastructure, and access to essential services. If your AI product falls into a high-risk category and you operate in the EU or serve EU customers, your success criteria must be documented before development, reviewed by relevant stakeholders including domain experts and compliance officers, and stored as part of your technical documentation for regulatory inspection. The criteria must specify accuracy metrics, performance requirements, and robustness measures. They must also define how you will test for bias and ensure human oversight. A Slack thread where people agreed to some thresholds does not meet the regulatory standard. A signed criteria document with version control does.

The AI Act also requires that you document the rationale for your chosen thresholds. It is not sufficient to say "we require 90% accuracy." You must explain why 90% is the right threshold given the use case, the state of the art, the human baseline, and the potential harms from false positives and false negatives. This rationale is exactly what a good criteria review meeting produces. When Product, Engineering, Legal, and Domain Experts debate whether 90% or 95% is the right threshold, they are building the justification the regulator will ask for. The meeting notes and the change log become part of your compliance documentation. If you cannot explain why you chose a threshold, you probably chose it arbitrarily, and arbitrary thresholds are not defensible in a regulatory audit.

For non-EU teams, the AI Act still matters because it sets the global standard. If you want to sell AI products internationally, you will need criteria documentation that meets EU requirements even if you're based in the US or Asia. The companies that treat criteria review as a checkbox exercise will struggle to enter regulated markets. The companies that treat it as a forcing function for rigorous thinking will have documentation ready when the regulator asks. This is not a new burden. It's a formalization of what good teams already do. The only teams hurt by documentation requirements are teams that were building systems without knowing what success looked like.

## Making Review Meetings Work

The criteria review meeting is not a status update or a brainstorming session. It is a decision-making meeting, and decision-making meetings require preparation, facilitation, and follow-through. Preparation means every stakeholder reads the criteria document before the meeting and comes with written questions or objections. If someone shows up without having read the document, you reschedule. Facilitation means someone owns the meeting agenda, keeps discussion on track, and forces resolution of open questions. The facilitator is not a note-taker. They are a referee who ensures every criterion gets challenged and every disagreement gets resolved or escalated. Follow-through means action items have owners and deadlines, and the next meeting does not happen until all action items are closed.

The meeting should be 90 minutes, no more. If you need more than 90 minutes, your criteria document is too long or too vague. Split it into must-haves and nice-to-haves, review the must-haves in the first meeting, and schedule a second meeting for the rest. If you need more than two meetings, your project is too large to have a single set of success criteria. Break it into phases with separate criteria documents for each phase. The output of the meeting is a decision: sign-off, no sign-off with action items, or project cancellation. Anything else is a waste of time.

Getting sign-off before building is not bureaucracy. It is risk management. Every hour spent debating criteria in a conference room saves ten hours of rework when the system is half-built and someone realizes you optimized for the wrong metric. Every signature on the criteria document is insurance against scope creep, post-hoc rationalization, and blame-shifting when something goes wrong. The teams that skip criteria review or treat it as a formality are the teams that rebuild their models three times and still don't deploy. The teams that run rigorous, contentious, stakeholder-inclusive criteria reviews are the teams that ship systems that work and get used.

Now that you have sign-off on what success looks like, you need to understand what failure looks like. Not all criteria failures are equally bad. Missing your latency target by 20% might annoy users. Misclassifying a medical diagnosis might kill someone. The next step is to tier your criteria by impact and figure out what breaks if each one is wrong.
