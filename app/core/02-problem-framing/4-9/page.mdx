# 4.9 â€” The Success Criteria Review: Getting Sign-Off Before Building

In March 2025, a Series B fintech startup building a credit underwriting AI spent fourteen weeks developing a model that no one would deploy. The team had documented success criteria in a shared Google Doc: 92% approval accuracy, sub-200ms inference latency, and "fair outcomes across protected classes." Engineering built exactly what was specified. The model hit 93.4% accuracy and averaged 140ms response time. But when Legal reviewed the fairness testing methodology two weeks before launch, they refused sign-off. The criteria document said "fair outcomes" but never defined the metric, the comparison baseline, or the threshold. Engineering had used demographic parity at 5% tolerance. Legal required equalized odds at 2% tolerance, a metric the model couldn't hit without retraining on rebalanced data. Compliance had never been in the criteria review meeting because Product thought "fairness" was just a data science problem. The company delayed launch by eleven weeks, burned $340,000 in rework, and lost their first enterprise customer to a competitor. The VP of Product left three months later. The root cause wasn't a bad model or unclear writing. It was treating success criteria as a draft document instead of a signed contract between stakeholders.

## Why Success Criteria Need Formal Review

Success criteria are not documentation. They are a binding agreement between everyone who can kill your project. Product can kill it for missing business metrics. Engineering can kill it for violating performance budgets. Legal can kill it for compliance gaps. Domain experts can kill it for producing nonsense outputs. QA can kill it for untestable acceptance conditions. If any of these groups sees the criteria for the first time after you've built the system, you have built the wrong system. The purpose of a formal review is to surface disagreements when they cost zero dollars to resolve instead of when they cost six figures in rework. A good criteria review meeting is contentious, specific, and ends with written sign-off from every stakeholder who has veto power over deployment. A bad criteria review is a calendar placeholder where people nod along to slides and submit "looks good" in Slack. You need the former.

The review serves three functions that no other process step provides. First, it forces precision on vague words. "Accurate" means nothing until you specify the metric, the test set, the baseline, and the threshold. "Fast" means nothing until you specify the percentile, the load condition, and the measurement point. A review meeting is where Product says "accurate predictions" and Engineering says "accurate compared to what" and you don't leave the room until you have a number and a test procedure. Second, the review surfaces hidden constraints. Legal might require audit logs that add 40ms to every request. Domain experts might require explanations that double your inference cost. These constraints change what's buildable, and you need to know them before sprint planning, not during QA. Third, the review creates accountability. When everyone signs a document that says "we will deploy if and only if the system achieves X, Y, and Z," no one can later add a secret criterion W or claim they never agreed to the tradeoffs. The sign-off is a forcing function for honesty.

## Who Must Be in the Room

The criteria review meeting requires every role that can veto deployment. If someone can say "we're not shipping this" after the system is built, they must be in the criteria review before you write the first line of code. Missing a stakeholder is the most common reason criteria reviews fail. The core attendees are Product, Engineering, Domain Experts, Legal, and QA. Product owns the business metrics and user impact thresholds. Engineering owns the performance and cost budgets. Domain experts own the correctness and safety definitions. Legal owns compliance and risk. QA owns testability and acceptance procedures. If your product touches regulated domains, add Compliance as a separate role from Legal. If you're building internal tooling, add representatives from the teams who will use the system. If you're replacing a human workflow, bring the humans who currently do the work.

Product and Engineering are never sufficient by themselves. The credit underwriting startup failed because Legal wasn't in the room. A healthcare AI company spent nine weeks rebuilding a triage model because the clinical domain expert who reviewed criteria had never worked in an emergency department and didn't know that ED nurses need predictions in under 10 seconds, not under 60 seconds. A logistics company's route optimization system passed all success criteria but drivers refused to use it because the criteria never specified "routes must avoid left turns across traffic during peak hours," a constraint every driver knew but no one from the warehouse team thought to mention. The stakeholder list is not "who understands AI" or "who requested the feature." It's "who can prevent deployment and who will be blamed if something breaks."

Domain experts are not optional. If you're building medical AI, you need a practicing clinician, not a data scientist with a biology degree. If you're building legal AI, you need a lawyer who practices in the relevant jurisdiction, not a paralegal. If you're building content moderation AI, you need a trust and safety specialist who has reviewed millions of violations, not a policy intern. The domain expert's job is to translate real-world correctness into measurable criteria. They are the ones who know that a radiology model must never miss a pneumothorax even if it means 100 false positives, or that a contract review model must flag every indemnification clause even if it means flagging some standard warranty language. Engineering and Product will optimize for aggregate metrics. Domain experts enforce the "never do this" constraints that prevent catastrophic failures on edge cases.

## The Review Meeting Format

A successful criteria review meeting has three phases: presentation, debate, and decision. The presentation phase is not a readout of the document. Everyone has read the document before the meeting. The presentation is a 10-minute summary of the three hardest tradeoffs and the three biggest risks. Product presents the business context and the user impact thresholds. Engineering presents the performance and cost constraints. The criteria owner, usually a PM or tech lead, presents the proposed success metrics and asks for the specific questions the team needs to resolve. If you're spending more than 15 minutes on presentation, you're wasting time. The point is to get to debate.

The debate phase is where you test every word that means two different things to two different people. Someone reads each criterion aloud. Someone else explains how they would test it. If two people describe different tests, the criterion is ambiguous and must be rewritten in the meeting. You are looking for gaps, conflicts, and hidden assumptions. Gaps are things the criteria don't measure that someone cares about. Conflicts are criteria that can't all be true at the same time. Hidden assumptions are things the criteria assume but don't state, like "the input data will be formatted correctly" or "users will provide feedback within 24 hours." When you find a gap, you either add a criterion or you explicitly document that the gap is out of scope and get agreement from the person who raised it. When you find a conflict, you either relax one threshold or you declare the project infeasible and stop. When you find a hidden assumption, you either validate it with data or you turn it into a testable criterion.

The decision phase is binary. Either the group agrees on every criterion and signs off, or they don't and the meeting ends with assigned action items to resolve the open questions. There is no "we mostly agree" sign-off. Partial agreement is no agreement. If Legal says "I need to check whether 95% accuracy meets our regulatory threshold," the review is incomplete and you schedule a follow-up. If the domain expert says "I think 200ms is fine but I want to test it with five users first," you do the user test before sign-off, not after. The output of a successful review meeting is a version-controlled document with a signature block showing name, role, date, and explicit statement of agreement. Digital signatures are fine. Email confirmations are fine. Verbal "sounds good" is not fine. The signature means "I agree this is the right system to build and I commit to supporting deployment if these criteria are met."

## Common Failure Modes in Reviews

The most common failure mode is rubber-stamping. Rubber-stamping happens when stakeholders show up but don't engage. They don't read the document before the meeting. They don't ask questions. They say "looks good to me" because they're busy and the meeting is cutting into their sprint planning time. Rubber-stamping is worse than skipping the review entirely because it creates false confidence. The team thinks they have alignment, but they have attendance records. You prevent rubber-stamping by making the review meeting adversarial by design. The facilitator's job is to ask "Legal, what's the compliance risk if we miss this threshold?" and "Engineering, what breaks if we require this latency?" and "Domain expert, describe the worst-case failure if this metric is wrong." If someone gives a generic answer, you push. If someone says "I trust the team," you ask them to explain the criterion in their own words. Agreement is cheap. Informed agreement is expensive. You need the expensive version.

The second failure mode is bikeshedding, where the meeting spends 40 minutes debating the name of a metric and five minutes on the threshold that actually matters. Bikeshedding happens when criteria are presented without prioritization. Every criterion looks equally important, so people focus on the ones they understand instead of the ones that carry risk. You prevent bikeshedding by tagging every criterion with a priority: must-have, performance target, or aspirational. Must-have criteria are binary pass-fail requirements. If you miss them, you don't deploy. Performance targets are thresholds you expect to hit but might negotiate under specific conditions. Aspirational criteria are nice-to-haves that don't block deployment. The review meeting focuses on must-haves first, performance targets second, and aspirational criteria only if there's time. If someone tries to debate the wording of an aspirational criterion, you table it.

The third failure mode is missing stakeholders. You think you have everyone, but you don't. The criteria review happens without Security, and then three days before launch Security requires pen-testing that adds two weeks to the timeline. The review happens without the Data Engineering team, and then you discover the input pipeline can't deliver data at the required freshness SLA. The review happens without Finance, and then Finance says your projected inference cost violates the unit economics. You prevent missing stakeholders by doing a pre-review checklist. For each criterion, ask "who can block deployment if this is wrong?" and "who gets fired if this breaks in production?" Those people must be in the review. If you're unsure whether a role is a stakeholder, invite them and let them decline. False negatives are catastrophic. False positives are one wasted hour.

## Handling Disagreements Between Stakeholders

Disagreements in criteria reviews are good. They surface real conflicts before those conflicts become rework. The question is not whether stakeholders will disagree but how you resolve disagreement without diluting the criteria into uselessness. There are three types of disagreement: metric choice, threshold calibration, and priority conflict. Each requires a different resolution strategy.

Metric choice disagreements happen when two stakeholders want to measure the same thing differently. Product wants approval rate because that's what the business dashboard tracks. Engineering wants precision-recall because that's what the model outputs. Legal wants false negative rate on protected classes because that's what the auditor will check. These are not the same metric, and you can't optimize for all of them simultaneously. The resolution is not compromise. The resolution is decision rights. Someone has to own the final call, usually Product for business metrics, Legal for compliance metrics, and Engineering for performance metrics. The non-owners can add constraints. Legal can say "I don't care about your approval rate as long as false negative rate on protected classes is below 2%." Engineering can say "I don't care about precision-recall as long as inference latency stays under 200ms." But one person owns the primary success metric, and their metric is the one the team optimizes for.

Threshold calibration disagreements happen when stakeholders agree on the metric but not the number. Engineering says 95% accuracy is achievable. Product says 98% is required for user trust. Domain experts say 92% is the human baseline and anything above that is acceptable. You resolve calibration disagreements with data and with tiering. Data means you look at the human baseline, the competitor baseline, and the cost curve for improving the metric. If going from 95% to 98% accuracy requires doubling your training data budget and delaying launch by two months, that's a trade-off Product can evaluate against business impact. Tiering means you set a launch threshold and a stretch threshold. You deploy at 95%, but you commit to improving to 98% within six months post-launch. The criteria document specifies both thresholds and the timeline for the stretch goal. If Product refuses to launch below 98% and Engineering can't deliver 98% in the required timeline, the project is not feasible and you stop.

Priority conflicts happen when criteria are mutually exclusive. Legal requires explanations for every prediction. Engineering says explanations add 300ms to response time. The performance budget is 200ms. You cannot satisfy both criteria. The resolution is escalation. The criteria review meeting is not the place to make existential trade-offs between compliance and performance. That decision belongs to the executive team. Your job in the review meeting is to document the conflict clearly and escalate it with a recommendation. The recommendation includes the cost of each option, the risk of each option, and your proposed path. If the executives decide compliance wins, you relax the latency budget or cut scope. If the executives decide performance wins, you document the legal risk and get sign-off from Legal that they understand the tradeoff. What you never do is paper over the conflict with vague language like "we'll do our best to minimize latency while providing explanations." That is not a criterion. That is a recipe for rework.

## The Sign-Off Artifact

The sign-off artifact is a version-controlled document that records who agreed to what and when. It is not the criteria document itself. It is a signature page appended to the criteria document, showing the name, role, date, and statement of agreement for each stakeholder. The statement of agreement is explicit: "I agree that the success criteria in version 2.3 of this document, dated April 15, 2026, define the system we should build. I commit to supporting deployment if these criteria are met, and I understand that requests for additional criteria after sign-off require re-opening this review." The signature page is not editable after sign-off. If you need to change the criteria, you version the document, update the criteria, and hold a new review meeting with new signatures.

The sign-off artifact commits the team to two things. First, it commits each stakeholder to deploying the system if the criteria are met. Product cannot later say "the model is too conservative" if it hits the specified precision threshold. Legal cannot later add a new compliance requirement that wasn't in the signed criteria. Engineering cannot later claim the performance budget was unrealistic if they signed off on it. The sign-off is a one-way door. You can always decide not to deploy a system that meets criteria, but you cannot block deployment of a system that meets criteria unless you can demonstrate a new risk that was not foreseeable at the time of sign-off. Second, it commits the team to not building the system if the criteria are unachievable. If Engineering signs off on a 50ms latency requirement and later discovers that the required model architecture can't run under 80ms, they must escalate immediately. The sign-off is not a promise to try hard. It's a commitment that the criteria are feasible.

The artifact includes a change log that records every modification to the criteria and the reason for the change. If Product tightens the accuracy threshold from 90% to 95% after the initial draft, the change log shows the date, the old value, the new value, and the justification. If Legal adds a new criterion for audit logging, the change log shows who requested it and why. The change log prevents revisionist history. Six months into development, someone will claim "we always said we needed 95% accuracy" when the original criteria said 90%. The change log proves what was agreed when. It also prevents criteria drift. If the criteria document has 20 changes between draft and sign-off, something is wrong. Either the initial criteria were poorly researched, or stakeholders are treating the document as a wishlist instead of a contract. A well-run criteria process has three to five revisions before sign-off, not thirty.

## How Sign-Off Prevents Scope Creep

Scope creep in AI projects usually appears as criteria creep. The signed criteria say 90% accuracy. The model achieves 91%. Then Product says "could we get to 93% if we add two more features to the input?" Engineering says yes, it's possible, and now you're optimizing for a threshold no one agreed to. Or the criteria say "classify support tickets into five categories." The model works. Then a VP sees a demo and says "this is great, could it also extract the customer's sentiment and urgency?" Now you're building a different system. Sign-off prevents scope creep by making criteria changes visible and costly. If Product wants to raise the accuracy threshold, they must reopen the criteria review, document the justification, and get new sign-off from all stakeholders. That process takes time and political capital. It forces Product to justify why 93% is worth delaying launch by four weeks. Most of the time, the answer is it's not worth it, and the team ships at 91%.

The sign-off also prevents post-hoc rationalization. Post-hoc rationalization happens when a system fails to meet criteria and someone invents a reason why the criterion was wrong. The model achieves 87% accuracy instead of 90%, and Product says "actually, 87% is fine because our users don't care about accuracy, they care about speed." If that were true, the criterion should have been speed, not accuracy. The sign-off forces honest evaluation. Either the system meets the criteria or it doesn't. If it doesn't, you have three options: fix the system, relax the criterion with a new review and sign-off, or cancel the project. What you cannot do is pretend the criterion never mattered. The signature page has Product's name on it, committing to 90% accuracy as a deployment gate. If Product now claims 87% is acceptable, they are either admitting the original criterion was wrong or admitting they want to ship a system that doesn't meet requirements. Either admission requires explaining to stakeholders why their time in the criteria review was wasted.

Sign-off also protects Engineering from feature requests disguised as bug fixes. A user complains that the sentiment classifier labels "this product is not terrible" as neutral instead of positive. Product files a ticket saying "the model is wrong, fix it." Engineering looks at the criteria. The criteria specify 85% agreement with human labels on a specific test set. The model achieves 87%. The user's example is not a bug. It's a disagreement about what "positive sentiment" means, and that disagreement should have been resolved in the criteria review by specifying the edge cases and the labeling guidelines. Engineering can point to the sign-off and say "this behavior is within spec, if you want different behavior we need to reopen criteria and retrain the model." Without sign-off, Engineering has no defense against infinite nitpicking.

## When to Re-Open a Signed-Off Criteria Document

You re-open a signed-off criteria document in exactly three situations: when you discover a new risk that was not foreseeable at sign-off, when a regulatory or business requirement changes externally, or when the project becomes infeasible under the current criteria. New risk means something you could not have known during the original review. A competitor launches a feature that changes user expectations. A security researcher publishes an attack that applies to your model architecture. A production incident at another company reveals a failure mode you didn't consider. These are valid reasons to re-open. "We didn't think about this carefully enough" is not a valid reason. That's a failure of the original review process, not new information.

External changes are rare but legitimate. The EU AI Act adds your use case to the high-risk category, and now you need criteria for human oversight and audit trails that weren't required when you started. A key customer changes their SLA from 500ms to 200ms response time, and that customer represents 40% of your revenue. Your cloud provider deprecates the GPU instance type you planned to use, and the replacement instance has different cost and performance characteristics. These changes are outside the team's control and they materially affect what's buildable. You re-open the criteria, update the relevant thresholds or add new criteria, and hold a new review meeting with sign-off. The change log documents the external forcing function.

Infeasibility is the hardest case. Engineering signed off on criteria they believed were achievable, but halfway through development they discover those criteria are mutually exclusive or require technology that doesn't exist. The model can hit 95% accuracy or 100ms latency, but not both. The explanation method that Legal requires doesn't work with the model architecture that Engineering chose. Infeasibility requires escalation, not quiet re-scoping. Engineering must present data showing why the criteria are unachievable, propose alternative criteria with trade-offs, and get executive approval to re-open. If the infeasibility was foreseeable at the time of sign-off, that's an Engineering failure and Engineering owns the consequences. If the infeasibility emerged from new information or incorrect assumptions that everyone shared, you re-open without blame.

When you re-open criteria, you do not edit the existing document. You create a new version, mark the previous version as superseded, and document the reason for reopening in the change log. The new version goes through the full review and sign-off process. Stakeholders are not obligated to agree to relaxed criteria. If Product signed off on 95% accuracy and Engineering now says only 90% is feasible, Product can decide the project is no longer worth building and cancel it. That is a legitimate outcome. The criteria review is not a negotiation where you lowball your initial offer and then meet in the middle. It's a commitment, and if you can't meet the commitment, the other party has no obligation to accept less.

## The 2026 Context: EU AI Act Documentation Requirements

As of February 2025, the EU AI Act requires documented and verifiable success criteria for any AI system classified as high-risk. High-risk systems include those used for credit scoring, employment decisions, law enforcement, critical infrastructure, and access to essential services. If your AI product falls into a high-risk category and you operate in the EU or serve EU customers, your success criteria must be documented before development, reviewed by relevant stakeholders including domain experts and compliance officers, and stored as part of your technical documentation for regulatory inspection. The criteria must specify accuracy metrics, performance requirements, and robustness measures. They must also define how you will test for bias and ensure human oversight. A Slack thread where people agreed to some thresholds does not meet the regulatory standard. A signed criteria document with version control does.

The AI Act also requires that you document the rationale for your chosen thresholds. It is not sufficient to say "we require 90% accuracy." You must explain why 90% is the right threshold given the use case, the state of the art, the human baseline, and the potential harms from false positives and false negatives. This rationale is exactly what a good criteria review meeting produces. When Product, Engineering, Legal, and Domain Experts debate whether 90% or 95% is the right threshold, they are building the justification the regulator will ask for. The meeting notes and the change log become part of your compliance documentation. If you cannot explain why you chose a threshold, you probably chose it arbitrarily, and arbitrary thresholds are not defensible in a regulatory audit.

For non-EU teams, the AI Act still matters because it sets the global standard. If you want to sell AI products internationally, you will need criteria documentation that meets EU requirements even if you're based in the US or Asia. The companies that treat criteria review as a checkbox exercise will struggle to enter regulated markets. The companies that treat it as a forcing function for rigorous thinking will have documentation ready when the regulator asks. This is not a new burden. It's a formalization of what good teams already do. The only teams hurt by documentation requirements are teams that were building systems without knowing what success looked like.

## Making Review Meetings Work

The criteria review meeting is not a status update or a brainstorming session. It is a decision-making meeting, and decision-making meetings require preparation, facilitation, and follow-through. Preparation means every stakeholder reads the criteria document before the meeting and comes with written questions or objections. If someone shows up without having read the document, you reschedule. Facilitation means someone owns the meeting agenda, keeps discussion on track, and forces resolution of open questions. The facilitator is not a note-taker. They are a referee who ensures every criterion gets challenged and every disagreement gets resolved or escalated. Follow-through means action items have owners and deadlines, and the next meeting does not happen until all action items are closed.

The meeting should be 90 minutes, no more. If you need more than 90 minutes, your criteria document is too long or too vague. Split it into must-haves and nice-to-haves, review the must-haves in the first meeting, and schedule a second meeting for the rest. If you need more than two meetings, your project is too large to have a single set of success criteria. Break it into phases with separate criteria documents for each phase. The output of the meeting is a decision: sign-off, no sign-off with action items, or project cancellation. Anything else is a waste of time.

The pre-meeting preparation phase is where most review failures actually originate. When you send the criteria document, you do not just attach it to a calendar invite. You send it with specific instructions. Each stakeholder must read the document and submit written feedback 24 hours before the meeting. The feedback template asks three questions: which criteria do you disagree with, which criteria are unclear, and what criteria are missing. If a stakeholder does not submit feedback, you follow up directly. If they still do not respond, you escalate to their manager. You do not walk into a review meeting with stakeholders who have not engaged with the material. That meeting will be a waste because the real review will happen afterward when people finally read the document and raise objections.

During the meeting itself, the facilitator enforces a strict protocol. Each criterion is read aloud. The owner of that criterion explains the rationale and the measurement method. The facilitator asks each stakeholder in turn whether they approve, have concerns, or dissent. If anyone voices a concern, the discussion stays on that criterion until the concern is resolved, documented as a known risk, or escalated as an action item. You do not move to the next criterion while disagreement remains unaddressed. This discipline prevents the common pattern where meetings discuss everything superficially and resolve nothing substantively.

The post-meeting follow-up determines whether the review actually accomplished anything. Within 24 hours of the meeting, the facilitator sends a summary document to all attendees. This summary lists every criterion reviewed, every objection raised, every decision made, and every action item assigned. For action items, the summary includes the owner, the deadline, and the specific question to be resolved. Stakeholders have 48 hours to correct any misrepresentations in the summary. After 48 hours, the summary becomes the official record. If action items exist, you schedule the next review meeting for after all action items are closed. You do not hold a second review meeting to discuss what was not finished from the first. You wait until the work is done.

## The Review as Organizational Learning

A well-run criteria review teaches your organization how to think about AI systems. Most teams building AI products in 2026 are still learning what questions to ask, what risks to consider, and what trade-offs matter. The review meeting is where junior product managers learn that accuracy is not a single number but a distribution across use cases. It is where engineers learn that Legal cares about auditability as much as performance. It is where Legal learns that some compliance requirements are architecturally expensive and require early involvement. It is where executives learn that their intuition about what is easy or hard in AI is often wrong.

The review creates a shared vocabulary. When Product and Engineering both understand that they agreed to 90 percent accuracy measured on a specific test set using a specific metric, they can have productive conversations later about whether the system is meeting that bar. When Legal and Engineering both understand that explainability requires model architecture choices that affect latency, they can negotiate those trade-offs rationally instead of fighting about them during launch review. The precision forced by the review meeting becomes the foundation for every subsequent discussion about the project.

The review also surfaces organizational dysfunction that needs fixing. If Legal is consistently surprised by compliance requirements in review meetings, your requirements gathering process is broken. If Domain Experts regularly reject criteria that Product and Engineering thought were solid, your user research process is broken. If Engineering keeps signing off on infeasible criteria and then reporting infeasibility months later, your estimation process is broken. The review does not fix these dysfunctions, but it makes them visible so leadership can address them.

## Common Anti-Patterns in Criteria Reviews

The drive-by review is the most common anti-pattern. Product schedules a 30-minute meeting, presents slides about the project, shows some proposed metrics, and asks if anyone has concerns. No one has read a document because no document exists. No one raises concerns because they have not had time to think through implications. The meeting ends with vague agreement and no commitments. Two months later, someone realizes the criteria were nonsense and the project restarts. The drive-by review is worse than no review because it creates false documentation that a review happened.

The rubber-stamp review is the second most common anti-pattern. The criteria document exists and is detailed. The meeting happens with all stakeholders present. But no one challenges anything. Everyone says the criteria look fine. Sign-off is immediate and unanimous. This happens when the team has not empowered stakeholders to dissent, when stakeholders do not understand their veto power, or when the organizational culture punishes critical feedback. The result is the same as the drive-by review: criteria that look solid until they encounter reality.

The endless debate anti-pattern is less common but equally destructive. The review meeting becomes a philosophy seminar where stakeholders argue about the nature of accuracy, the epistemology of ground truth, and whether any metric can truly capture quality. These debates are intellectually interesting and practically useless. The facilitator must cut them off. If a stakeholder wants to debate fundamentals, that debate happens outside the review meeting. The review meeting is for deciding on specific, measurable criteria, not for resolving philosophical disagreements about measurement theory.

The retrofit anti-pattern happens when the team has already built the system and is now trying to define success criteria that the existing system can meet. This is not a criteria review. This is post-hoc rationalization. The team picks metrics where the system performs well and ignores metrics where it performs poorly. The criteria become a description of what was built rather than a specification of what should be built. If you are in a criteria review and someone says we should use metric X because that is what the model already outputs, you are in a retrofit. Stop the meeting and restart the project with proper framing.

## The Criteria Review in Regulated Contexts

In regulated industries, the criteria review is not just good practice. It is often a legal requirement. The EU AI Act mandates that high-risk AI systems have documented, stakeholder-reviewed success criteria before deployment. Healthcare regulations in many jurisdictions require clinical validation protocols that specify success metrics and review processes. Financial services regulations require model risk management frameworks that include criteria definition and stakeholder sign-off. If you are building AI in these domains, the criteria review is your compliance artifact. The meeting notes, the sign-off document, and the change log are part of your regulatory documentation.

This means the review must meet higher standards than an informal product discussion. The document must be version-controlled with timestamps. The sign-off must include not just names but roles and organizational authority. The meeting must have minutes that record who said what and what decisions were made. If a regulator audits your AI system three years after launch and asks how you determined that 95 percent accuracy was sufficient, you must be able to produce the criteria document, the review meeting minutes, and the rationale from the stakeholders who signed off. If you cannot, you have a compliance gap.

Regulated contexts also mean you cannot skip stakeholders who are inconvenient or slow to respond. If your compliance officer is busy and does not want to attend the review, you wait until they are available. If your medical director is out of the office, you reschedule. If your chief legal counsel wants their deputy to attend instead, you verify that the deputy has authority to sign off on behalf of Legal. You do not proceed with a review that is missing a required stakeholder because you are on a deadline. The deadline is not more important than the regulatory requirement.

## The Cost of Skipping the Review

Teams skip criteria reviews because they think reviews slow them down. The opposite is true. A rigorous review takes one week of calendar time: three days for stakeholders to read and submit feedback, one day for the review meeting, three days for follow-up and sign-off. If you skip the review, you save one week. Then you spend three months building to the wrong criteria, two weeks in launch review discovering the criteria were wrong, and six weeks rebuilding. You saved one week and lost four months. The math is not close.

The teams that consistently ship AI products that work and get adopted are the teams that treat the criteria review as the most important meeting in the project lifecycle. They prepare for it seriously. They invite the right people. They enforce pre-reading. They document decisions. They get real sign-off. They treat the signed criteria document as a binding contract. These teams have fewer surprises, less rework, and higher deployment rates. They are not slower. They are more efficient because they find and fix misalignment when it is cheap to fix.

The cost of skipping extends beyond the immediate project. When you ship a system that fails because criteria were never properly reviewed, you damage trust between teams. Engineering stops trusting Product to know what they want. Product stops trusting Engineering to deliver what was asked for. Legal stops trusting either group to consider compliance early enough. This trust erosion makes future projects harder because everyone is defensive and risk-averse. A single criteria review failure can poison collaboration for six months or more.

Skipping also creates precedent. When one team skips the criteria review and happens to get lucky, other teams see that and conclude the review is optional. The organizational standard degrades. Eventually someone skips the review and does not get lucky, and the resulting failure is larger than it would have been if the standard had been maintained. You cannot selectively apply rigor. Either criteria review is mandatory for all AI projects above a certain risk threshold, or it is optional for all projects and you accept the consequences.

## Practical Tactics for Running Effective Reviews

Start the review preparation at least two weeks before the target meeting date. You cannot write a good criteria document in two days. You need time to think through failure modes, research baseline metrics, consult with domain experts, and iterate on the thresholds. The document should go through at least two internal drafts within the core team before you send it to stakeholders for review. If you are sending stakeholders your first draft, you are wasting their time and your credibility.

When you schedule the review meeting, block 90 minutes on the calendar but tell stakeholders to plan for 60 minutes. The extra 30 minutes is buffer for debates that run long. If you finish early, everyone gets time back. If you run over slightly, you have room. Never schedule a criteria review for 30 minutes. That is not enough time to do it properly. Never schedule it for more than two hours. If you need more than two hours, your criteria document is too ambitious or too vague and needs to be split.

Send the criteria document to stakeholders at least five business days before the meeting. Do not send it the day before and expect thoughtful feedback. Include a cover email that explains what you need from each stakeholder. Do not just attach the document. Tell Legal that you need them to review the compliance criteria on pages three and four. Tell the domain expert that you need them to validate the accuracy thresholds on page two. Tell Engineering that you need them to confirm feasibility of the performance criteria on page five. Specific requests get better engagement than generic "please review" emails.

Use a shared document format that supports inline comments. Google Docs, Microsoft Word with track changes, or a wiki page with commenting. Do not send a PDF. You want stakeholders to comment directly on specific sentences, not send you separate emails with feedback. Inline comments keep feedback attached to context and make it easier to address. When stakeholders comment, respond to every comment before the meeting. Acknowledge the feedback, explain your thinking, or update the document. Walking into the meeting with unaddressed comments signals that you did not take the feedback seriously.

During the meeting, share your screen showing the criteria document. Walk through it section by section. Do not read it aloud word for word, but highlight the key points and the areas where you need decisions. When you reach a section with comments or known disagreements, stop and facilitate resolution. Do not move forward while disagreement is active. The meeting is not a presentation. It is a working session.

After the meeting, send the summary document within 24 hours while the discussion is fresh in everyone's memory. If you wait a week, people will not remember what was decided and you will get challenges to the summary. The summary is your last chance to catch misunderstandings before they become problems. If someone reads the summary and says that is not what I agreed to, you address it immediately. You do not wait until launch to discover the misunderstanding.

## Building a Criteria Review Culture

Organizations that do criteria reviews well have built a culture where the review is understood as a value-add process, not a bureaucratic hurdle. This culture does not appear by accident. It is built through repeated positive experiences where the review catches problems early and prevents expensive failures. The first few criteria reviews in an organization are often awkward and uncomfortable. Stakeholders do not know what to expect. They have not built the muscle for constructive critique. The meetings feel adversarial. This is normal.

The way you build the culture is by making the review process transparent and by celebrating catches. When a criteria review surfaces a major misalignment that would have caused rework, you acknowledge it publicly. You send a retrospective to the broader team explaining what was caught and what it would have cost if it had not been caught. You make the value visible. Over time, stakeholders start to see the review as insurance rather than overhead. They want their projects to go through rigorous review because they have seen what happens when projects skip it.

You also build the culture by making the criteria document template and examples accessible. New teams should not have to invent the document structure from scratch. You provide a template with the standard sections, example criteria for common task types, and guidance on how to write good justifications. You run training sessions where experienced practitioners share what makes a good criteria document and what makes a bad one. You create a library of past criteria documents that teams can reference. This infrastructure makes it easier to do the review well, which makes teams more likely to do it at all.

Leadership reinforcement is critical. If executives ask why a project was delayed and the answer is we did a thorough criteria review that identified scope problems, the executive should praise the team for catching it early, not punish them for the delay. If the executive instead says you should have moved faster and figured it out as you went, the team learns that criteria reviews are career-limiting. The behavior you reward is the behavior you get. Reward rigor and you get rigor. Reward speed over correctness and you get fast failures.

Getting sign-off before building is not bureaucracy. It is risk management. Every hour spent debating criteria in a conference room saves ten hours of rework when the system is half-built and someone realizes you optimized for the wrong metric. Every signature on the criteria document is insurance against scope creep, post-hoc rationalization, and blame-shifting when something goes wrong. The teams that skip criteria review or treat it as a formality are the teams that rebuild their models three times and still don't deploy. The teams that run rigorous, contentious, stakeholder-inclusive criteria reviews are the teams that ship systems that work and get used.

Now that you have sign-off on what success looks like, you need to understand what failure looks like. Not all criteria failures are equally bad. Missing your latency target by 20 percent might annoy users. Misclassifying a medical diagnosis might kill someone. The next step is to tier your criteria by impact and figure out what breaks if each one is wrong.
