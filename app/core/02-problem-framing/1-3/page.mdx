# 1.3 — The Framing-to-Evaluation Pipeline

In 2025, a fintech startup built a loan approval system designed to process applications faster than human underwriters. The model was trained on two years of historical loan data and achieved 92 percent accuracy on a held-out test set. The team celebrated. Executives approved the budget for full deployment.

Three months after launch, regulators audited the system and found that it was rejecting loan applications from certain demographic groups at disproportionate rates. The company was fined, the system was shut down, and the head of AI was asked to explain how this had happened.

The answer was not that the model was biased in some mysterious, unfixable way. The answer was that the team had never defined what "fair" meant during problem framing, so they never built an evaluation to measure fairness. They had optimized for accuracy—a metric they could measure—but accuracy was not the right metric. They needed to measure accuracy across demographic subgroups, false positive and false negative rates by group, and compliance with regulatory definitions of fairness.

None of that was in their eval suite because none of it was in their problem framing. The framing said "predict loan approval with high accuracy." It did not say "predict loan approval with high accuracy while maintaining equal false positive rates across protected groups." The evaluation measured what the framing specified, which was incomplete.

This is the iron law of the framing-to-evaluation pipeline: you can only measure what you define, and you can only define what you think to ask about during framing. If fairness is not in the framing, it will not be in the eval. If edge case handling is not in the framing, it will not be in the eval. If user satisfaction is not in the framing, it will not be in the eval.

Every decision you make during problem framing determines what you can measure, and what you can measure determines what you can improve. If you frame poorly, you build blindly.

## The Dependency Chain

The relationship between framing and evaluation is not indirect or abstract. It is a direct dependency chain, where each step depends on the outputs of the previous step:

**Problem framing defines the task.** What is the system supposed to do? What are the inputs and outputs? What are the success criteria? What are the constraints and edge cases?

**Task definition determines what ground truth looks like.** If the task is "classify customer support tickets into five categories," then ground truth is a dataset of tickets labeled with the correct category. If the task is "generate a summary of a meeting transcript," then ground truth is a set of transcripts paired with high-quality human-written summaries.

**Ground truth enables evaluation.** You cannot measure how well a system performs without something to compare its outputs against. If you do not have ground truth, you cannot build automated evals. You are stuck with manual review or vibes-based assessment, neither of which scales.

**Evaluation produces metrics.** Once you have ground truth and a way to compare system outputs to it, you can compute metrics: accuracy, precision, recall, F1, BLEU, ROUGE, human preference win rate, whatever is appropriate for the task.

**Metrics guide iteration.** You improve the system by changing something—prompt, model, data, architecture—and measuring whether the metric improves. Without metrics, you cannot iterate systematically. You are guessing.

**Metrics feed into monitoring.** In production, you use the same metrics to detect when the system degrades. If accuracy drops, or if false positive rates spike, you get alerted and investigate.

If any step in this chain is missing or broken, the entire pipeline collapses. If your framing does not define success criteria, you cannot create ground truth that reflects those criteria. If you do not have ground truth, you cannot build evals. If you do not have evals, you cannot measure progress or detect failure.

## What Happens When Framing Is Vague

Consider a team building a "content recommendation engine." The framing is vague: "Recommend relevant content to users." No further detail is provided about what "relevant" means.

The data science team needs to build an evaluation set, so they ask, "What makes a recommendation relevant?" The product manager says, "You know, something the user would be interested in." That is still vague, so the data scientist makes a decision: they will measure relevance using click-through rate. If users click on the recommendation, it must be relevant.

This seems reasonable until you think about it for ten seconds. Click-through rate measures whether users click, not whether they get value from the content. Clickbait headlines get high click-through rates. Misleading thumbnails get clicks. Content that triggers curiosity but delivers no substance gets clicks. If you optimize a recommendation engine for clicks, you will end up surfacing content that is attention-grabbing but low-quality.

The real goal might have been something else entirely: increase time spent on the platform, or increase user satisfaction, or increase the likelihood that users return tomorrow. But because the framing did not specify the goal, the team picked a proxy metric—clicks—that was easy to measure but misaligned with the actual goal.

Now suppose the team realizes six months later that click-through rate was the wrong metric. They want to switch to measuring user satisfaction. But they do not have ground truth for user satisfaction because they never collected it. They never collected it because it was not in the original framing. Now they have to go back, survey users, gather new data, build a new eval set, and retrain the model. They have lost six months.

If the framing had been precise from the start—"Recommend content that users rate as valuable, where valuable is defined as a four or five star rating on a post-consumption survey"—the team would have known to collect satisfaction ratings from day one. They would have built ground truth around satisfaction, designed evals around satisfaction, and optimized the model for satisfaction. The problem would have been solved correctly the first time.

## How Precise Framing Creates Clean Eval Pipelines

Now consider a different team building a document summarization system. Their framing is detailed:

- Task: Given a 10-20 page internal report, generate a one-paragraph summary that captures the main conclusion, the key supporting evidence, and any critical caveats.
- Success criteria: Summaries should be rated by domain experts as accurate (no factual errors), complete (includes main points), and concise (no unnecessary detail). Target: 85 percent of summaries rated 4 or 5 out of 5 by at least two independent raters.
- Edge cases: Handle reports with unclear conclusions, reports with conflicting data, reports where the main point is buried in an appendix.

This framing immediately tells you what ground truth looks like: a set of internal reports, each paired with a human-written summary that meets the quality bar. It tells you how to create that ground truth: hire domain experts to write summaries, and have other experts rate them.

It tells you how to evaluate the model: generate summaries, have experts rate them on accuracy, completeness, and conciseness, and measure the percentage that score 4 or 5 out of 5. It tells you what metrics to track: inter-rater agreement, rating distributions, and error types (factual errors vs. missing information vs. unnecessary verbosity).

It even tells you what edge cases to include in the eval set: reports with unclear conclusions, conflicting data, or buried main points. You can deliberately curate an eval set that includes these cases and measure how well the system handles them.

Because the framing is precise, the eval pipeline writes itself. You know exactly what data to collect, exactly what to measure, and exactly what success looks like. You are not guessing. You are not debating. You are executing against a clear spec.

## The Hidden Dependency: Success Criteria to Ground Truth

One of the least understood aspects of the framing-to-eval pipeline is the dependency between success criteria and ground truth. You cannot create ground truth until you have defined what "good" means, because ground truth is a collection of examples of "good."

If your success criterion is "summaries should be concise," you need ground truth summaries that are concise. But how concise? One sentence? One paragraph? Do you penalize including background information, or is some background acceptable? These are judgment calls, and they need to be made during framing, not during data labeling.

If you do not make these calls during framing, your ground truth will be inconsistent. One labeler will write concise, minimal summaries. Another will write longer, more contextualized summaries. The model will be trained on mixed signals, and it will produce inconsistent outputs.

Well-framed problems include annotation guidelines as part of the framing document. "Summaries should be one paragraph, approximately 100-150 words. Include the main conclusion in the first sentence. Include 2-3 key supporting points. Do not include background information unless it is necessary to understand the conclusion. Do not include caveats unless they meaningfully change the interpretation of the conclusion."

With guidelines this specific, labelers produce consistent ground truth. The model learns a coherent task. Evals are meaningful. Iteration is productive.

Without guidelines, you get noisy data, inconsistent model behavior, and evals that do not correlate with real-world performance.

## Edge Cases and Failure Modes

Another critical part of the framing-to-eval pipeline is identifying edge cases and failure modes during framing, so you can test for them during evaluation.

A poorly framed problem ignores edge cases. "Build a sentiment classifier." Fine. What happens if the input is sarcastic? What if it is written in a mix of languages? What if it contains emojis or slang? What if it is a statement with no clear sentiment, like "The meeting is at 3pm"? If these cases are not considered during framing, they will not be in the eval set, and the model will fail on them in production.

A well-framed problem enumerates edge cases and decides how to handle them. "Sarcasm: if detected, flag for manual review rather than auto-classifying. Mixed languages: classify based on the majority language. Emojis: treat as sentiment signals. Neutral statements: add a 'neutral' category rather than forcing positive/negative."

Once you have decided how to handle edge cases, you can build an eval set that includes them and measure whether the system handles them correctly. You can track edge case performance separately from mainline performance. You can set acceptable thresholds for each category.

This is not possible if edge cases are not identified during framing. You will discover them in production, after users encounter them, and you will scramble to fix them reactively.

## The Chain in Reverse: Monitoring Back to Framing

The dependency chain does not just flow forward from framing to eval. It flows backward from monitoring to framing.

In production, you monitor the system to detect when performance degrades. But what you monitor depends on what you defined during framing.

If your framing said "minimize false positives because they are costly," you monitor false positive rate in production. If the rate spikes, you investigate. If your framing said "ensure fairness across demographic groups," you monitor metrics by subgroup. If disparities emerge, you fix them.

If your framing was vague, you do not know what to monitor. You might track overall accuracy, but accuracy can stay high while the system fails in important ways. False positives might spike. Performance on a critical subgroup might degrade. User satisfaction might drop. You will not notice until it is too late.

The teams that build reliable AI systems in production are the ones that define monitoring requirements during framing. "We will track false positive rate, false negative rate, latency at p50 and p99, and user satisfaction score. Alerts fire if false positive rate exceeds 2 percent, latency p99 exceeds 500ms, or satisfaction drops below 4.0." These thresholds come from the success criteria defined during framing.

## The Compounding Cost of Skipping Framing

Here is what the failure cascade looks like when you skip rigorous framing:

**Week 1:** You start building without a clear task definition. You make assumptions about what the system should do.

**Week 4:** You need an eval set. You realize you do not have ground truth because you never defined what "good" looks like. You improvise. You label some data based on your current understanding.

**Week 8:** You train a model. It performs well on your improvised eval set. You ship it.

**Week 12:** Users complain. The system is failing in ways you did not anticipate. You investigate and realize the eval set did not cover important cases.

**Week 16:** You go back and collect better ground truth. You rebuild the eval set. You retrain the model.

**Week 20:** You ship the updated model. It is better, but still not quite right. There are edge cases you still have not covered.

**Week 24:** You iterate again. You are now six months in, and you still do not have a production-ready system.

If you had spent Week 1 doing rigorous problem framing—defining the task, enumerating edge cases, specifying success criteria, designing the eval pipeline—you would have built the right system by Week 12.

The time you save by skipping framing is illusory. You pay it back with interest during iteration, debugging, and rework.

## The Positive Spiral

The inverse is also true. When framing is done well, every downstream step becomes easier and faster.

You define the task precisely, so collecting ground truth is straightforward. You have annotation guidelines, so labelers are consistent. You have an eval set that reflects real-world use cases, so model performance on the eval correlates with user satisfaction. You have clear metrics, so iteration is fast and productive. You know what to monitor, so you catch issues before they escalate.

Good framing creates a virtuous cycle. Clear framing leads to good ground truth. Good ground truth leads to meaningful evals. Meaningful evals lead to effective iteration. Effective iteration leads to systems that work.

Teams that internalize this dependency chain treat framing as the most important step in the process. They do not rush it. They do not treat it as a formality. They invest the time to get it right, knowing that everything else depends on it.

## A Worked Example: Fraud Detection

Imagine you are building a fraud detection system for an e-commerce platform. Here is how the framing-to-eval pipeline works in practice.

**Framing:** Define the task as "classify each transaction as legitimate or fraudulent, optimizing for high recall (catch as many fraudulent transactions as possible) while keeping precision high enough that fewer than 5 percent of flagged transactions are false positives."

**Ground truth:** Create a labeled dataset of transactions where fraud status is known. Include a mix of fraud types (stolen credit cards, account takeover, friendly fraud). Include edge cases (high-value legitimate purchases, purchases from new users, international transactions).

**Eval design:** Measure recall and precision on the eval set. Break down performance by fraud type and edge case category. Set thresholds: recall must be at least 90 percent, precision must be at least 95 percent.

**Metrics:** Track recall, precision, false positive rate, false negative rate, and the distribution of fraud types in false negatives. Track performance over time to detect model drift.

**Iteration:** If recall is below target, investigate which fraud types are being missed. Collect more examples of those types. Retrain. If precision is below target, investigate false positives. Adjust thresholds or add features that distinguish edge cases from fraud.

**Monitoring:** In production, track the same metrics. Alert if recall drops below 85 percent or precision drops below 90 percent. Monitor false positive rate by customer segment to ensure you are not disproportionately flagging legitimate users in certain groups.

Every step follows directly from the framing. The framing defines recall and precision as the key metrics, so that is what you measure. It identifies fraud types and edge cases, so that is what you include in ground truth. It sets thresholds, so you know when the system is underperforming. The pipeline is coherent from start to finish because the framing was clear.

## The Foundational Argument

This chapter makes the case for why problem framing is foundational: it is the only step that determines what you can measure, and measurement is the only way to build reliable systems.

You cannot improve what you cannot measure. You cannot measure what you have not defined. You cannot define what you have not thought carefully about during framing.

Framing is not optional. It is not a box to check. It is the foundation of everything that comes after. Skip it, and you build on sand. Do it well, and you build on bedrock.

The next question is how much rigor you need, and the answer depends on the consequences of getting it wrong.
