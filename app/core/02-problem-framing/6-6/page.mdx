# 6.6 â€” The Task Complexity Matrix

In March 2023, a Series B fintech startup burned through eighteen engineer-months building a customer support chatbot that could answer tax questions. The team hit their milestone: the bot worked. It answered questions. It passed internal QA. They launched to three thousand customers in early access and within six days pulled the feature entirely. The problem was not that the model failed to generate responses. The problem was that no one on the team could reliably determine when those responses were correct. Their tax compliance officer would flag an answer as dangerous, their senior support rep would defend it, and their contract CPA would shrug and say it depended on the customer's state. The build took three months. The realization that they had no evaluation infrastructure took six days. The decision to kill the feature took one tense board meeting.

The root cause was a category error. They treated the project as a standard software build: scope it, build it, test it, ship it. They allocated resources as if complexity lived in the implementation. But this was not a normal software project. The hard part was never getting the model to produce text. The hard part was knowing whether that text was correct, and they discovered this only after the system was live and their compliance risk was sitting in three thousand inboxes. They had conflated build difficulty with evaluation difficulty, and that mistake cost them a quarter of roadmap progress.

## The Two Dimensions of Complexity

When you assess task complexity, you need two numbers, not one. The first is **build difficulty**: how hard is it to get a system that produces output. The second is **evaluation difficulty**: how hard is it to measure whether that output is correct. These are not correlated. Some tasks are trivial to build and trivial to evaluate. Others are trivial to build and nearly impossible to evaluate. A few are brutal on both axes. Your resource allocation, timeline, team composition, and risk posture all depend on where your task sits in this two-dimensional space.

Build difficulty is the familiar kind of complexity. It is the technical challenge of assembling data, training or prompting a model, integrating APIs, handling edge cases, optimizing latency, scaling throughput. If you have built software, you know how to estimate this. You break the problem into components, you count the unknowns, you assess your team's skill gaps, you multiply by your historical velocity and add a buffer. Build difficulty is legible. It shows up in sprint planning. It is what engineers argue about in design reviews.

Evaluation difficulty is less familiar and more treacherous. It is the challenge of defining correctness, generating test cases, running measurements, interpreting results, and building confidence that your system works. For some tasks this is easy: you have a dataset with labels, you compute accuracy, you are done. For other tasks this is a research problem in its own right. You need expert human raters. You need multi-stage rubrics. You need adversarial testing. You need statistical methods to reconcile disagreements. You need to run live experiments to see if users actually benefit. Evaluation difficulty is often invisible during planning, and it does not show up in your sprint backlog until you are trying to ship.

The key insight is that these two dimensions are independent. A task can be easy to build and hard to evaluate. A task can be hard to build and easy to evaluate. The difficulty of writing code does not predict the difficulty of measuring success. In 2026, with frontier models like GPT-4o and Claude 3.5 Sonnet, most generation tasks are easy to build: you write a prompt, you call an API, you get output. But evaluation has not gotten easier. If anything, it has gotten harder, because the output is more fluent and more persuasive and more likely to fool a casual reader. The result is that the modern AI product landscape is dominated by tasks in the Easy Build, Hard Eval quadrant, and most teams are not prepared for this.

## The Four Quadrants

Plot build difficulty on one axis and evaluation difficulty on the other, and you get four quadrants. Each quadrant has different resource needs, different timelines, different risks, and different team compositions. If you treat all tasks the same, you will misallocate resources and miss deadlines. If you understand which quadrant you are in, you can plan appropriately.

**Easy Build, Easy Eval** is the safe zone. These are tasks where you can prompt a model or call a fine-tuned classifier, and you have a clear ground truth dataset to measure accuracy. Examples: sentiment classification on product reviews where you have labeled data. Spam detection on user-generated content where you have historical flags. Extracting structured fields from invoices where you have a validation dataset. Translating UI strings between languages where you have human-verified reference translations. For these tasks, you can build a prototype in days, run evaluation in hours, and iterate quickly. This is where you start your roadmap if you have no AI infrastructure yet, because you can prove your pipeline works before you tackle harder problems.

**Easy Build, Hard Eval** is the modern default. These are tasks where the model produces fluent output, but verifying correctness requires expert judgment, subjective assessment, or real-world outcome measurement. Examples: generating customer support responses where quality depends on tone, accuracy, and helpfulness. Writing marketing copy where success is measured by conversion rates weeks later. Summarizing legal documents where errors can be subtle and costly. Generating code where correctness depends on context that is not in the prompt. In 2026, this quadrant contains most of the product value in AI systems, and it is where most teams underestimate effort. You can build the feature in two weeks, but standing up the evaluation infrastructure takes two months. You need human raters, rubrics, inter-rater reliability studies, and statistical frameworks to interpret results. You need to run A/B tests to measure downstream impact. If you allocate resources as if this is an Easy Build, Easy Eval task, you will ship a feature you cannot measure and cannot trust.

**Hard Build, Easy Eval** is the classic machine learning research problem. These are tasks where getting a system to work is technically challenging, but once it works you can measure success objectively. Examples: training a custom model to predict equipment failure from sensor data, where you have historical failure logs. Building a recommendation system that optimizes for click-through rate, where you can measure CTR directly. Developing a fraud detection model with severe class imbalance and adversarial drift, where you have labeled fraud cases. Fine-tuning a model for low-resource language translation, where you have reference translations and BLEU scores. These tasks require ML expertise, experimentation, hyperparameter tuning, and infrastructure work. But once you have a working system, evaluation is straightforward. You run your test set, you get a number, you know if you improved. This quadrant is where traditional ML teams are comfortable. They know how to allocate resources, estimate timelines, and manage risk.

**Hard Build, Hard Eval** is the danger zone. These are tasks where both the implementation and the measurement are research problems. Examples: building a multimodal model that generates design mockups from natural language, where evaluating design quality requires expert designers and user testing. Creating a medical diagnosis assistant that interprets radiology images and patient history, where evaluation requires board-certified radiologists and long-term patient outcome tracking. Developing a legal contract drafting tool, where correctness is determined by enforceability in court and depends on jurisdiction. Building a scientific research assistant that proposes novel experiments, where validation requires running the experiments and waiting for results. These are the highest-risk, highest-effort tasks in your roadmap. They require deep expertise on both the build and the evaluation side. They take months or years. They often fail. You should only attempt these if they are core to your product differentiation and you have the resources to sustain a long exploratory effort.

## The Deceptively Simple Anti-Pattern

The most common mistake is misclassifying an Easy Build, Hard Eval task as Easy Build, Easy Eval. The model generates output quickly, so the team assumes the task is easy. They allocate two weeks for the feature, ship it, and then realize they have no idea if it works. This is the deceptively simple anti-pattern, and it is everywhere in 2026.

You see this with customer support bots. The bot responds to user questions in seconds. Engineers look at the output and think it looks good. They ship it. But "looks good" is not the same as "is correct." To know if the bot is correct, you need to compare its answers to verified support documentation, or have expert support reps grade a sample of responses, or measure downstream metrics like customer satisfaction and repeat contact rate. All of that is evaluation work, and none of it is trivial. The build was easy. The evaluation is hard.

You see this with code generation tools. The model writes code that compiles. Developers test it on a few cases and it works. They merge it. But "compiles and runs" is not the same as "correct for all edge cases and maintainable and secure." To know if the generated code is production-ready, you need extensive testing, security review, and long-term observation of failure modes in deployment. The build was easy. The evaluation is hard.

You see this with content moderation. The model flags toxic content. The team reviews a handful of examples and the flags seem reasonable. They deploy it. But "seems reasonable" is not the same as "meets community standards and minimizes false positives and does not introduce demographic bias." To know if the moderation system works, you need to sample flagged and unflagged content, grade it with trained raters using detailed rubrics, measure precision and recall, audit for bias across demographic groups, and monitor appeals and user complaints. The build was easy. The evaluation is hard.

The anti-pattern is treating generation tasks like classification tasks. Classification tasks often live in Easy Build, Easy Eval: you have labels, you compute accuracy, you are done. Generation tasks often live in Easy Build, Hard Eval: you produce output, but verifying quality is a separate workstream. If you are building a generation feature and you do not have a plan for evaluation that is as detailed as your plan for build, you are walking into the trap.

## Complexity and Failure Tiers

The task complexity matrix interacts with your failure tiers from earlier in this chapter. A task's position in the matrix tells you how hard it is to build and measure. Its failure tier tells you what happens if it breaks. Combine these and you get a risk score that determines priority and resource allocation.

A **Tier 3 task in Hard Build, Hard Eval** is your highest-risk item. If it fails, your product is unusable or dangerous. If it succeeds, you do not have reliable measurement to confirm success. This is the scenario where you burn months of runway and still do not know if you are ready to ship. You should only take this on if it is your core product and you have the capital and expertise to sustain a long research effort. If you are an early-stage startup and your only feature is a Tier 3 Hard Build, Hard Eval task, you are in trouble. You should reframe to find an Easy Build, Easy Eval proxy task that de-risks your roadmap.

A **Tier 1 task in Easy Build, Hard Eval** is low-risk but still deserves evaluation rigor. Even though failure is non-critical, you should measure success properly. This is where you build your evaluation muscles. You are not under time pressure, so you can invest in rater training, rubric design, and inter-rater reliability studies. You are building process and tooling that you will reuse for higher-tier tasks later. Do not skip evaluation just because the feature is low-stakes. Use low-stakes features to practice evaluation discipline.

A **Tier 2 task in Hard Build, Easy Eval** is a classic engineering problem. The build is hard, but you can measure success objectively, so you can iterate with confidence. This is a good fit for your senior engineers. Give them the time and resources to solve the technical problem, and trust that the evaluation will be straightforward once they succeed. The risk here is underestimating build time, not underestimating evaluation time.

A **Tier 3 task in Easy Build, Easy Eval** is your safest high-stakes feature. You can build it quickly and measure it reliably. This is where you want to start if you are under pressure to ship something critical. The risk is low because both build and evaluation are tractable. If you have a choice between two Tier 3 features and one is Easy Build, Easy Eval and the other is Easy Build, Hard Eval, ship the first one first. Prove your pipeline works on the easy case before you tackle the hard one.

## Sequencing Your Roadmap with the Matrix

The complexity matrix gives you a sequencing strategy for your roadmap. You do not tackle tasks in random order. You sequence them to derisk your execution and build capability progressively.

Start with **Easy Build, Easy Eval** tasks, even if they are not your core product. These tasks prove your infrastructure works. They validate that you can ingest data, call models, log outputs, run evaluation, and interpret results. They let you build tooling and process without the pressure of a hard deadline or a high-stakes launch. If you are a new team with no AI deployment experience, your first milestone should be shipping an Easy Build, Easy Eval feature to production, even if it is a minor feature. This is your baseline competence check.

Next, move to **Easy Build, Hard Eval** tasks in Tier 1 or Tier 2. These tasks teach you how to do evaluation properly. You build rater workflows, rubrics, statistical methods, and observability dashboards. You learn how long evaluation takes, how much it costs, and where the bottlenecks are. You make mistakes in a low-stakes environment and fix your process before you attempt a Tier 3 feature. By the time you ship your first Tier 3 Easy Build, Hard Eval feature, you should have a mature evaluation pipeline and a team that knows how to use it.

Then, tackle **Hard Build, Easy Eval** tasks if they are on your roadmap. These are your technical deep dives. You solve hard engineering problems, but you have the safety net of objective measurement. You can iterate quickly because you get clear feedback from your test set. You build ML expertise and infrastructure depth. If your product requires custom models or complex data pipelines, this is where you invest.

Finally, attempt **Hard Build, Hard Eval** tasks only if they are core to your differentiation and you have proven capability in the other quadrants. These are multi-quarter efforts. They require both engineering excellence and evaluation rigor. They are high-risk and high-reward. If you succeed, you have built something defensible that competitors cannot easily replicate. If you fail, you have burned significant runway. Do not attempt these until you have shipped multiple features in the other quadrants and built a team that can handle both hard builds and hard evaluation.

## The 2026 Landscape Shift

In 2026, the complexity landscape has shifted dramatically compared to 2020 or even 2023. Frontier models like GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro have moved a huge number of tasks from Hard Build to Easy Build. Fine-tuning is often unnecessary. Custom model training is rarely required. Prompt engineering and API integration can solve problems that once required months of ML research. This is good news: you can prototype faster, iterate more quickly, and ship features with smaller teams.

But evaluation difficulty has not decreased. In some ways, it has increased. Frontier models produce fluent, confident output that looks correct even when it is wrong. They handle complex prompts and generate long-form content that requires expert review. They generalize across domains, which means you cannot rely on narrow test sets. They exhibit emergent behaviors that are hard to predict and hard to measure. The result is that most tasks have shifted from Hard Build, Hard Eval to Easy Build, Hard Eval. The bottleneck is no longer getting the model to work. The bottleneck is knowing whether it works.

This shift changes how you allocate resources. In the past, you might have had three engineers working on model training and one person working on evaluation. In 2026, you might have one engineer integrating the API and three people working on evaluation infrastructure, rater workflows, and observability. If your team composition still reflects the old world, you are misallocated. If your roadmap still assumes that evaluation is a small add-on after the build is done, you are planning for failure.

The teams that win in 2026 are the teams that treat evaluation as a first-class engineering problem. They allocate budget to rater tools, rubric design, statistical analysis, and continuous monitoring. They hire people who know how to design experiments, interpret ambiguous results, and build consensus around subjective quality judgments. They sequence their roadmap to build evaluation capability early, and they refuse to ship features they cannot measure. The task complexity matrix is the tool that makes this resource allocation legible. It forces you to separate build difficulty from evaluation difficulty, and it gives you a framework to plan for both.

With the task complexity matrix in hand and your roadmap sequenced by risk, you are ready for the final step in the framing process: the sign-off checklist that ensures your framing is complete and your team is aligned before you commit to execution.
