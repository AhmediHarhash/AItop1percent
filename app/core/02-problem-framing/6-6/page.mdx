# 6.6 â€” The Task Complexity Matrix

In March 2023, a Series B fintech startup burned through eighteen engineer-months building a customer support chatbot that could answer tax questions. The team hit their milestone: the bot worked. It answered questions. It passed internal QA. They launched to three thousand customers in early access and within six days pulled the feature entirely. The problem was not that the model failed to generate responses. The problem was that no one on the team could reliably determine when those responses were correct. Their tax compliance officer would flag an answer as dangerous, their senior support rep would defend it, and their contract CPA would shrug and say it depended on the customer's state. The build took three months. The realization that they had no evaluation infrastructure took six days. The decision to kill the feature took one tense board meeting.

The root cause was a category error. They treated the project as a standard software build: scope it, build it, test it, ship it. They allocated resources as if complexity lived in the implementation. But this was not a normal software project. The hard part was never getting the model to produce text. The hard part was knowing whether that text was correct, and they discovered this only after the system was live and their compliance risk was sitting in three thousand inboxes. They had conflated build difficulty with evaluation difficulty, and that mistake cost them a quarter of roadmap progress.

## The Two Dimensions of Complexity

When you assess task complexity, you need two numbers, not one. The first is **build difficulty**: how hard is it to get a system that produces output. The second is **evaluation difficulty**: how hard is it to measure whether that output is correct. These are not correlated. Some tasks are trivial to build and trivial to evaluate. Others are trivial to build and nearly impossible to evaluate. A few are brutal on both axes. Your resource allocation, timeline, team composition, and risk posture all depend on where your task sits in this two-dimensional space.

Build difficulty is the familiar kind of complexity. It is the technical challenge of assembling data, training or prompting a model, integrating APIs, handling edge cases, optimizing latency, scaling throughput. If you have built software, you know how to estimate this. You break the problem into components, you count the unknowns, you assess your team's skill gaps, you multiply by your historical velocity and add a buffer. Build difficulty is legible. It shows up in sprint planning. It is what engineers argue about in design reviews.

Evaluation difficulty is less familiar and more treacherous. It is the challenge of defining correctness, generating test cases, running measurements, interpreting results, and building confidence that your system works. For some tasks this is easy: you have a dataset with labels, you compute accuracy, you are done. For other tasks this is a research problem in its own right. You need expert human raters. You need multi-stage rubrics. You need adversarial testing. You need statistical methods to reconcile disagreements. You need to run live experiments to see if users actually benefit. Evaluation difficulty is often invisible during planning, and it does not show up in your sprint backlog until you are trying to ship.

The key insight is that these two dimensions are independent. A task can be easy to build and hard to evaluate. A task can be hard to build and easy to evaluate. The difficulty of writing code does not predict the difficulty of measuring success. In 2026, with frontier models like GPT-5 and Claude Opus 4.5, most generation tasks are easy to build: you write a prompt, you call an API, you get output. But evaluation has not gotten easier. If anything, it has gotten harder, because the output is more fluent and more persuasive and more likely to fool a casual reader. The result is that the modern AI product landscape is dominated by tasks in the Easy Build, Hard Eval quadrant, and most teams are not prepared for this.

## The Four Quadrants

Plot build difficulty on one axis and evaluation difficulty on the other, and you get four quadrants. Each quadrant has different resource needs, different timelines, different risks, and different team compositions. If you treat all tasks the same, you will misallocate resources and miss deadlines. If you understand which quadrant you are in, you can plan appropriately.

**Easy Build, Easy Eval** is the safe zone. These are tasks where you can prompt a model or call a fine-tuned classifier, and you have a clear ground truth dataset to measure accuracy. Examples: sentiment classification on product reviews where you have labeled data. Spam detection on user-generated content where you have historical flags. Extracting structured fields from invoices where you have a validation dataset. Translating UI strings between languages where you have human-verified reference translations. For these tasks, you can build a prototype in days, run evaluation in hours, and iterate quickly. This is where you start your roadmap if you have no AI infrastructure yet, because you can prove your pipeline works before you tackle harder problems.

**Easy Build, Hard Eval** is the modern default. These are tasks where the model produces fluent output, but verifying correctness requires expert judgment, subjective assessment, or real-world outcome measurement. Examples: generating customer support responses where quality depends on tone, accuracy, and helpfulness. Writing marketing copy where success is measured by conversion rates weeks later. Summarizing legal documents where errors can be subtle and costly. Generating code where correctness depends on context that is not in the prompt. In 2026, this quadrant contains most of the product value in AI systems, and it is where most teams underestimate effort. You can build the feature in two weeks, but standing up the evaluation infrastructure takes two months. You need human raters, rubrics, inter-rater reliability studies, and statistical frameworks to interpret results. You need to run A/B tests to measure downstream impact. If you allocate resources as if this is an Easy Build, Easy Eval task, you will ship a feature you cannot measure and cannot trust.

**Hard Build, Easy Eval** is the classic machine learning research problem. These are tasks where getting a system to work is technically challenging, but once it works you can measure success objectively. Examples: training a custom model to predict equipment failure from sensor data, where you have historical failure logs. Building a recommendation system that optimizes for click-through rate, where you can measure CTR directly. Developing a fraud detection model with severe class imbalance and adversarial drift, where you have labeled fraud cases. Fine-tuning a model for low-resource language translation, where you have reference translations and BLEU scores. These tasks require ML expertise, experimentation, hyperparameter tuning, and infrastructure work. But once you have a working system, evaluation is straightforward. You run your test set, you get a number, you know if you improved. This quadrant is where traditional ML teams are comfortable. They know how to allocate resources, estimate timelines, and manage risk.

**Hard Build, Hard Eval** is the danger zone. These are tasks where both the implementation and the measurement are research problems. Examples: building a multimodal model that generates design mockups from natural language, where evaluating design quality requires expert designers and user testing. Creating a medical diagnosis assistant that interprets radiology images and patient history, where evaluation requires board-certified radiologists and long-term patient outcome tracking. Developing a legal contract drafting tool, where correctness is determined by enforceability in court and depends on jurisdiction. Building a scientific research assistant that proposes novel experiments, where validation requires running the experiments and waiting for results. These are the highest-risk, highest-effort tasks in your roadmap. They require deep expertise on both the build and the evaluation side. They take months or years. They often fail. You should only attempt these if they are core to your product differentiation and you have the resources to sustain a long exploratory effort.

## The Deceptively Simple Anti-Pattern

The most common mistake is misclassifying an Easy Build, Hard Eval task as Easy Build, Easy Eval. The model generates output quickly, so the team assumes the task is easy. They allocate two weeks for the feature, ship it, and then realize they have no idea if it works. This is the deceptively simple anti-pattern, and it is everywhere in 2026.

You see this with customer support bots. The bot responds to user questions in seconds. Engineers look at the output and think it looks good. They ship it. But "looks good" is not the same as "is correct." To know if the bot is correct, you need to compare its answers to verified support documentation, or have expert support reps grade a sample of responses, or measure downstream metrics like customer satisfaction and repeat contact rate. All of that is evaluation work, and none of it is trivial. The build was easy. The evaluation is hard.

You see this with code generation tools. The model writes code that compiles. Developers test it on a few cases and it works. They merge it. But "compiles and runs" is not the same as "correct for all edge cases and maintainable and secure." To know if the generated code is production-ready, you need extensive testing, security review, and long-term observation of failure modes in deployment. The build was easy. The evaluation is hard.

You see this with content moderation. The model flags toxic content. The team reviews a handful of examples and the flags seem reasonable. They deploy it. But "seems reasonable" is not the same as "meets community standards and minimizes false positives and does not introduce demographic bias." To know if the moderation system works, you need to sample flagged and unflagged content, grade it with trained raters using detailed rubrics, measure precision and recall, audit for bias across demographic groups, and monitor appeals and user complaints. The build was easy. The evaluation is hard.

The anti-pattern is treating generation tasks like classification tasks. Classification tasks often live in Easy Build, Easy Eval: you have labels, you compute accuracy, you are done. Generation tasks often live in Easy Build, Hard Eval: you produce output, but verifying quality is a separate workstream. If you are building a generation feature and you do not have a plan for evaluation that is as detailed as your plan for build, you are walking into the trap.

## Complexity and Failure Tiers

The task complexity matrix interacts with your failure tiers from earlier in this chapter. A task's position in the matrix tells you how hard it is to build and measure. Its failure tier tells you what happens if it breaks. Combine these and you get a risk score that determines priority and resource allocation.

A **Tier 3 task in Hard Build, Hard Eval** is your highest-risk item. If it fails, your product is unusable or dangerous. If it succeeds, you do not have reliable measurement to confirm success. This is the scenario where you burn months of runway and still do not know if you are ready to ship. You should only take this on if it is your core product and you have the capital and expertise to sustain a long research effort. If you are an early-stage startup and your only feature is a Tier 3 Hard Build, Hard Eval task, you are in trouble. You should reframe to find an Easy Build, Easy Eval proxy task that de-risks your roadmap.

A **Tier 1 task in Easy Build, Hard Eval** is low-risk but still deserves evaluation rigor. Even though failure is non-critical, you should measure success properly. This is where you build your evaluation muscles. You are not under time pressure, so you can invest in rater training, rubric design, and inter-rater reliability studies. You are building process and tooling that you will reuse for higher-tier tasks later. Do not skip evaluation just because the feature is low-stakes. Use low-stakes features to practice evaluation discipline.

A **Tier 2 task in Hard Build, Easy Eval** is a classic engineering problem. The build is hard, but you can measure success objectively, so you can iterate with confidence. This is a good fit for your senior engineers. Give them the time and resources to solve the technical problem, and trust that the evaluation will be straightforward once they succeed. The risk here is underestimating build time, not underestimating evaluation time.

A **Tier 3 task in Easy Build, Easy Eval** is your safest high-stakes feature. You can build it quickly and measure it reliably. This is where you want to start if you are under pressure to ship something critical. The risk is low because both build and evaluation are tractable. If you have a choice between two Tier 3 features and one is Easy Build, Easy Eval and the other is Easy Build, Hard Eval, ship the first one first. Prove your pipeline works on the easy case before you tackle the hard one.

## Sequencing Your Roadmap with the Matrix

The complexity matrix gives you a sequencing strategy for your roadmap. You do not tackle tasks in random order. You sequence them to derisk your execution and build capability progressively.

Start with **Easy Build, Easy Eval** tasks, even if they are not your core product. These tasks prove your infrastructure works. They validate that you can ingest data, call models, log outputs, run evaluation, and interpret results. They let you build tooling and process without the pressure of a hard deadline or a high-stakes launch. If you are a new team with no AI deployment experience, your first milestone should be shipping an Easy Build, Easy Eval feature to production, even if it is a minor feature. This is your baseline competence check.

Next, move to **Easy Build, Hard Eval** tasks in Tier 1 or Tier 2. These tasks teach you how to do evaluation properly. You build rater workflows, rubrics, statistical methods, and observability dashboards. You learn how long evaluation takes, how much it costs, and where the bottlenecks are. You make mistakes in a low-stakes environment and fix your process before you attempt a Tier 3 feature. By the time you ship your first Tier 3 Easy Build, Hard Eval feature, you should have a mature evaluation pipeline and a team that knows how to use it.

Then, tackle **Hard Build, Easy Eval** tasks if they are on your roadmap. These are your technical deep dives. You solve hard engineering problems, but you have the safety net of objective measurement. You can iterate quickly because you get clear feedback from your test set. You build ML expertise and infrastructure depth. If your product requires custom models or complex data pipelines, this is where you invest.

Finally, attempt **Hard Build, Hard Eval** tasks only if they are core to your differentiation and you have proven capability in the other quadrants. These are multi-quarter efforts. They require both engineering excellence and evaluation rigor. They are high-risk and high-reward. If you succeed, you have built something defensible that competitors cannot easily replicate. If you fail, you have burned significant runway. Do not attempt these until you have shipped multiple features in the other quadrants and built a team that can handle both hard builds and hard evaluation.

## The 2026 Landscape Shift

In 2026, the complexity landscape has shifted dramatically compared to 2020 or even 2023. Frontier models like GPT-5, Claude Opus 4.5, and Gemini 1.5 Pro have moved a huge number of tasks from Hard Build to Easy Build. Fine-tuning is often unnecessary. Custom model training is rarely required. Prompt engineering and API integration can solve problems that once required months of ML research. This is good news: you can prototype faster, iterate more quickly, and ship features with smaller teams.

But evaluation difficulty has not decreased. In some ways, it has increased. Frontier models produce fluent, confident output that looks correct even when it is wrong. They handle complex prompts and generate long-form content that requires expert review. They generalize across domains, which means you cannot rely on narrow test sets. They exhibit emergent behaviors that are hard to predict and hard to measure. The result is that most tasks have shifted from Hard Build, Hard Eval to Easy Build, Hard Eval. The bottleneck is no longer getting the model to work. The bottleneck is knowing whether it works.

This shift changes how you allocate resources. In the past, you might have had three engineers working on model training and one person working on evaluation. In 2026, you might have one engineer integrating the API and three people working on evaluation infrastructure, rater workflows, and observability. If your team composition still reflects the old world, you are misallocated. If your roadmap still assumes that evaluation is a small add-on after the build is done, you are planning for failure.

The teams that win in 2026 are the teams that treat evaluation as a first-class engineering problem. They allocate budget to rater tools, rubric design, statistical analysis, and continuous monitoring. They hire people who know how to design experiments, interpret ambiguous results, and build consensus around subjective quality judgments. They sequence their roadmap to build evaluation capability early, and they refuse to ship features they cannot measure. The task complexity matrix is the tool that makes this resource allocation legible. It forces you to separate build difficulty from evaluation difficulty, and it gives you a framework to plan for both.

## Complexity Inflation Over Time

Tasks do not maintain constant complexity. What starts as an Easy Build, Easy Eval task can evolve into Easy Build, Hard Eval as your product matures and your requirements become more sophisticated. This is **complexity inflation**, and failing to account for it is one of the most common causes of roadmap derailment.

Complexity inflation happens for several reasons. First, your initial version of a feature uses simple heuristics or narrow criteria that are easy to evaluate. Over time, users demand more nuance, more edge case handling, and more contextual adaptation. The evaluation criteria become subjective and multidimensional. What was once "does this email get classified correctly" becomes "does this email get classified correctly, with appropriate confidence, using context from previous emails, while respecting user preferences and privacy policies." The build might still be straightforward, but evaluation now requires expert judgment across multiple dimensions.

Second, your input distribution changes. Early adopters are forgiving and use the system in predictable ways. As you scale, you encounter adversarial users, edge cases, and input patterns you never anticipated. Your evaluation dataset, which was representative of early users, is no longer representative of the full user base. You need to continuously sample new data, label it, and update your evaluation benchmarks. This ongoing evaluation work is a hidden complexity cost that most teams underestimate.

Third, regulatory and compliance requirements increase. A feature that initially had no regulatory implications might attract scrutiny as your user base grows or as new regulations come into force. The EU AI Act, fully enforced in 2026, has moved many previously unregulated tasks into regulated categories. What was once a simple product feature is now a high-risk AI system that requires conformity assessment, documentation, and ongoing monitoring. Evaluation complexity explodes because you now need to demonstrate compliance, not just functional correctness.

Your task complexity assessment should be time-bounded. When you classify a task as Easy Build, Easy Eval, you should also estimate how long that classification will hold. For some tasks, the classification is stable for years. For others, it degrades in months. Your roadmap should include periodic complexity reassessments where you revisit your tasks and update their classifications based on how requirements, inputs, and regulations have evolved. Teams that treat complexity as static get blindsided when their "easy" features suddenly require expert evaluation infrastructure they do not have.

## Using Complexity to Inform Model Selection

The task complexity matrix is not just a planning tool. It is also a model selection tool. Different tasks require different models, and matching model capabilities to task complexity is how you optimize for cost, latency, and reliability.

For **Easy Build, Easy Eval tasks**, you can often use smaller, faster, cheaper models. If your task is sentiment classification on product reviews and you have a labeled test set, you do not need GPT-5. You might not even need a fine-tuned model. A well-prompted smaller model like GPT-4 Mini or Claude 3 Haiku can deliver 95% accuracy at a fraction of the cost and latency. Your evaluation infrastructure can quickly tell you if the smaller model is sufficient, and you save resources for harder problems.

For **Easy Build, Hard Eval tasks**, you should start with frontier models. When evaluation is the bottleneck, you want the highest-quality outputs you can get, because every percentage point of improvement reduces the human review burden. If your task is generating customer support responses and you need expert raters to evaluate quality, the cost of the raters far exceeds the cost of the model. Using GPT-5 or Claude Opus 4.5 might cost twice as much per inference as using a smaller model, but if it reduces your review workload by 20%, the total cost is lower.

For **Hard Build, Easy Eval tasks**, model selection depends on whether the build difficulty comes from data scarcity or algorithmic complexity. If you have limited training data, you might need a larger model that can learn from fewer examples, or you might need a smaller model that you fine-tune carefully to avoid overfitting. If the difficulty is algorithmic, such as optimizing a ranking function, model size might matter less than architecture and hyperparameters. The key is that evaluation is cheap, so you can experiment aggressively and use your test set to guide model selection.

For **Hard Build, Hard Eval tasks**, you need frontier models and you need to invest in both build and evaluation. There are no shortcuts. You use the best models available, you build the evaluation infrastructure properly, and you iterate carefully. These are your highest-risk, highest-investment features, and cutting corners on model quality or evaluation rigor is how you turn a hard problem into a failed project.

The complexity matrix also informs your **model version upgrade strategy**. When a new model version is released, you do not upgrade blindly. You assess the task complexity for each feature. Easy Eval features get upgraded quickly because you can test the new model and confirm it is better within hours. Hard Eval features get upgraded cautiously because you need to run human evaluations, analyze subtle quality changes, and potentially retrain your raters on new error patterns. The matrix tells you which upgrades are low-risk and which require careful validation.

## Complexity and Team Composition

The task complexity matrix determines what skills your team needs. Different quadrants require different expertise, and hiring for the wrong quadrant is a common mistake in early-stage AI teams.

For **Easy Build, Easy Eval**, you need engineers who can integrate APIs, manage data pipelines, and run automated evaluations. You do not need ML researchers or human evaluation specialists. This is the lowest-skill quadrant, and it is where many teams start their AI journey. You can build these features with generalist software engineers who learn prompt engineering and basic evaluation methods.

For **Easy Build, Hard Eval**, you need evaluation specialists. These are people who know how to design rubrics, recruit and train raters, analyze inter-rater agreement, interpret subjective data, and build consensus around quality judgments. You might also need domain experts who can serve as gold-standard raters or who can review and validate outputs in specialized areas. This is the quadrant where most teams are under-invested in 2026. They hire engineers to build the feature but they do not hire evaluation experts to measure it, and they end up with systems they cannot trust.

For **Hard Build, Easy Eval**, you need ML engineers and researchers who can train custom models, tune hyperparameters, manage experiments, and optimize performance. You also need infrastructure engineers who can build scalable training and inference pipelines. This is the traditional ML team composition, and it is what most AI teams looked like before the frontier model era. In 2026, fewer tasks require this level of build expertise, but when they do, you need specialists.

For **Hard Build, Hard Eval**, you need both ML specialists and evaluation specialists. This is the most expensive team composition, and it is only justified for core features that are central to your product differentiation. You should limit the number of these features in your roadmap because they consume resources at both ends. If your entire product is in this quadrant, you either have a very well-funded research project or you have misframed your problems and need to find simpler proxies.

Your hiring plan should be derived from your roadmap complexity profile. If 70% of your roadmap is Easy Build, Hard Eval, you should hire more evaluation specialists than ML engineers. If 70% of your roadmap is Hard Build, Easy Eval, you should hire more ML engineers than evaluation specialists. Most teams hire based on generic job descriptions without mapping headcount to roadmap complexity, and they end up with skill mismatches that slow execution.

## Communicating Complexity to Stakeholders

The task complexity matrix is a communication tool for aligning with non-technical stakeholders. Product managers, executives, and investors often underestimate the difficulty of AI projects because they see the demo and assume the hard part is done. The matrix makes the hidden work visible.

When you present a roadmap to stakeholders, you should include a complexity profile for each feature. Show which quadrant each feature falls into, explain what that means for timeline and resources, and highlight where the risks are. A feature in Easy Build, Hard Eval looks simple but has hidden evaluation costs. A feature in Hard Build, Hard Eval looks ambitious and is actually more ambitious than it looks. This framing sets realistic expectations and prevents the "why is this taking so long" conversation three months into a six-month build.

You should also use the matrix to explain trade-offs. If a stakeholder asks why you are not building a particular feature, you can show them that it falls into Hard Build, Hard Eval and would consume resources that are currently allocated to three Easy Build, Hard Eval features that deliver more near-term value. The matrix gives you a rational basis for prioritization that goes beyond intuition and politics.

For investors and board members, the complexity matrix helps explain why your burn rate is what it is and why headcount is allocated the way it is. If you are hiring evaluation specialists, you can show that 60% of your roadmap is in Easy Build, Hard Eval and that evaluation is your bottleneck. If you are investing in infrastructure for automated testing, you can show that you are moving tasks from Hard Eval to Easy Eval by building reusable evaluation tooling. The matrix makes your resource allocation legible and defensible.

## Complexity Documentation in the Problem Framing Artifact

The task complexity assessment should be a formal section in your problem framing document. For each task, you should record both the build difficulty and the evaluation difficulty, the reasoning behind the classification, the expected timeline for each phase, and the skill requirements. This documentation serves several purposes.

First, it forces you to think explicitly about complexity before you commit resources. The act of classifying the task and justifying the classification surfaces assumptions and risks that might otherwise remain implicit. If you cannot explain why a task is Easy Eval, you probably have not thought through evaluation rigor, and you should do that before you start building.

Second, it creates a historical record that you can revisit. When you reassess complexity six months later, you can compare the actual difficulty to the predicted difficulty and understand where your estimates were wrong. This feedback loop improves your complexity assessment skills over time. Teams that do not document their initial assessments cannot learn from their estimation errors.

Third, it aligns your team. When engineering, product, and evaluation all see the same complexity classification, they can coordinate their work appropriately. Engineering knows whether to build for rapid iteration or for long-term stability. Product knows whether to promise a feature in the next sprint or the next quarter. Evaluation knows whether to plan for automated tests or human rater workflows. Without shared understanding of complexity, these teams optimize for different things and work at cross purposes.

The complexity documentation should be updated whenever requirements change, whenever the input distribution shifts, or whenever regulations affect the task. This living document is part of your operational discipline, not a one-time planning artifact. Teams that treat complexity as a static assessment made during initial scoping are surprised when their Easy Build, Easy Eval task becomes Easy Build, Hard Eval and their timeline doubles.

## The Complexity Reality Check

Before you finalize your roadmap, you should run a **complexity reality check**. This is a structured review where you examine your entire set of planned tasks, classify them in the matrix, and ask whether your resource allocation matches your complexity profile.

Calculate the percentage of tasks in each quadrant. If 80% of your tasks are Easy Build, Hard Eval but you have zero evaluation specialists on your team, your roadmap is not feasible. If 50% of your tasks are Hard Build but you have no ML engineers, you are planning to fail. The complexity profile should match your team's capabilities, or you need to adjust one or the other.

Calculate the total evaluation workload. For each Hard Eval task, estimate how many hours of human evaluation you will need per month. Sum across all tasks. If the total is four hundred hours per month and you have one part-time rater, you have a ten-to-one mismatch. Either you need to hire more raters, or you need to cut Hard Eval tasks, or you need to invest in tooling that makes evaluation faster.

Calculate the total build workload and compare it to your engineering capacity. For each Hard Build task, estimate the engineer-months required. Sum across all tasks and compare to your available engineering capacity over your planning horizon. If you are over-allocated, something has to give. The complexity matrix helps you make rational cuts based on difficulty, not just based on what product managers are loudest about.

Review each Hard Build, Hard Eval task individually and ask whether it is truly necessary. These tasks are resource black holes. They should only exist if they are core to your product differentiation and you have exhausted all options for reframing them into easier quadrants. Sometimes a Hard Build, Hard Eval task can be broken into stages where the first stage is Easy Build, Hard Eval and delivers partial value while you work on the full solution. This staged approach de-risks the roadmap and gives you checkpoints where you can decide whether to continue investing.

The reality check should result in one of three outcomes. First, your roadmap is feasible given your current team and resources, and you proceed with execution. Second, your roadmap is not feasible, and you need to cut features, extend timelines, or hire additional capacity. Third, you discover that you have systematically underestimated complexity across multiple tasks, which suggests you need to improve your estimation process before you can plan reliably. All three outcomes are valuable. The first gives you confidence to execute. The second prevents you from committing to an impossible plan. The third reveals a capability gap that you must address before you can scale.

With the task complexity matrix in hand and your roadmap sequenced by risk, you are ready for the final step in the framing process: the handoff artifacts that package your framing work for evaluation and engineering teams.
