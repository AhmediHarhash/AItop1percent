# 4.5 — Negative Success Criteria: What the System Must Never Do

The medical information assistant was performing beautifully. It answered patient questions about symptoms, explained treatment options clearly, and helped users understand their conditions. Patient satisfaction scores were strong. The system met all its functional criteria for accuracy and all its behavioral criteria for empathy and clarity. Then a patient asked about a rare condition, and the system confidently provided detailed treatment recommendations that were completely fabricated. No such treatments existed. The patient followed the advice. The consequences were severe.

The investigation revealed that the system had been optimized entirely for positive success criteria: accuracy on common questions, helpful tone, comprehensive responses. Nobody had defined negative success criteria: the things the system must never do under any circumstances. The team had specified what success looked like but not what failure looked like. They had defined desired behaviors but not forbidden ones. The system was helpful 99% of the time, but that final 1% involved failure modes that made the entire product unacceptable.

This is the critical gap that negative success criteria address. They define failure modes as explicit criteria, establish hard boundaries that cannot be crossed, and specify what is not acceptable regardless of how well the system performs otherwise. While positive criteria tell you what good looks like, negative criteria tell you what bad looks like and make it measurable and testable.

## Why Negative Criteria Are Essential

Positive success criteria naturally focus teams on optimization. You define accuracy targets, latency goals, satisfaction scores. Teams work to maximize these metrics. The system gets better and better at the defined positive criteria.

But optimization without boundaries is dangerous. A system optimized purely for helpfulness might provide confident answers even when uncertain. A system optimized purely for user satisfaction might tell users what they want to hear rather than what is true. A system optimized purely for task completion might take shortcuts that violate safety or privacy constraints.

Negative criteria establish the guardrails within which optimization happens. They say: optimize for these positive criteria, but never cross these lines. They prevent teams from achieving good average performance through approaches that occasionally produce catastrophic failures.

The relationship between positive and negative criteria is asymmetric. A system that meets most positive criteria but violates negative criteria has failed. A system that is helpful, accurate, fast, and well-designed 99% of the time but occasionally generates dangerous medical advice has failed. A system that answers questions correctly 95% of the time but leaks user data has failed. A system that provides excellent customer service but occasionally makes unauthorized purchases has failed.

This asymmetry reflects the reality that certain failure modes are unacceptable regardless of overall performance. One racist response can destroy brand reputation even if 10,000 previous responses were perfect. One hallucinated citation in a legal brief can lead to sanctions even if the rest of the brief is excellent. One privacy violation can trigger regulatory action even if the system otherwise performs beautifully.

Negative criteria make these boundaries explicit, testable, and non-negotiable.

## Hallucination Boundaries

Hallucination—the generation of plausible-sounding but factually incorrect information—is one of the most critical categories of negative criteria for many AI systems. While positive criteria might specify accuracy targets like "90% factually correct," negative criteria specify absolute boundaries around specific types of hallucination.

**Never Fabricate Citations**

For any system that provides citations, references, or links to sources, a core negative criterion is: the system must never generate citations that do not exist. Every citation must be verifiable. Every reference must be real. Every link must point to actual content.

This criterion is absolute, not statistical. It is not acceptable for 95% of citations to be real. If the system generates fake citations 5% of the time, users cannot trust any citation, and the entire feature is compromised.

Testing this criterion requires checking every citation generated by the system in evaluation. For a document Q&A system, the criterion might specify: when evaluated on 1,000 test queries, zero generated citations may reference documents that do not exist in the knowledge base; zero generated quotes may attribute text to sources that do not contain that text; zero generated page numbers may reference pages that do not exist or do not contain the claimed information.

**Never Invent Facts Not in Source Material**

For systems grounded in specific sources—retrieval-augmented generation, document Q&A, knowledge base assistants—a core negative criterion is: the system must never present as fact information that is not present in the provided sources.

This does not mean the system must refuse to answer questions beyond the sources. It means it must not present invented information as if it came from the sources. Acceptable responses to questions not covered by sources include: "I do not have information about that in the available documents," or "The documents do not address that question," or "I cannot find relevant information in the knowledge base."

Unacceptable responses include: presenting fabricated facts confidently, generating plausible-sounding details not in the sources, or conflating information from sources with generated speculation without distinguishing them.

Testing requires comparing system outputs to source material. For a customer support knowledge base system, the criterion might specify: when evaluated on 500 test questions, zero responses may contain factual claims that cannot be traced to specific passages in the knowledge base; all responses must either cite sources for claims or explicitly acknowledge when information is not available.

**Never Present Uncertain Information as Definitive**

For systems that operate with uncertainty—which is all AI systems—a negative criterion is: the system must never present uncertain, speculative, or low-confidence information as if it were certain and authoritative.

This criterion prevents the most dangerous form of hallucination: confident wrongness. A system that says "I am not sure, but it might be X" when wrong is far less harmful than one that says "It is definitely X" when wrong.

Testing requires examining how the system handles cases where it lacks sufficient information or confidence. For a diagnostic assistant, the criterion might specify: when evaluated on ambiguous cases in the test set where no definitive diagnosis is possible given the information, zero responses may present a single diagnosis as certain; all responses must acknowledge uncertainty and present multiple possibilities or request additional information.

## Forbidden Outputs

Beyond hallucination, negative criteria define categories of content the system must never produce, regardless of the query or context.

**No Harmful Content**

The system must not generate content that could cause physical, emotional, or financial harm. This includes:

No dangerous instructions: The system must not provide instructions for illegal activities, self-harm, harming others, or dangerous actions that could cause injury.

No promotion of harmful behaviors: The system must not encourage or normalize self-harm, eating disorders, substance abuse, or other harmful behaviors.

No extreme or disturbing content: The system must not generate graphic violence, extreme disturbing scenarios, or content designed to shock or distress.

Testing requires adversarial evaluation where the system is deliberately prompted to generate harmful content. For a general-purpose assistant, the criterion might specify: when evaluated on 200 adversarial prompts designed to elicit harmful content, the system must refuse or deflect 100% of requests for dangerous instructions, must refuse 100% of requests that encourage self-harm, and must refuse 100% of requests for disturbing content beyond standard content policy boundaries.

**No Personally Identifiable Information in Responses**

For systems that process user data or have access to personal information, a critical negative criterion is: the system must never include PII in responses unless explicitly authorized for that specific piece of information in that specific context.

This prevents privacy violations where the system might inadvertently expose one user's data to another user, leak training data containing personal information, or reveal sensitive information that should not be disclosed.

Testing requires verifying that responses do not contain names, addresses, phone numbers, email addresses, identification numbers, or other PII except when explicitly intended. For a customer service system, the criterion might specify: when evaluated on 1,000 test conversations, zero responses may include PII from other users' accounts; zero responses may include full credit card numbers, social security numbers, or other sensitive identifiers; the system may only reference the current user's information when explicitly needed to answer their question.

**No Unauthorized Actions**

For systems that can take actions—make purchases, send emails, modify data, execute commands—a negative criterion is: the system must never take actions that exceed its authorized scope or that the user has not explicitly consented to.

This prevents agentic AI from becoming dangerous through overreach. A shopping assistant that adds items to a cart is acceptable. One that completes purchases without explicit user confirmation is not. A calendar assistant that suggests meeting times is acceptable. One that accepts meetings on your behalf without asking is not.

Testing requires verifying boundaries around action-taking. For an email assistant, the criterion might specify: the system may draft emails but must never send emails without explicit user approval; the system may suggest calendar events but must never accept or decline invitations without explicit user confirmation; zero actions in the test set may be taken without proper authorization checks.

**No Disclosure of System Prompts or Internal Instructions**

For systems that use prompts or system instructions to control behavior, a negative criterion is: the system must never disclose its system prompt, internal instructions, or implementation details when prompted by users.

This prevents prompt injection attacks and protects proprietary approaches. Users should not be able to extract system prompts through clever prompting, social engineering, or adversarial techniques.

Testing requires adversarial evaluation with techniques designed to extract system prompts. For any production AI system, the criterion might specify: when evaluated on 100 prompt injection attempts using known extraction techniques, zero attempts may successfully cause the system to reveal its system prompt or internal instructions.

## Out-of-Scope Handling

Many AI systems have defined scope boundaries: topics they should address and topics they should not. Negative criteria specify how the system must behave when queries fall outside scope.

**Must Refuse Gracefully When Asked About Topics Outside Scope**

Rather than attempting to answer queries outside scope and potentially providing low-quality or harmful responses, the system should refuse politely and, when possible, redirect.

For a financial advisory assistant restricted to investment advice, the criterion might specify: when asked about tax law, legal matters, or estate planning, the system must refuse to provide advice and suggest consulting appropriate specialists; zero out-of-scope queries may result in the system attempting to provide advice in areas outside its expertise.

For a children's educational assistant, the criterion might specify: when asked about adult topics, the system must decline to engage and redirect to age-appropriate content; zero responses may provide inappropriate content for the target age range.

Testing requires evaluation on deliberately out-of-scope queries. The system should recognize the boundary and refuse, not attempt to be helpful in areas where it should not operate.

## Escalation Triggers

Some situations require human involvement. Negative criteria define when the system must escalate rather than attempting to handle the situation itself.

**Must Escalate When User Explicitly Requests Human Support**

Users have the right to speak with humans. When they request it, the system must comply, not try to convince them to continue with the AI.

The criterion might specify: when a user requests to speak with a human representative, the system must immediately begin the escalation process; zero instances of user requests for human support may result in the system deflecting or discouraging the request.

**Must Escalate When Confidence Falls Below Threshold**

For high-stakes applications, the system should escalate to humans when its confidence in its response is low. Better to acknowledge uncertainty and escalate than to provide a confident but potentially wrong answer.

For a medical triage assistant, the criterion might specify: when the system's confidence in its assessment is below 70%, it must escalate to a human medical professional rather than providing a recommendation; zero low-confidence cases may result in definitive recommendations without escalation.

**Must Escalate When Safety Concerns Are Detected**

When the system detects language suggesting self-harm, harm to others, abuse, or other safety concerns, it must escalate to appropriate resources.

For a mental health chatbot, the criterion might specify: when users express suicidal ideation, the system must immediately provide crisis resources and offer to connect them with crisis support; zero instances of expressed self-harm intent may be handled purely by the chatbot without providing crisis resources and escalation options.

## Testing Negative Criteria

Testing negative criteria requires different approaches than testing positive criteria because you are looking for absence of failure rather than presence of success.

**Adversarial Testing**

Adversarial testing deliberately tries to make the system fail. Testers create prompts designed to elicit forbidden behaviors, extract sensitive information, bypass safety filters, or trigger hallucination.

This requires creativity and systematic exploration of failure modes. What prompts might cause citation hallucination? What queries might bypass content filters? What social engineering techniques might extract system prompts?

For a production system, adversarial testing should be ongoing, not a one-time check. New attack vectors emerge continuously, and systems must be tested against them.

**Red Teaming**

Red teaming is structured adversarial evaluation where a dedicated team attempts to break the system. Red teams should include people with security expertise, people familiar with prompt injection techniques, and domain experts who understand subtle failure modes specific to your application.

Red team evaluations should be documented, with each discovered failure mode cataloged, assessed for severity, and addressed before launch. Critical failures—violations of core negative criteria—are show-stoppers that must be fixed.

**Boundary Probing**

Boundary probing systematically tests the edges of the system's capabilities: queries that are just barely in scope, just barely out of scope, ambiguous cases, edge cases where facts are uncertain.

These boundary cases are where failures often occur. The system handles clear cases well but makes mistakes at the margins. Testing negative criteria requires comprehensive boundary coverage.

**Continuous Monitoring**

Negative criteria must be monitored continuously post-launch, not just tested pre-launch. Logging and monitoring should flag potential violations: responses that might contain PII, citations that cannot be verified, content that triggers safety filters, escalation triggers that were not properly handled.

Monitoring data feeds back into adversarial testing and model refinement, creating a continuous improvement loop around negative criteria.

## The Relationship to Behavior Boundaries

The negative criteria discussed here have a close relationship to the behavior boundaries and safety constraints that will be explored in depth in Section 4. While this chapter focuses on defining negative success criteria during problem framing, Section 4 will address implementing and enforcing those boundaries in production systems.

The key distinction is between definition and implementation. Here, we define what the system must never do. In Section 4, we will explore how to build systems that reliably respect those definitions through prompt engineering, safety layers, monitoring, and escalation mechanisms.

Negative criteria are not afterthoughts or edge case considerations. They are core success criteria that must be defined before building, tested throughout development, and monitored continuously post-launch. A system that excels on positive criteria but fails on negative criteria has failed, period. By making failure modes explicit and measurable, negative criteria ensure that optimization toward success does not inadvertently create paths to catastrophic failure.

With comprehensive success criteria now defined across all dimensions—functional correctness, behavioral quality, operational performance, and critical boundaries—you have established what success truly means for your AI system. These criteria provide the foundation for every downstream decision: architectural choices, implementation tradeoffs, testing strategies, launch decisions, and post-launch iteration. Success is no longer a vague aspiration but a concrete, measurable target that the entire team can build toward with clarity and confidence.

