# 4.2 â€” Functional Success Criteria

The legal research assistant seemed to work perfectly in demos. It analyzed case law, extracted relevant precedents, and generated summaries that looked professional. The prototype impressed the law firm's partners, and the team got approval to deploy it to fifty associates for real casework. Within two weeks, the complaints started rolling in.

Some associates reported that the system missed key citations that were clearly relevant to their cases. Others found that it included citations that were tangentially related but not actually applicable. Some responses were too brief to be useful, others were so lengthy that associates spent more time reading the summary than they would have spent reviewing the original documents. The system was technically functional in that it executed its task and returned responses, but it was not functionally successful in that those responses did not meet the actual needs of the users.

The problem was not the technology. The problem was that the team had never precisely defined what "correctly analyzing case law" meant. They knew they wanted accurate, relevant results, but they never specified what accuracy meant in this context, what level of completeness was required, what level of recall mattered, how to handle ambiguous cases, or what format the outputs should take. They had built a system without functional success criteria, and they paid the price in user frustration and lost credibility.

## What Functional Criteria Define

Functional success criteria define what the system's output must contain and what it must not contain. They specify the core task performance: does the system do what it is supposed to do, according to a precise definition of "do" and "supposed to"?

For AI systems, functional criteria typically cover several dimensions:

Correctness: Is the information in the output factually accurate? Does it contain errors or hallucinations? If the task involves reasoning, is the logic sound?

Completeness: Does the output include everything it should include? For extraction tasks, does it catch all relevant entities? For summarization, does it cover all key points?

Relevance: Does the output focus on what matters for the specific query or context? Does it include irrelevant information that distracts from the core response?

Coherence: Is the output logically structured? Do the parts fit together meaningfully? For generation tasks, does the text flow naturally?

These dimensions interact. A response can be factually correct but incomplete. It can be complete but include so much irrelevant detail that the relevant parts are buried. It can be correct, complete, and relevant but so incoherent that users cannot understand it.

The key to effective functional criteria is making these dimensions concrete for your specific task. "The system should be accurate" is not a functional criterion. "When extracting contract clauses, the system must achieve 95% precision and 90% recall on our evaluation dataset of 200 annotated contracts, with precision defined as the percentage of extracted clauses that are actually present in the contract and recall defined as the percentage of annotatable clauses that were successfully extracted" is a functional criterion.

## Task-Type-Specific Functional Criteria

Different task types require different functional criteria because the definition of success varies by task.

**Generation Tasks**

When your AI generates text from scratch, such as writing responses, creating content, or composing messages, functional criteria typically focus on:

Fluency: Is the generated text grammatically correct and natural-sounding? Does it read like human-written content? This can be measured through automated fluency scores, human ratings, or both.

Relevance: Does the generated content address the intended topic or query? For customer service responses, does it answer the customer's question? For content generation, does it stay on topic?

Factual accuracy: Are the facts stated in the generated text correct? This is particularly critical for any generation task that includes information that can be verified. The criterion should specify what sources count as authoritative for fact-checking.

Appropriate length: Is the output the right length for its purpose? Too brief and it might not be useful. Too long and it wastes user time. This should be specified as a range with clear boundaries.

Citation accuracy: If the generated text includes citations or references to sources, are they accurate and correctly formatted? A common failure mode is generating plausible-sounding citations that do not actually exist.

For a customer service response generator, functional criteria might specify: responses must be grammatically fluent with no more than one minor error per 100 words, must directly address the question asked as verified by a rubric-based evaluation, must include only facts that can be verified in the knowledge base, must be between 50 and 300 words, and must cite specific help articles when referencing policy or procedure.

**Extraction Tasks**

When your AI extracts structured information from unstructured text, such as pulling entities from documents or identifying key data from conversations, functional criteria typically focus on:

Precision: What percentage of extracted items are actually correct? High precision means few false positives. The criterion should specify precision targets overall and potentially for specific entity types or categories.

Recall: What percentage of extractable items are successfully found? High recall means few false negatives. The criterion should specify recall targets and may need to prioritize differently for different item types.

Format compliance: Does the extracted data conform to the required format or schema? For structured extraction, this is critical. The criterion should specify exact format requirements and allowed variations.

Handling of ambiguous cases: What should the system do when the text is ambiguous or unclear? Should it make a best guess, return multiple possibilities, or flag the case as uncertain? The criterion should specify the expected behavior.

For a contract analysis system extracting key terms, functional criteria might specify: precision of at least 95% for party names, dates, and financial terms; recall of at least 90% for all material terms as defined in the evaluation rubric; all extracted dates must be in ISO 8601 format; when a term is present but ambiguous, the system must extract all reasonable interpretations and flag the ambiguity rather than choosing one.

**Classification Tasks**

When your AI categorizes inputs into predefined classes, such as triaging support tickets or determining document types, functional criteria typically focus on:

Overall accuracy: What percentage of classifications are correct? This is the basic measure but often not sufficient on its own.

Per-class accuracy: How well does the system perform for each individual class? Systems often perform well on common classes but poorly on rare ones. Criteria should specify minimum performance levels for each class or for classes that matter most.

Handling of edge cases: How should the system handle inputs that do not clearly fit any class? Should it refuse to classify, choose the closest match, or assign a confidence score? The criterion should specify the expected behavior.

Confidence calibration: When the system reports confidence scores, are they reliable? A system that reports 90% confidence should be correct 90% of the time. Criteria should specify calibration requirements if confidence scores are user-facing.

For a support ticket triage system, functional criteria might specify: overall accuracy of at least 85% on the evaluation dataset of 1000 labeled tickets; accuracy of at least 80% for each of the five primary categories and at least 70% for each of the three rare categories; tickets that do not fit any category should be routed to a general queue; when confidence is below 0.7, the ticket should be flagged for human review; confidence scores should be calibrated such that tickets with confidence above 0.9 are correct at least 85% of the time.

**Transformation Tasks**

When your AI transforms content from one format or style to another, such as summarizing documents or translating between languages, functional criteria typically focus on:

Semantic preservation: Does the transformed output preserve the essential meaning of the input? What level of detail loss is acceptable? The criterion should specify what must be preserved and what can be simplified or omitted.

Format compliance: Does the output conform to the required format? For summaries, is it the right length and structure? For translations, does it follow the target language's conventions?

Completeness: Are all important elements from the input represented in the output? The criterion should define what counts as "important" in the context of the transformation.

For a document summarization system, functional criteria might specify: summaries must preserve all key facts as defined by a structured rubric applied to the source document; summaries must be between 10% and 20% of the original document length; summaries must follow the standard executive summary format with introduction, key findings, and implications sections; when evaluated by human raters, at least 90% of summaries must receive a score of 4 or higher on a 5-point semantic preservation scale.

**Reasoning Tasks**

When your AI performs reasoning or analysis, such as answering questions that require inference or providing recommendations based on multiple factors, functional criteria typically focus on:

Logical validity: Is the reasoning logically sound? Are conclusions properly supported by premises? The criterion should specify what counts as valid reasoning in your domain.

Consideration of alternatives: Does the system consider multiple possibilities or just jump to a conclusion? For complex decisions, criteria should specify that multiple options must be evaluated.

Appropriate uncertainty: Does the system acknowledge when it is uncertain? Does it avoid overconfident conclusions when evidence is weak? Criteria should specify when and how uncertainty should be communicated.

For a medical diagnostic assistant, functional criteria might specify: diagnoses must include reasoning that traces from symptoms to potential conditions; when multiple conditions are consistent with symptoms, all conditions with at least 10% likelihood should be mentioned; when symptoms are ambiguous or could indicate multiple conditions, the system must explicitly acknowledge uncertainty and suggest additional tests rather than recommending a single diagnosis.

## Writing Testable Functional Criteria

The difference between aspirational statements and functional criteria is testability. A criterion is only useful if you can determine whether it has been met.

Consider the difference between these two attempts to specify response quality for a question-answering system:

Vague: "Responses should be helpful and accurate."

Testable: "When tested against the evaluation dataset of 500 questions, at least 90% of responses must receive a score of 4 or 5 on the accuracy rubric and at least 85% must receive a score of 4 or 5 on the helpfulness rubric, where accuracy is rated by comparing responses to expert-authored answers and helpfulness is rated by whether the response fully addresses the question asked."

The second version is testable because it specifies:

What to test: the evaluation dataset of 500 questions.

How to measure: scoring on accuracy and helpfulness rubrics.

What counts as success: specific percentage thresholds.

How the rubrics are applied: what accuracy and helpfulness mean in concrete terms.

To make functional criteria testable, they must include or reference:

An evaluation dataset or evaluation procedure: You need a way to test the system. This might be a fixed dataset of examples with ground truth, a procedure for generating test cases, or a specification for human evaluation.

Metrics with target thresholds: You need quantitative measures and specific targets. "High accuracy" is not a target. "At least 90% accuracy" is a target.

Definitions of the metrics: What exactly are you measuring? Accuracy on what basis? Precision and recall relative to what ground truth?

Rubrics or scoring procedures for subjective dimensions: When the criterion involves human judgment, you need a rubric that makes the judgment as objective as possible.

A particularly effective test of whether your criteria are testable is the "new team member test." If a new engineer joined your team tomorrow, could they read your functional criteria and implement a test suite to verify them? If not, your criteria are not specific enough.

## The Relationship Between Criteria and Architecture

Functional criteria should be defined before architecture decisions, but they inform those decisions. The criteria tell you what level of performance you need, which constrains your architectural choices.

If your functional criteria require 99% precision, you cannot use an architecture that tops out at 95% precision. If your criteria require handling 100,000 queries per day, you cannot use an approach that only scales to 10,000. If your criteria require responses to be based solely on specific documents, you cannot use a pure generation approach without retrieval and citation.

This means functional criteria serve as a forcing function for realistic planning. When you specify that you need 95% recall on a complex extraction task, you are forced to confront whether that is achievable with current technology, what resources it will require, and what tradeoffs it implies.

Conversely, you should resist the temptation to let available technology dictate your criteria. The criteria should reflect what success actually means for your use case. If you need 95% recall for the product to be useful, that is your criterion even if current models only achieve 85%. In that case, your options are to invest more in improving the model, to narrow the scope to cases where 85% is sufficient, or to acknowledge that the project is not yet feasible and should be deferred.

## Iteration and Refinement

Functional criteria are not set in stone forever, but they should be stable enough to guide development. It is reasonable to refine criteria after initial testing reveals what is realistic, but wholesale changes to criteria mid-project are a red flag.

A healthy refinement process looks like this: You define initial criteria based on your understanding of the task. You build a prototype and evaluate it against the criteria. You discover that some criteria were too strict given technical constraints and user needs, while others were too lenient. You refine the criteria and communicate the changes. The revised criteria guide the next iteration.

An unhealthy process looks like this: You define criteria. The system does not meet them. Rather than improving the system or acknowledging that it is not ready, you change the criteria to match what the system achieved. This defeats the purpose of having criteria in the first place.

The key distinction is whether refinement is driven by learning about the problem or by the desire to declare success. Refinement based on learning is healthy. Refinement based on retrofitting criteria to match whatever you built is not.

With functional criteria clearly defined and testable, you have established what your system must do correctly. But correctness alone does not determine success. How the system behaves, the experience it creates, and the impression it leaves matter equally. That is the domain of behavioral success criteria, which we turn to next.

