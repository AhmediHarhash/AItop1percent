# 2.2 — Single-Turn vs Multi-Turn Decomposition

The demo went perfectly.

A healthcare startup was showing their medical chatbot to potential investors. The founder asked it: "What are the symptoms of Type 2 diabetes?" The bot listed them clearly: increased thirst, frequent urination, fatigue, blurred vision. Investors nodded. Then the founder asked: "What are the treatment options?" The bot explained lifestyle changes, medication, and monitoring. More nods.

Two weeks later, they launched to 5,000 beta users. Within three days, their support queue was flooded.

Users were asking things like: "My dad has that. Should he see someone?" The bot replied with generic diabetes information, completely missing that "that" referred to the symptoms from three turns earlier. Users typed: "No, I mean the other thing you mentioned." The bot had no idea what "the other thing" was. Conversations spiraled into confusion.

What happened? The team had built a perfect single-turn question-answering system. They'd tested it with isolated queries. Each individual response was great. But users don't interact in isolated queries. They have conversations. And conversations are a completely different problem.

The team had decomposed their system for single-turn interactions. They never decomposed for multi-turn. That's like designing a car for straight roads and discovering too late that real driving involves turns, hills, and traffic.

## What Single-Turn Actually Means

Let me start by defining what single-turn really means, because most teams misunderstand it.

Single-turn doesn't mean "one question and one answer, then the conversation ends." It means: each turn is independent. The input to turn N doesn't depend on turns 1 through N-1. Each query is self-contained.

"What are the symptoms of diabetes?" is a single-turn query. It has everything needed to produce an answer. No context required.

"What about Type 1?" is not a single-turn query, even though it's short. It's incomplete without knowing that the previous turn was about diabetes symptoms. It depends on context.

Single-turn systems decompose into: query understanding, retrieval, generation, formatting. That's it. Four sub-problems. Each one processes a single input to produce a single output.

Query understanding takes the user's query and extracts intent and entities. For "What are the symptoms of diabetes?" it identifies: intent is SYMPTOMS, entity is DIABETES.

Retrieval finds relevant documents. It searches the knowledge base for diabetes symptom information.

Generation synthesizes a response from retrieved content. It reads the documents and produces a natural language answer.

Formatting makes the response user-friendly. Maybe it adds bullet points, highlights key terms, or includes citations.

Four sub-problems. Clean interfaces. Independent components. This is totally buildable.

The problem is: real users don't use systems this way.

## What Multi-Turn Actually Requires

Multi-turn means: each turn depends on previous turns. The system must track what's been said, what's been established, and what the user is referring to.

"What are the symptoms of diabetes?" followed by "What about Type 1?" is multi-turn. The second query only makes sense in context of the first.

Now decomposition gets vastly more complex. You need all four single-turn sub-problems plus a bunch of new ones.

Sub-problem five: Conversation memory. What has been discussed? You need to store previous queries, responses, and extracted information. Not just the text—the semantic content. When the user said "diabetes," what did they mean? When you responded with symptoms, what exactly did you list?

Sub-problem six: Context tracking. What is the current topic? What entities are active? When the user says "it," what does "it" refer to? This requires maintaining a context state that updates after each turn.

Sub-problem seven: Reference resolution. When the user says "that," "it," or "the other thing," what are they pointing to? You need to map pronouns and references to specific entities or concepts from previous turns.

Sub-problem eight: Intent evolution. The user starts asking about symptoms, then shifts to treatment, then asks about prevention. The intent is changing across turns. You need to detect these shifts and understand how they relate.

Sub-problem nine: Topic switching. The user asks about diabetes, you answer, then they ask about hypertension. Is this a new conversation or a related follow-up? Do you maintain context or reset?

Sub-problem ten: Repair sequences. The user says something unclear. You ask for clarification. They provide it. Now you need to go back and answer the original question using the clarified information. That's a multi-turn repair pattern.

Sub-problem eleven: Escalation across turns. The user asks a question. You answer. They're unsatisfied and ask a follow-up. You answer again. They're still unsatisfied. At what point do you escalate to a human? This decision requires analyzing the trajectory of the conversation, not just the current turn.

We went from four sub-problems to eleven. That's why teams get destroyed when they build single-turn and discover multi-turn in production.

## The Sub-Problems Unique To Multi-Turn

Let me dig into the sub-problems that only exist in multi-turn, because this is where the hidden complexity lives.

State management is the foundation. You need to decide: what state do we maintain between turns? Just the previous turn? The whole conversation? A summary of the conversation? Different answers have different trade-offs.

If you only track the previous turn, you can't handle: "Tell me about diabetes. What are the symptoms? What about causes?" The third query needs context from turn one, not just turn two.

If you track the whole conversation verbatim, your context window fills up fast. After ten turns, you're feeding hundreds of tokens of conversation history into every subsequent turn. That's slow and expensive.

If you track a summary, you lose detail. The user might refer back to something specific you said six turns ago. If it's not in the summary, you can't resolve the reference.

State management is an entire sub-problem with no perfect solution. You need a strategy, and that strategy changes based on your use case.

Reference resolution is the most underestimated sub-problem. Users are incredibly imprecise in conversation. They say:

- "Tell me more about that"
- "What did you mean when you said X?"
- "Does the same apply to my situation?"
- "Is there a faster way?"

Each of these requires resolving an implicit reference. "That" refers to what? "X" is which part of my previous response? "My situation" means which details they mentioned? "Faster way" compared to which method?

You can't resolve these references without understanding what's been established in the conversation. That means parsing previous turns, identifying entities and topics, and maintaining a reference graph.

I've seen teams try to solve this by just feeding the whole conversation history into the LLM and hoping it figures out references. That works 70% of the time. The other 30%, it guesses wrong, and the conversation becomes incoherent.

Proper reference resolution requires explicit tracking: what entities have been mentioned? What topics have been discussed? What concepts have been explained? Then, when the user says "that," you can look up what they're likely referring to.

Intent evolution is subtle but critical. In single-turn, intent is static. In multi-turn, intent changes and builds across turns.

User asks: "What are diabetes symptoms?" Intent: LEARN_SYMPTOMS.
User asks: "How serious is that?" Intent: ASSESS_SEVERITY, but still about diabetes symptoms.
User asks: "Should I see a doctor?" Intent: GET_RECOMMENDATION, in the context of diabetes symptoms and severity.

Each turn has its own intent, but they're linked. The third turn only makes sense as part of a trajectory from learning to assessment to action.

If you treat each turn independently, you answer: "Whether you should see a doctor depends on your symptoms." But the user already told you the symptoms—by implication, by asking about them. You should answer: "Given the symptoms you're asking about, yes, if you're experiencing several of these, you should consult a doctor."

That requires tracking intent across turns and understanding the conversation's progression.

## Why Teams Discover Multi-Turn Too Late

Here's why this problem catches teams off guard: demos are single-turn. Internal testing is single-turn. Even early beta often looks single-turn.

In demos, you prepare the queries. You make each one self-contained. You ask: "What are the symptoms?" Then you reset and ask: "What are the treatment options?" You never chain them because you're showing capabilities, not simulating real use.

In internal testing, engineers and PMs know how to ask self-contained questions. They're precise. They say: "What are the treatment options for Type 2 diabetes?" Not: "What about treatment?" They don't need context because they're testing specific functionality.

In early beta, users are tentative. They're learning how the system works. They ask simple, complete questions. They don't yet trust it enough to have real conversations.

Then the system gets real usage. Users treat it like a conversation partner. They expect it to remember what they said. They use pronouns and references. They ask follow-ups. And the system, designed for single-turn, falls apart.

The team scrambles. They add conversation history to the context. It helps a little, but now they have new problems: hallucinations increase because the context is long, latency increases because they're processing more tokens, costs increase because every turn is expensive.

They're patching a single-turn system to handle multi-turn. It's like adding wings to a car. You should have designed an airplane.

## The Evaluation Gap

Here's the insidious part: single-turn evaluation looks great even when multi-turn is broken.

You evaluate 1,000 queries. Each one is answered correctly in isolation. Your accuracy is 94%. You ship.

Users have conversations. Across turns, accuracy drops to 68%. Why? Because your eval set didn't include: "What about Type 1?" without context, or "Is that dangerous?" without knowing what "that" refers to, or "My dad has those symptoms" without tracking that "those symptoms" came from three turns ago.

Your single-turn eval measured: can you answer well-formed, complete questions? Your users need: can you maintain coherent conversations?

Different problem. Different eval. Different decomposition.

Teams that decompose only for single-turn build eval sets that are useless for production. They measure the wrong thing and get surprised by failures they never tested for.

## How Multi-Turn Changes Evaluation Strategy

When you properly decompose for multi-turn, your evaluation strategy changes completely.

You can't evaluate turn-by-turn anymore. You need conversation-level evaluation. Did the full conversation succeed or fail?

A user asks: "What are diabetes symptoms?" You answer. They ask: "Should I be worried?" You answer. They ask: "What should I do?" You answer. Did this conversation achieve the user's goal? You can't know by evaluating each turn separately. You need to judge the whole arc.

That means you need conversation-level ground truth. Not just: "This response is correct." But: "This conversation successfully helped the user understand their risk and know what action to take."

You need conversation-level labels. Not just 1,000 query-response pairs. 200 multi-turn conversations with success/failure labels and specific failure mode annotations.

You need conversation-level metrics. Not just accuracy per turn. Metrics like: successful resolution rate, escalation rate, conversation length to resolution, user satisfaction per conversation.

This is a completely different eval infrastructure. If you haven't decomposed for multi-turn, you don't know you need it.

## The Components That Change

Let me show you how specific components change when you decompose for multi-turn.

Query understanding in single-turn: Extract intent and entities from the query text.

Query understanding in multi-turn: Extract intent and entities from the query text plus conversation context. Resolve references. Detect topic shifts. Identify clarification requests. Understand how the current query relates to previous turns.

That's not the same component anymore. It has different inputs (query plus context), different outputs (intent plus references plus conversation state updates), and different failure modes (reference resolution errors, topic shift detection failures).

Retrieval in single-turn: Find documents relevant to the query.

Retrieval in multi-turn: Find documents relevant to the current query in the context of the conversation. If the user asked about diabetes symptoms, then asks about treatment, you're not searching from scratch. You're narrowing to diabetes treatment, not general treatment information.

Different retrieval strategy. Context-aware search. Possibly even using previous retrieved documents to inform current retrieval.

Generation in single-turn: Synthesize a response from retrieved documents.

Generation in multi-turn: Synthesize a response that answers the current query, acknowledges relevant context from previous turns, maintains conversational coherence, and sets up natural follow-up questions.

If the user asks: "Is that serious?" you don't just say "Type 2 diabetes can be serious if unmanaged." You say: "The symptoms you asked about—increased thirst, frequent urination—can indicate Type 2 diabetes, which is serious if left unmanaged but highly treatable with proper care."

You're incorporating context. You're building on previous turns. You're creating conversation flow.

## The Architecture Shift

Decomposing for multi-turn forces an architecture shift.

Single-turn architecture: stateless request handler. Each request is independent. You can scale horizontally. You can cache aggressively. You can optimize each turn independently.

Multi-turn architecture: stateful conversation manager. You need to maintain state across turns. You need session management. You need state storage and retrieval. You need to handle state consistency when users switch devices or when sessions timeout.

The infrastructure is different. The complexity is higher. The failure modes are new: state corruption, state loss, state conflicts, state synchronization issues.

If you designed for single-turn, your infrastructure doesn't support this. You need to rebuild.

## When Single-Turn Is Actually Enough

I don't want you to think every AI system needs multi-turn decomposition. Sometimes single-turn is legitimately sufficient.

Single-turn is enough when: user queries are truly independent. A search engine is mostly single-turn. Each search is a new question. Users don't expect the search engine to remember their previous search when they type a new query.

Single-turn is enough when: the interface discourages conversation. A form-based interface where users fill in fields and get a result is single-turn. They're not having a dialogue. They're submitting a request.

Single-turn is enough when: multi-turn adds no value. An image classification API is single-turn. Each image is classified independently. There's no context to track across images.

The mistake is building single-turn when users expect multi-turn. If your interface is a chat window, users will treat it like a conversation. If you call it a "chatbot," users will expect it to remember context. If you're replacing a human conversation, multi-turn is mandatory.

## The Hybrid Approach

Some systems need both. You have single-turn queries and multi-turn conversations in the same product.

A documentation chatbot might get: "How do I install the library?" That's single-turn. It might also get: "I tried that. It failed with error X. What now?" That's multi-turn.

You need to decompose for both. That means: detect whether the current query is self-contained or context-dependent. Route accordingly. Use single-turn pipeline for self-contained queries (faster, cheaper, simpler). Use multi-turn pipeline for context-dependent queries (slower, more complex, but handles conversation).

This is the most complex decomposition. You now have a routing layer, two separate pipelines, and a decision logic for when to use each.

But it's better than forcing everything through the multi-turn pipeline (expensive, slow) or failing on context-dependent queries (bad user experience).

## What This Means For Your Roadmap

If you're building a conversational AI product, you need to decide: are we single-turn or multi-turn?

If you're single-turn, decompose accordingly. Optimize for speed and accuracy per query. Don't build conversation infrastructure you don't need.

If you're multi-turn, decompose accordingly from day one. Don't build a single-turn system and retrofit conversation later. The architecture is fundamentally different.

If you're hybrid, decompose for both. Design the routing logic. Build separate pipelines. Plan for the complexity.

This decision drives everything: your architecture, your eval strategy, your data requirements, your infrastructure, your team composition.

Teams that make this decision explicitly ship products that work. Teams that don't make it at all build single-turn by default, discover multi-turn in production, and scramble to catch up.

## The Red Flags

Here's how you know if you've under-decomposed for multi-turn:

Red flag one: Users say "it forgets what I told it." That's state management failure.

Red flag two: Users repeat information across turns. They say: "My dad has diabetes" then three turns later they say: "My dad has diabetes, should he..." They're repeating because they don't trust your system to remember. That's context tracking failure.

Red flag three: Users get frustrated with pronouns. They say "that" and you give the wrong answer, so they retype the full noun phrase. That's reference resolution failure.

Red flag four: Conversations go in circles. User asks, you answer, they ask a slight variation, you give the same answer. That's intent evolution failure—you're not tracking that they're trying to go deeper.

Red flag five: Users escalate to humans after one failed turn. They should give you 2-3 tries to resolve, but they don't trust you to recover. That's repair sequence failure.

If you see these patterns in user behavior, you built single-turn for a multi-turn problem.

## What You Should Do Differently

If you're in the design phase, ask: will users have multi-turn conversations? If yes, decompose for it now.

Add state management to your sub-problems. Define what state you'll maintain and how.

Add reference resolution. Plan how you'll track entities and resolve pronouns.

Add context tracking. Decide how you'll detect topic shifts and intent evolution.

Build conversation-level evals. Not just turn-level. Create multi-turn test cases that validate conversation coherence.

Design your infrastructure for state. Session management, state storage, state consistency.

If you're already in production with a single-turn system and users need multi-turn, don't patch it. Redesign.

You might need to rebuild the query understanding layer to handle context. You might need to add a state management layer. You might need to change your retrieval strategy to be context-aware. You might need to rebuild your eval infrastructure.

That's expensive. It's less expensive than continuing to ship a fundamentally wrong architecture.

## The Competitive Advantage

Here's the opportunity: most teams under-decompose for multi-turn. They build single-turn, discover the gap, and patch badly.

If you properly decompose for multi-turn from the start, you ship a product that feels coherent and intelligent. Users can have real conversations. Context is maintained. References resolve correctly. The experience is smooth.

That's rare. That's valuable. That's a competitive advantage.

The teams that win in conversational AI aren't the ones with the best LLM. They're the ones who decomposed the problem correctly and built the infrastructure to support real conversations.

Single-turn is easy. Multi-turn is hard. And hard problems, solved well, create defensibility.

In the next section, we'll tackle another problem that looks simple until you decompose it: retrieval-augmented systems, where five distinct sub-problems hide inside what most teams treat as one monolithic "RAG pipeline."
