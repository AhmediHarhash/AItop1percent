# 1.1 — The Cost of Skipping Problem Framing

In early 2024, a mid-sized healthcare technology company spent seven months and approximately 2.3 million dollars building an AI-powered chatbot to help nurses document patient interactions. The team followed all the best practices they could find: they used the latest language models, implemented retrieval-augmented generation, built a slick user interface, and conducted extensive prompt engineering. The system worked beautifully in demos. Executives were thrilled. The engineering team was proud.

Then they deployed it to a pilot group of 50 nurses.

Within two weeks, usage dropped to near zero. The problem was not that the chatbot failed to work—it generated coherent, grammatically correct summaries. The problem was that nurses did not need a conversational interface to document patient interactions. What they actually needed was a fast, structured data-entry system that could capture observations in under 30 seconds during or immediately after patient contact. They needed something that integrated with their existing workflow of glancing at a tablet between patient rooms, not something that required them to have a conversation.

The chatbot assumed the problem was "nurses struggle to articulate their observations in written form." The real problem was "nurses have no time to write anything at all." One required natural language generation. The other required workflow automation and structured input. The company had built an elegant solution to the wrong problem.

This story is not an outlier. It represents the most common failure mode in AI product development: skipping or rushing through problem framing and jumping straight to implementation. The consequences of this mistake compound through every subsequent phase of development, and by the time teams realize they have built the wrong thing, they have usually invested too much to start over.

## The Pattern Behind Every Failed AI Project

When AI projects fail—and industry data from 2025-2026 consistently shows that 60 to 80 percent of AI initiatives fail to reach production or deliver meaningful business value—the root cause is rarely technical. Models work. Infrastructure scales. Engineers are competent. The failure happens earlier, in the problem definition phase, where teams make assumptions about what users need, what success looks like, and what the system should actually do.

McKinsey's 2025 report on AI adoption found that the primary barrier to AI value creation was not a lack of talent, compute, or data. It was the inability to correctly identify and frame high-value use cases. PwC's 2026 survey of enterprise AI leaders echoed this finding: among projects that failed to deliver ROI, 73 percent cited "misaligned problem definition" as a contributing factor, while only 31 percent cited technical challenges.

The pattern is consistent across industries and company sizes. A retail company builds a recommendation engine optimized for click-through rate when the business actually cares about purchase conversion and customer lifetime value. A financial services firm builds a fraud detection model optimized for precision when the regulatory requirement is actually high recall with explainability. A logistics company builds a route optimization system that minimizes total distance when drivers actually care about predictable shift lengths and avoiding late-night deliveries.

In each case, the team asked "how do we build this?" before asking "what are we actually trying to accomplish, and for whom?" They optimized a metric that seemed reasonable in isolation but was misaligned with the actual success criteria. They solved the problem they assumed existed rather than the problem that actually existed.

## Three Case Studies in Misframing

Consider the case of a SaaS company that wanted to "reduce customer support volume" using AI. The product team framed the problem as "customers ask repetitive questions that could be answered by documentation." The solution: a chatbot trained on the knowledge base, deployed on the support portal.

Three months after launch, support ticket volume had not decreased. Investigation revealed that customers were not asking repetitive questions because they were too lazy to read documentation. They were asking questions because the product itself was confusing, because error messages were unclear, and because certain workflows were genuinely broken. The chatbot could retrieve documentation, but that did not help users who were experiencing bugs or design flaws. The real problem was not information access—it was product usability.

Had the team framed the problem more carefully, they would have started by analyzing support tickets to understand the distribution of issue types. They would have discovered that 40 percent of tickets were bug reports, 25 percent were feature requests, 20 percent were complaints about confusing UX, and only 15 percent were questions answerable by existing documentation. The correct intervention was not a chatbot. It was fixing bugs, improving error messages, and redesigning confusing flows. AI was not the right tool at all.

Now consider a different failure mode: a team that correctly identified the problem but framed it at the wrong level of abstraction. A large e-commerce company wanted to "personalize the shopping experience" and framed the problem as "build a model that predicts what products a user will buy." This framing sounds reasonable, but it collapses several distinct sub-problems into one:

- Predicting which category of product a user is interested in (clothing vs electronics vs home goods)
- Predicting which specific items within a category will appeal to a user's taste
- Predicting which price range is acceptable given the user's budget and purchase history
- Predicting the right time to show a recommendation (immediate visit vs retargeting email vs push notification)
- Predicting whether a user is browsing for inspiration or shopping with intent to buy

A monolithic "predict what users will buy" model cannot optimize for all of these simultaneously because they have different success criteria, different input features, different latency requirements, and different consequences for getting it wrong. Showing the wrong category is a different kind of failure than showing the right category but wrong price range. Interrupting a user who is casually browsing is different than failing to retarget a user who abandoned a cart.

The team spent five months building a single large model. It performed mediocrely on all tasks. When they eventually re-framed the problem as a pipeline of smaller, specialized models—each optimized for a specific sub-task—they saw measurably better results in two months.

The third failure mode is optimizing the wrong metric. A content moderation team at a social media platform was tasked with "reducing harmful content." They framed the problem as "maximize precision and recall on a toxicity classifier." The model achieved 94 percent precision and 89 percent recall on the test set. The team celebrated.

When deployed, the system triggered a user revolt. The problem was that "toxicity" was not the same thing as "harmful." The model flagged profanity, slang, and informal language used in marginalized communities at a much higher rate than genuinely harmful harassment or misinformation. It optimized the metric perfectly but optimized the wrong metric. Precision and recall on a toxicity label did not align with the actual goal, which was protecting users from harm while preserving legitimate expression.

A better framing would have started with "what does harm actually mean in this context?" and decomposed it into categories: targeted harassment, misinformation, graphic violence, self-harm content, spam. Each category requires different handling, different precision-recall trade-offs, and different appeals processes. Collapsing them into a single "toxicity" score made the problem tractable for modeling but misaligned it with the actual human values at stake.

## The Tangible Costs

The cost of poor problem framing is not abstract. It manifests in measurable, painful ways.

**Time:** The healthcare chatbot team spent seven months building the wrong thing. When they finally accepted that the project had missed the mark, they had to start over. The total time from project kickoff to delivering a system that nurses actually used was 18 months. A well-framed problem would have pointed them toward workflow automation from day one, cutting the timeline to under six months.

**Money:** The direct cost of the failed chatbot was 2.3 million dollars in engineering time, infrastructure, and vendor contracts. The opportunity cost—what the team could have built instead—was likely much higher. Every dollar spent on the wrong solution is a dollar not spent on the right one.

**Trust:** After the chatbot failure, the healthcare company's executive team became skeptical of AI investments. When the engineering team proposed a different AI project six months later—one that was actually well-framed and high-value—they faced resistance. "We already tried AI and it did not work" became the default response. Poorly framed projects do not just fail in isolation; they poison the well for future initiatives.

**Team morale:** Engineers who spend months building something that no one uses experience a specific kind of demoralization. It is not the same as failing because the problem was too hard or the technology was not ready. It is the feeling of realizing that the hard work was wasted because someone should have asked better questions at the beginning. High-performing engineers leave teams where this happens repeatedly.

**Opportunity cost:** While the SaaS company spent three months building a support chatbot, their competitors were fixing the actual usability issues that drove support tickets in the first place. By the time the chatbot team realized their mistake, they had fallen further behind in product quality.

These costs are not one-time events. Poor problem framing creates a cascade of downstream failures. You cannot build good ground truth for a poorly framed problem because you do not know what "good" means. You cannot design effective evals because you do not have clear success criteria. You cannot monitor the system in production because you do not know what to measure. You cannot iterate toward a better solution because you do not have a clear target.

## Why "Move Fast" Without Framing Means "Move Fast in the Wrong Direction"

There is a pervasive belief in the technology industry that speed is the ultimate virtue. "Move fast and break things" was Facebook's original motto. Startups are told to "ship fast, iterate faster." Engineers are rewarded for bias toward action.

This mindset works when you are exploring a well-understood problem space and the cost of failure is low. If you are testing website layouts or experimenting with email subject lines, rapid iteration makes sense. You can try ten variants in a week, measure what works, and move on.

AI projects are different. The cost of building and deploying a model is high. The feedback loop is long. If you train a model on the wrong task, you will not know until weeks or months later when users do not engage, business metrics do not improve, or the system fails in production. By that time, you have already made dozens of downstream decisions—data pipelines, infrastructure, team structure, vendor contracts—that are expensive to reverse.

Moving fast in the wrong direction is worse than moving slowly in the right direction. A team that spends two weeks carefully framing a problem and then six weeks building the right solution ships a working product in two months. A team that skips framing and spends six weeks building the wrong thing, then another two weeks realizing it is wrong, then another eight weeks rebuilding, ships a working product in four months—if they are lucky.

The irony is that problem framing, done well, actually accelerates development. When you have a clear task definition, well-specified inputs and outputs, and concrete success criteria, implementation becomes straightforward. Engineers know exactly what to build. Decisions about architecture, data, and models flow naturally from the framing. You avoid the endless debates and mid-project pivots that slow teams down.

Conversely, when framing is vague, every downstream decision becomes a negotiation. What counts as a successful output? Which edge cases matter? How do we handle ambiguity? These questions should have been answered during problem framing. When they were not, teams argue about them during implementation, during testing, during deployment, and sometimes even after the product has shipped.

## The Industry Data

The statistics on AI project failure are stark and consistent across sources. Gartner's 2025 report found that only 20 percent of AI projects moved from pilot to production. Deloitte's 2026 survey found that 63 percent of organizations reported AI initiatives that failed to deliver expected business outcomes. Forrester's research showed that the median time from AI project start to abandonment was nine months.

When researchers dig into why projects fail, technical reasons rank surprisingly low. "Model performance was insufficient" accounts for only about 15 percent of failures. "We could not get enough data" accounts for another 20 percent. The majority of failures—roughly 60 to 65 percent—are attributed to organizational, strategic, or scoping issues. The problem was not that the team could not build the system. The problem was that they built the wrong system, or they built a system that did not integrate with real workflows, or they built a system whose success could not be measured.

All of these are problem framing failures.

The companies that succeed with AI share a common trait: they invest heavily in upfront problem definition. They spend time with end users. They decompose vague goals into specific tasks. They define success criteria before writing code. They prototype the input-output interface before training models. They treat framing as a first-class engineering discipline, not a formality to rush through.

## A Different Starting Point

Imagine the healthcare company had started differently. Before committing to a chatbot, they spent one week interviewing nurses. They shadowed them during shifts. They timed how long nurses spent on documentation. They asked what would save the most time. They discovered that nurses already knew what to document—they just needed a faster way to capture it.

With this insight, the team reframed the problem: "Reduce the time nurses spend on documentation from five minutes per patient to under 30 seconds." This framing immediately pointed toward different solutions: voice dictation with structured fields, mobile-first UI optimized for one-handed use, smart defaults based on patient history, integration with the EMR system to auto-populate fields.

They built a prototype in two weeks—not a full system, just a mockup showing the workflow. They tested it with three nurses. The feedback was immediate and clear: "Yes, this would save me hours every week." The team knew they were on the right track.

They built the real system in four months. Adoption was above 80 percent within the first month. Documentation time dropped from five minutes to 40 seconds. Nurses loved it. The executive team considered it one of the most successful internal tools the company had ever built.

The difference between this outcome and the chatbot failure was not engineering skill or model quality. It was problem framing.

The next question, then, is: what does good problem framing actually look like, and how is it different from other activities that might seem similar?
