# 5.2 — Input Specification: What the System Receives

A customer service AI system at a major bank went live in March 2025. For the first two weeks, it worked flawlessly in production. Customer satisfaction scores were up. Resolution times were down. The product team was celebrating.

Then tax season hit.

Users started uploading images of tax forms, asking questions about 1099s and W-2s. They pasted entire IRS documents into the chat. They attached PDFs of mortgage statements that were 80 pages long. They switched between English and Spanish mid-conversation. They asked questions about products the system had never been trained on.

The system began to fail in spectacular ways. It timed out on large PDFs. It hallucinated answers about tax implications because it confused 1099 forms with account statements. It responded in English when users wrote in Spanish. It crashed when users uploaded images in formats the model couldn't process.

None of these were model failures. These were input specification failures.

The team had defined what the system should do, but they had never formally defined what the system could receive. They tested with clean, simple queries during development. They assumed users would behave the same way. Users didn't. Real-world inputs were messy, varied, and pushed every boundary the team hadn't thought to set.

This is why input specification is not optional. It's the first line of defense between your controlled development environment and the chaos of production.

## The Complete Input Space

Let me break down what actually goes into an AI system. Most teams think narrowly about user inputs: the query someone types, the document they upload, the button they click. But that's only a fraction of the input space.

The complete input to an AI system includes five categories: user-provided inputs, system context, retrieved data, conversation history, and tool outputs. If you don't specify all five, you have gaps. Those gaps become failures in production.

**User-provided inputs** are what most people think of first. This includes text queries typed into a chat box, files uploaded by users, voice audio recorded from microphone input, form fields filled out in a UI, dropdown selections and button clicks, images or videos submitted for analysis, and structured data like spreadsheets or CSVs. These are explicit inputs that users consciously provide.

**System context** is everything the system knows about the user and their environment that they didn't explicitly provide this time. This includes user profile information like name, account type, subscription tier, and preferences. It includes session state like how long they've been logged in, what pages they visited before this interaction, what actions they took recently. It includes account state like current balance, active services, pending transactions, support ticket history. It includes permissions like what data they can access, what actions they're allowed to take, what features are enabled for their account. It includes locale information like timezone, language preference, country, currency. This context shapes how the system interprets and responds to user inputs.

**Retrieved data** is information the system fetches in response to a user input. This includes results from a knowledge base or documentation search, records from a database query, responses from internal or external APIs, real-time data like current prices or availability, historical data like past transactions or interactions. This data flows into the system as input even though the user didn't directly provide it.

**Conversation history** is the context from previous turns in a multi-turn interaction. This includes the user's previous messages, the system's previous responses, summarized context if the conversation is too long to fit in the model's context window, learned preferences from earlier in the conversation, and disambiguation or clarification that happened in earlier turns. Many AI failures happen because the system loses track of conversation history or interprets it incorrectly.

**Tool outputs** are results from previous steps in agentic or multi-step systems. If your system uses tools—like calling an API, querying a database, running a calculation, or invoking another model—the outputs from those tool calls become inputs to the next step. These need to be specified just like any other input, including their format, possible values, and error conditions.

Each of these five categories must be explicitly defined in your input specification. If you skip one, you create blind spots. The system will encounter those inputs in production, and it won't know how to handle them because you never designed for them.

## Specifying User-Provided Inputs

Let's start with the most visible category: what users explicitly provide. For each type of user input, you need to specify format, constraints, required versus optional, default values, and validation rules.

For text inputs, specify: maximum and minimum length in characters, allowed character sets such as alphanumeric, Unicode, or ASCII only, formatting expectations like plain text versus markdown versus HTML, language constraints such as English only or multilingual support, and special character handling like how to process line breaks, tabs, or emoji. Don't assume "text is text." A 10-word query and a 10,000-word document are both text inputs, but they require completely different processing.

For file uploads, specify: allowed file formats like PDF, DOCX, JPG, PNG, CSV; maximum file size in megabytes; maximum page count for documents; image resolution requirements; whether the system supports compressed files or archives; and whether files can contain multiple documents or must be single-document. The bank's customer service system failed because they never specified these constraints. Users uploaded 80-page PDFs and the system choked.

For voice inputs, specify: audio format like WAV, MP3, or FLAC; sample rate and bit depth; maximum duration in seconds or minutes; whether background noise will be filtered; whether multiple speakers are supported; and what languages are supported. Voice input is not the same as text input. The preprocessing is different, the error modes are different, and the quality depends on audio characteristics you must define.

For structured inputs like forms, specify: required fields versus optional fields; data types such as string, integer, date, or boolean; valid ranges or enumerations like dropdown options; format requirements for dates, phone numbers, or email addresses; and cross-field validation rules like "end date must be after start date." Structured inputs are easier to validate than free text, but only if you specify what valid means.

For images and video, specify: supported formats like JPG, PNG, MP4; resolution requirements such as minimum or maximum dimensions; aspect ratio constraints; whether the system handles color, grayscale, or both; maximum file size; and whether video inputs must include audio or can be video-only. Image and video inputs are complex. You need to specify more than just "we accept images."

For each input type, also specify what happens when inputs violate constraints. Do you reject them with an error message? Do you attempt to process them with a warning? Do you automatically resize, truncate, or convert them? This error handling must be part of the input spec, not left to implementation discretion.

## Specifying System Context

System context is the invisible input that most teams forget to specify. But it's just as important as user inputs, because it changes how the system interprets everything the user provides.

For user profiles, specify: what profile fields the system has access to, what format they're in, which fields are guaranteed to be populated versus which might be null, and how often profile data is refreshed. If your system personalizes responses based on user role but doesn't specify what happens when role is null, you have a gap.

For session state, specify: what session information is available, how long sessions persist, whether the system can access cross-session history, and what happens when session data is stale or missing. If your system depends on "what the user did earlier in this session" but doesn't specify what counts as "this session," you'll get inconsistent behavior.

For account state, specify: what account data the system can access, what format it's in, whether it's real-time or cached, and how the system handles accounts in unusual states like suspended, trial, or transitioning between tiers. Many production failures happen because the system encounters an account state that wasn't anticipated during development.

For permissions, specify: how permissions are represented, what the default permissions are, how the system handles users with no permissions specified, and what happens when users attempt actions they're not permitted to perform. Don't assume permissions are always clear-cut. They're often messy, and you need to specify how to handle ambiguity.

For locale and language, specify: what locale information is available, what the default locale is when none is specified, whether the system auto-detects language or uses a preference, and how the system handles mismatches between user language and available content language. The bank's system failed when users switched to Spanish because it never specified how to handle language switching.

System context is input you don't see in demos or test cases, but it's critical in production. Specify it completely, or accept that you'll discover gaps when real users with real profiles and real account states start using your system.

## Specifying Retrieved Data

Retrieved data is tricky because it's dynamic. The system doesn't know what it will retrieve until it retrieves it. But you can—and must—specify the interface, the format, and the expected characteristics.

For knowledge base retrieval, specify: how many documents the system retrieves per query, what fields each document contains such as title, body, source, timestamp, what the maximum length of retrieved content is, whether retrieval results include relevance scores or rankings, and what happens when retrieval returns zero results. Don't just say "the system retrieves documents." Specify what those documents look like.

For database queries, specify: what tables or collections the system can query, what fields are guaranteed to be present, what data types those fields have, what the expected cardinality is such as one record, many records, or possibly none, and how the system handles missing or malformed data. Database schemas drift over time. Your input spec needs to define the contract so you notice when the schema changes in a way that breaks your assumptions.

For API calls, specify: what endpoints the system calls, what request format is used, what response format is expected, what fields are required versus optional in responses, what the expected response time is, and how the system handles API errors, timeouts, or rate limits. External APIs are a common source of input variability. You need to specify both the happy path and the failure modes.

For real-time data, specify: what data sources are used, how fresh the data is, what happens when data is unavailable or stale, and how the system handles conflicting data from multiple sources. Real-time data is inherently variable. Your spec needs to account for that variability.

For each type of retrieved data, also specify the fallback behavior when retrieval fails. Does the system refuse to answer? Does it provide a best-effort response with a disclaimer? Does it escalate to a human? This belongs in the input spec because retrieval failures are a normal part of the input space.

## Specifying Conversation History

For single-turn systems, conversation history is not an issue. For multi-turn systems, it's one of the most complex parts of the input spec.

For previous messages, specify: how many previous turns the system considers, whether the system uses the full message history or a summary, what format the history is stored in, and whether the history includes only user messages, only system responses, or both. A common mistake is to say "the system uses conversation history" without specifying what that means in practice.

For context window management, specify: what happens when conversation history exceeds the model's context window, whether the system truncates, summarizes, or uses a sliding window, what the priority order is for what to keep versus what to discard, and whether the system preserves key information like user name, account details, or current task across context resets. Context window limits are a hard constraint. Your spec must define how you handle them.

For multi-session history, specify: whether the system remembers information across sessions, how long that memory persists, what triggers memory updates or resets, and how the system handles conflicts between current input and historical context. If your system learns preferences over time, the spec must define how those preferences are represented and applied.

For clarifications and corrections, specify: how the system interprets corrections like "no, I meant X," whether clarifications override previous statements or add to them, and how long corrections remain in effect. Users frequently clarify or correct themselves. If your spec doesn't account for that, your system will misinterpret them.

Conversation history is a form of state. The input spec must define how that state is represented, updated, and used. Without this, your multi-turn system will have inconsistent behavior that's nearly impossible to debug.

## Specifying Tool Outputs

Agentic systems that use tools have an additional input category: outputs from previous tool calls. These are inputs to the next step in the reasoning chain, and they need specification just like any other input.

For each tool your system can invoke, specify: what format the tool output takes, what fields are guaranteed versus optional, what the expected value ranges are, what error codes or messages the tool can return, and how the system handles tool failures or timeouts. Tools are not magic. They have interfaces, and those interfaces need formal specifications.

For multi-step workflows, specify: what information flows from one step to the next, how outputs from step N are transformed into inputs for step N+1, whether the system validates tool outputs before using them, and what happens when tool outputs are malformed or incomplete. Don't assume tool outputs are always clean. They often aren't.

For parallel tool calls, specify: how the system combines outputs from multiple tools, what happens when tools return conflicting information, whether all tools must succeed or if partial results are acceptable, and how the system prioritizes information when multiple sources are available. Parallel tool execution is common in modern agentic systems. Your spec needs to account for it.

## What Good Input Specs Look Like in Practice

Let me show you what a complete input specification looks like for a real system. Suppose you're building an expense report analyzer that helps finance teams audit employee expense submissions.

**User-provided inputs:**
- Expense report file: PDF or image formats JPG, PNG; maximum 20 pages or 10MB; must contain itemized expenses with dates, amounts, categories, and vendor names
- User query: text input, 10 to 500 characters, plain text only, English language
- Date range filter: optional, ISO 8601 format, start date and end date, defaults to current month if not specified

**System context:**
- User role: employee, manager, or finance admin, determines what reports they can access and what actions they can take
- Company expense policy: JSON object with policy rules such as per diem limits, allowable categories, receipt requirements
- User's previous submissions: array of past expense reports for comparison and anomaly detection

**Retrieved data:**
- Vendor database: lookup of known vendors to validate merchant names, includes address, category, and risk flags
- Exchange rates: real-time currency conversion rates for international expenses, fetched from external API with 1-hour cache
- Historical averages: aggregated expense data by category and location for anomaly detection

**Conversation history:**
- Previous 5 turns in the conversation, including user questions and system responses
- Persistent context: which expense report is currently being analyzed, which line items have been flagged or approved

**Tool outputs:**
- OCR tool: extracted text from uploaded PDF or image, includes confidence scores per field, returns structured data with line items or error message if OCR fails
- Policy checker tool: compliance results per line item, includes pass/fail status, violated rule if applicable, suggested correction

This is a complete input specification. Someone could implement the system from this. Someone could write test cases. Someone could build the frontend and backend integration. There's no ambiguity about what the system receives or in what format.

## Why Most Teams Underspecify Inputs

Why do so many teams skip detailed input specs? Because it feels like unnecessary work. The model seems to handle anything you throw at it. Why constrain inputs when the model is flexible?

Because production is not your demo environment. In demos, you control the inputs. You use clean test cases. You avoid edge cases. In production, users do whatever they want. They push boundaries you didn't know existed.

The bank's customer service system worked perfectly in demos because the team tested with simple queries and small documents. They never tested 80-page PDFs or multilingual input or users who paste entire web pages into the chat box. When those inputs hit production, the system broke.

Input specs are not about constraining what the model can do. They're about defining the boundaries of your system so you can validate inputs, reject invalid inputs gracefully, test systematically, and debug effectively when things go wrong.

The teams that skip input specs end up building reactive systems that handle whatever shows up and fail unpredictably. The teams that write detailed input specs build robust systems that handle expected inputs correctly and reject invalid inputs cleanly.

## What You Should Specify Before You Build

Before you write a single line of code, sit down with your team and answer these questions:

What exactly will users provide as input? In what formats? With what constraints?

What system context will be available? Where does it come from? What happens when it's missing?

What data will the system retrieve? From where? In what format? How will it handle retrieval failures?

What conversation history will the system maintain? For how long? In what format?

What tools will the system call? What outputs will those tools produce? How will the system handle tool failures?

Write down the answers. Get agreement from product, engineering, and data science. Turn the answers into a formal input specification with formats, types, constraints, and failure modes.

Then build your system to match the spec. Validate inputs at the boundary. Reject invalid inputs with clear error messages. Test with inputs that match the spec and inputs that violate it.

This feels like extra work at the beginning. It saves you weeks or months of debugging later. The bank's system took seven weeks to stabilize after the tax season failures because they had to reverse-engineer what the input space actually was and retrofit validation everywhere. If they'd spent three days writing an input spec up front, they'd have avoided seven weeks of firefighting.

In the next section, we'll dive into the dark side of input specification: the long tail of edge cases, variability, and adversarial inputs that your system will encounter in production and how to prepare for them.

