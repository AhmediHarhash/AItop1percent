# 6.5 — Priority Ordering: Which Tasks to Evaluate First

In late 2023, a team building a financial advisory chatbot had twelve tasks in their registry. They wanted comprehensive evaluation coverage before launch, so they started building eval infrastructure for all twelve tasks simultaneously. Four months later, they had partial coverage on most tasks and complete coverage on none. The launch date arrived. They shipped with incomplete evaluation and immediately discovered critical failures in high-impact tasks. A task that recommended investment strategies—which ran thousands of times per day and could cost users money if wrong—had only five test cases. A task that formatted output text—which was low-risk and purely cosmetic—had eighty test cases because it was easy to test.

They had allocated evaluation resources by ease rather than importance. This is a common failure mode. Without explicit priority ordering, teams evaluate what is convenient instead of what matters.

Priority ordering is how you decide which tasks to evaluate first when you cannot evaluate everything at once. It is a forcing function that makes you confront trade-offs explicitly instead of letting them emerge accidentally.

## The Priority Matrix: Impact Times Frequency Times Difficulty

The core framework for priority ordering is a three-dimensional assessment: impact, frequency, and difficulty. Each task in your registry gets scored on all three dimensions, and the combination determines priority.

Impact measures the consequence of task failure. What happens if this task produces wrong outputs? The scale is typically: low impact means minor user inconvenience or cosmetic issues, medium impact means significant user frustration or workflow disruption, high impact means financial loss, safety risk, legal liability, or severe user harm.

For a financial advisory chatbot, a task that recommends specific investments has high impact because wrong recommendations could lose users money. A task that suggests conversation topics has low impact because wrong suggestions are just mildly annoying. A task that answers questions about account features has medium impact because wrong answers cause confusion and support burden but no financial harm.

Impact is not about how often the failure will happen. It is about how bad it is when it does happen. A task that fails rarely but catastrophically is high impact. A task that fails often but harmlessly is low impact.

Frequency measures how often the task executes. The scale is: low frequency means dozens to hundreds of times per month, medium frequency means thousands to tens of thousands of times per month, high frequency means hundreds of thousands or millions of times per month.

For the same chatbot, a task that handles onboarding questions might run thousands of times per month (medium frequency). A task that provides daily portfolio summaries might run millions of times per month (high frequency). A task that helps with tax planning might run hundreds of times per month during tax season but rarely otherwise (low frequency, with seasonal spikes).

Frequency matters because high-frequency tasks affect more users. A bug that affects a low-frequency task might impact a few dozen users. A bug in a high-frequency task impacts thousands.

Difficulty measures how hard the task is to get right. The scale is: easy means straightforward rules or patterns with clear correctness criteria, medium means some ambiguity or judgment required but mostly deterministic, hard means substantial ambiguity, requiring complex reasoning or domain expertise.

For the chatbot, a task that extracts account numbers from text is easy—there is a clear format and unambiguous correctness. A task that classifies user intent into predefined categories is medium—there are some boundary cases but mostly clear. A task that provides personalized investment advice is hard—it requires understanding user context, risk tolerance, financial goals, and market conditions.

Difficulty shapes how much effort evaluation requires. Easy tasks can be evaluated with simple automated checks. Hard tasks need extensive test coverage, expert review, and ongoing validation.

## The Priority Calculation

Once you score each task on impact, frequency, and difficulty, you combine them to determine priority.

High impact, high frequency, high difficulty: evaluate first. These tasks are the most critical. They run constantly, have serious consequences when they fail, and are hard to get right. They need comprehensive evaluation before launch and continuous monitoring after launch. For the financial chatbot, personalized investment recommendations fit this profile. They need extensive test coverage, expert validation, and production monitoring.

High impact, high frequency, low difficulty: evaluate early but with simpler methods. These tasks are critical and common but not complex. You can use automated checks and lightweight validation. For the chatbot, extracting account information might fit here. It is important and frequent, but you can validate it with exact-match tests against labeled examples.

High impact, low frequency, high difficulty: evaluate thoroughly despite low volume. These tasks can cause serious harm even if they run rarely. They need comprehensive test coverage and expert review. For the chatbot, tax planning advice might fit here. It only runs seasonally, but wrong advice could have legal and financial consequences for users.

Low impact, high frequency, low difficulty: evaluate with automated checks. These tasks are common but not risky or complex. Set up simple regression tests and monitor for obvious failures, but do not invest heavily in comprehensive evaluation. For the chatbot, conversation pleasantries like "How can I help you today?" fit here. They run constantly but failures are merely awkward, not harmful.

Low impact, low frequency, any difficulty: evaluate last, or accept risk and skip evaluation. These tasks do not affect many users and do not cause serious harm when they fail. If you have unlimited resources, evaluate them. If not, focus elsewhere. For the chatbot, holiday-themed greeting messages fit here. They are low-stakes and rare.

The priority matrix forces you to make trade-offs explicit. You cannot evaluate everything deeply. Where do you invest your limited evaluation resources?

## The Critical Path Principle

The priority matrix gives you a general framework, but there is an override: tasks on the critical user journey get evaluated first regardless of their individual scores.

The critical path is the sequence of tasks users must complete to accomplish their primary goal. For the financial chatbot, the critical path might be: authenticate user, retrieve account information, understand user question, generate appropriate response, format and return response. If any task on this path fails, the core user experience breaks.

Critical path tasks get priority even if they are individually low-risk. A task that retrieves account information might be technically easy and have low impact if it fails occasionally. But if it is on the critical path and a failure blocks the entire interaction, it needs solid evaluation.

This is different from the impact-frequency-difficulty calculation. A critical path task with medium impact and medium frequency still gets evaluated before a non-critical task with high impact and low frequency. The critical path is what users encounter every time they use the system. Non-critical features are optional or peripheral.

Identify the critical path during problem framing. Map out the task sequence for core user journeys. Mark which tasks are mandatory for completing those journeys. These tasks form the evaluation backbone.

## The Safety First Principle

Another override: any task that could cause harm gets evaluated before any task that merely causes inconvenience, regardless of frequency or difficulty.

Harm includes financial loss, physical danger, privacy violations, discrimination, emotional distress, or reputational damage. If a task could produce outputs that harm users, it needs thorough evaluation even if it is rare or simple.

For a medical AI, a task that suggests diagnosis or treatment has harm potential and gets evaluated first. For a content moderation AI, a task that decides whether to remove content has harm potential—both false positives (removing legitimate content) and false negatives (leaving harmful content) can cause harm.

The safety first principle trumps convenience. You do not get to ship a harm-capable task with light evaluation just because it is hard to evaluate. You either invest in proper evaluation or you do not ship the task.

This principle prevents the "we shipped it because we could build it" problem. Just because a feature is technically feasible does not mean it is safe. Evaluation is where you verify safety. If you cannot evaluate it safely, you cannot ship it safely.

## Pragmatic Ordering: Build Momentum with Quick Wins

While the priority matrix and override principles tell you what should be evaluated first from a product perspective, there is also a pragmatic consideration: build momentum by getting quick wins early.

If you have two tasks with similar priority, evaluate the one that is easier to evaluate first. This has strategic value. It demonstrates progress, builds confidence in your evaluation infrastructure, and establishes patterns other team members can follow.

For tasks with automated metrics—extraction, classification, simple transformations—you can build evaluation harnesses quickly. Run these first. Show that the evaluation process works. Catch bugs. Report results. This builds trust and proves value.

For tasks that require human evaluation—generation, reasoning, complex judgment—the ramp-up time is longer. You need to recruit raters, write guidelines, run calibration, collect ratings, analyze results. Starting these in parallel with quick-win tasks is smart, but do not wait for them to complete before showing any evaluation results.

Quick wins also reveal infrastructure problems early. If your evaluation harness has bugs or your test data has quality issues, you want to discover that on a simple task where debugging is easy, not on a complex task where many things could be wrong.

The pragmatic ordering principle: among tasks of similar priority, evaluate the easy ones first to build momentum and validate your evaluation infrastructure.

## The Evaluation Roadmap

Priority ordering produces an evaluation roadmap: a phased plan for building out eval coverage, task by task, with timelines tied to the product roadmap.

The roadmap has phases. Phase one is pre-launch: evaluate the critical path and highest-risk tasks before shipping anything. Phase two is post-launch stabilization: expand coverage to medium-priority tasks while monitoring production for failures in launched tasks. Phase three is comprehensive coverage: fill gaps in low-priority tasks and refine evaluation for tasks that have been running.

For each task in each phase, specify: what evaluation strategy will be used, what ground truth is needed, what metrics will be tracked, who is responsible, and what timeline is realistic. This prevents the "we will evaluate everything eventually" vagueness that leads to nothing getting evaluated properly.

Tie the roadmap to product milestones. If the product launch is in three months, phase one evaluation must complete before then. If a major feature addition is planned for month six, evaluation for that feature must be in the phase two timeline.

The roadmap also reveals resource constraints. If phase one requires 500 hours of expert review and you have one expert working half-time, that is twenty weeks of work. If your launch is in twelve weeks, you have a resource problem. The roadmap makes this visible so you can solve it: hire more experts, reduce scope, or extend the timeline.

Sharing the evaluation roadmap with leadership and stakeholders sets expectations. They see what will be evaluated when, what will not be evaluated until later, and what trade-offs have been made. This prevents surprises when someone asks "Have we tested X?" and the answer is "That is in phase three, not scheduled until month nine."

## Priority Ordering as Risk Management

Priority ordering is ultimately a risk management exercise. You are deciding where to accept risk and where to invest in risk reduction.

Evaluating a task reduces the risk that it will fail in production. But evaluation has costs: time, money, and opportunity cost. You cannot eliminate all risk, so you prioritize risk reduction where it matters most.

High-priority tasks get deep evaluation because the risk of failure is unacceptable. Low-priority tasks get light evaluation or no evaluation because the risk of failure is tolerable. The priority ordering framework makes these trade-offs explicit and defensible.

When something goes wrong in production, you can point to the priority ordering and explain why certain tasks were evaluated more thoroughly than others. If the failure happened in a low-priority task, the answer is: "We made a conscious decision to focus resources on higher-priority tasks. The risk-reward trade-off was appropriate." If the failure happened in a high-priority task despite thorough evaluation, the answer is: "We invested appropriately in evaluation, but some risk is irreducible. Here is how we will improve."

Without explicit priority ordering, you have no defense when someone asks why a task that failed in production was not evaluated more thoroughly. "We ran out of time" or "we did not think of it" are not satisfying answers. "We evaluated the critical path first, then high-impact tasks, and this was deprioritized based on low frequency and low harm potential" is a satisfying answer.

## Why Priority Ordering Prevents the Spread-Too-Thin Anti-Pattern

The most common evaluation failure is spreading resources too thin: trying to evaluate everything at once and evaluating nothing well. Teams build shallow test coverage across all tasks instead of deep coverage on critical tasks.

Shallow coverage feels productive. You have test cases for every task. You can report metrics for every task. But shallow coverage catches only obvious bugs. It misses edge cases, misses adversarial inputs, misses subtle quality degradation. It gives false confidence.

Deep coverage on critical tasks catches real problems. You have comprehensive test cases covering common and rare scenarios. You have adversarial tests. You have quality benchmarks. You have ongoing monitoring. When you ship, you actually know those tasks work.

Priority ordering forces depth over breadth. It says: evaluate these five tasks thoroughly before touching the other seven. Ship with confidence in the critical tasks and accept risk in the peripheral tasks. Over time, expand coverage. But start with depth where it matters.

This is psychologically hard for teams. It feels wrong to ship with gaps in test coverage. But shipping with shallow coverage everywhere is worse than shipping with deep coverage on critical tasks and no coverage on peripheral tasks. At least in the latter case, you know where the gaps are and can monitor those areas carefully in production.

Priority ordering makes the trade-off explicit and defensible. It is not "we did not have time to test everything." It is "we tested the critical path thoroughly and deprioritized low-impact, low-frequency tasks based on rational risk assessment."

The next step is understanding which tasks are inherently hard to build versus hard to evaluate, because that combination shapes where you invest resources. That is the task complexity matrix.
