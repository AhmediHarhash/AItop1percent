# 2.3 — Decomposing Retrieval-Augmented Systems

The legal tech startup had a brilliant idea: build an AI that could search through case law and answer legal research questions. They raised $3 million on the strength of the pitch.

Six months later, they had a working prototype. It used state-of-the-art embeddings, a vector database with millions of case documents, and a top-tier LLM for generation. The demos were impressive. Lawyers asked questions like "What's the standard for summary judgment in contract disputes?" and the AI returned detailed, well-cited answers.

They launched to 200 paying customers. Within two weeks, 60% had churned.

The feedback was brutal: "It finds the right cases but misses the key details." "It cites cases that aren't relevant to my jurisdiction." "It sometimes invents case law that doesn't exist." "I can't trust it because I don't know where the information came from."

What went wrong? The team had built retrieval-augmented generation as one monolithic system. They'd treated it like a single problem: put documents in, get answers out. They'd tuned it as a whole. When it failed, they couldn't debug it because they didn't know which part was broken.

Was retrieval finding the wrong documents? Was ranking putting the right documents in the wrong order? Was generation misinterpreting the retrieved content? Was citation failing to show sources? They had no idea because they'd never decomposed the system into its sub-problems.

That's the trap with RAG. It feels like one problem—augment generation with retrieval—but it's actually five distinct problems, each with different failure modes, different evaluation criteria, and different solutions.

## The Five Sub-Problems Hiding In RAG

Let me show you what's actually inside every retrieval-augmented system.

Sub-problem one: Query understanding. What is the user actually asking? This isn't just parsing the text. It's understanding intent, identifying the information need, and determining what kind of retrieval is required.

"What's the standard for summary judgment in contract disputes?" needs legal cases about summary judgment in contract law. "What's the summary judgment standard?" is ambiguous—standard for what type of case? Query understanding must detect this ambiguity and either clarify or make a reasonable assumption.

Sub-problem two: Retrieval. Finding relevant documents from your knowledge base. This includes: selecting the right retrieval method (semantic search, keyword search, hybrid), executing the search, and returning a candidate set of documents.

For the legal research example, retrieval must find cases that discuss summary judgment standards in contract disputes, not just cases that mention "summary judgment" or "contracts" separately.

Sub-problem three: Ranking and reranking. Ordering retrieved documents by relevance. Initial retrieval might return 100 documents. Ranking determines which ones are most relevant and should be sent to the LLM.

Some cases are directly on point. Others are tangentially related. Others are from non-binding jurisdictions. Ranking needs to prioritize correctly.

Sub-problem four: Synthesis. Generating an answer from the retrieved content. This is where the LLM reads the top-ranked documents and produces a natural language response that addresses the user's query.

The LLM must: extract relevant information from multiple documents, reconcile conflicting information, present a coherent answer, and avoid hallucinating details not in the retrieved content.

Sub-problem five: Citation and attribution. Proving where the answer came from. Users need to verify that the answer is grounded in real sources. This means: identifying which parts of the response came from which documents, providing specific citations, and enabling users to check the source material.

For legal research, this is non-negotiable. A lawyer can't use information they can't verify. Citation failures make the entire system useless.

These five sub-problems have different inputs, different outputs, different failure modes, and different ways to improve them. Treating them as one problem makes your system unmaintainable.

## Why Teams Treat RAG As One Problem

Here's why teams build monolithic RAG systems: the happy path looks simple.

You embed documents. You embed the query. You find nearest neighbors in vector space. You send the top results to an LLM with a prompt like "Answer this question using only the provided context." The LLM generates a response. Done.

In demos with cherry-picked queries and pre-selected documents, this works beautifully. The temptation is to ship it as-is.

But the happy path is 60% of cases. The other 40% are:

- Queries that are ambiguous and need clarification before retrieval
- Retrieval that returns too many marginally relevant results and buries the best one
- Ranking that prioritizes recency over relevance
- Synthesis that misinterprets a subtle detail in the retrieved content
- Citations that point to the wrong document or paragraph

When you treat RAG as one problem, you can't address these failures systematically. You end up tweaking the prompt, adjusting the embedding model, or changing the retrieval threshold—all at once, all without understanding which part is actually broken.

## Query Understanding: The First Sub-Problem

Let's decompose each sub-problem individually, starting with query understanding.

Most teams skip this entirely. They take the user's raw query and send it directly to retrieval. For simple, well-formed queries, that's fine. For real user queries, it's a disaster.

Real users ask:
- "Why was I charged?"—missing context about what charge
- "Same question but for California"—reference to a previous query
- "What's the latest guidance?"—ambiguous about latest guidance on what
- "Can you explain that in simpler terms?"—not a new question, a reformulation request

Each of these needs query understanding before retrieval.

"Why was I charged?" requires: detecting that context is missing, either asking the user for clarification or using conversation history to infer what charge they mean.

"Same question but for California" requires: identifying that this is a query modification, retrieving the previous query, and augmenting it with the jurisdiction constraint.

"What's the latest guidance?" requires: detecting the ambiguity and either asking for clarification or using context to infer the topic.

"Can you explain that in simpler terms?" requires: recognizing this isn't a new retrieval request, it's a synthesis modification—retrieve the same documents but generate a simpler response.

Query understanding determines whether you retrieve at all, what you retrieve, and how you retrieve it. If you get this wrong, everything downstream fails.

The legal tech startup failed here constantly. Users would ask: "What about New York?" The system would retrieve documents about New York without understanding that the user meant "apply the previous answer to New York law." It returned general New York legal information instead of New York summary judgment standards.

Query understanding should have detected the reference, retrieved the previous query context, and modified the retrieval query accordingly.

## Retrieval: The Second Sub-Problem

Retrieval seems simple: find documents similar to the query. It's not.

You need to decide: what retrieval method? Semantic search with embeddings finds conceptually similar content but misses exact keyword matches. Keyword search finds exact matches but misses paraphrased content. Hybrid search combines both but requires balancing the two signals.

For legal research, you need hybrid. Case law uses specific terminology that must match exactly—statutory citations, case names, legal standards. But it also discusses concepts that might be paraphrased—equity, good faith, materiality. Pure semantic search misses exact citations. Pure keyword search misses conceptual matches.

The team started with pure semantic search. It failed on queries like "cases citing Smith v. Jones." The embedding model generalized away the specific case name and returned cases about similar topics. Users wanted the exact case or cases that cited it. That requires keyword matching on the case name.

You also need to decide: what's the retrieval scope? Are you searching all documents or filtering first? For legal research, you might filter by jurisdiction, date range, court level, or practice area before searching.

A query about California contract law shouldn't retrieve cases from New York or federal courts, unless the user explicitly wants them. Retrieval should respect scope constraints.

The legal startup initially searched everything. That meant answers mixed binding precedent from California Supreme Court with non-binding persuasive authority from other states. Lawyers couldn't use that—they need to know what's actually binding.

Retrieval needs metadata filtering. Search within California state court cases, not across all cases.

## Ranking: The Third Sub-Problem

Retrieval returns candidates. Ranking determines which ones the LLM sees.

You might retrieve 100 documents. Your LLM context window fits 10. Which 10? The answer determines response quality.

Initial ranking is usually by similarity score—cosine similarity for embeddings, BM25 score for keyword search. That's a starting point, but it's often wrong.

Similarity measures relevance in embedding space, not actual utility for answering the query. A document might be semantically similar but not useful—it discusses the topic but doesn't answer the question.

Reranking addresses this. You take the top 50 documents from initial retrieval and rerank them using a model trained specifically for relevance, or using signals like: recency, authority, completeness, specificity.

For legal research, reranking should prioritize:
- Binding authority over persuasive authority
- Recent cases over old cases (unless specifically researching historical standards)
- Cases directly on point over cases discussing related issues
- Majority opinions over dissents or concurrences

The legal startup's ranking was purely by embedding similarity. That surfaced old, overruled cases as often as current binding precedent. It ranked dissenting opinions equal to majority opinions. Users got answers that were plausible but legally incorrect.

Reranking with legal-specific signals would have caught this. Weight cases from higher courts more. Weight recent cases more unless the query is historical. Exclude dissents unless the query asks for them.

## Synthesis: The Fourth Sub-Problem

Synthesis is where the LLM reads retrieved documents and generates a response.

This is the most visible part of RAG, so teams focus on it. But synthesis quality depends entirely on what the previous sub-problems provided. If retrieval found the wrong documents, if ranking prioritized the wrong ones, synthesis can't fix it.

Assuming retrieval and ranking worked, synthesis has its own challenges.

Challenge one: Reconciling conflicting information. Different documents might give different answers. The LLM must: recognize the conflict, explain it, and ideally tell the user why the conflict exists.

For legal research, two cases might state different standards for summary judgment. One is older, one is newer. The LLM should say: "The standard was X under Case A, but was updated to Y under Case B, which supersedes Case A."

The startup's synthesis would just pick one or blend them incoherently. It didn't recognize temporal precedence or hierarchical authority.

Challenge two: Staying grounded. LLMs love to elaborate beyond the provided context. They'll take a fact from the documents and expand it with plausible-sounding details that aren't actually there.

For legal research, this is catastrophic. Inventing details about a case means citing non-existent law. Lawyers who rely on that could face sanctions.

Synthesis must be ruthlessly grounded. If it's not in the retrieved documents, don't say it. If you're uncertain, flag it.

Challenge three: Handling insufficient information. Sometimes the retrieved documents don't contain enough information to answer the query fully. The LLM must recognize this and say: "Based on the available documents, I can tell you X, but I don't have information about Y."

The startup's synthesis would hallucinate when information was missing. Instead of saying "I don't have enough information about summary judgment in arbitration cases," it would generate a plausible-sounding answer based on general summary judgment standards, which might not apply to arbitration.

## Citation: The Fifth Sub-Problem

Citation is the most neglected sub-problem in RAG, and it's the one that makes or breaks trust.

Users need to know: where did this information come from? Not vaguely—specifically. Which document? Which section? Which paragraph?

There are multiple citation strategies:

Strategy one: Inline citations. As you generate each sentence, cite the source. "Summary judgment is appropriate when there's no genuine dispute of material fact (Anderson v. Liberty Lobby, 477 U.S. 242)."

Strategy two: End-of-response citations. List all sources at the end. "Sources: Anderson v. Liberty Lobby, 477 U.S. 242; Celotex Corp. v. Catrett, 477 U.S. 317."

Strategy three: Chunk-level citations. For each chunk of retrieved content, show which part of the response came from it.

The right strategy depends on use case. For legal research, inline citations are mandatory—lawyers need to verify each specific claim.

The legal startup initially had no citations at all. They added them later as an afterthought: a list of retrieved documents at the bottom of the response. Useless. Users couldn't tell which document supported which claim.

When they switched to inline citations, a new problem emerged: attribution errors. The LLM would cite the wrong case for a claim. It would attribute a standard from Case A to Case B. Even when the information was correct, the citation was wrong.

This is a specific failure mode of the citation sub-problem. The LLM generates text and attributes sources based on approximate recall, not precise tracking. To fix it, you need structured citation: track which chunks were used for which parts of the response, enforce that citations only reference documents that were actually retrieved, validate citations against the source material.

## Why Decomposition Matters For Debugging

Here's the payoff of decomposition: when something fails, you know where to look.

A user reports: "It gave me an answer about New York law but I asked about California." You debug:

Is it a query understanding failure? Check: did we correctly extract "California" as a jurisdiction constraint?

Is it a retrieval failure? Check: did we filter retrieval to California cases or search everything?

Is it a ranking failure? Check: did we retrieve California cases but rank New York cases higher?

Is it a synthesis failure? Check: did we retrieve the right California cases but the LLM pulled in out-of-context knowledge about New York?

Each sub-problem has logs, metrics, and evaluation criteria. You check each one systematically. You find: retrieval didn't apply the jurisdiction filter. Now you know exactly what to fix.

Without decomposition, you just see: "wrong answer." You don't know if it's retrieval, ranking, synthesis, or something else. You guess. You tweak prompts. You hope it improves. It usually doesn't.

## Sub-Problems Have Different Evaluation Criteria

Here's another reason decomposition matters: each sub-problem needs different metrics.

Query understanding: Measure accuracy of intent extraction, entity recognition, and ambiguity detection. Did we correctly identify what the user wanted to retrieve?

Retrieval: Measure recall and precision. Did we retrieve the relevant documents? Did we avoid irrelevant ones?

Ranking: Measure whether the most relevant documents are at the top. Use metrics like NDCG (Normalized Discounted Cumulative Gain) that reward putting highly relevant documents early.

Synthesis: Measure groundedness (is the response supported by retrieved content?), completeness (did it address the query?), and coherence (is it well-written?).

Citation: Measure citation accuracy (are sources correctly attributed?), citation coverage (is every claim cited?), and citation precision (do citations point to the specific relevant content?).

If you treat RAG as one system, you measure end-to-end accuracy: was the final response good? That's useful but insufficient. When accuracy is low, you don't know which sub-problem to fix.

If you decompose, you measure each sub-problem independently. You discover: retrieval recall is 95%, ranking is 78%, synthesis groundedness is 88%, citation accuracy is 62%. Now you know: focus on citation and ranking. Don't waste time improving retrieval—it's already working well.

## Chunking Strategy As A Sub-Problem

Let me add a sixth sub-problem that's often overlooked: chunking strategy.

Before retrieval, you need to split documents into chunks. How you chunk determines what retrieval finds and what the LLM reads.

Chunk too small (e.g., 100 tokens), and you fragment context. A legal standard might be explained across three paragraphs. If you chunk by paragraph, you retrieve fragment one but miss the rest. The LLM sees incomplete information.

Chunk too large (e.g., 2,000 tokens), and you dilute relevance. A document section might discuss five topics. Only one is relevant to the query. But you retrieve the whole section. The LLM has to sift through noise to find the signal.

Chunk by fixed size, and you break mid-sentence or mid-thought. Chunk by semantic boundaries (paragraphs, sections), and you respect document structure but might create uneven chunk sizes.

For legal cases, the right chunking strategy is: preserve logical units. A case has headnotes, facts, analysis, holding, and dissents. Chunk by these sections. Don't split a section mid-paragraph.

The legal startup initially chunked by 500-token windows with 100-token overlap. It broke mid-sentence constantly. Retrieval would return a chunk that started halfway through explaining a legal test, making it incomprehensible.

They switched to section-based chunking. Each case section became a chunk. Retrieval now returned coherent units that the LLM could actually use.

Chunking isn't part of query understanding, retrieval, ranking, synthesis, or citation. It's its own sub-problem with its own trade-offs and evaluation criteria.

## Embedding Quality As A Sub-Problem

Seventh sub-problem: embedding quality.

Retrieval depends on embeddings—vector representations of text. If embeddings don't capture the semantic meaning of legal concepts, retrieval fails.

Generic embeddings trained on web text don't understand legal nuance. "Material fact" in law has a specific meaning different from "material" in everyday use. Generic embeddings might conflate them.

Legal-specific embeddings capture this nuance. They're trained on legal text, so they understand legal terminology, case citations, and legal reasoning patterns.

The legal startup started with OpenAI's general embeddings. They worked okay for common legal concepts but failed on specialized areas: tax law, patent law, securities regulation. The domain-specific vocabulary wasn't well represented.

They switched to a legal-specific embedding model. Retrieval quality jumped 20% on specialized queries.

Embedding quality is a sub-problem you can optimize independently: try different embedding models, fine-tune embeddings on your domain, or use multiple embeddings and ensemble the results.

## Context Window Management As A Sub-Problem

Eighth sub-problem: context window management.

Your LLM has a finite context window. You can't send all retrieved documents. You need to decide: which content goes in? How much? In what order?

Simple strategy: take the top N chunks by ranking score, concatenate them, send to LLM. This works until you hit context limits or until the top N isn't diverse enough.

Advanced strategy: select chunks that maximize coverage of the query while minimizing redundancy. If three chunks say the same thing, include one. If chunks cover different aspects of the query, include all.

For legal research, context window management also needs to prioritize structure. Send the case holding before the facts. Send majority opinions before concurrences. The order matters for how the LLM interprets and synthesizes.

The legal startup sent chunks in retrieval score order. Often that meant reading case facts first, then the holding. Legally backwards. Holdings are the law. Facts are just the context.

They restructured: send holdings first, analysis second, facts third. Synthesis improved because the LLM now understood the legal rule before seeing the specific factual application.

## Hybrid Search: Combining Semantic and Keyword

Ninth sub-problem: hybrid search strategy.

Pure semantic search misses exact matches. Pure keyword search misses paraphrased concepts. Hybrid combines both, but how?

Strategy one: Retrieve with both methods, merge results, deduplicate.

Strategy two: Retrieve with both methods, ensemble the scores (e.g., weighted average of semantic similarity and BM25 score).

Strategy three: Use one method as primary, the other as fallback. Try semantic first; if confidence is low, fall back to keyword.

Each strategy has trade-offs. Merging is simple but doesn't prioritize. Ensembling requires tuning weights. Fallback adds latency.

For legal research, the right approach is ensembling with legal-specific weights. Exact case citations and statutory references get high keyword weight. Conceptual legal questions get high semantic weight.

The startup initially just concatenated semantic and keyword results. Duplicates appeared. Ranking was inconsistent. They switched to weighted ensembling: 70% semantic, 30% keyword. Tuned based on query type: citation queries flip to 70% keyword.

## When To Build Sub-Problems Separately

You might be thinking: do I really need to build nine separate components?

Not necessarily. Decomposition is conceptual before it's architectural. You can implement multiple sub-problems in one function. The key is: understand them as distinct problems so you can evaluate and improve them independently.

But some sub-problems benefit from separate implementation:

Query understanding often becomes a separate service because it needs to handle conversation context, user history, and clarification dialogues.

Reranking often becomes a separate model because the reranking signals are different from retrieval signals.

Citation often becomes post-processing because it requires parsing the generated response and matching it back to source chunks.

The legal startup ended up with: a query understanding module, a retrieval service (handling both semantic and keyword), a reranking model, a synthesis prompt, and a citation validator. Five components, each testable independently.

When synthesis quality was low, they could test: is reranking sending the right documents? Yes. Is synthesis using them correctly? No. Then they knew to focus on the synthesis prompt, not the retrieval pipeline.

## The 2026 Context: Advanced RAG Patterns

In 2026, RAG has evolved beyond single-shot retrieval. Advanced patterns add even more sub-problems.

Iterative retrieval: The LLM reads initial documents, identifies information gaps, issues a second retrieval query, reads those documents, and synthesizes. Now you need: gap detection as a sub-problem, query reformulation as a sub-problem, and multi-round synthesis.

Query decomposition: Break a complex query into sub-queries, retrieve for each, synthesize partial answers, combine them. Sub-problems: query decomposition logic, sub-query routing, partial answer integration.

Multi-hop reasoning: Answer a query that requires information from multiple sources in sequence. "What was the summary judgment standard in the jurisdiction where Smith v. Jones was decided?" First retrieve Smith v. Jones to find the jurisdiction, then retrieve summary judgment standards for that jurisdiction. Sub-problems: hop detection, dependency tracking, sequential retrieval coordination.

Each advanced pattern adds sub-problems. Teams that haven't decomposed basic RAG can't even approach these patterns.

## What You Should Do Differently

If you're building a RAG system, decompose it into at least the five core sub-problems: query understanding, retrieval, ranking, synthesis, citation.

For each sub-problem, define: What are the inputs? What are the outputs? What does success look like? How do we measure it?

Build evaluation datasets for each sub-problem. Don't just evaluate end-to-end. Test retrieval recall separately. Test ranking quality separately. Test synthesis groundedness separately.

When something fails, debug by sub-problem. Trace through: query understanding logs, retrieval results, ranking scores, synthesis inputs, citation outputs. Find the failure point.

Optimize incrementally. If ranking is the bottleneck, improve ranking. Don't rewrite the whole pipeline.

If you're already in production with a monolithic RAG system, consider refactoring. Add logging at sub-problem boundaries. Add metrics for each component. Gradually separate concerns so you can evaluate and improve independently.

The teams that win with RAG aren't the ones using the fanciest embedding models or the biggest LLMs. They're the ones who decomposed the problem, understood each piece, and built a system they can actually debug and improve.

RAG looks simple from the outside. It's not. It's at least five sub-problems, each with distinct challenges. Treat it as one problem, and you'll ship a system you can't maintain. Decompose it properly, and you'll build something reliable.

In the next section, we'll tackle the most complex decomposition challenge of all: agentic workflows, where AI systems don't just respond to queries—they make decisions, use tools, and chain multi-step actions together.
