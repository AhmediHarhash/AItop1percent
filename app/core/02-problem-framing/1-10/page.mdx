# 1.10 — Constraint Capture: Latency, Cost, Privacy, Compliance, UX

In early 2025, a startup built a beautiful AI-powered customer support chatbot. The product worked flawlessly in demos. The model was accurate, the UX was clean, and the stakeholders were thrilled. Two weeks before launch, the finance team ran the numbers. At the expected query volume, the system would cost $47,000 per month in API calls — against a budget of $8,000. The team had never documented a cost constraint. They had to rearchitect the entire system to use a smaller model with aggressive caching, which delayed launch by ten weeks and introduced quality regressions they spent another month fixing. The failure was not technical. The failure was that cost was treated as an implementation detail instead of a framing constraint.

Most framing documents describe what the system should **do** — the tasks it solves, the outcomes it delivers, the metrics it optimizes. Very few framing documents describe what the system **cannot do** or **must respect**. These are constraints: the boundaries that shape every downstream decision about architecture, model selection, data handling, and deployment. If you do not capture constraints during framing, you will discover them later — when they are much more expensive to accommodate.

This subchapter covers the five constraint categories that every AI framing document must address in 2026: latency, cost, privacy, compliance, and user experience. For each category, we will define the constraint, explain why it matters, show how to document it, and explore what happens when constraints conflict.

## Latency: Response Time Budgets by Use Case

Latency is the time between when a user submits a request and when they receive a response. In classical software, latency is a performance metric. In AI systems, latency is a **product constraint** because it directly determines what models you can use, what infrastructure you need, and whether the product is even viable.

Latency requirements vary by use case. A voice assistant that takes five seconds to respond feels broken, even if the answer is perfect. A batch analytics job that takes five hours is fine. The mistake most teams make is setting a single latency target for the entire system. You need **latency budgets by task type**.

Here are the latency categories for AI systems in 2026:

**Voice and real-time interaction**: under 300 milliseconds. This is the threshold where conversation feels natural. If your system powers a voice assistant, a live coding tool, or a real-time customer support chat, you must hit this budget. Anything slower and users perceive lag. At this latency, you are limited to small models (under 10 billion parameters), edge deployment, or aggressive caching.

**Chatbots and async interaction**: under 3 seconds. This is the threshold where users stay engaged. If your system powers a Slack bot, a web-based support assistant, or an email draft tool, this is your budget. Users will tolerate a short wait if the response is high-quality. At this latency, you can use mid-size models (10 to 70 billion parameters) with API-based inference.

**Summarization and background tasks**: under 30 seconds. This is the threshold where users accept that the system is doing work. If your system summarizes a meeting, generates a report, or analyzes a document, users expect to wait. At this latency, you can use large models (70 billion parameters and above) or run multi-step pipelines.

**Batch and offline tasks**: unlimited. This is for systems that run on a schedule or process data asynchronously. If your system powers overnight data pipelines, model training, or periodic audits, latency is not a constraint. You optimize for cost and quality instead.

How do you document latency constraints? Do not just write "the system should be fast." Write explicit budgets with justification. For example:

"Task type: Customer support chat. Latency budget: under 3 seconds for 95% of queries. Justification: User research shows that users abandon chat sessions after 5 seconds of waiting. We target 3 seconds to stay comfortably below that threshold."

"Task type: Voice assistant. Latency budget: under 300 milliseconds for 99% of queries. Justification: Voice interaction requires sub-second response to feel natural. We target 300ms to account for network latency and audio processing time."

The key is to tie latency budgets to **user expectations** rather than arbitrary technical goals. If you cannot explain why your latency budget matters to the user, it is not a real constraint.

## Cost: Cost-Per-Query Targets by Tier

Cost is the variable expense of running the AI system per request. In 2026, cost is dominated by inference: the price of calling an API, running a self-hosted model, or processing embeddings. Cost is often the constraint that kills AI projects because teams underestimate it during framing and discover the economics do not work when they scale.

Cost constraints must be documented **per query** and **per user tier** if your product has multiple pricing levels. Here is why: if you are building a free tier, your cost per query might need to be under $0.001 to be profitable. If you are building an enterprise tier that charges $50 per user per month, you can afford $0.10 per query. The model you choose and the architecture you build will be completely different for these two scenarios.

Here is how to frame cost constraints:

**Identify your cost sources**: API calls to third-party models, self-hosted inference, embeddings generation, vector database queries, data storage, egress fees. Most teams only think about model inference costs and miss the rest.

**Set a cost-per-query target**: Based on your business model, what can you afford to spend per request? If your product is ad-supported, your target might be fractions of a cent. If your product is B2B SaaS, your target might be several cents. If your product is enterprise-only, your target might be dollars.

**Break down by task type**: Just like latency, cost varies by task. A simple classification task might cost $0.0001. A multi-step agentic workflow might cost $0.50. Document the budget for each task type.

**Include token budgets**: In 2026, most model APIs charge per token. Your framing should specify token limits for prompts and responses. For example: "Maximum prompt length: 2,000 tokens. Maximum response length: 500 tokens. Justification: At $0.03 per 1,000 tokens, this keeps cost per query under $0.075."

**Plan for scale**: Cost constraints are not just about today. If your system handles 100 queries per day, cost is not a concern. If it handles 100 million queries per day, cost becomes existential. Document your expected query volume and calculate total monthly cost. If the number is higher than your budget, you need to reframe.

**Include API call limits**: Many teams forget that third-party APIs have rate limits and quota caps. If your framing assumes you will call GPT-4 Turbo ten million times per day, you need to confirm that OpenAI will actually sell you that much capacity. Document API limits as constraints.

Example cost constraint:

"Cost-per-query target: under $0.02 for free tier users, under $0.20 for enterprise tier users. Expected query volume: 5 million per month in year one. Total monthly budget: $100,000. Cost breakdown: $0.015 per query for model inference, $0.003 per query for embeddings, $0.002 per query for vector DB lookups. Token budgets: 1,500 prompt tokens, 300 response tokens."

When cost constraints conflict with quality or latency, you have a trade-off that must be escalated. Do not silently relax cost limits without stakeholder approval.

## Privacy: PII Handling, Data Residency, GDPR, CCPA, Consent

Privacy constraints govern what data you can collect, where you can store it, who you can share it with, and how long you can keep it. In 2026, privacy is not optional — it is legally enforced across most markets. The EU has GDPR, California has CCPA, and dozens of other jurisdictions have similar laws. If your AI system processes personal data, you have privacy constraints whether you documented them or not. The question is whether you discover them during framing or during a lawsuit.

Here are the privacy constraints you must document:

**PII handling**: What types of personally identifiable information will your system process? Names, email addresses, phone numbers, IP addresses, device IDs, location data, biometric data, health data, financial data? For each type, document whether you collect it, how you use it, where you store it, and how long you retain it. If you send PII to a third-party API like OpenAI or Anthropic, you need to understand their data policies. As of 2026, most major API providers offer zero-retention options for enterprise customers, but you must enable them explicitly.

**Data residency**: Where will your data be stored? Some industries and jurisdictions require that data stay within specific geographic boundaries. If you are building a healthcare app in the EU, patient data must stay in the EU. If you are building a financial app in China, data must stay in China. Document data residency requirements and ensure your infrastructure complies.

**GDPR compliance**: If you serve EU users, you must comply with GDPR. This means: obtaining consent before collecting personal data, allowing users to access their data, allowing users to delete their data, allowing users to export their data, documenting your legal basis for processing, and appointing a data protection officer if you process data at scale. GDPR applies to training data, inference data, and logs. If your framing does not address GDPR, you will not be able to ship in the EU.

**CCPA compliance**: If you serve California users, you must comply with CCPA. This means: disclosing what personal data you collect, allowing users to opt out of data sales, allowing users to delete their data, and providing a "Do Not Sell My Personal Information" link. CCPA is less strict than GDPR but still mandatory.

**Consent requirements**: Do users need to opt in before you use their data for AI? In many jurisdictions, the answer is yes. Document what consent mechanisms you will implement and what users are consenting to. If your system uses user data for model fine-tuning, you need explicit consent for that.

**Anonymization and pseudonymization**: Can you anonymize data to reduce privacy risk? In some cases, yes. In other cases, anonymization is impossible because you need identifiers to function. Document your anonymization strategy if you have one.

Example privacy constraint:

"PII handling: The system will process user email addresses, names, and conversation history. Email and name will be stored in a GDPR-compliant database in the EU. Conversation history will be retained for 90 days, then automatically deleted. PII will NOT be sent to third-party APIs unless the user has opted in via explicit consent. Data residency: EU region only. GDPR compliance: Full compliance required, including right to access, right to deletion, right to export. CCPA compliance: Required for users in California. Consent: Users must opt in before their data is used for model improvement."

If legal or compliance has not reviewed your privacy constraints, you are taking on massive risk. Privacy violations can result in fines up to 4% of global revenue under GDPR.

## Compliance: EU AI Act 2026, SOC 2, HIPAA, Industry-Specific

Compliance constraints are the regulatory and certification requirements that govern how you build and deploy AI systems. In 2026, compliance is no longer a niche concern — it is mainstream. The EU AI Act went into effect in 2025 and is now fully enforceable. It classifies AI systems by risk level and imposes obligations on high-risk systems. If your AI system falls under a high-risk category, you must comply or you cannot operate in the EU.

Here are the compliance constraints you must document:

**EU AI Act classification**: Does your AI system fall under a prohibited, high-risk, limited-risk, or minimal-risk category? Prohibited systems cannot be deployed at all (for example, social scoring systems). High-risk systems require conformity assessments, risk management, data governance, transparency, human oversight, and robustness testing. High-risk categories include: biometric identification, critical infrastructure, education and employment, law enforcement, migration and border control, administration of justice, and certain financial and insurance systems. If your system is high-risk, compliance is not optional. Document your classification and the obligations it triggers.

**SOC 2**: If you sell to enterprise customers, many will require SOC 2 Type II certification. This is an audit of your security, availability, processing integrity, confidentiality, and privacy controls. SOC 2 does not directly regulate AI, but it does require that you document how your AI systems handle data and that you have controls to prevent unauthorized access or data breaches. Document whether SOC 2 is required and what controls you need to implement.

**HIPAA**: If your system processes protected health information in the United States, you must comply with HIPAA. This means: encrypting data at rest and in transit, logging access, signing business associate agreements with any third-party vendors, and conducting regular risk assessments. HIPAA applies to healthcare providers, payers, and their business associates. If your AI system analyzes patient data, diagnoses conditions, or recommends treatments, HIPAA compliance is mandatory. Document your HIPAA obligations and ensure your architecture supports them.

**Industry-specific regulations**: Many industries have AI-specific rules. Financial services have model risk management requirements under SR 11-7. Pharmaceuticals have FDA approval processes for AI-based medical devices. Automotive has safety standards for autonomous driving systems. Legal has rules around attorney-client privilege. Document any industry-specific regulations that apply to your system.

**Audit and explainability requirements**: Many compliance frameworks require that you can explain why your AI system made a decision. If you are using a black-box model, this can be a blocker. Document explainability requirements and how you will meet them (for example, using interpretable models, SHAP values, or human-in-the-loop workflows).

Example compliance constraint:

"EU AI Act: This system is classified as high-risk under Annex III category 5 (employment decisions). Compliance requirements: risk management system, data governance, technical documentation, record-keeping, transparency obligations, human oversight, accuracy and robustness testing. SOC 2: Type II certification required by Q4 2026. HIPAA: Not applicable. Industry-specific: Must comply with EEOC guidelines on AI in hiring. Explainability: Must provide human-readable explanations for all hiring recommendations."

If you do not involve legal and compliance during framing, you will discover these constraints when you try to launch — and they will force you to reframe.

## UX: Tone, Length, Format, Accessibility, Mobile vs Desktop

User experience constraints define how the AI system should interact with users. These are not technical constraints in the traditional sense, but they shape architecture just as much as latency or cost. A chatbot that must respond in under 50 words requires a different architecture than one that can write long-form essays. A voice assistant that must work offline requires edge deployment. A mobile app that must work on low-bandwidth connections requires aggressive compression.

Here are the UX constraints you must document:

**Tone and voice**: What personality should the AI system have? Formal or casual? Friendly or neutral? Empathetic or detached? Tone is not cosmetic — it is a constraint. If your brand voice is casual and conversational, you cannot use a stiff legal-sounding model. Document tone with examples.

**Response length**: How long should responses be? Some use cases require short, concise answers (for example, voice assistants, mobile notifications, live chat). Other use cases require long, detailed responses (for example, report generation, email drafting). Document expected length ranges. For example: "Responses should be 1-3 sentences for mobile chat, 1-2 paragraphs for desktop chat, 1-2 pages for report generation."

**Format**: What format should responses take? Plain text, markdown, JSON, HTML, bullet points, numbered lists? Format is an architectural constraint because it determines how you parse and render output. Document expected formats with examples.

**Accessibility**: Does your system need to comply with WCAG 2.1 or other accessibility standards? If your AI system generates web content, it must be accessible to users with disabilities. This means: proper heading structure, alt text for images, keyboard navigation, screen reader compatibility. Document accessibility requirements.

**Mobile vs desktop**: Will users access your system on mobile, desktop, or both? Mobile has different constraints: smaller screens, touch interfaces, limited bandwidth, background processing restrictions. If your system must work on mobile, document those constraints. For example: "Must support iOS 15+ and Android 12+. Must work on 3G connections with graceful degradation. Must render correctly on screens down to 375px wide."

**Multilingual support**: Does your system need to support multiple languages? If yes, which ones? Multilingual support is a major architectural decision because it affects model selection, data pipelines, and evaluation. Document language requirements during framing, not during implementation.

Example UX constraint:

"Tone: Friendly and professional, similar to Slack's brand voice. Response length: 1-3 sentences for chat, 1 paragraph for summaries, 1-2 pages for reports. Format: Markdown for chat, JSON for API integrations. Accessibility: WCAG 2.1 AA compliance required for all web interfaces. Mobile: Must support iOS 16+ and Android 13+, must render on 375px screens, must work on 4G connections. Multilingual: English and Spanish required for v1, French and German deferred to v2."

UX constraints are often soft — they can be negotiated with product and design. But once they are agreed, they become hard constraints that engineering must respect.

## The Constraint Matrix: Map Constraints to Tasks

Once you have documented all your constraints, you need to map them to your task taxonomy. Not all constraints apply to all tasks. A background batch job does not have latency constraints. A voice assistant does not have cost constraints if it is only used by internal employees. A classification task does not have UX tone constraints.

The constraint matrix is a table that maps each task to its active constraints. Here is an example:

| Task | Latency | Cost | Privacy | Compliance | UX |
|------|---------|------|---------|------------|-----|
| Customer support chat | Under 3s | Under $0.02 | No PII to API | GDPR, CCPA | Friendly tone, 1-3 sentences |
| Report generation | Under 30s | Under $0.50 | PII allowed internally | SOC 2 | Formal tone, 1-2 pages |
| Voice assistant | Under 300ms | Under $0.01 | No PII retention | GDPR | Casual tone, 1 sentence |
| Batch analytics | Unlimited | Under $5 per run | Anonymized only | HIPAA | N/A |

This matrix becomes a design reference for engineering. When they are choosing a model or building a pipeline, they look up the task and see exactly what constraints apply.

## What Happens When Constraints Conflict

The hardest part of constraint capture is resolving conflicts. What do you do when you cannot meet the latency budget without exceeding the cost budget? What do you do when privacy rules prevent you from using the data you need for quality? What do you do when compliance requires explainability but your best model is a black box?

The answer is: you escalate. Constraint conflicts are trade-off decisions, and trade-off decisions require stakeholder alignment. Do not silently relax a constraint and hope nobody notices. Document the conflict, present options, and get a decision from the right stakeholder.

For example: "Conflict: We cannot meet the 300ms latency budget for voice assistant while staying under the $0.01 cost-per-query limit. Options: (1) Relax latency to 500ms, which allows us to use a larger model and stay under cost. (2) Increase cost budget to $0.05, which allows us to use edge deployment and meet latency. (3) Reduce quality expectations and use a smaller model that meets both constraints but has higher error rates. Recommendation: Option 2. Justification: Voice latency is critical to UX, and users will not tolerate 500ms lag. We recommend increasing cost budget and offsetting with higher pricing."

Constraint conflicts are normal. The failure mode is not having them — the failure mode is discovering them after you have already built the system.

## The Constraint Negotiation Meeting

When you have documented all your constraints and identified conflicts, run a constraint negotiation meeting with stakeholders. The agenda is simple:

1. Present the full constraint matrix.
2. Highlight any conflicts or trade-offs.
3. For each conflict, present options and a recommendation.
4. Get stakeholder decisions on which constraints to relax or which budgets to increase.
5. Document final agreed constraints in the framing doc.

This meeting is not about debating whether constraints are valid — that was already decided by legal, compliance, and product. This meeting is about deciding what to do when reality does not allow you to meet all constraints simultaneously.

## Practical Takeaways

**Constraints are not nice-to-have.** They are mandatory sections of your framing document. If you do not document them, you will discover them later when they are much more expensive to accommodate.

**Break constraints by task type.** Do not set a single latency budget or cost budget for the entire system. Different tasks have different constraints.

**Tie constraints to user impact or legal requirements.** Every constraint should have a justification. If you cannot explain why the constraint matters, it is not a real constraint.

**Map constraints to tasks.** Use a constraint matrix to make explicit which constraints apply to which tasks.

**Escalate conflicts early.** Do not try to resolve trade-offs on your own. Get stakeholder alignment on what to prioritize.

**Involve legal and compliance early.** Privacy and regulatory constraints are non-negotiable. If you do not involve legal during framing, you will pay for it at launch time.

With constraints documented and conflicts resolved, the next step is to make equally explicit what you are **not** solving. Non-goals are the boundaries that prevent scope creep, set expectations, and save teams from building features nobody asked for.
