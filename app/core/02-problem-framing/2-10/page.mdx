# 2.10 — Decomposition Case Studies

Theory clarifies thinking. Practice reveals where theory breaks down. This chapter walks through three complete decomposition exercises, from initial problem statement to final sub-problem structure, showing the decisions, dead ends, and revisions that happen in real projects. Each case comes from actual systems built between 2023 and 2025, with details changed to protect confidentiality but patterns preserved to illustrate principles.

## Case Study One: Enterprise Document Q&A System

The initial problem statement arrived in a two-sentence email from the VP of Operations at a 3,000-person financial services company: "Our employees waste hours searching for information in company documents. Can AI help them find answers faster?"

The product team's first instinct was to frame this as a retrieval problem: build a better search engine. But better search doesn't answer questions—it returns documents. The real problem was "employees need direct answers to specific questions, with citations back to source documents." That reframing shifted the approach from retrieval to question answering, which triggered a completely different decomposition.

**First-pass decomposition** looked like this: take a user question, find relevant documents, extract the answer, return it. Three sub-problems: retrieval, extraction, presentation. The team built a prototype in two weeks. It worked on simple questions like "what is our travel reimbursement policy?" but failed catastrophically on anything complex.

The failure revealed missing sub-problems. When users asked "can I expense client dinners?" the system needed to understand that "expense" meant reimbursement, that "client dinners" meant business meals, and that the question was really "are business meals with clients reimbursable?" The retrieval module needed a query understanding layer first.

**Second-pass decomposition** added query understanding as a sub-problem. Now the flow was: understand the query, retrieve documents, extract the answer, present it. Better, but still incomplete. Testing revealed another gap: sometimes no document contained the answer. The system would hallucinate a plausible-sounding response rather than admit uncertainty. Users lost trust fast.

This triggered a critical insight: question answering isn't one sub-problem, it's a cascade of decisions. Can we answer this question at all? If yes, from which documents? If we're uncertain, how do we communicate that? If we can't answer, who should the user ask instead?

**Third-pass decomposition** emerged from mapping the full decision tree:

**Query Understanding** — Transform the user's natural language question into a structured representation. Resolve ambiguous terms. Expand abbreviations. Identify question type: factual lookup, policy interpretation, procedural how-to, opinion-seeking. This sub-problem outputs a normalized query plus metadata about question type and key entities.

**Document Retrieval** — Given a normalized query, find the top-K most relevant documents from the company corpus. This sub-problem uses semantic search over document embeddings, with reranking based on recency, authority, and relevance. Output: ranked list of document chunks, each with a relevance score.

**Passage Extraction** — Given a question and a set of document chunks, identify specific passages that contain potential answers. Not entire documents—specific paragraphs or sentences. This sub-problem might find zero passages (no answer exists) or multiple passages (answer appears in several places). Output: extracted passages with start/end positions and source metadata.

**Answer Synthesis** — Given a question and a set of passages, generate a natural language answer. If passages agree, synthesize a single coherent answer. If they conflict, note the conflict. If they're incomplete, synthesize what's known and flag gaps. Output: natural language answer, confidence score, and list of supporting passages.

**Citation Generation** — Given an answer and supporting passages, generate precise citations. Not just "see document X"—"see document X, section 3.2, last updated October 2024." Users need to verify answers, which requires precise pointers. Output: formatted citations with deep links.

**Confidence Scoring** — Given an answer, assess how confident the system is. Was the answer found in one authoritative document or cobbled together from vague hints across five documents? Did the passages directly state the answer or require inference? Output: confidence score (high, medium, low) with explanation.

**Escalation Decision** — Given a question and confidence score, decide whether to show the answer or escalate to a human. Low-confidence answers on critical topics (legal, compliance, HR) escalate automatically. High-confidence answers on routine topics display directly. Output: show answer or route to specific expert.

That's seven sub-problems, up from the initial three. The decomposition revealed hidden complexity—but also clarified what needed to be built.

**Dependency mapping** showed a clean linear flow: query understanding feeds retrieval, which feeds passage extraction, which feeds answer synthesis, which feeds citation generation, which feeds confidence scoring, which feeds escalation decision. One exception: confidence scoring needed access to both the original query (to assess question complexity) and retrieval scores (to assess source quality), creating a cross-dependency.

**Evaluation strategy** followed naturally from the decomposition. Query understanding could be evaluated on a dataset of 500 real user questions annotated with normalized forms. Retrieval could be evaluated using precision-at-K against a ground truth set of question-document pairs. Passage extraction could be evaluated on exact match of extracted spans against human-labeled passages. Answer synthesis required human eval, but the decomposition made it tractable: evaluators only judged whether the synthesis accurately reflected the passages, not whether the entire system worked end-to-end. Confidence scoring could be evaluated by calibration: do 90% of high-confidence answers turn out correct?

The final system launched with all seven sub-problems implemented at varying levels of sophistication. Query understanding used a fine-tuned language model. Retrieval used a hybrid of keyword search and dense embeddings. Passage extraction used a span-based model. Answer synthesis used GPT-4 with carefully designed prompts. Citations were rule-based. Confidence scoring combined model logits, passage agreement, and source authority into a weighted formula. Escalation used a simple threshold: anything below 70% confidence on a sensitive topic escalated.

Six months post-launch, usage data validated the decomposition. The escalation sub-problem turned out to be critical: 22% of queries escalated, and user surveys showed that explicit "I'm not sure, ask Jane in Legal" responses built more trust than mediocre answers would have. The team also learned that query understanding mattered less than expected—most users asked clear questions—but passage extraction mattered more, because precise citations drove adoption among skeptical power users.

## Case Study Two: Customer-Facing Coding Assistant

A developer tools company wanted to build an AI assistant that helped developers write code faster. The initial problem statement was ambitiously vague: "Help developers write code faster."

The team knew that vagueness was fatal, so they spent two weeks interviewing developers about where they got stuck. The pattern was clear: developers didn't need help writing code from scratch—they needed help with specific friction points. Understanding a foreign API. Debugging a cryptic error. Writing boilerplate. Explaining legacy code. The real problem was "reduce friction at specific points in the development workflow."

That reframing led to decomposition by friction point rather than by technical capability. Instead of "code generation" as a monolithic sub-problem, they decomposed by use case.

**First-pass decomposition** identified six friction points:

**Intent Classification** — When a developer types a request in the assistant, what are they actually trying to do? Generate new code? Explain existing code? Debug an error? Ask a conceptual question? The system needed to classify intent before deciding how to respond. Output: intent label plus extracted entities (function names, error messages, file paths).

**Context Gathering** — Different intents need different context. If the developer is asking "why does this function crash?" the system needs the function's code, recent changes, and the error traceback. If they're asking "how do I use this API?" the system needs API documentation. Context gathering became a sub-problem: given an intent and a workspace state, collect the minimal relevant context. Output: structured context bundle (code snippets, docs, error logs).

**Code Generation** — Given a natural language request and context, generate syntactically correct, idiomatically appropriate code. This isn't just "run a model"—it's "run a model, parse the output, insert it at the right location in the file, preserve indentation, add necessary imports." Output: code snippet with metadata about where it should be inserted.

**Code Explanation** — Given a code snippet, generate a natural language explanation. Developers use this to understand unfamiliar code, onboard to a new codebase, or document their own code. The challenge is pitching the explanation at the right level: a senior developer needs high-level architecture, a junior developer needs line-by-line breakdown. Output: multi-level explanation (summary, detailed, line-by-line).

**Error Detection** — Given a code snippet, identify potential bugs, security vulnerabilities, or style violations before the developer runs the code. This sub-problem runs static analysis, linting, and model-based bug prediction. Output: list of issues with severity, location, and fix suggestions.

**Fix Suggestion** — Given an error message and code, suggest a fix. This differs from code generation because the context is different: you have a concrete failure, a stack trace, and often logs. The system needs to diagnose root cause and propose a targeted fix. Output: diagnosis explanation plus proposed code change.

**Test Generation** — Given a function, generate unit tests. Developers know they should write tests but often don't because it's tedious. Automating test generation removes friction. Output: test code with assertions covering common cases and edge cases.

That's seven sub-problems, each corresponding to a specific developer workflow moment. The decomposition made the roadmap obvious: launch with intent classification, context gathering, and code generation (the core happy path), then add explanation, error detection, fixes, and test generation in subsequent releases.

**Dependency mapping** revealed a hub-and-spoke pattern. Intent classification was the hub: every interaction started there. Each other sub-problem was a spoke: once intent was classified, the system routed to the appropriate module. Context gathering had a dependency on intent (different intents need different context), but the other sub-problems were largely independent. This meant the team could parallelize work: one engineer on code generation, another on explanation, another on error detection.

**Evaluation strategy** varied dramatically by sub-problem. Intent classification was a standard ML problem: accuracy, precision, recall on a labeled dataset. Context gathering was harder—how do you measure whether the system gathered the "right" context? The team settled on a proxy: given the gathered context, can a human (or another model) successfully answer the developer's question? If yes, context was sufficient. If no, context was incomplete.

Code generation used a mix of automated and human eval. Automated: does the generated code compile? Does it pass type checking? Does it match basic style guidelines? Human: is the code idiomatic? Does it actually solve the problem? Would you merge this in a code review? The team built a weekly eval ritual: engineers reviewed 50 random generated code snippets and rated them on a 1-5 scale.

Code explanation used human eval exclusively: developers read the explanation and rated it on clarity, accuracy, and usefulness. Error detection used precision and recall: how many real bugs did it catch, and how many false positives did it raise? Fix suggestion used a success metric: after applying the suggested fix, does the code work?

Test generation used coverage and mutation testing: do the generated tests actually cover the function's behavior, and do they catch real bugs when you introduce faults?

**Launch and iteration:** The team shipped v1 with intent classification, context gathering, and code generation after three months. Adoption was strong among early adopters but plateaued. User interviews revealed why: developers loved the assistant when it worked but lost trust when it failed. A failed code generation attempt (syntactically correct but semantically wrong code) was worse than no help at all.

This insight triggered a decomposition revision. The team added a new sub-problem: **confidence estimation**. Before showing generated code, the system assessed confidence. High confidence: show the code directly. Medium confidence: show the code with a warning "this might need adjustment." Low confidence: don't generate, instead offer an explanation of the task and suggest documentation links. This sub-problem massively improved trust.

Six months post-launch, the data validated the decomposition but also revealed a missing sub-problem: **workflow integration**. Developers didn't want to context-switch to a separate assistant tool—they wanted the assistant embedded in their IDE, terminal, and code review flow. The team added a new sub-problem: multi-surface integration, building plugins for VSCode, JetBrains, and GitHub. This wasn't a "code generation" problem—it was a delivery and UX problem, but it was essential. Decomposition isn't just about AI capabilities; it's about the full user experience.

## Case Study Three: Voice-Based Medical Triage Agent

A telemedicine company wanted to help patients assess the urgency of their symptoms and decide whether to book an appointment, go to urgent care, or call 911. Initial problem statement: "Help patients figure out if they need to see a doctor."

This problem was immediately recognized as high-stakes. A wrong answer could delay critical care or send someone to the ER unnecessarily. The team started with a risk-first decomposition: what could go wrong, and how do we prevent it?

**Risk mapping** identified three failure modes: under-triage (tell someone with a heart attack to wait), over-triage (send someone with a cold to the ER), and user frustration (make the process so tedious that users abandon it and don't get any help). Each failure mode suggested sub-problems.

**First-pass decomposition** from a safety perspective:

**Speech Recognition** — Patients describe symptoms verbally. The system needs to transcribe accurately, especially medical terms. A misheard "chest pain" as "test pain" could be fatal. Output: transcribed text with confidence scores per word.

**Symptom Extraction** — Given a patient's verbal description, extract structured symptoms. "I've had a headache for three days and I feel dizzy" should extract headache (duration: 3 days) and dizziness (duration: unspecified). Output: structured symptom list with attributes.

**Risk Assessment** — Given symptoms, assess urgency. This is the highest-stakes sub-problem. It can't just be a model—it needs to encode medical protocols (e.g., chest pain plus shortness of breath always escalates). Output: risk level (immediate, urgent, routine, self-care) with reasoning.

**Protocol Matching** — Given symptoms and risk level, match to clinical protocols. If a patient has flu-like symptoms, the protocol might be "rest, fluids, monitor for worsening." If they have chest pain, the protocol is "call 911 immediately." Output: matched protocol with specific instructions.

**Escalation Decision** — Given risk level and protocol, decide next steps. Immediate: call 911. Urgent: book urgent care within 4 hours. Routine: book primary care appointment within a week. Self-care: provide home care instructions. Output: escalation action.

**Appointment Booking** — If escalation requires booking, integrate with the scheduling system. Check availability, book a slot, send confirmation. Output: booking confirmation with time and provider.

**Documentation** — Generate a summary of the interaction for the patient's medical record. Include symptoms reported, risk assessment, protocol followed, and action taken. This is legally and clinically essential. Output: structured clinical note.

That's seven sub-problems. The decomposition clarified the architecture: a pipeline from speech to documentation, with risk assessment as the critical decision point.

**Dependency mapping** showed a strict linear flow with one exception: risk assessment needed to loop back to symptom extraction if it detected gaps. If a patient said "I feel sick" but didn't mention specific symptoms, risk assessment couldn't proceed. The system needed to prompt for details: "Can you describe what kind of sick? Pain? Fever? Nausea?" This created a feedback loop between risk assessment and symptom extraction, complicating the architecture but improving safety.

**Evaluation strategy** was dominated by safety requirements. Speech recognition used Word Error Rate (WER) on medical terminology specifically—overall WER was less important than accuracy on critical terms like "chest," "pain," "bleeding," "unconscious." The team built a test set of 1,000 utterances covering common symptoms and edge cases.

Symptom extraction used exact match against clinician annotations. Given a patient utterance, did the system extract the same symptoms a nurse would? The team hired RNs to annotate 2,000 sample interactions.

Risk assessment couldn't be evaluated by ML metrics alone—it required clinical validation. The team partnered with ER physicians to review 500 sample cases. For each case, the physician independently assessed risk, then compared to the system's assessment. Agreement rate needed to exceed 95% on high-urgency cases (where disagreement could be fatal) and 85% overall.

Protocol matching and escalation decision were evaluated together: given a symptom set and risk level, did the system recommend the same action a triage nurse would? This used a similar physician review process.

Appointment booking was evaluated on technical success rate: did the booking API call succeed? Was the confirmation sent? This was the only sub-problem evaluated without clinical input.

Documentation was evaluated on completeness and compliance: did the note include all required fields for the medical record? Did it meet regulatory standards (HIPAA, HITECH)? Legal review was essential here.

**Launch strategy** was ultra-conservative. The team launched with all seven sub-problems implemented, but with a critical safety feature: every interaction was reviewed by a human triage nurse within 30 minutes. The AI made recommendations; the nurse confirmed or overrode them. This parallel operation ran for six months, generating a massive dataset of AI recommendations and human corrections.

Analysis of corrections revealed decomposition gaps. Nurses frequently overrode the escalation decision not because the risk assessment was wrong but because they considered non-medical factors: does the patient have transportation to urgent care? Can they afford the ER? Do they have someone to watch their kids if they go to the hospital? These weren't medical questions, but they affected the right recommendation.

The team added an eighth sub-problem: **contextual factor assessment**. Before finalizing escalation, the system asked about access barriers: "Do you have a way to get to urgent care right now?" "Is cost a concern?" Based on answers, it adjusted recommendations. A low-income patient without transportation who needed urgent care got connected to a community health van service, not just told to "go to urgent care."

**One year post-launch**, the system handled 60% of interactions without human override, with nurse review happening asynchronously (within 24 hours instead of 30 minutes) for non-urgent cases. The remaining 40% escalated to nurses immediately, mostly cases where symptoms were vague or risk was borderline.

The decomposition had evolved from seven sub-problems to eight, with additional safety layers: automatic escalation if speech recognition confidence was below 85% on critical terms, automatic escalation if symptom extraction found zero symptoms (likely a complete failure), and automatic escalation if the patient mentioned anything pregnancy-related (too high-risk for the AI to handle alone).

## What These Cases Teach

Three different problems, three different decomposition structures. The document Q&A system decomposed linearly: each sub-problem fed the next. The coding assistant decomposed hub-and-spoke: intent classification routed to specialized modules. The medical triage agent decomposed as a safety-critical pipeline with feedback loops.

The commonality: decomposition emerged from deeply understanding the problem, mapping failure modes, and iterating based on real usage. None of these teams got decomposition right on the first try. All of them revised their sub-problem structure as they learned.

The lesson isn't "here's how to decompose your problem." It's "here's how to think about decomposition as a discovery process." Start with a hypothesis, build enough to test it, learn where it breaks, refine. Repeat until the decomposition feels natural, the evaluation strategy is clear, and the roadmap makes sense.

As you move from case studies to understanding where AI fits in the broader user journey, you'll see that decomposition doesn't stop at the AI system boundary—it extends to the entire workflow.
