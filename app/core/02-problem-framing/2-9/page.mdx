# 2.9 — Decomposition for Iterative Development

Three months into building a clinical documentation assistant, the founding team at MedScribe faced a crisis. They had decomposed their problem beautifully: extract patient symptoms, map to medical codes, generate clinical notes, verify against EHR data, format for billing compliance. The architecture was sound. The dependency graph was clean. But they had made a fatal mistake in execution order.

They built from left to right, finishing each sub-problem completely before moving to the next. Six engineers spent eight weeks perfecting symptom extraction, achieving 94% accuracy on their benchmark. Then they moved to medical coding. Then note generation. Twelve weeks in, they still had never generated a single end-to-end clinical note. When they finally connected all the pieces in week thirteen, they discovered their symptom extraction output format was incompatible with their coding module's expected input. The billing compliance formatter needed metadata their note generator never produced. Most critically, doctors hated the output because it read nothing like their actual workflow—a problem invisible until the system ran end-to-end.

They had optimized each sub-problem in isolation and built a system that didn't work as a whole. The lesson was expensive but clear: decomposition is not just about breaking problems apart. It's about sequencing the reassembly so you learn fast and ship incrementally.

## The Walking Skeleton: Your First Priority

The term "walking skeleton" comes from Alistair Cockburn, and it describes exactly what MedScribe should have built first: a thin, end-to-end implementation that touches every sub-problem but implements none deeply. A skeleton that walks—barely, awkwardly—but demonstrates the system works as a connected whole.

For MedScribe, a walking skeleton would have looked like this on day one: accept a simple patient utterance, extract symptoms using basic keyword matching, map to medical codes using a hardcoded lookup table, generate a note using a template with fill-in-the-blank logic, skip EHR verification entirely, and format for billing using the simplest possible structure. Total time to build: three days. Total value: immense.

This skeleton reveals the integration points immediately. It shows whether the output of symptom extraction actually feeds cleanly into medical coding. It exposes format mismatches before you've invested weeks perfecting individual components. Most importantly, it gives you something to put in front of users—even if you preface it with "this is extremely basic"—so you learn whether the end-to-end flow matches their workflow.

The walking skeleton approach inverts the traditional engineering instinct. Instead of building sub-problem one to perfection, then sub-problem two, then three, you build sub-problem one at 20% quality, sub-problem two at 20%, sub-problem three at 20%, connect them, validate the flow, then deepen. You're trading depth-first for breadth-first. You're prioritizing integration risk over component quality early on.

Once your skeleton walks, you systematically deepen each sub-problem. But now you deepen with full context. You know what format the next module needs. You know what latency budget you're working with. You know whether users care more about coverage or precision in this particular module. You're no longer optimizing in a vacuum.

## Priority Heuristics: Choosing What to Deepen First

After your walking skeleton works, you face a new question: which sub-problem do you invest in deepening first? You can't deepen everything simultaneously. You need a sequencing strategy.

Three heuristics dominate in practice: highest-risk first, highest-unlock first, and fastest-feedback first. Each optimizes for a different failure mode.

**Highest-risk first** means attacking the sub-problems where you're most uncertain. If you've decomposed a legal contract review system into clause extraction, risk classification, precedent matching, and summary generation, and you've never built a risk classifier for contracts before, you start there. You stub out the other modules and focus on proving the risky one is feasible. This heuristic protects you from spending six months building a system only to discover the core technical assumption doesn't hold. The earlier you invalidate a bad assumption, the less you waste.

The risk might be technical: can we actually achieve the accuracy we need? It might be data-related: do we have enough training examples for this sub-problem? It might be latency-related: can we run this model fast enough to meet user expectations? It might be compliance-related: can we log enough information to satisfy auditors? Whatever the risk, surface it early. Build just enough of that sub-problem to retire the risk, then move on.

**Highest-unlock first** means attacking bottleneck sub-problems that block progress on multiple others. If you've decomposed a code review assistant into diff parsing, bug detection, style checking, security scanning, and report generation, and the report generation module needs to integrate outputs from all the others, you don't start with report generation. You start with diff parsing, because every other module depends on clean, structured diffs. Unlocking the bottleneck unblocks parallel work.

This heuristic is particularly valuable when you're scaling a team. If you have five engineers and five sub-problems, and four of those sub-problems depend on the first one, sequencing matters enormously. Build the dependency first, stub it out minimally so others can build against a stable interface, then parallelize the rest. If you sequence wrong, you get four engineers waiting while one engineer finishes the bottleneck.

**Fastest-feedback first** means attacking sub-problems where you can establish evaluation and iteration loops most quickly. If you've decomposed a resume screening system into resume parsing, skill extraction, experience matching, culture fit assessment, and ranking, and you have ground truth data for skill extraction but only vague intuitions about culture fit, you start with skill extraction. You can measure it. You can iterate. You can show progress.

This heuristic is particularly powerful in organizational contexts where stakeholders need proof of progress. It's also powerful for team morale. Building something you can't evaluate feels like wandering in the dark. Building something where you can measure improvement weekly feels like progress. Early wins on measurable sub-problems buy you patience for later work on harder-to-evaluate sub-problems.

## Stubbing Out Sub-Problems: The Art of the Fake

Iterative development requires building some sub-problems before others. But those sub-problems don't exist in isolation—they're part of a system. How do you build sub-problem three when sub-problem five isn't implemented yet?

You stub it out. A stub is a fake implementation that returns plausible output so the system can run end-to-end, even though the sub-problem isn't really solved.

The simplest stub is a hardcoded response. If you're building a chatbot and you haven't implemented intent classification yet, you stub it out by always returning "general_question" as the intent. Your downstream modules receive an intent, the system runs, you validate integration, and you learn whether your architecture actually works. Later, you replace the stub with a real classifier.

A more sophisticated stub is a rule-based fallback. If you're building a pricing optimization engine and you haven't trained your demand forecasting model yet, you stub it out with a simple rule: predict demand equal to last week's demand. It's wrong, but it's wrong in a plausible way. Your system runs, you validate integration, you tune downstream modules, and when you finally implement real forecasting, everything else already works.

The best stubs are statistically plausible even if they're not intelligent. If your sub-problem is classifying customer support tickets into ten categories, a stub that always returns the most common category is bad—it doesn't exercise the system realistically. A better stub randomly samples from the true category distribution you see in production. It won't classify correctly, but it will expose any downstream logic that breaks when rare categories appear.

Stubs should be clearly marked and trivial to replace. The worst kind of stub is one that accidentally ships to production because no one remembered it was a stub. Use feature flags, configuration files, or even just comments in code that shout "THIS IS A STUB." When you're ready to replace it, you should know exactly where to look.

## The MVP Decomposition: Sequencing Across Versions

Decomposition doesn't just sequence work within a single version—it sequences work across multiple product releases. Which sub-problems are essential for v1? Which can wait for v2? Which are nice-to-haves for v3?

The MVP decomposition starts with a brutally honest question: what is the minimum sub-problem set that delivers user value? Not complete value. Not ideal value. Minimum viable value.

Consider an AI-powered expense report system. Full decomposition might include: receipt OCR, expense categorization, policy compliance checking, duplicate detection, currency conversion, mileage calculation, approval routing, fraud detection, and GL code assignment. That's nine sub-problems. Building all nine before launch means six months of development. MVP decomposition asks: which three sub-problems let us ship something useful in six weeks?

The answer depends on your user's current pain point. If they're manually typing receipt data, then receipt OCR plus basic categorization plus approval routing might be enough. The system saves them typing even if it doesn't catch duplicates or detect fraud yet. You ship with three sub-problems, learn from real usage, and decide what to add next based on actual user complaints, not hypothetical feature completeness.

The key insight is that sub-problems have different value-to-effort ratios at different stages. Fraud detection might be essential at scale but unnecessary when you have fifty users. Duplicate detection might seem critical in theory but rare in practice. Currency conversion might matter for 5% of your users. MVP decomposition prioritizes sub-problems that matter now and defers sub-problems that matter later.

This doesn't mean shipping broken software. It means shipping software that solves a narrower problem well rather than a broad problem poorly. A receipt OCR system that only handles restaurant receipts but handles them perfectly is more valuable than one that handles all receipt types badly. Narrow your problem scope by deprioritizing sub-problems, not by lowering quality within each sub-problem you keep.

## Incremental Shipping Without Breaking Trust

The hardest part of iterative decomposition is maintaining user trust while shipping incomplete systems. Users don't think in terms of sub-problems. They think in terms of "does this work for me?" If you ship an expense system that works for restaurants but fails on hotel receipts, and you don't tell them, they'll try it on a hotel receipt, it will fail, and they'll conclude your system doesn't work—even though it works perfectly for restaurants.

The solution is explicit scope communication. Don't ship a "receipt OCR system" that secretly only handles restaurants. Ship a "restaurant receipt OCR system" that explicitly states its limitations. When users try a hotel receipt, the system says "I only handle restaurant receipts right now—hotel receipts coming in v2." Trust is preserved because you set accurate expectations.

This principle extends to all sub-problem sequencing. If you've built sentiment analysis and intent classification but not entity extraction yet, don't market your chatbot as "understanding everything users say." Market it as "understanding the topic and tone of user messages, with specific detail extraction coming soon." When a user asks "what did I say about pricing?" and the system can't answer, they're not surprised—you told them upfront it couldn't do that yet.

Incremental shipping also requires feature flags and graceful degradation. If you're deepening your sentiment analysis from three classes (positive, negative, neutral) to nine fine-grained emotions, you need to ship the new version incrementally. Start with a flag that enables the new version for 10% of users, monitor quality, and ramp up. If the new version underperforms, fall back to the old one automatically. Users never experience a regression because you shipped carefully.

The worst iterative development strategy is one where v2 breaks what v1 did well. If your v1 expense system handled restaurant receipts perfectly and your v2 adds hotel receipts but now restaurant receipt accuracy drops from 95% to 87%, you've betrayed users. Iterative development means adding capabilities, not trading them. Your test suite must ensure that every sub-problem you've already shipped maintains quality as you deepen others.

## Mapping Decomposition to Your Roadmap

Decomposition shouldn't live only in engineering's head—it should structure your product roadmap. Each sub-problem becomes a roadmap candidate. Each quarter, you decide which sub-problems to deepen, which to add, and which to defer.

A well-structured roadmap driven by decomposition looks like this. Q1: ship walking skeleton with three core sub-problems at MVP quality. Q2: deepen sub-problem one from 70% accuracy to 90%, based on user feedback that it's the biggest pain point. Q3: add sub-problem four, which users have been requesting. Q4: deepen sub-problem two from 80% latency compliance to 95%, because scale is straining the system.

This roadmap is legible to both engineering and business stakeholders. Engineers understand which sub-problems they're building and why. Product managers understand what user value each sub-problem unlocks. Executives understand how capabilities accumulate over time. Everyone shares the same decomposition-based vocabulary.

Contrast this with a roadmap that says "improve the AI" or "make the system smarter." Those goals are vague and unmeasurable. Decomposition makes roadmaps concrete. You're not improving "the AI"—you're improving the entity extraction sub-problem from 82% F1 to 90% F1. You're not making the system smarter—you're adding a new sub-problem for handling multi-step queries. Precision matters.

The decomposition-driven roadmap also makes trade-offs explicit. When a stakeholder asks "why aren't we building feature X?" you can answer "because feature X depends on sub-problem Y, which we haven't implemented yet, and implementing Y requires three months." The conversation shifts from "why not?" to "is Y worth three months?" That's a better conversation.

## The Feedback Loop Between Decomposition and Reality

The final lesson is that decomposition isn't static. Your initial decomposition is a hypothesis. Iterative development tests that hypothesis. Sometimes reality forces you to re-decompose.

MedScribe eventually learned this. After their painful launch, they revisited their decomposition. They realized their "symptom extraction" sub-problem was actually two sub-problems: extracting explicit symptoms the patient stated, and inferring implicit symptoms from context. They had conflated them, which made the module impossible to evaluate cleanly. They split it. Accuracy improved.

They also discovered that "verify against EHR data" wasn't a single sub-problem—it was three: verify patient identity, check for contradictory prior records, and flag missing information. Each needed different data sources and different logic. Splitting them clarified responsibilities and sped up development.

Decomposition evolves as you learn. The structure that makes sense on day one might not make sense on day ninety. Stay flexible. If a sub-problem keeps causing problems, maybe it's actually two sub-problems masquerading as one. If two sub-problems always change together, maybe they're one sub-problem split artificially. Let reality teach you.

Iterative development isn't just about shipping incrementally—it's about learning how to decompose correctly by watching how your decomposition performs under real use. The best teams treat decomposition as a living document, revisited quarterly, refined as understanding deepens. The worst teams decompose once, enshrine it, and wonder why the system feels increasingly awkward as they build.

As you move from iterative sequencing to practical examples, you'll see exactly how these principles play out across different problem types.
