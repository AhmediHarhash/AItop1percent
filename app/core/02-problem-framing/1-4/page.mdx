# 1.4 — Problem Framing Across Risk Tiers

A software engineer at a small startup is building an internal tool that summarizes Slack conversations to help her team catch up on threads they missed. She spends 15 minutes thinking about the problem, sketches a prompt on a napkin, writes 50 lines of Python, and ships it to the team Slack workspace that afternoon. It works well enough. No one gets hurt if it occasionally misses a detail or summarizes something oddly. The team gives feedback, she tweaks the prompt, and everyone moves on.

Three thousand miles away, a team of 40 engineers at a medical device company is building an AI system that will assist radiologists in detecting early-stage lung cancer from CT scans. The project has been in planning for four months. The problem framing document is 60 pages long and has been reviewed by clinicians, legal counsel, regulatory experts, and the executive team. The document defines 14 specific sub-tasks, 9 categories of edge cases, 23 success criteria, and 7 failure modes that require human escalation. It includes a formal threat model, a compliance matrix mapping to FDA regulations, and a detailed plan for clinical validation. The team will spend another six months building ground truth datasets and evaluation infrastructure before they write a single line of model code.

Both teams are doing problem framing. But the depth, rigor, and formality of their framing processes are radically different, and appropriately so. The difference is not that one team is more competent or more careful. The difference is that the consequences of getting it wrong are incomparable.

If the Slack summarization tool hallucinates a detail, someone might be briefly confused. If the lung cancer detection system misses a tumor, a patient could die.

This is the core principle of risk-scaled framing: the depth and rigor of your problem framing process should match the consequences of failure.

## Risk Tiers and Framing Effort

In Section 1, we introduced a four-tier risk framework for AI systems:

- Tier 1: Low-risk internal tools with minimal consequences for errors
- Tier 2: User-facing, non-critical systems where errors are annoying but not harmful
- Tier 3: Systems with financial, legal, or reputational impact
- Tier 4: Safety-critical, regulated systems where errors can cause serious harm

Each tier requires a different level of framing rigor, different documentation standards, and different review processes.

**Tier 1: Napkin Sketch Framing**

For low-risk internal tools, formal problem framing is often overkill. The cost of over-engineering the process exceeds the cost of getting the problem slightly wrong. A lightweight framing approach is appropriate:

- Spend 15 minutes to an hour thinking through the task
- Write a few bullet points: what the system does, what inputs it takes, what outputs it produces, what success looks like
- Identify one or two obvious edge cases
- Sketch a rough eval plan (often just "try it and see if it is useful")
- Ship quickly, gather feedback, iterate

The Slack summarization tool fits here. The engineer does not need a 20-page framing document. She needs clarity on the core task (summarize missed Slack threads), a rough quality bar (good enough that teammates find it useful), and a feedback loop (teammates will tell her if it is not working).

The risk is contained. If the tool does not work, the team stops using it. No one is harmed. The cost of failure is an afternoon of wasted engineering time. The cost of over-engineering the framing process would be higher than the cost of failure.

Other Tier 1 examples: internal dashboards, exploratory data analysis tools, personal productivity scripts, proof-of-concept prototypes that will never see production.

**Tier 2: Structured Framing Document**

For user-facing, non-critical systems, you need more rigor. Errors are not catastrophic, but they affect real users, and fixing them post-launch is expensive. A structured framing process is appropriate:

- Spend a few days on framing
- Write a multi-page document covering task definition, success criteria, input/output specs, edge cases, failure modes, and evaluation strategy
- Get feedback from stakeholders (product, design, eng) and revise
- Define metrics and set acceptable thresholds
- Plan for monitoring and user feedback loops

An example: a recommendation engine for a content platform. If recommendations are bad, users will be annoyed and engagement might drop, but no one is harmed. The framing should be thorough enough to avoid shipping something obviously broken, but it does not need legal review or formal verification.

The document might include:

- Task breakdown: candidate generation, ranking, diversity filtering, personalization
- Success criteria: increase time on platform by 10 percent, maintain or improve user satisfaction scores, ensure diversity (no more than 2 recommendations from the same creator in top 10)
- Edge cases: new users with no history, users with niche interests, users who consume content in multiple languages
- Failure modes: recommending inappropriate content, filter bubbles, staleness (showing the same recommendations repeatedly)
- Eval plan: offline metrics (precision at k, diversity score), online A/B test (engagement, satisfaction)

This level of detail is enough to build the system correctly without over-investing in process. The team knows what they are building, how to evaluate it, and what to monitor. They can iterate based on user feedback.

Other Tier 2 examples: e-commerce search, social media feeds, email spam filters, internal workflow automation for non-critical tasks.

**Tier 3: Formal Specs with Sign-Off**

For systems with financial, legal, or reputational impact, framing becomes a formal process with accountability. Errors can cost money, trigger lawsuits, or damage the company's reputation. The framing must be detailed, reviewed by experts, and approved by decision-makers who understand the risks.

The process:

- Spend weeks on framing, involving domain experts, legal, compliance, and executive stakeholders
- Produce a comprehensive document (often 20-50 pages) covering task definition, success criteria, constraints, failure modes, risk assessment, compliance requirements, and evaluation plan
- Conduct formal review with sign-off from legal, compliance, and executive leadership
- Define hard thresholds for acceptable performance and error rates
- Plan for extensive testing, including adversarial testing and stress testing
- Document assumptions and constraints for audit purposes

An example: a credit scoring model used to make lending decisions. Errors can result in regulatory fines, discrimination lawsuits, and reputational damage. The framing must address:

- Task definition: predict probability of default, predict creditworthiness score, or classify into risk tiers?
- Success criteria: accuracy, but also fairness across protected groups (race, gender, age), compliance with Equal Credit Opportunity Act and state-level lending regulations, explainability requirements
- Constraints: must use only permissible data (cannot use protected attributes directly), must provide explanations for adverse decisions, must be auditable
- Failure modes: disparate impact on protected groups, incorrect risk assessment leading to bad loans or unfair denials, model drift as economic conditions change
- Evaluation: offline metrics (AUC, calibration, fairness metrics by subgroup), compliance testing (disparate impact analysis), adversarial testing (can the model be manipulated?), ongoing monitoring (performance by subgroup, drift detection)

The framing document is reviewed by legal counsel to ensure compliance with regulations. It is reviewed by the executive team to ensure alignment with business goals and risk tolerance. It is reviewed by data scientists to ensure technical feasibility. Sign-off is required before any model development begins.

This level of process is not bureaucratic overhead. It is necessary risk management. The cost of getting it wrong—fines, lawsuits, loss of trust—far exceeds the cost of rigorous upfront framing.

Other Tier 3 examples: fraud detection, loan underwriting, insurance pricing, hiring and promotion algorithms, content moderation at scale.

**Tier 4: Formal Verification and Regulatory Compliance**

For safety-critical and regulated systems, problem framing is not just a planning exercise. It is a compliance requirement, often mandated by law. The framing must be exhaustive, formally verified where possible, and designed to withstand regulatory audits.

The process:

- Spend months on framing, involving cross-functional teams including engineers, domain experts, safety engineers, legal, regulatory affairs, and often external auditors
- Produce extensive documentation (50-100+ pages) including task specifications, safety requirements, failure mode and effects analysis (FMEA), threat models, compliance matrices, validation plans, and risk mitigation strategies
- Conduct formal reviews at multiple stages, with sign-off from regulatory experts and executive leadership
- Map every requirement to specific regulations (e.g., FDA guidelines for medical AI, EU AI Act requirements for high-risk systems)
- Define not just success criteria but also safety invariants—conditions that must never be violated
- Plan for clinical trials, third-party audits, or other forms of external validation

An example: the lung cancer detection system mentioned at the start of this chapter. The framing must address:

- Task definition: detect nodules, classify nodules by malignancy likelihood, or recommend follow-up actions? Each has different regulatory implications.
- Success criteria: sensitivity and specificity thresholds set based on clinical evidence, performance across patient demographics (age, sex, smoking history), performance on different scanner types and imaging protocols
- Safety requirements: the system must never replace the radiologist's judgment, must flag uncertainty, must escalate edge cases to human review
- Failure modes: false negatives (missed cancers), false positives (unnecessary biopsies), errors on underrepresented populations, adversarial inputs (corrupted images)
- Compliance: FDA 510(k) or De Novo clearance requirements, HIPAA compliance for data handling, EU Medical Device Regulation (MDR) if selling in Europe, state-level telehealth regulations if used for remote diagnosis
- Validation plan: retrospective study on historical data, prospective clinical trial, ongoing post-market surveillance

The framing document is reviewed by oncologists to ensure clinical validity. It is reviewed by regulatory experts to ensure compliance with FDA and EU requirements. It is reviewed by safety engineers to ensure failure modes are identified and mitigated. It is reviewed by legal to ensure liability is managed.

The EU AI Act, which came into full force in 2026, imposes explicit documentation requirements for high-risk AI systems. Article 11 requires technical documentation including:

- A detailed description of the system and its intended purpose
- Risk management measures
- Data governance and training data characteristics
- Validation and testing procedures
- Human oversight measures
- Cybersecurity measures

For systems like the lung cancer detector, the problem framing document is the foundation of this technical documentation. It must be detailed enough to satisfy regulators, auditors, and—in the event of harm—courts.

This level of rigor is not optional. It is legally required. The cost of non-compliance is not just reputational damage; it is regulatory action, product recalls, and potentially criminal liability.

Other Tier 4 examples: autonomous vehicle decision systems, medical diagnostic AI, AI-assisted drug dosing, aviation safety systems, nuclear power plant monitoring.

## Matching Framing Effort to Consequences

The principle underlying this tier-based approach is simple: match your framing effort to the consequences of getting it wrong.

If the worst-case outcome is "the tool is not useful and we stop using it," spend an hour on framing. If the worst-case outcome is "we get sued and fined," spend weeks. If the worst-case outcome is "someone dies," spend months and involve every relevant expert.

This is not about being cautious or risk-averse. It is about being rational. Over-investing in framing for a Tier 1 tool is wasteful. Under-investing in framing for a Tier 4 system is negligent.

The mistake teams make is applying the same framing process to every project, regardless of risk. Startups that treat every internal tool like a safety-critical system slow themselves down unnecessarily. Enterprises that treat safety-critical systems like internal prototypes expose themselves to catastrophic risk.

The right approach is to assess the risk tier first, then apply the appropriate level of framing rigor.

## How to Assess Risk Tier

Assessing risk tier is not always obvious. A system that seems low-risk on the surface might have hidden high-risk implications. Here are the key questions:

**Who is affected by errors?** Internal users only (lower risk) or external users, customers, patients, or the public (higher risk)?

**What are the consequences of errors?** Annoyance (lower risk), wasted time or money (medium risk), financial harm or legal liability (high risk), physical harm or death (highest risk)?

**Is the system regulated?** If it falls under FDA, FTC, EEOC, GDPR, EU AI Act, or other regulatory frameworks, it is automatically higher risk.

**What is the decision authority?** Is the AI making recommendations that humans review and approve (lower risk), or is it making autonomous decisions (higher risk)?

**How reversible are errors?** Can errors be caught and corrected before harm occurs (lower risk), or are they difficult to detect and costly to reverse (higher risk)?

**What is the sensitivity of the data?** Public data (lower risk) or personal, financial, health, or legally protected data (higher risk)?

An example that illustrates hidden risk: a company builds an internal tool to help HR managers draft job postings. On the surface, this seems like Tier 1—low-risk internal tool. But if the tool generates language that violates employment discrimination law (e.g., age-biased phrasing like "recent college graduate" or "digital native"), the company could face EEOC complaints. The risk is actually Tier 3.

A different example: a customer support chatbot for a telecom company. It seems like Tier 2—user-facing, non-critical. But if the chatbot provides incorrect information about contract terms, billing, or cancellation policies, the company could face lawsuits for deceptive practices. The risk is Tier 3.

When in doubt, assume higher risk and apply more rigorous framing. The cost of over-preparing is low compared to the cost of under-preparing.

## EU AI Act and Documentation Requirements

The EU AI Act, which reached full enforcement in 2026, introduced a risk-based regulatory framework that directly mirrors the tier-based framing approach.

The Act classifies AI systems into four categories:

- Unacceptable risk: banned outright (e.g., social scoring, real-time biometric surveillance in public spaces)
- High risk: subject to strict requirements (e.g., hiring tools, credit scoring, medical devices, critical infrastructure)
- Limited risk: subject to transparency obligations (e.g., chatbots must disclose they are AI)
- Minimal risk: no specific regulation (e.g., AI-powered video games)

For high-risk systems, the Act requires:

- Risk management system
- Data governance and training data documentation
- Technical documentation enabling authorities to assess compliance
- Automatic logging of events
- Human oversight
- Robustness, accuracy, and cybersecurity measures

The technical documentation must include a detailed description of the system, its intended purpose, the data used, the validation and testing procedures, and the measures in place to ensure safety and compliance.

This is not optional. Companies that deploy high-risk AI systems in the EU without proper documentation face fines of up to 6 percent of global revenue or 30 million euros, whichever is higher.

For companies operating globally, the EU AI Act sets a de facto standard. Even if you are based in the US, if you have EU customers or users, you must comply. This means Tier 3 and Tier 4 systems require formal problem framing with detailed documentation, regardless of where you are located.

The Act also introduces obligations for "general-purpose AI" models (large language models, foundation models), requiring transparency about training data, energy consumption, and capabilities. This affects how framing is done for systems built on top of these models—you must document not just your application layer but also the foundation model's characteristics and limitations.

## Examples at Each Tier

**Tier 1: Internal Slack summarization tool**

Framing effort: 30 minutes. Napkin sketch. Core task: read missed Slack threads, generate one-paragraph summary, post to user's DM. Success: teammates find it helpful. Edge cases: threads with code snippets, threads with sensitive info. Eval: ask three teammates if it is useful. No formal documentation.

**Tier 2: E-commerce product recommendation engine**

Framing effort: 3 days. Structured document. Task breakdown: candidate retrieval, ranking, diversity filtering. Success criteria: 10 percent engagement lift, maintain 4.0+ satisfaction score. Edge cases: new users, niche products, seasonal spikes. Failure modes: recommending out-of-stock items, filter bubbles. Eval: offline metrics plus A/B test. Monitoring: engagement, diversity, satisfaction.

**Tier 3: Credit scoring model**

Framing effort: 3 weeks. Formal spec with legal review. Task: predict default probability. Success criteria: AUC above 0.75, calibration error below 5 percent, no disparate impact (pass 4/5ths rule), explainability for adverse actions. Constraints: use only permissible features, comply with ECOA and state regulations. Failure modes: disparate impact, drift, adversarial gaming. Eval: offline metrics, fairness testing, compliance review, adversarial testing. Monitoring: performance by demographic subgroup, drift detection, audit logs.

**Tier 4: Lung cancer detection system**

Framing effort: 4 months. Exhaustive documentation with clinical and regulatory review. Task: detect and classify lung nodules, recommend follow-up. Success criteria: sensitivity 92 percent, specificity 88 percent, performance parity across demographics and scanner types. Safety requirements: human-in-the-loop, uncertainty flagging, escalation protocols. Failure modes: false negatives, false positives, bias, adversarial inputs. Compliance: FDA 510(k), HIPAA, EU MDR. Eval: retrospective clinical study, prospective trial, post-market surveillance. Monitoring: real-time performance tracking, adverse event reporting.

## The Gradient, Not a Cliff

While we describe four discrete tiers, the reality is a gradient. Some systems fall between tiers. Some start at one tier and migrate to another as usage scales or scope changes.

An internal tool might start as Tier 1, but if it becomes mission-critical or if external users start depending on it, it moves to Tier 2 or Tier 3. A prototype might be Tier 1 during development, but production deployment might be Tier 3.

The key is to reassess risk tier at each stage of the lifecycle: initial development, pilot deployment, full production, feature expansion. As the system evolves, the appropriate framing rigor evolves with it.

## When in Doubt, Frame More, Not Less

The asymmetry of consequences favors over-framing rather than under-framing. If you invest a week in framing for a Tier 1 tool that could have been shipped with an hour of thought, you wasted a few days. If you invest an hour in framing for a Tier 3 system that needed weeks of rigorous framing, you might face lawsuits, regulatory fines, or catastrophic reputational damage.

When in doubt, assume higher risk and apply more rigor. You can always scale back if you determine the risk is lower than expected. You cannot easily scale up rigor after you have already shipped a poorly framed system.

The next chapter will dive into the mechanics of task decomposition—how to break down complex, vague goals into specific, measurable tasks that can be built and evaluated.
