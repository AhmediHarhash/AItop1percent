# 1.5 â€” The Framing Document: What It Looks Like

In early 2024, a fintech company spent six months building an AI-powered fraud detection system. The engineering team shipped a sophisticated ensemble of models with impressive precision scores. The data science team published internal case studies. Leadership scheduled a launch event. Then, three weeks before go-live, the compliance team saw the spec for the first time and asked a single question: "How do we explain declined transactions to customers?" Nobody had an answer. The system had no interpretability layer, no fallback mechanism, and no compliance workflow integration. The launch was postponed indefinitely.

The root cause was not technical failure. The team had built exactly what they thought they were supposed to build. The problem was that nobody had written down, in clear shared language, what the system was actually supposed to accomplish and under what constraints. There was no artifact that forced alignment before code was written.

This is what a framing document prevents.

## The Artifact That Forces Alignment

A Problem Framing Spec is a living document that captures the shared understanding of what you are building and why. It is not a Product Requirements Document, which focuses on features and user stories. It is not a Technical Design Doc, which focuses on architecture and implementation. It sits upstream of both. It answers the question: "What problem are we solving, and how will we know we have solved it?"

The framing document serves three critical functions. First, it forces the team to make explicit decisions about scope, success criteria, and constraints before investing in implementation. Second, it creates a shared reference point that prevents scope creep and misalignment as the project evolves. Third, it provides a structured format for stakeholder sign-off, ensuring that business, product, engineering, data science, legal, and compliance are all aligned before significant resources are committed.

In practice, the best framing documents are between five and fifteen pages. Shorter than that and you are probably being vague. Longer than that and you are probably over-specifying implementation details that belong in a later design doc. The goal is clarity, not comprehensiveness.

## Section One: Problem Statement

This is the anchor. Everything else in the document refers back to this. A good problem statement is specific, measurable, and framed in terms of user or business outcomes, not technical capabilities.

**Bad problem statement:** "Use GPT-4 to improve customer support."

**Good problem statement:** "Reduce average time to resolution for Tier 1 customer support tickets from 18 hours to under 6 hours, while maintaining or improving customer satisfaction scores, by automating initial triage and response generation for common issues."

The difference is specificity. The good version tells you what success looks like, what the current baseline is, and what constraints matter. It does not prescribe a solution, but it makes clear what outcome you are optimizing for.

The problem statement should also specify who has the problem. Is this a user-facing problem, an internal operations problem, or a business efficiency problem? Different stakeholders care about different outcomes, and making this explicit prevents later misalignment.

## Section Two: Business Context

This section answers: Why does this problem matter now? What is the opportunity cost of not solving it? What is the expected ROI or strategic value?

Business context grounds the technical work in real-world stakes. It might include market pressure, competitive dynamics, regulatory changes, or shifts in user behavior. For the fraud detection example, the business context might note that fraud losses increased thirty percent year-over-year, that competitors have deployed similar systems, and that manual review is not scaling with transaction volume.

This section also defines the success horizon. Is this a short-term tactical fix or a multi-year strategic initiative? What is the expected timeline from initial deployment to full rollout? What dependencies exist with other projects or org changes?

Including business context in the framing document does two things. It helps engineers and data scientists understand why this work is prioritized over other work. And it provides a reality check: if the business context is weak or unclear, that is a signal that the problem may not be worth solving yet.

## Section Three: User Needs

This section describes the end users of the system and what they need from it. Note that "users" might be internal or external. For the fraud detection system, users might include cardholders who receive fraud alerts, fraud analysts who review flagged transactions, and customer support reps who handle disputes.

User needs should be written as concrete scenarios, not abstract requirements. What are users trying to accomplish? What pain points do they experience with the current system? What would "better" look like from their perspective?

A common mistake is to describe what the system will do, rather than what users need. "The system will classify transactions as fraudulent or legitimate" is a system behavior, not a user need. "Cardholders need immediate notification of suspicious activity so they can freeze their card before more charges occur" is a user need.

This section should also acknowledge conflicts between different user groups. Cardholders want frictionless transactions; fraud analysts want aggressive flagging. Making these trade-offs explicit helps the team prioritize when conflicts arise during implementation.

## Section Four: Task Decomposition

This is where you break the high-level problem into specific AI tasks. For fraud detection, the task decomposition might include: binary classification for transaction risk scoring, anomaly detection for unusual spending patterns, natural language generation for fraud alert messages, and clustering for identifying fraud rings.

Each task should specify inputs, outputs, and success criteria. For binary classification: inputs are transaction metadata and user history, outputs are fraud probability scores, success criteria include precision and recall targets at different decision thresholds.

Task decomposition forces you to think about the system as a pipeline of distinct capabilities, not a monolithic "AI solution." This makes it easier to prototype components independently, to identify which parts need human oversight, and to plan the engineering work.

It also surfaces dependencies. If the alert generation task depends on the classification task producing interpretable scores, that dependency needs to be explicit. If the anomaly detection task requires historical user data that does not currently exist, that is a gap to address before implementation begins.

## Section Five: Success Criteria

This is the most important section. If you cannot measure success, you cannot determine if the system is working. Success criteria should include both business metrics and technical metrics, and they should specify acceptable ranges, not just directional goals.

For fraud detection, business metrics might include: fraud loss rate below X dollars per million transactions, false positive rate below Y percent, customer satisfaction with fraud alerts above Z score. Technical metrics might include: model precision and recall at chosen thresholds, inference latency below 100 milliseconds, system uptime above 99.9 percent.

Success criteria should also specify the evaluation protocol. How will you measure these metrics? What data will you use? What is the cadence for evaluation? Who is responsible for tracking and reporting?

Critically, success criteria must include failure modes. What are the unacceptable outcomes? For fraud detection: blocking legitimate transactions for high-value customers, failing to detect coordinated fraud attacks, leaking sensitive transaction data. Defining failure modes makes explicit what you are optimizing against, not just what you are optimizing for.

## Section Six: Input and Output Specifications

This section defines the data contracts for the system. What inputs does the system consume? What outputs does it produce? What format, schema, and quality requirements apply?

For fraud detection, input specifications might include: transaction amount, merchant category code, geolocation, device fingerprint, user transaction history for the past 90 days, time since last transaction. Output specifications might include: fraud risk score between 0 and 1, human-readable justification for score, recommended action such as approve or review or block, confidence interval for score.

Input and output specifications should be detailed enough that someone could implement a mock system or write integration tests without further clarification. They should also specify edge cases: what happens if input fields are missing, malformed, or outside expected ranges? What outputs are guaranteed vs optional?

This section prevents integration nightmares. If the fraud detection system expects transaction history but the payment processing system does not log it, that gap needs to be identified during framing, not during implementation.

## Section Seven: Constraints

Constraints define the boundaries within which the system must operate. They fall into several categories: performance, cost, privacy, compliance, and operational.

Performance constraints include latency requirements, throughput requirements, and accuracy requirements. For fraud detection: every transaction must be scored in under 100 milliseconds, the system must handle 10,000 transactions per second during peak load, false positive rate must stay below 1 percent.

Cost constraints include model inference cost, data storage cost, and human review cost. If the fraud detection system costs more to operate than the fraud losses it prevents, it is not a viable solution.

Privacy constraints include data access restrictions, retention policies, and anonymization requirements. If the system needs to access sensitive user data, how is that data protected? Who can access it? How long is it stored?

Compliance constraints include regulatory requirements, audit trails, and explainability requirements. For financial services, this might include GDPR, PSD2, Fair Lending regulations, and SOC 2 compliance. The compliance team should review this section before sign-off.

Operational constraints include deployment environment, integration points, and maintenance requirements. Does the system run in the cloud or on-premise? Does it integrate with existing fraud tools? Who is on-call when it breaks?

Constraints are not nice-to-haves. They are hard boundaries. If you cannot meet a constraint, you need to either change the approach or negotiate the constraint before proceeding.

## Section Eight: Non-Goals

This section is as important as the goals. Non-goals define what you are explicitly not building, not solving, and not optimizing for. They prevent scope creep and set expectations for stakeholders.

For fraud detection, non-goals might include: we are not replacing human fraud analysts, we are not handling account takeover or identity theft (only transaction fraud), we are not optimizing for fraud prevention on transactions below ten dollars, we are not building a customer-facing dispute resolution tool.

Non-goals should be specific enough to rule out concrete feature requests. "We are not building everything" is not a useful non-goal. "We are not expanding this system to cover ACH transfers, wire transfers, or cryptocurrency transactions in the initial version" is useful.

Non-goals also help with prioritization. When a stakeholder asks for a new feature, you can refer back to the non-goals section and ask: is this in scope or out of scope? If it is out of scope, does it justify revisiting the framing, or should it be deferred to a future version?

## Section Nine: Risk Assessment

This section identifies the risks that could cause the project to fail or underperform, and describes mitigation strategies.

Risks fall into several categories: technical, data, organizational, and market. Technical risks might include: model performance does not meet targets, inference latency exceeds constraints, integration with legacy systems fails. Data risks might include: insufficient training data, data quality issues, distribution shift between training and production data. Organizational risks might include: key stakeholders leave, budget gets cut, compliance requirements change mid-project. Market risks might include: competitors ship first, user needs shift, regulatory landscape changes.

For each risk, specify the likelihood, the impact if it occurs, and the mitigation plan. For example: "Risk: insufficient labeled fraud data. Likelihood: medium. Impact: high (cannot train supervised model). Mitigation: allocate budget for manual labeling, explore semi-supervised approaches, partner with fraud consortium for shared data."

Risk assessment is not about eliminating all risk. It is about making risks explicit so the team and stakeholders can make informed decisions about whether to proceed, how to allocate resources, and what contingency plans to prepare.

## Section Ten: Stakeholder Sign-Off

The final section is a sign-off sheet. List every stakeholder group that needs to approve the framing before implementation begins: product, engineering, data science, legal, compliance, finance, customer support, security.

For each stakeholder, specify what they are signing off on. The legal team is signing off on privacy and compliance constraints. The finance team is signing off on cost constraints and expected ROI. The product team is signing off on user needs and success criteria. The engineering team is signing off on feasibility and timeline.

Stakeholder sign-off serves two purposes. It ensures that all relevant perspectives are incorporated into the framing before resources are committed. And it creates shared accountability: if the project runs into trouble later, you can refer back to what everyone agreed to at the beginning.

In practice, getting sign-off often reveals misalignments that were not visible earlier. The legal team might flag a privacy constraint that invalidates the proposed approach. The finance team might balk at the expected cost. The product team might disagree with the success criteria. Better to surface these conflicts during framing than during implementation.

## What This Document Is Not

The framing document is not a Product Requirements Document. A PRD specifies user-facing features, UI workflows, and acceptance criteria for shipped functionality. The framing document is upstream of the PRD. It defines the problem and success criteria; the PRD defines the solution and feature set.

The framing document is not a Technical Design Doc. A design doc specifies architecture, APIs, data schemas, and implementation details. The framing document is intentionally implementation-agnostic. It describes what the system must accomplish and under what constraints, but it does not prescribe how.

The framing document is also not static. It evolves as you learn more about the problem, the data, and the feasibility of different approaches. The difference between a framing document and a specification is that the framing document is meant to be revised when reality contradicts assumptions. A good framing process includes regular checkpoints to revisit and update the document as the project progresses.

## Who Writes It and Who Reviews It

In the best teams, the framing document is co-authored by product, engineering, and data science. Product brings the user needs and business context. Data science brings the task decomposition and success criteria. Engineering brings the constraints and risk assessment.

The drafting process should be collaborative, not sequential. Do not have product write the problem statement and then hand it off to data science to fill in the task decomposition. Instead, schedule working sessions where all three groups hammer out the framing together. Misalignments surface faster in conversation than in review cycles.

Once a draft exists, it should be reviewed by every stakeholder group that will be affected by the system: legal, compliance, finance, security, customer support, operations. Each group reviews the sections relevant to their domain and flags concerns or gaps.

The final sign-off should happen in a single meeting where all stakeholders are present. This prevents the "telephone game" where each stakeholder signs off on a slightly different version of the document and later discovers conflicts.

## Using the Framing Document in Practice

Once the framing document is signed off, it becomes the source of truth for the project. When a stakeholder asks for a new feature, you refer back to the framing document: is this feature necessary to meet the success criteria, or is it scope creep? When an engineering decision requires a trade-off, you refer back to the constraints: which constraint is harder, latency or cost?

The framing document also provides the foundation for evaluation. If the success criteria are well-defined, you can build an evaluation harness that directly measures whether the system is meeting its goals. If the input and output specifications are clear, you can write integration tests before the system is fully implemented.

Finally, the framing document is the starting point for the next project. When you build version two, you do not start from scratch. You revisit the original framing, identify what changed, and update the document accordingly. Over time, this creates organizational memory: a record of what problems you have solved, what approaches worked, and what lessons you learned.

The framing document is not glamorous. It does not involve new models, clever algorithms, or impressive demos. But it is the artifact that separates teams that ship high-impact AI systems from teams that build impressive prototypes that never make it to production. In the next section, we will look at the failure patterns that emerge when teams skip this step or do it poorly.

