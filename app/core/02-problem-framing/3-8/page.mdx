# 3.8 â€” Compound Tasks and Task Chains

In mid-2025, a customer support platform launched an AI-powered question-answering feature. The marketing materials promised "instant, accurate answers to customer questions." The demo was impressive: ask a question about a product feature, get a well-written answer citing relevant documentation. The team had invested heavily in their retrieval system, tested their generation model thoroughly, and validated answer quality on hundreds of test questions. They felt confident.

Two weeks after launch, customer satisfaction with AI-generated answers was 67 percent, far below the 90 percent threshold for automated responses. The team was baffled. When they spot-checked answers, they looked correct. When they tested the retrieval system in isolation, it found the right documents. When they tested the generation model in isolation, it produced good answers from provided context. But when they examined the end-to-end system behavior, they discovered the problem: the retrieval system sometimes returned documents that were tangentially related but not directly relevant, and the generation model, trained to always produce an answer, would synthesize those tangential documents into a confident-sounding but incorrect response. Neither component was failing in isolation, but their interaction created a failure mode neither team had tested for.

This story illustrates why understanding compound tasks and task chains is essential for building reliable AI products. Real features almost never consist of a single isolated task. They chain multiple task types together, and the interactions between those tasks create failure modes that are invisible if you only evaluate components in isolation.

## The Anatomy of a Task Chain

Consider a seemingly simple question-answering feature. The user asks a question, the system returns an answer. From the user's perspective, this is a single interaction. But decompose it into tasks, and you find a complex chain.

First, classification: what type of question is this? Is it asking for facts, requesting a procedure, seeking an opinion, or something else? The answer determines which downstream components to invoke.

Second, retrieval: find information relevant to the question. This might involve searching documentation, querying a database, or calling an API. The quality of retrieval directly constrains the quality of the final answer: if the right information is not retrieved, the system cannot produce a correct answer.

Third, extraction: pull key facts from the retrieved information. Not every sentence in a retrieved document is relevant. The extraction task identifies which specific information should inform the answer.

Fourth, generation: synthesize the extracted information into a coherent answer. This involves paraphrasing, combining information from multiple sources, and formatting the response appropriately.

Fifth, classification again: confidence check. How confident is the system in this answer? Should it be shown to the user, flagged for review, or rejected?

Each of these tasks could be evaluated independently. You could measure classification accuracy, retrieval precision and recall, extraction F1 score, generation fluency, and confidence calibration. But if you only evaluate the tasks independently, you miss the critical question: does the chain as a whole produce correct answers?

This is the fundamental challenge of compound tasks: component-level quality does not guarantee system-level quality. The chain can fail even when every component works well in isolation.

## Why Chain Structure Matters for Evaluation

The structure of your task chain determines your evaluation strategy. Understanding the flow of information through the chain tells you where errors can be introduced, how errors propagate, and where evaluation should focus.

Sequential chains are the simplest structure: task A feeds into task B, which feeds into task C, and so on. Each task depends on the previous task's output. The evaluation challenge is that errors accumulate: if task A has 90 percent accuracy and task B has 90 percent accuracy, the combined accuracy is not 90 percent but 81 percent (0.9 times 0.9). With longer chains, this accumulation becomes severe.

Branching chains split the flow: task A produces output that feeds into both task B and task C, which then feed into task D. This structure appears in systems that process input through multiple parallel pipelines then combine results. The evaluation challenge is ensuring that all branches are tested and that the combination step correctly handles cases where branches disagree.

Conditional chains select which downstream task to invoke based on upstream outputs: task A classifies the input, and depending on the classification, either task B or task C is invoked. This structure requires evaluating each branch separately and ensuring that the routing logic is correct.

Feedback loops occur when a task's output can influence earlier tasks in the chain: task A produces output, task B evaluates it, and if task B rejects it, task A tries again. This structure creates complex dynamics where errors can either be corrected by the loop or amplified through repeated attempts.

Parallel chains with aggregation run multiple independent task chains and combine their outputs: multiple retrieval systems find different documents, multiple generation models produce different answers, and an aggregation step selects or synthesizes the best output. This structure requires evaluating each parallel chain and the aggregation logic.

Understanding your chain structure tells you which evaluation patterns to use. Sequential chains need end-to-end evaluation to measure cumulative errors. Branching chains need branch coverage testing. Conditional chains need evaluation of routing logic. Feedback loops need analysis of convergence and stability. Parallel chains need evaluation of the aggregation strategy.

## Error Propagation in Task Chains

The most insidious property of task chains is error propagation: a small error in an early task can cascade into a large error in the final output. Conversely, a high-quality later task can sometimes compensate for errors in earlier tasks. Understanding how errors propagate through your specific chain is essential for prioritizing improvements.

Consider the question-answering chain described earlier. If the classification task misidentifies the question type, it might invoke the wrong retrieval strategy. If retrieval is optimized for factual questions but the question is actually procedural, it might return documents about system architecture when it should return step-by-step guides. The extraction task then pulls facts from the wrong documents. The generation task synthesizes those facts into a fluent but incorrect answer. And the confidence check, not knowing that the upstream chain went wrong, assigns high confidence because the answer is fluent and coherent.

This is cascading failure: one error triggers downstream errors, and each stage amplifies the problem. By the time the error reaches the user, it is large and obvious, but the root cause was a subtle misclassification at the start.

Error propagation can also work in reverse. A high-quality generation model might be able to partially compensate for poor retrieval by recognizing that the retrieved documents are not quite relevant and hedging the answer appropriately. A robust extraction task might filter out misleading information from retrieved documents, preventing it from polluting the generated answer. Understanding these compensating factors helps you allocate effort: if downstream tasks can compensate for upstream errors, you might invest more in making those downstream tasks robust.

The key to managing error propagation is identifying the critical path: which tasks have errors that propagate most severely? For many chains, retrieval is the critical path. If the right information is not retrieved, no amount of clever generation can produce a correct answer. For other chains, classification is critical: if you route to the wrong pipeline, everything downstream fails. Identifying the critical path tells you where to invest in quality.

## The Weakest Link Principle

A common mental model for task chains is the weakest link principle: the overall system quality is limited by the worst-performing task in the chain. If your retrieval has 95 percent recall but your generation has 70 percent accuracy, your end-to-end system will be limited by generation, not retrieval.

This principle is useful but oversimplified. It assumes that tasks contribute equally to overall quality, which is rarely true. Some tasks are more critical than others. It also assumes that tasks do not compensate for each other, which, as discussed above, they sometimes do.

A more nuanced version of the principle is: system quality is limited by the worst-performing task on the critical path, accounting for how downstream tasks handle upstream errors. This mouthful captures the reality that some tasks matter more than others, and robustness to upstream errors is a valuable property.

The practical implication is that optimizing the wrong task wastes effort. If you spend months improving extraction from 85 percent to 95 percent F1, but retrieval only has 60 percent recall, the improvement in extraction will have minimal impact on end-to-end quality. The effort should have gone into retrieval.

Identifying the weakest link requires end-to-end evaluation combined with ablation studies: measure system performance with each task held at its current quality, then replace one task with a hypothetically perfect version and re-measure. The task whose perfection leads to the largest improvement in end-to-end quality is the current weakest link. This tells you where to focus improvement efforts.

## End-to-End vs. Per-Task Evaluation

The central evaluation tension for task chains is balancing end-to-end evaluation (measuring the whole system) with per-task evaluation (measuring individual components). Both are necessary, but they serve different purposes.

End-to-end evaluation measures what users experience. It tells you whether the overall system works, whether quality is acceptable, and whether the system is ready to deploy. End-to-end evaluation catches integration failures: cases where individual components work fine but their interaction produces errors. It also measures the cumulative effect of errors across the chain.

The limitation of end-to-end evaluation is that it does not tell you where failures originate. If an end-to-end test fails, you know the system produced a bad output, but you do not know whether the problem was in retrieval, extraction, generation, or some interaction between them. Debugging requires additional work to trace the error back to its source.

Per-task evaluation measures individual components in isolation. It tells you whether each task performs well on its specific sub-problem. Per-task evaluation enables root cause analysis: when an end-to-end test fails, you can check which task introduced the error. It also allows you to optimize tasks independently: improve retrieval without needing to re-evaluate the entire chain every time.

The limitation of per-task evaluation is that it does not capture interactions. A retrieval system that performs well on benchmark datasets might perform poorly when its outputs are fed into your specific generation model. A generation model that performs well on curated inputs might fail when given the messy, imperfect outputs of your actual retrieval system.

The best practice is to use both evaluation strategies in combination. End-to-end evaluation establishes whether the system is working and sets the quality bar. When end-to-end tests fail, per-task evaluation identifies root causes. When optimizing a specific task, per-task evaluation provides fast feedback, but you validate improvements with end-to-end tests before deployment.

Additionally, you need integration tests: evaluations that focus specifically on the handoffs between tasks. Does the retrieval output format match what the extraction task expects? Does the extraction task handle cases where retrieval returns zero results? Does the generation task appropriately handle low-confidence extractions? Integration tests catch the edge cases that fall between tasks.

## Identifying the Critical Path

For any task chain, some tasks are more important than others. The critical path is the sequence of tasks where errors have the largest impact on end-to-end quality. Identifying the critical path tells you where to focus evaluation and improvement efforts.

One approach to identifying the critical path is error injection: artificially degrade one task at a time and measure the impact on end-to-end quality. The task whose degradation causes the largest drop in end-to-end quality is on the critical path. For example, if you reduce retrieval recall from 90 percent to 80 percent and end-to-end accuracy drops from 85 percent to 70 percent, but reducing generation quality from 90 percent to 80 percent only drops end-to-end accuracy to 82 percent, then retrieval is more critical than generation.

Another approach is sensitivity analysis: measure how much end-to-end quality changes in response to small changes in each task's quality. Tasks with high sensitivity are on the critical path.

A third approach is manual inspection of failures. When end-to-end tests fail, trace back to find the root cause. If most failures trace back to retrieval, retrieval is on the critical path. If most failures trace back to generation, generation is critical.

Once you have identified the critical path, you can allocate evaluation resources accordingly. Tasks on the critical path deserve more rigorous evaluation, larger test sets, and more frequent monitoring. Tasks off the critical path can be evaluated more lightly, since their errors have less impact.

Importantly, the critical path can change over time. Early in development, retrieval might be the bottleneck. After you improve retrieval, generation becomes the bottleneck. After you improve generation, classification becomes the bottleneck. Regularly re-assess the critical path to ensure you are focusing on the right problems.

## Evaluation Strategy for Task Chains

Given the complexity of task chains, what is a practical evaluation strategy that balances thoroughness with feasibility?

First, decompose your feature into tasks. Use the decomposition techniques from Chapter 2 to identify every distinct task in your chain. For each task, specify its type (classification, retrieval, extraction, generation, transformation, reasoning), its inputs, its outputs, and its dependencies.

Second, establish per-task evaluation for every task in the chain. Define metrics, collect ground truth, and measure each task's performance in isolation. This creates a baseline: you know how well each component works when given ideal inputs.

Third, implement end-to-end evaluation on representative examples. Create test cases that cover common user scenarios, edge cases, and known failure modes. Measure end-to-end quality: does the system produce correct outputs for these cases?

Fourth, identify the critical path through error injection, sensitivity analysis, or failure analysis. Determine which tasks have the largest impact on end-to-end quality.

Fifth, add integration tests for critical handoffs. Focus on the interfaces between tasks on the critical path. Ensure that the output of one task is correctly consumed by the next, that edge cases are handled, and that errors are surfaced rather than silently propagated.

Sixth, implement error tracing. When an end-to-end test fails, you need a mechanism to trace the error back to its source. This might mean logging intermediate outputs, instrumenting the chain to record which task made which decision, or running ablation tests to isolate the failing component.

Seventh, monitor the critical path in production. Track per-task metrics for critical tasks and end-to-end metrics for the overall system. Set up alerts for degradation in either. Use the monitoring data to detect when the critical path shifts and re-prioritize accordingly.

Eighth, plan for iteration. Your task chain will evolve. You will add new tasks, remove old ones, and change how tasks interact. Keep your evaluation strategy in sync with the actual implementation. When the chain changes, update your tests, re-identify the critical path, and re-validate end-to-end quality.

This strategy ensures that you understand both component behavior and system behavior, that you can debug failures efficiently, and that you focus evaluation effort on the tasks that matter most.

## Task Chains in Production

When task chains fail in production, the failures are often subtle and hard to diagnose. A component might degrade slightly, causing a small increase in errors that propagates through the chain and emerges as a large quality drop. Or two components might interact in an unexpected way under a rare input pattern, causing intermittent failures.

The key to maintaining task chain quality in production is observability. You need visibility into every task in the chain: what inputs it received, what outputs it produced, how long it took, and how confident it was. With this visibility, you can trace failures back to their source and understand how errors propagate.

Instrumentation should capture both success and failure cases. When an end-to-end interaction succeeds, log the intermediate outputs so you can understand what correct operation looks like. When it fails, log the intermediate outputs so you can diagnose the root cause. Over time, this log data becomes a valuable resource for understanding system behavior and identifying patterns in failures.

Additionally, implement circuit breakers for critical tasks. If a task's error rate exceeds a threshold, the circuit breaker can route traffic to a fallback path or gracefully degrade functionality rather than allowing errors to propagate through the entire chain. This prevents cascading failures from reaching users.

Finally, run regular end-to-end regression tests in production. Use a small sample of live traffic to continuously validate that the system is working as expected. Compare current end-to-end quality to historical baselines. If quality drops, investigate which task in the chain degraded.

Task chains are complex, and that complexity creates fragility. But with disciplined decomposition, multi-level evaluation, and robust observability, you can build chains that are both powerful and reliable.

## The Compounding Effect of Quality

One final insight about task chains: quality compounds. A chain of high-quality tasks produces dramatically better results than a chain of mediocre tasks, and the difference is multiplicative, not additive.

If you have a three-task chain where each task has 95 percent accuracy, your end-to-end accuracy is approximately 85 percent. If you improve each task to 98 percent accuracy, your end-to-end accuracy jumps to approximately 94 percent. That small improvement in each component (3 percentage points) produces a large improvement in the overall system (9 percentage points).

Conversely, a chain with even one weak link suffers dramatically. If two tasks have 98 percent accuracy but one task has 85 percent accuracy, your end-to-end accuracy is approximately 82 percent, dragged down by the weak link.

This compounding effect means that incremental improvements in each task create outsized improvements in the overall system. It also means that neglecting any task in the chain creates disproportionate damage. The practical takeaway is that for task chains, "good enough" is not good enough. Every task needs to be high quality for the chain to work well.

---

Now that we understand task types and how they combine into chains, we turn to the strategic question: how do the nine product archetypes from Section 1 map to specific task profiles, and why does that mapping shape your entire evaluation approach?
