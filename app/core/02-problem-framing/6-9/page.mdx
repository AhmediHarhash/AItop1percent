# 6.9 — The Framing Review: A Team Exercise

In early 2025, a major e-commerce company launched a project to build an AI-powered product recommendation engine. The data science team spent six weeks on framing: they defined the problem, decomposed tasks, wrote detailed success criteria, and created comprehensive I/O specifications. They were proud of their work. The documents were thorough, well-structured, and technically sound. They sent the framing package to stakeholders — product, engineering, legal, and leadership — and asked for feedback. Two weeks later, they had received no responses. They followed up. Product said they had not had time to review it. Legal said the documents were too technical to understand. Leadership said they assumed engineering had it covered. Nobody had actually read the framing documents.

The team proceeded with development. Three months later, when they were ready to launch, product raised concerns that the recommendation engine was optimizing for engagement rather than revenue, which was not aligned with the business goal. Legal flagged that the system was using purchase history data in ways that might violate user privacy expectations. Leadership questioned why the team had built ten features when the original scope was supposed to be three. All of these issues were addressed in the framing documents, but because nobody had reviewed them, the misalignments went undetected until it was too late to fix cheaply.

The problem was not the quality of the framing work. The problem was that framing documents sitting in a shared folder are not the same as stakeholder alignment. Stakeholder alignment requires **active review**: a structured session where key stakeholders walk through the framing artifacts together, ask questions, surface concerns, and commit to the plan. Without that session, framing is just a document that people may or may not read.

This subchapter provides a detailed format for running a framing review: a two-hour team exercise designed to ensure that all key stakeholders understand, agree with, and commit to the framing before building begins.

## Why a Framing Review Is Not Optional

Framing review is the last cheap checkpoint before you commit significant resources to building. It is the moment where abstract plans meet organizational reality. It serves three purposes:

**First, it ensures completeness**. Walking through framing artifacts with stakeholders often reveals gaps that were invisible when you were writing them. Someone will ask a question you did not think of. Someone will point out a constraint you missed. Someone will identify a task you forgot to decompose. These discoveries are cheap to address during review. They are expensive to address during evaluation or production.

**Second, it ensures alignment**. Alignment is not the same as approval. Approval is when someone says "looks good" without reading the details. Alignment is when someone understands what you are proposing, agrees with it, and commits to supporting it. Alignment only happens when stakeholders engage with the details, which requires a structured conversation, not an email.

**Third, it ensures accountability**. At the end of a framing review, you get explicit sign-off from stakeholders. They cannot claim later that they did not know what you were building or that they never agreed to the plan. Sign-off creates shared responsibility. If the framing is wrong, it is not just the engineering team's failure — it is a shared failure across all stakeholders who signed off.

## The Framing Review Format: A Two-Hour Session

The review session is two hours long. Two hours is enough to go deep on every framing artifact without exhausting people's attention. Longer sessions lose focus. Shorter sessions force you to skip important details.

**Participants**: The review includes all key stakeholders: engineering lead, product lead, domain expert representative, legal or compliance representative, and leadership sponsor. Optionally, include design or UX lead if the system has significant user-facing components. The total group should be six to ten people. Smaller groups move faster. Larger groups become unmanageable.

**Format**: The session is structured into six 20-minute segments, each focused on a specific framing artifact. The DRI facilitates the session, presenting each artifact and leading the discussion. Stakeholders ask questions, raise concerns, and suggest changes. The session is collaborative, not a presentation.

**Output**: A signed-off framing package with documented action items for any gaps identified. The package is considered final once action items are resolved and stakeholders re-confirm sign-off.

Here is the detailed structure.

## Segment 1: Problem Statement and Task Decomposition (20 Minutes)

**What to cover**: Walk through the problem framing spec. Present the problem statement, business context, user needs, constraints, and non-goals. Then present the task registry, showing how you decomposed the problem into tasks.

**Questions to ask**:
- Does this problem statement accurately reflect what we are trying to solve?
- Is the business context aligned with our strategic priorities?
- Are user needs validated with actual user research, or are we making assumptions?
- Are the constraints realistic? Are there other constraints we missed?
- Do the non-goals make sense? Is there anything in the non-goals that should actually be in scope?
- Does the task decomposition cover all the work the system needs to do?
- Are there tasks we missed? Are there tasks that should be combined or split?

**Red flags to watch for**:
- Stakeholders who say "I did not realize we were solving this problem" — indicates misalignment on scope.
- Constraints that stakeholders dispute — "we do not actually need to integrate with that system" or "we can get more budget."
- Non-goals that someone quietly disagrees with — they nod along but you can tell they think it should be in scope.
- Tasks that domain experts do not recognize — "we do not actually do it that way."

**Outcome**: Confirm that everyone agrees on what problem you are solving, why it matters, and how you have broken it down. Document any disagreements or gaps as action items.

## Segment 2: Success Criteria (20 Minutes)

**What to cover**: Walk through the success criteria matrix. For each major task, present the functional correctness criteria, behavioral criteria, performance criteria, and negative criteria. Highlight the thresholds you have defined and explain how you will measure each criterion.

**Questions to ask**:
- Are these criteria measurable? Can we actually evaluate these in practice?
- Are the thresholds realistic? Too strict? Too loose?
- Are we missing any criteria that matter? What could go wrong that we are not testing for?
- Do these criteria align with what users care about, or are they purely technical?
- Can we explain to a non-technical stakeholder why these criteria matter?

**Red flags to watch for**:
- Criteria that nobody can explain how to measure — "we will evaluate helpfulness" without a clear method.
- Thresholds that domain experts think are unrealistic — "99% accuracy is impossible for this task."
- Criteria that product or users would not care about — "we optimize for token efficiency" when users care about response quality.
- Missing negative criteria — nobody has thought about what the system must NOT do.
- Criteria that conflict with constraints — "we require real-time responses" but the constraint says batch processing is acceptable.

**Outcome**: Confirm that everyone agrees on what success looks like and how it will be measured. Document any criteria that need to be added, removed, or refined as action items.

## Segment 3: I/O Specifications (20 Minutes)

**What to cover**: Walk through the I/O specification for each major task. Show input schemas, output schemas, edge cases, uncertainty handling, and tool contracts. Provide examples of real inputs and expected outputs.

**Questions to ask**:
- Are the input and output schemas complete? Are we missing any fields?
- Are the schemas aligned with our existing systems and data formats?
- Have we covered the major edge cases? What else could go wrong?
- Is the uncertainty handling strategy clear? What happens when the system is not confident?
- Are tool contracts defined for all external integrations? What happens if a tool fails?

**Red flags to watch for**:
- I/O specs that do not match what engineering plans to build — "this schema does not match our database structure."
- Missing edge cases — "what happens if the user inputs an empty string?" and nobody knows.
- Vague uncertainty handling — "we will handle it somehow."
- Tool contracts that assume external systems will always work — no timeout, no fallback, no error handling.
- Inconsistency between I/O specs and success criteria — criteria reference fields that are not in the output schema.

**Outcome**: Confirm that I/O specs are precise enough to implement and evaluate. Document any missing fields, edge cases, or tool contracts as action items.

## Segment 4: Constraints and Non-Goals (20 Minutes)

**What to cover**: Revisit constraints and non-goals in detail. Walk through each constraint and explain how it shapes the system design. Walk through each non-goal and explain why it is out of scope.

**Questions to ask**:
- Are these constraints hard or soft? Can we negotiate any of them?
- Are there hidden constraints we have not documented? Organizational politics, legacy system limitations, vendor lock-in?
- Are the non-goals truly out of scope, or are they just deferred?
- Is there alignment across stakeholders on what is in scope versus out of scope?

**Red flags to watch for**:
- Constraints that are aspirational, not real — "we should integrate with system X" but nobody has checked if it is technically feasible.
- Non-goals that someone quietly believes should be goals — they do not speak up now but will push for them later.
- Lack of clarity on hard versus soft constraints — "we need to launch in Q2" but nobody knows if that is a business requirement or a rough target.

**Outcome**: Confirm that constraints are realistic and that everyone agrees on what is out of scope. Document any constraints that need validation or non-goals that need reconsideration as action items.

## Segment 5: Evaluation Readiness (20 Minutes)

**What to cover**: Walk through the eval readiness checklist. For each item, state whether it is currently true or false. For items that are false, explain the plan to make them true.

**Questions to ask**:
- Do we have the data we need for evaluation? Labeled examples, ground truth, production logs?
- Is the system far enough along that evaluation will be meaningful, or are we too early?
- Do we have the evaluation tooling in place? Automated metrics, human evaluation interface, LLM-as-judge prompts?
- Is the evaluation team trained and ready?
- Have we defined the triage process for when evaluation uncovers failures?

**Red flags to watch for**:
- Eval readiness checklist items that are marked "yes" but stakeholders do not believe they are actually true.
- Missing data — "we assume we can label examples quickly" without validating that assumption.
- Evaluation that depends on external resources that are not secured — "we will hire contractors to evaluate" but no budget approved yet.
- No triage process defined — "we will figure it out when failures happen."

**Outcome**: Confirm that the team is ready to start evaluation or identify the gaps that must be closed first. Document missing prerequisites as action items with owners and deadlines.

## Segment 6: Open Questions and Action Items (20 Minutes)

**What to cover**: Open the floor for any questions, concerns, or gaps that were not covered in the previous segments. Triage all action items collected during the session.

**Questions to ask**:
- What are we missing?
- What could go wrong that we have not discussed?
- What assumptions are we making that we should validate?
- Is there anything that makes you uncomfortable or uncertain about this plan?

**Outcome**: A final list of action items, each with an owner and a deadline. Agreement from all stakeholders on next steps.

## Getting Explicit Sign-Off

At the end of the session, the DRI asks for explicit sign-off from each stakeholder. Sign-off is not passive — it requires each person to say out loud or in writing that they agree with the framing and commit to supporting it.

The DRI asks each stakeholder: "Based on this framing and the action items we have identified, are you aligned? Do you commit to supporting this plan?" Each stakeholder answers yes or no. If anyone says no, you discuss why and what needs to change for them to say yes.

Sign-off is documented. The DRI sends a follow-up email summarizing the session, listing the action items with owners and deadlines, and confirming who signed off. This creates a written record that prevents future disputes.

## How to Run the Review Remotely (2026 Reality: Distributed Teams)

In 2026, most teams are distributed. Running a framing review remotely requires extra structure to keep people engaged.

**Use video**: Audio-only calls lose too much signal. You need to see people's faces to gauge whether they are confused, skeptical, or disengaged.

**Use shared documents**: Put the framing documents in a shared tool (Google Docs, Notion, Miro) that everyone can see during the session. As you discuss each section, highlight it in the doc so everyone is looking at the same thing.

**Use breakout rooms for large groups**: If you have more than eight people, use breakout rooms to discuss specific sections in smaller groups, then reconvene to share findings.

**Record the session**: Record the session and share it with stakeholders who could not attend. But make clear that the recording is a reference, not a substitute for participation — key stakeholders must attend live.

**Use polls or reactions**: Use Zoom polls or Slack reactions to get quick feedback. "Does everyone agree with this constraint? React with thumbs up for yes, thumbs down for no." This surfaces disagreement quickly.

## The Follow-Up: Tracking Action Items to Completion

The framing review is not over when the meeting ends. It is over when all action items are resolved and stakeholders re-confirm sign-off.

The DRI owns tracking action items. Within two days of the review session, the DRI sends a summary email with the action item list. Each action item includes: what needs to be done, who owns it, and the deadline.

The DRI checks in with action item owners weekly until all items are closed. When items are resolved, the DRI updates the framing documents and shares the updated version with stakeholders.

Once all action items are closed, the DRI sends a final sign-off request: "All action items from the framing review are now complete. Updated framing documents are attached. Please confirm your sign-off." Stakeholders reply with confirmation.

Only then is the framing review complete, and the team is cleared to proceed to evaluation and building.

## Why This Process Prevents Expensive Surprises

The framing review catches issues that email review misses. In an email review, stakeholders skim the documents, maybe leave a comment or two, and move on. They do not deeply engage. In a live session, they cannot hide. They have to answer questions. They have to explain their concerns. They have to commit publicly.

This process also forces clarity on action items. In email threads, action items get lost. In a live session with a DRI capturing them in real time, nothing gets lost. Every gap has an owner and a deadline.

Finally, this process creates shared accountability. When ten people sit in a room and sign off on a plan, they all own it. If it fails, they cannot blame the engineering team. They were in the room. They agreed. They share responsibility.

## Practical Takeaways

**Run a framing review before building begins**: This is a mandatory checkpoint, not optional.

**Make it a two-hour structured session**: Six segments, 20 minutes each, covering problem statement, task decomposition, success criteria, I/O specs, constraints, and eval readiness.

**Invite all key stakeholders**: Product, engineering, domain experts, legal, leadership. Keep the group to six to ten people.

**Ask hard questions**: Do not just present the framing. Challenge it. Surface concerns. Find gaps.

**Get explicit sign-off**: Each stakeholder must say yes out loud or in writing.

**Track action items to completion**: The review is not over until all gaps are resolved.

**Run it remotely if needed**: Use video, shared docs, and structured engagement to keep distributed teams focused.

The framing review ensures that your framing work is not just technically sound, but also organizationally aligned. But even the best framing can fail if you do not have the data to execute on it — that is the next checkpoint: the data feasibility assessment.
