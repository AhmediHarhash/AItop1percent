# 3.3 — Generation Tasks in Depth

A marketing agency deployed an AI system to write email subject lines for their clients' campaigns in late 2025. The system was fast, creative, and produced hundreds of variations. The team loved it. They evaluated it by comparing outputs to examples written by their senior copywriters. The similarity scores looked good. The outputs sounded professional. They shipped it.

Three months later, they had data. The AI-generated subject lines had lower open rates than human-written ones. Not catastrophically lower—just consistently underperforming by three to five percentage points. That difference translated to significant revenue impact at scale.

The puzzling part: the AI subject lines looked good. They were grammatically correct, relevant to the campaign, and creative. Side-by-side, they were hard to distinguish from human-written lines. The evaluation metrics had been positive. The stakeholders had approved the outputs. So what went wrong?

The team eventually realized their evaluation had missed the actual goal. They had been measuring whether the AI outputs looked like good copywriting. They should have been measuring whether the outputs would make people click. Those are different questions. The first is about matching a style. The second is about achieving an outcome. Generation tasks are uniquely vulnerable to this trap: creating outputs that look correct but miss the deeper objective.

This is the central challenge of generation tasks. The output space is enormous. Quality is multidimensional. Multiple outputs can be correct. And "correct" itself is often the wrong standard—you need "effective" or "appropriate" or "engaging," which are subjective and context-dependent.

## The Spectrum of Generation Tasks

Not all generation tasks are created equal. They exist on a spectrum from highly constrained to completely open-ended, and this spectrum determines how hard they are to evaluate.

At the constrained end, you have templated generation: filling in structures with specific information. Writing a standard business email, populating a form letter, generating a product description that follows a strict format. These tasks have clear structure requirements, specific information that must be included, and relatively little creative freedom. Evaluation is easier because you can check for required elements, verify factual accuracy, and assess whether the template is followed correctly.

Moving toward the middle, you have bounded generation: creating content within defined parameters but with creative flexibility. Writing a summary of a news article, generating an answer to a customer question, drafting a bug report from user feedback. These tasks have constraints (stay relevant, be accurate, maintain appropriate length) but allow variation in how you meet those constraints. Evaluation becomes harder because you're balancing multiple quality dimensions and accepting different approaches as valid.

At the open end, you have free-form generation: creating original content with minimal constraints. Creative writing, brainstorming ideas, generating marketing concepts, writing opinion pieces. These tasks have enormous creative freedom. Quality depends heavily on subjective factors like creativity, engagement, and originality. Evaluation is hardest because there's no clear ground truth and quality judgments vary significantly between evaluators.

The 2026 reality is that most production generation systems operate in the bounded space. Completely open-ended generation is too risky for most business applications. Pure template filling doesn't need modern LLMs. The sweet spot is tasks that need flexibility and intelligence but within defined guardrails: answer customer questions accurately but in a friendly tone, summarize documents concisely but preserve key details, generate code that solves the problem but follows style guidelines.

Understanding where your generation task falls on this spectrum tells you what evaluation approach makes sense. The more constrained your task, the more you can rely on objective criteria and automated checks. The more open-ended your task, the more you need human judgment and rubric-based assessment.

## Types of Generation Tasks

Let's map out the common categories of generation tasks you'll encounter in production AI systems, understand their distinct characteristics, and identify what makes each one challenging to evaluate.

**Open-ended generation** creates content with minimal constraints. This includes creative writing, idea generation, open-ended conversation, and exploratory content creation. The defining characteristic is that the output is not fully specified by the input. The same prompt can validly produce radically different outputs. Quality criteria are subjective: creativity, originality, engagement, emotional impact. Evaluation requires human judgment against high-level quality rubrics. This is the hardest type to evaluate reliably.

**Constrained generation** produces content within defined boundaries. This includes summarization (stay accurate, hit target length), question answering (relevant, complete, accurate), product descriptions (highlight key features, match brand voice), and technical documentation (clear, accurate, complete). The constraints limit the valid output space but don't specify a unique answer. Quality criteria blend objective factors (accuracy, completeness) and subjective factors (clarity, tone). Evaluation combines automated checks for objective criteria and human or LLM judgment for subjective criteria.

**Templated generation** fills in structured content patterns. This includes form letters, standardized reports, structured emails, and format-driven content. The template defines most of the structure. The generation task is selecting or creating the variable elements. Quality criteria are mostly objective: are required fields filled correctly? Is the format followed? Is the information accurate? Evaluation can be largely automated with schema validation, field verification, and format checking.

**Code generation** produces executable code from natural language descriptions. This is a specialized generation type with unique characteristics. The output must be syntactically valid. It must solve the specified problem. It should follow style and best practice guidelines. Quality criteria include correctness (does it work?), efficiency (does it perform well?), maintainability (is it readable and well-structured?), and security (does it avoid vulnerabilities?). Evaluation combines automated testing (does it run? does it pass tests?) and code review (is it well-written?).

**Conversational generation** produces responses in dialogue. This includes chatbot responses, virtual assistant interactions, and conversational AI systems. The generation must be contextually appropriate, taking into account conversation history and user intent. Quality criteria include relevance, coherence, helpfulness, appropriate tone, and factual accuracy. Evaluation is complex because you're assessing not just individual responses but conversation flow and context maintenance.

**Explanation generation** produces reasoning explanations or justifications. This includes explaining AI decisions, generating step-by-step solutions, creating educational content, and providing rationales for recommendations. The generation must accurately represent the underlying logic or reasoning. Quality criteria include accuracy (does the explanation match the actual process?), clarity (is it understandable?), completeness (does it cover the key points?), and pedagogical effectiveness (does it help the user understand?). Evaluation requires verifying that explanations are correct and assessing whether they're clear and useful.

## What Makes Generation Uniquely Challenging

Generation tasks are the hardest to evaluate for several interconnected reasons. Understanding these challenges helps you design evaluation approaches that address them.

**The many-correct-answers problem**: For most generation tasks, many outputs can be equally good. Ask ten people to summarize the same article, and you'll get ten different summaries. Many might be equally valid, just emphasizing different aspects or using different language. This means you can't evaluate generation by comparing to a single reference answer. You need evaluation approaches that recognize multiple valid outputs.

**The quality-is-multidimensional problem**: Good generation isn't a single property. It's a combination of fluency, coherence, relevance, accuracy, completeness, creativity, appropriateness, engagement, and other dimensions. Different tasks weight these dimensions differently. An informative summary needs accuracy and completeness more than creativity. A marketing tagline needs creativity and engagement more than comprehensive information. You need evaluation that captures multiple dimensions rather than reducing to a single score.

**The context-dependence problem**: Whether a generated output is good depends on context. The same email response might be perfect for one customer and terrible for another, depending on their relationship with your company, the nature of their issue, and their communication style. The same product description might work for one audience and fall flat with another. Context-independent evaluation misses these nuances.

**The no-clear-ground-truth problem**: In extraction or classification, you can verify against objective fact. In generation, there often isn't an objective fact to verify against. What's the "correct" marketing tagline? What's the "right" way to explain a technical concept? These questions don't have definitive answers. They have better and worse approaches, but the judgment is subjective.

**The plausibility-versus-correctness problem**: Modern language models are remarkably good at generating plausible-sounding text. Fluent, coherent, confident-sounding outputs that are completely wrong. This is especially dangerous in generation because the output looks right. Unlike extraction where a wrong number stands out, or classification where a category is clearly wrong, generation errors can hide in professional-sounding prose. Evaluation must distinguish plausible from correct.

**The emergent-failure-modes problem**: Generation can fail in subtle ways that compound over time. Tone drift, where the style slowly shifts away from guidelines. Repetition, where the system reuses the same phrases. Degrading specificity, where responses become more generic. These failures emerge gradually and aren't caught by example-by-example evaluation. You need monitoring that tracks quality trends over time.

## Evaluation Strategies for Generation

Given these challenges, how do you actually evaluate generation tasks? The answer is: multiple approaches, combined, tailored to your specific task and constraints.

**Reference-based automated metrics** compare generated outputs to reference examples. BLEU, ROUGE, METEOR, and BERTScore fall into this category. These metrics measure similarity in different ways: word overlap, n-gram overlap, semantic similarity. They're useful for tasks where staying close to reference style matters (translation, summarization). They're less useful for creative tasks where variation is desirable. They're fast and cheap to compute but correlate imperfectly with human quality judgments. Use them as rough signals, not as primary evaluation.

**Rubric-based human evaluation** has humans rate outputs against defined quality criteria. You create a rubric listing the quality dimensions that matter for your task. For each dimension, you define rating levels with clear descriptions. Evaluators assess each output on each dimension. This captures multidimensional quality and incorporates human judgment. The challenge is cost and scale. Human evaluation is expensive and slow. Use it for establishing ground truth, validating automated approaches, and evaluating high-stakes or novel outputs.

**LLM-as-judge evaluation** uses a language model to evaluate outputs from your production model. You give the judge model the input, the generated output, and evaluation criteria or rubrics. The judge model rates the output. This scales better than human evaluation and can capture nuanced quality dimensions that automated metrics miss. The challenge is judge model quality and potential biases. Use it for scaled evaluation of quality dimensions that are hard to capture with simple metrics. Validate your judge model against human judgments to ensure it's reliable.

**Task-specific automated checks** verify objective properties of generated outputs. For code generation, run the code and check if it works. For structured output generation, validate against the schema. For factual generation, check facts against knowledge bases. For summarization, verify that key entities from the source appear in the summary. These checks don't assess overall quality, but they catch specific failure modes reliably. Use them as guardrails and quick filters.

**User outcome metrics** measure what happens after generation. For email subject lines, track open rates. For chatbot responses, measure user satisfaction or conversation success. For marketing copy, track conversion. For code generation, measure whether developers accept or modify the generated code. These metrics tell you whether your generation achieves its actual goal. The challenge is that outcomes are delayed and influenced by many factors beyond generation quality. Use them as the ultimate validation but not as the only evaluation.

**A/B testing in production** compares different generation approaches with real users. This is the gold standard for understanding what works. Deploy variation A to half your users and variation B to the other half. Measure outcomes. Learn what's actually better. The challenge is that you need real users, real stakes, and enough volume for statistical significance. Use it to make final decisions between approaches that pass your other evaluation checks.

## Ground Truth for Generation Tasks

Collecting ground truth for generation is fundamentally different from other task types. You're not labeling facts. You're creating reference examples that demonstrate quality.

The collection process starts with hiring the right annotators. For generation ground truth, you need people who can produce high-quality outputs, not just label existing content. If you're building a copywriting system, you need skilled copywriters. If you're building a code generation system, you need experienced developers. If you're building a summarization system, you need people who write clear, accurate summaries.

You give annotators inputs and ask them to produce outputs. But you don't just collect one output per input. You often collect multiple outputs from different annotators to capture the range of valid approaches. You might collect ratings or rankings to identify which outputs are better. You might collect explanations of why certain outputs are high quality.

You need detailed guidelines that explain what makes a good output for your specific task. The guidelines cover the quality dimensions that matter: tone, style, level of detail, inclusion criteria, accuracy requirements, formatting expectations. The guidelines include examples of good outputs and bad outputs with explanations of what makes them good or bad.

Quality control is critical. You review the collected outputs to ensure they meet your quality bar. You measure inter-annotator agreement where possible. You provide feedback to annotators and iterate on guidelines. Poor quality reference examples will mislead your evaluation system.

The volume you need depends on your task and your evaluation approach. If you're using supervised fine-tuning, you might need thousands of examples. If you're using reference examples for automated metric comparison, you might need hundreds. If you're using them as demonstrations for few-shot prompting, you might need dozens of very high-quality examples.

The cost is higher than other ground truth types because you're paying for creative work, not just labeling. Budget accordingly. Consider the ROI: high-quality reference examples enable better evaluation, which enables building better systems, which creates more user value.

## The 2026 Context: When Fluency Isn't Enough

In 2026, the generation landscape has shifted dramatically from even two years ago. Models are fluent by default. Coherence is no longer a challenge. The outputs sound good. They read well. They look professional.

This changes what you need to evaluate. In the GPT-3 era, you had to worry about coherence and fluency. In the GPT-4 era, you worried more about factual accuracy and relevance. In 2026, with Claude Opus 4.5 and comparable models, fluency and coherence are table stakes. The differentiation is in correctness, appropriateness, and effectiveness.

Your evaluation must go deeper than surface quality. Is the output factually correct? Check facts against authoritative sources. Is it appropriate for the context? Verify that tone, style, and level of detail match the use case. Is it effective at achieving the goal? Measure outcomes and user satisfaction.

The risk in 2026 is over-relying on how good outputs look. They all look good. The question is whether they're right and whether they work. Design your evaluation to answer those questions, not just to confirm that the text sounds professional.

The opportunity in 2026 is that generation is good enough for production use in domains that were too risky before. Customer-facing communication, technical documentation, code assistance, content creation—these are all viable applications. But viable doesn't mean easy. The evaluation challenge has shifted from "can the model write coherent text?" to "is the model's text achieving our objectives?"

That shift demands evaluation systems that measure what matters, not just what's easy to measure. That's what we'll explore as we continue building your understanding of how different task types demand different evaluation approaches.

Next, we'll examine extraction and structuring tasks, where the challenge is very different: the answer exists in the input, but finding it precisely is harder than it looks.
