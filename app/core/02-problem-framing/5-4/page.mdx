# 5.4 — Output Specification: What the System Produces

In November 2024, a legal tech startup demoed their AI contract analysis tool to a prestigious law firm. The demo was impressive. The AI read a fifty-page commercial contract and extracted key terms in seconds. The partners were amazed. They signed a pilot agreement on the spot.

Three months into the pilot, the firm cancelled the contract.

What went wrong? The AI worked exactly as advertised. It extracted party names, dates, payment terms, liability clauses, termination provisions—everything the demo promised. But when the firm's associates tried to use the extracted data, they discovered problems.

Party names were inconsistent. Sometimes "International Business Machines Corporation" appeared as "IBM" and sometimes as "International Business Machines" and sometimes as "IBM Corporation." Dates were formatted differently across fields—some ISO 8601, some US format, some European format. Payment terms were unstructured text strings that required manual parsing. Liability caps were numbers without currency indicators. Some required fields were missing with no explanation.

The associates spent more time cleaning and validating the AI's output than they would have spent reviewing the contracts manually. The firm's IT team couldn't integrate the data into their contract management system because the schema kept changing. The partners felt misled. The startup scrambled to fix issues that should never have made it to production.

The root cause was not model accuracy. The AI was correctly identifying terms. The root cause was the absence of an output specification. Nobody had defined exactly what format the output should take, what fields were required versus optional, what data types to use, how to handle missing information, or what schema the downstream systems expected.

The demo showed that the AI could extract information. It did not show that the output was usable. There is a crucial difference.

## Structured versus Unstructured Outputs

The first and most important decision in output specification is whether your system produces structured data, unstructured data, or a mix of both. This decision shapes everything else.

**Structured outputs** are data that conform to a predefined schema. They have field names, data types, validation rules, and nesting structures. Examples include: a JSON object with defined fields, a database row with typed columns, an XML document with a fixed schema, form data with specified types, classification labels with confidence scores. Structured outputs are machine-readable. They can be validated automatically. They can be fed directly into downstream systems or APIs.

**Unstructured outputs** are natural language or media that do not conform to a fixed schema. Examples include: conversational responses, generated articles or documents, summaries of retrieved information, explanations of decisions, creative content like marketing copy. Unstructured outputs are human-readable. They are flexible and expressive. But they are hard to validate automatically and difficult to integrate with downstream systems.

**Mixed outputs** combine both. A conversational AI might return natural language text plus structured metadata like confidence scores, source citations, and suggested actions. A document generation system might produce formatted text plus structured fields like title, author, date, and category. Mixed outputs are common in modern AI systems, but they require careful specification of both the structured and unstructured components.

The legal tech startup's problem was treating structured outputs as if they were unstructured. They extracted fields from contracts—a structured task—but they didn't enforce a schema. Each contract produced slightly different output. Field names varied. Data types were inconsistent. This made the output unusable for integration.

If your output is structured, you need a formal schema. If your output is unstructured, you need format requirements and structural expectations. If your output is mixed, you need both.

## Specifying Structured Outputs

For any system that produces structured data, the output specification must include a complete schema definition. Let me break down what that means.

**Field names** must be specified exactly. Not "party name" and "name of party" and "contract party" used interchangeably. Pick one canonical name and use it consistently. For the legal contract example: party_name, effective_date, termination_date, payment_amount, payment_currency, payment_frequency, liability_cap, governing_law. Every field name must be unambiguous.

**Data types** must be specified for every field. Is payment_amount an integer, a float, or a string? Is effective_date a timestamp, an ISO 8601 string, or a human-readable date? Is party_name a single string or an array of strings if there are multiple parties? Different types require different validation and different parsing logic downstream. Specify them up front.

**Required versus optional** must be defined per field. Which fields must always be present? Which fields can be null or missing if the source data doesn't contain them? For a contract, party names and effective date might be required because every contract has them. Liability cap might be optional because not all contracts include one. Define this explicitly so downstream systems know what they can depend on.

**Validation rules** must be specified for fields with constraints. If payment_amount must be positive, say so. If effective_date must be before termination_date, specify that validation rule. If party_name must not be empty, define the constraint. These rules should be enforced before the output leaves the system so downstream systems receive only valid data.

**Nesting and relationships** must be defined for complex structures. If a contract has multiple payment schedules, is payment_schedule a single object or an array? If obligations have sub-obligations, how is that represented? Nested structures need schema definitions at every level, not just the top level.

**Enumerations and controlled vocabularies** must be defined for fields with limited valid values. If contract_type can only be "purchase agreement," "service agreement," "NDA," or "employment contract," specify the enumeration. Do not allow free-text values like "PA" or "services agreement" that are semantically equivalent but string-different. Enumerations make validation and downstream processing much simpler.

**Default values** must be specified for optional fields when a default makes sense. If payment_currency is optional but defaults to USD when not specified, say so. If confidence scores default to 0.0 when the field is not extracted, specify that. Defaults reduce ambiguity about missing values.

**Error values** must be defined for cases where extraction fails. Is a failed extraction represented as null, as an empty string, as a special error code, or as an object with an error message? Different downstream systems have different conventions. Pick one and document it.

For the legal contract system, a proper output schema would look like this:

```
party_names: array of strings, required, at least one element, no duplicates
effective_date: ISO 8601 date string, required
termination_date: ISO 8601 date string or null, optional
payment_terms: object with fields amount (float), currency (ISO 4217 code), frequency (enum: one_time, monthly, quarterly, annually), required if contract includes payment
liability_cap: object with fields amount (float), currency (ISO 4217 code), optional
governing_law: string, jurisdiction name, optional
confidence_scores: object with per-field confidence (float 0.0 to 1.0), required
```

This is a proper structured output specification. Someone could implement validation logic from this. Someone could design a database schema. Someone could write integration code. There is no ambiguity.

## Specifying Unstructured Outputs

Unstructured outputs are harder to specify because they are not machine-parseable. But that does not mean you can skip specification. You still need to define format, length, structure, and tone.

**Format** specifies whether the output is plain text, markdown, HTML, or some other format. This matters for rendering. If your frontend expects markdown but the model produces plain text, formatting will be broken. If the output will be displayed in different contexts—web, mobile, email—you may need multiple format variants. Specify which format applies where.

**Length constraints** define minimum and maximum length. Is the output a one-sentence summary or a three-paragraph explanation? Is it 50 words or 500 words? Length constraints help control output quality and fit the UI. A conversational AI that sometimes responds with one word and sometimes with five paragraphs feels inconsistent. Specify the expected range.

**Structural expectations** define the shape of the output even if it's not formally structured. For a natural language explanation, you might specify: one-sentence introduction, two to three paragraphs of detail, one-sentence conclusion. For a marketing email, you might specify: subject line under 60 characters, greeting, body with three key points, call-to-action, sign-off. These structural requirements give shape to unstructured text.

**Tone and style** should be specified if they matter. Is the output formal or casual? Technical or accessible? Concise or detailed? Empathetic or neutral? Tone is not a technical constraint, but it affects user perception. If the tone is inconsistent—sometimes formal, sometimes casual—users find the experience jarring. Specify the tone so it can be evaluated.

**Citations and attribution** should be specified if the output includes factual claims. Should the output cite sources? Should it include links? Should it quote directly or paraphrase? How should it handle claims it cannot verify? These are output requirements, not just model behaviors. Define them.

**Formatting conventions** should be specified for things like lists, headers, emphasis, and whitespace. Should lists be bulleted or numbered? Should key terms be bolded? Should paragraphs be separated by line breaks? These details seem minor, but inconsistent formatting makes output feel unpolished.

For a conversational support AI, an unstructured output spec might look like this:

```
Format: markdown
Length: 50 to 300 words per response
Structure: greeting if first turn, direct answer to user question, supporting detail if needed, offer of further help if appropriate
Tone: friendly and professional, empathetic when user is frustrated, concise but not curt
Citations: include source links when providing factual information, use format "According to [source]" for attributed claims
Formatting: use bullet lists for multiple items, bold key terms, one line break between paragraphs
```

This is a proper unstructured output specification. Someone could evaluate adherence to it. Someone could design prompts to elicit it. Someone could review outputs against it. It is not as rigid as a structured schema, but it is far more precise than "the system generates helpful responses."

## Specifying Mixed Outputs

Many modern AI systems produce mixed outputs: natural language plus structured data. A conversational AI might return a text response plus metadata. A document generator might produce formatted text plus extracted entities. These systems need specifications for both components.

For the natural language component, use the unstructured output spec guidelines above: format, length, structure, tone, citations, formatting conventions.

For the structured component, use the structured output spec guidelines: field names, data types, required versus optional, validation rules, nesting.

Critically, specify how the two components relate. Does the structured data annotate the natural language? Does the natural language explain the structured data? Are they independent? This relationship must be clear.

For example, a customer support AI that provides conversational responses plus structured actions might have this output spec:

```
response_text: markdown string, 50 to 300 words, friendly tone, includes citations
metadata: object with fields:
  - confidence: float 0.0 to 1.0, required
  - sources: array of source objects with title and URL, optional
  - suggested_action: enum (escalate, resolve, request_more_info), optional
  - escalation_reason: string, required if suggested_action is escalate
```

The response_text is unstructured. The metadata is structured. The spec defines both and explains how they relate: the metadata provides additional context about the response and suggests next steps.

## Metadata Outputs

Most AI systems should include metadata in their outputs, even if the primary output is unstructured. Metadata provides observability, debuggability, and integration hooks.

**Confidence scores** indicate how confident the system is in its output. For structured extraction, this might be per-field confidence. For classification, it's confidence in the predicted class. For generation, it might be an overall quality score. Confidence scores enable downstream systems to decide whether to use the output directly, request human review, or reject it.

**Source citations** indicate where information came from. This is critical for transparency and trust. If the system retrieves information from documents, cite which documents. If it calls APIs, log which APIs. If it synthesizes from multiple sources, list them. Citations make outputs verifiable.

**Processing time** indicates how long the request took. This is useful for performance monitoring and debugging. If a query takes 30 seconds instead of the usual 3 seconds, something is wrong. Logging processing time in the output makes this visible.

**Model version** indicates which version of the model produced the output. This is essential for reproducibility and debugging. If quality suddenly drops, you need to know if it correlates with a model version change. Logging model version in every output makes this analysis possible.

**Trace IDs** allow you to link outputs back to inputs and internal processing steps. If a user reports a bad output, the trace ID lets you reconstruct exactly what happened: what the input was, what retrieval returned, what the model saw, what intermediate steps occurred. Without trace IDs, debugging production issues is nearly impossible.

**Error codes and messages** should be included when something goes wrong. If a required field cannot be extracted, the output should include an error code explaining why. If retrieval failed, the output should indicate that. Error information in the output allows downstream systems to handle failures gracefully.

For high-quality AI systems, metadata is not optional. It is part of the output specification. Define what metadata fields are required, what types they have, and what they mean.

## Side Effects as Outputs

Many AI systems have side effects beyond their primary output. These side effects are outputs too, and they need specification.

**Tool calls** in agentic systems are side effects. If your system calls an API to fetch data, sends an email, updates a database record, or triggers a workflow, those actions are outputs. They must be specified: what tools can be called, under what conditions, with what parameters, with what error handling.

**Database writes** are side effects. If your system logs interactions, updates user state, stores retrieved documents, or caches results, those writes are outputs. Specify what data is written, to where, in what format, and with what retention policy.

**Notifications** are side effects. If your system sends alerts to users, emails to admins, or messages to Slack, those are outputs. Specify when notifications are sent, to whom, with what content, and through what channels.

**Escalation triggers** are side effects. If your system flags cases for human review, creates support tickets, or routes requests to specialists, those escalations are outputs. Specify what triggers escalation, what information is passed to humans, and what the expected response time is.

Side effects often have more impact than primary outputs. An AI that generates a good response but accidentally triggers a duplicate notification is a failure. An AI that correctly extracts data but writes it to the wrong database is a failure. Side effects must be part of the output specification, not an afterthought.

## The Output Schema as the Evaluation Target

Here is why output specifications matter so much: they define what you evaluate. Every field in your output schema becomes something you can measure.

For structured outputs, you can measure per-field precision and recall. Does the system correctly extract party_name? Does it correctly classify contract_type? You can measure format compliance. Do all dates follow ISO 8601? Are all amounts positive? You can measure completeness. Are required fields always populated?

For unstructured outputs, you can measure length compliance, format adherence, structural consistency, and tone alignment. You can measure citation quality. You can measure user satisfaction with output quality.

For metadata, you can measure confidence calibration, processing time distributions, and error rate by error code.

Without an output specification, evaluation is subjective. You read outputs and say "this looks good." With an output specification, evaluation is systematic. You measure adherence to the spec. You track quality over time. You catch regressions immediately.

The output spec transforms evaluation from an art into a science.

## How Output Specs Prevent the "It Works But..." Problem

The legal tech startup's failure is an example of what I call the "it works but it's not what we expected" problem. The AI worked. It extracted information accurately. But the output was not usable because the schema was undefined and inconsistent.

This problem is incredibly common. Teams build AI systems that technically work—the model produces output, the output is often correct—but the output does not meet unstated expectations about format, structure, completeness, or integration requirements.

The root cause is the gap between demonstration and specification. Demos show that the system can produce output. Specifications define exactly what output the system must produce. Most teams do demos but skip specifications. Then they discover in production that "produces output" and "produces usable output in the right format" are not the same thing.

Output specifications close this gap. They force you to define usability requirements before you build. They make expectations explicit so everyone—product, engineering, data science, integration—is aligned on what the output must look like.

If the legal tech startup had written an output specification during framing, they would have discovered the schema inconsistencies before the pilot. They would have enforced field names, data types, and validation rules. They would have tested integration with the law firm's contract management system. They would have avoided three months of frustration and a cancelled contract.

## What You Should Specify Before You Build

Before you write model training code or generation prompts, sit down with your team and answer these questions:

Is your output structured, unstructured, or mixed? For structured outputs, what is the complete schema with field names, types, required versus optional, validation rules, and nesting? For unstructured outputs, what are the format, length, structure, tone, and citation requirements? For mixed outputs, how do the structured and unstructured components relate?

What metadata should every output include? Confidence scores, source citations, processing time, model version, trace IDs, error codes?

What side effects does your system have? Tool calls, database writes, notifications, escalations? What are the specifications for those side effects?

How will you validate outputs? What checks will run automatically? What criteria define a valid output versus an invalid one?

How will you evaluate quality? What metrics map to your output specification?

Write down the answers. Build a formal output schema or output spec document. Get sign-off from product, engineering, data science, and integration. Then build your system to produce outputs that match the spec.

Validate outputs against the spec before they leave the system. Reject or flag outputs that violate the spec. Track compliance over time. When you update the model, verify that outputs still match the spec.

This discipline feels like overhead at the start. It prevents the "it works but it's not what we expected" problem that kills pilots and delays launches. In the next section, we will examine how output quality dimensions differ across task types and how to tailor your output specification to the specific nature of your task.

