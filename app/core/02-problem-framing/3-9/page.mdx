# 3.9 â€” Task Taxonomy Meets Product Archetypes

In late 2025, two startups raised Series A funding within weeks of each other. Both described their products as "AI-powered customer service platforms." Investors, customers, and even the teams themselves talked about the products as if they were competing directly. Eighteen months later, one company had achieved product-market fit and was scaling rapidly. The other had pivoted twice and was struggling to define a viable product.

The difference was not in execution quality or team talent. It was that the two products, despite identical marketing descriptions, had completely different task profiles. The successful company had built a retrieval-augmented generation system: given a customer question, retrieve relevant documentation, generate an answer, and present it to the support agent as a suggestion. The core tasks were retrieval, extraction, and generation. Their evaluation strategy focused on retrieval recall, answer factuality, and generation fluency. They hired engineers with search and NLP backgrounds.

The struggling company had built an autonomous agent: given a customer question, classify the intent, determine which actions to take, execute those actions (creating tickets, updating records, calling APIs), and generate a response. The core tasks were classification, reasoning (to plan the sequence of actions), tool selection, and generation. They needed to evaluate intent accuracy, action appropriateness, API reliability, and response quality. They needed engineers with agent architecture and reasoning experience.

Both products lived in the same market category, but they were solving fundamentally different technical problems. The task profile determined what skills the team needed, what evaluation infrastructure to build, what metrics mattered, and ultimately whether the product would work. The company that understood its task profile early thrived. The company that treated all "AI customer service" products as equivalent struggled.

This chapter maps the nine product archetypes from Section 1 to their dominant task types and explains why understanding your product's task profile is the foundation for everything that follows.

## Why Task Profiles Matter

Your product's task profile is the set of task types that appear in your system and their relative importance. A chatbot's task profile is dominated by generation, with supporting classification and reasoning. A content moderation system's task profile is dominated by classification, with supporting reasoning for edge cases. A data extraction tool's task profile is dominated by extraction, with supporting classification to route documents.

The task profile shapes five critical dimensions of your product development.

First, team composition. Different task types require different expertise. Retrieval tasks need engineers who understand information retrieval, embeddings, and ranking. Generation tasks need engineers who understand language models, prompting, and fluency. Reasoning tasks need engineers who understand multi-step inference, chain-of-thought, and evaluation of logical soundness. If your task profile is retrieval-heavy but your team is full of generation experts, you will struggle.

Second, evaluation strategy. Each task type has characteristic evaluation patterns, metrics, and ground truth formats. Classification tasks use accuracy, precision, recall, and F1. Retrieval tasks use precision at k, recall at k, and normalized discounted cumulative gain. Generation tasks use fluency, factuality, and human preference ratings. If you do not know your task profile, you do not know which metrics to track.

Third, ground truth requirements. Classification ground truth is labels. Retrieval ground truth is relevance judgments. Generation ground truth is reference outputs or human preference rankings. Extraction ground truth is annotated spans. Each type requires different tooling, different annotator training, and different quality control processes. Mismatching ground truth format to task type leads to unusable evaluation data.

Fourth, risk profile. Some task types are inherently riskier than others. Generation tasks risk hallucination. Reasoning tasks risk confident wrong answers. Classification tasks risk disparate impact if classes correlate with protected attributes. Retrieval tasks risk surfacing sensitive information. Your task profile determines which risks to prioritize and which safeguards to implement.

Fifth, infrastructure needs. Retrieval tasks need vector databases and search infrastructure. Generation tasks need inference infrastructure for large language models. Reasoning tasks need orchestration for multi-step processes. Extraction tasks need structured data pipelines. Your task profile determines what infrastructure to build or buy.

Understanding your product's task profile early lets you make informed decisions about all five dimensions. Misunderstanding it leads to wasted effort, wrong hires, inappropriate metrics, and infrastructure that does not match your needs.

## Mapping Archetypes to Task Types

Let us map each of the nine product archetypes from Section 1 to their dominant task types.

Chatbots are primarily generation systems. The user sends a message, and the system generates a response. Supporting tasks include classification (intent detection, sentiment analysis) and reasoning (determining what information to include in the response, planning multi-turn conversations). Evaluation focuses on response quality: fluency, relevance, factuality, and user satisfaction. Ground truth is often human preference judgments or ratings of response quality.

RAG systems (retrieval-augmented generation) combine retrieval, extraction, and generation. Given a query, retrieve relevant documents, extract key information, and generate a response grounded in that information. The task chain is explicit: retrieval feeds into extraction, which feeds into generation. Evaluation must cover all three tasks: retrieval recall (did we find the right documents?), extraction precision (did we pull the right facts?), and generation faithfulness (does the response match the extracted facts?). Ground truth includes relevance judgments for retrieval, annotated spans for extraction, and reference answers or factuality ratings for generation.

Agents are dominated by reasoning and classification. Given a task, the agent must reason about which actions to take and in what order, classify which tools to use, then execute the plan. Generation is used to produce the final response. Evaluation focuses on action appropriateness (did the agent choose the right actions?), plan correctness (were actions sequenced correctly?), and outcome success (did the agent accomplish the task?). Ground truth often involves expert judgment about whether the agent's reasoning and actions were sound.

Code assistants combine generation, transformation, and reasoning. They generate code, transform code from one style or paradigm to another, and reason about what code structure is appropriate for the user's goal. Evaluation includes code correctness (does the code run?), code quality (is it idiomatic and maintainable?), and security (are there vulnerabilities?). Ground truth includes test cases for correctness and expert review for quality and security.

Content moderation systems are primarily classification: is this content allowed or not? Supporting reasoning tasks determine why borderline content should be allowed or blocked and how to apply policies to edge cases. Evaluation focuses on classification accuracy, false positive and false negative rates, and consistency across similar cases. Ground truth is policy-compliant labels, often requiring expert annotators who deeply understand the content policy.

Data extraction tools are dominated by extraction: given a document, pull out structured information. Supporting classification determines document type and routes to the appropriate extraction pipeline. Evaluation focuses on extraction precision and recall: did we extract all the information, and is the extracted information correct? Ground truth is annotated documents with labeled spans or structured outputs.

Search engines are retrieval systems with ranking (a form of classification that orders results by relevance). Given a query, retrieve candidate documents and rank them. Evaluation focuses on ranking quality: are the most relevant results at the top? Ground truth is relevance judgments, often collected through click data or explicit ratings.

Voice assistants chain transformation tasks (speech recognition and text-to-speech) with classification (intent detection) and generation (response formulation). The task chain is long: audio to text (transformation), text to intent (classification), intent to response plan (reasoning), plan to response text (generation), text to audio (transformation). Evaluation must cover the entire chain and each component. Ground truth includes transcription accuracy, intent labels, response quality ratings, and speech naturalness ratings.

Multimodal systems combine extraction from non-text inputs (images, video, audio) with reasoning and generation. Given a multimodal input, extract perceptual information (what objects are in the image, what is happening in the video), reason about relationships and meanings, and generate a response. Evaluation focuses on perception accuracy (did we correctly identify what is in the input?) and reasoning quality (did we correctly understand what it means?). Ground truth includes annotated images or videos and reference descriptions or answers.

Each archetype has a characteristic task profile. Knowing your archetype tells you which tasks to expect and therefore which evaluation strategies to prepare.

## Task Profile Analysis: A Case Study

Consider a hypothetical AI product: a legal research assistant. The user describes a legal question or scenario, and the system recommends relevant case law and statutes. On the surface, this looks like a search engine: query in, ranked results out. But analyzing the task profile reveals it is more complex.

First, the user's input is natural language, often ambiguous and underspecified. The system must classify what area of law is relevant, what type of legal authority is needed (case law vs. statutes vs. secondary sources), and what jurisdiction applies. This is classification.

Second, the system must transform the natural language query into a structured search: identify key concepts, legal terms, and factual patterns. This is transformation and extraction combined.

Third, the system must retrieve candidate legal authorities from a database. This is retrieval.

Fourth, the system must reason about which authorities are most relevant: which cases have similar facts, which statutes apply to the situation, which precedents control. This is reasoning.

Fifth, the system must rank the authorities and present them to the user. This is classification (ranking).

Sixth, the system may generate explanations of why each authority is relevant. This is generation.

The task profile is therefore: classification (intent and jurisdiction), transformation (query formalization), extraction (key concepts), retrieval (candidate search), reasoning (relevance determination), classification (ranking), and generation (explanations). This is a complex profile, spanning nearly every task type.

Understanding this profile tells the team what to build. They need:
- A classifier for legal domain and jurisdiction
- A query transformation pipeline to convert natural language to structured search
- An extraction system to identify key legal concepts
- A retrieval system over a legal corpus
- A reasoning system to evaluate relevance based on legal principles
- A ranking algorithm that reflects legal relevance, not just keyword match
- A generation system to explain relevance

Each component needs separate evaluation. The team needs legal experts to create ground truth (they cannot rely on generic crowdsourced labels). They need to track metrics for each task type: classification accuracy, retrieval recall, reasoning soundness, ranking quality, and explanation clarity.

Without this task profile analysis, the team might have treated this as a simple search problem and failed to recognize the reasoning and transformation components. The product would have launched with keyword search and no understanding of legal relevance. It would have failed.

## Task Profiles and Risk Tiers

The task profile also determines risk. Section 1 introduced risk tiers based on impact and error cost. Task types map to characteristic risks, and a product's task profile determines its overall risk exposure.

Generation tasks risk hallucination: the model confidently states false information. This is a critical risk for products where factuality matters (medical advice, legal guidance, financial recommendations).

Reasoning tasks risk confident wrong answers: the model produces an answer that sounds correct but is based on flawed logic. This is a critical risk for decision support products (hiring recommendations, diagnostic tools, strategic planning).

Classification tasks risk disparate impact: if the training data encodes biases, the classifier may systematically misclassify certain groups. This is a critical risk for products that affect access to opportunities (hiring, lending, admissions).

Retrieval tasks risk information exposure: the system might retrieve and surface sensitive information that should not be shown to the user. This is a critical risk for products handling confidential data (internal search tools, document management systems).

Extraction tasks risk data corruption: incorrect extraction can lead to downstream systems using wrong information. This is a critical risk for products that populate databases or drive automated decisions (invoice processing, medical record extraction).

Transformation tasks risk semantic drift: the meaning of the input might be subtly altered in the transformation. This is a critical risk for products where precision matters (legal translation, medical record conversion).

A product's risk tier is determined not just by its domain but by its task profile. A chatbot in a low-stakes domain (entertainment recommendations) has lower risk even though it uses generation, because hallucination about movie plots is not dangerous. A chatbot in a high-stakes domain (medical advice) has very high risk because hallucination can harm users.

Understanding the mapping from task types to risks helps you anticipate which failures to guard against and which evaluation to prioritize.

## Building Your Task Profile Map

To create your product's task profile map, follow this process.

First, identify your product archetype from Section 1. Are you building a chatbot, RAG system, agent, code assistant, content moderator, data extractor, search engine, voice assistant, or multimodal system? This gives you a starting point.

Second, decompose your product into features using the techniques from Chapter 2. For each feature, break it down into sub-problems.

Third, classify each sub-problem by task type using the taxonomy from this chapter. Is it classification, retrieval, extraction, generation, transformation, reasoning, or some combination?

Fourth, map the dependencies: which tasks feed into which other tasks? This creates your task chain diagram.

Fifth, identify the dominant tasks: which task types appear most frequently? Which are on the critical path? This tells you where to focus effort.

Sixth, assess the risk associated with each task based on the task-type-to-risk mapping above. Which tasks are most dangerous if they fail?

Seventh, determine the evaluation strategy for each task based on its type. What metrics matter? What ground truth do you need?

Eighth, validate the map with your team. Does it match how they think about the product? Are there tasks missing or misclassified?

This map becomes a foundational artifact. It drives hiring (we need retrieval experts), infrastructure decisions (we need vector search), evaluation planning (we need annotated relevance judgments), and risk mitigation (we need safeguards against hallucination).

## When Task Profiles Change

Task profiles are not static. As your product evolves, the mix of tasks changes, and with it, your evaluation needs.

A product might start as a simple classifier (intent detection for customer support messages) then add retrieval (suggesting relevant help articles) then add generation (drafting responses) then add reasoning (deciding which actions to take). Each addition changes the task profile and requires new evaluation infrastructure.

The risk is that teams layer on new tasks without updating their evaluation strategy. They keep using the metrics from the original task profile even though those metrics do not cover the new tasks. A team that started by tracking classification accuracy might add a generation component but forget to track hallucination rate or fluency. The product launches with a blindspot.

The discipline is to update your task profile map whenever you add or significantly change a feature. Re-run the analysis: what tasks are now present? What is the new critical path? What risks do the new tasks introduce? What evaluation do they require? Then update your metrics dashboard, ground truth collection, and team skills accordingly.

Products that maintain an accurate task profile map stay aligned. Products that let the map drift find themselves measuring the wrong things and missing the real problems.

## Task Profiles as Communication Tool

Beyond its technical value, the task profile map is a communication tool. It creates shared understanding across disciplines.

Engineers understand the technical architecture: which components exist, how they connect, what APIs they expose. But engineers often do not think in terms of task types and evaluation strategies.

Product managers understand user value: what features users want, what problems the product solves. But product managers often do not understand the technical distinctions between task types.

Data scientists understand model performance: what metrics look good, where models struggle. But data scientists often do not see how their task fits into the broader product.

The task profile map bridges these perspectives. It shows engineers how their components map to task types that have known evaluation patterns. It shows product managers which features rely on which tasks and therefore which risks to expect. It shows data scientists how their task fits into the larger system and why end-to-end evaluation matters.

When everyone shares a task profile map, conversations become more productive. Instead of arguing about vague quality concerns, you can point to specific tasks and their metrics. Instead of debating whether a feature is "ready," you can check whether each task in the feature's chain meets its quality threshold.

The map creates a shared language for discussing what the product does, what quality means, and what needs to improve.

## From Task Profile to Evaluation Strategy

The task profile is not the end goal; it is the foundation for the evaluation strategy you will build in Section 3. The profile tells you what to measure. The evaluation strategy tells you how to measure it, how often, and what thresholds to enforce.

For each task in your profile, you will define:
- Metrics: which numbers to track
- Ground truth format: what annotated data looks like
- Collection process: how to gather ground truth at scale
- Quality thresholds: what values constitute acceptable performance
- Monitoring cadence: how often to re-evaluate
- Failure modes: what errors to watch for
- Mitigation strategies: what to do when quality drops

Section 3 will walk through this process in detail. But none of it is possible without a clear task profile. You cannot define metrics until you know which tasks you are measuring. You cannot collect ground truth until you know what format it should take. You cannot set thresholds until you know what risks each task introduces.

The task profile is the bridge from problem decomposition (Chapter 2) to evaluation strategy (Section 3). It takes the abstract "what are we building?" and turns it into the concrete "what do we need to measure?"

---

With the mapping between archetypes and task types established, we now turn to the practical artifact that makes this knowledge actionable: the task registry, a living document that catalogs every AI task in your system and serves as the single source of truth for evaluation.
