# 3.2 — The Five Core Task Types

When you strip away the hype, the demos, the research papers, and the marketing language, almost every AI product task falls into one of five fundamental categories. These aren't arbitrary academic divisions. They're operational categories that directly determine how you build, evaluate, and monitor your system. Learn to recognize these five types, and you gain a mental model that clarifies every AI product decision you make.

The five core task types are: Generation, Extraction, Classification, Transformation, and Reasoning. Each has a distinct signature in how it processes input, produces output, and defines success. Each requires different evaluation strategies, different ground truth collection approaches, different failure mode monitoring, and different quality assurance processes.

Let's examine each type in detail, understand what makes it unique, and learn how to identify which type you're dealing with when you encounter a new AI product challenge.

## Generation: Creating New Content

Generation tasks create new content that didn't exist in the input. The system produces original output based on instructions, context, or prompts. The output space is enormous—for any given input, there are countless possible outputs, many of which could be considered correct or acceptable.

Examples of generation tasks include writing marketing copy, composing emails, generating code from natural language descriptions, creating product descriptions, drafting reports, answering open-ended questions, brainstorming ideas, writing creative fiction, and producing conversational responses in chatbots.

What makes generation distinct is the creative nature of the output. The system isn't finding information that already exists. It isn't categorizing input into predefined buckets. It's synthesizing new content. This means there's no single correct answer. Multiple outputs can be equally valid. Quality is multidimensional and often subjective.

The output space determines everything. In generation, the output space is effectively infinite. Ask ten skilled humans to write a product description for the same item, and you'll get ten different descriptions. Some might emphasize features. Some might emphasize benefits. Some might be playful. Some might be serious. Some might be short. Some might be long. Many could work equally well for different audiences or contexts.

This enormous output space makes generation the hardest task type to evaluate. You can't use exact match metrics—the chance that your model produces word-for-word the same output as your reference example is essentially zero, even if both outputs are excellent. You can't use simple similarity metrics—two outputs can be equally good while being quite dissimilar in word choice and structure.

Generation evaluation requires human judgment, multi-dimensional rubrics, or sophisticated LLM-as-judge systems. You evaluate not whether the output matches a reference but whether it satisfies quality criteria: Is it fluent? Is it coherent? Is it relevant to the prompt? Is it factually accurate? Does it match the desired tone and style? Is it appropriate for the audience? Does it achieve the intended purpose?

Ground truth for generation tasks consists of reference examples, not definitive answers. When you collect ground truth for a generation task, you're collecting examples of high-quality outputs that demonstrate what good looks like. You're not collecting the only correct outputs. Your reference examples serve as anchors and guides, not as targets for exact matching.

## Extraction: Pulling Structured Information

Extraction tasks pull specific pieces of information from unstructured or semi-structured input. The system identifies and extracts facts, entities, values, or relationships that exist in the source material. The output space is constrained by what's actually present in the input.

Examples of extraction tasks include pulling vendor names and amounts from invoices, extracting dates and participants from meeting notes, finding key terms from contracts, pulling product specifications from datasheets, extracting contact information from business cards, identifying symptoms from medical records, and pulling structured data from free-form survey responses.

What makes extraction distinct is that the answer exists in the input. You're not creating new content. You're finding and pulling out information that's verifiably present. This means extraction has objective ground truth. The invoice either says "$1,247.83" or it doesn't. The contract either mentions "30-day notice period" or it doesn't.

The output space in extraction is bounded by the input. You're not generating arbitrary text. You're selecting and structuring information from what's there. This makes extraction much more objectively evaluable than generation.

Extraction evaluation uses precision, recall, and accuracy metrics. Precision measures how much of what you extracted is correct. Recall measures how much of what should be extracted you actually found. You can measure exact matches for numbers and identifiers. You can measure fuzzy matches for text fields where minor variations might be acceptable. You can measure field-level accuracy for structured extraction where you're populating multiple fields.

Ground truth for extraction tasks consists of labeled examples with verified correct extractions. When you collect ground truth for extraction, annotators mark the specific pieces of information that should be extracted from each example. The vendor name should be "Acme Corporation." The total should be "$1,247.83." The date should be "March 15, 2024." These are facts that can be verified against the source material.

The challenge in extraction isn't subjectivity—it's edge cases. What do you do with partial matches? How do you handle ambiguous boundaries? What counts as a single entity versus multiple entities? How do you deal with overlapping extractions or multi-value fields? These questions require clear guidelines, but once you've defined the rules, evaluation is objective.

## Classification: Categorizing Input

Classification tasks assign input to one or more categories from a predefined set. The system examines the input and determines which category or categories apply. The output space is finite and known in advance.

Examples of classification tasks include intent detection in chatbots, sentiment analysis of reviews, content moderation of user posts, spam detection in email, topic categorization of articles, priority assignment for support tickets, language detection, document type identification, and risk level assessment.

What makes classification distinct is the bounded output space. You're not generating free-form text. You're not extracting arbitrary information. You're choosing from a fixed set of categories. This might be binary classification (spam or not spam), multi-class classification (route to sales, support, or billing), multi-label classification (tag with all applicable topics), or hierarchical classification (broad category, then subcategory).

The finite output space makes classification the easiest task type to evaluate objectively. You can measure accuracy: what percentage of inputs got the correct category? You can measure per-category precision and recall: for each category, how often do you correctly identify it, and how often do you miss it? You can build confusion matrices that show which categories get mixed up with which others.

Classification evaluation uses well-established metrics from traditional machine learning: accuracy, precision, recall, F1 score, ROC curves for binary classification, per-class metrics for multi-class problems. These metrics have clear interpretations and established thresholds for what constitutes good performance in different domains.

Ground truth for classification consists of labeled examples where each input has been assigned its correct category or categories. When you collect ground truth for classification, domain experts review each example and determine which category applies. There's often inter-annotator disagreement, especially at category boundaries, but the disagreement is about which finite category fits best, not about generating unlimited variations.

The challenge in classification is the fuzzy middle: examples that fall between categories, rare categories that barely appear in the data, and overlapping categories where multiple labels might apply. But even these challenges are manageable because the category space is bounded. You know all the possible outputs in advance.

## Transformation: Converting Form

Transformation tasks convert input from one form to another while preserving the essential information. The system restructures, reformats, or translates content without adding or removing significant information. The output space is constrained by the input content and the target format.

Examples of transformation tasks include translating text between languages, converting prose to structured formats like JSON or tables, reformatting dates or addresses, changing writing style or reading level, converting code between programming languages, transcribing speech to text, summarizing while preserving key points, and converting markdown to HTML.

What makes transformation distinct is the preservation constraint. Unlike generation, you're not creating entirely new content. Unlike extraction, you're not just pulling out pieces. Unlike classification, you're not mapping to predefined categories. You're changing the form while keeping the substance.

The output space in transformation is larger than extraction or classification but smaller than open-ended generation. You're producing new text or structure, but it must faithfully represent the input. There might be multiple valid transformations, but there are also clearly wrong transformations that lose or distort information.

Transformation evaluation depends on the specific transformation type. Translation is evaluated with metrics like BLEU and specialized translation quality metrics. Structured output transformation (prose to JSON) is evaluated with schema validation and information preservation checks. Summarization is evaluated with both automatic metrics (ROUGE) and human assessment of whether key information is retained. Style transfer is evaluated by checking that content is preserved while style attributes change.

Ground truth for transformation consists of reference transformations that demonstrate the desired mapping. When you collect ground truth for translation, you have professional translators produce target language versions. When you collect ground truth for prose-to-JSON transformation, you have annotators create the structured representation. The references show what correct transformation looks like.

The challenge in transformation is balancing fidelity and form. How much can you adapt to fit the target format before you've changed the meaning? How do you handle information that doesn't map cleanly? When is paraphrasing acceptable versus when do you need exact preservation? These questions vary by transformation type.

## Reasoning: Multi-Step Inference and Judgment

Reasoning tasks require multi-step inference, logical deduction, judgment based on principles or rules, or synthesis across multiple pieces of information. The system doesn't just pattern-match or retrieve—it thinks through a problem, applies logic, or makes principled decisions.

Examples of reasoning tasks include answering questions that require multi-step logic, diagnosing problems from symptoms and context, making recommendations based on constraints and preferences, evaluating whether a claim is supported by evidence, planning sequences of actions to achieve goals, debugging code by tracing through execution, making ethical judgments based on principles, and comparing options against multiple criteria.

What makes reasoning distinct is the multi-step nature. The system needs to combine information, apply logic, follow chains of inference, or evaluate tradeoffs. A single lookup or pattern match isn't enough. The task requires something that looks like thinking.

The output space in reasoning varies. Sometimes reasoning produces a structured answer (a diagnosis, a recommendation). Sometimes it produces an explanation of the reasoning process. Sometimes it produces both. The critical element is that the output must be justified by sound reasoning, not just plausible-sounding text.

Reasoning evaluation is the most complex because you need to assess both the final answer and the reasoning process. Is the conclusion correct? Is the reasoning valid? Are the intermediate steps sound? Does the explanation make sense? Did the system consider relevant factors and ignore irrelevant ones?

Some reasoning tasks have verifiable answers and can be evaluated objectively (mathematical reasoning, logical puzzles). Others require expert judgment (medical diagnosis, legal reasoning, strategic recommendations). Most require a combination: check that the conclusion is reasonable and that the reasoning process is sound.

Ground truth for reasoning consists of worked examples showing both correct answers and correct reasoning processes. When you collect ground truth for reasoning tasks, experts solve problems and explain their thinking. The ground truth captures not just what the right answer is but why it's right and how to get there.

The challenge in reasoning is that the task often combines other task types. A reasoning task might include classification (what type of problem is this?), extraction (what are the relevant facts?), and generation (explain the reasoning). You need to evaluate the reasoning as a whole while also evaluating the component parts.

## Identifying Your Primary Task Type

Most real AI systems combine multiple task types. A document processing pipeline might do classification (document type), extraction (pull fields), transformation (convert to target format), and validation (reasoning about whether the output makes sense). A chatbot might do classification (intent), extraction (key details), reasoning (determine response strategy), and generation (produce the response).

When your system combines task types, you need to identify the primary task type for each component. The component that directly produces the output the user cares about is usually the primary task. The other components are supporting tasks.

For a document extraction system, extraction is the primary task. Even if you use classification to determine document type and reasoning to validate extracted values, the core task is extraction. That determines your primary evaluation strategy.

For a creative writing assistant, generation is the primary task. Even if you use classification to detect writing prompts and reasoning to maintain consistency, the core task is generation. That determines your primary evaluation strategy.

For a support ticket router, classification is the primary task. Even if you use extraction to pull key details and reasoning to handle ambiguous cases, the core task is classification. That determines your primary evaluation strategy.

Understanding your primary task type tells you what evaluation approach to build first, what metrics to prioritize, what kind of ground truth to collect, and what failure modes to watch for most carefully. The supporting tasks need evaluation too, but the primary task defines your core evaluation strategy.

## Why Task Type Determines Everything

Once you correctly identify your task type, everything else follows. You know immediately what kind of evaluation system you need. You know what metrics make sense and which ones don't. You know what ground truth collection looks like. You know what failure modes to expect and monitor.

Classification task? You need labeled examples, accuracy metrics, and per-category analysis. You watch for category drift and confusion patterns.

Extraction task? You need verified extractions, precision and recall metrics, and field-level validation. You watch for missing fields and incorrect values.

Generation task? You need reference examples, rubric-based evaluation, and human or LLM judges. You watch for factual errors and quality degradation.

Transformation task? You need reference transformations, format validation, and information preservation checks. You watch for content loss and format violations.

Reasoning task? You need worked examples with explanations, process validation, and expert review. You watch for logical errors and missing steps.

The taxonomy is your Rosetta Stone. It translates between product intent and evaluation design. It tells you what to build, how to measure it, and how to know when it's working.

In the next section, we'll dive deep into generation tasks, the most challenging task type to evaluate and increasingly the most common in modern AI products.
