# 2.1 — From Business Goal to AI Sub-Problems

Three weeks before their Series B announcement, the executive team at a fast-growing SaaS company called an emergency meeting.

They'd spent six months and $800,000 building an AI customer support system. The CEO had promised investors it would "reduce support costs by 30% while improving customer satisfaction." The demo had been flawless. The board loved it. Tech press was ready to cover the launch.

Then they did a pilot with 1,000 real customers.

Support costs went up by 12%. Customer satisfaction dropped. The support team was now handling the same volume of tickets plus a new category: "the bot gave me the wrong answer and now I'm confused."

What went wrong? I talked to the engineering lead a month later, after they'd shelved the project. She showed me the original spec. It was one sentence: "Build an AI chatbot to improve customer support."

That sentence killed six months of work.

Here's why: they'd jumped directly from a business goal—reduce costs, improve satisfaction—to a technical implementation—build a chatbot. They skipped the crucial step in between: decomposing the problem into specific, solvable AI sub-problems.

The business wanted cost reduction. What they built was a conversational interface. Those aren't the same thing. And nobody caught it because nobody broke down the problem properly.

## The Decomposition Funnel

Let me show you what proper problem decomposition looks like. It's a funnel with four distinct layers, and you can't skip any of them.

Layer one: Business Outcome. This is what executives care about. It's usually financial or strategic. "Reduce support costs by 30%." "Increase user engagement by 20%." "Reduce time to resolution for customer issues." These are legitimate business goals. They're also not AI problems yet.

Layer two: User Task. This is what users are actually trying to do. It's the job they're hiring your product to accomplish. For the customer support example, users are trying to: get answers to specific questions about their account, troubleshoot technical issues, understand billing charges, or escalate complex problems to humans. These are user tasks. They're getting closer to AI problems, but we're not there yet.

Layer three: AI Capability. This is what AI actually needs to do to support the user task. For "get answers to specific questions," the AI capability might be: "retrieve relevant information from the knowledge base and synthesize it into a response that matches the user's context." Now we're in AI territory. But we're still not at the implementation level.

Layer four: Technical Sub-Problems. This is where we break the AI capability into specific, testable components. For our knowledge base retrieval capability, the technical sub-problems are: query understanding (what is the user actually asking), retrieval (finding relevant documents), ranking (ordering by relevance), synthesis (generating an answer from retrieved content), citation (showing where the answer came from), and escalation detection (knowing when to hand off to a human).

Each layer answers a different question. Business outcome answers: why are we doing this? User task answers: what does success look like for users? AI capability answers: what does AI need to be able to do? Technical sub-problems answer: what specific components do we need to build?

The SaaS company that burned $800,000 jumped from layer one directly to layer four. They went from "reduce costs" to "build retrieval and generation" without understanding layers two and three. That's why it failed.

## Why Each Layer Matters

Let me walk through why you can't skip any of these layers.

If you skip business outcome, you build technically impressive AI that doesn't move business metrics. I've seen teams build brilliant sentiment analysis systems that had zero impact on revenue because nobody asked: what business outcome does sentiment analysis drive? You end up with a solution looking for a problem.

If you skip user task, you optimize for the wrong thing. The SaaS company thought the user task was "talk to a bot." It wasn't. The actual user tasks were "understand my bill," "fix my broken integration," "change my subscription." A conversational interface doesn't inherently solve any of those. It's just a different way to fail at solving them.

If you skip AI capability, you can't validate your approach before building. You end up discovering six months in that the AI capability you actually need—like analyzing unstructured log files to diagnose technical issues—is way harder than the one you assumed—answering FAQ-style questions. By then you've already built the wrong architecture.

If you skip technical sub-problems, you build a monolith you can't debug, evaluate, or improve. When something fails, you can't pinpoint where. Was it retrieval? Ranking? Synthesis? You don't know because you never separated them. You end up tweaking prompts randomly and hoping for improvement.

Each layer is a validation checkpoint. If you can't clearly articulate what success looks like at each layer, you're not ready to move to the next one.

## The Worked Example: Customer Support

Let me show you what this decomposition looks like in practice, using the customer support example properly this time.

Business outcome: "Reduce support costs by 30% over six months while maintaining or improving customer satisfaction scores."

Now we validate: Is that actually achievable with AI? What drives support costs? We analyze the data. Turns out 60% of support tickets are about five common topics: billing questions, password resets, feature availability, integration setup, and account changes. Those are potentially automatable. The other 40% are complex debugging, custom implementation help, and escalations. Those aren't. So our refined business outcome is: "Automate resolution of the 60% of tickets that are common questions, reducing costs while maintaining satisfaction."

User task: Now we break down what users are trying to do in that 60%:
- Understand a specific charge on their bill
- Reset their password or recover their account
- Find out if a feature exists or how to use it
- Connect an integration to their account
- Update payment method or plan details

These are distinct tasks. They require different information, different validation, different error handling. We've decomposed one vague user goal—"get support"—into five specific tasks.

AI capability: For each user task, we define what AI must do:
- Billing questions: retrieve transaction history, match charges to services, explain pricing tiers
- Password reset: verify identity, send reset link, confirm security requirements
- Feature questions: search documentation, match user intent to feature descriptions, provide usage steps
- Integration setup: diagnose connection failures, provide credential setup guidance, validate configuration
- Account changes: verify permissions, validate changes against business rules, confirm updates

Notice we're not saying "use RAG" or "call function X." We're defining the capability without prescribing the implementation. This keeps us flexible.

Technical sub-problems: Now we decompose each AI capability into buildable components. Let's take billing questions as the example:

Sub-problem one: Query understanding. Transform "why was I charged $47 on March 3rd?" into structured intent: user wants to understand a specific charge, amount is $47, date is March 3rd.

Sub-problem two: Retrieval. Find the transaction record matching the date and amount. Find the service or usage that triggered that charge. Find the relevant pricing documentation.

Sub-problem three: Ranking. If multiple transactions match, order by relevance (exact match on amount and date ranks highest).

Sub-problem four: Synthesis. Generate an explanation that connects the transaction to the service, explains the pricing logic, and references the user's specific plan.

Sub-problem five: Citation. Show the specific transaction ID, link to pricing documentation, reference the service usage that triggered the charge.

Sub-problem six: Escalation detection. If the user disputes the charge, if the transaction has anomalies, or if the pricing logic is unclear, flag for human review instead of trying to resolve.

These are six distinct technical problems. Each has clear inputs and outputs. Each can be built separately. Each can be evaluated independently. Each can be improved without touching the others.

That's proper decomposition.

## The Jump That Kills Projects

Let me show you the single most common mistake in AI product development: jumping from business goal to technical implementation without the middle layers.

It looks like this:

Business says: "We need to improve our customer support." Engineering hears: "Build a chatbot." They start building retrieval, prompts, and response generation. Three months later they demo it. Business says: "This doesn't solve our problem." Engineering says: "But it's a working chatbot!" Both are right. Both are frustrated.

What happened? They skipped user tasks and AI capabilities. They never asked: what are users actually trying to do? What specific capabilities would help them do those things? They jumped from "improve support" to "build retrieval pipeline."

Here's another version I see constantly:

Business goal: "Increase engagement." Implementation: "Add AI-generated personalized recommendations." Nobody asked: what user task does that serve? Are users trying to discover new content? Are they trying to complete their current task faster? Are they trying to understand something? Different tasks require different capabilities and different sub-problems.

The team builds a recommendation engine. It works technically. It has no impact on engagement because it's solving a problem users don't have.

The jump kills projects because it creates a gap between what you build and what actually matters. You end up with technically correct solutions to the wrong problem.

## How To Validate Each Layer

Here's how you validate each layer with the right stakeholder before moving forward.

Business outcome validation: Work with executives and finance. Ask: What metric defines success? What's the baseline? What's the target? What's the timeline? If we hit this target, what business decision does it unlock? If they can't answer concretely, the business outcome isn't clear enough yet.

For "reduce support costs by 30%," you validate by asking: How do we currently measure support costs? What's included—just labor, or also tooling and overhead? What's the current cost? What's the acceptable trade-off for quality? Get numbers. Get agreement.

User task validation: Work with product managers and talk to actual users. Ask: What are users trying to accomplish? What's blocking them today? How do they currently solve this? What would make their life easier? Watch real sessions. Read real tickets. Don't assume. Validate.

For customer support, you validate by analyzing ticket categories, doing user interviews, and watching support conversations. You find out that users aren't trying to "chat with a bot." They're trying to "understand this confusing charge before they cancel their subscription." Totally different task.

AI capability validation: Work with ML engineers and AI researchers. Ask: Is this technically feasible? What's the state of the art? What data do we need? What quality bar can we realistically achieve? What are the known failure modes?

For billing explanation, you validate by checking: Can current AI reliably extract dates and amounts from natural language? Can it match transactions to services? Can it generate accurate explanations without hallucinating? You might discover that explaining certain pricing edge cases is beyond current AI capability, so you scope it out.

Technical sub-problem validation: Work with engineers. Ask: Can we build this component independently? Can we test it separately? Can we measure its quality? Do we have the data it needs? What are its dependencies?

For each sub-problem, you validate by defining its interface: what goes in, what comes out, what "correct" looks like. If you can't define that clearly, it's not decomposed enough yet.

## The Decomposition Reveals Risks Early

Here's the hidden benefit of proper decomposition: it surfaces risks before you build.

When you decompose "improve customer support" all the way to technical sub-problems, you discover that escalation detection is really hard. How do you know when the user is frustrated? How do you distinguish "I need more information" from "this isn't working, get me a human"? You realize this is a distinct sub-problem that might fail even if everything else works.

That's a risk. And now you know it before you've built the system. You can address it in design—maybe you always offer a human escalation option. Maybe you build a separate frustration detection component. Maybe you start with a category of questions where escalation is clear-cut.

Without decomposition, you discover the escalation problem in production, when users are angry that they can't reach a human. By then it's a crisis, not a design choice.

Decomposition converts surprises into planned trade-offs.

## The Power Of Clear Interfaces

When you decompose into technical sub-problems, you create clear interfaces between components. That clarity pays dividends throughout the product lifecycle.

You can evaluate each component separately. Your retrieval quality might be 90% but your synthesis quality might be 65%. Now you know where to focus. Without decomposition, you just know "the chatbot is 70% good" and you don't know why.

You can improve components independently. You discover a better ranking algorithm. You swap it in. Because it has a clear interface—takes in documents and a query, outputs ranked documents—you can upgrade it without touching retrieval or synthesis.

You can test at the right level. Unit tests for query understanding. Integration tests for retrieval plus ranking. End-to-end tests for the full pipeline. Each test validates a different layer. Without clear sub-problems, you end up with vague end-to-end tests that fail for unclear reasons.

You can build in parallel. One team works on retrieval. Another works on synthesis. They agree on the interface. They build simultaneously. Without decomposition, everything is coupled and you can't parallelize.

## When Decomposition Changes Your Architecture

Sometimes decomposition reveals that your initial architecture is wrong.

The SaaS company initially thought they were building a conversational chatbot—a single system that handles dialogue. When they properly decomposed it, they realized they were actually building five separate automations for five different user tasks, plus a routing layer to figure out which automation handles which request, plus a dialogue wrapper to make it feel conversational.

That's a completely different architecture. Instead of one chatbot model, they needed: a classifier to route tasks, five task-specific pipelines, a state manager to track conversation context, and a response formatter to maintain conversational flow.

They would have discovered this eventually—in production, when the single-model approach failed. Decomposition surfaced it in the design phase, when architecture changes are cheap.

## The Difference Between Decomposition And Over-Engineering

You might be thinking: "Isn't this just over-engineering? Do I really need six sub-problems when I could just use one LLM call?"

Here's the difference: decomposition is conceptual before it's technical. You're breaking down the problem to understand it, not necessarily to build it as separate services.

You might decompose retrieval into query understanding, document retrieval, and ranking—three sub-problems. Then you implement all three in a single function. That's fine. The decomposition still helps because you can evaluate each piece, you understand the failure modes, and you know where to optimize.

Over-engineering is building six microservices when one function would do. Decomposition is understanding the six sub-problems even if you implement them together.

The key is: if you can't explain the sub-problems, you don't understand the system well enough to build it reliably. Whether you implement them as separate components is a separate decision.

## What This Looks Like In Practice

Let me show you what a team that does this right looks like.

They start a new AI project. Week one is decomposition. They don't write code. They write documents.

Document one: Business outcome. One page. What's the goal? What's the metric? What's success? Who signs off?

Document two: User tasks. Two pages. What are users trying to do? What's the current experience? What are the pain points? Backed by data and user interviews.

Document three: AI capabilities. Two pages. For each user task, what must AI do? What's feasible? What's the quality bar? What are the risks?

Document four: Technical sub-problems. Five pages. For each capability, what are the components? What are their interfaces? How do we evaluate each one?

They review these documents with stakeholders at each layer. They get sign-off. They identify gaps and risks. Then—only then—they start building.

This feels slow. It's not. It's dramatically faster than building the wrong thing, discovering it six months later, and starting over.

The teams that skip decomposition feel fast at the start. They ship demos in week three. They're in production by month two. Then they spend month three through month twelve fixing fundamental design flaws that proper decomposition would have caught in week one.

## The Question That Tests Decomposition

Here's how you know if you've decomposed properly: Can you explain to a non-technical stakeholder what happens at each layer and why it matters?

If someone asks: "Why are we building query understanding as a separate sub-problem?" you should be able to say: "Because users often ask ambiguous questions. If we understand their intent incorrectly, we retrieve the wrong information, which means even perfect synthesis produces the wrong answer. Query understanding is where we can catch and clarify ambiguity before it cascades."

If you can't explain it clearly, you haven't decomposed it enough. You're still in the "build AI stuff" phase, not the "build specific components that solve specific problems" phase.

## What You Should Do Next

If you're about to start an AI project, stop before writing code. Do the decomposition.

Write down the business outcome. Get it validated with metrics and sign-off.

Write down the user tasks. Validate them with real user research, not assumptions.

Write down the AI capabilities needed. Validate technical feasibility and quality expectations.

Write down the technical sub-problems. Validate that each has clear inputs, outputs, and success criteria.

If you can complete this decomposition cleanly, you're ready to build. If you can't, you're not. Keep decomposing until each layer is clear.

The teams that win aren't the ones who ship fastest. They're the ones who ship the right thing. And shipping the right thing starts with understanding what you're actually building—not at the "build AI" level, but at the "solve specific sub-problem X to enable capability Y to support user task Z to achieve business outcome W" level.

That's decomposition. And it's the difference between projects that work and projects that burn time and money before getting canceled.

In the next section, we'll dig into one of the most deceptive problems in AI: the gap between single-turn and multi-turn systems, and why teams that only decompose for single-turn interactions discover brutal complexity when conversations extend across multiple exchanges.
