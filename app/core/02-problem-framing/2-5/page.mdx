# 2.5 — Decomposing Multimodal Tasks

When a major healthcare startup launched their AI-powered medical imaging assistant in early 2025, they thought they had built one sophisticated system. The product promise was elegant: upload a radiology scan, get an instant preliminary report with highlighted areas of concern. Doctors loved the demo. Investors loved the pitch. But three months post-launch, the system was producing brilliantly written reports about completely wrong findings. The chest X-rays were being analyzed as if they were abdominal CT scans. The text generation was flawless, but it was describing hallucinated problems.

The root cause wasn't model quality or training data volume. The team had treated "analyze medical images and generate reports" as a single monolithic task. They hadn't decomposed the problem into its constituent parts: image ingestion and normalization, modality detection, anatomical region identification, pathology detection, clinical reasoning, finding prioritization, and natural language report generation. Each of these sub-problems had different failure modes, different evaluation needs, and different accuracy requirements. When they finally decomposed the system and evaluated each stage independently, they discovered their modality classifier was failing 8% of the time, silently poisoning everything downstream.

This is the challenge of multimodal AI: what looks like one task is actually a cascade of perception, understanding, reasoning, and generation sub-problems that span different modalities. Decomposition isn't just good engineering practice for multimodal systems — it's the only way to build them reliably.

## Why Multimodal Tasks Aren't Single Tasks

The fundamental mistake teams make with multimodal AI is treating the entire pipeline as a black box. When your system ingests an image and produces text, or processes audio and generates images, or analyzes documents and extracts structured data, you're not building one AI task. You're building a chain of distinct cognitive operations, each operating on different types of data, each with different technical characteristics.

Consider a document understanding system that extracts key information from insurance claims forms. The user experience is simple: upload a PDF, get structured data back. But decomposition reveals the true complexity. First, there's document ingestion — handling different file formats, page orientations, and scan qualities. Then optical character recognition to convert pixels to text. Layout understanding to identify which text belongs to which fields, which elements are tables versus paragraphs, which sections are form structure versus content. Table extraction as a specialized sub-problem, because tables have spatial relationships that pure text doesn't capture. Entity recognition to identify claim numbers, dates, dollar amounts, names. Semantic parsing to understand relationships between extracted entities. Validation logic to catch impossible values. And finally, formatting the output into your target schema.

That's eight distinct sub-problems, and each one can fail independently. Your OCR might work perfectly but your layout understanding might merge two separate fields into one. Your entity recognition might be excellent but your table extraction might miss column headers. If you evaluate this as a single end-to-end task, you only know that accuracy is 73%. You don't know which of the eight sub-problems is dragging down quality, which one to improve first, or where to allocate engineering resources.

Proper decomposition changes everything. You can now measure OCR character accuracy, layout segmentation F1 score, table extraction precision and recall, entity recognition accuracy by entity type, semantic parsing correctness, and validation coverage. You discover that OCR is 99.2% accurate, layout understanding is 96% accurate, but table extraction is only 71% accurate. Now you know where to focus.

## Separating Perception from Reasoning from Generation

The most important decomposition boundary in multimodal systems is between perception, reasoning, and generation. These are fundamentally different cognitive operations with different technical characteristics, different failure modes, and different evaluation approaches.

Perception is about converting raw sensory input into structured representations. In image systems, it's detecting objects, recognizing text, identifying faces, understanding spatial relationships. In audio systems, it's speech recognition, speaker identification, emotion detection, background noise separation. In video, it's frame analysis, motion tracking, scene segmentation. Perception failures are usually about missing information or misidentifying what's present.

Reasoning is about understanding meaning, making inferences, applying logic, and forming conclusions based on what was perceived. Given that an image contains a stop sign, reasoning determines whether the vehicle should stop based on context. Given that a document mentions a dollar amount and a date, reasoning determines whether this represents a payment, a budget, or a quote. Reasoning failures are usually about incorrect interpretations or faulty logic chains.

Generation is about producing output in a target modality. Creating natural language descriptions, synthesizing speech, generating images, formatting structured data. Generation failures are usually about fluency, appropriateness, or formatting issues, not about correctness of underlying understanding.

These boundaries matter because they have different bottlenecks and different solution approaches. A medical image analysis system that produces inaccurate reports might have a perception problem where tumors aren't being detected, a reasoning problem where detected anomalies aren't being correctly classified as malignant versus benign, or a generation problem where findings are being described ambiguously. The fixes are completely different. Perception problems might need better image preprocessing or more diverse training data. Reasoning problems might need improved medical knowledge bases or better logical inference chains. Generation problems might need better language models or more structured output templates.

When you decompose along perception-reasoning-generation boundaries, you can also optimize each stage differently. Perception often benefits from specialized models trained on large unlabeled datasets. Reasoning often benefits from structured knowledge and explicit logic. Generation often benefits from large language models fine-tuned for fluency and style.

## Cross-Modal Dependencies: When Understanding Spans Modalities

Some of the hardest decomposition decisions in multimodal systems involve cross-modal dependencies — when understanding in one modality depends on information from another modality. These dependencies reveal whether you truly have separable sub-problems or whether certain tasks must be solved jointly.

Consider a video content moderation system. You might initially decompose this into audio analysis (detecting offensive language), visual analysis (detecting prohibited content), and text analysis (processing on-screen text and captions). But what happens when someone says "this is fine" while showing something that clearly isn't fine? The sarcasm depends on the mismatch between audio and visual channels. You can't detect it by analyzing either modality alone.

This is a cross-modal dependency. The reasoning task requires aligned inputs from both perception tasks. Your decomposition needs to reflect this. You might have audio perception and visual perception as separate parallel tasks that feed into a joint reasoning task that analyzes both streams together. The dependency structure determines your architecture.

Cross-modal dependencies show up constantly in document understanding. A table in a research paper might have a caption that reads "Patient outcomes by treatment group." The table itself contains numbers. Neither the text nor the table structure alone gives you complete understanding — you need both. The table extraction task and the caption identification task can run independently, but the semantic understanding task depends on both.

Similarly, in presentation slide analysis, an image might show a product screenshot while the slide title reads "Customer Dashboard Performance Issues." Understanding what the image depicts requires reading the title. The image perception task and the text extraction task are separable, but the image interpretation reasoning task has a cross-modal dependency.

When you identify cross-modal dependencies during decomposition, you're revealing which sub-problems must be synchronized, which ones need to share context, and where your architecture needs join points. This prevents the classic mistake of building separate pipelines for each modality that never communicate, leaving you unable to handle cases where understanding requires multiple modalities.

## Document Processing as a Case Study in Multimodal Decomposition

Document processing systems provide an excellent case study in multimodal decomposition because they combine visual understanding (layout, structure), text understanding (content, meaning), and spatial reasoning (relationships between elements).

Start with the raw input: a scanned document, a PDF, or an image file. The first sub-problem is document ingestion and normalization — handling different file formats, converting PDFs to images if needed, correcting orientation, removing noise from scans, handling multi-page documents. This seems trivial until you encounter documents scanned upside down, sideways, or with coffee stains obscuring text.

Next comes optical character recognition — converting pixels into text. But OCR isn't a single task either. It includes text detection (where is text located?), text recognition (what do those characters say?), and text ordering (what's the reading sequence?). Each can fail independently. Text detection might miss small footnotes. Text recognition might confuse similar characters. Text ordering might incorrectly sequence two-column layouts.

Parallel to OCR, you have layout understanding — identifying visual elements like headers, paragraphs, tables, images, forms, signatures. This is a perception task operating on visual features, not text content. Layout understanding tells you this region is a table even before you've recognized what text is in it.

Table extraction is complex enough to be its own sub-problem. You need to detect table boundaries, identify rows and columns, recognize merged cells, extract cell contents, understand header relationships, and preserve the spatial relationships that give tables their meaning. A table with perfect OCR but broken structure is useless.

Once you have text and layout, semantic parsing begins. This is where you move from perception to reasoning. Which text blocks are field labels versus field values? What's a section header versus content? How do different elements relate to each other? This requires understanding document semantics, not just visual appearance.

Entity recognition and information extraction operate on the semantically parsed content. Identifying dates, amounts, names, addresses, claim numbers, policy numbers. This is a reasoning task that depends on both the text content and the structural understanding.

Finally, output formatting takes all the extracted and understood information and produces it in your target schema — JSON, database records, API payloads, whatever your system needs.

The critical insight is that each of these sub-problems has different failure modes, different accuracy levels, and different improvement strategies. When a document processing system has 80% end-to-end accuracy, decomposition reveals whether the problem is in OCR quality, layout understanding, table extraction, semantic parsing, or entity recognition. Without decomposition, you're blind.

## Voice Systems: The Five-Stage Decomposition

Voice-based AI systems make the serial nature of multimodal decomposition especially clear. When a user speaks to a voice assistant and gets a spoken response, that interaction passes through five distinct sub-problems: automatic speech recognition, natural language understanding, dialog management, natural language generation, and text-to-speech synthesis.

Automatic speech recognition converts audio to text. This sub-problem has to handle accents, background noise, speaking rate, pronunciation variations, and audio quality. ASR failures produce transcription errors — the system heard the wrong words.

Natural language understanding takes the transcribed text and extracts intent and entities. If someone says "set a timer for 15 minutes," NLU needs to identify the intent as setting a timer and extract "15 minutes" as the duration entity. NLU failures mean the system understood what you said but misinterpreted what you meant.

Dialog management decides what action to take based on the understood intent. It maintains conversation state, handles clarifications, manages multi-turn interactions, and routes to appropriate services. Dialog management failures manifest as the system taking the wrong action or losing context across turns.

Natural language generation creates the text response. It takes structured data about what happened and produces a fluent, natural response. NLG failures produce awkward phrasing, unnatural responses, or unclear communication, even when the right action was taken.

Text-to-speech converts the response text into audio. It handles pronunciation, prosody, emphasis, and naturalness. TTS failures sound robotic, mispronounce words, or have poor audio quality.

These five sub-problems are strictly serial — each depends on the previous one. But they fail independently. You might have excellent ASR but poor NLU, resulting in perfect transcriptions of misunderstood intent. You might have excellent NLU but poor dialog management, resulting in correct single-turn understanding but broken multi-turn conversations. You might take the right action but generate an awkward response.

Decomposition allows you to measure and improve each stage independently. You can evaluate ASR with word error rate, NLU with intent classification accuracy and entity extraction F1, dialog management with task completion rate, NLG with fluency and informativeness metrics, and TTS with audio quality and naturalness scores. Without decomposition, you only know that the end-to-end conversation succeeded or failed, which tells you almost nothing about what to fix.

## Why Multimodal Decomposition Reveals Bottlenecks

The most valuable outcome of multimodal decomposition is bottleneck identification. In any system with serial components, the slowest component determines overall latency. In any system with accuracy dependencies, the least accurate component often determines overall quality.

Imagine a video content analysis system that identifies products shown in social media videos for e-commerce integration. The system processes videos through several stages: video decoding and frame extraction, object detection in frames, product recognition for detected objects, brand identification, and catalog matching. End-to-end, the system takes 8 seconds per video and achieves 68% accuracy.

Without decomposition, you might invest in faster GPUs or better models generally. With decomposition, you measure each stage independently. Frame extraction takes 0.5 seconds. Object detection takes 4.2 seconds. Product recognition takes 2.8 seconds. Brand identification takes 0.4 seconds. Catalog matching takes 0.1 seconds. Object detection is your latency bottleneck — it's consuming more than half your total time.

For accuracy, you measure each stage separately. Object detection achieves 94% recall (finds 94% of products in frames). Product recognition achieves 89% accuracy on detected objects. Brand identification achieves 96% accuracy. Catalog matching achieves 99% accuracy. Your overall accuracy of 68% is roughly 0.94 × 0.89 × 0.96 × 0.99 = 78%, but that's the best-case where errors don't compound. Product recognition is your accuracy bottleneck.

Now you have actionable intelligence. To improve latency, optimize object detection — try a faster model architecture, reduce frame resolution, or skip redundant frames. To improve accuracy, focus on product recognition — collect more training data for ambiguous products, add a validation stage, or implement confidence thresholds.

This is why multimodal decomposition isn't optional for production systems. Without it, you waste resources optimizing components that aren't bottlenecks. With it, you focus improvement efforts where they matter most.

## The Reality Check for 2026

As of 2026, multimodal AI has become mainstream, but successful production systems all share a common pattern: rigorous decomposition. The teams shipping reliable multimodal products aren't treating perception, reasoning, and generation as a single black box. They're decomposing ruthlessly, evaluating each sub-problem independently, identifying bottlenecks, and optimizing strategically.

The healthcare imaging company from our opening story rebuilt their system with proper decomposition. They now have separate evaluation metrics for image modality classification (99.7% accuracy), anatomical region detection (98.1% accuracy), pathology detection (91.3% sensitivity, 96.8% specificity), clinical finding ranking (89% agreement with radiologist rankings), and report generation (4.2 out of 5 average quality score from physicians). When a report is wrong, they know exactly which stage failed. When they want to improve quality, they know which component to upgrade.

Multimodal decomposition reveals that the "AI" in your AI product is actually many different AI tasks, each with different characteristics, each needing different evaluation, each offering different optimization opportunities. The sophistication of your product isn't in treating it as one complex task — it's in breaking it into manageable sub-problems and orchestrating them effectively.

This decomposition discipline becomes even more critical when you add humans into the system, deciding where automation ends and human judgment begins for each sub-problem — a topic we turn to next.
