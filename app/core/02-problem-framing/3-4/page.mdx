# 3.4 — Extraction and Structuring Tasks

An insurance company built an AI system in early 2025 to extract claim information from scanned forms. The forms were semi-structured: printed templates filled in by hand or typed by claimants and medical providers. The system needed to pull out policy numbers, dates of service, procedure codes, diagnosis codes, provider names, and dollar amounts. On paper, this looked straightforward. The information existed in the documents. The fields were predefined. The task was just finding and extracting the right values.

They built the system using a modern multimodal LLM. They collected 500 labeled forms with correct extractions marked by experienced claims processors. They evaluated using exact match: did the extracted value match the labeled value character-for-character? Their initial results looked promising: 91% exact match on policy numbers, 87% on dates, 94% on procedure codes.

They shipped to production. Within two weeks, they discovered their evaluation had been dangerously misleading. The system was working great on clean forms but failing catastrophically on messy ones. Handwritten dates where "3" looked like "8" were extracted wrong. Policy numbers where an "O" could be zero or letter O were ambiguous. Provider names with middle initials sometimes included the initial, sometimes didn't. Dollar amounts with commas were sometimes extracted with the comma, sometimes without.

The exact match metric had hidden all these problems. A date extracted as "03/15/2024" when the label said "3/15/2024" counted as a complete failure, even though the values were equivalent. A provider name extracted as "Smith J. Medical Group" when the label said "Smith Medical Group" counted as wrong, even though both were defensible interpretations of the handwritten form.

The system was actually performing reasonably well. The evaluation metric was just measuring the wrong thing. They needed fuzzy matching for text fields, normalized comparison for dates and numbers, and tolerance for minor variations that didn't affect semantic meaning.

This is the paradox of extraction tasks. They seem more objective and easier to evaluate than generation tasks. The answer exists in the source material. You can verify correctness. But the objectivity is deceptive. Edge cases, format variations, ambiguous boundaries, and interpretation differences create massive evaluation complexity. Extraction is only easy to evaluate when you've solved all the hard problems about what counts as correct.

## What Makes Extraction Different

Extraction fundamentally differs from generation in one critical way: the information you're extracting already exists in the input. You're not creating new content. You're finding and pulling out specific pieces of information.

This creates verifiable ground truth. If an invoice shows "Total: $1,247.83," then that's the correct total to extract. If a contract states "30-day notice period," then that's the correct notice period. You can objectively verify whether an extraction is correct by checking it against the source material.

This verifiability makes extraction seem easier to evaluate than generation. In generation, multiple outputs can be equally good, and quality is subjective. In extraction, there's a right answer and wrong answers. You can measure accuracy objectively.

But this apparent simplicity conceals real complexity. What counts as the right answer when the source material is ambiguous? What about when there are multiple valid ways to represent the same information? What about partial matches, overlapping entities, or nested structures? What about when the extraction task requires interpretation or normalization?

The output space in extraction is constrained by the input but not uniquely determined by it. Different extraction approaches might structure the same information differently, all validly. Different annotators might make different judgment calls at boundaries and edge cases. The source material might be messy, ambiguous, or incomplete.

Extraction sits in an interesting middle ground. More constrained and verifiable than generation. Less mechanically objective than classification. Easier to evaluate when your definitions are crisp. Surprisingly hard to evaluate when you account for real-world messiness.

## Types of Extraction Tasks

Extraction tasks come in several flavors, each with distinct characteristics and evaluation challenges.

**Entity extraction** identifies and extracts specific entities from text: names, dates, locations, organizations, monetary amounts, percentages, product names, or other defined entity types. The challenge is boundary detection (where does the entity start and end?), entity typing (is "Apple" a fruit or a company?), and handling overlapping or nested entities. Evaluation uses precision (how many extracted entities are correct?), recall (how many true entities were found?), and often distinguishes between strict matching (exact boundary match) and lenient matching (overlapping boundaries acceptable).

**Data structuring** converts unstructured or semi-structured input into structured formats: turning prose into tables, extracting JSON from documents, populating database records from free text, or building knowledge graphs from narratives. The challenge is deciding what information maps to which fields, handling missing values, and representing relationships. Evaluation checks field-level accuracy, schema compliance, and information completeness. You verify that all source information made it into the structure and that no hallucinated information was added.

**Key-value extraction** pulls out attribute-value pairs from documents: extracting line items from invoices, pulling specifications from datasheets, getting metadata from forms, or extracting parameters from technical documents. The challenge is identifying which text represents a key versus a value, handling multi-value attributes, and dealing with hierarchical or nested key-value structures. Evaluation measures extraction accuracy per key type and checks that values are correctly associated with their keys.

**Relationship extraction** identifies connections between entities: who did what to whom, what caused what, which items belong to which categories, or how concepts relate to each other. The challenge is that relationships are often implicit rather than explicitly stated, and relationship types might be ambiguous. Evaluation verifies that extracted relationships are present in the source and correctly represent the stated or implied connections.

**Summarization-as-extraction** pulls out key points, main ideas, or salient information without generating new text. This differs from abstractive summarization (which is generation) by constraining the summary to information and often phrases directly from the source. The challenge is defining what counts as a key point and how much paraphrasing is acceptable. Evaluation checks that summary points are grounded in the source and that important information isn't omitted.

**Form filling** populates predefined fields from unstructured sources: filling out intake forms from consultation notes, populating shipping forms from customer emails, or completing reports from raw data. The challenge is handling fields where the information isn't explicitly stated in the source or where inference is needed. Evaluation checks field-by-field accuracy and flags fields that were filled with information not justified by the source.

## The Edge Cases That Make Extraction Hard

Extraction would be straightforward if source material were clean, unambiguous, and well-formatted. In production, it never is. The edge cases are where extraction evaluation gets complicated.

**Ambiguous boundaries**: Where does one entity end and another begin? In "Dr. John Smith Jr.," is the extracted name "John Smith," "Dr. John Smith," "John Smith Jr.," or "Dr. John Smith Jr."? Different annotators might make different choices. Different use cases might need different granularity. Your evaluation needs to decide which boundaries are acceptable and whether variations that include or exclude titles, suffixes, or middle names count as correct.

**Format variations**: The same information can be represented in different formats. Dates can be "3/15/2024," "03/15/2024," "March 15, 2024," "2024-03-15," or "15 Mar 2024." Phone numbers can be "(555) 123-4567," "555-123-4567," "555.123.4567," or "5551234567." Are these all correct extractions of the same value? Your evaluation needs to decide whether to require format normalization and what formats are acceptable.

**Partial matches**: What if you extract "Smith Medical Group" when the full name in the document is "Smith Medical Group, Inc."? Is that close enough? What if you extract "approximately $50,000" when the document says "$50,000"—should you include the qualifier? What if you extract "Q1 2024" when the document says "the first quarter of 2024"? Partial matches sit in the fuzzy zone between correct and incorrect. Your evaluation needs clear rules about when partial is acceptable.

**Overlapping entities**: Sometimes entities overlap or nest. In "Apple CEO Tim Cook," you have a company entity "Apple," a person entity "Tim Cook," and a relationship between them. Do you extract one entity, both entities, or both plus the relationship? Different extraction tasks might make different choices. Your evaluation needs to account for whatever representation your task requires.

**Multi-value fields**: Some fields have multiple valid values. A medical record might list multiple diagnoses, multiple medications, or multiple procedures. Do you need to extract all of them? In what order? What if your system extracts three out of four values—is that 75% correct or completely wrong? Your evaluation needs to handle multi-value extraction with appropriate metrics like set-based precision and recall.

**Implicit information**: Sometimes the information you need to extract isn't explicitly stated. A document might say "we'll reach out next week" without specifying a date. Or it might say "the usual discount applies" without stating the percentage. If your extraction task requires filling these fields, does the system leave them blank, make an inference, or mark them as uncertain? Your evaluation needs to handle cases where perfect extraction isn't possible from the source material alone.

**Conflicting information**: Source documents sometimes contradict themselves. A form might have one total in the summary section and a different total when you sum the line items. A contract might state one deadline in the body and a different one in an amendment. When your extraction system encounters conflicts, what should it do? Extract both? Choose one? Flag the conflict? Your evaluation needs to account for how your system handles contradictions.

## Evaluation Strategies for Extraction

Given these complexities, how do you evaluate extraction tasks effectively? The answer depends on your specific task requirements and tolerance for different types of errors.

**Exact match** is the strictest evaluation: the extracted value must match the reference value character-for-character. This makes sense for identifiers that have canonical formats (policy numbers, product codes, IDs) or numbers where precision matters (dollar amounts in financial contexts, dosages in medical contexts). Exact match is objective and unambiguous but intolerant of minor variations that might not matter for your use case. Use it when precision is critical and when formats are standardized.

**Normalized match** first normalizes both the extracted value and the reference value to a standard format, then compares. Dates get converted to a standard date format. Numbers get converted to standard numeric format with consistent decimal separators and precision. Phone numbers get converted to a standard pattern. Names get normalized for capitalization and whitespace. This catches format variations that don't represent real errors. Use it when the semantic value matters more than the specific representation.

**Fuzzy match** allows for minor differences like typos, spacing variations, or small edits. String similarity metrics like Levenshtein distance measure how many character changes would be needed to turn the extracted value into the reference value. You set a threshold: if similarity is above the threshold, count it as correct. This is useful for text fields where human data entry might introduce minor variations. Use it carefully—fuzzy matching can hide real errors if the threshold is too lenient.

**Token-based match** compares at the word level rather than character level. "Smith Medical Group" and "Medical Group Smith" might be considered equivalent because they contain the same tokens. This is more forgiving of word order variations but risks accepting extractions that change meaning. Use it when token presence matters more than order, but validate that meaning is preserved.

**Field-level accuracy** evaluates structured extraction by measuring what percentage of fields were extracted correctly. For a form with twenty fields, if your system got eighteen right, that's 90% field-level accuracy. You can weight fields differently if some are more important. This gives you overall system performance and per-field breakdown showing which fields are problematic. Use it for structured extraction where you're populating multiple fields from a source.

**Precision and recall** measure extraction completeness and accuracy. Precision is what percentage of your extractions are correct. Recall is what percentage of the true values you successfully extracted. F1 score combines both. These metrics are essential when you're extracting variable numbers of entities from each source (all the names mentioned in a document, all the dates, all the dollar amounts). You need to know both whether your extractions are correct and whether you're missing things.

**Set-based metrics** evaluate multi-value extractions by treating the extracted values as a set and comparing to the reference set. Exact set match requires extracting exactly the right set. Set precision and recall measure how much of your extracted set is correct and how much of the true set you found. This handles multi-value fields properly where order doesn't matter but completeness does.

**Boundary evaluation** specifically measures whether entity boundaries are correct. Strict boundary matching requires exact start and end positions. Lenient boundary matching accepts partial overlap. This matters for entity extraction where getting the right span is important (extracting "Dr. Smith" versus "Smith" versus "Dr. John Smith").

**Downstream validation** checks whether extracted values are usable for their intended purpose. If you extract a date, can you parse it? If you extract a dollar amount, can you perform arithmetic with it? If you extract a name, does it appear in your customer database? This catches extractions that look plausible but don't work in practice. Use it as a supplement to direct accuracy metrics.

## Ground Truth for Extraction Tasks

Ground truth collection for extraction requires careful attention to ensure annotations are correct, consistent, and complete.

The collection process starts with clear annotation guidelines. What counts as a valid entity? Where should boundaries be drawn? What format should values use? How should annotators handle edge cases? The guidelines need examples of correct annotations and common mistakes to avoid.

You need skilled annotators who understand the domain. Extracting medical information from clinical notes requires medical knowledge. Extracting legal terms from contracts requires legal expertise. Extracting financial data from statements requires financial literacy. Generalist annotators will make mistakes that domain experts would catch.

The annotation process should support efficiency and accuracy. For structured extraction, use annotation tools that let annotators highlight text and assign it to fields. For entity extraction, use tools that let annotators span text and label entity types. Good tooling reduces annotation time and annotation errors.

Quality control is critical. Have multiple annotators label the same examples and measure inter-annotator agreement. Where annotators disagree, that reveals ambiguity in your guidelines or genuine ambiguity in the source material. Resolve disagreements through discussion and update guidelines to address the ambiguous cases.

Collect edge cases deliberately. Your ground truth dataset should include not just clean examples but messy ones: ambiguous boundaries, format variations, partial information, conflicting information, unusual layouts. Edge cases are where your system will fail in production. Your evaluation needs to test them.

The volume you need depends on the complexity and variability of your extraction task. For simple extraction from standardized forms, you might need hundreds of labeled examples. For complex extraction from highly varied documents, you might need thousands. For extraction tasks with many field types or entity types, you need enough examples of each type to measure per-type performance.

Document annotation decisions. When your annotator makes a judgment call, have them document why. This creates institutional knowledge about how to handle ambiguous cases. It also helps you understand whether your guidelines are complete or need refinement.

## The 2026 Context: Complex Document Extraction

In 2026, extraction has become a core enterprise use case for LLMs, particularly for complex document processing. The multimodal capabilities of modern models mean you can extract from PDFs, scanned images, tables, forms, and mixed-format documents without complex preprocessing pipelines.

This opens up use cases that were impractical with earlier technology: extracting data from multi-page contracts where relevant information is scattered across sections, pulling structured information from scanned handwritten forms, extracting tables and figures from research papers, processing invoices that vary widely in format across vendors.

The evaluation challenge has shifted. Early extraction systems handled standardized, clean documents. You could rely on templates and rules. Evaluation was about measuring accuracy on well-defined fields. Modern extraction systems handle messy, varied, complex documents. Evaluation must account for interpretation requirements, layout understanding, and handling of incomplete or ambiguous information.

Your extraction evaluation in 2026 needs to test robustness, not just accuracy on clean examples. Does your system handle documents where the information is present but not in the expected location? Does it handle documents with extra information your schema doesn't model? Does it gracefully degrade when information is missing or ambiguous rather than hallucinating values?

The opportunity is huge. Extraction is more verifiable than generation, more valuable than classification, and newly feasible at scale with modern models. But the evaluation must be rigorous. Extraction failures in production have clear downstream impacts: financial errors, compliance failures, operational mistakes. Your evaluation system must catch these failures before they reach production.

That rigor starts with understanding what makes extraction distinct from other task types and building evaluation strategies that match the specific challenges of pulling structured information from messy reality. Once you've mastered extraction evaluation, you can ship extraction systems with confidence that they'll perform in production as they did in evaluation.

Next, we'll examine classification and routing tasks, where the bounded output space makes evaluation more tractable but getting the boundaries right is the hard part.
