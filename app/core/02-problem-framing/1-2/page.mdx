# 1.2 — What Problem Framing Actually Is (and Is Not)

A product manager walks into a meeting with a one-page brief titled "Problem Definition for AI Assistant Project." The document states: "Sales reps spend too much time writing follow-up emails. We need an AI assistant to automate this task. Success will be measured by adoption rate and time saved."

The engineering lead reads it and asks, "What makes a good follow-up email in this context?" The product manager replies, "You know, something professional and personalized." The engineer pushes: "What does personalized mean? What information needs to be included? How do we know if the AI-generated email is better or worse than what a human would write?" The product manager says, "We can figure that out as we go."

This is not problem framing. This is a vague goal statement with a proposed solution tacked on. It tells you what to build—an AI assistant—but it does not tell you what problem you are actually solving, what success looks like, or how you will know if you have succeeded.

Real problem framing would decompose "sales follow-up emails" into specific sub-tasks. It would define what "good" means in measurable terms. It would specify the inputs available (CRM data, meeting notes, previous email threads) and the required outputs (email draft with specific sections, suggested next steps, tone constraints). It would identify edge cases (what if the meeting went poorly? what if the prospect asked a technical question the rep cannot answer?). It would clarify non-goals (we are not trying to replace sales reps, just save them time on routine tasks).

The difference between the vague brief and real problem framing is the difference between a destination and a map. "Go west" is a destination. A map shows you the terrain, the obstacles, the decision points, and the criteria for knowing when you have arrived.

## Problem Framing vs. Adjacent Activities

Problem framing is often confused with activities that feel similar but serve different purposes. Let's distinguish it from the things it is not.

**Problem framing is not requirements gathering.** Requirements gathering is about collecting constraints and specifications from stakeholders. "The system must respond in under 200 milliseconds." "The output must be formatted as JSON." "The system must support 10,000 concurrent users." These are important, but they come after you have framed the problem. You cannot gather requirements for a system until you know what task the system is performing. Requirements gathering assumes you already know what you are building. Problem framing is the step where you figure out what you should be building in the first place.

**Problem framing is not product discovery.** Product discovery is the process of figuring out whether a problem is worth solving and whether users will adopt a solution. It involves user interviews, market research, prototyping, and validation. Discovery answers the question "Should we build this?" Problem framing answers the question "If we build this, what exactly are we building, and how will we know if it works?" Discovery helps you decide to invest in a problem. Framing helps you solve the problem correctly once you have committed to it.

**Problem framing is not prompt engineering.** Prompt engineering is the craft of writing effective instructions for language models. It is a technical skill focused on model interaction. Problem framing happens before you touch a model. It is the work of defining the task so precisely that prompt engineering becomes straightforward. If your problem framing says "generate a professional follow-up email that includes a summary of action items and a suggested next meeting time," then prompt engineering is easy—you just translate that spec into model instructions. If your problem framing is vague, prompt engineering becomes a guessing game, and you will waste weeks tweaking prompts without knowing what you are optimizing for.

**Problem framing is not project scoping.** Project scoping is about estimating effort, allocating resources, and setting timelines. "This will take three engineers two months." Problem framing defines the task that will take three engineers two months. You cannot scope a project accurately until you have framed the problem, because you do not know how complex the solution will be until you understand the problem in detail.

**Problem framing is not writing a specification document.** A specification document is an output of problem framing, but it is not the same thing as the framing itself. Problem framing is a thinking process. It is the work of asking questions, challenging assumptions, decomposing vague goals into concrete tasks, and making implicit criteria explicit. The document that results from this process is valuable, but the real value is in the thinking that produced the document. Teams that treat framing as "fill out the template" miss the point entirely.

## Where Framing Sits in the AI Development Lifecycle

AI product development typically follows a lifecycle that looks something like this:

1. Identify a business goal or user need
2. Decide that AI might be part of the solution
3. Frame the problem
4. Gather and prepare data
5. Choose a model architecture
6. Train and tune the model
7. Evaluate the model
8. Deploy the system
9. Monitor performance in production

Problem framing happens at step three, after you have decided that AI is worth exploring but before you start building anything. It is the bridge between the high-level goal and the technical implementation.

The mistake teams make is treating step three as a formality. They assume the problem is already framed because someone wrote a one-pager or because the goal seems obvious. "We need a recommendation engine" feels like a framed problem. It is not. A well-framed version would specify:

- What are we recommending? (Products? Content? Actions?)
- To whom? (New users? Returning users? Specific segments?)
- In what context? (Homepage? Search results? Email?)
- What makes a recommendation good? (Relevance? Novelty? Diversity? Click-through? Purchase conversion?)
- What are the constraints? (Latency? Explainability? Diversity requirements?)
- What are the failure modes we care about? (Recommending out-of-stock items? Filter bubbles? Offensive content?)
- How will we measure success? (Online A/B test? Offline metrics? User surveys?)

This level of detail is not pedantic. It is necessary. Every one of these questions will come up during implementation. If you answer them during framing, you make clear, confident decisions. If you skip framing, you answer them ad hoc under time pressure, often inconsistently, and you end up with a system that does not quite work.

Framing also determines what happens in every subsequent step. The data you gather depends on the inputs you defined during framing. The model architecture you choose depends on the task structure. The evaluation strategy depends on the success criteria. The monitoring plan depends on the failure modes you identified. If framing is sloppy, every downstream step inherits that sloppiness.

## What a Well-Framed Problem Looks Like

A well-framed problem has five key characteristics:

**First, it is specific.** Vague framing: "Help users find relevant information." Specific framing: "Given a user query in natural language, return the top five internal documents ranked by relevance, where relevance is defined as containing answers to the user's question, measured by human raters using a three-point scale."

The specific version tells you exactly what the system does, what the input is, what the output is, and how quality is defined. You could hand this to an engineer and they would know what to build.

**Second, it is decomposed.** Complex problems are broken into smaller, well-defined sub-tasks. Instead of "build a virtual assistant," a well-framed problem might decompose the assistant into:

- Intent classification: which of twelve predefined user intents does this query match?
- Entity extraction: pull out relevant entities like dates, names, project IDs.
- Response generation: given intent and entities, generate a response using template filling or language model generation.
- Escalation logic: if confidence is below threshold or intent is outside scope, route to human.

Each sub-task can be built, evaluated, and iterated on independently. This makes the system modular, testable, and easier to debug.

**Third, it includes explicit success criteria.** Not just "the system should work well," but "success means 90 percent of users rate the output as helpful or better, and the system handles queries in under one second." Success criteria should be measurable, and they should connect to the actual business or user goal.

**Fourth, it specifies inputs and outputs precisely.** What data is available at inference time? What format is it in? What is the expected output format? What happens if required inputs are missing? A well-framed problem answers these questions upfront, so you do not discover halfway through development that a critical input is not actually available in production.

**Fifth, it identifies constraints and non-goals.** Constraints: "Must run on device, cannot send data to cloud." "Must provide explanations for every decision." "Must handle queries in 15 languages." Non-goals: "We are not trying to handle adversarial inputs." "We are not optimizing for rare edge cases that occur in fewer than 0.1 percent of queries." Being explicit about what you are not trying to do is just as important as defining what you are trying to do, because it prevents scope creep and focuses effort.

## What a Poorly Framed Problem Looks Like

Poorly framed problems share common red flags:

**Vague success criteria.** "The system should be accurate." Accurate according to whom? Measured how? What level of accuracy is acceptable? If you cannot answer these questions precisely, you cannot build or evaluate the system.

**Solution disguised as a problem.** "We need a chatbot" is a solution. The problem might be "users cannot find answers in our documentation" or "support tickets are overwhelming our team." Starting with a solution prevents you from considering alternatives that might work better.

**Missing task decomposition.** "Build an AI that understands customer feedback" is not a framed problem. It is a vague aspiration. Does "understand" mean sentiment classification? Topic extraction? Root cause analysis? Action item generation? All of the above? Until you decompose "understand" into specific tasks, you cannot build anything.

**No connection between framing and evaluation.** If your problem framing does not tell you how to measure success, it is incomplete. You should be able to read a well-framed problem and immediately sketch an eval plan. If you cannot, the framing is missing critical detail.

**Implicit assumptions that are never validated.** "Users want personalized recommendations" might sound obvious, but it is an assumption. Maybe users want serendipity, not personalization. Maybe they want transparency about why something is recommended. If your framing assumes user preferences without validating them, you are building on sand.

## The Key Outputs of Problem Framing

When problem framing is done well, you should be able to produce the following outputs:

**Task decomposition:** A breakdown of the high-level goal into specific, well-defined sub-tasks. Each sub-task should be narrow enough that you can describe its input, output, and success criteria in a few sentences.

**Success criteria:** A clear definition of what "good" looks like, measurable and connected to user or business value. Ideally, success criteria come in layers: technical metrics (latency, accuracy), user metrics (satisfaction, engagement), and business metrics (revenue, cost savings).

**Input-output specifications:** Exactly what data the system receives at inference time and exactly what it should produce. Include data types, formats, edge cases (missing data, malformed inputs), and examples.

**Constraints:** Technical constraints (latency, throughput, cost), regulatory constraints (privacy, compliance), user experience constraints (explainability, controllability), and operational constraints (monitoring, debugging).

**Non-goals:** Things you are explicitly choosing not to optimize for or not to support. This prevents scope creep and keeps the team focused.

**Failure modes and risk assessment:** What are the ways this system could fail, and what are the consequences? This feeds directly into eval design and monitoring strategy.

If you can produce these outputs for your problem, you have framed it well. If you cannot, you are not ready to start building.

## Why Framing Is a Thinking Discipline, Not a Document Template

Many organizations try to standardize problem framing by creating templates. "Fill out this form before starting an AI project." The form has sections for problem statement, success metrics, data sources, and so on.

Templates can be useful as checklists, but they are not a substitute for thinking. Problem framing is not a box-checking exercise. It is a process of interrogating assumptions, uncovering hidden complexity, and making implicit knowledge explicit.

A template might ask, "What is the success metric?" but it does not force you to think about whether that metric actually aligns with the goal, whether it can be gamed, or whether optimizing for it will produce unintended consequences. That kind of thinking requires experience, judgment, and often collaboration with domain experts, users, and stakeholders.

Good problem framing involves asking questions like:

- Who is the end user, and what do they actually need?
- What does success look like from their perspective, not just from a technical perspective?
- What happens if the system is wrong? How bad is the failure?
- What are we assuming that might not be true?
- What is the simplest version of this problem that would still deliver value?

These questions do not fit neatly into a form. They require conversation, iteration, and sometimes going back to users or stakeholders to validate assumptions.

The best framings emerge from a process that looks more like investigative journalism than form-filling. You start with a vague goal. You interview users and stakeholders. You look at data. You prototype the input-output interface. You identify edge cases. You debate trade-offs. You write down what you learned. You revise as you learn more.

This process takes time—sometimes days, sometimes weeks, depending on problem complexity—but it is time well spent. A week of rigorous framing can save months of building the wrong thing.

## Framing as a Shared Mental Model

One of the most valuable outcomes of problem framing is not the document itself but the shared understanding it creates across the team. When engineers, product managers, designers, and domain experts all agree on what the problem is, what success looks like, and what the system should do, decisions become faster and more consistent.

Without shared framing, teams argue endlessly about details. "Should we optimize for precision or recall?" "Should we support this edge case?" "Should we retrain the model weekly or monthly?" These questions have no right answer in the abstract. The right answer depends on the problem framing—what matters to users, what the consequences of errors are, what the operational constraints are.

When framing is clear, these debates resolve quickly. "We said precision is more important because false positives are costly. So we optimize for precision." "We said we are not supporting rare edge cases in version one. So we defer this." The framing acts as a tie-breaker and a north star.

When framing is vague, every decision becomes a negotiation, and different team members operate with different mental models of what they are building. The result is a system that feels incoherent, where different parts optimize for different goals, and no one is quite sure what success looks like.

## The Bridge to Everything Else

Problem framing is foundational because it determines everything that comes after. A well-framed problem makes data collection straightforward—you know exactly what inputs and labels you need. It makes model selection easier—you know what kind of task you are solving. It makes evaluation possible—you have clear success criteria and can design tests around them. It makes deployment safer—you have identified failure modes and can monitor for them.

A poorly framed problem poisons every downstream step. You collect data without knowing what labels you need, so you end up with noisy or irrelevant annotations. You choose a model based on what is popular rather than what fits the task. You cannot evaluate effectively because you do not have clear success criteria. You deploy and hope for the best because you do not know what to monitor.

The next step is understanding how framing connects to evaluation, and why every decision you make during framing directly determines what you can measure downstream.
