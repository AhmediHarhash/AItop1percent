# 2.11 — User Journey Mapping: Where AI Sits in the Workflow

The fraud detection model was technically perfect. It caught 94% of fraudulent transactions with only 2% false positives—numbers that exceeded every benchmark. The data science team celebrated. Six weeks after deployment, the fraud ops team was in revolt.

The problem wasn't accuracy. It was workflow. The model flagged suspicious transactions and dumped them into a queue for human review. But the fraud analysts couldn't do anything with the queue because the model gave no context. A flagged transaction showed up as "Transaction ID 47382910, flagged for review." The analyst then spent five minutes pulling up the customer's history, checking recent activity patterns, reading notes from previous cases, and verifying merchant information—all context the model had access to during inference but never passed along.

The fraud analysts were slower with the AI than without it. Worse, the model's output format didn't match the case management system's input format. Analysts had to manually copy transaction IDs from the model's queue into the case management system, one by one, triggering the lookup process. A technically perfect model was creating busywork instead of reducing it.

The data scientists had optimized the model in isolation. They never mapped the user journey. They never asked: what happens before a transaction reaches the model? What happens after the model flags it? What information flows through each step? What tools do analysts use? What format do those tools expect? They built a technically excellent component that failed as part of a system because they ignored the workflow it sat inside.

## The Invisible Workflow: What Happens Before and After AI

AI systems don't exist in a vacuum. They exist as steps in a user workflow. Someone does something before interacting with your AI. Someone does something after. Understanding those before-and-after steps is not optional—it determines whether your AI creates value or friction.

User journey mapping starts with a simple exercise: draw the full workflow. Not just the AI part—the entire process from beginning to end, including all human steps, all tools, all handoffs, all decision points.

For the fraud detection system, the full journey looked like this:

**Step 1:** Payment processor receives transaction and runs basic checks (billing address validation, card verification).

**Step 2:** Transaction data flows into the fraud detection model.

**Step 3:** Model scores transaction and flags high-risk ones.

**Step 4:** Flagged transactions appear in analyst queue.

**Step 5:** Analyst pulls transaction from queue.

**Step 6:** Analyst looks up customer history in CRM system.

**Step 7:** Analyst checks recent activity in transaction database.

**Step 8:** Analyst reviews merchant information in merchant database.

**Step 9:** Analyst reads notes from previous fraud cases involving this customer.

**Step 10:** Analyst makes decision: approve, decline, or escalate.

**Step 11:** Decision gets logged in case management system.

**Step 12:** If declined, customer service is notified to handle customer complaint.

**Step 13:** If approved, transaction proceeds to settlement.

The AI model was step 3 in a 13-step process. Optimizing step 3 in isolation was pointless if it made steps 6 through 9 harder.

Journey mapping reveals this. It forces you to see the AI as one component in a system, not the entire system. And once you see the full journey, you can design the AI to fit the workflow instead of forcing the workflow to adapt to the AI.

## Where AI Replaces vs Augments vs Adds

AI can intervene in a workflow three ways: it can replace a step, augment a step, or add a new step. Each has different implications for design and adoption.

**Replacing a step** means the AI does something a human used to do, and the human stops doing it. In the fraud detection system, the AI replaced manual rule-based scoring. Before the AI, analysts applied a checklist: is the transaction over 500 dollars? Is it from a new device? Is it international? The AI replaced that checklist with a learned model. The human step disappeared from the workflow.

Replacement works when the AI can perform the step more accurately, faster, or more consistently than a human, and when the step is well-defined enough that you can verify the AI does it correctly. Replacement fails when the step requires judgment the AI lacks, when errors are costly, or when users don't trust the replacement.

**Augmenting a step** means the AI helps a human do something they still do, but makes them faster or better. In the fraud detection system, the AI could have augmented step 6 through 9 by automatically pulling customer history, recent activity, and merchant information and presenting it in a summary alongside the flag. The analyst still makes the decision, but makes it faster because the prep work is done.

Augmentation works when the human has expertise or judgment the AI lacks, but the AI can reduce tedious work. It also works when errors are high-stakes and you need a human in the loop, but the human's time is the bottleneck. Augmentation is often safer than replacement because the human remains accountable.

**Adding a step** means the AI introduces something new to the workflow that wasn't there before. For example, the fraud detection model could add a step: after an analyst approves a flagged transaction, the AI could generate a brief explanation of why it flagged it and what the analyst's reasoning was for overriding, then save that explanation for future training. This step didn't exist before—it's new workflow enabled by AI.

Adding steps is risky because it increases workflow length. Every additional step is a chance to slow people down or create frustration. Adding a step only makes sense if the step unlocks new value: better auditability, faster learning, improved compliance. Otherwise, you're just adding busywork.

The best AI systems do all three: replace tedious steps, augment high-judgment steps, and selectively add steps that unlock new capabilities. But this requires mapping the journey first so you know which steps are tedious, which require judgment, and where new steps create value.

## The Context Your AI Inherits

AI doesn't start with a blank slate. It inherits context from upstream steps in the workflow. Understanding what context flows into your AI is critical for designing its inputs and setting expectations for its outputs.

Consider a customer support chatbot. The chatbot doesn't exist in isolation—it appears after a user has already navigated to the help center, browsed FAQs, failed to find an answer, and clicked "chat with support." That journey gives the chatbot critical context: the user is already frustrated (they didn't find an answer), they've already seen certain FAQs (don't repeat them), and they expect quick resolution (they're in a hurry).

If the chatbot ignores this context and starts with "Hi, how can I help you today?" it forces the user to re-explain their problem from scratch. Better design: "I see you were looking at the billing FAQ. Can I help with a billing question?" The chatbot acknowledges the upstream journey and uses it to shortcut the conversation.

Inherited context comes in many forms:

**User state:** Is the user logged in? What's their account status? What tier of service do they have? If your AI system doesn't have access to this state, it can't personalize responses or enforce access controls.

**Prior actions:** What did the user just do? Did they submit a form? Click a link? Upload a file? Prior actions reveal intent. A user who just clicked "cancel subscription" and then opens a chatbot probably has a retention-related question, not a sales question.

**Temporal context:** What time is it? What day of the week? Is this a holiday? Temporal context affects urgency. A support request at 11 PM is more urgent than one at 11 AM because the user can't wait for business hours.

**System state:** What's the status of upstream services? Is the payment processor down? Is there a known outage? If the user is asking "why can't I check out?" and the payment processor is down, your AI should know that and say "we're experiencing payment issues right now" instead of troubleshooting the user's device.

The worst AI systems ignore inherited context and force users to manually provide it. A chatbot that asks "are you a customer or a prospect?" when the user is logged into their account is wasting time. A recommendation engine that suggests products the user just bought is ignoring purchase history. A voice assistant that asks "what city are you in?" when the user's phone has location enabled is ignoring available context.

Good AI design audits the workflow: what information is already available upstream? Can we pass it to the AI? If we can't (because of system limitations or privacy constraints), can we at least acknowledge the gap and ask the user politely instead of pretending we don't need it?

## The Expectations Downstream Steps Have

Just as your AI inherits context from upstream steps, downstream steps have expectations for your AI's output. If your output doesn't match those expectations, you create integration friction.

Return to the fraud detection system. The downstream step after the model flagged a transaction was "analyst reviews it in the case management system." The case management system expected structured input: transaction ID, customer ID, risk score, reason codes, supporting evidence. The model output a JSON blob with transaction ID and a score. That's it. No reason codes, no evidence.

The gap meant analysts couldn't use the model output directly. They had to manually translate "transaction flagged" into the format the case management system required. This was pure waste—a format mismatch that added minutes to every case.

Designing AI for workflow integration means understanding downstream expectations:

**Format expectations:** What structure does the next step need? If the downstream system expects CSV, don't output JSON. If it expects a specific schema (defined fields, specific data types), match that schema. If you can't match it exactly, build a thin translation layer—but don't force humans to do translation manually.

**Latency expectations:** How fast does the downstream step need the output? If a customer is waiting on hold, you have seconds. If the downstream step is a nightly batch job, you have hours. Latency requirements shape model architecture (fast simple models vs slow accurate ones) and infrastructure (synchronous APIs vs asynchronous queues).

**Reliability expectations:** How often can the AI fail before the downstream step breaks? If the downstream step is fully automated, AI failures halt the workflow unless you design fallbacks. If a human is in the loop, occasional failures are tolerable. High-reliability expectations require defensive design: fallback models, error handling, retries, circuit breakers.

**Completeness expectations:** Does the downstream step need every case handled, or can it tolerate gaps? A content moderation system that flags 80% of violations might be acceptable if human moderators review the rest. A billing system that processes 80% of invoices and drops the rest is unacceptable. Completeness requirements determine whether your AI needs to handle edge cases or can defer them.

**Auditability expectations:** Does the downstream step need to know why the AI made a decision? For fraud detection, yes—analysts need to explain decisions to customers. For email spam filtering, no—users don't care why an email was flagged, they just delete it. Auditability requirements determine whether you need explainability features (reason codes, highlighted evidence, counterfactual explanations) or can skip them.

Ignoring downstream expectations creates integration debt. The AI works in isolation, but duct tape and manual workarounds hold the workflow together. Eventually, the workarounds become so painful that teams replace the AI not because it's inaccurate but because it's too hard to integrate.

## Hidden Requirements Revealed by Journey Mapping

The most valuable outcome of journey mapping is discovering requirements that aren't obvious when you only think about the AI in isolation. These hidden requirements determine whether your system succeeds or fails in production.

**The support agent who needs to see reasoning:** You're building a customer support chatbot. Journey mapping reveals that when the chatbot can't resolve an issue, it escalates to a human agent. The agent needs to see the conversation history (easy) but also the chatbot's reasoning (hard). Why did the chatbot suggest solution A instead of solution B? What information was it missing that caused escalation? If the agent can't see this, they have to start from scratch, negating the chatbot's value. Hidden requirement: explainability for human handoff.

**The billing system that needs structured output:** You're building an AI system that extracts invoice data from PDFs. Journey mapping reveals the output feeds directly into an accounting system with a fixed schema. The accounting system expects line items with specific fields: item description, quantity, unit price, tax rate, total. Your extraction model outputs freeform text. Hidden requirement: structured extraction with schema validation, or a post-processing step that transforms freeform output into the required schema.

**The compliance team that needs audit logs:** You're building a loan approval AI. Journey mapping reveals that every decision must be auditable for regulatory compliance. When a loan is denied, the company must be able to show why—not just "the model said no" but specific factors that drove the decision. The compliance team needs logs: what data went into the model, what the model predicted, what decision was made, and why. Hidden requirement: comprehensive logging and audit trail generation, probably with retention policies (keep logs for seven years).

**The A/B testing framework that needs stable identifiers:** You're building a recommendation engine. Journey mapping reveals that the product team runs constant A/B tests to optimize conversion. Tests require stable user identifiers so you can track whether user 12345 saw variant A or B and what they did afterward. Your recommendation model generates results based on transient session IDs. Hidden requirement: map recommendations back to stable user IDs, or integrate with the experiment tracking system.

**The customer who needs to correct mistakes:** You're building a voice assistant for home automation. Journey mapping reveals that when the assistant misunderstands a command, users want to correct it immediately: "No, I said turn off the lights, not turn on the lights." Your system processes each command independently with no memory of previous commands. Hidden requirement: multi-turn dialogue with correction handling, requiring session state and error recovery.

Journey mapping surfaces these requirements because it forces you to look beyond "does the AI work?" and ask "does the AI fit into the system?" You can build a technically perfect model that meets every ML metric and still fail to deliver value because you missed a workflow requirement.

## Mapping the Journey: A Practical Approach

How do you actually map a user journey in practice? Start by picking a specific user persona and a specific task. Don't try to map "all users doing all things"—start narrow.

**Step 1: Identify the trigger.** What causes the user to start this workflow? A notification? A scheduled task? A problem they're trying to solve? Write down the trigger.

**Step 2: List every action the user takes, in order.** Be specific. Not "user searches for information" but "user opens search bar, types query, presses enter, scans results, clicks third result, reads page, hits back button, clicks second result." Walk through the task yourself or watch someone else do it. Write down every single action.

**Step 3: List every system interaction.** Between user actions, what do systems do? When the user presses enter on a search query, the system routes the query to a search API, retrieves results, ranks them, and returns them. Write this down.

**Step 4: Mark where AI intervenes.** Now look at the full flow and identify where AI replaces, augments, or adds to existing steps. Be explicit: is this a replacement, augmentation, or addition?

**Step 5: Identify context flows.** For each step, write down what information is available. At step 3, you have the user's query and their search history. At step 7, you also have which result they clicked. Context accumulates as you move through the journey. Mark what's available at each point.

**Step 6: Identify expectations.** For each step after AI intervention, write down what that step expects from the AI. Format, latency, reliability, completeness, explainability.

**Step 7: Look for gaps.** Where does the AI's output not match the next step's input? Where does the AI lack context it should have? Where does the workflow force unnecessary manual work?

This process takes a few hours for a simple workflow, a few days for a complex one. It's time well spent. Teams that skip journey mapping build AI that works in demos and fails in production. Teams that do journey mapping build AI that fits naturally into workflows and gets adopted.

## Why AI That Ignores the Journey Creates Friction

The fraud detection story that opened this chapter ended with a redesign. After the fraud ops team's complaints, the data science team mapped the journey. They realized the model needed to output not just a flag but a full case brief: transaction details, customer history summary, recent activity patterns, merchant risk profile, and a ranked list of reasons for the flag.

They rebuilt the model's output layer to generate this brief. They also added a direct integration with the case management system so flagged transactions appeared pre-populated with all relevant information. Analysts went from five minutes of manual lookup per case to zero. The AI went from "technically excellent but operationally useless" to "indispensable."

The technical accuracy didn't change. The model still caught 94% of fraud with 2% false positives. What changed was workflow fit. The AI stopped being a standalone component and became part of the system.

This is the lesson: AI that ignores the journey creates friction instead of value, no matter how accurate it is. AI that respects the journey—that inherits context from upstream steps, that meets downstream expectations, that reveals hidden requirements through careful mapping—becomes a natural extension of the workflow.

As you move from problem decomposition to the broader challenge of building AI systems that work in the real world, journey mapping becomes your bridge between technical capability and user value.
