# 1.7 â€” Framing for Existing Products vs Greenfield

In late 2023, two teams at different companies set out to build AI-powered writing assistants. Team A was at an established document collaboration platform with 50 million users. Team B was a startup with zero users and a clean slate. Both teams had talented engineers, experienced PMs, and funding. Eighteen months later, Team A had shipped a feature that 30 percent of their user base used weekly. Team B had burned through their runway and shut down.

The difference was not technical capability. Both teams built similar models with similar performance. The difference was that Team A had an existing product with existing users, existing data, and existing constraints. They knew what users needed because they had years of support tickets and feature requests. They knew what integration points mattered because they had a system to integrate with. They knew what "good enough" looked like because they had a baseline to beat. Team B had none of this. They had infinite freedom and no ground truth. They built a beautiful product that solved a problem nobody had in a way nobody wanted.

This is the core difference between framing for existing products and framing for greenfield. Existing products give you constraints, data, and feedback loops that dramatically reduce the risk of solving the wrong problem. Greenfield gives you freedom, which is both an opportunity and a liability.

## Framing for Existing Products: Working Within Constraints

When you are adding AI to an existing product, the problem framing process is anchored by reality. You have users who are already accomplishing tasks, often in inefficient or frustrating ways. You have data about what they are doing, where they are getting stuck, and what they are asking for. You have a system architecture, a data model, and operational infrastructure that the AI feature must integrate with.

This drastically narrows the problem space. You are not asking "What should we build?" You are asking "How can AI improve this specific workflow?" The framing document becomes more concrete and more constrained.

**Start with the existing workflow.** The first step is to map out the current user experience in detail. What are users trying to accomplish? What steps do they take? Where do they spend time? Where do they make errors? Where do they abandon the task?

For the document collaboration platform, the existing workflow for writing might look like: draft content in a blank document, revise for clarity, check grammar and tone, share with collaborators for feedback, incorporate feedback, finalize. The question is: where in this workflow can AI provide the most value with the least disruption?

This is different from greenfield, where you might start by asking "What does the ideal writing experience look like?" For an existing product, the ideal experience is constrained by what users already know how to do. If you introduce AI in a way that requires users to learn a completely new workflow, adoption will be slow or nonexistent. The best AI features for existing products are those that make the existing workflow faster or easier, not those that replace it entirely.

**Leverage existing data.** Existing products have a massive advantage: historical data. You know what users have written, what edits they have made, what suggestions they have accepted or rejected. This data is gold for framing, because it tells you what the distribution of real-world tasks looks like.

For the document platform, historical data might reveal that most users spend more time editing than drafting, that grammar errors are common but tone issues are rare, that documents under 500 words are edited differently than long-form reports. This informs the framing: if users spend more time editing, maybe the AI should focus on revision assistance rather than drafting from scratch. If short documents and long documents have different patterns, maybe you need different success criteria for each.

Greenfield projects do not have this luxury. You can commission user research, but it will be based on what people say they need, not what they actually do. The gap between stated needs and revealed preferences is often large.

**Identify integration points early.** Adding AI to an existing product means integrating with existing systems: databases, APIs, user interfaces, authentication, permissions, logging, monitoring. These integration points are constraints that shape what is feasible.

For the document platform, integration constraints might include: the AI suggestions need to appear inline with the text, the system must respect document permissions and user roles, the feature must work on mobile and web, the latency must be low enough that suggestions appear in real time, the system must handle documents in multiple languages.

These constraints are not problems to be solved later. They are framing decisions. If real-time inline suggestions are a hard requirement, that rules out model architectures with high latency. If multi-language support is required, that shapes the data and model selection. Identifying these constraints during framing prevents wasted effort on approaches that are technically impressive but not deployable.

**Manage user expectations carefully.** Existing products have an installed user base with established expectations. When you introduce AI, you are changing the experience for users who did not ask for it and may not want it. This is a framing problem, not just a product problem.

The framing document should include a section on expectation management: How will the feature be introduced? Will it be opt-in or default-on? What happens when the AI makes a mistake? How do users override or disable it? What communication and onboarding are needed?

For the document platform, expectation management might include: the feature is opt-in for the first three months, users can dismiss individual suggestions, there is a feedback button to report bad suggestions, the onboarding tutorial explains how the feature works and sets realistic expectations about accuracy.

Greenfield products do not have this problem, because users have no prior expectations. But they have the opposite problem: users have no mental model for how the product works, which makes adoption harder.

**Define success relative to the baseline.** For existing products, success is comparative. The AI feature needs to be better than the status quo. This gives you a clear benchmark.

The framing document should specify the baseline performance: What is the current user experience? How long does the task take? What is the error rate? What is the user satisfaction score? Then specify the target improvement: reduce time by X percent, reduce errors by Y percent, increase satisfaction by Z points.

For the document platform, the baseline might be: users spend an average of 15 minutes editing a 1000-word document, grammar errors are present in 40 percent of final documents, satisfaction with the editing experience is 3.2 out of 5. The target might be: reduce editing time to under 10 minutes, reduce grammar error rate to under 10 percent, increase satisfaction to 4.0 out of 5.

Greenfield products do not have a baseline. You are not replacing an existing workflow; you are creating a new one. This makes success criteria harder to define, because you cannot measure improvement, only absolute performance.

## Framing for Greenfield: Managing Infinite Scope

Greenfield AI projects are rare in 2026. Most enterprise AI work is adding AI to existing products, not building AI-native products from scratch. But greenfield projects still happen, especially in new markets or when the existing workflow is so broken that incremental improvement is not enough.

The core challenge of greenfield framing is that you have no constraints and no data. Everything is a hypothesis. This is both liberating and dangerous.

**Start with the job to be done.** Without an existing product to reference, you need to ground the framing in the fundamental user need. What job are users hiring this product to do? What are they doing today to get this job done, and why is it inadequate?

For the greenfield writing assistant startup, the job to be done might be: "Create professional business documents quickly and confidently, without needing to be an expert writer." The current solutions might be: hiring a copywriter, using templates, or writing it yourself and hoping it is good enough. The inadequacy might be: copywriters are expensive and slow, templates are generic and rigid, writing it yourself is time-consuming and nerve-wracking.

The job-to-be-done framing forces specificity. "Help people write better" is too vague. "Create professional business documents quickly and confidently" is specific enough to guide decisions about what features to build and what to defer.

**Ruthlessly constrain the scope.** The biggest risk in greenfield is over-scoping. Without an existing product to anchor the feature set, it is tempting to build everything. The framing document must define a minimum viable scope that is ambitious enough to provide real value but narrow enough to ship in a reasonable timeframe.

For the writing assistant startup, the initial scope might be: business emails and short memos under 500 words, English only, desktop web only, single-user accounts. Non-goals might include: long-form reports, creative writing, collaboration features, mobile apps, enterprise accounts.

This is painful, because it means saying no to features that seem obviously valuable. But without constraints, greenfield projects spiral into multi-year efforts that never ship.

**Synthesize data where it does not exist.** Greenfield projects cannot rely on historical data, but they can create synthetic baselines. This might include: user interviews, observational studies, competitive analysis, or building a quick prototype to gather data.

For the writing assistant startup, synthetic data might include: interviewing 30 potential users about their current writing process, analyzing 200 business emails to identify common patterns and pain points, benchmarking competitor products to understand what "good enough" looks like.

This data is not as reliable as historical usage data, but it is better than intuition. The framing document should make explicit what assumptions are based on real data vs synthetic data vs hypothesis, and should include a plan for validating assumptions early.

**Build in feedback loops from day one.** Greenfield products cannot iterate on an existing user base, so they need to build feedback loops into the initial version. This might include: telemetry to track how users interact with the product, in-app feedback prompts, user interviews after the first week of use, A/B tests to validate design decisions.

For the writing assistant, feedback loops might include: logging every suggestion the user accepts or rejects, prompting users to rate the usefulness of the output after each session, conducting follow-up interviews with the first 100 users.

The framing document should specify what feedback loops will be built, what metrics will be tracked, and how the team will use this data to validate or revise the original framing.

**Expect to reframe early and often.** Greenfield projects are high-uncertainty. The initial framing is a best guess, and it is likely to be wrong in some important ways. The framing document should acknowledge this and specify decision points for reframing.

For the writing assistant, decision points might include: after the first 100 users, after the first month of usage data, after the first round of user interviews. At each decision point, the team revisits the framing: Are users using the product as expected? Are the success criteria still relevant? Have we learned something that invalidates the original assumptions?

This is different from existing products, where reframing is usually triggered by clear failure signals like low adoption or negative feedback. For greenfield, reframing is proactive: you assume you are wrong about something and actively look for evidence of what.

## The Hybrid Case: Replacing Rule-Based Systems

A common scenario in 2026 is replacing an existing rule-based or heuristic system with AI. This is not pure greenfield, because you have an existing system, but it is also not pure incremental improvement, because you are replacing the core logic.

An e-commerce company had a rule-based fraud detection system with hundreds of hand-tuned rules. The rules were brittle, required constant maintenance, and had high false positive rates. The company decided to replace the rule-based system with a machine learning model. This is a hybrid framing problem.

**You have a baseline, but it is adversarial.** The existing system defines the baseline, but the baseline is not what you want to preserve. The goal is to do better than the rule-based system, not to replicate it. This means you need to define success in terms of outcomes, not behavior.

For fraud detection, the success criteria should not be "match the rule-based system's decisions 95 percent of the time." That just replicates the existing problems. The success criteria should be "reduce false positive rate by 50 percent while maintaining or improving fraud catch rate."

**You have data, but it is biased.** The historical data reflects the behavior of the old system, not ground truth. If the rule-based system had systematic biases or blind spots, those biases will be present in the data. The framing document should identify known biases and specify how they will be addressed.

For fraud detection, known biases might include: the rule-based system over-flagged international transactions, under-flagged fraud on low-value transactions, and had no visibility into fraud patterns that emerged after the rules were written. The framing should specify: the new system should not inherit the international transaction bias, should cover low-value transactions, and should be evaluated on recent fraud patterns that the old system missed.

**You have users, but they are used to the old system.** Replacing a rule-based system means changing the user experience, which requires expectation management. Users who are used to the old system will compare the new system to what they had, not to an ideal.

For fraud detection, this might mean: fraud analysts are used to seeing the specific rule that triggered a flag, and they use that information to decide whether to escalate. The new ML system does not have human-readable rules, so the framing document needs to address: how will analysts understand why a transaction was flagged? What explainability features are required? How will the transition be communicated?

**Integration is both a constraint and an opportunity.** The existing system has integration points that the new system must preserve, but the new system might also enable new integrations that were not feasible with the old system.

For fraud detection, existing integrations might include: the fraud score must be available within 100ms of transaction authorization, the system must integrate with the case management tool that analysts use, the system must log decisions for compliance audits. New opportunities might include: the ML system can incorporate features that the rule-based system could not handle, such as transaction history patterns or device fingerprinting.

The framing document should specify which integrations are must-haves and which are nice-to-haves for the initial version.

## Existing vs Greenfield: Summary

The framing process is fundamentally the same: define the problem, specify success criteria, identify constraints, get stakeholder alignment. But the emphasis and risk profile are different.

For existing products, the risk is integration complexity and user resistance. The framing document should emphasize baseline measurement, integration requirements, and expectation management.

For greenfield products, the risk is over-scoping and solving the wrong problem. The framing document should emphasize ruthless scope constraint, synthetic data to validate assumptions, and feedback loops to detect when the initial framing is wrong.

For hybrid cases, the risk is inheriting the problems of the old system while introducing new problems. The framing document should emphasize unbiased success criteria, data quality, and explainability.

In all cases, the framing document is the tool that forces you to think through these risks before you commit to implementation. In the next section, we will look at how to recognize when your original framing is wrong and when it is time to reframe.

