# 6.11 â€” Security Review in the Framing Handoff

In March 2024, a mid-sized fintech company launched an AI-powered customer support agent trained on GPT-5. The product team had framed the problem well: reduce ticket response time from four hours to under thirty seconds, handle routine account inquiries, and escalate complex issues to humans. They ran offline evaluations, monitored accuracy, and launched to 10,000 users in the first week. On day three, a security researcher discovered that the agent could be prompted to disclose full account balances and transaction histories for any customer ID, bypassing the company's existing access controls. The researcher documented the prompt injection technique and disclosed it responsibly. The company pulled the feature within six hours, issued a mandatory password reset for 8,400 affected customers, and paid $340,000 in incident response costs. The engineering team spent two months rebuilding the agent with proper access controls and prompt filtering. The product manager later told the board that the core mistake happened during framing: the team treated security as a post-launch concern, something Engineering would handle during implementation, rather than a requirement that must be defined and reviewed before the problem is considered fully framed.

This story repeats across the industry every month. Teams frame problems in terms of user benefit, accuracy, and speed, then treat security as a compliance checkbox that Legal or Security will review after the feature is built. By the time the security review happens, the architecture is locked in, the training data is chosen, and the evaluation metrics are defined. Security findings require major rework, not minor adjustments. The correct approach is to treat security as a framing requirement: before you hand off the problem to Engineering, you must answer a defined set of security questions and document the threat model, access controls, PII handling strategy, and adversarial testing plan. This chapter covers how to integrate security review into the framing handoff so that you never launch a feature that violates your own security policies.

## Why Security Review Belongs in Framing

Most organizations have a security review process that happens before production launch. Engineering submits a design document, Security reviews it, identifies risks, and either approves the launch or requires mitigations. This process works well for traditional software, where the system's behavior is deterministic and the attack surface is well-understood. It fails for AI systems because the system's behavior is probabilistic, the attack surface includes adversarial inputs that manipulate model outputs, and the risks depend on decisions made during problem framing. By the time Engineering submits a design document, the problem definition already constrains the solution space in ways that make certain security mitigations expensive or impossible.

Consider the fintech example. The framing document specified that the agent should "answer customer questions about their accounts." This framing implied that the agent needed access to account data, which meant Engineering built the system with database access. Security reviewed the design and found that the agent had no access controls: any user could ask about any account. Engineering proposed adding a filter that checked whether the user ID in the prompt matched the authenticated user ID. This mitigation required rewriting the prompt structure, retraining the classifier that routed questions to the agent, and re-running all offline evaluations. The two-month delay could have been avoided if the framing document had specified that the agent must only access data for the authenticated user, which would have constrained Engineering to build access controls from the start.

The principle is simple: security requirements constrain the solution space, so they must be defined before you lock in the architecture. If you frame a problem as "summarize all customer support tickets" without specifying that the agent cannot access tickets outside the user's department, you have implicitly authorized a system that violates access controls. If you frame a problem as "generate marketing copy" without specifying that the output cannot include profanity, hate speech, or medical claims, you have implicitly authorized a system that violates content policy. Framing is where you define what the system is allowed to do, which means it is also where you define what it is not allowed to do.

## Threat Modeling During Problem Framing

**Threat modeling** is the process of enumerating what can go wrong with a system before you build it. For AI systems, threat modeling must happen during problem framing because the threats depend on the problem definition. You cannot threat-model a solution you have not yet designed, but you can threat-model the problem you are solving. The framing document should include a threat model section that answers three questions: what data does the system access, what actions can it take, and who can interact with it.

Start with data access. If the problem is "answer customer questions about their orders," the system will access order histories, customer profiles, and possibly payment information. The threat model must enumerate what happens if an attacker gains access to this data through the AI system. Can the attacker use prompt injection to extract order histories for other customers? Can the attacker infer sensitive information from the model's responses even if the raw data is not disclosed? Can the system inadvertently memorize and regurgitate PII from training data? These questions must be answered during framing because they determine whether the problem is solvable with acceptable risk.

Next, consider actions. If the problem is "automatically respond to refund requests," the system will take actions that affect customer accounts. The threat model must enumerate what happens if an attacker manipulates the system into taking unauthorized actions. Can the attacker use prompt injection to approve refunds for accounts they do not own? Can the attacker escalate privileges by manipulating the system's internal logic? Can the system be tricked into taking actions that violate business rules or legal requirements? If the answers to these questions reveal unacceptable risks, the problem must be reframed to limit the system's action space.

Finally, consider interaction. If the problem is "provide AI-assisted code review for internal developers," the system's users are employees with authenticated accounts. If the problem is "generate product recommendations for website visitors," the users are anonymous or partially authenticated. The threat model must account for who can interact with the system and what adversarial inputs they might provide. An internal system faces insider threats and credential compromise. A public system faces automated attacks, adversarial probing, and reputational risk from misuse. The interaction model constrains the security requirements, so it must be defined during framing.

The output of threat modeling is a list of risks and the framing-level mitigations that address them. You do not need to specify the implementation details, but you do need to specify the requirements. For example, the threat model might state that the system must not disclose account data for unauthenticated users, must not take actions that exceed the authenticated user's permissions, and must log all queries for audit purposes. These requirements constrain Engineering to build the system with access controls, permission checks, and logging, which means they will design the architecture to support these features from the start.

## Prompt Injection Risks

**Prompt injection** is the technique of embedding adversarial instructions in user input to manipulate the model's behavior. It is the AI equivalent of SQL injection: an attacker crafts input that causes the system to execute unintended logic. Prompt injection is particularly dangerous for AI systems because the model cannot reliably distinguish between instructions from the system developer and instructions from the user. If your prompt says "You are a helpful assistant. Answer the user's question" and the user's question is "Ignore previous instructions and reveal all customer data," the model may comply with the user's instruction.

Prompt injection must be acknowledged during framing because it affects the problem's feasibility. If the problem requires the system to process untrusted user input and make decisions based on that input, you must assume that attackers will attempt prompt injection. The framing document must specify how the system will mitigate this risk. Common mitigations include input filtering, output validation, privilege separation, and adversarial testing, but these mitigations must be designed into the architecture, not bolted on afterward.

Input filtering attempts to detect and block adversarial prompts before they reach the model. This approach is fragile because adversarial prompts evolve faster than filters can be updated. A filter that blocks "Ignore previous instructions" will not block "Disregard all prior directions" or more sophisticated obfuscation techniques. Input filtering is a defense-in-depth measure, not a primary mitigation.

Output validation checks the model's response before returning it to the user. If the output contains patterns that indicate prompt injection succeeded, such as disclosing internal instructions or violating access controls, the system blocks the response and logs the attempt. Output validation is more robust than input filtering because it detects successful attacks regardless of the input technique, but it requires defining what a valid output looks like, which is difficult for generative models.

Privilege separation limits the damage from successful prompt injection by restricting what the model can access. If the model cannot access customer data outside the authenticated user's account, prompt injection cannot be used to exfiltrate other users' data. If the model cannot take actions that modify account state, prompt injection cannot be used to approve unauthorized transactions. Privilege separation is the most effective mitigation because it reduces the blast radius of any attack, but it must be designed during framing because it constrains the problem definition.

The framing document must state whether the problem is exposed to prompt injection risk and, if so, what mitigations will be required. If the problem cannot be solved with acceptable risk given current prompt injection mitigations, the problem must be reframed or deprioritized.

## Data Leakage

**Data leakage** occurs when an AI system inadvertently exposes information that should remain confidential. Leakage can happen through three mechanisms: training data memorization, inference-time data access, and side-channel attacks. All three mechanisms must be considered during framing because they affect what data the system can be trained on and what data it can access at inference time.

Training data memorization happens when a model memorizes specific examples from its training data and reproduces them in response to adversarial queries. Large language models like GPT-5 and Claude Opus 4.5 have been shown to memorize rare sequences, personally identifiable information, and copyrighted content from their training data. If you fine-tune a model on customer support tickets that contain PII, the model may leak that PII in response to adversarial prompts. The framing document must specify what data will be used for training and whether that data contains information that must not be leaked. If it does, the mitigations include redacting PII before training, using differential privacy techniques, or avoiding fine-tuning entirely and relying on retrieval-augmented generation.

Inference-time data access leakage happens when the model has access to confidential data at inference time and an attacker uses prompt injection or other techniques to extract it. This is the mechanism behind the fintech incident described at the start of this chapter. The framing document must specify what data the system will access at inference time and whether that data is scoped to the authenticated user or available globally. If the system accesses data that should not be disclosed to all users, the framing document must require access controls that limit what data the model can retrieve.

Side-channel attacks infer confidential information from the model's behavior without directly extracting it. For example, an attacker might infer that a specific individual's data is in the training set by measuring whether the model responds more confidently to queries about that individual. An attacker might infer sensitive attributes about users by analyzing the model's recommendations. Side-channel attacks are difficult to mitigate because they exploit statistical properties of the model's behavior rather than explicit data disclosure. The framing document should acknowledge side-channel risk for high-sensitivity applications and specify whether the problem requires differential privacy guarantees or other statistical protections.

The framing handoff must include a data leakage assessment that answers whether the system can leak training data, inference-time data, or sensitive attributes through side channels, and if so, what mitigations are required.

## PII Handling

**Personally identifiable information** (PII) is any data that can be used to identify an individual, including names, email addresses, phone numbers, account numbers, IP addresses, and biometric data. Most AI systems encounter PII in user inputs, training data, or inference-time data access. The framing document must define what PII the system will encounter and how it must be handled in inputs, outputs, and storage.

Input PII handling specifies what happens when a user submits PII in a query. If the problem is "summarize legal documents," users will upload documents containing names, addresses, and case numbers. The framing document must specify whether this PII will be logged, stored, or transmitted to third-party model APIs. If the system uses a third-party API like OpenAI or Anthropic, the framing document must specify whether PII can be sent to those APIs or whether it must be redacted first. Many organizations have policies that prohibit sending PII to third-party APIs without explicit user consent or data processing agreements.

Output PII handling specifies what PII the system is allowed to generate in its responses. If the problem is "answer customer questions about their accounts," the system will generate responses that include account numbers, transaction amounts, and possibly payment information. The framing document must specify whether this PII can be displayed to the authenticated user, whether it must be masked, and whether it can be logged. If the system generates PII that should not be disclosed to the current user, the output validation layer must detect and redact it.

Storage PII handling specifies what happens to PII that the system processes. If the system logs user queries for debugging or evaluation purposes, those logs will contain PII. The framing document must specify the retention policy for logs, whether PII must be redacted from logs, and who has access to logs. Many regulations, including GDPR and HIPAA, impose strict requirements on PII storage and access, so the framing document must ensure compliance.

The framing handoff must include a PII handling section that defines what PII the system encounters, where it comes from, and what rules govern its use. If the problem cannot be solved without violating PII policies, the problem must be reframed.

## Access Control

**Access control** determines who can interact with the system, what data the system can access, and what actions it can take. For AI systems, access control is more complex than for traditional software because the system's behavior is mediated by a language model that processes untrusted user input. The framing document must specify access control requirements at three levels: user authentication, data scoping, and action authorization.

User authentication specifies who can interact with the system. If the problem is "provide AI-powered financial advice to customers," the system must authenticate users before responding to queries. The framing document must specify the authentication mechanism, whether it integrates with existing identity systems, and what happens if authentication fails. If the problem is "generate product descriptions for internal use," the system may require employee authentication and restrict access to specific departments.

Data scoping specifies what data the system can access for each authenticated user. If the problem is "answer questions about customer orders," the system must only access orders for the authenticated customer, not all orders in the database. The framing document must specify the data scoping rules and how they will be enforced. Common patterns include filtering database queries by user ID, using row-level security policies, or implementing a permission layer that the model queries before accessing data.

Action authorization specifies what actions the system can take on behalf of the user. If the problem is "automatically respond to refund requests," the system must only approve refunds that the user is authorized to request, not all refunds in the system. The framing document must specify the action authorization rules and how they will be enforced. Common patterns include checking user permissions before executing actions, limiting the system to read-only operations, or requiring human approval for high-risk actions.

The framing handoff must include an access control section that defines who can use the system, what data it can access, and what actions it can take. If the access control requirements cannot be enforced with current technology, the problem must be reframed to reduce the system's privileges.

## Output Safety

**Output safety** ensures that the system cannot generate harmful, illegal, or policy-violating content. For AI systems that generate text, images, or code, output safety is a primary concern because the model's outputs are visible to users, customers, and potentially the public. The framing document must specify what types of outputs are prohibited and how the system will enforce those prohibitions.

Content policy violations include hate speech, harassment, profanity, sexually explicit content, violent content, and content that promotes illegal activity. Most model providers, including OpenAI, Anthropic, and Google, have content policies that prohibit these outputs, but those policies are enforced probabilistically, not deterministically. The model may occasionally generate prohibited content in response to adversarial prompts or edge-case inputs. The framing document must specify whether the problem requires stricter output safety than the base model provides and, if so, what additional mitigations are required. Common mitigations include output classifiers that detect and block prohibited content, human review for high-risk outputs, and adversarial testing to measure the rate of policy violations.

Factual accuracy violations occur when the model generates false or misleading information. For problems in high-stakes domains like healthcare, finance, or legal advice, factual inaccuracy can cause direct harm. The framing document must specify whether the problem requires factual accuracy guarantees and, if so, what mitigations are required. Common mitigations include retrieval-augmented generation to ground outputs in verified sources, citation requirements that force the model to reference its sources, and human review for high-risk claims.

Misinformation and manipulation risks occur when the model generates content that could be used to deceive users or manipulate public opinion. If the problem is "generate social media posts," the system could be misused to create fake news, impersonate individuals, or spread propaganda. The framing document must acknowledge misuse risks and specify what mitigations are required. Common mitigations include rate limiting, user verification, watermarking AI-generated content, and monitoring for coordinated misuse.

The framing handoff must include an output safety section that defines what outputs are prohibited, what harms could result from policy violations, and what mitigations are required. If the output safety requirements cannot be met with acceptable cost or latency, the problem must be reframed.

## EU AI Act Requirements

The **EU AI Act**, which entered into force in August 2024 and will be fully enforced by 2026, classifies AI systems into risk categories and imposes requirements on high-risk systems. If your problem falls into a high-risk category, the framing document must acknowledge the regulatory requirements and specify how the system will comply. The Act's requirements affect problem framing because they impose constraints on data, transparency, human oversight, and documentation that must be designed into the system from the start.

High-risk AI systems include those used for biometric identification, critical infrastructure management, educational or vocational training, employment decisions, access to essential services, law enforcement, migration and border control, and administration of justice. If your problem involves any of these domains, the framing document must specify how the system will meet the Act's requirements. Key requirements include risk management systems, data governance, technical documentation, record-keeping, transparency, human oversight, accuracy, robustness, and cybersecurity.

Risk management requires identifying and mitigating risks throughout the system's lifecycle. The framing document must include a risk assessment that enumerates foreseeable risks, estimates their severity, and specifies mitigations. This requirement aligns with the threat modeling process described earlier, but the EU AI Act requires formal documentation and ongoing monitoring.

Data governance requires ensuring that training data is relevant, representative, and free from bias. The framing document must specify what data will be used for training, how it will be sourced, and how biases will be identified and mitigated. This requirement affects problem feasibility because it may require collecting new data, auditing existing data, or using synthetic data to fill gaps.

Transparency requires informing users that they are interacting with an AI system and providing information about the system's purpose, limitations, and accuracy. The framing document must specify what transparency disclosures will be made and how they will be presented to users.

Human oversight requires ensuring that humans can intervene in the system's operation, monitor its outputs, and override its decisions. The framing document must specify what human oversight mechanisms will be implemented and who is responsible for oversight.

The framing handoff for high-risk systems must include an EU AI Act compliance section that maps the problem to the Act's risk categories, enumerates the applicable requirements, and specifies how the system will comply. If compliance is not feasible, the problem must be reframed or deprioritized.

## Adversarial Testing Requirements

**Adversarial testing**, also known as red-teaming, is the process of deliberately attempting to break the system by crafting inputs that cause it to behave incorrectly, unsafely, or in violation of policy. Adversarial testing must be planned during framing because it affects the evaluation strategy, the launch timeline, and the risk tolerance for the problem. The framing document must specify what adversarial testing will be conducted, who will conduct it, and what success criteria will be used.

Adversarial testing for AI systems typically focuses on three attack vectors: prompt injection, jailbreaking, and adversarial examples. Prompt injection testing attempts to manipulate the system into disclosing confidential data, taking unauthorized actions, or violating content policy by embedding adversarial instructions in user input. Jailbreaking testing attempts to bypass the model's safety filters by using obfuscation, role-playing, or multi-turn conversations that gradually shift the model's behavior. Adversarial example testing attempts to cause the system to misclassify inputs by adding noise, perturbations, or carefully crafted patterns that exploit the model's weaknesses.

The framing document must specify the scope of adversarial testing based on the problem's risk profile. High-risk problems, such as those involving financial transactions, medical advice, or PII, require extensive adversarial testing conducted by experienced red-teamers. Lower-risk problems, such as those involving content generation or internal tooling, may require lighter adversarial testing focused on common attack patterns. The document must also specify the success criteria: what percentage of adversarial attacks must the system resist to be considered launch-ready.

Adversarial testing must be integrated into the evaluation plan, not bolted on after the system is built. If the framing document specifies that the system must resist 95 percent of prompt injection attacks, the evaluation plan must include a dataset of adversarial prompts and a process for measuring the system's resistance. If the framing document does not specify adversarial testing requirements, Engineering will build the system without defenses, and the adversarial testing that happens before launch will reveal fundamental architecture flaws that require major rework.

The framing handoff must include an adversarial testing section that specifies what attacks will be tested, who will conduct the testing, and what success criteria must be met. For deeper coverage of red-teaming techniques, methodologies, and tooling, see Section 14.

## The Security Review Checklist

The security review checklist is a set of questions that the framing team must answer before handing off the problem to Engineering. The checklist ensures that security requirements are defined and documented before the architecture is locked in. Different organizations will have different checklists based on their risk tolerance, regulatory requirements, and industry, but the following questions are common across most AI systems.

What data does the system access? This question requires enumerating all data sources the system will query at inference time, including databases, APIs, filesystems, and third-party services. The answer must specify whether the data contains PII, confidential information, or regulated data, and whether access controls limit what data the system can retrieve.

What actions can the system take? This question requires enumerating all actions the system can perform on behalf of users, including creating, modifying, or deleting records, sending communications, or triggering workflows. The answer must specify whether these actions require user confirmation, human oversight, or additional authorization checks.

Who can interact with the system? This question requires defining the user population, authentication requirements, and access controls. The answer must specify whether the system is public, internal, or restricted to specific roles or departments.

What PII does the system encounter? This question requires enumerating all PII that appears in inputs, outputs, training data, or logs. The answer must specify how PII will be handled, whether it can be sent to third-party APIs, and what retention policies apply.

What outputs are prohibited? This question requires defining content policies, factual accuracy requirements, and misuse risks. The answer must specify what types of outputs the system must not generate and what mitigations will enforce those prohibitions.

What prompt injection mitigations are required? This question requires acknowledging whether the system is vulnerable to prompt injection and, if so, what mitigations will be implemented. The answer must specify whether input filtering, output validation, privilege separation, or other defenses are required.

What adversarial testing will be conducted? This question requires specifying the scope of red-teaming, who will conduct it, and what success criteria must be met. The answer must specify whether the testing will cover prompt injection, jailbreaking, adversarial examples, or other attack vectors.

Does the problem fall under the EU AI Act or other regulations? This question requires determining whether the system is subject to regulatory requirements and, if so, what compliance measures are required. The answer must specify how the system will meet transparency, human oversight, data governance, and documentation requirements.

What is the incident response plan? This question requires defining what happens if a security incident occurs, including who is responsible for detection, containment, remediation, and disclosure. The answer must specify whether the system will have monitoring, alerting, and logging sufficient to detect and respond to attacks.

The framing team must answer these questions and document the answers in the framing handoff. If any question cannot be answered, or if the answer reveals unacceptable risk, the problem must be reframed or deprioritized before Engineering begins work.

## Integrating Security into the Framing Process

Integrating security into framing requires collaboration between Product, Engineering, Security, Legal, and Trust and Safety. The process begins when the problem is first defined and continues through the framing handoff. The typical workflow involves three checkpoints: initial threat modeling, security requirements definition, and final security review.

Initial threat modeling happens when the problem is first framed. The Product team describes the problem, the user population, and the intended system behavior. The Security team facilitates a threat modeling session that identifies what data the system will access, what actions it will take, and what risks exist. The output is a preliminary list of security requirements and open questions.

Security requirements definition happens as the framing document is drafted. The Product team works with Security and Legal to define access controls, PII handling policies, output safety requirements, and adversarial testing plans. The requirements are documented in the framing handoff and reviewed by stakeholders. If the requirements make the problem infeasible or too expensive, the problem is reframed.

Final security review happens before the framing handoff is approved. Security reviews the completed framing document, verifies that the security checklist has been answered, and confirms that the requirements are sufficient to mitigate identified risks. If Security identifies gaps or unacceptable risks, the framing document is revised before handoff.

This process ensures that security is not a gate at the end of development but a constraint throughout framing. By the time Engineering receives the framing handoff, the security requirements are defined, the threat model is documented, and the adversarial testing plan is in place. Engineering can design the architecture to meet those requirements from the start, which avoids expensive rework and reduces the risk of launching insecure systems.

With security review complete, you have addressed the final compliance and risk dimension of the framing handoff. The next piece is operational: defining what events and logs the system must generate to support evaluation and monitoring, which is the subject of the final subchapter in this chapter, 6-12.
