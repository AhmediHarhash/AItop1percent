# 3.10 â€” Building Your Product's Task Registry

In early 2026, a rapidly growing AI company found itself in a familiar but painful situation. The engineering team had grown from five to forty people over eighteen months. The product had evolved from a single feature to a suite of interconnected capabilities. And nobody could answer a simple question: what AI tasks are we actually running in production?

Different engineers had different answers. The founding team remembered the original architecture. The newer engineers knew the systems they had built. But there was no single source of truth. When the VP of Engineering asked "how do we evaluate our content generation quality?" three different engineers gave three different answers, each describing evaluation practices for different parts of the system that happened to generate content. When a major customer asked "how do you ensure your classification does not exhibit bias?" the team scrambled to enumerate which systems even performed classification.

The breaking point came during a security audit. The auditor asked to see a list of all AI components that processed user data, along with their evaluation practices and quality metrics. The team spent three weeks compiling the list, discovering several systems that nobody currently on the team had known existed (legacy code still running in production) and multiple places where evaluation had never been implemented at all.

The solution was creating a task registry: a living document that listed every AI task in the system, its properties, its evaluation approach, and its current status. Building the registry took two weeks of painful archaeology through code and documentation. But once complete, it became the single source of truth for everything AI-related in the product. When someone asked "how do we evaluate X?" the answer was in the registry. When planning new features, engineers checked the registry to see if similar tasks existed. When onboarding new team members, they read the registry to understand what the product actually did.

Two years later, that company's task registry was cited by acquirers as one of the reasons they felt confident in the acquisition. It was the proof that the company understood its own product and had evaluation rigor. This chapter explains how to build your own task registry and why it is one of the highest-value artifacts you will create.

## What Is a Task Registry?

A task registry is a structured document that catalogs every AI task in your product. For each task, it records:

The task name: a clear, descriptive identifier that everyone on the team uses consistently.

The task type: classification, retrieval, extraction, generation, transformation, or reasoning, using the taxonomy from this chapter.

A description: what the task does, in plain language that a non-engineer can understand.

Inputs: what data the task receives, in what format.

Outputs: what data the task produces, in what format.

Risk tier: low, medium, high, or critical, using the framework from Section 1.

Evaluation approach: how the task is evaluated, including metrics, ground truth format, and testing strategy.

Ground truth format: what annotated examples look like for this task.

Owner: who is responsible for maintaining and improving this task.

Status: development, testing, production, deprecated.

Links: pointers to related code, documentation, evaluation datasets, and dashboards.

The registry is a living document, not a one-time artifact. It evolves as the product evolves. When a new task is added, it gets an entry in the registry. When a task changes significantly, its entry is updated. When a task is deprecated, its status changes but the entry remains for historical reference.

The registry serves multiple purposes: onboarding (new team members read it to understand the product), planning (when designing new features, check if similar tasks exist), evaluation (the registry defines what to measure), compliance (auditors and customers can see exactly what AI tasks you run), and incident response (when something breaks, the registry helps identify which task failed).

## Why You Need a Registry

The alternative to a registry is tribal knowledge: information that exists in people's heads but is not written down. Tribal knowledge works when your team is small and stable. It fails as soon as you add people, build complexity, or experience turnover.

Without a registry, you face several failure modes.

Lost tasks: systems that were built months or years ago and are still running in production, but nobody remembers they exist or how they work. These systems become technical debt bombs: they break, and nobody knows how to fix them.

Duplicate effort: two teams build similar tasks independently because neither knows the other's work exists. The tasks use different evaluation approaches, different ground truth, and different metrics, making it impossible to compare their quality or reuse components.

Evaluation gaps: tasks that were deployed without evaluation because nobody tracked which tasks had been evaluated and which had not. These gaps become risks: you discover during an incident that a critical task has never been tested.

Knowledge bottlenecks: critical information about a task lives in one person's head. When that person leaves or moves to a different project, the knowledge is lost.

Inconsistent terminology: different parts of the organization use different names for the same task, or the same name for different tasks, creating confusion and miscommunication.

Compliance failures: when auditors, customers, or regulators ask about your AI systems, you cannot quickly provide accurate answers because the information is scattered across code, documentation, and human memory.

The registry solves all of these problems by creating a single, authoritative, written record of what AI tasks exist and how they are managed.

## Building Your Registry: The Initial Pass

Creating your first task registry involves four steps: discovery, classification, documentation, and validation.

Discovery means finding every AI task in your system. Start with your task profile map from the previous chapter. That gives you the tasks you know about. Then do a code audit: search your codebase for model inference calls, API calls to AI services, and data processing pipelines that might contain AI tasks. Interview engineers: ask each team what AI components they have built. Check deployment logs: what AI services are being called in production? The goal is a complete list of tasks, even ones you did not know existed.

You will likely discover tasks that surprise you. Legacy experiments that somehow made it to production. Tasks that were built as temporary solutions but became permanent. Features that use AI in ways the product team did not realize. This discovery phase often reveals technical debt and risk that nobody had visibility into.

Classification means assigning each discovered task to a task type. Is it classification, retrieval, extraction, generation, transformation, or reasoning? Use the definitions from earlier in this chapter. If a task combines multiple types (common for compound tasks), list the primary type and note the secondary types in the description. The classification determines what evaluation approach is appropriate.

Documentation means filling in the registry fields for each task. Write clear descriptions that someone unfamiliar with the code could understand. Specify inputs and outputs precisely: what are the data types, schemas, and formats? Determine the risk tier based on what happens if the task fails and who is affected. Describe the current evaluation approach, even if that approach is "no formal evaluation yet" (better to acknowledge gaps than hide them). Identify who owns the task: who should be contacted if it breaks or needs improvement?

This documentation phase is time-consuming. For a product with dozens of AI tasks, expect to spend days or weeks writing thorough entries. But the investment pays off immediately: as you document, you will discover inconsistencies, gaps, and risks that were invisible before.

Validation means reviewing the registry with your team. Share the draft with engineers, product managers, and data scientists. Ask: is anything missing? Are the descriptions accurate? Are the owners correct? Are the risk tiers appropriate? This review process surfaces knowledge that was siloed in individuals and creates shared understanding.

Once the initial registry is complete, treat it as the authoritative reference. When someone asks "what AI tasks do we have?" the answer is "check the registry." When planning a new feature, start by reading the relevant registry entries. When onboarding a new engineer, have them read the registry as part of their onboarding.

## Registry Schema and Template

A task registry can be implemented as a spreadsheet, a database, a document, or a dedicated tool. The format matters less than the discipline of keeping it updated. Here is a recommended schema:

Task ID: a unique identifier, like TASK-001 or "intent-classifier-v2". This ID is used in code, documentation, and conversations to refer unambiguously to this task.

Task Name: a human-readable name, like "Customer Intent Classifier" or "Document Retrieval."

Task Type: classification, retrieval, extraction, generation, transformation, or reasoning. For compound tasks, list multiple types.

Description: a paragraph explaining what the task does, why it exists, and how it fits into the product. Write for an audience that includes non-engineers.

Inputs: specify the input data format. For example: "User message (string, max 1000 characters)" or "Search query (JSON object with fields: text, filters, user_context)."

Outputs: specify the output data format. For example: "Intent label (one of: refund_request, product_question, complaint) with confidence score (float, 0 to 1)" or "Ranked list of document IDs (array of strings, max 10 items)."

Risk Tier: low, medium, high, or critical, based on Section 1's framework. Include a brief justification: why this tier?

Evaluation Approach: describe how the task is evaluated. Mention metrics (accuracy, precision, recall, BLEU score, human rating), ground truth source (annotated dataset, user feedback, expert review), and testing frequency (continuous, weekly, quarterly).

Ground Truth Format: specify what annotated examples look like. For a classifier: "labeled messages in JSON format." For a generation task: "reference outputs or human preference ratings." For a retrieval task: "relevance judgments on a scale of 0 (not relevant) to 2 (highly relevant)."

Evaluation Dataset: link to the dataset used for evaluation. If no dataset exists, note "no formal dataset yet" and flag this as a gap.

Metrics Dashboard: link to the dashboard or tool where this task's metrics are tracked. If no dashboard exists, flag this as a gap.

Owner: name and contact for the person responsible for this task. This should be someone who understands the task deeply and can answer questions or troubleshoot issues.

Status: development, testing, production, or deprecated. Include the date the task entered its current status.

Dependencies: which other tasks or systems does this task depend on? Which tasks depend on it?

Related Links: pointers to code repositories, design documents, evaluation reports, incident postmortems, or any other relevant resources.

Notes: any additional context that does not fit in the above fields. Use this for historical context, known issues, future plans, or anything else worth documenting.

This schema provides enough structure to be useful without being so rigid that it becomes burdensome to maintain.

## Template Walkthrough: A Realistic Example

Let us populate the registry template for a realistic task.

Task ID: TASK-042

Task Name: Support Ticket Categorizer

Task Type: Classification

Description: Classifies incoming customer support tickets into one of twelve categories (billing, technical issue, feature request, account access, refund, shipping, product question, complaint, cancellation, upgrade, feedback, other). This classification routes tickets to the appropriate support team and sets initial priority. The classifier runs on every new ticket within seconds of submission.

Inputs: Support ticket text (string, typically 50-500 words) and ticket metadata (JSON object containing user_id, account_tier, timestamp, and subject_line).

Outputs: Category label (string, one of twelve values) and confidence score (float, 0.0 to 1.0). If confidence is below 0.6, the ticket is routed to a human reviewer instead of being automatically routed.

Risk Tier: Medium. Misclassification delays response to customer issues but does not cause direct harm. Customers whose tickets are misrouted experience longer wait times, which impacts satisfaction but is not a safety or compliance issue.

Evaluation Approach: Monthly evaluation on a held-out test set of 500 annotated tickets. Metrics tracked: overall accuracy, per-category precision and recall, and confusion matrix. Additionally, support team leads flag misrouted tickets, which are added to a "hard cases" dataset and reviewed quarterly.

Ground Truth Format: Support tickets labeled by experienced support agents. Labels are verified by a second agent to ensure consistency. Format is JSON with fields: ticket_id, text, metadata, label, annotator_id, annotation_timestamp.

Evaluation Dataset: Link to internal dataset repository, dataset name "support-tickets-labeled-2025-2026", 12,000 examples total, updated quarterly with new annotations.

Metrics Dashboard: Link to internal monitoring dashboard, panel "Support Ticket Categorizer - Accuracy and Latency."

Owner: Alex Chen, Data Science team, alex@company.com

Status: Production since March 2025

Dependencies: Depends on ticket ingestion API (provides input data). Used by ticket routing service (consumes output). Indirectly affects response time SLA (misrouted tickets delay resolution).

Related Links: Code repo (link), original design doc (link), May 2025 evaluation report (link), September 2025 postmortem on billing misclassifications (link)

Notes: The original model was a rules-based system replaced in March 2025 with a fine-tuned language model. The language model improved overall accuracy from 78 percent to 91 percent but introduced a bias toward labeling ambiguous tickets as "other." We are addressing this by adding more diverse training examples for edge case categories. Next evaluation scheduled for March 2026.

This entry gives anyone in the organization enough information to understand what the task does, how well it works, where to find more details, and who to contact with questions. It also documents history (the transition from rules to ML) and known issues (the "other" category bias).

## Keeping the Registry Current

The hardest part of maintaining a task registry is not building it but keeping it updated as the product evolves. Here are strategies to make this sustainable.

First, make registry updates part of your development process. When an engineer starts work on a new AI task, the first step is creating a registry entry (even if marked "development" status). When a task reaches production, the engineer updates the entry with evaluation results, dashboard links, and production date. When a task is deprecated, the engineer updates the status field. This embeds registry maintenance into normal workflow rather than treating it as a separate chore.

Second, schedule regular registry reviews. Quarterly, have the team review the entire registry: are descriptions still accurate? Have risk tiers changed? Are owners still correct? Are evaluation approaches being followed? This catches drift before it becomes severe.

Third, appoint a registry maintainer. This person is not responsible for filling in all the entries (that is distributed across task owners), but they are responsible for enforcing standards, catching inconsistencies, and ensuring the registry stays complete and accurate. For small teams, this might be a part-time role. For large teams, it might be a dedicated role.

Fourth, integrate the registry with other tools. If your registry is a database or structured document, you can generate dashboards, reports, or alerts from it. For example, automatically generate a list of all high-risk tasks that lack formal evaluation, or create a map showing which tasks depend on each other. These integrations make the registry more useful and incentivize keeping it updated.

Fifth, use the registry in practice. Reference it in planning meetings, link to it from documentation, point new hires to it during onboarding. The more the registry is used, the more value it provides, and the more motivated people are to keep it current.

Sixth, celebrate good registry hygiene. When someone updates their task entry promptly or adds valuable context, acknowledge it. Make it a norm that maintaining the registry is part of being a good team member, not an annoying bureaucratic chore.

## The Registry's Relationship to Evaluation Strategy

The task registry is not a substitute for an evaluation strategy; it is the foundation for one. The registry tells you what to evaluate. The evaluation strategy (which you will build in Section 3) tells you how to evaluate it in detail.

Each entry in the registry will eventually link to an evaluation plan that specifies:
- Metrics definitions and acceptable thresholds
- Ground truth collection process and quality control
- Evaluation frequency and triggers (when to re-evaluate)
- Failure modes and mitigation strategies
- Integration with continuous monitoring

Section 3 will walk through building these evaluation plans. But you cannot build them without first knowing what tasks exist, which is what the registry provides.

The registry also connects to ground truth management (Section 4). The "Ground Truth Format" field in the registry specifies what your annotated examples should look like. Section 4 will cover how to collect, validate, and maintain that ground truth at scale. The registry provides the schema; Section 4 provides the process.

Think of the registry as the index: it points you to what exists. The evaluation strategy and ground truth processes fill in the details of how to measure and improve what exists.

## The Registry as Institutional Knowledge

One of the subtler benefits of a task registry is that it captures institutional knowledge that would otherwise be lost. When an engineer leaves the company or moves to a different team, their knowledge about the tasks they built often leaves with them. Future engineers must reverse-engineer the code to understand what it does and why.

A well-maintained registry prevents this knowledge loss. The registry entry documents not just what the task does but why it was built that way, what tradeoffs were considered, what evaluation showed, and what issues arose in production. This context is invaluable for future maintainers.

For example, imagine a registry entry that includes this note: "We initially tried using a pre-trained classifier but found it misclassified 40 percent of tickets in the 'billing' category because the training data used different terminology than our customers. We switched to fine-tuning on our own labeled data and improved billing recall from 60 percent to 88 percent." A future engineer reading this understands not just the current approach but why that approach was chosen and what alternatives were tried.

This historical context prevents repeated mistakes. Without it, a new engineer might say "why are we fine-tuning? Let us just use a pre-trained model, it will be easier." They try it, hit the same problems, and waste time rediscovering what was already known. With the registry, they read the note and understand why fine-tuning is necessary.

Institutional knowledge is one of the most valuable and fragile assets in a growing organization. A task registry is a simple, low-tech way to preserve it.

## Preventing the "Nobody Knows What We're Evaluating" Problem

One of the most common dysfunction patterns in AI teams is what we call the "nobody knows what we are evaluating" problem. Different people believe different things about what is being measured, how it is being measured, and whether it is working.

An engineer believes their classification task is being evaluated monthly. The data scientist who set up the evaluation left the company a year ago, and the evaluation actually has not run in ten months. Nobody noticed because there is no single source of truth.

A product manager believes the content generation feature is evaluated for factuality. The engineer who built it only evaluated fluency because they did not have ground truth for factuality. The product manager assumes factuality is covered, so they launch to a domain where hallucination is dangerous.

A data scientist evaluates a retrieval task using recall at 10. An engineer on a different team assumes it is being evaluated using precision at 5 because that is what a previous retrieval task used. They discuss "improving evaluation" and talk past each other because they are not measuring the same thing.

The task registry prevents all of these failures by creating a single, authoritative answer to the question "what are we evaluating?" When someone wonders whether factuality is being measured, they check the registry. When someone wants to know which metric is used for retrieval, they check the registry. When someone wonders if evaluation is still running, they check the registry and see that the dashboard link returns a 404, revealing that the evaluation is broken.

This shared ground truth for evaluation is one of the highest-leverage artifacts an AI team can create. It is not glamorous. It does not involve cutting-edge models or algorithms. But it prevents an enormous amount of confusion, duplication, and risk.

## Starting Small: The Minimum Viable Registry

If building a complete task registry feels overwhelming, start small. The minimum viable registry has three columns: Task Name, Task Type, and Owner. That is enough to answer the question "what AI tasks do we have, and who knows about them?"

You can expand from there. Add Risk Tier so you know which tasks are most critical. Add Status so you know what is in production. Add Evaluation Approach so you know which tasks are being measured. Add Links so you can find related code and documentation.

Even a minimal registry is better than no registry. It creates the habit of documenting AI tasks and gives you a foundation to build on. Over time, you can flesh out the entries, add fields, and build integrations.

The key is to start now, with whatever level of detail is feasible, rather than waiting until you have time to build the perfect registry. The perfect registry never happens. The minimal registry that exists and is maintained beats the comprehensive registry that never gets built.

## Registry Anti-Patterns

Several anti-patterns commonly derail task registry efforts.

The "we will do it later" anti-pattern: the team acknowledges that a registry would be valuable but decides to wait until the product is more stable or the team is less busy. The problem is that products never stabilize, and teams never become less busy. Waiting to build the registry just makes it harder, because the product becomes more complex and knowledge becomes more fragmented.

The "too much detail" anti-pattern: the team creates a registry schema with dozens of fields, many of which require significant effort to fill in. Engineers find maintaining the registry burdensome and stop updating it. The registry becomes stale and useless. Better to have a simple registry that is maintained than a comprehensive registry that is abandoned.

The "one person's job" anti-pattern: the team assigns one person to maintain the registry, and that person becomes a bottleneck. They do not have deep knowledge of every task, so they have to interview engineers constantly. Engineers see the registry as someone else's job, not their responsibility. Better to distribute ownership: each task's owner maintains their own registry entry, and one person enforces standards and consistency.

The "no enforcement" anti-pattern: the team builds a registry but does not enforce using it. New tasks are added without registry entries. Existing tasks change without registry updates. The registry quickly diverges from reality and loses credibility. Better to make registry updates a required part of code review and deployment processes.

Avoiding these anti-patterns requires discipline and culture: the team must agree that the registry is valuable, that keeping it updated is everyone's responsibility, and that using it in daily work is the norm.

## The Task Registry as Living Documentation

The best way to think about a task registry is as living documentation: it is not a one-time artifact but an evolving record of the product's current state. It is documentation that people actually read and rely on because it is kept accurate and useful.

Living documentation is the opposite of the traditional model where documentation is written once, becomes outdated immediately, and is never trusted. Living documentation stays synchronized with reality because updating it is part of the workflow.

The task registry is the most important piece of living documentation for an AI product. It answers the foundational questions: what are we building, how are we evaluating it, and who is responsible? These questions do not become less important over time; they become more important as the product and team grow.

If you take one practice from this book and implement it religiously, make it the task registry. It is the single artifact that will save you the most confusion, prevent the most risk, and enable the most effective evaluation. Everything else in this book builds on knowing what tasks you have.

---

With the task registry in place, you now have a complete picture of your product's task landscape and a foundation for systematic evaluation. In Section 3, we turn to building the evaluation strategy itself: defining metrics, collecting ground truth, and establishing the measurement infrastructure that will ensure your AI product works reliably.
