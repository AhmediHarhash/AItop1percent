# 5.8 — I/O Specs as Evaluation Scaffolding

In mid-2025, a healthcare AI company was preparing to launch a clinical decision support system. They had spent eight months building the model, tuning prompts, and refining the architecture. But when leadership asked "How do we know this is ready to ship?" the team scrambled. They had no formal evaluation suite. They had tested the system manually on a few dozen cases, but they did not have systematic coverage of edge cases, failure modes, or adversarial inputs. They spent the next six weeks building an eval suite from scratch, trying to reverse-engineer test cases from the product requirements. The process was chaotic: engineers debating what to test, product managers arguing about what "correct" meant, domain experts suggesting hundreds of edge cases with no prioritization. The eval suite they eventually built was incomplete, inconsistent, and hard to maintain.

The root cause was that they built the eval suite **after** they built the system. They had no blueprint. They had no systematic way to generate test cases. They had no shared definition of what the system should do. If they had started with a strong I/O spec, they would have had all of that. Every field in your I/O spec is something you can test. Every quality dimension is something you can measure. Every edge case is something you can evaluate. The I/O spec is not just a contract for what the system does — it is the **scaffolding** for your entire evaluation strategy.

This subchapter covers how to use your I/O spec as the foundation for building a comprehensive, maintainable evaluation suite, how to generate test cases systematically, and why teams with strong I/O specs ship faster and with higher confidence.

## The Direct Line from Spec to Eval

There is a direct mapping from every element of your I/O spec to your evaluation suite. Here is how it works:

**Input spec becomes test case inputs.** Your input spec defines what kinds of inputs the system should handle: required fields, optional fields, data types, constraints, variability. Each of these becomes a dimension in your test distribution. If your input spec says "the user query can be 1 to 500 characters," you need test cases with short queries, long queries, and boundary cases like exactly 500 characters. If your input spec says "the system supports English and Spanish," you need test cases in both languages.

**Output spec becomes expected outputs.** Your output spec defines what the system should return: the schema, the fields, the data types, the format. Each test case needs an expected output that matches this spec. If your output spec says "the response includes a summary, a confidence score, and citations," every test case should verify that these fields are present and correctly formatted.

**Quality dimensions become rubric criteria.** Your I/O spec includes quality dimensions: accuracy, relevance, tone, safety, fairness. Each of these becomes a scoring dimension in your evaluation rubric. If your spec says "responses must be professional and empathetic," your rubric needs criteria for measuring professionalism and empathy.

**Uncertainty spec becomes confidence evaluation.** Your I/O spec defines how the system should express uncertainty: confidence scores, uncertainty flags, fallback behavior. Each test case should verify that the system's confidence aligns with the quality of the output. If the system is uncertain, it should say so. If the system is confident, it should be right.

**Constraints become pass/fail checks.** Your I/O spec includes hard constraints: latency limits, cost budgets, privacy requirements, content moderation rules. Each of these becomes a binary check in your eval suite. If your spec says "responses must be under 2 seconds," every test case should measure latency and fail if it exceeds 2 seconds.

This mapping is not theoretical. It is **mechanical**. You can write a script that reads your I/O spec and generates the skeleton of your eval suite. The spec tells you what to test, how to test it, and what correct looks like.

## Input Spec to Test Case Inputs

Your input spec is a **catalogue of variability**: all the ways inputs can differ. This catalogue directly translates into your test distribution.

Here is how to generate test case inputs from your input spec:

**Happy path cases.** For each required field, create test cases with typical, well-formed inputs. If your spec says "user query is a natural language question," create test cases with common questions: "What is the weather today?" "How do I reset my password?" "What is the status of my order?"

**Edge cases from constraints.** For each constraint, create test cases at the boundaries. If your spec says "query length is 1 to 500 characters," create test cases with 1 character, 500 characters, and near-boundary values like 499 and 501 characters. If your spec says "date must be in the future," create test cases with today's date, tomorrow's date, and yesterday's date.

**Variability cases.** For each dimension of variability, create test cases covering the range. If your spec says "queries can be simple or complex," create test cases for both. If your spec says "tone can be formal or casual," create test cases for both.

**Optional field cases.** For each optional field, create test cases with and without the field. If your spec says "user context is optional," create test cases where context is provided and where it is missing.

**Invalid input cases.** For each constraint, create test cases that violate the constraint. If your spec says "email must be a valid email address," create test cases with malformed emails: missing at-sign, invalid domain, special characters. These cases test your error handling.

**Adversarial cases.** For each input field, create test cases designed to break the system: prompt injection, jailbreak attempts, gibberish, malicious content. If your spec says "user query is natural language," create test cases with SQL injection patterns, script tags, and Unicode exploits.

By systematically generating test cases from your input spec, you ensure that your eval suite covers the input space comprehensively. You are not guessing what to test — you are testing everything the spec says the system should handle.

## Output Spec to Expected Outputs

Your output spec defines **what correct looks like**. This becomes the expected output for each test case.

Here is how to generate expected outputs from your output spec:

**Schema validation.** For each test case, define the expected output schema. If your spec says "the response is a JSON object with fields summary, confidence, and citations," the expected output for every test case should match that schema. You can validate this programmatically: does the output have the required fields? Are the data types correct?

**Field-level expectations.** For each field, define what a correct value looks like. If the output field is "confidence," define the range: 0.0 to 1.0. If the output field is "summary," define the format: plain text, one to three sentences. If the output field is "citations," define the structure: a list of URLs with titles.

**Quality expectations.** For each quality dimension, define what correct means. If your spec says "summaries must be accurate and concise," define accuracy as "contains no factual errors" and conciseness as "under 100 words." You can measure these with automated metrics or human evaluation.

**Uncertainty expectations.** For each test case, define the expected confidence level. If the input is a common, well-defined query, the system should be confident. If the input is ambiguous or adversarial, the system should express uncertainty. This turns your uncertainty spec into testable assertions.

By defining expected outputs systematically, you turn subjective quality judgments into measurable criteria. You are not debating whether an output is "good" — you are checking whether it meets the spec.

## Quality Dimensions to Rubric Criteria

Your I/O spec includes quality dimensions: accuracy, relevance, coherence, tone, safety. Each of these becomes a scoring dimension in your evaluation rubric.

Here is how to convert quality dimensions into rubric criteria:

**Define the scale.** For each dimension, define how you will score it. Binary pass/fail? A 1-to-5 scale? A 0-to-100 score? The scale depends on the dimension. Safety is often binary: safe or unsafe. Accuracy might be a continuous score: percentage of facts that are correct.

**Define the criteria for each score.** If you use a 1-to-5 scale, define what each score means. For accuracy: 5 means no errors, 4 means minor errors that do not affect the conclusion, 3 means errors that affect the conclusion, 2 means multiple major errors, 1 means completely wrong. Be specific. "Minor errors" is not specific. "Wrong date but correct event" is specific.

**Provide examples.** For each score, provide example outputs. This calibrates evaluators and ensures consistency. If two evaluators disagree on a score, they can refer to the examples to resolve the disagreement.

**Automate when possible.** Some dimensions can be scored automatically. Factual accuracy can be checked against a knowledge base. Tone can be analyzed with sentiment analysis. Content moderation can be checked with safety classifiers. Automate what you can and use human evaluation for the rest.

**Combine dimensions into an overall score.** If your spec has multiple quality dimensions, define how they combine. Weighted average? Minimum score across all dimensions? This depends on your product requirements. If safety is critical, you might use a minimum: the overall score is the lowest score across all dimensions. If all dimensions matter equally, you might use a weighted average.

The rubric is the bridge between your I/O spec and your evaluation results. The spec defines what quality means. The rubric defines how to measure it.

## Uncertainty Spec to Confidence Evaluation

Your I/O spec defines how the system should express uncertainty: confidence scores, uncertainty flags, fallback responses. This becomes a testable assertion: does the system's confidence align with the quality of the output?

Here is how to evaluate confidence:

**Calibration.** For each test case, compare the system's confidence score to the actual quality. If the system says it is 90 percent confident, is it right 90 percent of the time? If the system says it is 50 percent confident, is it right 50 percent of the time? Calibration measures whether confidence scores are meaningful.

**Uncertainty handling.** For test cases where the system should be uncertain, verify that it expresses uncertainty appropriately. If the input is ambiguous, does the system ask for clarification? If the system lacks information, does it say "I do not know"? If the system detects an adversarial input, does it refuse to answer?

**Confidence thresholds.** Your spec may define thresholds: "if confidence is below 0.7, ask for clarification." Test that the system respects these thresholds. Create test cases where confidence is below the threshold and verify that the system asks for clarification instead of returning a low-quality answer.

By testing confidence explicitly, you ensure that your system is not just accurate — it is honest about when it might be wrong.

## Generating Eval Cases from I/O Specs Systematically

Here is the end-to-end process for generating an eval suite from your I/O spec:

**Step 1: Extract test dimensions.** Read through your input spec and list every dimension of variability: required fields, optional fields, constraints, data types, edge cases.

**Step 2: Generate happy path cases.** For each dimension, create test cases with typical, well-formed inputs.

**Step 3: Generate edge cases.** For each constraint, create test cases at the boundaries.

**Step 4: Generate adversarial cases.** For each input field, create test cases designed to break the system.

**Step 5: Generate regression cases.** Review production incidents and create test cases for each failure mode you have seen.

**Step 6: Define expected outputs.** For each test case, define the expected output based on your output spec: schema, field values, quality scores, confidence levels.

**Step 7: Build the rubric.** Convert your quality dimensions into scoring criteria. Define the scale, the criteria, and the examples.

**Step 8: Automate what you can.** Write scripts to validate schema, check constraints, measure latency, and score automated dimensions like factual accuracy.

**Step 9: Run human evaluation for the rest.** For dimensions that cannot be automated, run human evaluation using the rubric.

**Step 10: Iterate.** As your product evolves, update the I/O spec, regenerate test cases, and update the rubric.

This process is **systematic**. You are not guessing what to test. You are deriving test cases directly from the spec.

## Why Teams with Strong I/O Specs Build Eval Suites Faster

Teams that skip the I/O spec and jump straight to building eval suites face three problems:

**Problem 1: Incomplete coverage.** Without a spec, you do not know what to test. You end up testing the cases you happen to think of, which means you miss edge cases, adversarial cases, and failure modes.

**Problem 2: Inconsistent criteria.** Without a spec, different stakeholders have different definitions of "correct." Product says the output should be concise. Domain experts say it should be comprehensive. Legal says it should be conservative. You build test cases that reflect one stakeholder's view, then get pushback from others.

**Problem 3: Unmaintainable test suites.** Without a spec, test cases are ad hoc. When the product changes, you do not know which test cases are still valid. You end up with hundreds of test cases that are hard to organize, hard to update, and hard to interpret.

Teams with strong I/O specs do not have these problems. The spec is the **source of truth**. Everyone agrees on what the system should do. Test cases are derived systematically. When the product changes, you update the spec, and the test cases follow.

This is why teams with strong I/O specs build eval suites faster and more comprehensively. They are not reinventing the wheel. They are following a process.

## The Feedback Loop: Eval Results Improve the Spec

Building the eval suite is not the end. Running the eval suite produces results, and those results reveal gaps in your I/O spec.

Here is the feedback loop:

**You write the I/O spec.** You define inputs, outputs, quality dimensions, constraints.

**You generate the eval suite from the spec.** You create test cases, expected outputs, and rubric criteria.

**You run the eval suite.** You test the system and collect results.

**You analyze failures.** You find cases where the system fails. Some failures reveal bugs in the implementation. Other failures reveal gaps in the spec.

**You update the spec.** If a failure reveals that your input spec missed a constraint, you add it. If a failure reveals that your output spec is ambiguous, you clarify it. If a failure reveals a new quality dimension, you add it.

**You regenerate the eval suite.** With the updated spec, you generate new test cases and update the rubric.

**You run the eval suite again.** You iterate.

This feedback loop is critical. Your first I/O spec will not be perfect. The eval suite is how you discover what you missed. Each iteration makes the spec more precise, which makes the eval suite more comprehensive, which makes the product more robust.

## Practical Takeaways

**Use the I/O spec as the blueprint for your eval suite.** Do not build test cases in a vacuum. Derive them systematically from the spec.

**Map every element of the spec to an eval dimension.** Input spec to test case inputs. Output spec to expected outputs. Quality dimensions to rubric criteria. Uncertainty spec to confidence evaluation.

**Generate test cases systematically.** Happy path, edge cases, adversarial cases, regression cases. Do not guess. Cover the input space.

**Define expected outputs explicitly.** Schema validation, field-level expectations, quality expectations, uncertainty expectations.

**Build a rubric from quality dimensions.** Define the scale, the criteria, the examples. Make scoring consistent and calibrated.

**Automate what you can.** Schema validation, constraint checks, automated metrics. Use human evaluation for the rest.

**Iterate on the spec.** Eval results reveal spec gaps. Update the spec and regenerate the eval suite.

The next question is: what happens when your product evolves? How do you version your I/O spec, manage breaking changes, and migrate downstream systems without breaking everything?
