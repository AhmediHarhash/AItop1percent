# 6.12 — Instrumentation Plan: Events and Logs for Eval and Monitoring

A Series B fintech startup launched their AI-powered fraud detection system in March 2024 with strong evaluation metrics from offline testing. The model showed 94% precision and 89% recall on their curated test set, and the product team was confident they'd built something transformative. They shipped to production with standard application logging—errors, response times, service health—and watched the dashboards for crashes. None came. Two months later, the fraud operations team reported that they were seeing more false positives than before the AI system launched, and customer support was fielding complaints about legitimate transactions being blocked. The ML team wanted to investigate, but they had no way to reproduce the production behavior. They couldn't see what inputs the model was receiving, what confidence scores it was producing, or which features were driving decisions. They had application logs showing that the service was healthy, but no evaluation-specific instrumentation showing whether the model was working correctly. They spent six weeks building retroactive logging, reprocessing historical data from database backups, and trying to reconstruct what had happened. By the time they identified the issue—a distribution shift in transaction patterns that their model hadn't seen in training—they'd lost customer trust and the fraud ops team had already reverted to the manual review process.

The root cause was not a technical limitation. The infrastructure could have supported comprehensive logging from day one. The mistake was conceptual: the team treated instrumentation as an engineering detail to be added after launch, not as a framing requirement to be specified before the first line of model code was written. They built a system they couldn't evaluate in production because they never defined what data they would need to evaluate it. You cannot debug what you cannot observe, and you cannot observe what you did not instrument.

## Instrumentation Is a Framing Artifact, Not an Implementation Detail

Most teams approach logging as a production concern: you add logs to help engineers debug crashes, monitor latency, and track service health. This works for deterministic software, where you can reproduce bugs locally and test behavior in staging environments. AI systems are different. The behavior depends on the data, and the data in production is never identical to the data in development. Your model will see inputs you never tested against, produce outputs you never anticipated, and fail in ways you cannot predict. The only way to understand production behavior is to log enough information to reconstruct every decision the system made.

**Instrumentation planning** is the process of defining, during problem framing, what events and data you will capture when your AI system runs in production. This is not about choosing a logging library or configuring a monitoring platform. Those are implementation choices that come later. Instrumentation planning is about specifying the contract between your AI system and your evaluation infrastructure: what information must be recorded, in what format, with what metadata, at what granularity, and with what retention policy. This specification becomes an artifact of the framing phase, alongside your success metrics, your evaluation dataset plan, and your risk documentation. You hand this spec to your engineering team and tell them: these are the events we must capture to evaluate whether this system is working.

Teams that skip this step discover the gap when they try to run their first production evaluation. They realize they're logging model outputs but not the inputs that produced them. They're capturing latency but not token counts. They're recording errors but not the intermediate steps that led to the error. They go back and add instrumentation, but now they're working with incomplete historical data and they've lost the ability to analyze the first weeks or months of production behavior. Instrumentation must be planned before launch because you cannot retroactively observe the past.

## What to Log: The AI-Specific Event Taxonomy

Application logs for traditional software capture a narrow set of events: requests, responses, errors, and performance metrics. AI systems require a much richer event taxonomy because the unit of work is not a single HTTP request, it's a multi-step inference process involving model calls, tool usage, prompt construction, output parsing, and decision logic. You need to log every step in this process if you want to evaluate and debug the system in production.

Start with **inputs**: every piece of data that influences the model's behavior. This includes the raw user input, the prompt that gets sent to the model (after any templating or context injection), the conversation history for multi-turn interactions, and any retrieved context from RAG systems. If your system uses tool calling or function calling, log the function definitions that were passed to the model. If you're doing classification or structured output generation, log the schema or format constraints. Inputs are the independent variable in your evaluation: you need them to understand what the model was asked to do.

Log **outputs**: the raw model response, the parsed or structured output your system extracted from that response, and any post-processing transformations you applied. If your model produces multiple candidate outputs (like with temperature sampling or best-of-n generation), log all candidates and the selection logic. If you're using chain-of-thought prompting or asking the model to show its reasoning, log the reasoning traces. Outputs are the dependent variable: they're what you're evaluating against your success metrics.

Capture **model call metadata**: which model was used (GPT-4, Claude Opus 4.5, a fine-tuned variant), what API version, what parameters (temperature, max tokens, stop sequences), how many tokens were consumed (prompt tokens, completion tokens, total), and what the API latency was. In 2026, you're likely making calls to multiple models or routing requests based on complexity, so you need to track which model handled which request. Token counts matter for cost analysis and for detecting prompt bloat. Latency matters for user experience and for identifying performance regressions.

Log **tool and function calls**: if your AI system uses function calling, log every function invocation with its arguments and return value. This is essential for multi-step agents where the model decides what tools to use. You need to see the full action sequence to understand whether the agent is using tools correctly or getting stuck in loops. Tool call logs also let you evaluate tool selection accuracy: did the model call the right function with the right arguments.

Record **confidence and uncertainty signals**: if your model produces confidence scores, probabilities, or logprobs, log them. If you're using multiple models and ensembling their outputs, log the individual predictions and the aggregation logic. Confidence signals are critical for evaluating calibration and for implementing confidence-based routing or fallback strategies. A model that's confidently wrong is different from one that's uncertain, and you can only distinguish them if you log the confidence.

Capture **errors and fallbacks**: when the model fails to produce a valid output, when parsing fails, when a tool call returns an error, when you hit rate limits or timeouts, log the error type, the error message, and the fallback behavior. If your system retries failed requests or falls back to a simpler model, log the retry attempts and the final outcome. Error logs are your primary signal for reliability evaluation.

Log **user feedback signals**: if users can upvote or downvote outputs, report errors, or provide corrections, log these events with a reference back to the original request. Explicit feedback is the highest-quality evaluation signal you have in production, and you need to link it to the logged inputs and outputs to build a feedback dataset.

Tag every event with **trace identifiers and request correlation metadata**: a unique request ID that ties together all the logs from a single user interaction, a session ID that groups multiple requests from the same user, a timestamp, a user ID or anonymized user token, and any relevant business context (customer tier, feature flags, A/B test variant). Trace IDs let you reconstruct the full sequence of events for a single request, which is essential for debugging multi-step agents or RAG pipelines.

This is not a minimal set. This is the standard set for a production AI system that you intend to evaluate and improve over time. You will log more than this for complex systems, and you should not log less unless you've explicitly decided you don't care about evaluating certain aspects of behavior.

## Structured Logging: Machine-Parseable, Not Human-Readable

Your logs must be structured data, not prose. Every log entry should be a JSON object (or Protobuf, or Avro, or any other structured format) with well-defined fields and consistent types. You should be able to load your logs into a dataframe or a database and run queries without writing custom parsers. This is not negotiable. If your logs are unstructured strings, you cannot do large-scale evaluation or analysis.

Traditional application logging encourages human-readable messages: "User 12345 submitted query 'what is the weather' and received response 'The weather is sunny' in 234ms." This format is fine for a human skimming logs to understand what happened, but it's terrible for machines. You cannot easily extract the user ID, the query, the response, or the latency without fragile string parsing. You cannot aggregate metrics across thousands of log lines. You cannot join logs with other datasets. Unstructured logs are write-only: you can append to them, but you cannot analyze them at scale.

Structured logging means every log entry has explicit fields with typed values. The same event would be logged as a JSON object: `{"event_type": "model_response", "request_id": "req_abc123", "user_id": "user_12345", "input": "what is the weather", "output": "The weather is sunny", "model": "gpt-4-turbo", "latency_ms": 234, "timestamp": "2026-01-30T14:23:45Z"}`. Now you can query by event type, filter by user, calculate average latency, and join with other datasets on request_id. You can load millions of log entries into a data warehouse and run SQL queries or pandas operations without writing a single regex.

Define a schema for your log events during the instrumentation planning phase. Specify the event types (model_request, model_response, tool_call, error, feedback), the required fields for each type, the field types (string, int, float, timestamp, array, object), and any validation rules. Treat your log schema with the same rigor you'd treat a database schema or an API contract. Version it. Document it. Enforce it. If your logging code emits a malformed event, it should fail loudly, not silently corrupt your logs.

Use a logging library or framework that supports structured logging natively. In Python, libraries like `structlog` or `python-json-logger` make this easy. In TypeScript, `pino` or `winston` with JSON formatters. Most modern observability platforms (Datadog, Honeycomb, New Relic, Grafana) expect structured logs and provide rich querying interfaces for JSON-formatted events. If you're building a custom logging pipeline, use a schema-aware format like JSON Schema or Protobuf definitions to ensure consistency.

## Trace IDs and Request Correlation: Linking Multi-Step Agent Actions

A single user request to an AI agent may trigger dozens or hundreds of internal operations: multiple model calls, tool invocations, database queries, API requests to external services, and intermediate processing steps. Your application logs will contain entries from all these operations, interleaved with logs from other concurrent requests. Without a way to correlate logs across these operations, you cannot reconstruct what happened during a single request.

**Trace IDs** solve this problem. A trace ID is a unique identifier that gets generated when a user request enters your system and gets attached to every log entry and event emitted during the processing of that request. When you query your logs for a specific trace ID, you get the complete timeline of everything that happened: when the request arrived, what prompt was constructed, which model was called, what tools were invoked, what the intermediate outputs were, and what the final response was. This is the fundamental primitive for debugging and evaluation.

Generate a trace ID at the entry point of your system—the API gateway, the web handler, the message queue consumer—and propagate it through every function call, every service boundary, and every log statement. In distributed systems, this is called distributed tracing, and the OpenTelemetry standard provides libraries and conventions for propagating trace context across services and processes. In 2026, most AI systems are distributed: your application server calls an LLM API, which may call back to your server via function calling, which may call a vector database or a search service. You need trace IDs to follow the request path through this distributed execution.

For multi-turn conversations or sessions, you also need **session IDs** to group related requests. A user might have a 20-turn conversation with your AI assistant, and you want to analyze the full conversation, not just individual turns. Session IDs let you reconstruct conversation history, evaluate how the system's behavior changes over the course of a session, and identify session-level patterns like user frustration or task abandonment.

Add **parent-child relationships** for nested operations. If a single user request triggers three model calls, log each model call with its own event, but also record that all three are children of the same parent request. This gives you a trace tree: the root is the user request, the children are the model calls, and the grandchildren might be tool calls triggered by those model calls. Trace trees let you visualize the execution flow and identify performance bottlenecks or logical errors in your agent's decision-making.

Include **business context** in your trace metadata: which feature the user was using, which A/B test variant they were assigned to, which customer tier they're in, which deployment version served the request. This context lets you slice your evaluation by meaningful dimensions. You can compare model performance across features, across test variants, across customer segments. Without this context, you can only evaluate the aggregate behavior of your system, not the behavior within specific use cases or user groups.

## The Evaluation Feedback Loop: Production Logs as Evaluation Data

The ultimate purpose of instrumentation is to close the loop between production and evaluation. Your production logs become your evaluation dataset. You export a sample of logs, run your evaluation metrics on the logged inputs and outputs, and measure whether your production system is meeting its success criteria. This is not a separate process from logging; it's the direct consequence of logging the right data in the right format.

Every production evaluation starts with a query against your logs: select all requests from the past week where the user gave negative feedback, or sample 1000 random requests from the past month, or filter to requests that used the new model variant. You load the results into a dataframe, extract the input and output fields, and pass them to your evaluation functions. If your logs are well-structured and complete, this process takes minutes. If your logs are incomplete or unstructured, this process takes days or becomes impossible.

The logged data must match the schema expected by your evaluation code. If your evaluator expects an input field called `user_query` and your logs use `query`, you've introduced a mapping step. If your evaluator expects a confidence score and your logs don't include one, you cannot run that evaluator on production data. Define your log schema to align with your evaluation schema from the start. Ideally, they should be identical: the same field names, the same data types, the same nested structures. This makes the evaluation pipeline trivial.

User feedback signals from production—upvotes, downvotes, corrections, bug reports—become ground truth labels for your evaluation dataset. When a user marks an output as incorrect, you have a labeled negative example. When they accept a suggestion, you have a labeled positive example. These labels are noisy and incomplete (most users don't give feedback, and feedback is biased toward extreme cases), but they're real-world signals that reflect actual user experience. Log feedback events with trace IDs so you can join them back to the original request logs and build a labeled dataset for retraining or for measuring precision and recall in production.

This feedback loop is continuous. You're not running one evaluation and calling it done. You're running evaluations weekly or daily, tracking metrics over time, detecting regressions, identifying new failure modes, and feeding the results back into your model development process. Instrumentation makes this loop fast and cheap. Without instrumentation, you'd need to manually label production data or run expensive user studies every time you wanted to evaluate a change.

## Sampling Strategies: You Cannot Log Everything

Comprehensive logging is expensive. You're multiplying your storage costs by orders of magnitude, you're adding latency to every request, and you're generating more data than you can analyze. At scale, you cannot log every field of every event at full fidelity for every request. You need a sampling strategy that captures enough data for evaluation while keeping costs and performance acceptable.

The simplest strategy is **uniform random sampling**: log full details for a random subset of requests (1%, 10%, whatever your budget allows) and log minimal details for the rest. This works if you have high request volume and you're looking for aggregate metrics. A 1% sample of a million requests is still 10,000 requests, which is plenty for statistical analysis. The downside is that you might miss rare events: if a particular failure mode happens once per 10,000 requests, your 1% sample might not capture it.

A better strategy is **stratified sampling**: sample at different rates for different types of requests. Log 100% of errors, 50% of requests with negative user feedback, 10% of requests from new users, and 1% of routine requests. This ensures you capture high-value signals (errors, failures, edge cases) while reducing logging volume for low-value signals (successful requests that look like thousands of others). Define your strata based on the dimensions you care about: user segment, feature, model variant, confidence score, output length.

For high-stakes systems, use **critical path logging**: log everything for requests that trigger certain conditions (high-value transactions, admin actions, model calls that fall below a confidence threshold) and sample everything else. This gives you full visibility into the cases that matter most while controlling costs on the long tail. The critical paths should be defined during framing based on your risk analysis and success metrics.

Consider **time-based sampling**: log full details during certain time windows (the first week after a deployment, during a scheduled evaluation period, during an incident investigation) and reduce logging outside those windows. This gives you detailed data when you're actively evaluating or debugging, and reduces costs during steady-state operation. Some teams implement dynamic sampling that increases logging rates automatically when anomaly detection systems flag unusual behavior.

For very large fields (full prompts, long outputs, conversation histories), use **field-level sampling or truncation**: log the first 500 characters of the output, or log the full output for only 10% of requests, or hash the output and log the hash plus a length metric. This reduces storage costs while preserving enough information for most analyses. If you later need the full output for specific requests, you can retrieve it from a separate archive or regenerate it from the logged inputs.

Whatever strategy you choose, specify it in your instrumentation plan. Your engineering team needs to know what to log, at what rates, with what retention policies. Your evaluation team needs to know what data will be available and how to account for sampling bias in their analyses.

## Privacy-Safe Logging: Evaluability Without PII

Logging inputs and outputs for AI systems creates immediate privacy and security concerns. User inputs may contain personally identifiable information (PII), sensitive business data, credentials, or confidential documents. If you log this data without redaction, you're creating a security risk and potentially violating privacy regulations (GDPR, CCPA, HIPAA).

The naive solution is to not log inputs at all, but this makes evaluation impossible. You cannot measure accuracy without seeing the inputs the model received. You cannot debug failures without reproducing the request. The correct solution is to log enough information for evaluation while removing or protecting sensitive data.

**Redaction** is the process of removing PII from logs before they're written to storage. Run a detection pass over user inputs and outputs to identify emails, phone numbers, credit card numbers, social security numbers, names, addresses, and other identifiable data. Replace these with placeholder tokens: `[EMAIL]`, `[PHONE]`, `[NAME]`. This preserves the structure and length of the input while removing the sensitive content. You can still evaluate whether the model handled an email address correctly, you just don't know whose email it was.

For more sophisticated systems, use **pseudonymization**: replace PII with consistent pseudonyms that let you track the same entity across requests without revealing the actual identity. If a user mentions their name in multiple turns of a conversation, replace it with the same pseudonym each time. This preserves relational structure (you can see that the same user appeared twice) without revealing who they are. Libraries like Microsoft's Presidio or custom NER models can automate this process.

For regulated environments (healthcare, finance), consider **on-device or local logging** where sensitive data never leaves the user's environment. Log only anonymized metadata (request IDs, latency, error codes, confidence scores) to central systems, and store full inputs and outputs locally on the user's device or in a secure enclave. You lose some evaluability, but you maintain compliance.

Be explicit about what you're logging and what you're redacting in your instrumentation plan. Your privacy and security teams need to review this plan before you launch. Your evaluation team needs to understand what data will and will not be available. If you redact parts of the input, you may not be able to run certain evaluators that depend on seeing the full input. This is a tradeoff you make during framing, not something you discover during evaluation.

In some cases, you can log **derived features instead of raw inputs**: instead of logging the full user query, log its length, its language, its detected intent, and its similarity to known templates. This gives you enough signal to analyze patterns without exposing the content. Instead of logging the full document uploaded by a user, log its file type, size, page count, and a hash. This is a degraded signal, but it's better than no signal.

## The Instrumentation Spec as a Deliverable

By the end of the framing phase, you should have a written instrumentation specification that defines what your system will log. This is a formal artifact, not a set of verbal agreements or vague intentions. It should be specific enough that your engineering team can implement it without asking clarifying questions, and comprehensive enough that your evaluation team knows exactly what data will be available.

The spec includes the **event schema**: a list of event types (model_request, model_response, tool_call, error, user_feedback) with the fields for each type, their data types, whether they're required or optional, and example values. This is the contract between your logging code and your evaluation code. Use JSON Schema or a similar schema definition language to make it precise and machine-readable.

Define **sampling rates and policies**: which events are logged at 100%, which are sampled, what the sampling rates are, and under what conditions sampling rates change. Specify retention policies: how long logs are kept in hot storage, when they're archived to cold storage, when they're deleted.

Specify **PII handling**: which fields may contain sensitive data, what redaction or pseudonymization will be applied, and who has access to raw vs. redacted logs. Include references to your privacy policy and compliance requirements.

Describe the **logging infrastructure**: where logs will be stored (which database, which cloud storage bucket, which observability platform), what the access controls are, what the query interfaces are. If you're using OpenTelemetry or a similar tracing framework, specify the configuration and the trace propagation strategy.

Include **alert and monitoring rules**: which logged metrics will trigger alerts (error rate thresholds, latency percentiles, token usage spikes), and who gets notified. This connects your instrumentation plan to your operational monitoring.

Document **evaluation integration**: how logs will be exported for evaluation, what the export format will be, how often exports happen, and what the evaluation pipeline expects. If you're using a tool like Weights and Biases, MLflow, or a custom evaluation platform, specify how logged data flows into that tool.

This spec gets reviewed by your engineering lead, your ML lead, your security team, and your privacy team. It gets versioned and stored alongside your other framing artifacts. When you hand off the project from framing to implementation, this spec is part of the handoff package.

## Instrumentation and Observability Platforms in 2026

The tooling landscape for AI observability has matured significantly. In 2026, you're not building logging infrastructure from scratch unless you have very specific requirements. Most teams use a combination of general-purpose observability platforms and AI-specific tools.

General-purpose platforms like **Datadog**, **Honeycomb**, **Grafana Cloud**, and **New Relic** support structured logging, distributed tracing, and rich querying. They integrate with OpenTelemetry for trace propagation across services. You can send your AI system logs to these platforms and use their dashboards, alerting, and anomaly detection features. They're built for high-volume production systems and handle the scaling and retention challenges for you.

AI-specific platforms like **Weights and Biases**, **Arize**, **Arthur**, and **WhyLabs** provide deeper integration with ML workflows. They understand model inputs and outputs as first-class concepts, not just arbitrary log fields. They provide built-in evaluation metrics, drift detection, and feedback loop integration. Some teams use these platforms in development and testing, then export production logs to them for continuous evaluation.

**OpenTelemetry** is the standard for distributed tracing and observability instrumentation. It provides libraries for most languages and frameworks, conventions for propagating trace context, and integrations with all major observability backends. If you're building a distributed AI system in 2026, you should be using OpenTelemetry for trace and span management. The community has started adding AI-specific semantic conventions (standard attribute names for model calls, token counts, prompt templates), which makes instrumentation more consistent across projects.

Some teams build **custom logging pipelines** using tools like Apache Kafka or AWS Kinesis for real-time log streaming, Parquet or Delta Lake for columnar storage, and DuckDB or ClickHouse for fast querying. This gives you full control over costs and data retention, but it requires more engineering investment. The tradeoff is flexibility vs. maintenance burden.

Choose your tooling during the framing phase, not after you've started logging. Your instrumentation plan should specify which platforms you're using and how they integrate with your system. This influences your logging schema, your trace ID format, and your sampling strategy.

## From Framing to Evaluation Readiness

Instrumentation planning is the last piece of the framing puzzle. You've defined your success metrics, you've scoped your evaluation dataset, you've identified your risks, you've reviewed your security posture, and now you've specified what data you'll capture to measure and monitor all of this in production. The framing work is complete.

You now have a complete package to hand to your evaluation and engineering teams: the problem definition, the success criteria, the evaluation plan, the dataset requirements, the risk mitigations, the security controls, and the instrumentation specification. These are not aspirations or rough ideas. These are precise, documented artifacts that define what success looks like and how you'll measure it. Your team is ready to move from framing into evaluation strategy, where they'll design the specific evaluation methods and build the infrastructure to execute them. The groundwork is laid. The system you build from this point forward will be one you can actually evaluate, debug, and improve, because you planned for observability from the start.

