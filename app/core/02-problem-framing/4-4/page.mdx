# 4.4 — Performance Success Criteria

The prototype was remarkable. The AI code reviewer provided insightful feedback, caught subtle bugs, and suggested elegant improvements that impressed even senior engineers. The CTO was sold. They approved budget for production deployment, expecting this would become a core part of the development workflow. Engineering spent three months building the production system, integrating it with the IDE, and preparing for launch.

Launch day arrived. Engineers started using the tool. Within hours, complaints filled Slack. The AI took 45 seconds to review a function. Real-time feedback was impossible. Engineers would write code, request a review, context-switch to another task, then try to remember what they were doing when the review finally arrived. The feature that had felt magical in demos felt broken in practice. Not because the feedback quality dropped—it was still excellent—but because the latency made it unusable.

The team had defined functional criteria for review quality and behavioral criteria for feedback tone, but they had never defined performance criteria. They never specified that reviews must complete within 5 seconds to be viable for real-time workflow. They never calculated cost-per-review to determine if the feature would be economically sustainable at scale. They never established throughput targets to ensure the system could handle their 500 engineers all requesting reviews simultaneously. They built a system that was technically correct and behaviorally excellent but operationally impossible.

## The Operational Viability Gap

Performance success criteria define the operational constraints within which your AI system must function: latency, throughput, cost-per-query, and token budgets. These criteria determine whether a technically correct system is actually shippable. They are the criteria that separate impressive demos from viable products.

Many AI projects make the mistake of treating performance as an implementation detail to be optimized later. This fails for two reasons. First, performance characteristics are often architectural constraints. If you need sub-second latency and your chosen approach requires 10 seconds, no amount of optimization will close that gap—you need a different architecture. Second, performance requirements often involve fundamental tradeoffs with quality. You can almost always make an AI system better by using a larger model, more sophisticated retrieval, or more extensive reasoning, but those improvements come at the cost of latency and expense.

Performance criteria must be defined at framing time, alongside functional and behavioral criteria. They are not separate concerns—they are interlinked dimensions of the same system. A customer service bot that gives perfect answers in 30 seconds has failed just as surely as one that gives mediocre answers instantly.

## Latency: The Primary Performance Criterion

Latency is the time between when a user submits a query and when they receive a complete response. For AI systems, latency is often the most critical performance criterion because it directly determines whether the system fits into user workflows.

Latency requirements vary dramatically by use case, and your criterion must be specific to your application context.

**Voice and Real-Time Interaction: Under 300 Milliseconds**

For voice assistants, real-time conversation agents, and any system that operates in spoken interaction, latency must feel instantaneous. Research on conversation dynamics shows that pauses longer than 300-500 milliseconds feel unnatural and disrupt flow. If your system is meant to participate in natural dialogue, your latency budget is under 300 milliseconds from speech-to-text to response generation to text-to-speech.

This is an extremely stringent requirement that constrains architecture significantly. You cannot use large models with extensive reasoning. You cannot do complex retrieval across large knowledge bases. You must optimize ruthlessly for speed, often accepting quality tradeoffs to meet the latency target.

**Chat and Interactive Tools: Under 3 Seconds**

For text-based chat interfaces, document analysis tools, and interactive assistants, latency should be under 3 seconds for the system to feel responsive. Users will tolerate a brief pause, but beyond 3 seconds they perceive the system as slow and start to disengage.

This is still strict but far more achievable than voice requirements. You can use moderately large models, perform retrieval over reasonably sized knowledge bases, and employ some reasoning chains. But you must still be intentional—using the largest available model with extensive multi-step processing will likely exceed your budget.

**Asynchronous Tasks: Under 30 Seconds**

For email responses, document generation, report creation, and similar tasks where the user does not expect immediate feedback, latency can stretch to 30 seconds or even a few minutes. The user submits a task and can do something else while waiting, returning when notified that the result is ready.

This more generous budget allows for more sophisticated processing: larger models, more extensive retrieval, multi-stage reasoning, quality checks, and refinement. But even here, unbounded latency is unacceptable. Users expect completion within a reasonable timeframe, and "reasonable" is usually measured in seconds to minutes, not hours.

**Batch Processing: Hours Acceptable**

For true batch workloads—analyzing thousands of documents overnight, generating monthly reports, processing historical data—latency is measured in hours and sometimes days. The criterion is not "fast" but "completes within the batch window."

Even for batch processing, you should specify completion targets. A batch job that processes 10,000 documents should complete within 8 hours, or 12 hours, or whatever window makes sense for your workflow. This prevents architectural choices that technically work but take so long that the outputs are no longer useful.

**Context-Dependent Latency Requirements**

Some systems require different latency targets for different query types. Simple informational queries might need sub-second response while complex analysis queries might accept 10 seconds. Your criteria should specify these tiers.

For a research assistant, criteria might specify: simple fact lookups must complete within 1 second; queries requiring document retrieval must complete within 5 seconds; queries requiring multi-document analysis may take up to 15 seconds; comprehensive research synthesis may take up to 60 seconds.

By tiering latency requirements, you avoid over-optimizing simple cases and under-delivering on complex ones. The system can route queries to appropriate processing paths based on complexity.

## Throughput: Handling Scale

Throughput defines how many queries your system can handle per unit time, typically measured in queries per second. This criterion determines whether your system can serve your user base at peak load.

Throughput requirements should be based on realistic usage projections that account for peak rather than average load.

If your customer service AI serves 10,000 customers and each customer averages 2 queries per day, that is 20,000 queries per day. But those queries are not evenly distributed. Peak hours might see 50x average load. You need to handle not 0.23 queries per second average but 11.5 queries per second at peak.

Throughput criteria should specify:

Peak queries per second: The maximum load the system must handle.

Sustained queries per second: The load the system must handle continuously without degradation.

Burst capacity: The ability to handle temporary spikes beyond sustained levels.

For a public-facing AI tool, criteria might specify: must sustain 100 queries per second continuously; must handle bursts up to 500 queries per second for up to 5 minutes; must gracefully degrade under load beyond 500 QPS by queueing requests rather than failing.

Throughput interacts with latency. A system that handles 100 QPS at 2-second latency is different from one that handles 100 QPS at 10-second latency. The second system requires far more concurrent processing capacity. Your criteria should specify both throughput and latency together.

## Cost: The Economic Viability Constraint

Cost-per-query determines whether your AI system is economically sustainable. A system that costs 5 dollars per query might be technically impressive but economically impossible for a consumer application with free tiers or low subscription prices.

Cost criteria should be specified as cost-per-query budgets that vary by user tier and use case.

**Free Tier: Under 1 Cent Per Query**

For free tier users or ad-supported applications, cost-per-query must be extremely low. The revenue per query from ads or conversion to paid tiers is measured in fractions of a cent. If your cost is 2 cents per query and your revenue per query is 0.5 cents, the unit economics do not work.

This constraint often means using smaller models, limiting query complexity, capping usage, or restricting free tier access to less expensive features.

**Paid Tier: Under 10 Cents Per Query**

For paid subscription products where users pay 10-20 dollars per month and make hundreds of queries, cost-per-query should be under 10 cents to maintain reasonable margins.

This budget allows for more sophisticated processing but still requires cost discipline. Using the largest available models for every query, extensive retrieval, or multi-step reasoning chains can quickly exceed this budget.

**Enterprise and High-Value Transactions: Under 50 Cents to Several Dollars Per Query**

For enterprise applications or high-value transactions—complex legal analysis, medical diagnosis support, financial recommendations—cost-per-query can be higher because the value delivered is proportionally higher.

But even here, costs matter. If your system costs 3 dollars per query and the customer uses it 1,000 times per month, that is 3,000 dollars in inference costs alone. The pricing must account for this, and the value delivered must justify it.

**Cost Components**

Cost-per-query includes multiple components:

Model inference: The cost of generating responses, typically charged per token by API providers or per GPU-second for self-hosted models.

Retrieval: The cost of vector search, database queries, or API calls to knowledge sources.

Pre-processing and post-processing: Any additional computation, including embedding generation, re-ranking, filtering, or verification steps.

Storage and caching: Costs for storing conversation history, caching results, or maintaining user-specific data.

Your criteria should specify total cost-per-query budget and potentially break down acceptable costs by component to guide optimization efforts.

## Token Budgets: The Hidden Constraint

Token budgets define maximum input and output tokens per request. They matter for three reasons: models have hard token limits, tokens directly impact cost, and tokens impact latency.

Token budget criteria should specify:

Maximum input tokens: How much context the system can accept. This determines how much conversation history you can include, how many documents you can retrieve, and how detailed user queries can be.

Maximum output tokens: How long responses can be. This determines whether you can generate detailed explanations, long-form content, or must limit responses to brief summaries.

For a document Q&A system, criteria might specify: maximum input tokens of 8,000, including up to 6,000 tokens from retrieved documents, 1,500 tokens from conversation history, and 500 tokens for system prompts; maximum output tokens of 1,000, sufficient for detailed answers with citations.

Token budgets force tradeoffs. If you want to include more conversation history, you have less room for retrieved documents. If you want longer responses, you must either reduce input context or increase total token budget at higher cost and latency.

## The Performance-Quality Tradeoff

The fundamental tension in performance criteria is that you can almost always improve quality by sacrificing performance. Larger models generate better responses but cost more and run slower. More retrieval steps improve accuracy but increase latency. Multi-stage reasoning improves correctness but multiplies costs.

This means performance criteria and quality criteria must be considered together. Your functional and behavioral criteria define the quality bar. Your performance criteria define the constraints within which you must meet that bar. The architecture and implementation must satisfy both.

In practice, this often means:

Tiering approaches: Use fast, cheap models for simple queries that do not need high quality. Reserve expensive, sophisticated processing for complex queries where quality matters most.

Quality-performance frontiers: Rather than a single implementation, explore multiple points on the quality-performance curve. Identify the minimum quality threshold from your functional criteria, then optimize for performance within that constraint.

Graceful degradation: Design systems to scale performance down under load while maintaining minimum quality standards. Reduce retrieval depth, use smaller models, or limit output length before failing entirely.

Performance criteria should not be afterthoughts to be optimized once the system works. They should be first-class constraints that shape architecture from the beginning.

## Setting Realistic Performance Targets

How do you know what performance targets are realistic? Three approaches:

**Benchmark Existing Solutions**

Look at what competitors or analogous products achieve. If every customer service chatbot in your industry responds within 3 seconds, that is likely a user expectation you must meet. If a competitor offers similar functionality at 1 cent per query, you cannot charge 10 cents per query without offering substantially more value.

**Measure User Tolerance**

For latency, you can conduct user research to measure tolerance thresholds. At what point do users perceive the system as slow? When do they start to abandon interactions? This gives you empirical targets.

For cost, work backward from pricing and volume projections. If users pay 15 dollars per month and make 300 queries, you have 5 cents per query to cover inference costs plus margin.

**Calculate Unit Economics**

Build a simple model: projected users, queries per user, cost-per-query, revenue per user. Does the math work? If not, either the pricing must increase, the costs must decrease, or the business model is not viable.

Realistic performance targets should be ambitious enough to drive good engineering but achievable with current technology and reasonable effort. Setting targets that require breakthroughs is setting up for failure.

## The SLO Framework

Service Level Objectives provide a structured way to define performance criteria. An SLO specifies a target level of service over a time period.

Example SLOs for an AI system:

99% of queries must complete within 5 seconds over any 24-hour period.

The system must maintain 99.5% uptime, measured as the percentage of minutes in a month where the system successfully responds to health checks.

Cost-per-query must average under 3 cents over a monthly period.

SLOs make performance criteria measurable and provide clear standards for operational success. They also allow for realistic engineering: 99% is achievable, 100% is not. The SLO framework acknowledges that occasional performance degradation is acceptable as long as it remains within defined boundaries.

With performance criteria established alongside functional and behavioral criteria, you have defined what success looks like across all critical dimensions: correctness, experience, and operational viability. But defining what the system should do is incomplete without defining what it must never do. That is where we turn our focus next: negative success criteria that establish hard boundaries and failure conditions that cannot be crossed.

