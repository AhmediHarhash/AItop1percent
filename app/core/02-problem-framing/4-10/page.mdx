# 4.10 — Failure Impact Analysis: What Breaks If Wrong, by Tier

In late 2024, a Series B healthcare startup launched an AI assistant that helped medical practices draft patient communication letters. The system generated discharge instructions, appointment reminders, and follow-up care guidelines. The team had excellent accuracy metrics: 94% of letters required no edits, and doctors loved the time savings. But they hadn't mapped failure impact by task type. When the system started generating prescription reminder letters, it occasionally confused dosage instructions, swapping "take one tablet twice daily" with "take two tablets once daily." The error rate was low, under 2%, well within their acceptable threshold for appointment reminders. Three patients were hospitalized. The startup shut down four months later, not from lawsuits but from complete loss of trust. Their investors had funded a productivity tool, not a liability machine.

The root cause wasn't the error rate. It was the failure to recognize that different outputs in the same system carried wildly different failure costs. Appointment reminder errors annoyed patients. Prescription instruction errors harmed them. The team had treated all text generation as equivalent risk because all tasks used the same underlying model and achieved similar accuracy scores. They never built the analysis that would have shown them which outputs required human verification and which could ship automatically. They optimized for average performance when they needed to optimize for worst-case impact.

## Why Failure Impact Analysis Comes Before Architecture

You cannot make sound architecture decisions without knowing what breaks if your system is wrong. Failure impact analysis is not a post-launch audit or a risk management exercise for the legal team. It is a product definition activity that happens during problem framing, before you write a line of code. The reason is simple: the cost of being wrong determines how you build the system, how you evaluate it, what latency you can tolerate, and whether you need a human in the loop. A system that generates social media captions can fail cheaply. A system that generates chemotherapy dosing recommendations cannot. These are not the same product with different domains. They are fundamentally different engineering problems.

Most teams skip this analysis because they assume they understand the stakes. Healthcare feels high-risk, marketing feels low-risk, and that intuition guides their diligence. But risk is not a property of the domain. It is a property of the specific task and its context. A healthcare chatbot that answers "what are your hours" has lower failure impact than a financial advisor chatbot that answers "should I withdraw my 401k early." The domain does not determine the tier. The consequence of being wrong determines the tier. You need a systematic framework to classify every task your system will perform, or you will misallocate your evaluation budget and ship the wrong things automatically.

## The Failure Tier Framework

**Failure tiers** are a classification system that maps every task in your AI system to the severity of harm if the output is wrong. There are four tiers, and they are not about how often failures happen. They are about what happens when they do. Tier 0 failures are cosmetic or annoying. The user notices something is off, but nothing breaks and no one loses anything of value. Tier 1 failures disrupt workflow. The user has to stop what they are doing and fix the problem, costing time and focus but not money or data. Tier 2 failures cause financial loss, data corruption, or damage to reputation. Someone loses something they cannot easily recover. Tier 3 failures risk physical harm, legal liability, or regulatory violation. Someone could be injured, sued, or put out of business.

The tier determines your evaluation strategy. Tier 0 tasks can launch with lightweight manual review and user feedback loops. Tier 1 tasks need structured evaluation sets and regression testing. Tier 2 tasks require domain expert review, adversarial testing, and monitoring dashboards. Tier 3 tasks demand formal verification, external audits, human-in-the-loop workflows, and incident response plans. The evaluation investment scales exponentially. A Tier 3 task needs ten times the evaluation rigor of a Tier 0 task, not because you care more, but because the cost of failure is ten times higher. Most teams evaluate everything at Tier 1 and wonder why their high-stakes features keep causing problems.

Tiers are not intrinsic to the AI task. They are determined by how the output is used. Text summarization is not inherently Tier 0 or Tier 3. Summarizing a podcast for show notes is Tier 0. Summarizing a legal contract for a business decision is Tier 2. Summarizing a patient chart for a surgical planning meeting is Tier 3. The same model, the same accuracy, the same prompt structure, but the failure impact differs by two full tiers. This is why you cannot tier your system by feature. You must tier every distinct use case, every output that a human or downstream system will act on.

## Tier 0: Cosmetic and Annoying

Tier 0 failures break nothing. The user sees an output that is wrong, awkward, or nonsensical, and they either ignore it or regenerate it. No workflow is blocked. No data is lost. The stakes are user experience and polish, not functionality. Examples include AI-generated image captions for social media, autocomplete suggestions in a note-taking app, creative writing prompts, and color palette recommendations. If the system hallucinates a fact in a marketing blog post and a human editor catches it before publishing, that is Tier 0. The failure was contained within the editorial workflow, and the cost was a few seconds of the editor's attention.

Tier 0 does not mean you do not care about quality. It means failure does not cascade. A bad suggestion does not break the next step. The user is aware they are reviewing AI output, and they treat it as a draft. You still want high accuracy because poor quality erodes trust and adoption, but you do not need formal evaluation infrastructure. You can ship with qualitative review, user feedback, and A/B testing. If your precision drops from 90% to 80%, users might complain, but they will not stop using the product or lose money.

The boundary between Tier 0 and Tier 1 is whether the user has to take corrective action that disrupts their task. If the AI generates a bad social media caption and the user just types a new one, that is Tier 0. If the AI mislabels an email as spam and the user has to dig through their spam folder to find an important message, that is Tier 1. The user's workflow was interrupted. The distinction matters because Tier 1 failures require you to measure and minimize false positives in ways that Tier 0 failures do not.

## Tier 1: Workflow Disruption

Tier 1 failures force the user to stop and fix the problem. The AI does something wrong, and the user has to intervene to get back on track. No one loses money or data, but time and focus are wasted. Examples include a code autocomplete that inserts the wrong function name and the developer has to delete it, a document classifier that misfires and the user has to manually refile the document, a calendar assistant that schedules a meeting at the wrong time and the user has to reschedule, or a search system that fails to return the right result and the user has to try a different query. The cost is friction, and friction compounds. A single Tier 1 failure is annoying. A dozen per day makes the tool unusable.

Tier 1 tasks require structured evaluation because you need to measure false positive and false negative rates and balance them against user tolerance. A spam filter that blocks real email is worse than a spam filter that lets spam through, because the user expects to delete spam but does not expect to lose real mail. You need to understand the asymmetry in failure cost, not just the overall accuracy. Tier 1 evaluation means building a labeled test set, running regression tests on every model update, and monitoring user correction rates in production. You cannot rely on vibes and manual spot checks.

The failure mode determines the tier, not the feature. A chatbot that answers customer support questions might be Tier 0 if it is clearly labeled as experimental and the user can easily escalate to a human. The same chatbot is Tier 1 if it is the primary support interface and wrong answers force the user to retry or give up. Context matters. You do not tier the model. You tier the deployment.

## Tier 2: Financial or Data Loss

Tier 2 failures cost the user something they cannot easily recover: money, proprietary data, customer relationships, or business reputation. The AI makes a decision or generates an output that, when acted upon, causes tangible harm. Examples include a fraud detection system that falsely blocks a high-value customer transaction, losing the sale and angering the customer. An expense categorization tool that misclassifies business expenses, leading to incorrect tax filings. A resume screening system that filters out qualified candidates, forcing a company to rehire and delay projects. A content moderation system that false-positives and removes legitimate user content, driving users to competitors. The harm is not immediate physical danger, but the financial or reputational cost is significant.

Tier 2 tasks require domain expert evaluation, adversarial testing, and monitoring dashboards. You cannot evaluate these tasks with crowdsourced labels or junior reviewers. You need people who understand the business impact of each failure mode. If you are building a contract analysis tool that flags risky clauses, your evaluators must be lawyers or contract specialists who know what actually constitutes risk. If you are building dynamic pricing, your evaluators must understand margin, elasticity, and competitive positioning. The evaluation set must include edge cases, adversarial examples, and historical failure modes, not just random samples from production.

Tier 2 systems require real-time monitoring because failure impact can be delayed. A miscategorized expense might not surface until tax season, months after the error. You need instrumentation that tracks downstream outcomes, not just model predictions. Did the blocked transaction reduce revenue? Did the false positive in fraud detection increase churn? You are measuring business metrics, not just accuracy. This is the tier where product and engineering must collaborate with finance, legal, and operations to define what failure looks like and how much of it you can tolerate.

## Tier 3: Safety, Legal, and Regulatory Risk

Tier 3 failures risk physical harm, legal liability, or regulatory violation. Someone could be injured, sued, fired, or imprisoned because your AI was wrong. Examples include medical diagnosis or treatment recommendations, legal advice that exposes a client to liability, autonomous vehicle decision-making, content moderation that fails to remove illegal content, employee performance evaluations that violate employment law, and financial advice that violates fiduciary duty. Tier 3 is not about reputation or lost revenue. It is about harm that cannot be undone and liability that cannot be insured away.

Tier 3 tasks require formal verification, external audits, human-in-the-loop workflows, and incident response plans. You do not ship a Tier 3 task with a single engineer's code review. You need multiple domain experts to review the evaluation plan, the test results, and the production deployment. You need a paper trail that shows you understood the risk and took every reasonable step to mitigate it. You need contracts with customers that clarify liability. You need insurance. You need a legal team to review your disclaimers and terms of service. This is not paranoia. This is the minimum standard in industries that have learned what unmitigated AI risk looks like.

Tier 3 systems must default to human override. If your AI flags a transaction as fraudulent, a human reviews it before blocking. If your AI drafts a legal motion, a lawyer reviews it before filing. If your AI recommends a medical treatment, a doctor reviews it before prescribing. The AI can surface the decision, but it cannot execute the decision. The human is not rubber-stamping. They are applying judgment the AI cannot replicate. The exception is if you have years of evidence that your AI is safer than the human baseline, which almost no one has. Until then, the human stays in the loop.

The EU AI Act, fully enforceable in 2026, explicitly categorizes AI systems by risk and mandates impact assessments for high-risk applications. Tier 3 maps directly to the EU's high-risk category, which includes systems used in employment, education, law enforcement, critical infrastructure, and health. If you are selling into Europe, failure impact analysis is not optional. It is a compliance requirement. You must document what your system does, what breaks if it is wrong, and how you are mitigating that risk. The regulation formalizes what good engineering teams were already doing. It just makes it legally mandatory.

## Mapping Failure Tiers to Your System

Every output your AI system produces must be mapped to a failure tier. Not every feature, not every model, but every distinct task where a user or downstream system acts on the output. Start by listing every decision your system makes or every artifact it generates. Then for each one, ask: what happens if this output is completely wrong? Not slightly off, not 95% correct, but maximally wrong in the worst plausible way. What is the concrete harm?

Walk through the user workflow. If the AI classifies an email as spam and the user never sees it, does the user lose an important opportunity? If so, that is Tier 2. If the AI suggests the wrong meeting time and the user has to reschedule, does that just waste five minutes, or does it cause them to miss a deadline? If the former, Tier 1. If the latter, Tier 2. If the AI generates a product description with a factually incorrect safety warning, does that expose the company to regulatory action? If so, Tier 3. Do not tier based on how unlikely the failure is. Tier based on the impact when it happens.

Different failure modes for the same task can belong to different tiers. A resume screening tool might have three failure modes: it rejects a qualified candidate (Tier 2, you lose the hire), it advances an unqualified candidate (Tier 1, a recruiter wastes time interviewing them), and it crashes and does not process any resumes (Tier 1, the recruiter falls back to manual review). The highest tier determines your evaluation standard. You evaluate the whole task as Tier 2 because the worst failure is Tier 2. You do not get to average the tiers.

Document the failure tier for every task in your framing document, alongside the success criteria. Write it as a table: task name, output, failure mode, tier, justification. This table becomes the foundation for your evaluation plan. Tier 3 tasks get formal test plans, external review, and human-in-the-loop workflows. Tier 2 tasks get domain expert evaluation and production monitoring. Tier 1 tasks get regression tests and user feedback loops. Tier 0 tasks get lightweight qualitative review. You allocate evaluation resources in proportion to failure impact, not in proportion to model complexity or user excitement.

## The Cascade Effect: Tier 0 Failures That Cause Tier 3 Harm

A Tier 0 failure in one component can cause a Tier 3 failure in a downstream component. This is the cascade effect, and it is the reason you cannot tier components in isolation. You must tier the system as a whole. A simple example: an AI transcription service that converts audio to text is Tier 0 if the user reviews the transcript and corrects errors. But if that transcript feeds into a medical record system without human review, and a doctor makes a treatment decision based on the incorrect transcript, the transcription task is now Tier 3. The task did not change. The system integration changed the failure impact.

Cascade analysis requires mapping data flow through your system. Start with the user-facing outputs and work backward. For each output, identify every AI component that contributed to it. Then ask: if component A is wrong, does that make component B wrong in a way that increases the failure tier? If yes, component A inherits the higher tier. A spell-checker in a text editor is Tier 0. A spell-checker in a legal document management system that auto-files motions is Tier 3 because a misspelled party name could invalidate a filing. The spell-checker did not get more sophisticated. The consequences got more severe.

The cascade effect is why you cannot treat evaluation as a per-model activity. You must evaluate the composed system, not just the individual models. Integration testing for AI systems is not about checking that APIs return valid JSON. It is about verifying that error modes in one component do not propagate silently to downstream components. You need end-to-end test cases that inject realistic errors at every stage and verify that the system either corrects them or surfaces them to the user. If your system has ten components and you evaluate each one in isolation at 95% accuracy, your end-to-end accuracy could be 60% if errors compound. You must measure the cascade.

## Failure Impact Across Industries

Healthcare is the obvious Tier 3 domain, but not all healthcare AI is Tier 3. A hospital scheduling assistant that books appointments is Tier 1. A symptom checker that recommends over-the-counter remedies is Tier 1 or Tier 2 depending on whether it disclaims medical advice. A clinical decision support tool that recommends treatments is Tier 3. A medical coding assistant that assigns billing codes is Tier 2 because incorrect coding can trigger audits and fines. The task determines the tier, and healthcare spans all four tiers. You cannot blanket-classify the domain.

Finance is similar. A robo-advisor that recommends portfolio allocation to retail investors is Tier 3 because it creates fiduciary liability. A transaction categorization tool for personal budgeting is Tier 1. A fraud detection system for a payment processor is Tier 2 because false positives lose revenue and false negatives lose money to fraud. A chatbot that answers "when is my payment due" is Tier 0 if the user can easily verify the answer elsewhere. The same infrastructure, the same models, but the failure tiers differ by two or three levels depending on what the output controls.

Content moderation is Tier 2 or Tier 3 depending on the platform and the regulatory environment. A moderation system that removes spam comments is Tier 1. A system that removes alleged copyright violations is Tier 2 because false positives could violate fair use and trigger legal disputes. A system that removes illegal content like child exploitation material is Tier 3 because failure to remove it exposes the platform to criminal liability. The EU Digital Services Act in 2026 makes this explicit: large platforms must have robust moderation systems and can be fined for systemic failures. The failure tier is not about what you want to prioritize. It is about what the law requires.

E-commerce product recommendations are usually Tier 0, but dynamic pricing can be Tier 2 if incorrect prices lose margin or violate pricing agreements with partners. A search ranking algorithm is Tier 1 if it wastes user time. The same algorithm is Tier 2 if it systematically buries a category of products and causes a revenue shortfall. A size recommendation tool is Tier 1 if it leads to returns. The same tool is Tier 2 if it causes a wave of returns that damages the brand. The business context determines the tier. You cannot tier the AI in the abstract.

## Documenting Failure Impact in the Framing Document

Your framing document must include a failure impact matrix: a table that lists every task, every failure mode, the assigned tier, and the justification. This table is not a risk register for the legal team. It is a product definition artifact that informs architecture, evaluation, and launch decisions. The table forces you to enumerate every output your system produces and every way that output can be wrong. It surfaces assumptions about what users will do with the output and whether they will review it before acting. It reveals which tasks need human-in-the-loop workflows and which can be fully automated.

The justification column is the most important. You must write a sentence or two explaining why you assigned the tier you did. Do not write "important task" or "high stakes." Write the specific harm that would occur if the output is maximally wrong. For a legal contract summarization tool, do not write "could cause legal issues." Write "if the summary omits a liability clause and the client signs the contract without lawyer review, the client could be exposed to uninsurable financial risk." The specificity forces you to think through the failure mode and the downstream consequence. If you cannot write a concrete harm, you have not understood the task well enough to tier it.

The failure impact matrix must be reviewed by stakeholders outside the core product team. Legal needs to confirm your Tier 3 classifications. Finance needs to confirm your Tier 2 cost estimates. Operations needs to confirm your workflow assumptions. The head of customer success needs to confirm whether users will review outputs or trust them blindly. This is not consensus-building. This is validation. You are checking whether your understanding of failure impact matches the reality of how your system will be used. If the legal team says a task you classified as Tier 1 is actually Tier 3, you do not negotiate. You update the tier and the evaluation plan.

The matrix should also include probability estimates where possible. How often do you expect each failure mode to occur in production? A Tier 3 failure that happens once per million requests has different risk exposure than one that happens once per thousand requests. You combine severity and frequency to calculate risk exposure and prioritize mitigation efforts. A high-severity, low-frequency failure might need robust detection and response systems but not necessarily higher accuracy thresholds. A moderate-severity, high-frequency failure might need threshold tuning and active learning to reduce the occurrence rate. Frequency without severity is noise. Severity without frequency is hypothetical. You need both to make rational decisions about where to invest.

## How Failure Tiers Drive Resource Allocation

Failure tiers determine how you allocate evaluation time, engineering effort, and product scope. Tier 3 tasks consume ten times the evaluation resources of Tier 0 tasks. If you have three Tier 3 tasks and ten Tier 0 tasks, you should spend more total effort evaluating the three Tier 3 tasks. This is not intuitive. Teams naturally spread effort evenly across features or in proportion to model complexity. But effort should be proportional to failure impact. A simple rule-based classifier for a Tier 3 task deserves more evaluation rigor than a state-of-the-art LLM for a Tier 0 task.

Tier also determines launch strategy. Tier 0 tasks can ship in beta with a disclaimer and a feedback button. Tier 1 tasks need a structured pilot with real users and monitoring. Tier 2 tasks need a controlled rollout with rollback capability and incident response plans. Tier 3 tasks need regulatory review, external audits, and staged launches with human oversight at every step. If your roadmap includes two Tier 3 tasks and you have six months to launch, you probably have time for one of them, not both. The tier forces you to prioritize or descope.

Some teams try to avoid Tier 3 tasks by adding disclaimers. "This is not medical advice." "This is not legal advice." "For informational purposes only." Disclaimers do not change the tier. They are a legal mitigation, not a product mitigation. If users treat your output as authoritative and act on it, the failure tier is determined by the harm that occurs, not by what your terms of service say. A medical symptom checker with a disclaimer is still Tier 3 if users follow its advice without seeing a doctor. You cannot disclaim your way out of a Tier 3 evaluation standard. You can only build to the standard or not ship the feature.

The resource allocation extends beyond evaluation into ongoing operations. Tier 3 systems require 24/7 monitoring, on-call engineering support, and incident response playbooks. Tier 2 systems need business-hours monitoring and escalation procedures. Tier 1 systems need periodic review and user feedback channels. Tier 0 systems can rely on user reports and batch processing of issues. The operational cost of running a Tier 3 system in production is five to ten times the cost of running a Tier 0 system at the same scale. If you have not budgeted for this operational overhead, you will either under-resource the system and create risk, or you will pull resources from other projects and create delivery problems.

## Failure Impact Analysis as Competitive Advantage

The companies that do failure impact analysis well ship better products faster than competitors who skip it. This seems counterintuitive. Analysis takes time. How does more analysis lead to faster shipping? The answer is that failure impact analysis prevents you from over-building low-risk features and under-building high-risk features. Most teams apply uniform rigor to everything, which means they waste effort on tasks that do not need it and under-invest in tasks that do.

A consumer social app that tiers its features correctly might discover that its AI-generated caption suggestions are Tier 0, its content moderation system is Tier 2, and its age verification system is Tier 3. The caption feature can ship with minimal evaluation, maybe a few hundred manual reviews and a feedback button. The moderation system needs a structured evaluation set, precision-recall targets per violation type, and a human review queue for edge cases. The age verification system needs formal testing, external audit, and legal review. If the team applies Tier 3 rigor to captions, they waste months. If they apply Tier 0 rigor to age verification, they create legal liability. Getting the tiers right means shipping the low-risk features fast and investing appropriately in the high-risk features.

Failure impact analysis also clarifies product strategy. When you tier your roadmap, you often discover that you have too many Tier 2 and Tier 3 features planned and not enough Tier 0 and Tier 1 features. High-tier features are expensive to build and expensive to maintain. A roadmap with ten Tier 3 features is not ambitious. It is infeasible. You either need to descope, extend timelines, or rethink the product to replace some high-tier features with lower-tier alternatives that deliver similar user value with less risk. The analysis forces this conversation early when you can still adjust the plan.

## Learning from Production Failures

Failure impact analysis is not a one-time exercise during framing. It is a living framework that evolves as you learn from production. When a failure occurs in production, you do not just fix the bug. You revisit the failure impact matrix and ask whether you tiered that task correctly. If a Tier 1 failure turned out to cause Tier 2 harm, you update the tier and the evaluation standards. If a Tier 3 feature has been running for a year with zero high-impact failures, you have evidence that your mitigations work, but you do not downgrade the tier. Severity is determined by what could happen, not by what has happened so far.

Production failures also reveal cascade effects you did not anticipate. You thought component A was Tier 0, but when it failed, it caused component B to produce Tier 2 outputs. You update the matrix to reflect that component A inherits Tier 2 because of the cascade. Over time, your failure impact matrix becomes more accurate and more specific. It captures the actual risk profile of your system based on evidence, not just theory.

The matrix also becomes an organizational memory. When someone proposes a new feature, you can look at similar features in the matrix and ask what tier the new feature should be. If the new feature is similar to an existing Tier 3 feature, you know what evaluation rigor is required before anyone writes code. The matrix prevents repeated mistakes where different teams building similar features make the same tiering errors because they did not learn from each other.

## Communicating Failure Impact to Non-Technical Stakeholders

Executives and non-technical stakeholders often struggle with failure tiers because the framework is probabilistic and scenario-based. They want simple answers. Is this feature safe to ship? The answer is always it depends on what you mean by safe and what you consider acceptable risk. Failure impact analysis gives you the tools to make that answer concrete.

When you present the failure impact matrix to executives, you do not show them the full table with every task and every failure mode. You summarize the distribution. We have twelve Tier 0 features, eight Tier 1 features, three Tier 2 features, and one Tier 3 feature. The Tier 3 feature requires external audit and six months of staged rollout. The Tier 2 features require domain expert evaluation and monitoring infrastructure. The Tier 1 features require structured testing but no external review. The Tier 0 features can ship with lightweight validation. Then you show them the timeline and resource implications. If we want to ship all of these in six months, we need to cut two of the Tier 2 features or extend the timeline by three months.

You also show them example failure scenarios for the highest-tier features. For the Tier 3 feature, here is what happens if the system is wrong. For the Tier 2 features, here are the financial and reputational consequences. You make it concrete. Executives can evaluate concrete harms in ways they cannot evaluate abstract risk scores. When you say Tier 3, they hear jargon. When you say if this feature is wrong, a patient could receive the wrong medication dosage and be hospitalized, they hear risk they can assess.

The communication should also clarify what you are not claiming. Failure impact analysis does not guarantee that failures will not occur. It ensures that you have understood the consequences of failure, allocated resources proportional to those consequences, and implemented appropriate mitigations. You are managing risk, not eliminating it. Some stakeholders expect AI projects to have zero risk of failure. That expectation is unrealistic. Your job is to show them the risks, quantify the mitigations, and let them decide whether the residual risk is acceptable.

## The Failure Impact Matrix as a Living Document

The failure impact matrix is not a static artifact you create during framing and never touch again. It evolves throughout the project lifecycle. During framing, the matrix is based on analysis and assumptions. During development, you refine the matrix as you learn more about how the system behaves. During beta testing, you update the matrix based on real user behavior. During production, you continuously revise the matrix based on incident data and monitoring.

Each update to the matrix should trigger a review of dependent decisions. If you uprank a feature from Tier 1 to Tier 2, you need to revisit the evaluation plan, the monitoring strategy, and potentially the launch timeline. If you downrank a feature from Tier 2 to Tier 1 based on new information, you might be able to reallocate resources to higher-priority risks. The matrix is the foundation. When the foundation shifts, you check whether the structure built on top of it is still sound.

The matrix also serves as documentation for audits and reviews. When a regulator asks how you assess the risk of your AI system, you show them the failure impact matrix. When a customer asks what happens if your system is wrong, you show them the relevant rows of the matrix and the mitigations you have in place. When an executive asks why a feature is taking so long, you show them the tier and the associated evaluation requirements. The matrix is not just an internal planning tool. It is a communication artifact that demonstrates rigor and accountability.

Version control for the matrix follows the same principles as version control for success criteria. Every change is dated and attributed. The change log explains why the tier changed, what new information triggered the change, and what decisions were updated as a result. When someone looks at the matrix six months from now, they can see not just the current state but the evolution. They can understand why a feature that launched as Tier 1 is now operated as Tier 2, or why a feature initially scoped as Tier 3 was descoped entirely because the risk was unacceptable.

## Building Failure Analysis Into Your Workflow

Failure impact analysis should not be a separate workstream that happens in isolation from other framing activities. It should be integrated into every conversation about what to build and how to build it. When Product proposes a new feature, the first question Engineering asks is what tier is this. When Engineering proposes a new model architecture, the first question Product asks is does this change the failure tier of any tasks. The tier becomes part of the shared vocabulary that both teams use to communicate about risk and scope.

Integration also means that failure impact analysis informs task decomposition. When you discover that a single feature contains both Tier 0 and Tier 3 components, you decompose it into separate tasks with different evaluation standards. You do not treat the whole feature as Tier 3 and over-evaluate the low-risk parts, and you do not treat it as Tier 0 and under-evaluate the high-risk parts. You split it and tier each component appropriately. This decomposition might change your product architecture, your API design, or your user interface. Tier differences often imply architectural boundaries.

The workflow integration extends to sprint planning and resource allocation. When you plan a sprint, you do not just assign story points based on effort. You assign evaluation budget based on tier. A Tier 3 task might require two days of development and three days of evaluation. A Tier 0 task might require two days of development and two hours of evaluation. If you plan only for development time, your sprint will overflow when evaluation work starts. Tier-aware planning prevents this surprise.

## When Failure Analysis Reveals Infeasibility

Sometimes failure impact analysis reveals that a feature is not worth building at all. The feature would be Tier 3, which means it requires external audit, months of evaluation, ongoing monitoring infrastructure, incident response plans, and legal review. The business value of the feature does not justify that investment. Or the feature would be Tier 2, but you already have three Tier 2 features in progress and your team does not have capacity for a fourth. The analysis forces a prioritization conversation that might not have happened otherwise.

This is a feature of the framework, not a bug. Failure impact analysis is a forcing function for honest resource allocation. Many teams commit to roadmaps that are infeasible given their evaluation capacity. They assume they can evaluate everything quickly or that some tasks will not need much evaluation. The tier framework makes the cost explicit. When Product sees that their ten planned features include two Tier 3 tasks and five Tier 2 tasks, they can do the math. If Tier 3 tasks take four times as long as Tier 1 tasks to evaluate, and Tier 2 tasks take twice as long, the roadmap requires more evaluation capacity than the team has. Product can cut scope, extend timelines, or hire more evaluators. What they cannot do is pretend the evaluation work will not take time.

Infeasibility discoveries early in framing are cheap. You cancel a feature before anyone writes code. Infeasibility discoveries during launch review are expensive. You have built the feature, and now you cannot ship it because evaluation is incomplete. The earlier the failure analysis happens, the more options you have. This is why it belongs in framing, not in QA.

## Failure Impact Analysis for Multi-Tenant Systems

Multi-tenant AI systems serve different customers with different risk profiles using the same underlying models and infrastructure. A document classification system might serve a law firm where misclassification is Tier 2 due to procedural consequences, and also serve a marketing agency where misclassification is Tier 1 because it just means a document goes to the wrong folder. The same task, the same model, but different tiers depending on the customer.

Multi-tenant systems require tenant-specific failure impact matrices. You cannot assign a single tier to a task and call it done. You must tier the task per tenant or per tenant category. Enterprise legal customers get Tier 2 evaluation standards. Small business marketing customers get Tier 1 standards. This does not mean you run different models for different tenants, though you might. It means you apply different quality thresholds, different monitoring, and different human-in-the-loop workflows based on tenant tier.

The operational complexity is significant. You need infrastructure that can route outputs through different evaluation pipelines based on tenant tier. You need SLAs that reflect tier differences. You need contracts that specify which tier a customer is purchasing. Some customers will want to purchase higher tiers for higher quality and higher cost. Others will accept lower tiers for lower cost. The tier becomes a product differentiation axis, not just an internal risk management tool.

Multi-tenant tiering also creates strategic questions. Do you allow low-tier customers to use features that would be high-tier if you evaluated them properly? Do you limit certain features to high-tier customers because you cannot afford to evaluate them at lower tiers? Do you disclose tier differences to customers or keep them internal? These questions have no universal answers. They depend on your market, your margin, and your risk tolerance. The failure impact framework gives you the information to make these decisions rationally.

## Cross-Functional Ownership of Failure Analysis

Failure impact analysis is not an Engineering deliverable or a Product deliverable. It is a cross-functional deliverable that requires input from Engineering, Product, Legal, Domain Experts, Operations, and sometimes Finance. Engineering understands technical failure modes and mitigation complexity. Product understands user workflows and business impact. Legal understands regulatory and liability exposure. Domain Experts understand professional standards and reputational risk. Operations understands downstream process dependencies. Finance understands cost of failures in dollar terms.

No single role has all the information needed to tier tasks correctly. This is why the failure impact matrix must be reviewed in the success criteria review meeting with all stakeholders present. When Engineering says a task is Tier 1 and Legal says it is Tier 3, that disagreement must be surfaced and resolved. Usually the disagreement comes from different understandings of how the output will be used. Engineering thinks users will review outputs before acting. Legal knows users will trust outputs blindly. Once the usage pattern is clarified, the tier becomes obvious.

Cross-functional ownership also means cross-functional accountability. When a production failure occurs and the tier was wrong, you do not blame the person who wrote the matrix. You ask why the review process did not catch the error. Was a stakeholder missing from the review? Did someone withhold information about usage patterns? Did assumptions change after framing and no one updated the matrix? The failure is a process failure, not an individual failure, and you fix the process.

## The Meta-Question: When Do You Tier?

Not every AI experiment needs a failure impact matrix. If you are prototyping to explore feasibility, you do not tier yet. If you are building a demo for internal stakeholders, you do not tier yet. You tier when you have decided that the project is likely to reach production and you need to plan evaluation and resource allocation. For most teams, this happens during the framing phase after the task definition is clear but before detailed engineering design begins.

The signal to trigger tiering is when someone asks how much evaluation this will need or when can we launch. Both questions depend on tier. If you cannot answer them without making up numbers, it is time to do the failure impact analysis. If you find yourself saying we will figure out evaluation as we go, you are skipping tiering and accepting the risk of building the wrong thing or under-evaluating the right thing.

Some organizations mandate tiering for all AI projects above a certain scope threshold. Any project with more than one month of engineering time must have a failure impact matrix reviewed by Legal and a domain expert. This mandate prevents teams from skipping the analysis because they are busy or because they think their project is special. The mandate creates consistency and ensures that the analysis happens early enough to influence architecture and planning.

Having established what success looks like and what failure costs, the next challenge is specifying exactly what goes into and comes out of your system — the input/output specification.
