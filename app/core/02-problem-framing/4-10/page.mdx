# 4.10 — Failure Impact Analysis: What Breaks If Wrong, by Tier

In late 2024, a Series B healthcare startup launched an AI assistant that helped medical practices draft patient communication letters. The system generated discharge instructions, appointment reminders, and follow-up care guidelines. The team had excellent accuracy metrics: 94% of letters required no edits, and doctors loved the time savings. But they hadn't mapped failure impact by task type. When the system started generating prescription reminder letters, it occasionally confused dosage instructions, swapping "take one tablet twice daily" with "take two tablets once daily." The error rate was low, under 2%, well within their acceptable threshold for appointment reminders. Three patients were hospitalized. The startup shut down four months later, not from lawsuits but from complete loss of trust. Their investors had funded a productivity tool, not a liability machine.

The root cause wasn't the error rate. It was the failure to recognize that different outputs in the same system carried wildly different failure costs. Appointment reminder errors annoyed patients. Prescription instruction errors harmed them. The team had treated all text generation as equivalent risk because all tasks used the same underlying model and achieved similar accuracy scores. They never built the analysis that would have shown them which outputs required human verification and which could ship automatically. They optimized for average performance when they needed to optimize for worst-case impact.

## Why Failure Impact Analysis Comes Before Architecture

You cannot make sound architecture decisions without knowing what breaks if your system is wrong. Failure impact analysis is not a post-launch audit or a risk management exercise for the legal team. It is a product definition activity that happens during problem framing, before you write a line of code. The reason is simple: the cost of being wrong determines how you build the system, how you evaluate it, what latency you can tolerate, and whether you need a human in the loop. A system that generates social media captions can fail cheaply. A system that generates chemotherapy dosing recommendations cannot. These are not the same product with different domains. They are fundamentally different engineering problems.

Most teams skip this analysis because they assume they understand the stakes. Healthcare feels high-risk, marketing feels low-risk, and that intuition guides their diligence. But risk is not a property of the domain. It is a property of the specific task and its context. A healthcare chatbot that answers "what are your hours" has lower failure impact than a financial advisor chatbot that answers "should I withdraw my 401k early." The domain does not determine the tier. The consequence of being wrong determines the tier. You need a systematic framework to classify every task your system will perform, or you will misallocate your evaluation budget and ship the wrong things automatically.

## The Failure Tier Framework

**Failure tiers** are a classification system that maps every task in your AI system to the severity of harm if the output is wrong. There are four tiers, and they are not about how often failures happen. They are about what happens when they do. Tier 0 failures are cosmetic or annoying. The user notices something is off, but nothing breaks and no one loses anything of value. Tier 1 failures disrupt workflow. The user has to stop what they are doing and fix the problem, costing time and focus but not money or data. Tier 2 failures cause financial loss, data corruption, or damage to reputation. Someone loses something they cannot easily recover. Tier 3 failures risk physical harm, legal liability, or regulatory violation. Someone could be injured, sued, or put out of business.

The tier determines your evaluation strategy. Tier 0 tasks can launch with lightweight manual review and user feedback loops. Tier 1 tasks need structured evaluation sets and regression testing. Tier 2 tasks require domain expert review, adversarial testing, and monitoring dashboards. Tier 3 tasks demand formal verification, external audits, human-in-the-loop workflows, and incident response plans. The evaluation investment scales exponentially. A Tier 3 task needs ten times the evaluation rigor of a Tier 0 task, not because you care more, but because the cost of failure is ten times higher. Most teams evaluate everything at Tier 1 and wonder why their high-stakes features keep causing problems.

Tiers are not intrinsic to the AI task. They are determined by how the output is used. Text summarization is not inherently Tier 0 or Tier 3. Summarizing a podcast for show notes is Tier 0. Summarizing a legal contract for a business decision is Tier 2. Summarizing a patient chart for a surgical planning meeting is Tier 3. The same model, the same accuracy, the same prompt structure, but the failure impact differs by two full tiers. This is why you cannot tier your system by feature. You must tier every distinct use case, every output that a human or downstream system will act on.

## Tier 0: Cosmetic and Annoying

Tier 0 failures break nothing. The user sees an output that is wrong, awkward, or nonsensical, and they either ignore it or regenerate it. No workflow is blocked. No data is lost. The stakes are user experience and polish, not functionality. Examples include AI-generated image captions for social media, autocomplete suggestions in a note-taking app, creative writing prompts, and color palette recommendations. If the system hallucinates a fact in a marketing blog post and a human editor catches it before publishing, that is Tier 0. The failure was contained within the editorial workflow, and the cost was a few seconds of the editor's attention.

Tier 0 does not mean you do not care about quality. It means failure does not cascade. A bad suggestion does not break the next step. The user is aware they are reviewing AI output, and they treat it as a draft. You still want high accuracy because poor quality erodes trust and adoption, but you do not need formal evaluation infrastructure. You can ship with qualitative review, user feedback, and A/B testing. If your precision drops from 90% to 80%, users might complain, but they will not stop using the product or lose money.

The boundary between Tier 0 and Tier 1 is whether the user has to take corrective action that disrupts their task. If the AI generates a bad social media caption and the user just types a new one, that is Tier 0. If the AI mislabels an email as spam and the user has to dig through their spam folder to find an important message, that is Tier 1. The user's workflow was interrupted. The distinction matters because Tier 1 failures require you to measure and minimize false positives in ways that Tier 0 failures do not.

## Tier 1: Workflow Disruption

Tier 1 failures force the user to stop and fix the problem. The AI does something wrong, and the user has to intervene to get back on track. No one loses money or data, but time and focus are wasted. Examples include a code autocomplete that inserts the wrong function name and the developer has to delete it, a document classifier that misfires and the user has to manually refile the document, a calendar assistant that schedules a meeting at the wrong time and the user has to reschedule, or a search system that fails to return the right result and the user has to try a different query. The cost is friction, and friction compounds. A single Tier 1 failure is annoying. A dozen per day makes the tool unusable.

Tier 1 tasks require structured evaluation because you need to measure false positive and false negative rates and balance them against user tolerance. A spam filter that blocks real email is worse than a spam filter that lets spam through, because the user expects to delete spam but does not expect to lose real mail. You need to understand the asymmetry in failure cost, not just the overall accuracy. Tier 1 evaluation means building a labeled test set, running regression tests on every model update, and monitoring user correction rates in production. You cannot rely on vibes and manual spot checks.

The failure mode determines the tier, not the feature. A chatbot that answers customer support questions might be Tier 0 if it is clearly labeled as experimental and the user can easily escalate to a human. The same chatbot is Tier 1 if it is the primary support interface and wrong answers force the user to retry or give up. Context matters. You do not tier the model. You tier the deployment.

## Tier 2: Financial or Data Loss

Tier 2 failures cost the user something they cannot easily recover: money, proprietary data, customer relationships, or business reputation. The AI makes a decision or generates an output that, when acted upon, causes tangible harm. Examples include a fraud detection system that falsely blocks a high-value customer transaction, losing the sale and angering the customer. An expense categorization tool that misclassifies business expenses, leading to incorrect tax filings. A resume screening system that filters out qualified candidates, forcing a company to rehire and delay projects. A content moderation system that false-positives and removes legitimate user content, driving users to competitors. The harm is not immediate physical danger, but the financial or reputational cost is significant.

Tier 2 tasks require domain expert evaluation, adversarial testing, and monitoring dashboards. You cannot evaluate these tasks with crowdsourced labels or junior reviewers. You need people who understand the business impact of each failure mode. If you are building a contract analysis tool that flags risky clauses, your evaluators must be lawyers or contract specialists who know what actually constitutes risk. If you are building dynamic pricing, your evaluators must understand margin, elasticity, and competitive positioning. The evaluation set must include edge cases, adversarial examples, and historical failure modes, not just random samples from production.

Tier 2 systems require real-time monitoring because failure impact can be delayed. A miscategorized expense might not surface until tax season, months after the error. You need instrumentation that tracks downstream outcomes, not just model predictions. Did the blocked transaction reduce revenue? Did the false positive in fraud detection increase churn? You are measuring business metrics, not just accuracy. This is the tier where product and engineering must collaborate with finance, legal, and operations to define what failure looks like and how much of it you can tolerate.

## Tier 3: Safety, Legal, and Regulatory Risk

Tier 3 failures risk physical harm, legal liability, or regulatory violation. Someone could be injured, sued, fired, or imprisoned because your AI was wrong. Examples include medical diagnosis or treatment recommendations, legal advice that exposes a client to liability, autonomous vehicle decision-making, content moderation that fails to remove illegal content, employee performance evaluations that violate employment law, and financial advice that violates fiduciary duty. Tier 3 is not about reputation or lost revenue. It is about harm that cannot be undone and liability that cannot be insured away.

Tier 3 tasks require formal verification, external audits, human-in-the-loop workflows, and incident response plans. You do not ship a Tier 3 task with a single engineer's code review. You need multiple domain experts to review the evaluation plan, the test results, and the production deployment. You need a paper trail that shows you understood the risk and took every reasonable step to mitigate it. You need contracts with customers that clarify liability. You need insurance. You need a legal team to review your disclaimers and terms of service. This is not paranoia. This is the minimum standard in industries that have learned what unmitigated AI risk looks like.

Tier 3 systems must default to human override. If your AI flags a transaction as fraudulent, a human reviews it before blocking. If your AI drafts a legal motion, a lawyer reviews it before filing. If your AI recommends a medical treatment, a doctor reviews it before prescribing. The AI can surface the decision, but it cannot execute the decision. The human is not rubber-stamping. They are applying judgment the AI cannot replicate. The exception is if you have years of evidence that your AI is safer than the human baseline, which almost no one has. Until then, the human stays in the loop.

The EU AI Act, fully enforceable in 2026, explicitly categorizes AI systems by risk and mandates impact assessments for high-risk applications. Tier 3 maps directly to the EU's high-risk category, which includes systems used in employment, education, law enforcement, critical infrastructure, and health. If you are selling into Europe, failure impact analysis is not optional. It is a compliance requirement. You must document what your system does, what breaks if it is wrong, and how you are mitigating that risk. The regulation formalizes what good engineering teams were already doing. It just makes it legally mandatory.

## Mapping Failure Tiers to Your System

Every output your AI system produces must be mapped to a failure tier. Not every feature, not every model, but every distinct task where a user or downstream system acts on the output. Start by listing every decision your system makes or every artifact it generates. Then for each one, ask: what happens if this output is completely wrong? Not slightly off, not 95% correct, but maximally wrong in the worst plausible way. What is the concrete harm?

Walk through the user workflow. If the AI classifies an email as spam and the user never sees it, does the user lose an important opportunity? If so, that is Tier 2. If the AI suggests the wrong meeting time and the user has to reschedule, does that just waste five minutes, or does it cause them to miss a deadline? If the former, Tier 1. If the latter, Tier 2. If the AI generates a product description with a factually incorrect safety warning, does that expose the company to regulatory action? If so, Tier 3. Do not tier based on how unlikely the failure is. Tier based on the impact when it happens.

Different failure modes for the same task can belong to different tiers. A resume screening tool might have three failure modes: it rejects a qualified candidate (Tier 2, you lose the hire), it advances an unqualified candidate (Tier 1, a recruiter wastes time interviewing them), and it crashes and does not process any resumes (Tier 1, the recruiter falls back to manual review). The highest tier determines your evaluation standard. You evaluate the whole task as Tier 2 because the worst failure is Tier 2. You do not get to average the tiers.

Document the failure tier for every task in your framing document, alongside the success criteria. Write it as a table: task name, output, failure mode, tier, justification. This table becomes the foundation for your evaluation plan. Tier 3 tasks get formal test plans, external review, and human-in-the-loop workflows. Tier 2 tasks get domain expert evaluation and production monitoring. Tier 1 tasks get regression tests and user feedback loops. Tier 0 tasks get lightweight qualitative review. You allocate evaluation resources in proportion to failure impact, not in proportion to model complexity or user excitement.

## The Cascade Effect: Tier 0 Failures That Cause Tier 3 Harm

A Tier 0 failure in one component can cause a Tier 3 failure in a downstream component. This is the cascade effect, and it is the reason you cannot tier components in isolation. You must tier the system as a whole. A simple example: an AI transcription service that converts audio to text is Tier 0 if the user reviews the transcript and corrects errors. But if that transcript feeds into a medical record system without human review, and a doctor makes a treatment decision based on the incorrect transcript, the transcription task is now Tier 3. The task did not change. The system integration changed the failure impact.

Cascade analysis requires mapping data flow through your system. Start with the user-facing outputs and work backward. For each output, identify every AI component that contributed to it. Then ask: if component A is wrong, does that make component B wrong in a way that increases the failure tier? If yes, component A inherits the higher tier. A spell-checker in a text editor is Tier 0. A spell-checker in a legal document management system that auto-files motions is Tier 3 because a misspelled party name could invalidate a filing. The spell-checker did not get more sophisticated. The consequences got more severe.

The cascade effect is why you cannot treat evaluation as a per-model activity. You must evaluate the composed system, not just the individual models. Integration testing for AI systems is not about checking that APIs return valid JSON. It is about verifying that error modes in one component do not propagate silently to downstream components. You need end-to-end test cases that inject realistic errors at every stage and verify that the system either corrects them or surfaces them to the user. If your system has ten components and you evaluate each one in isolation at 95% accuracy, your end-to-end accuracy could be 60% if errors compound. You must measure the cascade.

## Failure Impact Across Industries

Healthcare is the obvious Tier 3 domain, but not all healthcare AI is Tier 3. A hospital scheduling assistant that books appointments is Tier 1. A symptom checker that recommends over-the-counter remedies is Tier 1 or Tier 2 depending on whether it disclaims medical advice. A clinical decision support tool that recommends treatments is Tier 3. A medical coding assistant that assigns billing codes is Tier 2 because incorrect coding can trigger audits and fines. The task determines the tier, and healthcare spans all four tiers. You cannot blanket-classify the domain.

Finance is similar. A robo-advisor that recommends portfolio allocation to retail investors is Tier 3 because it creates fiduciary liability. A transaction categorization tool for personal budgeting is Tier 1. A fraud detection system for a payment processor is Tier 2 because false positives lose revenue and false negatives lose money to fraud. A chatbot that answers "when is my payment due" is Tier 0 if the user can easily verify the answer elsewhere. The same infrastructure, the same models, but the failure tiers differ by two or three levels depending on what the output controls.

Content moderation is Tier 2 or Tier 3 depending on the platform and the regulatory environment. A moderation system that removes spam comments is Tier 1. A system that removes alleged copyright violations is Tier 2 because false positives could violate fair use and trigger legal disputes. A system that removes illegal content like child exploitation material is Tier 3 because failure to remove it exposes the platform to criminal liability. The EU Digital Services Act in 2026 makes this explicit: large platforms must have robust moderation systems and can be fined for systemic failures. The failure tier is not about what you want to prioritize. It is about what the law requires.

E-commerce product recommendations are usually Tier 0, but dynamic pricing can be Tier 2 if incorrect prices lose margin or violate pricing agreements with partners. A search ranking algorithm is Tier 1 if it wastes user time. The same algorithm is Tier 2 if it systematically buries a category of products and causes a revenue shortfall. A size recommendation tool is Tier 1 if it leads to returns. The same tool is Tier 2 if it causes a wave of returns that damages the brand. The business context determines the tier. You cannot tier the AI in the abstract.

## Documenting Failure Impact in the Framing Document

Your framing document must include a failure impact matrix: a table that lists every task, every failure mode, the assigned tier, and the justification. This table is not a risk register for the legal team. It is a product definition artifact that informs architecture, evaluation, and launch decisions. The table forces you to enumerate every output your system produces and every way that output can be wrong. It surfaces assumptions about what users will do with the output and whether they will review it before acting. It reveals which tasks need human-in-the-loop workflows and which can be fully automated.

The justification column is the most important. You must write a sentence or two explaining why you assigned the tier you did. Do not write "important task" or "high stakes." Write the specific harm that would occur if the output is maximally wrong. For a legal contract summarization tool, do not write "could cause legal issues." Write "if the summary omits a liability clause and the client signs the contract without lawyer review, the client could be exposed to uninsurable financial risk." The specificity forces you to think through the failure mode and the downstream consequence. If you cannot write a concrete harm, you have not understood the task well enough to tier it.

The failure impact matrix must be reviewed by stakeholders outside the core product team. Legal needs to confirm your Tier 3 classifications. Finance needs to confirm your Tier 2 cost estimates. Operations needs to confirm your workflow assumptions. The head of customer success needs to confirm whether users will review outputs or trust them blindly. This is not consensus-building. This is validation. You are checking whether your understanding of failure impact matches the reality of how your system will be used. If the legal team says a task you classified as Tier 1 is actually Tier 3, you do not negotiate. You update the tier and the evaluation plan.

## How Failure Tiers Drive Resource Allocation

Failure tiers determine how you allocate evaluation time, engineering effort, and product scope. Tier 3 tasks consume ten times the evaluation resources of Tier 0 tasks. If you have three Tier 3 tasks and ten Tier 0 tasks, you should spend more total effort evaluating the three Tier 3 tasks. This is not intuitive. Teams naturally spread effort evenly across features or in proportion to model complexity. But effort should be proportional to failure impact. A simple rule-based classifier for a Tier 3 task deserves more evaluation rigor than a state-of-the-art LLM for a Tier 0 task.

Tier also determines launch strategy. Tier 0 tasks can ship in beta with a disclaimer and a feedback button. Tier 1 tasks need a structured pilot with real users and monitoring. Tier 2 tasks need a controlled rollout with rollback capability and incident response plans. Tier 3 tasks need regulatory review, external audits, and staged launches with human oversight at every step. If your roadmap includes two Tier 3 tasks and you have six months to launch, you probably have time for one of them, not both. The tier forces you to prioritize or descope.

Some teams try to avoid Tier 3 tasks by adding disclaimers. "This is not medical advice." "This is not legal advice." "For informational purposes only." Disclaimers do not change the tier. They are a legal mitigation, not a product mitigation. If users treat your output as authoritative and act on it, the failure tier is determined by the harm that occurs, not by what your terms of service say. A medical symptom checker with a disclaimer is still Tier 3 if users follow its advice without seeing a doctor. You cannot disclaim your way out of a Tier 3 evaluation standard. You can only build to the standard or not ship the feature.

Having established what success looks like and what failure costs, the next challenge is specifying exactly what goes into and comes out of your system — the input/output specification.
