# 5.7 — I/O Specs for Multi-Step and Agentic Systems

In early 2025, a fintech startup launched an AI agent to help small business owners manage their cash flow. The product worked beautifully in demos: the user would ask "How much can I spend this month?" and the agent would check their bank balance, analyze upcoming invoices, review payment schedules, and return a thoughtful answer. But in production, things fell apart. The agent would sometimes return stale balance data. Other times it would call the invoice analysis tool twice, doubling the processing cost. Occasionally it would generate a plan but fail to execute it, leaving users confused. The team spent weeks debugging. The root cause was not the models, not the prompts, not the tools themselves — it was the I/O contracts between the steps. The planner output a plan in one format, but the executor expected a different format. The tool selector returned confidence scores, but the interpreter ignored them. The state manager stored context in one schema, but the next step read it assuming a different schema. Every step worked in isolation, but the interfaces between them were misaligned.

This is the defining challenge of agentic systems: **the I/O contract gets complex fast**. In a single-step system, you have one input and one output. In a multi-step agentic system, you have dozens of intermediate I/O contracts, each one a potential point of failure. If you do not specify these contracts explicitly, you will debug interface mismatches forever.

This subchapter covers how to specify I/O contracts for multi-step and agentic systems, how to manage state across turns, how to define tool call specifications, and why the "contract stack" is critical for debugging.

## Why Multi-Step Systems Have a Contract Stack

A traditional API has a simple I/O contract: the user sends a request, the system sends a response. Done. An agentic system has a **contract stack**: a hierarchy of I/O contracts layered on top of each other. At the top is the system-level contract: what the user sees. Below that are the inter-step contracts: what each component of the system sends to the next component.

Here is what the contract stack looks like for a typical agentic system:

**Layer 1: User-facing I/O.** This is the top-level contract. The user sends a natural language query. The system returns a natural language answer, possibly with structured data, citations, and confidence scores. This is the only layer most users ever see.

**Layer 2: Planner I/O.** The planner receives the user query plus context (conversation history, user profile, available tools). It outputs a plan: a sequence of steps the agent should execute. The plan might be a list of tool calls, a reasoning chain, or a decision tree. The planner's output must be consumable by the next step.

**Layer 3: Tool Selector I/O.** The tool selector receives the plan (or the current step in the plan) and decides which tool to call. It outputs a tool choice: the name of the tool, the parameters to pass, and optionally a confidence score. The tool selector's output must match the tool executor's input schema.

**Layer 4: Tool Executor I/O.** The tool executor receives the tool choice and calls the tool. It outputs a tool result: the data returned by the tool, an error message if the tool failed, and metadata like latency and cost. The tool executor's output must be consumable by the interpreter.

**Layer 5: Interpreter I/O.** The interpreter receives the tool result and interprets it in the context of the user's original query. It outputs an understanding: what this result means, whether the query is now fully answered or more steps are needed, and what to do next. The interpreter's output feeds back into the planner or directly into the response generator.

**Layer 6: Response Generator I/O.** The response generator receives the final understanding and generates the user-facing response. It outputs the text, structured data, citations, and confidence scores that go back to the user.

Each of these layers is a separate I/O contract. If any of these contracts is misspecified or misaligned, the entire system breaks. The user never sees the internal failures — they just see a system that gives wrong answers, hangs, or errors out.

## Specifying Intermediate Outputs Between Steps

The first challenge in multi-step I/O specs is: **what does each step output?** This is harder than it sounds because intermediate outputs are often unstructured or semi-structured. The planner might output a plan as a natural language list, a JSON array, or a custom DSL. The tool selector might output a tool name as a string or a structured object with parameters. If you do not specify this exactly, different steps will assume different formats.

Here is how to specify intermediate outputs:

**Define the schema for each step's output.** Just like you would for the final user-facing output, define the schema for every intermediate output. If the planner outputs a plan, specify what a plan looks like: Is it a list of strings? A list of objects with step numbers and descriptions? A nested structure with conditional branches? If the tool selector outputs a tool choice, specify the fields: tool name, parameters, confidence score, reasoning.

**Document the contract explicitly.** Do not assume the next developer or the next model will infer the schema from examples. Write it down. If the planner outputs a JSON array, specify the exact JSON schema. If the tool selector outputs an enum, specify the valid values.

**Enforce the schema programmatically.** Use schema validation libraries to check that each step's output matches the expected schema before passing it to the next step. If the planner outputs a plan that does not match the schema, fail fast with a clear error message. Do not let misformatted data propagate downstream.

**Version the schemas.** Just like external APIs, internal step schemas evolve. If you change the planner's output schema, you need to update every step that consumes it. Version the schemas and use compatibility checks to prevent breaking changes from silently corrupting the pipeline.

## Inter-Step Contracts: What Format Does Step A Output for Step B to Consume?

The second challenge is: **how do steps communicate?** You have two options: **strict contracts** or **loose coupling**.

**Strict contracts** mean every step outputs exactly the schema the next step expects. The planner outputs a plan in the exact format the tool selector expects. The tool executor outputs results in the exact format the interpreter expects. This is the safest approach because it prevents format mismatches, but it is also the most brittle. If you change one step, you have to update all downstream steps.

**Loose coupling** means steps output in a canonical format, and each step is responsible for transforming the input it receives into the format it needs. The planner outputs a plan in a standard schema, and the tool selector has an adapter that transforms that schema into its internal representation. This is more flexible but introduces more failure modes: what if the adapter has a bug? What if the transformation loses information?

In 2026, the best practice is **strict contracts with explicit adapters**. Each step outputs a well-defined schema. If the next step expects a different schema, you write an explicit adapter function that does the transformation. The adapter is tested, versioned, and documented. This gives you the safety of strict contracts and the flexibility of loose coupling.

## State Management: What Persists Across Steps?

Multi-step systems have **state**: information that persists from one step to the next. In a single-turn chatbot, there is no state — each query is independent. In a multi-turn agentic system, state is everywhere. The conversation history is state. The user's profile is state. The intermediate results from previous tools are state. The agent's plan is state.

Here is how to specify state in your I/O contract:

**Define what state exists.** List every piece of information that persists across steps. For each piece of state, answer: What is it? Where is it stored? Who can read it? Who can write it? When does it get cleared?

**Specify the state schema.** State is data, so it needs a schema. If the conversation history is state, specify the schema for a conversation turn: timestamp, speaker, message, metadata. If the agent's plan is state, specify the schema for a plan step.

**Document the lifecycle.** When is state created? When is it updated? When is it deleted? If the agent's plan is state, does it get cleared after every successful execution, or does it persist until the user starts a new session?

**Control access.** Not every step should be able to read and write every piece of state. The tool executor should be able to read the user's query but not modify it. The planner should be able to write the plan but not modify the tool results. Define read and write permissions for each step.

**Handle concurrency.** In multi-user systems, state can be accessed by multiple processes at once. What happens if two steps try to update the same state at the same time? Do you use locks? Optimistic concurrency control? Event sourcing?

State management is the hardest part of agentic I/O specs because it cuts across the entire system. A bug in state management does not just break one step — it breaks every step that depends on that state.

## Tool Call Specifications

Tools are the building blocks of agentic systems. A tool is a function the agent can call to perform an action: query a database, send an email, fetch a web page, run a calculation. Each tool has its own I/O contract: what parameters it accepts, what it returns, what errors it can throw.

Here is how to specify tool call I/O:

**Define the tool signature.** For each tool, specify: tool name, description, input parameters (name, type, required or optional, description), output schema, possible error codes.

**Document the semantics.** What does the tool actually do? This is not obvious from the signature alone. Does "send email" send immediately or queue the email? Does "query database" return all results or just the first page? Be explicit.

**Specify error handling.** What errors can the tool return? Network timeout? Invalid parameters? Permission denied? For each error, specify: the error code, the error message format, and what the agent should do (retry, skip, fail, ask the user).

**Document side effects.** Does the tool modify state? Does it charge the user? Does it send data to a third party? If yes, document it. The agent needs to know whether a tool is safe to retry or whether retrying would cause double-charging or duplicate actions.

**Specify idempotency.** Can the tool be called multiple times with the same parameters safely? If yes, it is idempotent. If no, document the consequences of retrying. This is critical for error recovery.

Tool call specs are part of the I/O contract because the agent's I/O includes tool calls as structured outputs. If the agent outputs "call tool X with parameters Y," that output must match the tool's input schema. If it does not, the tool call fails.

## The Contract Stack as a Debugging Framework

When an agentic system fails, you need to know **which layer failed**. Did the user input violate the system-level contract? Did the planner output a malformed plan? Did the tool selector choose the wrong tool? Did the tool executor fail to call the tool? Did the interpreter misunderstand the result?

The contract stack gives you a **debugging framework**. You instrument each layer to log its inputs and outputs. When the system fails, you walk through the stack:

**Layer 1: User input.** Did the user input match the expected schema? If not, reject it early with a clear error.

**Layer 2: Planner output.** Did the planner output a valid plan? Does it match the expected schema? If not, the planner is the failure point.

**Layer 3: Tool selector output.** Did the tool selector choose a valid tool? Did it provide the required parameters? If not, the tool selector is the failure point.

**Layer 4: Tool executor output.** Did the tool return a result or an error? If it returned an error, is the error expected or unexpected? If unexpected, the tool is the failure point.

**Layer 5: Interpreter output.** Did the interpreter correctly understand the tool result? If not, the interpreter is the failure point.

**Layer 6: Response generator output.** Did the response match the user-facing schema? If not, the response generator is the failure point.

By logging the I/O at each layer, you can isolate failures to a specific step. This is **much faster** than trying to debug the entire system end-to-end.

## Practical Example: A Travel Booking Agent

Let's make this concrete with a travel booking agent. The user asks: "Book me a flight to London next week."

**Layer 1: User input.** The system receives the query and parses it into structured fields: intent is "book flight," destination is "London," time is "next week."

**Layer 2: Planner output.** The planner outputs a plan: Step 1: search available flights, Step 2: present options to user, Step 3: book selected flight.

**Layer 3: Tool selector output.** The tool selector outputs: call tool "search_flights" with parameters destination is "London," date_range is "next 7 days."

**Layer 4: Tool executor output.** The tool executor calls the search_flights tool and receives a list of flights with prices and times.

**Layer 5: Interpreter output.** The interpreter determines that multiple options exist and the user needs to choose. It outputs: "present options to user."

**Layer 6: Response generator output.** The response generator formats the flight options into a user-friendly message with buttons to select a flight.

At each layer, there is an I/O contract. If any contract is violated — if the planner outputs a plan in the wrong format, if the tool selector specifies an invalid tool, if the tool executor returns an unexpected error — the system fails. By specifying these contracts explicitly, you can catch failures at each layer instead of letting them propagate.

## Why Agentic I/O Specs Are Critical for Debugging

In single-step systems, debugging is straightforward: you give the system an input, you get an output, you check if the output is correct. In multi-step systems, you have to debug **the entire pipeline**. If the final output is wrong, it could be because the planner made a bad plan, the tool selector chose the wrong tool, the tool returned bad data, or the interpreter misunderstood the result.

Without explicit I/O specs for each step, you cannot debug systematically. You resort to reading logs, guessing where the failure happened, and making random fixes. With explicit I/O specs, you can:

**Validate each step independently.** You can test the planner by giving it a query and checking if it outputs a valid plan. You can test the tool selector by giving it a plan and checking if it chooses the right tool. You can test each layer in isolation.

**Trace failures through the stack.** When the system fails, you can trace the I/O at each layer to find where the contract was violated.

**Regression test each layer.** If you fix a bug in the planner, you can add a test case that verifies the planner now outputs the correct plan for that input. You do not have to test the entire end-to-end system every time.

This is why agentic I/O specs are not optional — they are the foundation of your testing and debugging strategy.

## Practical Takeaways

**Map the contract stack.** Before you build a multi-step system, list every step and define the I/O contract for each step.

**Specify intermediate outputs.** Do not assume steps will infer the schema. Define the schema for every intermediate output and enforce it programmatically.

**Document state explicitly.** List all state, define the schema, document the lifecycle, control access, handle concurrency.

**Write tool call specs.** For every tool, specify the signature, semantics, error handling, side effects, and idempotency.

**Use the contract stack for debugging.** Instrument each layer to log inputs and outputs. When the system fails, walk through the stack to isolate the failure.

**Version everything.** Schemas evolve. Use versioning and compatibility checks to prevent breaking changes.

The next question is: how do you turn these I/O specs into evaluation scaffolding? How does every field in your I/O spec become something you can test, measure, and improve?
