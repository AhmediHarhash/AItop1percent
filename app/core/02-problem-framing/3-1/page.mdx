# 3.1 — Why You Need a Task Taxonomy

In early 2024, a fintech startup spent three months building an evaluation pipeline for their document extraction system. They had assembled a team, collected hundreds of labeled examples, and built a dashboard tracking their model's performance. The scores looked impressive: their system achieved 87% on BLEU, 0.92 on ROUGE-L, and strong perplexity numbers. The CEO loved the demo. The metrics showed steady improvement week over week.

Then they shipped to production. Within two weeks, customer support was drowning in complaints. The system was extracting the wrong dollar amounts from invoices. It was missing vendor names entirely. It was hallucinating dates that didn't appear in the source documents. The evaluation metrics hadn't predicted any of this.

The problem wasn't the model. The problem wasn't the data. The problem was that they had classified their task incorrectly. They were building an extraction system but evaluating it like a generation system. BLEU and ROUGE measure how similar generated text is to reference text. They're designed for tasks where you're creating new content: translation, summarization, creative writing. But extraction isn't generation. In extraction, the answer already exists in the source document. You're not creating anything. You're finding and pulling out specific pieces of information that are verifiably present or absent.

When you evaluate an extraction task with generation metrics, you measure the wrong things. You reward fluency and linguistic similarity when you should be measuring precision and correctness. You treat hallucinations as creative variations instead of catastrophic failures. You optimize for smooth-sounding output when you need exact, verifiable facts.

This is why task taxonomy matters. Not as an academic exercise. Not as a classification game for researchers. As an operational necessity that determines whether your evaluation system actually measures what matters.

## The Taxonomy as Rosetta Stone

Task taxonomy is the bridge between product intent and evaluation design. When you correctly identify what type of task you're building, everything else follows. The taxonomy tells you what kind of ground truth to collect, what metrics to track, what evaluation methods to use, and what failure modes to watch for.

Get the taxonomy wrong, and every downstream decision compounds the error. You build the wrong dataset. You track the wrong metrics. You optimize for the wrong objectives. You ship a system that performs well on paper but fails in production.

Get the taxonomy right, and the path becomes clear. You know immediately whether you need human judgment or automated metrics. You know whether multiple answers can be correct or whether there's a single verifiable truth. You know whether to measure exact matches or semantic similarity. You know what to test, what to trust, and what to doubt.

The fintech team's mistake wasn't unique. It's one of the most common failure patterns in AI product development. Teams borrow evaluation approaches from one task type and apply them to another. They use classification metrics for generation tasks. They use generation metrics for extraction tasks. They use extraction metrics for reasoning tasks. Each mismatch creates a blind spot where the evaluation system fails to catch critical failures.

## Why Different Tasks Need Different Evaluation

Consider three different AI systems. The first generates marketing copy for email campaigns. The second extracts invoice line items from PDF receipts. The third classifies customer support tickets into routing categories.

All three systems take text as input and produce output. All three use large language models. All three need evaluation. But they need fundamentally different evaluation approaches.

The marketing copy generator has an enormous output space. For any given product and target audience, there are thousands of ways to write effective copy. Some emphasize features. Some emphasize benefits. Some use humor. Some use urgency. Some are short and punchy. Some are longer and more explanatory. Multiple outputs can be equally good. Quality is subjective and context-dependent. You can't evaluate this system with exact match metrics. You need human judgment, rubrics that capture multiple quality dimensions, or sophisticated LLM-as-judge systems that understand marketing principles.

The invoice extraction system has a tightly constrained output space. Each invoice contains specific pieces of information: vendor name, invoice number, date, line items with descriptions and amounts, subtotal, tax, total. These facts either appear in the document or they don't. The amounts are either correct or incorrect. There's no creative interpretation. You can evaluate this system with precision and recall. You can measure exact matches for numbers and fuzzy matches for text fields. You can verify every extraction against the source document. Human judgment is needed only for ambiguous edge cases.

The ticket routing classifier has a bounded output space. Each ticket goes into one of fifteen predefined categories: billing question, technical issue, account access, feature request, and so on. The categories are fixed. Each ticket has a correct category or sometimes multiple correct categories. You can evaluate this system with standard classification metrics: accuracy, precision per category, recall per category, confusion matrices that show which categories get mixed up. The evaluation is objective and reproducible.

If you try to evaluate all three systems the same way, you fail. If you use exact match metrics on marketing copy, you'll reject perfectly good creative variations. If you use human judgment on invoice extraction, you'll spend a fortune on annotation and still miss systematic errors. If you use generation metrics on ticket routing, you'll have no idea which categories are failing.

## How Taxonomy Determines Ground Truth

The task type determines not just how you evaluate but what kind of ground truth you need to collect in the first place.

For generation tasks, ground truth examples are references, not answers. When you collect ground truth for a summarization system, you're not collecting the one correct summary. You're collecting example summaries that demonstrate quality and style. Different humans will write different summaries of the same article, and many of those differences are matters of preference, not correctness. Your ground truth dataset shows the model what good looks like, but it doesn't define a single target to hit.

For extraction tasks, ground truth is verifiable fact. When you collect ground truth for an invoice extraction system, you're marking the specific pieces of information that appear in each document. The vendor name is either "Acme Corporation" or it isn't. The total is either $1,247.83 or it isn't. Your ground truth dataset defines correct answers that can be objectively verified.

For classification tasks, ground truth is labeled categories. When you collect ground truth for a routing system, you're assigning each example to one or more categories from a fixed set. The labels represent human judgment about which category fits best, and there's often inter-annotator disagreement at the boundaries. But the label space is finite and known in advance. Your ground truth dataset maps inputs to the category labels that human experts would assign.

The process of collecting ground truth looks completely different depending on the task type. For generation, you need creative writers who can produce high-quality examples. For extraction, you need careful annotators who can find and mark information precisely. For classification, you need domain experts who understand the category definitions and can make consistent judgment calls.

The cost profile is different. Generation ground truth is expensive per example but you might need fewer examples if they're high quality. Extraction ground truth is moderate cost and you need systematic coverage of document variations. Classification ground truth is cheaper per example but you need lots of examples to cover the full category distribution including edge cases.

The quality assurance process is different. For generation, you review examples for quality, coherence, and adherence to guidelines. For extraction, you verify that annotations are correct against the source material. For classification, you measure inter-annotator agreement and resolve disagreements.

## How Taxonomy Determines Metrics

Once you know your task type, the appropriate metrics become obvious. Some metrics are designed for open-ended generation. Some are designed for structured extraction. Some are designed for classification. Using the wrong metrics for your task type is like using a thermometer to measure distance.

Generation tasks need metrics that measure quality dimensions: fluency, coherence, relevance, factuality, creativity, tone, style. Some of these can be measured automatically with metrics like BLEU, ROUGE, or BERTScore. But these metrics have known limitations and often correlate poorly with human judgment. Increasingly, generation evaluation relies on human ratings using rubrics or on LLM-as-judge systems that can assess multiple quality dimensions.

Extraction tasks need metrics that measure accuracy and completeness: precision, recall, F1, exact match rate, fuzzy match rate, field-level accuracy. These metrics tell you whether you're finding all the information you should find and whether the information you're finding is correct. They work because extraction has verifiable ground truth. You can objectively determine whether an extraction is correct or not.

Classification tasks need metrics that measure category assignment: accuracy, per-class precision and recall, F1 scores, confusion matrices, ROC curves for binary classification. These metrics tell you not just overall performance but which specific categories are working and which are failing. They reveal patterns like category imbalance or systematic confusion between similar categories.

Routing tasks, which are a specialized form of classification, also need downstream metrics: what happens after the routing decision? If you route a ticket to the wrong department, how long does it take to reroute? If you route a user query to the wrong specialized model, what's the quality impact? Task taxonomy helps you think beyond the immediate task to its downstream effects.

## How Taxonomy Determines Failure Modes

Different task types fail in different ways. Understanding your task type tells you what to watch for in production.

Generation tasks fail through hallucination, irrelevance, incoherence, tone mismatch, or offensive content. They rarely fail completely—they usually produce plausible-sounding output that might be wrong. The failure mode is subtle and requires human judgment to catch. You need monitoring that samples outputs and evaluates them against quality rubrics.

Extraction tasks fail through omission, false positives, wrong values, or wrong field assignments. They fail more obviously—missing information or incorrect information creates immediate downstream problems. The failure mode is objective and can be caught with automated checks. You need monitoring that validates extracted values against expected formats and ranges, and you need human review of edge cases.

Classification tasks fail through misclassification, with predictable confusion patterns. Similar categories get mixed up. Rare categories get missed. Ambiguous examples get classified inconsistently. The failure mode shows up in confusion matrices and per-category metrics. You need monitoring that tracks category distribution over time and alerts when rare categories disappear or when confusion patterns change.

Reasoning tasks, which often combine multiple subtask types, fail through logical errors, fact errors, missing steps, or incorrect conclusions. They fail in complex ways that require tracing through the reasoning chain. The failure mode can appear at any step in the process. You need monitoring that evaluates not just final outputs but intermediate reasoning steps.

## Why Most Real Systems Combine Task Types

Here's where it gets tricky: most production AI systems don't do just one task type. They combine multiple types in a pipeline or a loop.

A customer support chatbot does classification (what's the user's intent?), extraction (what are the key details?), retrieval (what relevant context do we need?), reasoning (what's the right response approach?), and generation (produce the actual response). Each stage is a different task type. Each needs different evaluation.

A document processing system does extraction (pull the fields), classification (what type of document is this?), transformation (convert to the target format), and validation (does the output make sense?). Different task types at different stages.

An AI research assistant does classification (what's the query type?), retrieval (find relevant sources), extraction (pull key information), reasoning (synthesize across sources), and generation (write the answer). Five task types in one user interaction.

When your system combines multiple task types, you can't evaluate it as a single black box. You need to identify the task type for each component and evaluate each appropriately. You need to trace failures back to the specific task stage where they occur. You need to understand how errors compound across stages.

This is why task taxonomy is fundamental, not optional. It's not about putting tasks in neat boxes for academic papers. It's about operational clarity. About knowing what you're building, how to evaluate it, how to monitor it, and how to debug it when it fails.

The fintech team eventually rebuilt their evaluation system from scratch. They classified their task correctly as extraction. They replaced BLEU and ROUGE with precision, recall, and field-level accuracy metrics. They collected new ground truth with verified correct extractions. They built separate evaluation pipelines for different field types: exact match for amounts, fuzzy match for vendor names, date parsing validation for dates.

Within a month, their evaluation scores became predictive of production performance. They could spot problems in dev that would have caused production failures. They could track improvement on metrics that actually mattered. They could ship with confidence.

That's the power of getting taxonomy right. It's the difference between evaluation theater and evaluation that actually works. And that difference determines whether your AI system succeeds or fails in the real world.

In the next section, we'll map out the five core task types that cover the vast majority of AI product use cases and show you how to identify which type you're really building.
