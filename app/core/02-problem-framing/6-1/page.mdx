# 6.1 — The Framing-to-Eval Handoff

In late 2023, a healthcare AI company spent four months building what they thought was a well-framed document classification system. The problem framing team had created comprehensive documentation: a thirty-page specification with task definitions, success criteria, and stakeholder sign-off. They handed it to the evaluation team with confidence. Three weeks later, the eval lead called an emergency meeting. "We can't build test cases from this," she said, pointing to a success criterion that read: "Documents should be classified with high accuracy and appropriate sensitivity to medical context."

What did "high accuracy" mean? What threshold? What did "appropriate sensitivity" mean? How would they measure it? The framing document was beautifully written but operationally useless. The eval team had to reverse-engineer what the framing team actually meant, which took six weeks of back-and-forth meetings, delayed the entire project timeline, and created tension between the teams.

The handoff had failed. Not because anyone did bad work, but because nobody had defined what a successful handoff actually required.

## What Evaluation Teams Actually Need

The evaluation team's job is to build systems that measure whether your AI works. They cannot do that job with vague documentation, abstract principles, or elegant prose. They need concrete, testable, operationalizable artifacts.

Here is what a successful handoff includes. A task registry with every task your system handles, each task's type clearly specified from your taxonomy, and each task's risk tier marked. Not "the system processes customer inquiries" but "Task A1: Extract account number from text query. Type: extraction. Risk tier: medium. Task A2: Classify inquiry type into seven categories. Type: classification. Risk tier: low."

Success criteria with specific numeric thresholds. Not "high accuracy" but "classification accuracy above 92 percent on held-out test set, with no single category below 85 percent recall." Not "responses should be helpful" but "responses rated 4 or above on 5-point helpfulness scale by domain experts in blind evaluation, minimum 80 percent of cases."

Input and output specifications with exhaustive edge case documentation. The eval team needs to know what happens when inputs are malformed, missing required fields, or contain unexpected data types. They need to know the full range of valid outputs, including error states and refusal responses. They need examples of boundary cases where correct behavior is non-obvious.

A constraint document that specifies hard limits the system must respect. Maximum response latency. Minimum throughput. Data privacy requirements. Regulatory compliance requirements. These constraints shape how eval is designed. If you have a 200-millisecond latency constraint, the eval harness needs to measure latency. If you have a "no PII in logs" constraint, the eval process needs to check for PII leakage.

Finally, stakeholder sign-off that confirms all relevant parties agree on what the evaluation team is measuring. This prevents the scenario where the eval team builds a comprehensive test suite, reports that the system is working, and then product says "but that's not what we meant by working."

These five artifacts—task registry, success criteria with thresholds, I/O specs with edge cases, constraint document, and stakeholder sign-off—are the minimum viable handoff. Without all five, the eval team will be guessing.

## Why Most Handoffs Fail

The most common failure pattern is producing beautiful documentation that cannot be operationalized. The framing team writes eloquent descriptions of what the system should do, but those descriptions do not translate into test cases.

Consider this success criterion from a real project: "The system should provide accurate, timely, and contextually appropriate responses to user questions." That sentence is not wrong, but it is not testable. What does "accurate" mean? Factually correct? Tonally correct? Legally correct? What does "timely" mean? Under one second? Under ten seconds? What does "contextually appropriate" mean? How does an evaluator measure it?

The framing team wrote that criterion thinking it captured the essence of what they wanted. The eval team received it and had no idea how to measure it. The result was weeks of meetings to decompose that single sentence into something actionable.

Another common failure: success criteria written in natural language without clear measurement protocols. "Users should be satisfied with responses" is a valid goal, but how do you measure it? User surveys? Implicit signals like session duration? Explicit thumbs-up-thumbs-down feedback? The framing document needs to specify not just what success looks like but how you will know when you have achieved it.

A third failure pattern: input/output specifications that are too abstract to generate test cases from. The framing document says "the system accepts natural language queries and returns relevant information." Okay, but what specific query formats? What edge cases? What should happen if the query is ambiguous, off-topic, or in a language the system does not support? Without concrete examples, the eval team cannot build a test set.

Then there is the missing edge case problem. The framing team focuses on happy path scenarios because those are easiest to articulate. The eval team needs edge cases because those are where systems break. If the framing document does not catalogue input variability, adversarial cases, and boundary conditions, the eval team will discover them through trial and error—which is slow and expensive.

Finally, there is the implicit knowledge problem. The framing team has been thinking about this problem for weeks or months. They have context, assumptions, and domain knowledge that feel obvious to them. The eval team is coming in fresh. If critical information exists only in the framing team's heads and not in the documentation, the handoff will fail.

## The Eval-Ready Test

Here is a simple test for whether your framing artifacts are ready to hand off: give them to someone on the evaluation team who was not involved in framing. Ask them to build ten test cases based on the documentation. Do not answer questions. Do not provide clarification. Just let them try.

If they can build test cases that you would agree are correct—covering the right scenarios, checking the right criteria, structured in a usable way—then the handoff is ready. If they come back with questions, confusion, or test cases that miss the point, the artifacts are not ready yet.

This test reveals gaps immediately. You will discover success criteria that sound clear but are ambiguous when operationalized. You will find I/O specs that do not cover common cases. You will identify assumptions you made that were never written down.

Run this test before the formal handoff meeting. It is cheaper to iterate on documentation than to waste the eval team's time.

The eval-ready test is not about perfection. It is about usability. The question is not "did we document everything?" but "can the eval team proceed without asking us for clarification?" If the answer is no, you are not done.

## Structuring the Handoff Meeting

When the artifacts are eval-ready, schedule the formal handoff meeting. This is not a presentation. It is a working session where the framing team walks through each artifact and the eval team stress-tests them.

Start with the task registry. Go through each task. For each one, the eval team asks: Is the task type correct? Is the risk tier appropriate? Are there dependencies between tasks that we need to account for in evaluation? Are there tasks missing from the registry that we will encounter in production?

Then move to success criteria. For each criterion, the eval team asks: How exactly do we measure this? What data do we need? What is the threshold? What happens if we are close to the threshold but not quite there—is that a pass or a fail? Who adjudicates edge cases?

Next, review the I/O specifications. The eval team should push hard on edge cases. Walk through the input variability catalogue. For each type of variability, confirm what the expected behavior is. What happens with malformed inputs? What happens with inputs that are technically valid but semantically nonsensical? What happens with inputs that are adversarial or trying to break the system?

Then go through constraints. The eval team needs to understand which constraints are hard limits—the system absolutely must meet them—and which are targets—we would like to meet them but can accept some degradation if necessary. This distinction shapes how they design the eval harness and how they interpret results.

Finally, review the stakeholder sign-off. Make sure the eval team understands what each stakeholder group cares about. If legal signed off on privacy constraints, the eval team needs to know how to test for privacy violations. If product signed off on user satisfaction targets, the eval team needs to know how to collect satisfaction data.

Throughout the meeting, the eval team should be taking notes, asking questions, and proposing test scenarios. The framing team should be clarifying, correcting, and occasionally discovering gaps in their own documentation. This is an iterative conversation, not a one-way information transfer.

The meeting ends when the eval team confirms they can proceed. Not that they understand everything—they will discover new questions as they build—but that they have enough clarity to start building the evaluation harness and test cases.

## The Framing Team's Ongoing Responsibility

The handoff meeting is not the end of the framing team's involvement. It is the beginning of a different kind of involvement.

As the eval team builds test cases, they will discover gaps. They will find scenarios the framing document did not cover. They will encounter ambiguities that need resolution. They will realize that a success criterion that seemed clear on paper is actually ambiguous when you try to measure it.

When this happens, the eval team should not guess. They should go back to the framing team and ask. The framing team's responsibility is to be available for these questions and to answer them quickly. A day of delay for the eval team can cascade into a week of delay for the project.

The framing team should also maintain the framing artifacts as new information emerges. If the eval team discovers that a success criterion needs to be revised, update the document. If they identify a new edge case, add it to the I/O spec. The framing document is a living artifact, not a static contract.

This ongoing collaboration requires the right team norms. The eval team needs to feel comfortable asking questions without being seen as difficult or slow. The framing team needs to treat questions as useful feedback, not as failures of the original documentation. Both teams need to recognize that ambiguity is inevitable and that catching it early is a win, not a problem.

In practice, the best teams establish a regular check-in cadence during the eval build-out phase. Weekly 30-minute meetings where the eval team surfaces questions, the framing team provides answers, and both teams align on any updates to the shared artifacts. This prevents small misunderstandings from becoming large project delays.

## What Good Looks Like

Let me show you what a successful handoff looks like in practice.

A legal tech company was building an AI system to categorize and summarize legal documents. The framing team spent three weeks building the handoff artifacts. They created a task registry with twelve distinct tasks, each with a clear type, risk tier, and dependencies. They wrote success criteria with numeric thresholds for accuracy, latency, and coverage, and specified the evaluation protocol for each metric. They documented sixty-seven distinct input edge cases, from malformed PDFs to documents mixing multiple languages. They listed constraints including GDPR compliance, audit trail requirements, and maximum processing time.

They ran the eval-ready test with two members of the evaluation team. The evaluators built test cases for three days, then presented them back to the framing team. The framing team found two gaps in the I/O spec and one ambiguity in a success criterion. They updated the documentation and confirmed the changes with the evaluators.

In the formal handoff meeting, the eval team walked through every task, every success criterion, and every edge case. They asked dozens of questions. The framing team answered most of them from the documentation, clarified a few, and discovered three new edge cases they had not considered. They updated the artifacts in real time.

The eval team left the meeting with everything they needed. They built the evaluation harness in two weeks. They encountered five questions during that period, which the framing team answered in under 24 hours each time. When the harness was complete, it directly measured every success criterion the framing team had defined. No surprises. No misalignments.

That is what good looks like. Not zero questions, but fast answers. Not perfect documentation, but documentation good enough to build from. Not a one-time handoff, but an ongoing partnership.

## Why This Matters

The framing-to-eval handoff is a leverage point. If you get it right, evaluation proceeds smoothly, test cases align with actual product requirements, and you catch problems before they reach production. If you get it wrong, you waste weeks or months building the wrong tests, measuring the wrong things, and discovering misalignments late when they are expensive to fix.

Most teams treat the handoff as a formality. They write documentation, have a meeting, and assume everyone is aligned. Then they wonder why evaluation takes forever, why test results do not match stakeholder expectations, and why they keep discovering requirements after they have already shipped.

Elite teams treat the handoff as a critical phase transition. They invest in making artifacts eval-ready. They structure the handoff meeting as a working session, not a presentation. They maintain ongoing collaboration as the eval team builds. They understand that the time invested in a good handoff pays for itself ten times over in reduced confusion, faster iteration, and higher-quality evaluation.

The next step is mapping tasks to evaluation strategies, so the eval team knows not just what to test but how to test it.
