# 4.7 — Quantifying Criteria: Thresholds, Targets, and Baselines

In early 2025, a mid-sized healthcare company assembled a cross-functional team to build an AI system that would help intake coordinators schedule patient appointments more efficiently. The team spent two weeks in meetings defining success. Product said "it needs to be better than what we have now." Engineering said "we want to maximize accuracy." The clinical operations lead said "it should be really good." Everyone nodded in agreement. They built the system over three months. When they launched, the intake coordinators hated it. Product said "but it is better than the old system." The coordinators said "barely, and it is not worth the hassle of learning something new."

The problem was not the technology. The system was indeed more accurate than the manual scheduling process. The problem was that nobody had quantified what "better" meant. The old system had coordinators successfully scheduling appointments without conflicts 73% of the time. The new AI system achieved 78%. That is objectively better, but the 5% improvement was not enough to justify the workflow disruption. If the team had defined thresholds before building — "we need at least 85% accuracy to justify changing the workflow" — they would have known three months earlier that the current approach would not deliver enough value. They could have reframed, invested in a better model, or killed the project before wasting time and budget.

This is the discipline of quantifying success criteria: turning qualitative goals into specific numbers that define when you ship, when you iterate, and when you stop. Without quantification, you are building in the dark, hoping that whatever you produce will be good enough.

## The Three-Level Framework: Baseline, Threshold, Target

Quantifying success criteria requires defining three distinct levels of performance: baseline, threshold, and target. These are not interchangeable. Each serves a different purpose in framing and decision-making.

**Baseline** is where you are today. It is the performance of the current solution, whether that is a human process, an existing software system, a manual workflow, or no solution at all. Baseline tells you the starting point. It answers: what are we replacing or augmenting?

**Threshold** is the minimum acceptable performance to ship. It is the quality bar you must meet before deployment is justified. Threshold is a gate, not an aspiration. Below threshold, the system does not ship because it does not deliver enough value to offset its costs and risks. Threshold answers: what is the minimum quality we need to make this worth doing?

**Target** is where you want to be. It is the aspirational performance level that represents success beyond the minimum viable bar. Target is what you aim for during development, even though you know you might not hit it in version one. Target answers: if everything goes well, how good can this get?

The relationship between these three levels is critical. Baseline sets the floor. Threshold sets the gate. Target sets the ambition. The gap between baseline and threshold tells you how much improvement you must achieve to justify the project. The gap between threshold and target tells you how much upside remains after you ship.

In the healthcare scheduling example, a proper three-level framework would have looked like this: Baseline is 73% — that is the current manual success rate. Threshold is 85% — below that, the workflow disruption is not justified by the improvement. Target is 92% — that is what we believe is achievable with an excellent model and good data. Now you have clarity. You need 12 percentage points of improvement to ship. You have a stretch goal of 19 points. The project is framed with specific quantitative success gates.

## Establishing Baselines: Measuring What Exists Today

Baseline measurement is the first step in quantifying criteria, and it is harder than it sounds. The challenge is not arithmetic — it is deciding what to measure and how to measure it fairly.

If you are replacing a human process, your baseline is human performance. But human performance varies. Some customer service agents resolve 80% of inquiries, others resolve 50%. Do you baseline against the average agent, the median agent, the top quartile, or the worst performer? The answer depends on your deployment model. If the AI will replace all agents, you should baseline against average performance. If the AI will assist agents, you should baseline against top performer behavior because that is the quality bar the assisted workflow should meet.

If you are replacing an existing software system, your baseline is system performance as measured in production logs. This is more objective but still has nuance. Are you measuring overall performance or performance on the subset of tasks your new AI will handle? If your AI is designed for complex queries but your existing system handles simple queries well, you need to baseline specifically on complex query performance, not overall performance.

If you are building something new with no existing solution, your baseline is zero or "not done." This is the hardest case to quantify because there is no performance comparison, only a value proposition. You must establish a baseline of "manual effort required" or "time spent" or "problems unresolved" and show that your AI system improves on that dimension. For example, if legal teams currently spend 10 hours per contract manually reviewing compliance clauses, your baseline is 10 hours. Your AI system must reduce that time to justify adoption.

Baseline measurement must be honest. The temptation is to lowball the baseline to make your AI look better. "The current process fails 40% of the time" sounds more impressive than "the current process succeeds 60% of the time," but they are the same number framed differently. Stakeholders notice dishonest framing, and it erodes trust. Measure baseline performance rigorously, using the same evaluation methodology you will use for your AI system. If you are going to measure your AI's accuracy on a held-out test set, measure your baseline system's accuracy on the same test set.

A fair baseline gives you credibility when you claim improvement. If you say "we improved from 60% to 85%," stakeholders trust that the 60% was measured honestly, so the improvement is real.

## Setting Thresholds: The Minimum Acceptable Performance Gate

Threshold is the most important number in your success criteria framework. It is the line between ship and do not ship. Setting thresholds is part negotiation, part analysis, and part judgment.

The threshold must be achievable but meaningful. If you set the threshold too low — barely above baseline — you risk shipping a system that provides minimal value. Users will ask "why did we change our workflow for this?" If you set the threshold too high — near perfection — you will never ship because no AI system is perfect on day one.

How do you find the right threshold? There are several approaches, each applicable in different contexts.

**Value-based thresholding** starts with the question: what improvement justifies the cost and disruption of adopting this system? If switching to an AI system requires training 50 employees and changing established workflows, the improvement must be substantial enough to offset that investment. For example, if your current manual process resolves 65% of cases successfully, you might set a threshold of 80% based on stakeholder input that anything less would not justify retraining the team. This approach ties threshold to ROI.

**Stakeholder negotiation** involves asking the people who will use or be affected by the system what quality level they find acceptable. Customer service managers might say "we need the AI to handle at least 70% of inquiries because below that, the escalation volume overwhelms our human agents." Product teams might say "we need recommendation click-through rates of at least 5% because below that, the UI real estate is not justified." These are informed judgments based on operational constraints, and they give you a threshold grounded in real-world needs.

**Industry benchmarks** provide external reference points. If you are building a document classification system and published benchmarks show that state-of-the-art models achieve 90% accuracy on similar tasks, you might set your threshold at 85% — slightly below state-of-the-art but achievable. Benchmarks prevent you from setting thresholds that are unrealistic given current technology.

**User research** can reveal acceptable quality levels through usability testing. Show users outputs at different quality levels — 70% accurate, 80% accurate, 90% accurate — and ask which quality level they find acceptable. This is especially useful for subjective criteria like "helpfulness" or "naturalness." Users might say "at 75% accuracy, I do not trust it, but at 85%, I find it helpful." That tells you where to set the threshold.

**Risk analysis** applies when failure has consequences. For a fraud detection system, you might analyze the cost of false positives versus false negatives and set precision and recall thresholds based on acceptable risk. If a false positive costs 10 dollars in customer service overhead but a false negative costs 500 dollars in fraud loss, you set a higher threshold for recall than precision.

Threshold-setting is not purely analytical. It involves judgment and trade-offs. The key is to make the threshold explicit before you build. You document the rationale, you get stakeholder agreement, and you commit to shipping only if the threshold is met.

## Setting Targets: The Aspirational Performance Goal

Target is where you want to be if everything goes well. It is higher than threshold but grounded in technical feasibility. The purpose of a target is to give the team something to aim for during development without making it a hard gate.

Targets are useful because they create room for iteration. If your threshold is 80% and your target is 90%, you can ship at 82% and still feel good about the achievement while knowing there is headroom for improvement. Targets also communicate ambition. They tell stakeholders "we are not settling for the minimum; we are pushing for excellence."

Targets should be based on what is technically achievable given current models, data availability, and resource constraints. If state-of-the-art models on your task achieve 92% accuracy, setting a target of 99% is not aspirational — it is fantasy. A realistic target might be 88%, acknowledging that you probably will not match state-of-the-art in version one but you are aiming high.

Targets are also useful for internal prioritization. If you are at 78% accuracy and your threshold is 80%, you focus on crossing the gate. Once you cross the threshold, you shift to pursuing the target. This prevents perfectionism from delaying launch while still incentivizing quality improvement.

The relationship between threshold and target must be reasonable. If your threshold is 80% and your target is 81%, there is no meaningful gap — you are not communicating aspiration. If your threshold is 80% and your target is 98%, the gap is so large that the target has no credibility. A good rule of thumb is that the target should be 10-20% higher than the threshold in relative terms. If threshold is 80% accuracy, target might be 88-90%.

## Gap Analysis: Baseline to Threshold

Once you have defined baseline, threshold, and target, the next step is gap analysis. How far do you need to go from where you are to where you need to be?

The gap between baseline and threshold is the improvement requirement. If baseline is 60% and threshold is 85%, you need a 25 percentage point improvement. This is not trivial. It tells you that incremental improvements will not cut it — you need a substantial leap in performance.

Gap analysis informs technical strategy. A small gap between baseline and threshold suggests that modest model improvements, better prompts, or fine-tuning might get you there. A large gap suggests you need better data, a different model architecture, or a fundamentally different approach.

Gap analysis also informs go/no-go decisions. If your baseline is 50%, your threshold is 95%, and your technical assessment is that you can realistically get to 75%, you have a problem. The gap is unbridgeable with current resources. You can either lower the threshold through stakeholder negotiation, invest significantly more in model development, or kill the project before you waste time on something that cannot meet the minimum bar.

A real-world example: a financial services company wanted to automate contract clause extraction. Their baseline was a semi-automated process with 68% accuracy. Their threshold, set by Legal, was 95% accuracy because below that, the manual review burden would exceed the current process. The gap was 27 percentage points. The engineering team assessed that with available data and models, they could realistically achieve 82% accuracy. The gap was too large. They went back to Legal and negotiated: "We can build a system that achieves 82% accuracy and highlights low-confidence extractions for mandatory review. That reduces your review load by 50% without requiring 95% accuracy on all clauses." Legal accepted the reframe. The threshold was adjusted to 82% with 100% human review of flagged items. The project became viable.

Gap analysis prevents wishful thinking. It forces you to confront the distance between current state and minimum acceptable state and make honest assessments of feasibility.

## Why Thresholds Must Be Set Before Building

The single most important principle of threshold-setting is that it must happen before development, not after. Setting thresholds after you build is not criteria — it is retrofitting justifications to match whatever you already built.

The failure mode looks like this: you build a system without clear thresholds. You evaluate it and achieve 77% accuracy. Now you need to decide whether to ship. Product says "is 77% good enough?" Engineering says "it is better than baseline." Legal says "what is the threshold?" Someone says "let us set the threshold at 75% so we can ship." This is not threshold-setting. This is manufacturing approval.

Pre-defined thresholds create accountability. If you commit to an 85% threshold before building, you know what you are aiming for. When you hit 82%, you know you have more work to do. When you hit 87%, you know you have met the bar. The threshold is the target, not a post-hoc rationalization.

Pre-defined thresholds also prevent scope creep and gold-plating. Without a threshold, teams often chase perfection because "better" has no upper bound. With a threshold, teams know when to stop optimizing and ship. You hit the threshold, you validate it with stakeholders, and you launch. You do not spend three more months chasing another two percentage points unless there is a compelling reason.

Setting thresholds before building also aligns stakeholders. When Legal, Product, Engineering, and Domain Experts all agree that 85% is the threshold, everyone is working toward the same goal. When thresholds are vague or undefined, different stakeholders have different expectations, which leads to conflict at launch time.

The process for pre-defining thresholds is to convene stakeholders, present baseline data, discuss what improvement justifies the project, document the threshold, and get sign-off. This is not a five-minute conversation. It is a structured decision-making process that might take a week of analysis and discussion. But the time investment is worth it because it prevents months of wasted effort building something that does not meet stakeholder needs.

## Connecting Thresholds to SLOs

In production systems, thresholds evolve into Service Level Objectives — the ongoing performance commitments you make once the system is live. If your pre-launch threshold is 85% accuracy, your post-launch SLO might be "maintain 85% accuracy or higher measured weekly." The threshold is the gate to ship; the SLO is the gate to stay shipped.

SLOs add a time dimension to thresholds. It is not enough to hit 85% once during evaluation. You must maintain 85% in production under real-world conditions with evolving data and user behavior. This is why thresholds must be achievable with margin — if you barely scrape past the threshold during evaluation, you will likely fall below it in production when conditions change.

The relationship between thresholds and SLOs informs how you set thresholds. If your SLO will be "maintain 85%," your launch threshold should probably be 88-90% to give yourself headroom for production degradation. This prevents the common failure mode where a system launches at threshold and immediately violates SLOs when real-world data differs from test data.

Thresholds and SLOs also connect to monitoring and alerting, which we will explore in Section 5. When your SLO is 85% accuracy and your monitoring detects degradation to 83%, you trigger an alert and an investigation. The threshold you set during framing becomes the ongoing quality bar that governs production operations.

## Quantification for Multiple Criteria

Most systems have multiple success criteria, not just one accuracy number. You might have accuracy, latency, cost per query, user satisfaction, and safety as distinct criteria. Each needs its own baseline, threshold, and target.

The challenge with multiple criteria is that they often conflict. Improving accuracy might increase latency. Reducing cost might reduce accuracy. You cannot optimize all criteria simultaneously to their individual targets. You need a prioritization framework.

The prioritization framework defines which criteria are gates and which are goals. A gate criterion is a threshold you must meet to ship — it is non-negotiable. A goal criterion is something you aim for but can trade off if necessary. For example, accuracy might be a gate at 85%, latency might be a goal at 200ms, and cost might be a goal at 10 cents per query. This means you will not ship below 85% accuracy under any circumstances, but you might ship with 250ms latency if accuracy is solid.

Quantifying multiple criteria also requires understanding dependencies. If your latency threshold is 200ms and achieving 85% accuracy requires a model that runs in 400ms, you have a conflict. You must either relax one threshold, invest in optimization to meet both, or reframe the problem.

The multi-criteria threshold matrix is a useful tool. List each criterion, its baseline, threshold, target, and priority level. This creates a single reference showing all quantitative commitments and their relative importance. Stakeholders can review the matrix and validate that the thresholds reflect their needs and the priorities make sense.

## Real-World Example: Quantifying Criteria for a Content Moderation System

A social media company building an AI content moderation system for hate speech provides a concrete example of baseline-threshold-target quantification.

**Baseline**: The current human moderation process reviews flagged content with 89% precision and 62% recall. Precision is high because human moderators are cautious and mostly catch real violations when they review them. Recall is low because they cannot review all content, so many violations slip through.

**Threshold**: Product and Trust & Safety agree that the AI system must achieve at least 85% precision and at least 75% recall to justify deployment. Precision threshold is slightly lower than human baseline because some false positives are acceptable if overall recall improves significantly. Recall threshold is 13 points higher than baseline because the goal is to catch more violations.

**Target**: The target is 92% precision and 85% recall. This represents excellent performance — better than human baseline on both dimensions — but it is grounded in published benchmarks for hate speech detection models.

**Gap Analysis**: Precision gap is small — only 4 points from baseline to threshold. Recall gap is large — 13 points from baseline to threshold. The engineering team focuses on recall improvement as the critical path. They assess that with current models and training data, they can realistically achieve 88% precision and 78% recall, which meets both thresholds but not both targets.

**Decision**: The system ships when it hits 88% precision and 78% recall. It meets the gates. Post-launch, the team continues improving toward the 92%/85% target through model iteration and data collection.

This example shows quantification in action. Every criterion has a number. Every number has a rationale. Every rationale has stakeholder agreement. There is no ambiguity about what success means or when the system is ready to ship.

## The Discipline of Quantification

Quantifying success criteria is uncomfortable for teams used to qualitative goals. "It should be helpful" feels safer than "it must achieve 85% accuracy" because the former cannot fail and the latter can. But that discomfort is precisely the point. Quantification creates accountability.

When you commit to a number, you commit to measuring it. When you commit to measuring it, you commit to building evaluation infrastructure. When you commit to evaluation infrastructure, you commit to rigor. This chain of commitments is what separates serious AI product development from experimentation.

Quantification also enables progress tracking. When your goal is "make it better," you never know if you are done. When your goal is "reach 85% accuracy," you know exactly where you stand. You are at 78%, you need 7 more points, you can estimate how many experiments that might take.

The most common objection to quantification is "but some criteria are not quantifiable." User satisfaction, helpfulness, naturalness — these feel subjective. The response is not to abandon quantification but to define measurable proxies. User satisfaction can be measured with NPS scores or survey ratings. Helpfulness can be measured with task completion rates or time saved. Naturalness can be measured with human preference studies. Imperfect quantification is better than no quantification.

The discipline is to quantify everything you can, acknowledge where quantification has limits, and use a mix of quantitative and qualitative criteria with clear priorities. The baseline-threshold-target framework applies even to qualitative criteria. Baseline: "users currently rate satisfaction at 6.2 out of 10." Threshold: "we need ratings of at least 7.5 to justify the change." Target: "we aim for ratings of 8.5."

Having established how to quantify success criteria in terms that can be measured and validated, the next challenge is ensuring those criteria survive model changes — writing success definitions that outlive any specific technical implementation.
