# 3.5 — Classification and Routing Tasks

A customer support platform implemented an AI routing system in mid-2025 to automatically categorize incoming tickets and send them to the right team. They had five departments: Sales, Technical Support, Billing, Account Management, and Product Feedback. The system used an LLM to classify each ticket based on its content and route it accordingly.

They evaluated the system on 1,000 manually labeled tickets. Overall accuracy was 92%. The metrics looked great: high precision and recall across most categories, strong F1 scores, a confusion matrix that showed clear separation between categories. The product team was thrilled. They rolled it out to all incoming tickets.

Three weeks later, the Technical Support team was drowning. Their ticket queue had ballooned by 40%. Meanwhile, the Product Feedback queue had nearly dried up. The routing system was systematically misclassifying technical product feedback as technical support requests. Both involved technical content. Both mentioned features and functionality. But one category represented bugs and help requests that needed immediate response, while the other represented enhancement ideas that fed into the product roadmap.

The distinction was subtle. A ticket saying "the export function is really slow" could be either: a performance bug that needed fixing or feedback that the feature needs optimization. The classifier defaulted to Technical Support because that was the larger, more common category in the training data. It had learned the surface pattern (mentions of technical features) but not the deeper intent distinction (problem versus suggestion).

The evaluation metrics had been misleading. Ninety-two percent accuracy sounds good, but that means eight percent of thousands of daily tickets were misrouted. The confusion matrix had shown some mixing between Technical Support and Product Feedback, but the team had interpreted this as acceptable boundary ambiguity. They hadn't realized it would create operational chaos at scale.

This is the paradox of classification tasks. They're the easiest task type to evaluate by traditional metrics. You have bounded output space. You have clear ground truth. You can measure accuracy objectively. But getting the categories right is hard. Defining clear boundaries is hard. Handling the fuzzy middle where categories overlap is hard. And evaluation metrics that look good can hide category-level failures that create real problems.

## What Makes Classification Distinct

Classification assigns input to one or more categories from a predefined set. Unlike generation, which has infinite output space, classification has finite, known categories. Unlike extraction, which pulls information from input, classification assigns labels to input as a whole.

This bounded output space is what makes classification tractable. You know all possible outputs in advance. You can measure how often each category is predicted correctly. You can analyze which categories get confused with which. You can track category distribution over time and detect drift.

The categories might be binary (spam or not spam, safe or unsafe, positive or negative sentiment), multi-class (route to department A, B, C, D, or E), multi-label (tag with all applicable topics from a list), or hierarchical (broad category, then subcategory within that category).

The evaluation is objective once you've defined the categories and labeled your ground truth. Each input either does or doesn't belong to each category. You can measure accuracy, precision, recall, and F1 score with clear mathematical definitions. You can build confusion matrices that visualize classification performance across all categories.

This objectivity is classification's strength. You don't need subjective human judgment for evaluation once you have labeled data. You can track metrics that have established interpretations. You can compare classifiers using the same well-understood metrics.

But the objectivity masks the hard problems: defining the right category set, drawing clear boundaries, handling the fuzzy middle, dealing with category imbalance, and ensuring the classifier learns meaningful patterns rather than superficial correlations.

## Types of Classification Tasks in LLM Systems

Classification takes many forms in production AI systems. Understanding the common patterns helps you recognize the evaluation challenges specific to each.

**Intent classification** determines what the user is trying to do: asking a question, making a request, reporting a problem, providing feedback, or expressing frustration. This is foundational for chatbots, virtual assistants, and conversational systems. The challenge is that the same surface text can represent different intents depending on context. "I can't log in" might be a request for help or a complaint depending on tone and conversation history. Evaluation must account for context-dependent intent.

**Sentiment analysis** classifies text by emotional tone: positive, negative, neutral, or more granular emotions. This is used for review analysis, social media monitoring, and customer feedback processing. The challenge is that sentiment is often mixed or contextual. "This product is expensive but worth it" is both negative (expensive) and positive (worth it) depending on what aspect you're asking about. Evaluation needs to handle mixed sentiment and aspect-based sentiment.

**Content moderation** classifies content as safe or unsafe across multiple dimensions: hate speech, harassment, adult content, violence, misinformation, spam. This is critical for user-generated content platforms. The challenge is that moderation decisions are high-stakes (false positives frustrate users, false negatives create harm), context-dependent (satire versus hate speech), and culturally sensitive. Evaluation must optimize for both precision and recall while considering the relative costs of different error types.

**Topic categorization** assigns content to subject categories: news topics, product categories, support topics, or domain areas. This is used for content organization, search, and recommendation. The challenge is that topics often overlap (a news article might be both "politics" and "healthcare"), and topic boundaries are often arbitrary. Evaluation must handle multi-label classification and category hierarchy.

**Routing classification** determines which downstream system, agent, or workflow should handle a request. This is increasingly important in multi-agent systems where different specialized models or pipelines handle different request types. The challenge is that routing decisions have downstream consequences. A misrouted request might fail or produce poor results even if it eventually gets handled. Evaluation must consider not just routing accuracy but downstream success.

**Priority or urgency classification** determines how quickly something needs attention: urgent, high, medium, low priority, or SLA-based categories. This is used for ticket triage, alert management, and task prioritization. The challenge is that urgency is subjective and context-dependent. What's urgent for one customer might not be for another. Evaluation needs to balance classification accuracy with business impact.

**Entity type classification** determines what type of entity a mention refers to: person, organization, location, product, or domain-specific types. This is often part of larger extraction pipelines. The challenge is that the same text can represent different types ("Apple" as company versus fruit, "Java" as programming language versus location). Evaluation must handle type ambiguity and context-dependent typing.

**Risk or quality classification** assigns risk levels, quality grades, or compliance categories. This is used for content quality scoring, loan risk assessment, or safety classification. The challenge is that these categories often represent continuous gradients that get discretized into bins, and the boundaries are subjective. Evaluation must consider whether boundary cases are correctly handled.

## The Fuzzy Middle Problem

The hardest part of classification isn't the clear cases. It's the boundary cases where categories overlap or where the right classification is ambiguous.

Every category pair has a fuzzy middle: examples that could reasonably be classified either way. Technical support versus product feedback. Positive sentiment versus neutral. Urgent versus high priority. Harassment versus heated disagreement. These boundary cases are where classifiers struggle and where human annotators disagree.

The fuzzy middle creates several problems. First, your training data has inconsistent labels for boundary cases because different annotators make different judgment calls. This confuses the model and makes it harder to learn clear decision boundaries. Second, your evaluation metrics are noisy for boundary cases because the ground truth labels themselves are uncertain. Third, your production performance on boundary cases is unpredictable because the model hasn't learned a consistent pattern for ambiguous inputs.

How you handle the fuzzy middle determines how well your classifier works in practice. You have several options, each with tradeoffs.

**Refine category definitions** to reduce overlap. Instead of "technical support" and "product feedback," define "bug reports," "how-to questions," "feature requests," and "general feedback." More specific categories reduce ambiguity. The tradeoff is that you need more categories, more training data per category, and more complex routing logic.

**Accept multi-label classification** where applicable. Some inputs legitimately belong to multiple categories. Let your classifier assign multiple labels and handle multi-label examples downstream. The tradeoff is that your evaluation becomes more complex and your downstream systems must handle multi-label inputs.

**Add an "ambiguous" or "other" category** for inputs that don't fit cleanly. This acknowledges the fuzzy middle explicitly. The tradeoff is that you need a plan for what to do with ambiguous classifications—often human review.

**Use confidence scores** and threshold them. Your classifier outputs not just a category but a confidence score. Set a confidence threshold: only auto-route high-confidence classifications, send low-confidence cases for human review. The tradeoff is that you reduce automation rate but increase accuracy on auto-routed cases.

**Collect disagreement data** where multiple annotators label the same examples. Learn from the disagreement. Cases where annotators unanimously agree are clear examples. Cases where annotators disagree are fuzzy middle. You can train separate models for clear versus fuzzy cases or use disagreement patterns to identify categories that need definition refinement.

## Evaluation Metrics for Classification

Classification has well-established metrics, but using them correctly requires understanding what they measure and when they're appropriate.

**Accuracy** is the simplest metric: what percentage of predictions are correct? This works when categories are balanced and when all errors are equally bad. It's misleading when categories are imbalanced (99% accuracy on spam detection sounds good until you realize 99% of emails aren't spam) or when different error types have different costs (false positive in content moderation versus false negative).

**Precision** measures what percentage of your positive predictions are correct. High precision means when you classify something as category X, you're usually right. This matters when false positives are costly. In content moderation, falsely flagging safe content frustrates users. In spam detection, falsely flagging legitimate email as spam loses important messages.

**Recall** measures what percentage of the true positives you successfully identify. High recall means you're finding most of the examples that belong to category X. This matters when false negatives are costly. In content moderation, missing harmful content creates safety risks. In fraud detection, missing fraudulent transactions causes financial loss.

**F1 score** balances precision and recall in a single metric. It's the harmonic mean of the two. F1 is useful when you need to balance false positives and false negatives. But remember that F1 weights them equally. If one error type is more costly than the other, use weighted F1 or optimize precision and recall separately.

**Per-class metrics** break down precision, recall, and F1 by category. This is essential for multi-class classification because overall accuracy can hide category-level failures. Your classifier might be great at common categories and terrible at rare ones. Per-class metrics reveal which categories are working and which need improvement.

**Confusion matrix** shows exactly which categories get confused with which. Rows represent true categories, columns represent predicted categories. Cell values show count of examples. The diagonal shows correct classifications. Off-diagonal cells show confusions. This reveals systematic problems like "product feedback" being misclassified as "technical support."

**Macro-averaged metrics** compute metrics per category then average them, giving each category equal weight. This is appropriate when you care about all categories equally regardless of their frequency. Rare categories matter as much as common ones.

**Micro-averaged metrics** pool all examples together then compute metrics, giving each example equal weight. This is appropriate when you care about overall example-level performance. Common categories naturally dominate the metric.

**Weighted metrics** average per-category metrics weighted by category frequency. This balances category-level and example-level perspectives.

**ROC curves and AUC** for binary classification show tradeoffs between true positive rate and false positive rate at different thresholds. This is useful when you need to choose an operating point based on cost tradeoffs between error types.

**Calibration metrics** measure whether predicted probabilities match actual frequencies. A well-calibrated classifier that predicts 70% probability should be correct 70% of the time. Calibration matters when you use confidence scores for thresholding or when you expose probabilities to users.

## Ground Truth Collection for Classification

High-quality ground truth is critical for classification because your model learns directly from labeled examples and your evaluation depends entirely on label quality.

The collection process requires clear category definitions with examples and edge case handling. Your guidelines should explain each category, give example inputs that belong to the category, give example inputs that don't belong but might be confused with it, and explain how to handle common ambiguous cases.

You need annotators who understand the domain and the category distinctions. Intent classification for a technical product requires understanding of user needs and product features. Content moderation requires understanding of policy nuances and cultural context. Medical text classification requires medical knowledge. Don't cut corners on annotator quality.

Measure inter-annotator agreement to assess label quality. Cohen's kappa or Fleiss' kappa quantify agreement correcting for chance. High agreement means your categories are well-defined and annotators apply them consistently. Low agreement means your categories are ambiguous or your guidelines are unclear. Investigate disagreements and refine guidelines.

Handle disagreements systematically. When annotators disagree, don't just pick one label. Understand why they disagreed. Is the example genuinely ambiguous? Did one annotator misunderstand the guidelines? Does this reveal a needed category refinement? Resolve disagreements through discussion and consensus, or through expert adjudication.

Ensure category coverage and balance. You need enough examples of each category to train and evaluate reliably. Rare categories need deliberate collection to ensure adequate representation. Extreme imbalance (99% one category, 1% another) makes both training and evaluation harder.

Collect challenging examples deliberately. Don't just sample randomly. Actively seek out boundary cases, ambiguous examples, and cases where surface patterns might mislead. These hard examples are where your classifier will fail in production. Your ground truth needs to test them.

## Routing as a Special Case

Routing classification deserves special attention because it's increasingly central to modern AI architectures. Multi-agent systems, specialized model selection, workflow routing, and escalation decisions all depend on classification to determine which downstream system handles each request.

Routing is special because classification accuracy isn't the only thing that matters. What matters is downstream success: does the request get handled correctly after routing?

A perfectly accurate router that always selects the right specialized model is ideal. But a router with 95% accuracy might still be valuable if the 5% misrouted requests fail gracefully or get corrected through fallback mechanisms. Conversely, a 95% accurate router is disastrous if misrouted requests create cascading failures.

This means routing classification needs multi-level evaluation. First-level evaluation measures routing accuracy: does the classifier pick the right destination? Second-level evaluation measures downstream success: do routed requests get handled correctly? Third-level evaluation measures recovery: when routing fails, do fallback mechanisms catch it?

You also need to evaluate routing latency and cost. If your router is a complex model that takes 500ms to classify while your downstream handlers take 100ms to process, you've created a bottleneck. If your router calls an expensive API, you're adding cost to every request. Routing evaluation must balance accuracy, latency, and cost.

In 2026, routing has become a critical capability in production AI systems. The trend toward specialized models and multi-agent architectures means that intelligent routing determines whether systems work well or fail. A great router enables using specialized, efficient models for each request type. A poor router undermines the entire architecture.

## The 2026 Context: Classification as System Backbone

Classification in 2026 serves as the backbone of intelligent routing in multi-agent systems, content moderation at scale, and automated triage across enterprise workflows. The use case isn't just "categorize this item" in isolation. It's "make the first decision that determines everything downstream."

This elevates the stakes for classification evaluation. A misclassified item is annoying in isolation. A systematically biased classifier that misroutes 8% of a category is an operational crisis. A classifier that drifts over time and stops catching a once-rare-now-common category is a silent system degradation.

Your classification evaluation in 2026 needs to operate at multiple timescales. Real-time evaluation catches immediate failures. Daily evaluation tracks category distribution and confusion patterns. Weekly evaluation compares to historical baselines to detect drift. Monthly evaluation validates that the category system still matches the actual distribution of inputs.

You need evaluation that goes beyond batch accuracy. Test edge cases deliberately. Test boundary cases between similar categories. Test distribution shift by evaluating on data from different time periods or user segments. Test adversarial inputs designed to confuse the classifier.

The opportunity is that classification is well-understood, has solid metrics, and is easier to get right than generation or complex reasoning. The challenge is that in multi-agent architectures, classification failures multiply through the system. A misrouted request hits the wrong specialized model, produces poor output, frustrates the user, and wastes resources.

Getting classification right means defining categories that match your operational reality, collecting high-quality labeled data that represents real input distribution including edge cases, choosing metrics that match your error cost profile, and building monitoring that catches both immediate failures and gradual drift.

When you get it right, classification becomes a reliable foundation for complex AI systems. You can route with confidence, specialize models effectively, triage at scale, and build systems where each component handles what it does best.

That foundation, combined with the evaluation approaches we've covered for generation, extraction, and classification, gives you the toolkit to evaluate most AI product tasks you'll encounter. What remains is understanding how these task types combine in real systems and how to evaluate the complex reasoning that ties them together—which is where we head next.
