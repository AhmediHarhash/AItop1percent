# 6.4 — Building Your First Eval Cases from I/O Specs

In the spring of 2024, a team building a document analysis system asked me: "We have all the framing artifacts. We know our tasks, our eval strategies, our ground truth requirements. Now what? How do we actually build test cases?" They were staring at a sixty-page problem framing document and did not know where to start. I told them: start with your input variability catalogue and your output specifications. Everything you need to build your first test cases is already in those artifacts. They just needed to know how to translate them.

Three weeks later, they had built 120 test cases covering happy paths, edge cases, adversarial inputs, and failure modes. They ran their first evaluation and caught fifteen bugs before the system ever reached users. The test cases came directly from the framing work they had already done. No guessing. No wasted effort. Just systematic translation from problem definition to test coverage.

This is what happens when you build eval cases from I/O specs instead of making them up as you go.

## The Minimum Viable Eval Set

Before diving into test case construction, you need to understand what you are building toward. The minimum viable eval set is the smallest collection of test cases that gives you meaningful signal about whether your system works.

It is not comprehensive. It is not perfect. It is the starting point that lets you evaluate the system well enough to iterate. You will expand it over time as you discover new edge cases and failure modes, but you need the core set first.

The minimum viable eval set has five categories of test cases: happy path cases, edge cases, adversarial cases, regression cases, and negative cases. Each serves a distinct purpose.

Happy path cases validate that the system works for standard, expected inputs. These are the scenarios that should always succeed. If your system is extracting invoice numbers from invoices, the happy path cases are well-formatted invoices with clearly visible invoice numbers in standard locations. If your system is generating customer support responses, the happy path cases are common questions with straightforward answers.

Edge cases validate that the system handles unusual but legitimate inputs. These are scenarios that stretch the boundaries of normal operation but should still work. Invoices with invoice numbers in unexpected locations. Questions phrased in unusual ways. Inputs at the extremes of expected variation.

Adversarial cases validate that the system resists malicious or manipulative inputs. These are scenarios where someone is deliberately trying to break the system or make it produce harmful outputs. Prompt injection attacks. Inputs designed to trigger unsafe behavior. Questions designed to extract sensitive information.

Regression cases validate that changes to the system do not break previously working functionality. These start empty—you have not shipped yet, so you have no prior version to regress from. But you reserve slots for them because production will generate failures that need to become permanent test cases.

Negative cases validate that the system correctly refuses, escalates, or expresses uncertainty when appropriate. These are scenarios where the correct behavior is not to give an answer. Questions the system should not answer. Requests that should trigger refusal. Situations where the system should admit it does not know.

A good starting distribution for version one: 40 percent happy path, 30 percent edge cases, 15 percent adversarial, 0 percent regression (reserved for later), 15 percent negative cases. Adjust based on your risk profile. High-risk systems need more adversarial and negative coverage.

## Deriving Happy Path Cases from Core I/O Specs

Your input and output specifications already define the happy path. The core I/O spec describes the standard case: here is what normal input looks like, here is what correct output looks like. Turn these into test cases by instantiating concrete examples.

Start with the input specification. What does a standard input look like? If your system processes customer questions, what are the most common question types? If your system extracts data from forms, what does a typical form contain?

For each standard input type, create an example. Do not just write "a customer question about account balance." Write an actual question: "What is my current account balance?" Do not just write "a form with name and address." Create an actual form with realistic data.

Then define the expected output for each input. What should the system produce? If the input is "What is my current account balance?" the output might be "Your current balance is 1,247.83 dollars." If the input is a form, the output is the extracted structured data with all fields populated.

The expected output should be specific enough that you can check whether the system produced it. For tasks with exact answers, the expected output is exact: "1,247.83." For tasks with acceptable ranges, the expected output is a range: "between 1,000 and 2,000 dollars." For tasks with multiple acceptable outputs, the expected output is a set of acceptable variations.

Your happy path test cases should cover the major scenarios your system handles. If you defined ten task types in your registry, you need happy path cases for each type. If a task type has variants—like different question formats or different document layouts—you need cases covering the major variants.

A practical target: 20-30 happy path cases for a system with moderate complexity. Fewer if the system is simple and homogeneous. More if the system handles many distinct scenarios.

## Deriving Edge Cases from Input Variability Catalogue

Your input variability catalogue from earlier in problem framing is a gold mine for edge case generation. You already documented how inputs can vary: by length, format, language, domain, context, user type, complexity, and quality. Each dimension of variability suggests edge cases.

If length varies, create test cases at the extremes: very short inputs, very long inputs, inputs at the maximum length you will accept. If your system processes questions, include a one-word question and a 500-word question. If your system processes documents, include a one-page document and a 100-page document.

If format varies, create test cases for each format variant. Inputs with different punctuation styles. Inputs with mixed capitalization. Inputs with special characters. Inputs with formatting artifacts like stray line breaks or extra spaces. These are annoying details that break fragile systems.

If language varies, create test cases for language edge cases. Inputs in languages you do not officially support but might encounter. Inputs mixing multiple languages. Inputs with non-Latin scripts. Inputs with ambiguous language that could be interpreted multiple ways.

If domain varies, create test cases for domain boundary conditions. Inputs that blend multiple domains. Inputs that use domain-specific jargon you might not recognize. Inputs that reference concepts outside your training distribution.

If complexity varies, create test cases for high-complexity inputs. Questions with multiple sub-questions. Documents with complex nested structures. Inputs that require multi-hop reasoning. Inputs with ambiguous or contradictory information.

If quality varies, create test cases for low-quality inputs. Inputs with typos and grammatical errors. Inputs with missing information. Inputs that are unclear or poorly phrased. Real users produce messy inputs; your eval set should include them.

Each dimension of variability from your catalogue translates into a category of edge cases. The variability catalogue tells you where to look. You just need to instantiate concrete examples for each category.

For edge cases, the expected output might be "system handles this gracefully" rather than a specific correct answer. The goal is to verify that unusual inputs do not cause crashes, errors, or bizarre behavior. Graceful degradation is success.

## Deriving Adversarial Cases from Threat Model

Your threat model from early in problem framing identified how the system could be attacked or misused. Each threat suggests adversarial test cases.

If prompt injection is a threat, create test cases with prompt injection attempts. Inputs that try to override system instructions. Inputs that try to trick the system into ignoring constraints. Inputs that try to make the system reveal its prompt.

If harmful content generation is a threat, create test cases that attempt to elicit harmful outputs. Requests for illegal advice. Requests for content that violates policies. Requests designed to bypass safety filters.

If data extraction is a threat, create test cases that try to make the system reveal sensitive information. Questions probing for details about other users. Questions trying to infer system internals. Questions designed to leak training data.

If bias exploitation is a threat, create test cases that probe for biased behavior. Inputs designed to trigger demographic bias. Inputs testing for stereotype reinforcement. Inputs that could reveal unfair treatment of different user groups.

For adversarial cases, the expected output is usually refusal, error, or safe fallback. The system should not comply with adversarial requests. It should not produce harmful content. It should not leak sensitive information. A successful adversarial test is one where the system correctly identifies and rejects the attack.

Adversarial cases are especially important for high-risk systems. If your system handles medical advice, financial guidance, or content moderation, adversarial coverage should be much higher than 15 percent. You need to stress-test the safety boundaries thoroughly.

## Deriving Negative Cases from Success Criteria

Your success criteria include not just what the system should do but what it should not do. The negative success criteria from earlier in framing tell you where the system should refuse to act or admit uncertainty.

If your success criteria include "does not answer questions outside the defined scope," create test cases for out-of-scope questions. Questions about topics the system is not designed to handle. Questions that require information the system does not have access to. Questions that are technically in-domain but too complex or nuanced for the system to answer safely.

If your success criteria include "escalates to human when uncertain," create test cases for high-uncertainty scenarios. Ambiguous questions with multiple valid interpretations. Questions that require judgment calls. Questions where the available information is incomplete or contradictory.

If your success criteria include "refuses requests that violate policy," create test cases for policy violations. Requests the system should not fulfill. Questions the system should not answer. Scenarios where the correct response is "I cannot help with that."

For negative cases, the expected output is not a helpful answer. It is a refusal, escalation, or expression of uncertainty. "I don't have enough information to answer that safely." "This question requires human review." "I cannot assist with that request." These are correct outputs for negative cases.

Negative cases prevent the system from being overconfident or overreaching. They establish boundaries. A system that tries to answer every question will produce wrong answers. A system that knows when to say no is more reliable.

## The Anatomy of a Good Eval Case

What does a well-written test case look like? It has four components: input, expected output, evaluation criteria, and rationale.

The input is the exact input you will feed to the system. Not a description of the input—the literal input. If the input is a question, write the exact question. If the input is a document, include the actual document or a realistic sample.

The expected output is what the system should produce. For tasks with exact answers, this is the exact answer. For tasks with acceptable ranges, this is the range or a set of acceptable outputs. For tasks requiring judgment, this is a rubric or criteria for evaluating the output.

The evaluation criteria specify how to check whether the system output matches the expected output. For exact match tasks: does the output exactly match? For semantic match tasks: does the output convey the same meaning? For quality assessment tasks: does the output meet the quality rubric?

The rationale explains why this test case matters. What scenario does it represent? What failure mode does it guard against? What requirement from the framing document does it validate? The rationale is not for the system—it is for the humans maintaining the test set. It prevents test cases from becoming inscrutable over time.

A complete test case looks like this:

**Test Case 47: Out-of-scope question with plausible context**

**Input:** "What should I do if my cat is sneezing?"

**Expected Output:** Refusal with explanation. Example: "I'm designed to help with health insurance questions, not pet health. For veterinary advice, please consult a licensed vet."

**Evaluation Criteria:** Output must (1) refuse to answer, (2) explain why refusal is appropriate, (3) suggest alternative resource if available.

**Rationale:** Validates that system respects scope boundaries even when question is phrased naturally and could be misinterpreted as in-scope (health-related question). Guards against scope creep and overconfidence.

This format makes test cases self-documenting. A new team member can read the test case and understand what it checks and why it matters.

## How Many Cases for Version One?

For a minimum viable eval set, aim for 50-100 test cases. This is enough to cover major scenarios without drowning in test maintenance. Smaller systems with narrow scope can go lower. Larger systems with high variability should go higher.

The distribution I recommend for version one: 20 happy path cases covering core functionality, 15 edge cases from your variability catalogue, 10 adversarial cases from your threat model, 5 negative cases from your refusal criteria, and 0 regression cases with 10 slots reserved.

That is 50 cases to start. You can build this in a week if you have good framing artifacts. Each artifact—task registry, I/O specs, variability catalogue, threat model, success criteria—directly generates a category of test cases.

As you iterate, the test set grows. Production failures become regression cases. User feedback reveals new edge cases. Red team exercises uncover new adversarial vectors. After six months, you might have 300 cases. After a year, 500. But you start with 50.

Starting small has advantages. It is easy to review and maintain. It runs fast. It gives signal without overwhelming you with noise. You can iterate on the test format and tooling without managing a massive test set. Once the foundation is solid, scaling up is straightforward.

## The Eval Case as the Atomic Unit of Quality Assurance

Each test case is a permanent record of a requirement. It says: "This input should produce this output, and here is why that matters." Over time, your test set becomes institutional memory. It captures all the edge cases you have discovered, all the failure modes you have fixed, and all the requirements stakeholders have agreed to.

When someone asks "Does the system handle X?" you check the test set. If there is a test for X, you know the answer. If there is not, you add a test. The test set is the source of truth for what the system is supposed to do.

When you onboard new team members, you show them the test set. It teaches them what the system does, what edge cases matter, and what quality standards apply. It is more concrete than documentation and more comprehensive than examples.

When you make changes to the system, you run the test set. If tests that previously passed now fail, you have introduced a regression. If tests that previously failed now pass, you have fixed a bug. The test set is your safety net against accidental breakage.

Building your first eval cases from I/O specs is not busy work. It is translating your problem framing into executable quality assurance. The next section covers how to prioritize which tasks to evaluate first when you cannot evaluate everything at once.
