# Chapter 6 — From Framing to Evaluation Readiness

You've decomposed the problem. You've classified the tasks. You've defined success criteria and I/O specs. Now the question is: **is all of this actually usable by the teams who build evaluations, define ground truth, and monitor production?**

The handoff from problem framing to evaluation is where most teams drop the ball. The framing team produces beautiful documents that the eval team can't translate into test cases. The success criteria are clear in English but nobody knows how to measure them. The I/O specs describe what should happen but not what events to log so you can tell if it's happening.

This chapter is the bridge. It takes everything from Chapters 1 through 5 and packages it into artifacts that Section 3 (Evaluation Strategy) and Section 4 (Ground Truth) can consume directly. It covers the handoff checklist, task-to-eval mapping, data feasibility, security review, instrumentation planning, and the team exercise that catches framing gaps before they become production failures.

---

## What This Chapter Covers

- **6.1** — The Framing-to-Eval Handoff
- **6.2** — Mapping Tasks to Evaluation Strategies
- **6.3** — Mapping Tasks to Ground Truth Requirements
- **6.4** — Building Your First Eval Cases from I/O Specs
- **6.5** — Priority Ordering: Which Tasks to Evaluate First
- **6.6** — The Task Complexity Matrix
- **6.7** — Framing Documentation Templates
- **6.8** — Common Framing Failures That Surface During Evaluation
- **6.9** — The Framing Review: A Team Exercise
- **6.10** — Data Feasibility Checklist: Do We Have the Inputs, Logs, and Labels
- **6.11** — Security Review in the Framing Handoff
- **6.12** — Instrumentation Plan: Events and Logs for Eval and Monitoring

---

*Let's start with what the evaluation team actually needs from you — and why most handoffs fail.*
