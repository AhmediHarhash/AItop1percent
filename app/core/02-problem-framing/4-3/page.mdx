# 4.3 — Behavioral Success Criteria

The banking chatbot passed all its functional tests with flying colors. It accurately retrieved account balances, correctly processed transaction inquiries, and extracted dates and amounts from user requests with 95% precision. The engineering team felt confident. The system worked.

Two days after launch, customer complaints flooded in. Not about incorrect information—the facts were right. The complaints were about how the bot communicated. When customers expressed frustration about fees, the bot responded with technically correct but tone-deaf explanations that felt dismissive. When elderly customers asked for help with unfamiliar terms, the bot used jargon they did not understand. When someone mentioned financial stress, the bot provided account details without acknowledging the emotional context. One viral tweet captured the problem: "Your bot is technically correct and completely inhuman."

The system was functionally successful but behaviorally broken. It gave right answers in the wrong way. The team had spent months optimizing for accuracy but had never defined how the system should communicate—the tone it should strike, the style it should adopt, the situations where it should escalate rather than respond. They learned an expensive lesson: behavioral criteria matter as much as functional criteria, sometimes more.

## Why Behavioral Criteria Are Different

Behavioral success criteria define how your AI system should behave beyond mere correctness. They cover tone, style, safety, brand alignment, and persona adherence. They are the criteria that determine whether users feel good about interacting with your system, even when it gives them the information they asked for.

The fundamental difference between functional and behavioral criteria is that functional criteria focus on what is said while behavioral criteria focus on how it is said. A response can be factually perfect but behaviorally wrong. Consider a customer service scenario where a user asks about a denied claim. Three responses might all be functionally equivalent:

"Your claim was denied because you did not submit the required documentation by the deadline specified in Section 4.2 of the policy."

"I see your claim was denied. I know this is frustrating. The denial was based on missing documentation that needed to be submitted by the policy deadline. Let me explain what documents were missing and whether there is a process to appeal."

"Claim denied. Missing docs. Read your policy."

All three communicate the same core information, but they create entirely different user experiences. The first is cold and bureaucratic. The second is empathetic and helpful. The third is curt to the point of rudeness. If your functional criteria only specified "must correctly communicate claim status and reason," all three responses pass. If you want the second response rather than the first or third, you need behavioral criteria.

Behavioral criteria are inherently harder to specify than functional criteria. Correctness can often be objectively verified. Tone is more subjective. What feels professional to one user might feel cold to another. What feels friendly to one demographic might feel unprofessional to another. This subjectivity makes behavioral criteria challenging, but it does not make them optional. The challenge is to make behavioral expectations as explicit and measurable as possible.

## Types of Behavioral Criteria

**Tone**

Tone encompasses the emotional quality of the communication. Is the system professional, friendly, empathetic, neutral, enthusiastic, or something else? Tone criteria specify what emotional register the system should adopt and how that register should shift based on context.

For a healthcare assistant, tone criteria might specify: responses should be empathetic and reassuring for symptom-related queries, professional and precise for clinical information, and supportive but not patronizing for lifestyle advice. When users express worry or anxiety, the system should acknowledge the emotion before providing information.

For a technical support system, tone criteria might specify: responses should be patient and encouraging for basic questions, professional and efficient for experienced users, and apologetic when acknowledging system failures.

The key to effective tone criteria is context-sensitivity. Tone should not be monolithic across all interactions. A single "friendly" tone applied universally will feel inappropriate in serious situations. Criteria should specify how tone adapts to user needs, query types, and emotional context.

**Style**

Style encompasses the structural and rhetorical characteristics of communication. Is the system concise or detailed? Formal or casual? Does it use technical language or plain English? Does it employ lists and structure or flow in paragraphs?

Style criteria should align with user needs and brand expectations. A legal AI should trend formal and detailed. A consumer app should trend casual and concise. A teaching assistant should trend explanatory and structured.

For a financial advisory assistant, style criteria might specify: responses should be clear and jargon-free for retail customers but can include technical terminology for accredited investors; explanations should use concrete examples rather than abstract concepts; when discussing complex topics, use numbered lists to break down information into digestible steps.

For a creative writing assistant, style criteria might specify: suggestions should be specific rather than generic; feedback should include examples from the user's text; when multiple approaches are possible, present options rather than prescribing a single solution; maintain an encouraging tone that respects the user's creative vision.

Style criteria should also address structural consistency. Should the system always organize information the same way? Should summaries always start with the most important point? Should explanations always include examples? These patterns create a predictable, learnable experience for users.

**Safety**

Safety criteria define boundaries around harmful content, bias, privacy, and security. They specify what the system must never produce and how it should handle risky situations.

Safety criteria typically include:

No harmful content: The system must not generate content that could cause physical, emotional, or financial harm. This includes no dangerous instructions, no content that promotes self-harm, no advice that could lead to serious consequences if followed incorrectly.

No biased outputs: The system must not produce responses that discriminate based on protected characteristics or reinforce harmful stereotypes. This requires both explicit filtering and ongoing evaluation for subtle bias.

No privacy violations: The system must not disclose personally identifiable information, reveal data about users who are not the current user, or leak training data that should remain confidential.

No unauthorized actions: The system must not take actions beyond its authorized scope, commit the user to agreements without explicit consent, or access resources it should not access.

For a hiring assistant, safety criteria might specify: the system must not generate interview questions or evaluation criteria that correlate with protected characteristics; when describing candidates, it must focus on job-relevant qualifications and experience without referencing demographic attributes; it must not make final hiring decisions but only provide information to support human decision-making.

For a mental health chatbot, safety criteria might specify: the system must not provide clinical diagnoses; when users express thoughts of self-harm, the system must immediately provide crisis resources and offer to connect them with a human; the system must not store or repeat sensitive information shared by users in ways that could compromise privacy.

Safety criteria are unique in that they often represent hard boundaries rather than targets to optimize toward. A system that is helpful 95% of the time but occasionally generates dangerous advice has failed on safety, regardless of how well it performs on other criteria.

**Brand Alignment**

Brand alignment criteria ensure that the AI system reflects the organization's values, voice, and positioning. This is particularly important for customer-facing AI where the system represents the brand.

Brand alignment includes vocabulary choices, values expression, and positioning. Does your brand use industry jargon or plain language? Is your brand voice authoritative or conversational? Does your brand emphasize innovation, reliability, expertise, accessibility, or other qualities?

For a luxury brand's virtual shopping assistant, brand alignment criteria might specify: use elegant, elevated language without being pretentious; emphasize quality, craftsmanship, and heritage; recommend products that align with the user's style rather than pushing sales; maintain an exclusive feel while remaining welcoming.

For a budget airline's booking assistant, brand alignment criteria might specify: be direct and no-nonsense about what is included and what costs extra; emphasize value and transparency; use straightforward language without corporate speak; maintain a friendly but efficient tone that respects that customers chose the brand for practical reasons.

Brand misalignment creates cognitive dissonance for users. If your brand promises white-glove service but your chatbot is curt and transactional, users notice the disconnect and lose trust. Behavioral criteria must bridge the brand promise and the actual experience.

**Persona Adherence**

When your AI adopts a specific persona—a tutor, a coach, a specialist, a character—persona adherence criteria define how consistently it maintains that role.

Persona adherence includes staying in character, maintaining consistent knowledge boundaries, and exhibiting consistent personality traits. A financial advisor persona should not suddenly reference topics outside finance. A beginner-friendly tutor should not occasionally lapse into expert-level jargon. A formal assistant should not switch to slang.

For an executive coach chatbot, persona criteria might specify: maintain the perspective of an experienced business leader; ask probing questions rather than immediately providing answers; reference relevant frameworks and models that coaches commonly use; maintain consistency in the level of directness and challenge across conversations.

For a historical figure educational assistant that adopts the persona of a scientist, persona criteria might specify: explain concepts using knowledge and terminology available in the figure's era; reference the figure's actual work and discoveries; maintain the figure's documented communication style; acknowledge limitations of knowledge from that time period.

Persona adherence is particularly important for AI systems designed for repeated interaction. Users develop expectations about how the system will behave, and consistency builds trust. Inconsistent persona creates confusion and breaks immersion.

## Making Behavioral Criteria Measurable

The challenge with behavioral criteria is translating subjective qualities like "empathetic" or "professional" into something you can actually measure and test against.

**Rubrics with Examples**

The most effective approach is to create detailed rubrics that break down each behavioral dimension into concrete characteristics, then anchor those characteristics with specific examples.

A tone rubric for empathy might include:

Score 1: Response ignores emotional content of the user's message entirely. Provides information without acknowledgment of user's feelings.

Example: User says "I am really worried about this diagnosis." System responds "Your test results show elevated glucose levels. Normal range is 70-100 mg/dL."

Score 3: Response acknowledges emotion but in a formulaic way. Includes pro forma empathy statement before information.

Example: User says "I am really worried about this diagnosis." System responds "I understand this is concerning. Your test results show elevated glucose levels, which indicates prediabetes."

Score 5: Response meaningfully acknowledges emotion and integrates empathy throughout the information provided. Balances validation with helpful information.

Example: User says "I am really worried about this diagnosis." System responds "I can understand why this result would be worrying. The good news is that prediabetes is very manageable with lifestyle changes, and many people successfully prevent progression to diabetes. Let me explain what these results mean and what steps you can take."

With this rubric, human raters can evaluate responses consistently. When you specify that 85% of responses must score 4 or 5 on empathy for emotionally charged queries, you have created a measurable behavioral criterion.

**Anchoring Scales with Concrete Samples**

Rubrics become more reliable when they include multiple concrete examples at each score level, ideally drawn from actual system outputs or realistic scenarios. This anchoring helps raters calibrate their judgments and reduces subjectivity.

For a formality criterion, you might provide examples:

Too informal: "Hey, so that charge is just our standard fee LOL. It happens every month."

Appropriate casual: "That charge is our standard monthly fee. It is automatically applied to all accounts."

Appropriate formal: "The charge in question is the standard account maintenance fee, which is applied monthly in accordance with the terms outlined in your account agreement."

Too formal: "The aforementioned pecuniary assessment represents the standardized account maintenance fee, applied with monthly periodicity pursuant to the governing contractual instrument."

By showing the range and specifying which level is appropriate for your use case, you give evaluators clear guidance.

## Evaluation Approaches for Behavioral Criteria

**Human Evaluation with Calibrated Raters**

The gold standard for behavioral evaluation is human judgment, but it requires careful implementation. Effective human evaluation uses:

Calibrated raters: Evaluators are trained on your rubrics and tested for consistency. They review example cases together to align their interpretations.

Multiple raters per item: Each response is evaluated by at least two, preferably three raters. Majority voting or averaging reduces individual bias.

Regular calibration sessions: Raters meet periodically to discuss edge cases and maintain alignment.

Clear evaluation guidelines: Raters have written instructions that cover not just the rubrics but also how to handle ambiguous cases.

For critical behavioral criteria, especially around safety or brand alignment, human evaluation is essential. The cost is higher than automated evaluation, but the accuracy is worth it for high-stakes applications.

**LLM-as-Judge with Detailed Rubrics**

An increasingly viable approach is using a strong language model to evaluate behavioral criteria. The model is given the user query, the system response, and a detailed rubric, then asked to score the response.

This approach works surprisingly well when:

The rubric is highly detailed and includes examples at each score level.

The evaluation model is stronger than the model being evaluated.

The evaluation focuses on relatively objective aspects of behavior like structure, completeness of acknowledgment, or presence of specific elements.

The evaluation is calibrated against human judgments on a sample to ensure alignment.

LLM-as-judge is particularly useful for high-volume evaluation during development and for continuous monitoring post-launch. It allows you to evaluate thousands of responses for behavioral criteria that would be prohibitively expensive to human-rate.

However, LLM-as-judge should be validated against human evaluation, especially for nuanced criteria like empathy or appropriateness in sensitive contexts. Use LLM evaluation for speed and scale, but ground-truth it with human evaluation to ensure reliability.

**User Satisfaction as Proxy Metric**

For some behavioral criteria, direct user feedback serves as the ultimate measure. If your criterion is "users should feel the system understands them," you can measure this through satisfaction surveys, follow-up questions, or implicit signals like conversation length and user return rate.

Proxy metrics are valuable but should supplement, not replace, direct evaluation. User satisfaction is influenced by many factors beyond behavioral quality—functional correctness, speed, novelty effects, user expectations. You need both: direct measurement of behavioral criteria through human or LLM evaluation, and proxy metrics that tell you whether the criteria you chose actually correlate with user satisfaction.

## Why Behavioral Criteria Often Matter More Than Functional Criteria for Retention

An underappreciated truth about AI products is that behavioral criteria often have a larger impact on user retention than functional criteria, particularly once functional performance crosses a certain threshold.

If your system is functionally terrible—wrong answers, hallucinations, missing information—users will leave regardless of how good the tone is. But once you reach functional competence, behavior becomes the primary differentiator.

Consider two customer service chatbots. Both answer questions correctly 85% of the time. One uses a warm, empathetic tone, acknowledges frustration, and explains clearly. The other is curt, uses confusing jargon, and provides correct information in an unhelpful format. The second bot might technically be slightly more accurate, but users will prefer the first and return to it more often.

This dynamic is particularly strong in domains where emotional connection matters: healthcare, mental health, education, customer service, coaching, and personal finance. In these domains, users are not just seeking information—they are seeking an experience that feels supportive, respectful, and aligned with their needs.

Behavioral criteria also have disproportionate impact on viral moments, both positive and negative. A system that occasionally gives wrong answers but does so politely rarely generates outrage. A system that gives correct answers in a tone-deaf or offensive way generates headlines. Brand damage comes more often from behavioral failures than functional ones.

This means that while you must meet functional criteria to be viable, you must excel on behavioral criteria to be loved. Functional criteria determine whether users can use your system. Behavioral criteria determine whether they want to.

With both functional and behavioral criteria defined, you have specified what your system should do and how it should do it. But these criteria live within constraints—performance boundaries that determine whether your technically and behaviorally sound system is actually shippable. That is the domain of performance success criteria, where we turn our attention next.

