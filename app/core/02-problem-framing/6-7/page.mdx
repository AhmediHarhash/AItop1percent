# 6.7 — Framing Documentation Templates

In March 2025, a major healthcare company launched an internal initiative to standardize how AI teams documented their problem framing. Before this, every team had their own approach. The chatbot team used Google Docs with informal notes. The radiology assistant team had Notion pages with detailed medical specifications. The billing automation team kept everything in Jira tickets. When leadership asked for a portfolio review of all active AI projects, it took three weeks just to compile the information into a comparable format. Nobody could answer basic questions like "which teams have defined success criteria?" or "which projects have security reviews?" because every team spoke a different documentation language.

The company brought in a consulting firm to create standardization. The consultants produced a 47-page framing template with 200 fields. Teams hated it. Filling out the template took longer than doing the actual framing work. Within two months, teams were copying and pasting placeholder text just to get through reviews. The template became a compliance exercise, not a thinking tool.

The problem was not that templates are bad — it is that most organizations get templates wrong. They either have no templates, which creates chaos, or they have overly complex templates, which creates busywork. The right approach is a **minimal set of templates that capture essential decisions** without drowning teams in documentation overhead. This subchapter walks through the five core templates that every AI team needs for framing, what each contains, who owns it, and how they interconnect.

## Why Templates Matter: Consistency Without Bureaucracy

Templates serve two purposes. First, they ensure **completeness**. When you have a standard checklist, you do not forget critical pieces. Second, they enable **comparability**. When every team uses the same format, leadership can review ten projects and understand the risk profile, resource needs, and evaluation readiness across the portfolio.

The key is to keep templates lightweight. A good template is a structured prompt, not a form to fill out. It reminds you what questions to answer without dictating how to answer them. The best templates are two to four pages, not twenty.

Here are the five core framing templates, in the order teams typically create them.

## Template 1: Problem Framing Spec

**Purpose**: Document the high-level problem definition, business context, user needs, constraints, and stakeholder alignment before any technical work begins.

**Who creates it**: Product lead or technical product manager, in collaboration with engineering and domain experts.

**Who reviews it**: All key stakeholders: engineering, domain experts, legal, leadership.

**When it is created**: At the beginning of the framing process, before task decomposition.

**What it contains**:

**Problem Statement**: One paragraph describing the problem you are solving. Not the solution, the problem. "Our support team spends 40% of their time answering repetitive billing questions, which delays responses to complex issues and increases customer wait times." Not "We need a chatbot."

**Business Context**: Why this problem matters to the business. Revenue impact, cost savings, strategic alignment, competitive pressure, regulatory requirement. "Reducing support load by 30% saves 2 million dollars per year in staffing costs and improves NPS by reducing wait times."

**User Needs**: Who are the users, what do they need, and how does solving this problem serve them? Users may be customers, internal employees, or both. "Support agents need to focus on complex issues that require empathy and judgment. Customers need faster responses to routine questions."

**Constraints**: Hard boundaries that limit your solution space. Budget, timeline, technology stack, data availability, regulatory requirements, existing systems you must integrate with. "Must integrate with Salesforce. Cannot store PII outside the US. Must support real-time responses under 2 seconds. Budget is 500k for year one."

**Non-Goals**: Explicit scope exclusions. What you are NOT trying to do, even if it seems related. This prevents scope creep. "Non-goals: handling sales inquiries, escalating to phone support, replacing the existing knowledge base."

**Threat Model Summary**: High-level security and safety concerns that will be expanded later. Prompt injection risk, data leakage risk, misuse vectors, regulatory compliance requirements. "Primary threats: customer input may contain injection attempts, system must not leak other customers' data, must comply with GDPR right to explanation."

**Stakeholder Sign-Off**: Names and dates. Product lead, engineering lead, domain expert lead, legal representative, leadership sponsor. This is not a formality — it is a commitment. "Signed off by: Jane Doe (Product), John Smith (Engineering), Dr. Lee (Medical Director), Sarah Johnson (Legal), Tom Brown (VP of Operations), 2025-03-15."

**How it stays current**: The problem framing spec is usually stable once signed off, but it gets updated if scope changes, constraints shift, or new stakeholders get involved. The DRI owns keeping it current and re-socializing changes.

## Template 2: Task Registry

**Purpose**: A structured list of all AI tasks in the system, with metadata for each task that enables tracking and evaluation planning.

**Who creates it**: Engineering lead, with input from domain experts on task definitions.

**Who reviews it**: Domain experts verify task definitions, product verifies scope, security reviews task risk tiers.

**When it is created**: After problem framing is complete and during task decomposition.

**What it contains**:

A table with one row per task. Columns include:

**Task ID**: Unique identifier. "TASK-001"

**Task Name**: Descriptive name. "Extract customer intent from support message"

**Task Type**: Classification, generation, extraction, ranking, search, tool use, reasoning, etc. This comes from your task taxonomy.

**Risk Tier**: Low, medium, high, critical. This comes from your risk assessment.

**Owner**: The person responsible for defining success criteria and evaluation approach for this task. Usually a domain expert or senior engineer.

**Input Schema Reference**: Link to the I/O specification for this task.

**Output Schema Reference**: Link to the I/O specification for this task.

**Success Criteria Reference**: Link to the success criteria matrix entry for this task.

**Evaluation Approach**: High-level plan. "LLM-as-judge for intent classification, human review sample for edge cases."

**Dependencies**: Which other tasks does this depend on? If task B consumes the output of task A, note that.

**Status**: Framed, in development, in evaluation, in production.

The registry is a living document. As you decompose tasks further or discover new tasks during development, you add rows. As tasks move through the lifecycle, you update status.

**How it stays current**: The engineering lead or DRI updates the registry weekly during active development. In production, it is updated when tasks are added, changed, or deprecated.

## Template 3: Success Criteria Matrix

**Purpose**: For each task in the registry, define specific, measurable, testable criteria for what success looks like across four dimensions: functional correctness, behavioral safety, performance, and negative criteria.

**Who creates it**: Domain experts define functional and behavioral criteria. Engineering defines performance criteria. Security and legal define negative criteria.

**Who reviews it**: Cross-functional review. Engineering verifies measurability. Domain experts verify realistic thresholds. Product verifies alignment with user needs.

**When it is created**: After task decomposition and before engineering starts building.

**What it contains**:

A table with one section per task. For each task, define:

**Functional Correctness Criteria**: What does "right answer" mean? Examples: "Intent classification accuracy on labeled eval set greater than 90%." "Extracted account number matches ground truth format in 95% of cases."

**Behavioral Criteria**: How should the system behave in edge cases, ambiguous inputs, and adversarial scenarios? Examples: "When input is ambiguous, system asks clarifying question rather than guessing." "When user requests harmful action, system declines politely."

**Performance Criteria**: Latency, throughput, cost, and resource constraints. Examples: "Response latency p95 under 2 seconds." "Token cost per query under 0.02 dollars."

**Negative Criteria**: What must NOT happen. Examples: "Never expose other customers' data in responses." "Never generate financial advice without disclaimers." "Never claim certainty when confidence is low."

**Thresholds**: For each criterion, define the acceptable range. "Accuracy greater than or equal to 90%." "Latency under 2 seconds for 95% of queries." Thresholds are specific numbers, not vague goals.

**Measurement Method**: How will you measure this? "Human evaluation on 500-sample test set." "Automated regex check on output format." "LLM-as-judge with GPT-4 using rubric X."

**How it stays current**: Success criteria evolve as you learn from evaluation and production. When thresholds are too strict or too loose, you update them. The domain expert or task owner approves changes. Changes are versioned so you can track how criteria evolved over time.

## Template 4: I/O Specification

**Purpose**: Precise definition of input and output schemas for each task, including edge cases, uncertainty handling, and tool contracts.

**Who creates it**: Engineering lead, with domain expert input on edge cases and business logic.

**Who reviews it**: Domain experts verify completeness and correctness. Product verifies alignment with user workflows. Security reviews for injection risks.

**When it is created**: After task decomposition and in parallel with success criteria definition.

**What it contains**:

For each task, define:

**Input Schema**: The structure of inputs the task will receive. Field names, data types, constraints, optional versus required fields. Example: "Input: message (string, required, max 5000 characters), customer_id (string, required, format UUID), conversation_history (array of message objects, optional, max 20 messages)."

**Output Schema**: The structure of outputs the task will produce. Example: "Output: intent (enum: billing, technical, account, other), confidence (float, 0 to 1), clarifying_question (string, optional), reasoning (string, optional)."

**Edge Cases**: Specific scenarios that do not fit the happy path. Example: "If message is empty, return intent 'other' with confidence 0. If customer_id is invalid, log error and request valid ID. If conversation_history exceeds 20 messages, truncate to most recent 20."

**Uncertainty Handling**: What happens when the system is uncertain? Example: "If confidence is below 0.7, set clarifying_question field and do not take action until user responds."

**Tool Contracts**: If the task calls external tools or APIs, define the contract. Example: "Task calls billing_lookup API. Input: customer_id. Output: account_balance, last_payment_date, outstanding_invoices. Timeout: 5 seconds. Fallback: if API fails, inform user that billing info is temporarily unavailable."

**Data Flow**: Where does input come from, where does output go? Example: "Input comes from user message in chat interface. Output is passed to response_generator task and logged to analytics pipeline."

**How it stays current**: I/O specs are updated when you discover new edge cases during development or evaluation, when schemas change, or when integrations change. Engineering owns updates. Changes are versioned and communicated to downstream consumers.

## Template 5: Eval Readiness Checklist

**Purpose**: A binary checklist of all prerequisites that must be true before evaluation can begin. This prevents teams from starting evaluation too early, which wastes time and produces misleading results.

**Who creates it**: Evaluation lead or DRI, with input from engineering, domain experts, and data teams.

**Who reviews it**: Engineering verifies technical readiness. Domain experts verify data quality. Evaluation team verifies tooling is in place.

**When it is created**: After I/O specs and success criteria are defined, before evaluation begins.

**What it contains**:

A checklist with yes/no items:

**Framing Complete**:
- Problem framing spec signed off by stakeholders: Yes/No
- Task registry complete and reviewed: Yes/No
- Success criteria defined for all in-scope tasks: Yes/No
- I/O specifications complete for all in-scope tasks: Yes/No

**Data Ready**:
- Evaluation dataset exists: Yes/No
- Evaluation dataset is labeled with ground truth: Yes/No
- Evaluation dataset covers representative input distribution: Yes/No
- Evaluation dataset includes edge cases and adversarial examples: Yes/No
- Evaluation dataset is accessible to evaluation team: Yes/No

**System Ready**:
- System produces outputs that match I/O spec schema: Yes/No
- System runs end-to-end without critical errors: Yes/No
- System latency is within measurable range (not failing due to timeouts): Yes/No
- System is instrumented to log inputs, outputs, and metadata: Yes/No

**Evaluation Tooling Ready**:
- Evaluation harness is set up: Yes/No
- Automated metrics are implemented: Yes/No
- Human evaluation interface is ready (if applicable): Yes/No
- LLM-as-judge prompts and rubrics are defined (if applicable): Yes/No

**Team Ready**:
- Evaluators are trained on success criteria: Yes/No
- Evaluation runbook is documented: Yes/No
- Triage process for failures is defined: Yes/No

If any item is "No," evaluation is not ready to start. The checklist forces you to address gaps before you waste time running evals on an incomplete system.

**How it stays current**: The checklist is re-run before each major evaluation cycle. As the system evolves, new prerequisites may be added.

## The Relationship Between Templates: A Dependency Graph

These five templates are not independent documents. They reference each other, creating a web of dependencies that ensures consistency.

The **Task Registry** references tasks by ID. Each task row points to entries in the **Success Criteria Matrix** and the **I/O Specification**. When you look at TASK-001 in the registry, you can immediately navigate to its success criteria and I/O spec.

The **Success Criteria Matrix** references the **I/O Specification** to ensure that criteria are measurable given the defined inputs and outputs. If your I/O spec does not include a confidence score, you cannot have a success criterion that depends on confidence thresholds.

The **Eval Readiness Checklist** references the **Problem Framing Spec**, the **Task Registry**, the **Success Criteria Matrix**, and the **I/O Specification** to verify completeness. You cannot check "success criteria defined for all tasks" without a complete task registry.

This interconnection is critical. It means you cannot fake completeness. If your task registry says you have ten tasks but your success criteria matrix only covers five, the eval readiness checklist will catch it.

## Why Templates Create Consistency Across Teams and Products

When every team uses the same five templates, you unlock organizational leverage. Leadership can review framing docs from ten teams and immediately understand scope, risk, and readiness. Security can audit 50 projects by reviewing their threat model summaries and I/O specs without learning a new format for each project. Evaluation teams can compare results across products because success criteria are structured the same way.

Templates also enable **knowledge transfer**. When a new engineer joins a project, they can read the five framing documents and understand the system without tribal knowledge. When a team member leaves, their work is documented in a standard format that others can pick up.

Templates are also a forcing function for **thinking clearly**. When you have to fill in a field called "non-goals," you are forced to make explicit decisions about scope. When you have to define thresholds for success criteria, you cannot hide behind vague language like "good enough."

## The Anti-Pattern: Every Team Invents Their Own Format

The most common mistake is letting every team invent their own documentation format. This happens naturally in fast-moving organizations where teams are optimizing for speed. Each team picks the tools they like — Notion, Google Docs, Confluence, Jira, Linear — and creates ad hoc documents that fit their workflow.

This creates three problems. First, **inconsistency**. You cannot compare projects because they are documented differently. Second, **incompleteness**. Without a template, teams forget critical pieces. You discover six months into a project that nobody defined success criteria for half the tasks. Third, **rework**. When leadership or legal or security ask for information, teams have to reformat their work into a standard structure, which is pure waste.

The fix is to mandate the five core templates and make them easy to use. Easy means: available in the tools teams already use (if your team lives in Notion, give them Notion templates, not a Word doc), pre-filled with examples so teams understand what good looks like, and lightweight enough that filling them out takes hours, not days.

## Practical Takeaways

**Use the five core templates**: Problem Framing Spec, Task Registry, Success Criteria Matrix, I/O Specification, Eval Readiness Checklist. These are the minimum set. Add more only if you have a specific need.

**Keep templates lightweight**: Two to four pages per template. If your template is longer, it is too complex.

**Interconnect templates**: Use references between documents. Task IDs in the registry link to success criteria and I/O specs. This ensures consistency.

**Make templates easy to use**: Provide them in the tools your teams already use. Pre-fill with examples. Make them feel like helpful prompts, not bureaucratic forms.

**Mandate consistency**: Do not let every team invent their own format. Standardize on one set of templates across the organization.

**Version templates**: As you learn what works, update the templates. Communicate changes so teams know when and why the format evolved.

**Review completeness before evaluation**: Use the Eval Readiness Checklist to verify all templates are complete before you start evaluation. This prevents wasted effort.

The next question is: what happens when you run evaluation and discover your results are confusing, misleading, or useless — and the root cause is not your system, but your framing? That is the pattern we explore next: how framing failures surface during evaluation, and how to diagnose and fix them before they derail your project.
