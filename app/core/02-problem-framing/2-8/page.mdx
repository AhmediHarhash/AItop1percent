# 2.8 — Granularity: When to Stop Decomposing

A startup building an AI-powered legal document analysis system spent four months decomposing their problem. They started with "analyze legal contracts," decomposed it into contract type classification, clause extraction, entity recognition, risk assessment, and compliance checking. Then they decomposed further: clause extraction became structural analysis, semantic segmentation, boundary detection, and hierarchy construction. Risk assessment became risk identification, severity scoring, precedent matching, and impact analysis. They kept decomposing until they had 127 distinct sub-problems, each meticulously defined and documented.

Then they tried to build it. Six weeks later, they had made almost no progress. Every sub-problem depended on three other sub-problems. Integration complexity was overwhelming. Testing required coordinating dozens of components. Team members couldn't understand the system architecture without hours of explanation. They had over-decomposed, creating artificial boundaries that made the system impossible to build.

Meanwhile, a competitor launched with a simpler decomposition: document ingestion, clause extraction, risk analysis, report generation. Just four sub-problems. Their system wasn't as theoretically elegant, but it shipped in three months and worked reliably. They could iterate quickly, test each component, and explain the system in one diagram.

This is the Goldilocks problem of decomposition: too coarse and you can't evaluate or improve the system effectively; too fine and you can't build or ship it. Finding the right granularity isn't about following rules — it's about applying practical heuristics that balance clarity, buildability, and operational needs.

## The Purpose-Driven Granularity Test

The single most important principle for determining decomposition granularity is purpose alignment: each sub-problem should exist because it serves a specific purpose in how you build, evaluate, or operate your system. If you can't articulate why a sub-problem boundary exists and what it enables you to do better, the boundary is probably artificial.

The evaluation test is the clearest heuristic: can you define a clear, independent evaluation strategy for this sub-problem? If your clause extraction sub-problem is further decomposed into structural analysis, semantic segmentation, and boundary detection, can you evaluate each of those independently in a meaningful way? What does it mean for structural analysis to be correct if semantic segmentation hasn't run yet? How do you measure boundary detection accuracy without knowing whether the semantic segments are correct?

If you can't answer these questions with concrete metrics and test data, you've decomposed too finely. The sub-problems aren't independently evaluable, which means they're not genuinely separable concerns — they're artificial fragments of a single coherent task.

Apply this test to the legal document system. Can you evaluate clause extraction independently? Yes: given a document with known clauses, you can measure whether the system correctly identified and extracted each clause. That's precision and recall on clause boundaries. Can you evaluate risk analysis independently? Yes: given extracted clauses with known risk levels, you can measure whether the system correctly assessed risk severity. These sub-problems pass the evaluation test.

Can you evaluate semantic segmentation independently of boundary detection? Not really. Segments without boundaries aren't meaningful, and boundaries without semantic understanding don't tell you if you've segmented correctly. These should probably be one sub-problem: clause extraction with semantic boundaries.

The ownership test is equally practical: does each sub-problem have a clear single owner who can be responsible for its performance? In a working team, you need to be able to say "Alice owns clause extraction, Bob owns risk analysis, Carol owns compliance checking." If you have 127 sub-problems across a team of 8 people, you don't have clear ownership. Engineers are responsible for 15-20 sub-problems each, which means those sub-problems aren't actually separate work streams.

Conversely, if your sub-problems are too coarse, multiple team members will work on the same sub-problem but on different aspects, and you'll have coordination overhead and unclear accountability. If "document analysis" is a single sub-problem but three engineers are working on different parts of it, you've under-decomposed.

The right granularity typically maps to something like one or two sub-problems per team member, or one sub-problem per distinct technical skillset required. If clause extraction requires NLP expertise, risk analysis requires legal domain knowledge, and compliance checking requires regulatory expertise, those are natural sub-problem boundaries because they align with team capabilities.

## Failure Mode Diversity as a Granularity Signal

Another powerful heuristic for decomposition granularity is failure mode analysis: a well-scoped sub-problem should have a coherent set of related failure modes, not multiple distinct categories of failures that require completely different debugging and improvement approaches.

Consider a content moderation system with a sub-problem called "content analysis." What are the failure modes? False positives where safe content is flagged, false negatives where unsafe content is missed, incorrect severity ratings, wrong category classifications, missed context, and failed multi-modal understanding. These failure modes are quite different. False positives in text analysis have different causes than false positives in image analysis. Severity rating errors require different fixes than category classification errors.

This diversity of failure modes signals under-decomposition. The sub-problem is doing too many different things, each with its own failure characteristics. Splitting it into text analysis, image analysis, severity scoring, and category classification creates sub-problems with more coherent failure modes. Text analysis false positives are all about language understanding. Severity scoring errors are all about risk calibration. Now you can focus debugging and improvement efforts.

Conversely, if you decompose text analysis further into tokenization, embedding generation, attention scoring, and classification head, you might create sub-problems where failure modes aren't actually separable. A false positive might involve the interaction between embeddings and attention weights. You can't point to one sub-problem and say "this is where it failed" because the failure emerged from the combination. This signals over-decomposition — you've split a coherent cognitive operation into technical implementation details.

The practical test: when something goes wrong, can you identify which sub-problem failed without ambiguity? In the legal document system, if a contract risk is missed, can you determine whether clause extraction failed to identify the clause, or risk analysis failed to assess the extracted clause correctly, or compliance checking failed to flag the risk? If you can answer this clearly, your decomposition is at the right granularity. If failures span multiple sub-problems in ways that make it unclear where to fix the problem, you might have decomposed too finely or drawn boundaries in the wrong places.

## Integration Complexity as a Warning Sign

Over-decomposition reveals itself most clearly in integration complexity. When you have too many sub-problems with too many dependencies, you spend more time integrating components than building them. This creates three warning signs: coordination overhead, artificial intermediary data formats, and testing complexity.

Coordination overhead shows up when engineers spend most of their time in integration meetings rather than building. If your team has daily meetings to coordinate sub-problem interfaces, weekly meetings to discuss dependency updates, and constant Slack threads about what data format sub-problem A should output for sub-problem B to consume, you've probably over-decomposed. The sub-problems are so fine-grained and interdependent that each one touches multiple others, requiring constant coordination.

A well-decomposed system has clear boundaries with clean interfaces. Integration happens at defined points with well-specified contracts. You need integration planning, but it shouldn't dominate your time. If you're spending 40% of engineering time on integration, your sub-problems are probably too small.

Artificial intermediary data formats are another red flag. When you decompose, each sub-problem has inputs and outputs. But if you find yourself creating data formats that exist only to pass information from sub-problem A to sub-problem B, and no other part of the system cares about this format, you might have an artificial boundary.

In the legal document system's over-decomposed version, they had a data format for structural analysis output that was consumed only by semantic segmentation, which produced a different format consumed only by boundary detection, which produced yet another format consumed by hierarchy construction. None of these intermediate formats were useful for evaluation, debugging, or any purpose except feeding the next sub-problem in the chain. This is a strong signal that structural analysis, semantic segmentation, boundary detection, and hierarchy construction should be one sub-problem: clause extraction. Its output would be a simple list of clauses with boundaries and hierarchy — a natural, useful data format.

Testing complexity becomes unmanageable with over-decomposition. To test sub-problem X, you need to mock or simulate the outputs of sub-problems A, B, and C that X depends on. To integration test the chain, you need to coordinate test data across all sub-problems. If you have 127 sub-problems, creating comprehensive test coverage becomes heroic effort rather than routine engineering practice.

A good granularity heuristic: can you test each sub-problem in isolation with straightforward mocks and fixtures? If testing requires elaborate simulation of half a dozen other sub-problems, you've probably over-decomposed and created artificial dependencies.

## Team Comprehension as the Litmus Test

Perhaps the most practical test for decomposition granularity is team comprehension: can a new team member understand your system architecture in a reasonable amount of time? Can existing team members hold the entire decomposition in their heads while working?

Draw your system architecture showing sub-problems and dependencies. If the diagram fits on one page and can be explained in 15 minutes, you're probably at good granularity. If it requires multiple pages, cross-references, and an hour-long presentation with extensive background, you've likely over-decomposed.

This isn't about dumbing down your system — it's about matching granularity to cognitive limits. Humans can hold about 7-10 concepts in working memory simultaneously. If your top-level decomposition has 15-20 sub-problems, that's manageable. If it has 127 sub-problems, no one can reason about the system holistically. They'll understand their local piece but not how it fits together.

The comprehension test also reveals under-decomposition. If your architecture diagram has four boxes but engineers working on the system struggle to explain how things actually work, you've hidden complexity inside overly-coarse sub-problems. The boxes are too big and contain too much internal structure that should be made explicit.

A working system should have a decomposition that matches its conceptual structure. The legal document analysis system's competitor had four sub-problems: document ingestion, clause extraction, risk analysis, and report generation. Any team member could explain the flow in two sentences: "We ingest documents and extract clauses, analyze each clause for risks, then generate a report summarizing findings." That's a decomposition at the right granularity for team comprehension.

## Stopping Rules: Practical Heuristics

When you're actively decomposing a problem, how do you know when to stop breaking down a particular sub-problem? Here are practical stopping rules that work in production systems.

Stop when the sub-problem maps to a single model or algorithm. If clause extraction is handled by one fine-tuned language model, that's a natural stopping point. You could theoretically decompose it further into the model's internal operations (encoding, attention, decoding), but those aren't sub-problems you'll evaluate or improve independently — they're implementation details. Similarly, if risk scoring uses a specific scoring algorithm, that's a stopping point. The algorithm's internal steps aren't separate sub-problems.

Stop when further decomposition doesn't reveal new optimization or improvement opportunities. Decomposition is valuable when it helps you identify bottlenecks, allocate resources, or guide improvements. If breaking clause extraction into smaller pieces doesn't tell you anything new about how to improve it, don't decompose further. You've reached the level where improvement efforts work holistically on the sub-problem.

Stop when the sub-problem is small enough to test in isolation with reasonable effort. Can you create test datasets for this sub-problem, run evaluations, and interpret results without excessive setup? If yes, that's a good stopping point. If testing would be trivial because the sub-problem is too simple (a function that adds two numbers), you've over-decomposed. If testing requires simulating most of the rest of the system, you've under-decomposed.

Stop when the sub-problem has a single, clear failure mode category. Text-based clause extraction fails by missing clauses or misidentifying boundaries — that's one category of failure about text understanding. If you decompose further into character-level encoding failures versus word-level parsing failures versus sentence-level segmentation failures, you're creating sub-problems that can't fail independently. The system needs all three to work, so they share failure modes.

Stop when you can assign clear ownership. If one engineer or one small team can own this sub-problem — understanding its requirements, implementing solutions, monitoring performance, and making improvements — that's appropriate granularity. If ownership would need to be split across multiple teams for a single sub-problem, it might be too coarse. If one person owns ten sub-problems that they work on as a unit, those ten should probably be one.

## Signs You've Over-Decomposed

Recognizing over-decomposition early saves months of wasted effort. Here are the warning signs.

Sub-problems that can't be meaningfully tested alone are the clearest indicator. If you can't write an evaluation for a sub-problem without first implementing three other sub-problems it depends on, you've over-decomposed. In the 127-sub-problem legal system, "boundary detection" couldn't be tested without "semantic segmentation" producing its input, which couldn't be tested without "structural analysis" producing its input. These aren't separate sub-problems — they're steps in a single cognitive operation that should be tested together.

Artificial boundaries that create unnecessary integration complexity appear when sub-problems have heavy coupling through complex data formats or frequent interface changes. If changing how one sub-problem works requires changing three other sub-problems, those boundaries aren't providing isolation value. They're just creating coordination overhead.

Sub-problems where team members disagree about the boundary definition suggest the boundary is artificial. If engineers argue about whether feature X belongs in sub-problem A or sub-problem B, the boundary between A and B probably doesn't reflect a natural conceptual division. Well-placed boundaries are obvious — the sub-problems are doing clearly different things.

Implementation that's simpler than the architecture is a subtle but important signal. If your system design documentation is more complex than your actual code, you've over-formalized. Decomposition should clarify and guide implementation, not create layers of abstraction that the code doesn't actually need.

## Signs You've Under-Decomposed

Under-decomposition has its own warning signs, equally important to recognize.

Sub-problems with multiple distinct failure modes that require different debugging approaches signal under-decomposition. If "content moderation" fails sometimes because image analysis missed something, sometimes because text analysis was wrong, and sometimes because severity scoring was off, those are three different failure modes requiring three different improvement approaches. Split into image analysis, text analysis, and severity scoring.

Sub-problems where different team members disagree about how to evaluate quality indicate hidden complexity. If Alice thinks content moderation should be measured by false positive rate, Bob thinks it should be measured by false negative rate, and Carol thinks it should be measured by user appeal rate, you probably have one sub-problem doing three different things that should be separated: content classification, severity assessment, and action recommendation.

Sub-problems that need to be re-decomposed during implementation signal that the initial decomposition missed important structure. If you start building "risk analysis" and discover you actually need separate components for risk identification, risk scoring, and risk prioritization, your initial decomposition was too coarse.

Sub-problems that consistently take longer to implement than estimated often hide internal complexity. If every estimate for "clause extraction" is wrong because it's actually doing extraction, classification, and normalization, decompose those out explicitly.

## The 2026 Practice: Iterative Granularity Adjustment

By 2026, successful AI teams treat granularity as something you discover iteratively, not something you get perfect upfront. The pattern is to start with coarse decomposition (4-8 top-level sub-problems), implement enough to test the boundaries, then refine granularity based on what you learn.

The legal document startup eventually found the right granularity through iteration. They started over with a coarse decomposition: document processing, clause analysis, risk assessment, compliance checking, report generation. Five sub-problems. They implemented basic versions of each and started evaluating.

They discovered that document processing was actually too coarse — OCR failures had different characteristics than layout understanding failures, and they needed to measure and improve them separately. They split it into OCR and layout analysis. That felt right — each had clear evaluation metrics and different improvement strategies.

They discovered that compliance checking and risk assessment were heavily coupled — you couldn't assess risk without checking compliance requirements, and compliance violations were themselves risks. They merged them into a single risk and compliance analysis sub-problem. That simplified integration and reflected the reality that these weren't separable concerns.

Their final decomposition had six sub-problems: OCR, layout analysis, clause extraction, risk and compliance analysis, risk prioritization, and report generation. Each had clear ownership, clear evaluation metrics, coherent failure modes, and reasonable integration complexity. They could explain the system architecture in ten minutes. They could test each sub-problem independently. When something failed, they could identify which sub-problem to fix.

This iterative approach recognizes that you can't perfectly foresee the right granularity from requirements alone. You need to start building, test the boundaries you created, and adjust based on what you learn about failure modes, team dynamics, integration complexity, and improvement opportunities.

The sophistication isn't in getting granularity right the first time — it's in recognizing the signs of wrong granularity and being willing to refactor your decomposition. Merge sub-problems that can't be meaningfully separated. Split sub-problems with distinct failure modes. Keep adjusting until your decomposition feels natural: explainable, testable, buildable, and aligned with how your team actually works.

Problem decomposition isn't a one-time design exercise but an ongoing discipline of matching your system structure to the reality of how you build, evaluate, and operate AI products — a discipline that separates teams shipping reliable systems from teams drowning in complexity of their own creation.
