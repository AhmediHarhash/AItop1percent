# 4.8 — Criteria That Survive Model Changes

In late 2024, a fintech startup built a fraud detection system using GPT-4's function calling capabilities to extract transaction features and route them to specialized analysis modules. The success criteria were tightly coupled to the implementation: "The system must use GPT-4 function calling to extract at least 15 transaction features with 90% accuracy. It must invoke the correct analysis function for each transaction type. It must structure outputs in the GPT-4 function response format." These criteria were specific, measurable, and aligned with the architecture. The system launched successfully in January 2025.

By May 2025, OpenAI released GPT-4.5 with improved reasoning but different function calling syntax. By August, Claude 3.5 Sonnet offered better accuracy at lower cost. By November, the team wanted to experiment with a fine-tuned Llama model for cost optimization. Each time they considered a model change, they faced the same problem: their success criteria were written for GPT-4's specific capabilities. Switching models meant redefining success, which meant re-evaluating everything, which meant getting stakeholder sign-off again, which meant weeks of delay.

The team had made a classic mistake: they wrote model-coupled criteria instead of model-agnostic criteria. They defined success in terms of how the system worked rather than what the system achieved. When the implementation changed, the criteria became obsolete.

This is the discipline of future-proof success criteria: writing definitions of success that outlive any specific model, provider, or architecture. In 2026, where model churn is measured in months and competitive pressure drives constant experimentation, criteria durability is not optional — it is survival.

## The Problem: Model-Coupled Criteria Break on Every Change

Model-coupled criteria are success definitions that reference specific model features, capabilities, or behaviors. They fail when you switch models because the new model does not have those exact features, even if it achieves the same outcome.

The symptoms of model-coupled criteria are easy to spot. Your criteria mention specific model names: "must use GPT-4o," "must leverage Claude's extended context window," "must utilize Gemini's multimodal capabilities." Your criteria specify technical mechanisms: "must use function calling," "must generate JSON with specific schema," "must operate within 128k token context." Your criteria reference provider-specific features: "must use OpenAI embeddings," "must invoke Anthropic's constitutional AI," "must use Google's grounding API."

All of these are implementation details, not outcomes. They tell you how the system works, not what it delivers. When you swap GPT-4 for Claude, or upgrade from one model version to the next, these criteria no longer apply. You are forced to rewrite success definitions, which triggers re-evaluation, which requires stakeholder re-approval, which delays your ability to improve the system.

Model coupling creates several problems. First, it locks you into specific providers. If your criteria specify GPT-4 features and GPT-4 pricing triples, you cannot easily switch to a cheaper alternative because your success criteria are coupled to the expensive option. Second, it prevents upgrades. When a new model version is released, you cannot adopt it without rewriting criteria. Third, it creates technical debt. Your criteria document becomes a historical artifact describing a system that no longer exists rather than a living contract describing what the system must achieve.

The root cause is that teams write criteria during architecture discussions when they are thinking about implementation. "We will use function calling to extract features" becomes "must use function calling to extract features" in the criteria doc. The solution is to separate outcome from mechanism.

## Writing Model-Agnostic Criteria: Focus on Outcomes, Not Mechanisms

Model-agnostic criteria describe what the system must accomplish, not how it accomplishes it. They define success in terms of outputs, behaviors, and user impact, not technical approaches.

The shift from model-coupled to model-agnostic criteria is simple in principle but requires discipline in practice. Instead of "must use function calling to invoke tools," write "must successfully execute user-requested actions with 95% accuracy." Instead of "must operate within GPT-4's 128k context window," write "must handle documents up to 500 pages without loss of accuracy." Instead of "must use OpenAI embeddings for retrieval," write "must retrieve relevant context with 90% precision in top-5 results."

Notice the pattern: model-agnostic criteria specify the observable outcome — actions executed, pages handled, context retrieved — and the quality bar, but not the technical mechanism. This allows you to change the mechanism while preserving the criteria.

A concrete example: a legal document review system might initially use GPT-4 to extract key clauses. Model-coupled criteria would say "must use GPT-4 to identify indemnification, liability, and termination clauses." Model-agnostic criteria would say "must identify indemnification, liability, and termination clauses with 92% recall and 88% precision." The second version allows you to switch from GPT-4 to Claude to a fine-tuned model to a traditional NLP approach without redefining success. As long as the new implementation achieves 92% recall and 88% precision, it meets the criteria.

Model-agnostic criteria also make cross-model comparisons straightforward. When you evaluate GPT-4 and Claude against the same outcome-based criteria, you get an apples-to-apples comparison. When you have model-specific criteria, you are comparing apples to oranges because the success definitions differ.

## Specifying Output Requirements Without Specifying How to Generate Them

One of the hardest parts of writing model-agnostic criteria is specifying output requirements clearly without leaking implementation details. You need to be precise about what the output must contain, but you must avoid dictating how it is generated.

The technique is to define outputs in terms of semantic content and structure, not generation method. For example, instead of "must generate a JSON object using structured output mode," write "must produce output that includes: transaction ID, timestamp, amount, merchant, category, and fraud risk score. Output must be machine-parseable and conform to the defined schema."

This version specifies exactly what the output must contain and what properties it must have, but it does not care whether the model generates JSON natively, whether you parse it from natural language, or whether you use a post-processing step to structure it. The criteria are satisfied as long as the final output meets the requirements.

Similarly, instead of "must use chain-of-thought prompting to show reasoning," write "must provide an explanation of the decision that identifies which factors influenced the outcome." The first is a mechanism — chain-of-thought prompting — that ties you to a specific prompting technique. The second is an outcome — an explanation with specific content — that can be achieved through chain-of-thought, through structured reasoning templates, through post-hoc explanation generation, or through any other method.

Output requirements should cover three dimensions: content, format, and quality. Content specifies what information must be present. Format specifies how the output must be structured for consumption. Quality specifies accuracy, completeness, and reliability standards. None of these dimensions require specifying the generation mechanism.

For a customer support system, output requirements might look like: "Responses must address the customer's stated question. Responses must include relevant policy information when applicable. Responses must be in natural, conversational language. Responses must not make commitments outside approved authority levels. Response accuracy, measured by expert review, must be at least 90%." You can meet these requirements with GPT-4, Claude, a fine-tuned model, or a rule-based system augmented with AI. The criteria survive implementation changes.

## Avoiding Context-Window and Token-Limit Coupling

One of the most common forms of model coupling is baking context window limits into success criteria. "The system must handle documents up to 128k tokens" ties your criteria to GPT-4's context window. When you switch to Claude with a 200k context window, or to a model with a 32k window, your criteria no longer match.

The model-agnostic alternative is to specify input size requirements in business terms, not token terms. "The system must handle legal contracts up to 200 pages with full accuracy" is model-agnostic. It specifies a real-world input constraint — 200 pages — that stakeholders understand. How many tokens that translates to depends on the model and tokenization scheme, but that is an implementation detail.

When you define criteria in business terms, you also create better alignment with stakeholders. Legal stakeholders care about page count, not token count. They can tell you "most contracts we process are 50-150 pages, but we occasionally see 300-page contracts." That becomes the input size requirement. Whether you handle 300 pages with a long-context model, with chunking and retrieval, or with summarization and progressive analysis is a technical decision, not a criteria decision.

Token limits also affect cost and latency, which are legitimate criteria dimensions. But instead of saying "must operate within 32k tokens to control cost," say "cost per document must not exceed 5 dollars." This decouples the criteria from token economics, which vary by provider and change over time. If a new model offers better quality at higher token cost but still stays under 5 dollars per document, it meets the criteria.

Latency criteria should similarly avoid token coupling. Instead of "must process 50k tokens in under 10 seconds," write "must process a typical legal contract in under 10 seconds." This allows for implementation flexibility. You might use a smaller model with faster inference, or a larger model with better batching, or a hybrid approach. As long as latency stays under 10 seconds for the defined input, criteria are met.

## Defining Quality in Terms of User Value, Not Technical Metrics

Model-agnostic criteria emphasize user value over technical metrics. Technical metrics are useful for development and debugging, but they should not be the primary success criteria because they couple you to specific measurement approaches that may not transfer across models.

For example, perplexity is a common language model metric, but it is model-dependent. GPT-4 and Claude have different vocabularies and tokenization schemes, so perplexity scores are not comparable across them. If your success criteria specify "must achieve perplexity under 15," you cannot fairly evaluate alternative models.

The model-agnostic alternative is to define quality in terms of user outcomes. Instead of perplexity, use human preference ratings: "Generated summaries must achieve an average quality rating of 4.0 out of 5.0 from expert reviewers." Human ratings are model-agnostic. You can measure them consistently across GPT-4, Claude, or any other model.

Instead of F1 score on a specific benchmark dataset that might favor one model's training distribution, use task success rate: "The system must correctly classify 90% of customer inquiries into the appropriate support category." Task success is what users care about, and it can be measured consistently regardless of which model you use.

This does not mean you abandon technical metrics. Precision, recall, F1, accuracy, and latency are all valuable. But they should be defined in terms of task outcomes, not model internals. "Achieve 90% precision on fraud detection" is model-agnostic. "Achieve 90% precision using logits thresholding at 0.7" is model-coupled because it specifies a technical mechanism that might not apply to all models.

## Examples of Model-Coupled vs Model-Agnostic Criteria

To make the distinction concrete, here are paired examples showing model-coupled criteria and their model-agnostic equivalents.

**Model-Coupled**: "The system must use GPT-4's function calling to execute at least 12 distinct tool invocations."

**Model-Agnostic**: "The system must successfully execute user-requested actions across the full range of supported tools with 95% accuracy."

**Model-Coupled**: "The system must leverage Claude's 200k context window to process entire codebases without chunking."

**Model-Agnostic**: "The system must process codebases up to 50k lines of code while maintaining accurate cross-file reference resolution."

**Model-Coupled**: "Responses must be generated using GPT-4's structured output mode with JSON schema validation."

**Model-Agnostic**: "Responses must be machine-parseable and conform to the defined schema, with zero parsing errors on well-formed inputs."

**Model-Coupled**: "The system must use OpenAI embeddings for semantic search with cosine similarity thresholding at 0.85."

**Model-Agnostic**: "The system must retrieve semantically relevant context with at least 90% precision in the top-5 results."

**Model-Coupled**: "The system must operate within GPT-4 Turbo's 4096 token output limit."

**Model-Agnostic**: "Generated reports must be comprehensive enough to convey all key findings, typically 1000-2000 words."

**Model-Coupled**: "The system must use Anthropic's constitutional AI constraints to ensure safe outputs."

**Model-Agnostic**: "The system must refuse harmful requests 100% of the time and avoid generating unsafe content, as measured by expert safety review."

In each case, the model-agnostic version describes what must be achieved without specifying the technical mechanism. This allows you to change models, upgrade versions, or experiment with alternatives while preserving your success criteria.

## The Architecture Principle: Criteria Define the Contract, Implementation Fulfills It

The deeper principle behind model-agnostic criteria is the separation of contract and implementation. Success criteria are the contract: the non-negotiable promises your system makes about quality, capability, and behavior. Implementation is how you fulfill that contract: the models, architectures, and techniques you use.

Contracts should be stable. Implementations should be flexible. When you couple criteria to implementation, you lose that flexibility. Every time you want to change implementation, you must renegotiate the contract.

This principle is familiar in software engineering. API contracts define what a service must do, not how it does it. You can rewrite the internal implementation, swap databases, change algorithms, or optimize performance, and as long as the API contract is honored, clients do not care. Success criteria are the API contract for your AI system.

Stakeholders sign off on the contract, not the implementation. Legal reviews your criteria to ensure regulatory compliance. Product approves criteria to ensure user value. Domain experts validate criteria to ensure quality standards. They care about outcomes, not mechanisms. When you present model-specific criteria to stakeholders, you are asking them to approve implementation details they may not understand or care about.

The separation of contract and implementation also clarifies decision rights. Engineering owns the implementation: which model to use, how to prompt it, which architecture to deploy. Product and domain experts own the contract: what the system must achieve, what quality bar it must meet, what behaviors are required. When criteria are model-agnostic, these decision rights are clear. When criteria are model-coupled, engineering decisions leak into stakeholder approvals, slowing down iteration.

## The 2026 Context: Model Churn Is the Norm

In 2026, AI is moving faster than it ever has. New models are released every few months. Providers change pricing, deprecate old versions, and introduce new capabilities constantly. Startups emerge with specialized models that outperform general models on specific tasks. Open-source models close the gap with commercial models, creating new cost-performance trade-offs.

In this environment, model lock-in is a strategic vulnerability. If your success criteria tie you to a specific model or provider, you cannot adapt to market changes without restarting your criteria definition process. You lose months renegotiating stakeholder approvals while competitors ship improvements.

Model-agnostic criteria give you strategic flexibility. When a new model is released, you evaluate it against your existing criteria. If it meets or exceeds the thresholds, you can switch without re-framing the problem. When a provider raises prices, you can evaluate alternatives without rewriting success definitions. When an open-source model offers competitive quality, you can test it without stakeholder re-approval.

This flexibility accelerates iteration. Instead of spending weeks on criteria renegotiation every time you want to try a new model, you spend days on technical evaluation. The criteria remain constant; the implementation evolves.

Model-agnostic criteria also protect you from provider risk. If a provider changes their terms of service, introduces restrictions, or experiences outages, you can switch providers without disrupting your product promises. Your users and stakeholders do not care which model you use — they care that the system continues to meet quality standards.

The practical implication is that in 2026, writing model-coupled criteria is professional negligence. The rate of change is too fast, and the cost of re-framing is too high. Model-agnostic criteria are not a nice-to-have best practice — they are a requirement for operational agility.

## When Model-Specific Constraints Are Necessary

There are cases where model-specific constraints are legitimate criteria elements, but they are rarer than teams think. The valid cases typically involve regulatory compliance, privacy requirements, or hard technical constraints.

For example, if you operate under GDPR and must process data within EU borders, you might have a criterion that "the system must use a model deployed in EU data centers." This is model-specific, but it reflects a legal requirement, not a technical preference.

Similarly, if you have a regulatory requirement to use explainable models, you might specify "the model must provide feature-level explanations for decisions." This constrains your model choices, but it is driven by compliance, not by coupling to a specific implementation.

If you have a hard latency constraint due to real-time requirements — for example, "must respond within 100ms for interactive use" — this might limit you to smaller models or edge deployment. The constraint is real, but it should still be framed as an outcome requirement, not a model specification.

The test for whether a model-specific constraint belongs in your criteria is: does this constraint reflect a non-negotiable business, legal, or user requirement, or does it reflect an implementation preference? If it is a preference, keep it out of criteria. If it is a requirement, include it but document the rationale so stakeholders understand why it is necessary.

## Practical Implementation: The Criteria Review Checklist

To ensure your success criteria are model-agnostic, use this checklist during criteria reviews:

Does the criterion mention a specific model name or version? If yes, rewrite to describe the capability or outcome, not the model.

Does the criterion specify a technical mechanism like function calling, embeddings, or prompting techniques? If yes, rewrite to describe what must be achieved, not how to achieve it.

Does the criterion reference token counts or context windows? If yes, rewrite in terms of business inputs like page counts, document sizes, or conversation lengths.

Does the criterion specify provider-specific features or APIs? If yes, rewrite to describe the functional requirement those features serve.

Does the criterion use technical metrics that are model-dependent? If yes, supplement or replace with user-facing metrics that are model-agnostic.

Can you evaluate this criterion against multiple different models using the same measurement approach? If no, the criterion is likely model-coupled.

If you remove all references to specific models and technologies, does the criterion still clearly define success? If no, you have not separated outcome from mechanism.

This checklist surfaces coupling that is easy to miss when you are focused on getting criteria written. Run it as a final review before stakeholder sign-off to ensure your criteria will survive the inevitable model changes ahead.

## The Long-Term Value of Durability

Model-agnostic criteria are an investment. They take slightly more effort to write because you must think carefully about outcomes and abstractions rather than copying technical design docs into criteria. But the return on that investment is enormous.

Durable criteria reduce rework. You define success once, and you measure it consistently across model iterations, provider changes, and architecture evolutions. Durable criteria accelerate improvement. You can experiment with new models quickly because you do not need stakeholder re-approval for every experiment. Durable criteria clarify accountability. Engineering is accountable for meeting the criteria through whatever implementation they choose. Stakeholders are accountable for defining criteria that reflect real requirements.

Most importantly, durable criteria allow you to focus on the problem, not the technology. The problem you are solving — helping users complete tasks, making better decisions, automating workflows — is stable. The technology you use to solve it is ephemeral. When your criteria focus on the problem, they remain relevant. When they focus on the technology, they become obsolete as soon as the next model is released.

Having established how to write success criteria that survive model changes, the next critical step is getting stakeholder agreement on those criteria before a single line of code is written — the success criteria review and sign-off process.
