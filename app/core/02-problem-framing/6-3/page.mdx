# 6.3 — Mapping Tasks to Ground Truth Requirements

In mid-2024, a legal technology startup built an AI system to review contracts and flag potential issues. They had defined the tasks clearly, mapped them to evaluation strategies, and started implementation. Two months in, the evaluation team said they were ready to start testing. The product team asked: "What ground truth are you testing against?" The evaluation team paused. "We assumed you had labeled contracts." They did not. They had hundreds of contracts but zero labels. Nobody during framing had asked what kind of ground truth was needed, how much was needed, or who would create it.

The project ground to a halt. They spent the next six weeks scrambling to label contracts, realizing midway that they needed not just issue labels but also clause-level annotations showing where each issue appeared. They needed expert lawyers to do the labeling because the task was too complex for crowd workers. The labeling cost exceeded fifty thousand dollars and pushed the launch date back by three months. All because nobody mapped tasks to ground truth requirements during problem framing.

This is preventable. Every task type has predictable ground truth requirements. If you identify them during framing, you can budget time and resources. If you discover them during implementation, you pay a premium in delays and emergency spending.

## Ground Truth Requirements by Task Type

Generation tasks require reference outputs that serve as examples, not as the only correct answer. If you are generating customer service emails, your ground truth is a collection of high-quality emails written by expert agents, showing the range of acceptable responses. You also need rubrics that define quality dimensions: what makes an email accurate, helpful, appropriately toned, and compliant.

The reference outputs anchor the rubric. When you train raters, you show them examples and say, "This is what a five-out-of-five looks like. This is what a three looks like." Without reference examples, rubrics become abstract and raters interpret them inconsistently.

For generation tasks, you also need expert annotations on the examples. If you are generating medical advice, each reference response should be annotated with why it is correct: which medical guidelines it follows, what key information it includes, and what safety boundaries it respects. This turns the example into a teaching tool.

Extraction tasks require labeled examples with correct extractions marked, including boundary cases. If you are extracting dates from documents, your ground truth is a set of documents where every date is highlighted and labeled. If you are extracting named entities, every entity needs a boundary annotation showing where it starts and ends, plus a category label.

The critical detail for extraction ground truth is boundary cases. It is not enough to label easy examples where the correct extraction is obvious. You need examples where boundaries are ambiguous: does the date include the year or not? Does the entity name include the title or not? These boundary cases are where systems fail, and they must be in your ground truth.

Classification tasks need labeled datasets with correct categories assigned, including ambiguous examples with adjudicated labels. If you are classifying support tickets into categories, your ground truth is a set of tickets each labeled with the correct category. The tricky part is ambiguous cases where multiple categories seem valid. These need adjudication: a process where multiple labelers vote and a domain expert makes the final decision.

Ambiguous examples are not noise to be removed. They are signal. They show where the category boundaries are unclear and where your system will struggle. Your ground truth must include them, with clear decisions about how to handle them.

Transformation tasks require input-output pairs showing correct transformations. If you are summarizing documents, your ground truth is a set of documents paired with expert-written summaries. If you are translating, it is source text paired with correct translations. If you are reformatting data, it is original format paired with target format.

For transformation tasks, the ground truth must cover the range of input variability. If your inputs vary in length, complexity, domain, and format, your ground truth must represent that variability. A ground truth set with only simple examples will not prepare you for complex cases.

Reasoning tasks require expert-validated reasoning chains, not just correct conclusions. If you are solving math problems, your ground truth is not just the final answer but the step-by-step solution showing how to get there. If you are making medical diagnoses, it is the diagnostic reasoning path: symptoms observed, differential diagnoses considered, tests ordered, conclusion reached.

Reasoning ground truth is expensive because it requires experts to document their thinking. You cannot crowd-source this. A domain expert needs to work through each problem and write down their reasoning in enough detail that someone else could follow and verify it.

## The Ground Truth Planning Matrix

For each task in your registry, you need to document four things: what ground truth format is required, how much ground truth is needed for minimum viable evaluation, who creates it, and how it is validated.

Start with format. For an extraction task pulling addresses from forms, the format is: forms with addresses highlighted and normalized, including edge cases like PO boxes, international addresses, and addresses split across multiple fields. For a generation task writing product descriptions, the format is: reference descriptions written by expert copywriters, plus a rubric with scoring criteria for accuracy, appeal, and brand voice.

Then specify quantity. How much ground truth do you need for a meaningful evaluation? This depends on the task complexity and variability. A simple binary classification task might need 200 examples to start. A complex generation task covering many domains might need 1,000 examples to represent the distribution adequately.

The minimum viable set is not your long-term goal. It is the smallest set you need to start evaluating. You can expand later, but you need to define the floor during framing so you can plan resources.

Next, identify who creates the ground truth. This is often the most expensive decision. Domain experts are slow and expensive but produce high-quality labels. Trained annotators are faster and cheaper but need detailed instructions and quality control. Crowd workers are fastest and cheapest but only work for simple tasks.

The task type constrains the creator pool. Reasoning tasks need domain experts. Extraction tasks can use trained annotators if you provide clear guidelines. Classification tasks might work with crowd workers if the categories are unambiguous. Generation tasks need experts to write reference examples but might use trained raters for ongoing evaluation.

Finally, define validation. How do you ensure the ground truth is correct? For tasks with objective answers, you can use inter-annotator agreement: multiple people label the same examples, and you measure how often they agree. High agreement means the task is well-defined. Low agreement means you need clearer guidelines or more expert input.

For tasks with subjective judgments, you use expert review. A senior domain expert reviews a sample of labels and confirms they meet quality standards. If they find systematic errors, you revise the labeling guidelines and relabel.

This matrix is not optional. Without it, you are guessing about ground truth requirements, and guesses turn into expensive surprises.

## How Much Ground Truth Is Enough?

One of the most common questions is: how many examples do we need? The answer depends on three factors: task complexity, input variability, and evaluation confidence.

For simple tasks with low variability, you need fewer examples. A binary classification task with clear categories and homogeneous inputs might need 100-200 test cases. You are not trying to represent a complex distribution; you are just checking that the system applies a straightforward rule consistently.

For complex tasks with high variability, you need more examples. A generation task that handles dozens of domains, topics, and user intents might need 1,000 or more test cases to cover the space. If you test only a narrow slice, you will not know how the system performs on cases outside that slice.

Input variability is the biggest driver of required ground truth volume. If your inputs vary across ten dimensions—topic, tone, length, format, language, domain, user type, context, intent, and complexity—you need enough examples to cover meaningful combinations of those dimensions. A test set of 100 examples cannot represent that space. A test set of 1,000 examples might barely cover it.

Evaluation confidence also matters. If you need to detect a 2 percent change in performance, you need a large test set to measure it reliably. If you only need to know whether performance is acceptable or not, a smaller test set suffices.

A practical heuristic: start with the minimum viable set for each task. For classification, aim for 50 examples per category. For extraction, aim for 200 examples covering common and edge cases. For generation, aim for 500 examples covering major scenarios. For reasoning, aim for 100 examples with full reasoning chains.

These are floors, not ceilings. Plan to expand your ground truth over time as you discover new edge cases and failure modes. But you need the minimum viable set defined during framing so you can provision resources to create it.

## Who Pays for Ground Truth Creation?

Ground truth creation is expensive. For complex tasks requiring expert labeling, costs can easily reach tens of thousands of dollars. This needs to be part of the project budget from the beginning.

A common failure mode: the framing team defines ground truth requirements but does not allocate budget for creating it. The evaluation team inherits the requirement but has no resources to fulfill it. They either build with inadequate ground truth or delay the project while fighting for budget.

During framing, you need to estimate ground truth costs and get them approved. Here is how to estimate. Calculate the number of examples needed per task. Multiply by the time to label each example. Multiply by the hourly rate for the labeler type. Add overhead for quality control, rework, and platform costs.

For example: a generation task needs 500 examples, each requires 15 minutes of expert review to rate on multiple dimensions, experts cost 100 dollars per hour. That is 125 hours, or 12,500 dollars, plus 20 percent overhead for managing the process. Total: 15,000 dollars for one task.

If you have six tasks requiring similar ground truth, that is 90,000 dollars. That is not a rounding error. It needs to be in the project plan.

Some teams try to shortcut ground truth costs by using synthetic data or programmatic labeling. This works for some tasks, but not all. Synthetic data is great for augmentation but risky as the sole source of ground truth. Programmatic labeling works when you have clear heuristics, but many AI tasks are ambiguous by nature.

Budget real ground truth creation. It is cheaper than shipping a system you cannot evaluate.

## Ground Truth as a Dependency

Ground truth is a blocking dependency for evaluation. You cannot evaluate without it. This seems obvious, but teams regularly start implementation before ground truth is ready, then discover they cannot test anything.

The ground truth timeline needs to be part of the project timeline. If creating ground truth takes three months, evaluation cannot start until month four. If you need evaluation results to iterate on the system, and iteration takes two months, you cannot ship until month six. All of this should be visible during framing.

Some tasks allow parallel work. You can start building the system while ground truth is being created. But you cannot validate the system until ground truth is ready. Make this dependency explicit so nobody plans on timelines that ignore it.

For tasks with expensive ground truth, consider phased creation. Start with a small set to unblock initial evaluation. Expand it iteratively as you refine the system. This reduces upfront cost and lets you learn which examples are most valuable.

But even phased approaches need planning. You cannot just say "we will figure out ground truth later." You need to define the phase-one set, the timeline to create it, and the resource allocation. That happens during framing.

## Ground Truth Format Specifications

Different tasks need ground truth in different formats. The framing document should specify the exact format so the labeling team knows what to produce.

For extraction tasks: specify the schema for annotations. If you are extracting entities, define the entity types, the boundary marking convention, and the handling of nested or overlapping entities. If you are extracting key-value pairs, define the key taxonomy and the value format.

For classification tasks: specify the category list, the definition of each category, and the process for handling ambiguous cases. Include decision rules: if an example could fit multiple categories, which takes precedence?

For generation tasks: specify the format for reference examples and rubrics. What information does each example include? How are rubric dimensions defined? What anchoring examples illustrate each score level?

For reasoning tasks: specify the format for reasoning chains. How much detail is required at each step? Are intermediate calculations shown? Are alternative reasoning paths documented?

The format specification prevents rework. If the labeling team produces ground truth in the wrong format, you have to relabel, which costs time and money. Define the format during framing and validate it with the labeling team before they start work.

## Validating Ground Truth Quality

Once ground truth is created, you need to validate that it is correct and consistent. This validation process should be defined during framing.

For tasks with objective correctness, use inter-annotator agreement. Have two or three labelers label the same examples independently. Measure agreement with metrics like Cohen's kappa or Fleiss's kappa. If agreement is below 0.7, the labeling guidelines are ambiguous or the task is too complex for the chosen labelers.

For tasks with subjective judgments, use expert review. Have a senior expert review a sample of labels—typically 10 percent of the set. If they disagree with more than 5 percent of labels, investigate whether the labeling guidelines need revision or the labelers need retraining.

If validation reveals systematic errors, you need to fix them before using the ground truth for evaluation. Relabel the affected examples and update the guidelines to prevent the same errors in future labeling rounds.

Validation is not optional. Ground truth with uncaught errors will give you false confidence. You will think your system is performing well when it is actually conforming to incorrect labels.

## Ground Truth as a Living Artifact

Ground truth is not created once and frozen. It evolves as your understanding of the problem deepens and as you encounter new edge cases in production.

During framing, you should define the process for updating ground truth. Who decides when new examples are needed? Who labels them? How are they added to the test set? How do you track changes over time so you can compare results across versions?

A good practice is to version your ground truth like code. Each version has a clear definition, a change log, and evaluation results tied to it. When you update ground truth, you re-run all evaluations so you know whether apparent performance changes are due to system improvements or test set changes.

This process must be lightweight enough to use regularly but rigorous enough to maintain quality. Define it during framing, not during crisis response when you discover a gap.

Ground truth requirements are predictable. Map them during framing, budget for them, and create them before you need them. This prevents the scramble that kills timelines and budgets. The next section covers how to turn all these artifacts—task mappings, eval strategies, and ground truth specs—into concrete test cases you can run.
