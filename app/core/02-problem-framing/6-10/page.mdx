# 6.10 — Data Feasibility Checklist: Do We Have the Inputs, Logs, and Labels

In mid-2024, a healthcare startup set out to build an AI system that would analyze patient intake forms and automatically route patients to the appropriate specialist. The team spent eight weeks on problem framing. They defined the problem clearly, decomposed it into tasks, wrote detailed success criteria, and created precise I/O specifications. They got sign-off from medical directors, product leadership, and legal. They were ready to build.

Then someone asked: "Where is the training data?" The team assumed they had access to historical patient intake forms and routing decisions from the hospital's electronic health record system. When they reached out to the hospital IT department, they learned that intake forms were stored as scanned PDFs with no structured data extraction. The routing decisions existed only in clinicians' heads — there was no log of which patients went to which specialists and why. To get ground truth labels for evaluation, they would need clinicians to manually review and label thousands of historical cases, which would take six months and cost 200,000 dollars.

The project was not technically infeasible. The problem framing was solid. But it was **data infeasible**. They did not have the inputs, logs, or labels needed to build or evaluate the system. By the time they discovered this, they had already spent two months and significant budget on framing and planning. If they had run a data feasibility check at the beginning of framing, they would have known immediately that the project required a six-month data collection phase before any AI work could begin.

This is the classic "we found out too late" killer. Teams frame beautiful problems, define perfect success criteria, and spec detailed I/O — then discover they do not have the data to build or evaluate the system. Data feasibility is not a nice-to-have check. It is a go/no-go decision point that belongs at the very beginning of framing, not at the end.

This subchapter provides a comprehensive data feasibility checklist: the questions you must answer to confirm that you have the data needed to execute on your framing.

## The Data Feasibility Checklist: Seven Critical Questions

Data feasibility breaks down into seven questions. For each question, you need to answer yes or no. If the answer is no, you need a plan to fix it before you proceed. If the fix is expensive or slow, that becomes a project dependency that shapes your timeline and budget.

## Question 1: Do We Have Training Data?

**What this means**: Training data is the data you will use to build the system. For fine-tuning, this is labeled examples of inputs and desired outputs. For few-shot prompting, this is example demonstrations you include in the prompt. For RAG systems, this is the corpus of documents you will retrieve from.

**What yes looks like**: You have a dataset that contains examples of the inputs your system will handle and the outputs it should produce. The dataset is large enough to be useful. For fine-tuning, you typically need hundreds to thousands of labeled examples. For few-shot prompting, you need at least 5 to 20 high-quality examples per task. For RAG, you need a corpus that covers the knowledge domain your system will operate in.

**What no means**: You do not have training data, or the data you have is incomplete, outdated, or does not match the input distribution you expect in production. For example, you have historical customer support tickets, but they are from 2020 and your product has changed significantly since then. Or you have documents for a RAG corpus, but they are unstructured and need significant cleaning before they are usable.

**What to do about it**: If you do not have training data, your first project is building or acquiring it. This could mean labeling historical data, generating synthetic data, scraping or licensing external data, or running a data collection pilot with real users. Estimate the time and cost for data acquisition and add it to your project plan. If data acquisition takes six months, your timeline just got six months longer.

## Question 2: Do We Have Evaluation Data?

**What this means**: Evaluation data is the labeled dataset you will use to test your system. It must be separate from training data (no data leakage) and representative of the input distribution you will see in production. Each example in the evaluation set must have ground truth labels or expected outputs so you can measure whether your system is correct.

**What yes looks like**: You have a labeled evaluation dataset with at least 100 to 500 examples per task, depending on task complexity and risk tier. The dataset covers the range of inputs you expect in production: common cases, edge cases, ambiguous cases, and adversarial cases. The labels are high quality, ideally created or verified by domain experts. The dataset is version-controlled and documented so you can reproduce evaluation results.

**What no means**: You do not have evaluation data, or the evaluation data you have is too small, not representative, or not labeled with ground truth. For example, you have 50 labeled examples, which is not enough to measure performance reliably. Or your examples are all happy-path cases with no edge cases. Or your labels are crowdsourced from non-experts and you do not trust their quality.

**What to do about it**: If you do not have evaluation data, you need to create it. This is not optional. You cannot evaluate without it. Creating evaluation data typically involves: selecting a representative sample of inputs from production or synthetic generation, labeling them with ground truth using domain experts, and validating that the labels are consistent. Budget time and money for this. Labeling 500 examples with expert reviewers can take weeks and cost tens of thousands of dollars, depending on domain complexity.

## Question 3: Do We Have Production Logs?

**What this means**: Production logs are historical records of how users have interacted with your current system (if one exists) or similar systems. Logs contain real user inputs, system outputs, user behavior, and metadata like timestamps and session context. Logs are critical for understanding the real input distribution, identifying common failure modes, and generating realistic test cases.

**What yes looks like**: You have access to logs from a production system or a prototype. The logs contain the fields you need: user inputs, system outputs, timestamps, user identifiers (anonymized if necessary), and contextual metadata. The logs are clean enough to analyze without extensive preprocessing. You have enough log volume to identify patterns — ideally thousands to millions of interactions.

**What no means**: You do not have production logs because you are building something net-new, or you have logs but they are incomplete, inaccessible, or too messy to use. For example, logs exist but they are stored in a legacy system that your team cannot access. Or logs were never instrumented to capture the fields you need. Or logs contain so much noise that extracting useful signal requires a multi-week data engineering project.

**What to do about it**: If you do not have logs, you need to generate synthetic user interactions or run a pilot to collect real interactions. If you have logs but they are inaccessible or messy, you need to invest in data engineering to clean and structure them. If you are building something net-new and have no logs, plan for a phased rollout where you collect logs from early users before scaling.

## Question 4: Do We Have Ground Truth Labels?

**What this means**: Ground truth labels are expert annotations that define what the correct answer is for each example in your evaluation set. For a classification task, ground truth is the correct class label. For a generation task, ground truth is the ideal output or a rubric for judging output quality. Ground truth is what you measure your system against.

**What yes looks like**: You have expert-labeled ground truth for your evaluation dataset. The labels are created by people who understand the domain — doctors for medical tasks, lawyers for legal tasks, engineers for code tasks. The labels are consistent — if you gave the same example to two experts, they would produce the same or very similar labels. The labeling process is documented so you know how labels were created and can reproduce them.

**What no means**: You do not have ground truth labels, or the labels you have are low quality, inconsistent, or created by non-experts. For example, you crowdsourced labels from Mechanical Turk workers who do not understand your domain. Or you have labels from domain experts, but the experts disagreed on 40% of examples, indicating ambiguous criteria.

**What to do about it**: If you do not have ground truth labels, you need to create them. This requires recruiting domain experts, training them on your success criteria, having them label examples, measuring inter-rater agreement, and resolving disagreements through discussion or adjudication. This is time-consuming and expensive. Budget accordingly. If inter-rater agreement is low, you need to refine your success criteria to make them less ambiguous before labeling more data.

## Question 5: Do We Have Access to the Data?

**What this means**: Even if the data exists, can you actually use it? Access includes legal permissions (do you have the rights to use this data for AI training and evaluation?), technical access (can you retrieve and process the data?), and organizational access (do you have approval from the teams or vendors who own the data?).

**What yes looks like**: You have confirmed legal, technical, and organizational access to all the data you need. Legal has reviewed and approved data use. IT has provided access credentials and API endpoints. Data owners have signed off on your usage plan. You have tested that you can actually retrieve and process the data without hitting roadblocks.

**What no means**: The data exists but you cannot use it. Common blockers: data is subject to GDPR, HIPAA, or other privacy regulations and you have not obtained consent for AI use. Data is owned by a third-party vendor who has not granted access. Data is in a legacy system that IT will not give you credentials for. Data requires legal review that will take three months.

**What to do about it**: If access is blocked, you need to unblock it or find alternative data. Unblocking might involve: getting legal approval for data use (add legal review to your timeline), negotiating access with data owners, or building technical connectors to legacy systems. If unblocking is not feasible, you may need to generate synthetic data, license external data, or collect new data from scratch.

## Question 6: Is the Data Clean Enough?

**What this means**: Raw data is rarely usable as-is. Data quality issues include: missing fields, malformed values, duplicates, inconsistencies, noise, bias, and staleness. The question is whether the data is clean enough to use for training and evaluation without extensive preprocessing.

**What yes looks like**: You have reviewed a sample of the data and confirmed it is mostly complete and accurate. Missing fields are rare (under 5%). Malformed values are minimal and fixable with simple validation rules. The data is recent enough to reflect current reality. Any quality issues are documented and there is a plan to clean them.

**What no means**: The data is messy. Common issues: 30% of records have missing critical fields. Timestamps are unreliable. Free-text fields contain inconsistent formatting. The data is from 2019 and your domain has changed significantly. The data contains obvious errors that suggest the source system is buggy.

**What to do about it**: If the data is messy, you need to invest in data cleaning. Estimate the effort required: simple cleaning (removing duplicates, filling in missing values with defaults) takes days. Complex cleaning (resolving inconsistencies, deduplicating fuzzy matches, correcting errors) takes weeks or months. If cleaning is prohibitively expensive, you may need to find cleaner data or narrow your scope to use only the clean subset.

## Question 7: Is the Data Representative?

**What this means**: Does the data you have cover the input distribution you will see in production? If your production system will handle diverse user queries, your data needs to include that diversity. If it will encounter edge cases and adversarial inputs, your data needs to include those too.

**What yes looks like**: You have analyzed your data and confirmed it covers the major input types, user segments, and edge cases you expect in production. If you are building a customer support bot, your data includes questions from new users, power users, frustrated users, and edge cases like nonsensical inputs or attempts to jailbreak the system. If you are building a medical diagnosis assistant, your data includes common conditions, rare conditions, and ambiguous cases.

**What no means**: Your data is biased or incomplete. Common issues: your data is all from one user segment (power users but not novices), your data is all happy-path cases with no edge cases, your data is from a different context (you have data from email support but you are building a chat interface), or your data does not include adversarial examples.

**What to do about it**: If your data is not representative, you need to augment it. This might involve: collecting additional data from underrepresented segments, generating synthetic edge cases and adversarial examples, or sourcing external datasets that cover the gaps. Plan for the time and cost to do this before you start training or evaluating.

## The Feasibility Assessment: Green, Yellow, Red

For each of the seven questions, assign a status: **green** (data exists and is accessible), **yellow** (data exists but needs work), or **red** (data does not exist and must be created).

**Green**: You are data-ready. You can proceed with building and evaluation.

**Yellow**: You have data gaps that are fixable with reasonable effort. Estimate the time and cost to fix them and add it to your project plan. Typical yellow fixes take two to six weeks.

**Red**: You have major data gaps that require significant investment. Typical red fixes take three to twelve months and may require budget approval. Red status means you need to decide: invest in the data work, narrow your scope to what is data-feasible, or pause the project until data becomes available.

If any question is red, your project is at risk. Do not proceed to building until you have a clear plan to move it to yellow or green.

## How Data Feasibility Shapes Your Timeline and Budget

Data feasibility is not just a checklist — it is a planning input. The answers to these seven questions determine how long your project will take and how much it will cost.

If all seven questions are green, your timeline starts from framing and building. If three questions are yellow, add four to eight weeks for data work. If two questions are red, add six to twelve months for data acquisition and labeling.

Data work is also expensive. Labeling 1,000 examples with expert reviewers can cost 50,000 to 100,000 dollars depending on domain complexity. Cleaning messy datasets can require hiring data engineers. Acquiring external datasets can involve licensing fees.

Build data costs into your project budget from the beginning. Do not assume data is free or fast.

## The Build the Data Pipeline First Principle

If you do not have the data you need, that is your first project, not an afterthought. This is the "build the data pipeline first" principle.

You cannot train a model without training data. You cannot evaluate a system without evaluation data. You cannot improve a system without production logs. Data is the foundation. If the foundation is missing, everything else is built on sand.

This means: if your data feasibility assessment shows major gaps, **stop framing and start building the data pipeline**. Your first milestone is not "build the AI system." It is "collect and label the data needed to build the AI system."

This shift in mindset is critical. Many teams treat data as something they will "figure out later" and focus on the exciting work of building AI. Then they get stuck three months in because they do not have data to train on or evaluate with. The right approach is to front-load the data work. Get the data in place first, then build.

## Practical Example: A Data Feasibility Assessment

Here is what a real data feasibility assessment looks like for a customer support chatbot project.

**Question 1: Do we have training data?** Yellow. We have two years of customer support tickets, but they are unstructured email threads. We need to extract question-answer pairs and label them. Estimated effort: 4 weeks of data engineering + 2 weeks of labeling.

**Question 2: Do we have evaluation data?** Red. We have no labeled evaluation set. We need domain experts to label 500 examples. Estimated effort: 6 weeks + 30,000 dollars for expert labeling.

**Question 3: Do we have production logs?** Green. We have logs from our current support system with user inputs, agent responses, and resolution status.

**Question 4: Do we have ground truth labels?** Red. Same as question 2 — no expert labels yet.

**Question 5: Do we have access to the data?** Yellow. Legal needs to review data use for AI training under GDPR. Estimated effort: 2 weeks for legal review.

**Question 6: Is the data clean enough?** Yellow. 15% of tickets have missing fields. We need to clean and deduplicate. Estimated effort: 2 weeks.

**Question 7: Is the data representative?** Yellow. Our data is mostly from email, but we are building a chat interface. We need to collect a sample of chat interactions. Estimated effort: 4 weeks pilot with 100 users.

**Overall Assessment**: Two red (evaluation data and labels), four yellow, one green. Estimated time to move to green: 10 to 12 weeks. Estimated cost: 30,000 dollars for labeling + engineering time for data cleaning and pilot.

**Decision**: Pause AI development and focus on data work for three months. Revise project timeline to reflect data dependency.

## Practical Takeaways

**Run data feasibility at the beginning of framing, not the end**: Do not wait until you are ready to build to discover you do not have data.

**Answer all seven questions**: Training data, evaluation data, production logs, ground truth labels, access, quality, representativeness. Any gap is a risk.

**Assign green, yellow, red status**: Be honest about where you stand. Yellow and red require action plans with time and cost estimates.

**Budget for data work**: Data acquisition, cleaning, and labeling are expensive and slow. Build them into your timeline and budget from day one.

**Build the data pipeline first**: If you have major data gaps, make data work your first milestone. Do not build AI systems on missing foundations.

**Document your assessment**: Write down the answers to the seven questions, the status of each, and the plan to address gaps. Share this with stakeholders so everyone understands data dependencies.

Data feasibility is the unglamorous reality check that prevents expensive failures — but even with perfect data, your system can still be unsafe if you skip the next step: a thorough security review during the framing handoff.
