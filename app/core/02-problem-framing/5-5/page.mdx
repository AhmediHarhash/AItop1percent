# 5.5 â€” Output Quality Dimensions per Task Type

In mid-2025, a healthcare technology company with 180 employees spent four months building what their VP of Engineering called "the most comprehensively evaluated medical document parser in the market." Their system extracted patient information from clinical notes, and their evaluation framework measured 23 different quality dimensions including fluency, coherence, narrative consistency, and stylistic appropriateness. The team celebrated when their system achieved 94% on their aggregate quality score across all dimensions. Two weeks after launch, a hospital customer reported that the system was missing 40% of medication dosages and hallucinating allergy information that appeared nowhere in the source documents. The company's evaluation had measured the wrong things entirely. They had applied quality dimensions designed for text generation to what was fundamentally an extraction task, creating a measurement framework that gave perfect scores to outputs that were fluent, coherent, and completely wrong.

The hospital contract was paused pending a complete system rebuild. The engineering team spent six weeks developing a new evaluation framework focused on extraction-specific dimensions: exact match rates for structured fields, boundary detection accuracy for spans, precision and recall for each field type, and handling of missing versus null values. Their new aggregate score dropped to 71%, but this number actually meant something. It told them precisely where the system failed and what to fix. The original 94% had been meaningless theater, measuring how well their extraction system wrote prose instead of how accurately it extracted facts.

## Why Quality Dimensions Must Match Task Types

You cannot evaluate what you do not measure, and you cannot measure accurately if your dimensions do not align with your task type. Every AI task belongs to a fundamental category, and each category has its own intrinsic quality criteria. When you measure a classification system on fluency or an extraction system on creativity, you create what measurement theorists call construct invalidity. You are measuring something, but not the thing you claim to measure. Your scores become decorative rather than diagnostic.

The temptation to apply universal quality dimensions comes from a desire for simplicity. Product managers want one quality score that works everywhere. Engineers want reusable evaluation code. Executives want comparable metrics across different AI features. These desires are understandable and completely wrong. A dimension that matters critically for generation might be irrelevant for extraction, and a dimension that defines quality for classification might be impossible to measure for reasoning tasks. Task-type-specific dimensions are not a nicety but a necessity.

The five core task types each operate under different success criteria because they attempt fundamentally different operations. Generation creates new content, extraction finds existing content, classification assigns labels, transformation changes format while preserving meaning, and reasoning produces logical conclusions. These are not variations on a theme but distinct computational goals. Your evaluation framework must reflect this distinction or it will mislead you systematically.

## Generation Quality Dimensions

When your system produces new text, images, or structured content that did not exist in the input, you are performing generation. The quality dimensions for generation focus on the created artifact itself. **Fluency** measures whether the output reads naturally in the target language or format. A fluent generation follows grammatical rules, uses appropriate vocabulary, and flows without awkward constructions. You measure this through human evaluation, perplexity scores from language models, or grammaticality classifiers, depending on your domain and scale.

**Coherence** measures whether the generated content maintains internal consistency and logical flow. A coherent email response stays on topic across paragraphs. A coherent product description does not contradict itself about features. A coherent legal brief presents arguments in logical order without circular reasoning. Coherence operates at the document level while fluency operates at the sentence level. You can have fluent incoherence: every sentence is well-formed but the document makes no sense as a whole.

**Relevance** measures whether the generation addresses the input prompt or context appropriately. A highly fluent and coherent output that answers the wrong question fails on relevance. In customer support, relevance means addressing the customer's actual issue. In content marketing, relevance means matching the intended audience and goal. You measure relevance through semantic similarity to reference outputs, classifier models trained to detect off-topic responses, or human annotation of whether the output serves its purpose.

**Factual accuracy** measures whether claims made in the generation are true. This dimension matters enormously for informational content and barely at all for creative fiction. A financial report generation system must prioritize factual accuracy above nearly all else. A creative story generator might deprioritize it entirely. The mistake many teams make is assuming factual accuracy always matters or never matters, when the truth is task-dependent. You measure factual accuracy by checking claims against knowledge bases, running fact-checking models, or having domain experts verify specific statements.

**Tone** measures whether the generation matches the intended voice, formality level, and emotional register. A customer apology needs empathetic tone. A legal filing needs formal tone. A marketing email needs persuasive tone. Tone mismatches destroy trust even when the content is otherwise correct. A technically accurate but dismissive customer support response fails on tone. A legally sound but aggressive contract revision fails on tone when you needed collaborative. You measure tone through sentiment analysis, formality classifiers, and human ratings on specific dimensions like professionalism or warmth.

**Completeness** measures whether the generation includes all necessary information. A complete product description covers all key features. A complete email response addresses all questions from the original message. A complete report includes all required sections. Completeness differs from relevance because you can be relevant but incomplete, answering one of three questions thoroughly. You measure completeness by checking for required elements, comparing against checklists of necessary components, or having humans rate whether anything important is missing.

**Creativity** measures novelty and unexpectedness in the generation, and it only matters for specific use cases. A marketing slogan generator should optimize for creativity. A legal contract generator should minimize it. A technical documentation generator should avoid it entirely. Many teams waste resources measuring creativity for tasks where repetition and predictability are virtues. You measure creativity through diversity metrics across outputs, human ratings of novelty, or computational measures of deviation from training distribution.

## Extraction Quality Dimensions

When your system identifies and pulls specific information from existing content, you are performing extraction. The quality dimensions for extraction focus on finding exactly what exists, no more and no less. **Precision** measures what fraction of your system's extracted items are correct. If your system extracts 100 product names from a catalog description and 85 are actual product names, your precision is 85%. High precision means low false positives. You trust what the system extracts because it rarely invents things that are not there.

**Recall** measures what fraction of the true items in the source your system found. If a document contains 90 dates and your system extracts 70 of them, your recall is 78%. High recall means low false negatives. You trust that the system did not miss important information. The tension between precision and recall defines much of extraction quality. You can achieve 100% recall by extracting everything, but your precision will be terrible. You can achieve 100% precision by extracting only items you are absolutely certain about, but you will miss most of the content.

**Exact match** measures whether extracted values are character-for-character identical to the source. For many structured extraction tasks, close is not good enough. Extracting "March 15, 2025" when the document says "March 15th, 2025" might be semantically equivalent but fails exact match. Extracting "$1,000" when the document says "$1,000.00" fails exact match. Whether exact match matters depends on downstream use. If you are feeding values into a database with strict format requirements, exact match often matters critically. If you are summarizing for human review, semantic equivalence might suffice.

**Field-level accuracy** measures extraction quality separately for each type of information. A medical record extraction system might achieve 95% accuracy on patient names, 88% on medication names, 76% on dosages, and 62% on prescribing physician names. Aggregate accuracy hides these disparities. Field-level measurement reveals that your system handles some information types well and others poorly, guiding focused improvement. You cannot fix what you do not measure at sufficient granularity.

**Boundary correctness** measures whether extracted spans include exactly the right tokens, no more and no less. When extracting named entities from text, pulling "John Smith" is correct, pulling "John" is incomplete, and pulling "John Smith is" includes excess context. Boundary errors are particularly common at the edges of complex entities like multi-word product names or nested organizational structures. You measure boundary correctness by comparing character-level spans to gold-standard annotations.

**Handling of missing values** measures whether your system correctly identifies when requested information is absent versus incorrectly returning null for information that exists. A resume parser that marks "education" as null when the resume contains no education section behaves correctly. A parser that marks "education" as null when the resume says "PhD in Computer Science from MIT" fails. Many extraction systems conflate "I could not find this" with "This does not exist," creating systematic errors in downstream processing.

## Classification Quality Dimensions

When your system assigns inputs to predefined categories, you are performing classification. The quality dimensions for classification focus on decision accuracy and confidence. **Accuracy** measures what fraction of classifications are correct overall. If your sentiment classifier evaluates 1000 reviews and correctly labels 870 of them, your accuracy is 87%. Overall accuracy is necessary but insufficient because it hides class-specific failures.

**Per-class precision and recall** measure how well you classify each individual category. Your fraud detection classifier might have 95% precision and 45% recall on fraudulent transactions while having 98% recall and 92% precision on legitimate transactions. The aggregate accuracy of 93% hides the fact that you miss most fraud. Per-class metrics reveal the asymmetries that matter for product decisions. In medical diagnosis, missing a disease (low recall on positive cases) is much worse than false alarms (low precision on positive cases). In content moderation, false positives that remove legitimate content might be worse than false negatives that miss some violations.

**Confusion patterns** measure which classes your system confuses with each other. A customer support ticket router might frequently misclassify "billing question" as "account access issue" but rarely confuse "technical support" with "billing question." Understanding confusion patterns reveals systematic errors you can address. If two classes are consistently confused, you might need better distinction in your prompts, more training examples showing the boundary, or consolidation into a single category.

**Confidence calibration** measures whether your system's confidence scores correspond to actual accuracy. A well-calibrated classifier that reports 80% confidence should be correct about 80% of the time on cases where it reports that confidence level. Many AI systems are poorly calibrated, reporting high confidence on wrong answers and low confidence on correct ones. Miscalibration destroys trust because users cannot rely on confidence scores to decide when to trust the system. You measure calibration by bucketing predictions by confidence level and computing actual accuracy within each bucket.

**Handling of ambiguous inputs** measures how your system behaves when the correct classification is unclear or when multiple labels could reasonably apply. Some classification tasks have genuinely ambiguous cases. A customer message that both asks about a bill and reports a technical problem fits multiple categories. Your system can refuse classification, assign multiple labels, or force a single choice. Which approach is correct depends on downstream use, but you must measure how often ambiguous cases occur and how your system handles them.

## Transformation Quality Dimensions

When your system converts content from one format to another while preserving core information, you are performing transformation. Translation, summarization, and format conversion are all transformations. The quality dimensions for transformation focus on what is preserved and what is lost. **Information preservation** measures whether critical content survives the transformation. A document summarization system must preserve key facts and decisions while omitting details. A translation system must preserve meaning and intent while changing language. You measure information preservation by checking whether humans can answer the same questions about the transformed content as about the original.

**Format compliance** measures whether the output matches the required structure, schema, or linguistic conventions of the target format. A system that transforms clinical notes into HL7 FHIR messages must produce valid FHIR JSON. A system that translates English to Japanese must produce grammatically correct Japanese. A system that converts meeting transcripts to action items must produce items in the required template format. Format compliance is often binary: the output is valid or invalid. You measure it through schema validation, grammar checking, or template matching.

**Semantic equivalence** measures whether the transformation preserves meaning despite changing form. Two sentences can be semantically equivalent without being word-for-word identical. "The patient was discharged on March 15" and "March 15 was the discharge date for the patient" are semantically equivalent. "The patient was discharged on March 15" and "The patient went home on March 15" are similar but not equivalent because discharge is a formal medical process distinct from going home. You measure semantic equivalence through human judgment, by comparing embeddings in semantic space, or by checking whether logical inferences from the original remain valid for the transformation.

**Handling of untranslatable elements** measures how your system manages content that cannot be cleanly converted. When translating English to Japanese, some idioms have no direct equivalent. When summarizing technical documentation, some details cannot be omitted without losing critical information. When converting freeform text to structured data, some nuance cannot be captured in rigid fields. Your system can mark these elements explicitly, approximate them, or omit them. What matters is that you measure how often untranslatable elements occur and verify that your handling strategy serves user needs.

## Reasoning Quality Dimensions

When your system draws conclusions, makes inferences, or produces multi-step logical arguments, you are performing reasoning. The quality dimensions for reasoning focus on the soundness of the logic and the correctness of intermediate steps. **Logical validity** measures whether conclusions follow from premises. A reasoning system might produce true conclusions through invalid logic, or valid logic leading to false conclusions because of incorrect premises. You want both validity and soundness, but they are distinct dimensions. You measure logical validity by checking whether each inference step is justified, whether the reasoning avoids logical fallacies, and whether the conclusion necessarily follows from the premises.

**Completeness of reasoning chain** measures whether the system shows all necessary steps or skips crucial inferences. A math word problem solver that jumps from the problem statement directly to the answer without showing work might be correct but incomplete. A legal analysis system that states a conclusion without citing supporting precedent is incomplete. Completeness matters when humans need to verify reasoning, when the reasoning itself is the deliverable, or when intermediate steps are required for auditability. You measure completeness by counting expected reasoning steps and verifying that none are omitted.

**Correctness of intermediate steps** measures whether each step in a reasoning chain is individually correct, even if the final conclusion happens to be right. A system can reach the right answer through wrong reasoning, which is particularly dangerous because it appears to work until inputs shift slightly. In mathematical reasoning, each algebraic manipulation must be valid. In causal reasoning, each claimed cause-effect relationship must be supported. You measure intermediate step correctness by evaluating each step independently against ground truth or expert judgment.

**Handling of uncertainty** measures whether the system acknowledges and propagates uncertainty appropriately. Many reasoning tasks involve incomplete information or probabilistic relationships. A medical diagnosis system should reflect uncertainty in its conclusions when symptoms are ambiguous. A financial forecasting system should acknowledge uncertainty ranges. A legal reasoning system should note when case law is unsettled. Systems that express false certainty are often more dangerous than systems that reason slightly less well but acknowledge their limitations. You measure uncertainty handling by verifying that confidence levels match actual evidence strength and that the system expresses uncertainty when appropriate.

**Justification quality** measures how well the system explains its reasoning to users. A correct conclusion with no explanation is less useful than a correct conclusion with clear justification, and an incorrect conclusion with transparent reasoning is easier to debug than an incorrect conclusion with no explanation. Justification quality involves completeness, clarity, relevance of cited evidence, and appropriate level of detail for the audience. You measure justification quality through human ratings of explanation helpfulness, by checking whether justifications include necessary evidence, and by testing whether users can follow and verify the reasoning.

## Selecting Dimensions for Hybrid Tasks

Many real-world AI tasks combine multiple fundamental types. A customer support system might classify the issue, extract key details, reason about the appropriate response, and generate a reply. Each component needs evaluation on its type-specific dimensions. The classification component needs precision and recall per category. The extraction component needs field-level accuracy and boundary correctness. The reasoning component needs logical validity. The generation component needs fluency and tone.

You face a choice: evaluate each component separately or create a combined metric. Separate evaluation provides diagnostic power but requires more instrumentation and produces multiple numbers to track. Combined metrics are easier to communicate but hide component-level failures. The correct approach depends on your team's sophistication and your product's maturity. Early in development, separate component evaluation helps you identify which parts of the pipeline underperform. In production, combined metrics tied to business outcomes might suffice for monitoring, with component-level evaluation reserved for debugging.

When you do combine metrics, weight them according to user impact. If the customer support system's generated response tone matters more to satisfaction than extraction precision for internal logging, weight tone more heavily. If legal contract analysis's reasoning validity is critical but generation fluency is secondary, weight reasoning more heavily. Do not weight components equally by default. Equal weighting implies equal importance, which is rarely true.

## The False Confidence of Wrong Dimensions

Measuring the wrong dimensions does not just waste time; it actively misleads you. A high score on irrelevant metrics creates false confidence that your system works well. The healthcare document parser that scored 94% on generation quality dimensions was not just unevaluated but misevaluated. The team believed they had validated the system thoroughly. They had data, benchmarks, and impressive numbers. All of it measured the wrong things.

This false confidence is particularly dangerous because it passes through organizational layers. The engineering team reports great metrics to the product manager. The product manager reports successful testing to the executive team. The executive team approves launch. Each layer trusts the numbers without questioning whether the numbers measure what matters. The system fails in production not because nobody evaluated it but because everyone evaluated it against the wrong criteria.

The corrective is simple but requires discipline: before you measure anything, write down what success means for your specific task type, then select dimensions that directly measure those success criteria. If success means accurately extracting medication dosages from clinical notes, your dimensions should include dosage field precision and recall, not narrative coherence. If success means correctly classifying customer issues for routing, your dimensions should include per-class accuracy and confusion patterns, not creativity. The dimensions must match the task or they tell you nothing useful.

## Domain-Specific Dimension Applications

In legal AI, extraction tasks need extremely high precision on citations and entity recognition because a single wrong case citation can undermine an entire brief. Precision matters more than recall because a lawyer can tolerate the system missing some citations but cannot tolerate invented ones. Classification tasks for legal document categorization need per-class precision metrics because misrouting a court filing has serious procedural consequences. Reasoning tasks for legal analysis need justification quality because lawyers must verify and defend the AI's reasoning to courts and clients.

In healthcare AI, extraction tasks for clinical documentation need field-level accuracy metrics separate for different information types because extracting wrong medication dosages is more dangerous than extracting wrong appointment dates. Classification tasks for diagnosis support need confidence calibration because physicians rely on confidence scores to decide when to trust the system versus seek additional testing. Generation tasks for patient communication need tone appropriateness because the same information delivered in wrong tone can harm patient trust and compliance.

In e-commerce, extraction tasks for product attribute recognition from listings need high recall because missing attributes reduce search functionality and user experience. Classification tasks for product categorization need handling of ambiguous inputs because many products legitimately belong to multiple categories. Transformation tasks for translating product descriptions across languages need information preservation for key selling points while format compliance matters less because human review catches awkward phrasing before publication.

In customer support, generation tasks for response drafting need relevance and completeness because failing to address the customer's actual question forces escalation. Classification tasks for intent detection need per-class recall because failing to recognize a customer's issue category routes them to the wrong team and increases resolution time. Reasoning tasks for troubleshooting need completeness of reasoning chain because support agents must understand why the system suggested each step in order to explain it to customers.

## Mapping Quality Dimensions to Success Metrics

The quality dimensions you measure should connect directly to the success metrics that define whether your AI product achieves its goals. If your success metric is reduction in customer support ticket resolution time, your quality dimensions should measure aspects that affect resolution time: classification accuracy for routing, extraction recall for pulling ticket context, generation completeness for addressing all customer questions. If your success metric is contract review accuracy, your quality dimensions should measure extraction precision for key terms, reasoning correctness for identifying risks, and classification accuracy for contract type.

Many teams measure quality dimensions that have no plausible connection to success metrics. They measure creativity for systems where novelty has no value. They measure fluency for extraction systems where generated text is never shown to users. They measure overall accuracy when specific failure modes are what matters. This disconnect between quality dimensions and success metrics means that optimizing your measured quality does not improve your actual product outcomes.

The discipline is to draw the causal chain from quality dimension to user outcome. If you measure generation fluency, how does fluency affect user satisfaction, task completion, or business metrics? If you measure extraction recall, how does recall affect downstream processes and outcomes? If you cannot draw the chain, question whether the dimension matters. Some dimensions are diagnostic tools for debugging rather than optimization targets. That is fine, but you should not report them as primary quality metrics or optimize them at the expense of dimensions that actually matter.

You now understand that quality dimensions must be task-type-specific and that applying the wrong dimensions creates false confidence rather than no information. The next challenge is handling outputs that are neither fully correct nor fully wrong but exist in uncertain territory. Output specifications must address not just what good outputs look like but how to handle partial correctness, ambiguity, and cases where multiple outputs could be defensible.

