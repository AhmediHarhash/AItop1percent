# 4.1 â€” The Success Criteria Gap

The product launch party was in full swing. Champagne flowed, executives gave congratulatory speeches, and the engineering team finally relaxed after six months of intensive development. The AI-powered customer service assistant was live, handling real customer queries, and the dashboards showed healthy adoption numbers. Three months later, that same team sat in a tense conference room, unable to agree on whether their product was actually working.

The VP of Customer Experience insisted the system was failing because customers still called human support for complex issues. The engineering lead argued it was succeeding because the model's accuracy metrics looked strong in testing. The product manager pointed to user satisfaction scores that hovered around neutral. The CFO wanted to know if they were getting ROI, but nobody could agree on how to calculate it. They had built and shipped an AI product without ever defining what success looked like in concrete, measurable terms.

This scenario plays out with surprising frequency across organizations building AI systems. Teams invest months of effort, significant budget, and organizational capital into AI initiatives, only to discover after launch that they lack a shared, specific definition of success. The result is wasted cycles, misaligned expectations, unmeasurable progress, and ultimately, products that might be technically impressive but strategically unclear.

## The Vague Aspiration Problem

When you ask most teams what success looks like for their AI project, you get answers like these:

"The AI should give good answers."

"Users should be happy with the results."

"It should help reduce support costs."

"The system should be accurate and fast."

These statements feel like success criteria. They describe desired outcomes. They reflect genuine aspirations. But they are not actually criteria you can build against, test against, or evaluate against. They are vague aspirations masquerading as specifications.

The problem is not that these goals are wrong. The problem is that they are insufficiently specific to guide decision-making. What counts as a "good" answer? How happy is "happy"? How much cost reduction, by when, measured how? Accurate compared to what baseline, measured across what dataset? Fast enough for which use cases?

Without specificity, these aspirations cannot serve their essential functions. They cannot guide architectural decisions during development. They cannot inform prioritization choices when resources are constrained. They cannot settle disagreements when team members have different intuitions. They cannot determine whether the system is ready to ship. They cannot measure whether the launched system is succeeding.

Most teams recognize this problem in theory. They know they need specific criteria. Yet in practice, most teams still operate with criteria that are far too vague. The spectrum runs from completely vague aspirations on one end to precise, measurable targets on the other. Most teams land somewhere in the middle: criteria that are more specific than "give good answers" but still not specific enough to be truly useful.

## The Spectrum of Specificity

Consider how the aspiration "the AI should give good answers" might evolve:

At the low end: "Good answers" remains undefined. Different stakeholders have different mental models of what this means, but these models never get explicitly aligned.

Slightly better: "Good answers means accurate, helpful responses." Now we have two dimensions, but they're still subjective and unmeasurable.

Better still: "Accurate means factually correct based on our documentation. Helpful means answering the user's actual question." We're getting closer, but "factually correct" and "answering the question" are still somewhat fuzzy.

Much better: "Responses must contain only information present in the documentation, with citations. Responses must address the specific question asked, confirmed by having an independent rater assess whether the response answers the question."

Best: "Responses must contain only information present in the documentation, with exact citations to source documents. When tested against our evaluation dataset of 500 representative questions, at least 90% of responses must receive a score of 4 or 5 on our factual accuracy rubric, and at least 85% must receive a score of 4 or 5 on our relevance rubric, as judged by three calibrated human raters with majority agreement."

That final version is specific enough to actually use. You can test against it. You can make architecture decisions based on it. You can determine when you've succeeded. You can measure degradation if performance drops. You can compare alternatives based on how well they meet the criteria.

## Why Verbal Agreements Are Not Criteria

In the early stages of a project, it is common for success criteria to exist primarily in conversation. The PM explains to the engineering lead what good looks like. The engineering lead explains it to the team. Everyone nods. There is a shared sense that "we all know what we're trying to build."

This approach fails for several predictable reasons.

First, verbal descriptions are interpreted differently by different listeners. When someone says "accurate responses," the ML engineer might think about model accuracy metrics, the product manager might think about whether users find the information useful, and the executive sponsor might think about whether the system avoids embarrassing errors. These are related but distinct concepts, and the differences only surface later when disagreements emerge.

Second, verbal agreements are easily forgotten or misremembered. Six weeks into development, different team members will have different recollections of what was agreed upon. Without written criteria, there is no authoritative reference to resolve disputes.

Third, verbal discussions rarely force the level of specificity required for implementation. In conversation, it is easy to gloss over ambiguities with phrases like "you know what I mean" or "basically we want it to be good." When you are forced to write criteria down with precision, the ambiguities become visible and must be resolved.

Fourth, verbal agreements are not discoverable by people who join the project later. New team members, contractors, or consultants cannot go back and listen to the conversation where success was defined. They can only work from whatever written specifications exist.

The absence of written criteria does not prevent people from building. Engineers will implement based on their interpretation. But it does prevent the team from building the right thing in alignment. It guarantees that different parts of the system will optimize for different, potentially conflicting goals.

## The Cost of the Gap

The success criteria gap imposes several specific costs on AI projects.

Wasted development cycles are perhaps the most visible cost. Teams build features, test approaches, and iterate on implementations without a clear target. They build what seems right based on intuition, then discover later that it does not meet stakeholder expectations. The resulting rework is expensive in both time and morale.

Misaligned expectations create organizational friction. Stakeholders expect one kind of system, engineers build another, and users receive something neither group anticipated. The product manager expected a system that could handle complex queries, the engineers optimized for handling simple queries quickly, and users encounter a system that handles some cases well and others poorly with no clear pattern.

Unmeasurable progress makes it impossible to know if you are on track. Without concrete criteria, every demo and milestone review devolves into subjective assessments. Some people feel good about the progress, others feel concerned, but there is no objective way to assess whether the system is trending toward success.

Difficult launch decisions become more difficult still. Without clear criteria, the decision to ship becomes a matter of gut feel and political dynamics rather than data-driven assessment. Risk-averse stakeholders can always argue the system is not ready. Optimistic stakeholders can always argue it is good enough. Neither position can be definitively proven.

Post-launch confusion prevents effective iteration. Once the system is live, you need to know if it is meeting its goals so you can prioritize improvements. Without predefined criteria, every potential improvement becomes a matter of opinion rather than data. Should you focus on making responses faster or more detailed? Should you prioritize reducing errors or handling more edge cases? The answer depends on what success looks like, and if that was never defined, these decisions become political rather than analytical.

Perhaps most fundamentally, the gap prevents learning. One of the most valuable aspects of shipping AI products is learning what works and what does not. But learning requires measurement, and measurement requires criteria. Without criteria, you ship a product and have no rigorous way to assess its impact. You might have a vague sense that it is working or not working, but you cannot quantify what worked, what did not, and why.

## Where the Gap Comes From

Understanding why the success criteria gap is so common helps explain how to avoid it.

One driver is optimism at the outset. Early in a project, everyone is aligned and enthusiastic. The problem feels clear, the solution feels obvious, and the need for extensive specification feels like bureaucratic overhead. Teams want to start building, and stopping to write detailed criteria feels like a delay.

Another driver is the difficulty of the task itself. Defining good success criteria is genuinely hard. It requires thinking through edge cases, quantifying subjective qualities, and making difficult tradeoffs explicit. It is much easier to say "we want good responses" than to define what "good" means across different user segments, query types, and failure modes.

A third driver is the uncertainty inherent in AI projects. Unlike traditional software where requirements can be specified with precision, AI systems have fundamental performance uncertainty. You do not know exactly what the model will be capable of until you build and test it. This uncertainty makes some teams reluctant to commit to specific criteria upfront, fearing they will set targets that prove impossible or targets that are too easy.

A fourth driver is the cross-functional nature of AI projects. Success criteria often span multiple domains: technical performance, user experience, business impact, operational costs, safety and compliance. No single person has full ownership of all dimensions, and getting cross-functional alignment requires dedicated effort.

Finally, there is a cultural dimension. In some organizations, specificity feels risky. Writing down specific criteria creates accountability. If you commit to "90% accuracy" and achieve 85%, you have visibly failed. If you only ever said "high accuracy," you can argue that 85% is pretty good. This dynamic unconsciously pushes teams toward vagueness.

## Closing the Gap: Forcing Specificity Before Building

The solution to the success criteria gap is to force specificity before significant implementation begins. This does not mean you need perfect criteria before writing any code. It means you need concrete, written, measurable criteria before you commit to a particular architecture, before you scale up your team, and definitely before you announce a launch date.

The most effective approach is to make written success criteria a mandatory gate for project approval. Before engineering work begins, the project must have a document that specifies, at minimum:

Functional criteria: what the system must do correctly, with specific definitions of correctness and target performance levels.

Behavioral criteria: how the system should behave, including tone, style, safety requirements, and brand alignment expectations.

Performance criteria: latency, throughput, and cost targets that determine whether the system is operationally viable.

Negative criteria: explicit boundaries for what the system must never do, defining failure modes as precisely as success modes.

Each category of criteria will be explored in depth in the following sections, but the key is that all four must be addressed before building. It is acceptable for some criteria to be approximate early on, but they must be written down and agreed upon. "We think 90% accuracy but we will refine this after initial testing" is acceptable. "High accuracy" is not.

Forcing specificity requires assigning explicit ownership. Someone must be responsible for drafting the success criteria, circulating them for review, incorporating feedback, and getting formal signoff. This is typically a product management responsibility, but it requires close collaboration with engineering to ensure criteria are technically feasible and with business stakeholders to ensure criteria align with strategic goals.

It also requires a review process. Success criteria should be reviewed by all key stakeholders: engineering leadership, product leadership, the team that will maintain the system post-launch, representatives from impacted user groups if possible, legal and compliance if relevant, and executive sponsors. The review should explicitly address whether the criteria are specific enough, measurable enough, and complete enough.

One practical technique is the "how would we test this?" challenge. For each criterion, the team should be able to describe concretely how they would test whether the criterion is met. If you cannot describe a test procedure, the criterion is not specific enough. If the test procedure requires subjective judgment calls without clear rubrics, the criterion needs more detail.

Another technique is the "what does good look like?" exercise. For each criterion, the team should look at or create examples of outputs that clearly meet the criterion, clearly fail the criterion, and sit in the boundary zone. This exercise surfaces ambiguities and forces precision.

A third technique is the "alternative interpretation" challenge. For each criterion, team members intentionally try to interpret it in different ways, exposing places where the language is ambiguous. This is particularly valuable for catching cases where technical and non-technical stakeholders are using the same words to mean different things.

The investment in defining success criteria pays for itself many times over. An additional week spent on criteria definition can save months of rework. It aligns the team before effort is spent. It surfaces disagreements when they are cheap to resolve rather than expensive to unwind. It creates a foundation for measurement and learning. It transforms a vague aspiration into a concrete target.

Most importantly, it changes the nature of execution. With clear criteria, the team can build with confidence, knowing exactly what they are trying to achieve. They can make principled tradeoffs, prioritize effectively, and assess progress objectively. They can have data-driven discussions about whether the system is ready to ship. They can measure impact after launch and learn from what worked and what did not.

The success criteria gap is not an inevitable feature of AI projects. It is a choice, and it is a choice that determines whether your AI initiative will deliver measurable value or simply generate activity. In the next sections, we will explore how to define each category of success criteria with the precision required to close the gap and build AI systems that verifiably succeed.

