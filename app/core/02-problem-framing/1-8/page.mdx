# 1.8 â€” When to Re-Frame: Triggers and Signals

In early 2025, a fintech company launched an AI-powered investment recommendation system. The initial framing had been rigorous: clear success criteria, well-defined constraints, stakeholder sign-off from product, legal, and compliance. The model had passed all offline evaluations with strong performance. The first month after launch, usage was high and user feedback was positive. By month three, the team noticed something odd: users were following the recommendations less and less, even though the recommendations were getting more accurate over time according to the offline metrics. By month six, engagement had dropped by 60 percent.

What went wrong? The original framing had defined success as "recommend investments with higher expected returns than the user's current portfolio." The model was doing exactly that. But the team had not accounted for a critical factor: users cared more about risk-adjusted returns than raw returns, and the model's recommendations were optimizing for returns without properly accounting for volatility. The offline evaluation metrics were correct by the original framing, but the original framing had missed a key dimension of what users actually valued.

The team needed to reframe. The problem was not the model. It was the definition of the problem itself.

## The Tension Between Commitment and Flexibility

Reframing is hard because it requires admitting that the original framing was wrong. This is psychologically uncomfortable, especially when the team has invested months of work based on that framing. There is also organizational pressure to stay the course: stakeholders signed off on the original plan, roadmaps were built around it, resources were allocated. Changing the framing mid-project feels like failure.

But the alternative is worse. Sticking with the wrong framing leads to one of two outcomes: you ship a system that does not meet user needs, or you keep iterating on implementation hoping that better models or more data will fix the problem, when the real issue is that you are optimizing for the wrong goal.

The best teams treat reframing as a natural part of the development process, not a failure. The initial framing is a hypothesis. You commit to it long enough to learn whether it is correct, but you remain open to revising it when evidence accumulates that it is not.

The key is knowing when the evidence justifies reframing and when it is just noise.

## Trigger One: Persistent Evaluation Failures Despite Good Implementation

The clearest signal for reframing is when the system consistently underperforms on success criteria, even though the implementation is solid.

This is different from a bug or a modeling mistake. If the model is not converging during training, that is an implementation issue. If the inference pipeline has a latency spike, that is an infrastructure issue. These are problems you fix within the existing framing.

Reframing is needed when you have fixed all the implementation issues and the system still does not meet the success criteria. This usually means one of two things: the success criteria are unrealistic given the constraints, or the success criteria are not measuring what actually matters.

For the investment recommendation system, the team had tried multiple model architectures, expanded the training data, tuned hyperparameters extensively, and optimized the inference pipeline. The offline metrics kept improving. But user engagement kept declining. This was a signal that the offline metrics were not measuring the right thing.

**How to diagnose this:** Compare offline evaluation metrics to online engagement or outcome metrics. If offline metrics are strong but online metrics are weak, your evaluation protocol is not capturing what users care about. This is a framing problem, not a modeling problem.

The fix is to revise the success criteria in the framing document to include the missing dimension. For the investment system, the revised framing added risk-adjusted return metrics and user confidence scores alongside raw return predictions. This changed what the model optimized for and how it was evaluated.

## Trigger Two: User Feedback Does Not Match Expected Patterns

Another strong signal for reframing is when users describe the system in ways that contradict your original problem definition.

Imagine you framed the problem as "help users write emails faster" and built an AI drafting assistant. You expected users to use it for routine emails and provide feedback about speed and convenience. Instead, users are using it for high-stakes emails and providing feedback about confidence and accuracy. This is a mismatch. The problem you framed was about efficiency; the problem users are actually solving is about quality and risk reduction.

This happens more often than you might think, because user needs are hard to articulate in the abstract. Users often do not know what they need until they interact with a working system. The initial framing is based on stated needs; the reframing is based on revealed needs.

For a legal tech company, the original framing for a contract analysis tool was "reduce time spent reviewing contracts." The system was built to provide quick summaries and flag standard clauses. User feedback was lukewarm. When the team dug deeper, they found that users were not using the tool for routine contracts where speed mattered. They were using it for complex negotiations where thoroughness mattered. The problem was not speed; it was confidence that nothing important was missed.

**How to diagnose this:** Conduct user interviews and analyze support tickets for language that suggests a different problem framing. If users talk about "trust," "confidence," "understanding," or "risk," they are thinking about the problem differently than a framing focused on efficiency or cost reduction.

The fix is to update the problem statement and success criteria to reflect the revealed needs. For the legal tech company, the revised framing shifted from "reduce review time" to "increase reviewer confidence in identifying critical clauses," which led to different feature priorities and different evaluation metrics.

## Trigger Three: Stakeholder Disagreement on Whether the System Is Working

When stakeholders cannot agree on whether the system is succeeding, it is usually a sign that the original framing did not define success clearly enough, or that different stakeholders had different implicit definitions.

A healthcare company built an AI triage system for emergency departments. Three months after launch, the clinical staff said it was working well, the hospital administrators said it was failing, and the legal team said they could not tell. Why? The clinical staff evaluated it based on diagnostic accuracy, the administrators evaluated it based on patient throughput, and the legal team evaluated it based on liability risk. The original framing document had included all three goals, but it had not specified the relative priority or how to resolve conflicts when they arose.

This is a framing failure. The document should have made explicit what the primary success criterion was and what trade-offs were acceptable. Without that clarity, every stakeholder group filled in the gaps with their own assumptions, and now they are evaluating the system against different goals.

**How to diagnose this:** Run a stakeholder retrospective where each group independently describes whether the system is meeting expectations and why. If the answers are contradictory, you have a framing problem.

The fix is to revise the framing document to clarify the primary goal and the acceptable trade-offs. For the healthcare triage system, the revised framing specified that diagnostic accuracy was the primary goal, patient throughput was a secondary goal, and liability risk was a constraint. This gave the team a clear hierarchy for resolving conflicts.

## Trigger Four: Feature Creep Outside the Original Plan

If the team keeps building features that were not in the original framing, it is a sign that the framing did not fully capture the problem.

A customer support automation project was originally framed as "automate responses to tier-one questions about account balance and transaction history." Six months into development, the team had built features for password resets, fraud alerts, billing disputes, and account upgrades. None of these were in the original framing. Why did scope expand?

Two possibilities. One is that the team lost discipline and allowed scope creep. This is a project management problem. The other is that the original framing was too narrow, and as the team learned more about user needs, they realized that automating only balance inquiries was not valuable enough to justify the investment. This is a framing problem.

**How to diagnose this:** Review the feature backlog and identify features that are not traceable to the original framing. If more than 30 percent of the features are out of scope, you either have a discipline problem or a framing problem.

The fix is to decide whether the expanded scope reflects a better understanding of the problem or lack of focus. If it is a better understanding, revise the framing to include the new scope and update the success criteria and constraints accordingly. If it is lack of focus, refer back to the non-goals in the original framing and kill the out-of-scope features.

## Trigger Five: New Capabilities Make Original Constraints Obsolete

Sometimes the original framing was correct given the constraints at the time, but new model capabilities or tools remove those constraints, and the framing needs to be updated to take advantage.

In 2023, a document translation company framed their problem as "provide fast, rough translations for internal use only, not customer-facing." The constraint was that translation quality was not good enough for customer-facing use. By 2025, translation quality had improved dramatically with new models. The original framing was still guiding product decisions, but it was now too conservative. The constraint that motivated the framing no longer applied.

This is a subtle case, because the original framing was not wrong at the time. But sticking with it when the world has changed is a mistake.

**How to diagnose this:** Periodically review the constraints section of the framing document and ask: are these constraints still true? If a major constraint has been removed, revisit the problem statement and success criteria to see if you should be more ambitious.

The fix is to update the framing to reflect the new capabilities, which might mean expanding the scope, raising the quality bar, or targeting new use cases that were previously out of reach.

## Trigger Six: Market Shifts Change User Needs

External factors can invalidate the original framing. Regulatory changes, competitive moves, economic shifts, or changes in user behavior can all make the original problem less relevant or introduce new problems that are more urgent.

A travel booking company had framed their AI recommendation system around "suggest destinations based on user preferences." In 2024, a major airline alliance changed their loyalty program rules, making certain routes much more attractive. Suddenly, users cared more about maximizing loyalty points than about destination preferences. The original framing was not wrong, but it was no longer the highest-value problem to solve.

**How to diagnose this:** Monitor market signals and user feedback for language that suggests shifting priorities. If users start asking for features or optimizations that are not in the original framing, it is a signal that the environment has changed.

The fix is to reassess whether the original problem is still the right problem. If the market has shifted, it may be worth pausing the current project to reframe around the new priority, or to add the new priority as a co-equal goal.

## Signals vs Noise: When Not to Reframe

Not every problem is a framing problem. Some issues are implementation issues, data issues, or just bad luck. Reframing too often is as bad as reframing too rarely. The key is distinguishing signals from noise.

**Noise:** A single bad eval run, one negative user review, one stakeholder complaint, a temporary dip in engagement, a bug that causes incorrect outputs.

**Signal:** Consistent pattern of underperformance across multiple evals, systematic mismatch between user feedback and expected behavior, persistent stakeholder disagreement, ongoing scope creep, removal of major constraints, sustained market shift.

The difference is consistency and persistence. Noise is transient and local. Signals are persistent and systematic.

A good rule of thumb: do not reframe based on a single data point. Wait for convergent evidence from multiple sources. If offline metrics are bad and user feedback is bad and stakeholder alignment is breaking down, that is convergent evidence. If offline metrics are bad but user feedback is good and stakeholders are aligned, that is probably a metric problem, not a framing problem.

## The Reframing Process: What to Keep and What to Discard

When you decide to reframe, you do not start from scratch. Some parts of the original framing are still valid; others need to be revised.

**What to keep:** The business context, the stakeholder list, the integration constraints, the compliance requirements. These are usually stable.

**What to revisit:** The problem statement, the success criteria, the task decomposition, the non-goals. These are where the learning happens.

The reframing process should be faster than the original framing, because you have more data and more context. You are not speculating; you are incorporating evidence.

**Step one:** Write down what you learned that contradicts the original framing. For the investment recommendation system: "We assumed users cared most about maximizing returns, but usage patterns show they care more about risk-adjusted returns."

**Step two:** Revise the problem statement and success criteria to incorporate the new learning. "Recommend investments that maximize risk-adjusted returns within the user's stated risk tolerance."

**Step three:** Update the task decomposition and evaluation protocol to align with the new framing. This might mean adding new model components, such as a volatility estimator, or changing the evaluation metrics to include Sharpe ratio alongside raw return.

**Step four:** Get stakeholder buy-in for the revised framing. This is critical. If stakeholders are not aligned on the change, you will end up with the same misalignment issues that triggered the reframe.

## The Cost of Not Reframing When You Should

The biggest cost of failing to reframe is wasted effort. The team keeps iterating on implementation, trying different models, tuning hyperparameters, expanding the dataset, when the real problem is that they are solving the wrong problem.

This is insidious because the work feels productive. The models are getting better. The metrics are improving. The engineering is solid. But none of it translates to user value, because the metrics are measuring the wrong thing.

The second cost is opportunity cost. While the team is stuck optimizing the wrong problem, competitors are shipping solutions to the right problem. By the time you realize the framing is wrong, the market may have moved on.

The third cost is organizational trust. When a project drags on without delivering value, stakeholders lose confidence in the team. Even if the team eventually reframes and ships a successful product, the perception is that the project took too long and cost too much.

The best teams build reframing into the process. The framing document is a living artifact, not a static one. It is revisited at regular intervals and updated when evidence warrants. Reframing is not a failure; it is a sign that the team is learning.

## Building a Reframing Culture

The final challenge is cultural. In many organizations, changing direction mid-project is seen as a sign of poor planning or indecisiveness. This makes teams reluctant to reframe even when they should.

The antidote is to normalize reframing as part of the development process. Treat the initial framing as a hypothesis and specify in advance what evidence would cause you to revise it. Include decision points in the project plan: "After the first round of user testing, we will revisit the success criteria." "If offline metrics and online engagement diverge, we will reframe."

This shifts the conversation from "Should we change the plan?" to "What have we learned, and does it trigger one of our pre-specified decision points?" It makes reframing feel like following the process, not abandoning it.

The teams that ship high-impact AI systems are not the ones that get the framing right on the first try. They are the ones that recognize when the framing is wrong and have the discipline and organizational support to fix it.

In the next chapter, we will shift from process to taxonomy: how to break down high-level problems into specific AI tasks, and how to choose the right task formulation for your problem.

