# 5.6 — Handling Uncertainty in Outputs

In early 2025, a healthcare scheduling startup deployed an AI system to extract appointment details from patient phone calls. The system transcribed calls, extracted dates and times, and auto-populated the scheduling system with 94% accuracy on their test set. Within three weeks of launch, they'd created over 200 scheduling conflicts because the system confidently extracted wrong dates and times from ambiguous utterances. A patient said "next Thursday" on a Friday afternoon, and the system picked the Thursday six days away instead of the Thursday thirteen days away. Another said "two-thirty" and the system wrote 2:30 PM when the patient meant 2:30 AM for a pre-surgery prep call. The operations team spent $40,000 in overtime pay to manually call and reschedule patients. The root cause was not accuracy. The system had been designed to always output a date and time, even when the model's internal probability distribution showed massive uncertainty between multiple interpretations. They had optimized for never saying "I don't know," and it destroyed their launch.

This failure was conceptual, not technical. The team had designed an output schema that forced the model to commit to an answer in every case. They never included a mechanism for the system to express uncertainty, to abstain, or to escalate ambiguous cases to a human. They treated the AI like a deterministic function that maps inputs to outputs, when in reality it is a probabilistic system that should map inputs to distributions over outputs, and sometimes those distributions are flat and uninformative. You must design for uncertainty from the start, or your system will confidently lie to you in production.

## Why AI Systems Must Communicate Uncertainty

LLMs do not produce deterministic outputs. They produce probability distributions over token sequences, and the decoding process samples from or greedily selects from those distributions. When the model is very confident, the top token has a probability close to one and the output is reliable. When the model is uncertain, the probability mass is spread across many plausible tokens, and the output is a guess. The problem is that the text the model generates sounds equally confident in both cases. A model that is 95% sure and a model that is 40% sure will both produce fluent, authoritative-sounding sentences, because fluency and confidence are properties of the training data, not properties of the model's internal uncertainty.

This creates a dangerous mismatch between how the output reads and how much you should trust it. Users read confident-sounding text and assume the system knows what it's talking about. Product managers read high-quality prose and assume the model is certain. Engineers read structured JSON outputs and assume the extraction was clean. None of these assumptions are safe. You must explicitly surface uncertainty in the output, or everyone downstream will treat guesses as facts.

The **confidence score** is the most common mechanism for communicating uncertainty. It is a scalar value, typically between zero and one, that represents how confident the system is in the output it produced. But confidence scores are not automatic, and they are not the same as raw model probabilities. The logits or token probabilities that come out of the model's final layer are not calibrated confidence scores. A model might assign 0.8 probability to the top token and be wrong 40% of the time, because the model has not been trained to align its internal probabilities with real-world correctness rates. You must calibrate the confidence score separately.

**Calibration** means ensuring that when your system says it is 90% confident, it is actually correct 90% of the time. You calibrate by collecting a validation set of examples, running the model, recording the confidence scores, and comparing them to ground truth. If the model says 90% confident on 100 examples and only 70 of them are correct, the model is overconfident and you need to adjust the mapping from internal probabilities to reported confidence. Techniques like Platt scaling or isotonic regression can re-map uncalibrated scores to calibrated ones. The key is that calibration is an empirical process, not a theoretical one. You measure the model's performance across confidence buckets and adjust the score function until the buckets align with reality.

## Designing Output Schemas That Include Uncertainty Signals

Your output schema must have fields for uncertainty. This is not optional. Every structured output should include at least one of the following: a confidence score, an abstention flag, or an explanation of the uncertainty source. The confidence score is a number. The abstention flag is a boolean that indicates the system refused to answer. The explanation is a short text string that tells the user why the system is uncertain, such as "Multiple plausible interpretations of the date" or "Insufficient context in the input."

Here's what this looks like in practice. An extraction task that pulls a due date from a contract might have an output schema with four fields: the extracted date, a confidence score between zero and one, an abstention flag, and an uncertainty reason. If the model is very confident, the date field is populated, the confidence score is 0.95, the abstention flag is false, and the uncertainty reason is null. If the model is uncertain, the date field might still be populated with the best guess, but the confidence score is 0.40, the abstention flag is true, and the uncertainty reason says "Ambiguous reference to 'next quarter' without fiscal year context." The downstream system can then route this case to a human reviewer instead of auto-processing it.

The abstention flag is critical. It gives the system permission to say "I don't know." Many teams are afraid of abstention because they want the system to always provide value, but abstention is value. A system that refuses to answer when it's uncertain is far better than a system that guesses and presents the guess as fact. The healthcare scheduling startup would have saved $40,000 if their system had abstained on the ambiguous date cases and flagged them for manual review. Abstention is not a failure mode. It is a designed success mode for uncertain cases.

You also need to think about **partial uncertainty**. Sometimes the model is confident about part of the output and uncertain about another part. An invoice extraction system might be very confident about the vendor name and invoice number but uncertain about the line item descriptions. Your schema should allow field-level confidence scores, not just a single top-level score. This way the downstream system can accept the confident fields and escalate only the uncertain ones, rather than treating the entire output as unreliable.

## When to Abstain: The "I Don't Know" Design

The decision to abstain should be governed by a **confidence threshold**. You set a minimum confidence score, and if the system's confidence is below that threshold, it abstains. The threshold is not arbitrary. It should be set based on the cost of errors and the tier of the task. Tier 3 tasks, which are high-stakes and intolerable if wrong, need very high confidence thresholds. A system that auto-approves insurance claims might require 0.98 confidence before it processes a claim without human review. Tier 1 tasks, which are low-stakes and tolerable if wrong, can use lower thresholds. A system that suggests email subject lines might only need 0.60 confidence to show a suggestion, because the user will review it anyway.

Setting the threshold is an empirical process. You start with a guess, deploy the system, measure the precision and recall at that threshold, and adjust. If you're getting too many false positives, you raise the threshold. If you're abstaining too often and humans are overwhelmed with review work, you lower it. The right threshold balances automation rate against error rate, and it will be different for every task and every context.

You also need to consider **dynamic thresholds** based on input characteristics. Some inputs are inherently harder than others, and you might want higher confidence thresholds for those cases. A document extraction system might use a 0.85 threshold for clean, typed PDFs and a 0.95 threshold for handwritten or low-quality scans. An agentic system that routes customer support tickets might use a 0.80 threshold for tickets that match known patterns and a 0.90 threshold for edge cases that don't fit any historical category. The threshold can be a function of the input, not just a global constant.

The other side of abstention design is the **escalation workflow**. When the system abstains, where does the task go? You need a clear handoff to a human or a fallback system. This might be a queue that human reviewers pull from, or an alert that pings a Slack channel, or a form that asks the user to manually fill in the uncertain fields. The escalation path must be fast and low-friction, or the abstention will just create a bottleneck. The healthcare startup could have built an escalation workflow where uncertain appointment extractions went into a review queue that schedulers checked every hour, but they never designed that workflow because they never designed for uncertainty in the first place.

## The Overconfidence Problem in Modern LLMs

LLMs in 2025 and 2026 are systematically overconfident. This is not a bug. It is a feature of how they are trained. The models are trained on human-written text, and human-written text is confident. People do not write "I think this is probably correct but I'm only 70% sure." They write declarative sentences that sound authoritative. The model learns to mimic that style, and the result is that even when the model is uncertain, it produces text that sounds certain.

This problem is worse in instruction-tuned models like GPT-4, Claude Opus 4.5, and Gemini 2.0. These models are fine-tuned to be helpful and to follow instructions, which means they try very hard to give you an answer even when they don't have enough information. If you ask an ambiguous question, the model will pick the most plausible interpretation and run with it, rather than asking for clarification. If you ask for information the model doesn't know, it will sometimes hallucinate a plausible-sounding answer rather than saying "I don't have that information." This is not dishonesty. It is the model optimizing for the reward signal it was trained on, which penalizes unhelpful responses and rewards confident, complete answers.

You cannot trust the tone of the output as a signal of confidence. You must measure confidence separately, using the techniques described earlier. But even with calibrated confidence scores, you need to be aware that the model's text output might mislead users. If your system surfaces the raw model output to end users, you need to add **uncertainty cues** in the UI. This might be a disclaimer like "This answer is uncertain — please verify before acting on it," or a visual indicator like a yellow warning icon next to low-confidence outputs, or a rephrasing of the text to include hedging language like "This appears to be" or "Based on the available information."

Some teams try to solve this by instructing the model to include hedging language in its outputs when it is uncertain. This works sometimes, but it is not reliable. The model might ignore the instruction, or it might add hedging language even when it is confident, or it might hallucinate the hedging language without actually being uncertain. The safer approach is to measure confidence separately and add the uncertainty cues in post-processing, outside the model, so you have full control over when and how they appear.

## Surfacing Uncertainty to Users: UI and Language Patterns

The way you surface uncertainty depends on the user and the context. For internal tools used by expert reviewers, you can show raw confidence scores and detailed explanations. For consumer-facing products, you need simpler signals that non-technical users can understand. A confidence score of 0.73 means nothing to a patient trying to book an appointment. A message like "We're not sure about this date — please confirm" is much clearer.

**UI patterns** for uncertainty include traffic light colors, where green means high confidence, yellow means medium confidence, and red means low confidence or abstention. You can also use progressive disclosure, where low-confidence outputs are hidden behind a "Show uncertain results" toggle, so users don't see them unless they explicitly ask. Another pattern is the **confidence bar**, a horizontal bar that fills up based on the confidence score, similar to a signal strength indicator. These visual cues are faster to process than reading text explanations.

**Language patterns** for uncertainty include hedging words like "possibly," "likely," "appears to be," and "based on limited information." You can also use conditional phrasing like "If this is correct" or "Assuming the input is accurate." The key is to match the level of hedging to the level of uncertainty. A 0.95 confidence output might say "This is the due date," while a 0.60 confidence output says "This appears to be the due date, but please verify." You can generate this language programmatically based on the confidence score, or you can have the model generate it and then verify that it matches the measured confidence.

For high-stakes tasks, you might want to **disable auto-action** on uncertain outputs entirely. Instead of showing a low-confidence answer and hoping the user notices the warning, you just don't show the answer at all and force the user to manually complete the task. This is the safest approach for Tier 3 tasks where errors are catastrophic. The insurance claim system should not show an uncertain auto-approval. It should just route the claim to a human adjuster and not present the model's guess.

## Uncertainty in Agentic Systems

Agentic systems introduce a new type of uncertainty: **action uncertainty**. The model is not just uncertain about what the answer is. It is uncertain about what to do next. An agent that is trying to book a flight might be uncertain whether to call the search tool, the booking tool, or the customer profile tool. It might be uncertain about the arguments to pass to the tool. It might be uncertain about how to interpret the tool's response and what action to take based on that response.

You handle action uncertainty the same way you handle output uncertainty: with confidence scores, abstention, and escalation. The agent's action selection can include a confidence score for each possible action, and if no action has confidence above the threshold, the agent abstains and asks the user for guidance. This might look like "I'm not sure whether to search for flights or check your saved preferences first — which would you like me to do?" The agent surfaces its uncertainty and lets the user resolve it.

Another pattern is **rollback on uncertainty**. The agent takes an action, measures its confidence in the result, and if the confidence is too low, it undoes the action and tries a different approach. A booking agent might call the search tool, get back ambiguous results, realize it is uncertain which result matches the user's intent, and escalate to the user with a clarifying question instead of proceeding with a guess. This requires the agent to track its confidence at every step and have rollback logic built into the workflow.

The challenge with agentic systems is that uncertainty compounds across steps. If the agent is 90% confident at each of five steps, the overall confidence in the final outcome is 0.9 to the fifth power, which is about 59%. You need to track cumulative confidence across the agent's trajectory and set thresholds based on the end-to-end confidence, not just the confidence at individual steps. Some teams build a **confidence budget**, where the agent starts with a total allowable uncertainty and spends some of that budget at each step, and if it runs out of budget before completing the task, it escalates.

## The Relationship Between Uncertainty Handling and Human-in-the-Loop Design

Uncertainty handling is the foundation of human-in-the-loop systems. You cannot build an effective human-in-the-loop workflow without knowing when to loop the human in, and that decision is based on uncertainty. The system measures its confidence, compares it to a threshold, and if the confidence is too low, it routes the task to a human. The human reviews the uncertain case, provides the correct answer or action, and that feedback can be used to retrain the model and improve future confidence.

The **review interface** for uncertain cases should show the model's output, the confidence score, and the explanation of why the system is uncertain. This helps the reviewer understand what the model was thinking and make a more informed decision. If the system abstained on a date extraction because "Multiple plausible interpretations of 'next quarter,'" the reviewer can see the input text, see the model's top two guesses, and pick the correct one based on context the model didn't have. That decision becomes a new training example that teaches the model how to handle similar cases in the future.

You also need to measure **reviewer agreement with the model's uncertainty**. If the model says it is uncertain and the reviewer immediately picks the model's top guess without hesitation, the model might be under-confident and you should lower the threshold. If the model says it is confident and the reviewer frequently overrides it, the model is over-confident and you need better calibration. This feedback loop between the model's uncertainty and the reviewer's corrections is how you improve the system over time.

Some teams build **active learning** systems where the model's uncertainty is used to prioritize which examples to label next. The system processes a batch of inputs, measures confidence on each one, and sends the lowest-confidence cases to human labelers. Those labels are added to the training set and the model is retrained, which improves performance on the previously uncertain cases. This is much more efficient than random sampling, because you're focusing labeling effort on the examples where the model needs the most help.

The human-in-the-loop design must also account for reviewer capacity and availability. If your system abstains on 30% of inputs during peak hours and you only have two reviewers available, you will create a backlog that degrades the user experience just as much as wrong answers would have. Your uncertainty thresholds should be calibrated not just to accuracy targets but also to throughput constraints. During high-volume periods, you might need to lower your threshold slightly and accept more risk, or you need to scale your review team, or you need to implement tiered escalation where only the most uncertain cases go to expert reviewers while moderately uncertain cases get lightweight review or delayed processing.

## Uncertainty Metrics and Monitoring in Production

Once your system is deployed, you must continuously monitor uncertainty-related metrics to detect calibration drift, threshold mismatches, and emerging failure modes. These metrics are not nice-to-have observability features. They are operational requirements that determine whether your system remains trustworthy over time.

The first metric is **abstention rate**, which measures what percentage of inputs trigger abstention. If this rate suddenly increases, it signals that your production inputs have shifted away from your training distribution, or that your model performance has degraded, or that upstream changes are feeding the system different data than expected. A sustained high abstention rate means your automation value is declining and your human review team is overwhelmed. A sustained low abstention rate might mean your threshold is too permissive and you are accepting unreliable outputs.

The second metric is **override rate**, which measures how often human reviewers override the system's outputs. If reviewers consistently agree with the system's answers, your confidence threshold might be too conservative and you could automate more. If reviewers frequently override confident outputs, your calibration has drifted and you need to recalibrate immediately. Override rate should be tracked separately for different confidence bands, because the pattern of overrides tells you where your calibration is broken.

The third metric is **precision and recall at threshold**, which measures actual correctness rates for outputs that exceed your confidence threshold. Precision tells you what percentage of confident outputs are actually correct. Recall tells you what percentage of correct answers your system is producing versus abstaining on. These should match your target values from your initial calibration. If precision drops, you are letting too many wrong answers through. If recall drops, you are abstaining too aggressively and losing automation value.

The fourth metric is **confidence distribution shift**, which tracks how the distribution of confidence scores changes over time. If you see the distribution narrowing, with most scores clustering near the threshold, it suggests that your model is becoming less certain overall, possibly due to input drift or model degradation. If you see the distribution widening, with more very high and very low scores, it might indicate that your inputs are becoming more polarized between easy and hard cases. Monitoring this distribution helps you anticipate when recalibration will be needed.

You should also track **uncertainty reason frequencies**, if your system generates explanations for why it abstained. Patterns in these reasons reveal systematic gaps in your model's capabilities. If "insufficient context" is the most common abstention reason, you might need to improve your input requirements or provide richer context in prompts. If "ambiguous input format" dominates, you need better input validation. These patterns guide your model improvement priorities and your data collection efforts.

## The Economics of Uncertainty: Cost Models for Abstention

Every abstention decision has an economic cost, and your threshold setting should be informed by explicit cost modeling. This is not an abstract exercise. It is a practical calculation that determines whether your AI system delivers positive ROI or destroys value.

The cost of a confident wrong answer includes the direct damage caused by the error, the cost of detection and correction, the opportunity cost of delayed correction, and the reputational harm. For the healthcare scheduling system, a wrong appointment time costs one patient reschedule call at approximately fifteen dollars in labor, one potential missed appointment at approximately two hundred dollars in lost revenue, and cumulative trust erosion that is hard to quantify but real. If your error rate is 5% and you process one thousand appointments per day, that is fifty errors per day, which is seven hundred fifty dollars in direct labor costs and ten thousand dollars in lost revenue, not counting the long-term brand damage.

The cost of abstention includes the labor cost of human review, the delay introduced by routing to a human, and the reduced automation value. For the same healthcare system, a human-reviewed appointment takes an average of three minutes of scheduler time at approximately thirty dollars per hour, which is one dollar fifty cents per appointment. If you abstain on 20% of appointments, that is two hundred manual reviews per day, which is three hundred dollars in labor. But critically, this cost is linear and predictable. You can staff for it. You can budget for it. You can manage it.

Comparing these costs gives you a rational basis for threshold setting. If a wrong answer costs fourteen dollars in expected value and a human review costs one dollar fifty cents, you should abstain whenever your expected error cost exceeds one dollar fifty cents, which means you should abstain when your confidence is below approximately 89%. This is a stylized example, but the method is general: quantify both costs, set the threshold where they balance, and adjust based on observed outcomes.

Some systems use dynamic cost models where the cost of errors or the cost of abstention changes based on context. During business hours when human reviewers are available and appointments are scheduled weeks in advance, the cost of delay from human review is minimal. During evenings and weekends when reviewers are offline and appointments are urgent, the cost of delay is much higher. Your threshold might be higher during business hours, when you can afford to abstain more, and lower during off-hours, when you need to maximize automation even at the cost of higher error rates.

## Architecting for Uncertainty: System Design Patterns

Uncertainty handling affects your entire system architecture, not just your model inference layer. You need infrastructure to measure confidence, store uncertainty metadata, route uncertain cases, surface uncertainty to users, and monitor uncertainty metrics. This infrastructure must be built intentionally, not bolted on after the fact.

The **confidence measurement layer** sits between your model and your application logic. It takes the raw model output, computes a calibrated confidence score using your validation-derived calibration function, determines whether the output meets your threshold, and decides whether to proceed or abstain. This layer must be deterministic, versioned, and testable. When you update your calibration function, you version it, test it against a holdout set, and deploy it alongside your model. The confidence measurement logic is as critical as the model itself, and it deserves the same engineering rigor.

The **uncertainty router** takes abstained outputs and directs them to the appropriate fallback. This might be a human review queue, a secondary model with different capabilities, a rule-based fallback system, or a user prompt requesting clarification. The router must be instrumented so you can measure where uncertain cases are going, how long they take to resolve, and what the resolution outcomes are. This telemetry feeds back into your threshold tuning and your capacity planning for review teams.

The **metadata propagation layer** ensures that uncertainty information flows with the output through your entire system. Every output carries its confidence score, its uncertainty reason if applicable, and a flag indicating whether it was abstained or accepted. Downstream systems can check this metadata and adjust their behavior accordingly. A reporting system might exclude abstained values from aggregates or mark them as provisional. An API consumer might choose to retry with additional context when it receives a low-confidence response. This metadata is part of your output contract and must be stable and well-documented.

The **user experience layer** translates uncertainty into user-facing language and affordances. For internal tools, this might be a detailed confidence dashboard showing scores, reasons, and historical trends. For consumer products, this might be simple visual indicators and plain-language explanations. The UX must match your users' sophistication and decision-making needs. A professional using the system daily can learn to interpret confidence scores. A casual user needs simpler signals. The worst outcome is showing raw confidence scores to users who do not understand them and cannot act on them.

## Testing Uncertainty Handling Before Production

You must test your uncertainty handling infrastructure as rigorously as you test your model. This means creating test cases that specifically target uncertainty edge cases, validating that your calibration is stable, and verifying that your escalation workflows function under load.

Build a test set of **known-uncertain inputs** where you have ground truth but the inputs are inherently ambiguous. For the contract date extraction example, this might include contracts with conditional clauses, contracts referencing external schedules, and contracts with typos or formatting errors. Run these inputs through your system and verify that it correctly identifies them as uncertain, that the confidence scores are calibrated, and that the escalation workflow triggers appropriately. These test cases should be permanent fixtures in your test suite, not one-time exploratory tests.

Test your **calibration stability** by running your model on a fixed validation set multiple times and verifying that confidence scores are consistent. If you see significant variance in confidence scores for the same input across multiple runs, your calibration is unstable and you need to investigate whether it is due to model non-determinism, calibration function sensitivity, or bugs in your measurement logic. Confidence scores should be reproducible given the same model version and calibration version.

Test your **threshold behavior** by sweeping your threshold across a range of values and measuring precision, recall, and abstention rate at each point. This gives you a curve that shows the trade-off space. You can use this curve to validate that your chosen threshold produces the expected precision and recall targets. You can also use it to simulate the impact of threshold changes before deploying them to production.

Test your **escalation workflow under load** by simulating high abstention rates and verifying that your human review queues, alerting systems, and fallback mechanisms handle the volume without degrading or dropping tasks. If your system abstains on 50% of inputs during a traffic spike and your review queue crashes, you have turned a calibration decision into a reliability incident. Load testing your uncertainty infrastructure is as important as load testing your inference endpoints.

You should also test **edge cases in uncertainty propagation** to ensure that uncertainty information is preserved correctly as outputs move through your system. What happens when an uncertain output is used as input to a downstream task? Does the downstream task see the uncertainty metadata and handle it appropriately? What happens when an abstained output is cached and served to multiple consumers? Is the abstention reason visible to all of them? These propagation scenarios are easy to get wrong, and they can create situations where uncertainty information is lost and downstream systems treat uncertain data as confident.

## Calibration Across Distributions and Domains

Confidence calibration is not static. A model that is well-calibrated on one domain or one task might be poorly calibrated on another. If you train a contract extraction model on venture capital agreements and then deploy it on real estate leases, the confidence scores will be wrong. The model has not seen real estate language before, and its internal probabilities are not meaningful in that context. You need to recalibrate for each new domain.

The calibration process requires a validation set from the target domain. You collect examples, run the model, record the confidence scores and the outcomes, and fit a calibration function. If you're deploying to a new domain and you don't have ground truth yet, you can use **uncertainty-based sampling** to collect labels. You process a small batch of examples, measure confidence, send the lowest-confidence ones to human reviewers, and use those labels to fit an initial calibration. As you collect more data, you update the calibration function.

You also need to monitor calibration over time. Models drift, and inputs change, and what was well-calibrated six months ago might not be calibrated today. You should track the relationship between confidence scores and actual accuracy in production, and if you see divergence, you recalibrate. This is part of your ongoing evaluation and monitoring process, which you'll build in later sections of this book.

Another subtlety is **calibration for different output types**. A model might be well-calibrated on binary classification tasks and poorly calibrated on extraction tasks. It might be well-calibrated on short outputs and poorly calibrated on long outputs. You might need separate calibration functions for different task types, or even different fields within the same output schema. The vendor name field might need a different calibration than the line item field, because the model's uncertainty behaves differently for those two extraction targets.

Domain-specific calibration becomes particularly important when you operate in regulated industries where different domains have different risk profiles. A healthcare system that processes both routine administrative tasks and clinical decision support cannot use the same calibration across both. The clinical tasks require much tighter calibration because the consequences of miscalibration are severe. Your calibration strategy must segment by domain and by risk level, and your monitoring must track calibration health separately for each segment.

## Building Confidence Into the Output Spec Early

The reason the healthcare scheduling startup failed is that they designed the output schema without thinking about uncertainty. They defined the schema as a simple structure with date and time fields, and they built the entire system around that schema before they ever tested it on ambiguous inputs. By the time they discovered the overconfidence problem, they had already integrated the schema into their scheduling system, their database, their API, and their frontend. Adding uncertainty fields would have required rewriting all of that integration code.

You avoid this by including uncertainty in the output spec from day one. Every output schema starts with a confidence score field, even if you don't use it in the first prototype. Every schema has an optional uncertainty explanation field. Every schema has a convention for how to represent abstention, whether that's a null value, a special sentinel, or an explicit flag. This way, when you inevitably discover that some inputs are uncertain, you already have the infrastructure to handle them.

This is not over-engineering. It is acknowledging reality. AI systems are probabilistic, and uncertainty is inherent. Pretending it doesn't exist is what leads to production disasters. The teams that succeed are the ones who design for uncertainty early, measure it carefully, surface it clearly, and route uncertain cases to humans or fallback systems. The teams that fail are the ones who assume the model is always right, or who discover uncertainty too late and try to patch it in after the fact.

You now have an output spec that defines structure, quality dimensions, and uncertainty handling. The next step is to think about how that spec evolves over time, because your product will change and your schema will need to change with it.

