# 1.6 — Common Framing Anti-Patterns

In mid-2023, a healthcare startup raised a Series B on the promise of using GPT-4 to automate clinical documentation. The founder had seen a demo where the model transcribed doctor-patient conversations and generated progress notes that looked clinically plausible. Investors were impressed. The team was energized. Six months later, the product had been piloted at three hospitals and quietly shelved at all three. The problem was not the technology. The problem was that nobody had asked the right questions before building: What do doctors actually need from documentation? What are the compliance requirements for medical records? What happens when the model generates a plausible-sounding but clinically incorrect note?

The team had started with a solution and worked backwards to find a problem. This is the most common framing anti-pattern, and it is far from the only one.

## Anti-Pattern One: Solution-First Thinking

Solution-first thinking happens when a team begins with a technology capability and then searches for a problem to apply it to. The canonical version sounds like: "We have access to GPT-4, what can we build with it?" or "We just hired a computer vision expert, what problems can we solve with computer vision?"

This is backwards. The technology should serve the problem, not the other way around. When you start with a solution, you inevitably warp the problem definition to fit the tool you want to use. You ignore problems that do not fit your preferred approach, even if they have higher business value. You downplay constraints that make your preferred solution infeasible.

In practice, solution-first thinking leads to a predictable failure mode: the team builds something technically impressive that nobody needs. The demo looks great. The engineering is solid. The model performance is state-of-the-art. But when you try to deploy it, you discover that it does not solve a problem users actually have, or it solves the problem in a way that does not fit their workflow, or it introduces new problems that outweigh the value it provides.

The healthcare startup is a textbook example. The GPT-4 transcription demo was impressive. But doctors did not need transcription. They needed documentation that satisfied insurance billing requirements, that integrated with their EHR system, that met legal standards for medical records, and that took less time than manual entry. The generated notes were plausible but not compliant. The system did not integrate with the EHR. And doctors still had to review and edit every note, which took nearly as long as writing it from scratch.

**How to avoid this:** Start every framing process with the problem, not the solution. Before you mention a model or technique, write down the user need and the success criteria. If you cannot describe the problem without referencing a specific technology, you are probably doing solution-first thinking. The litmus test: could this problem be solved with a different technology? If not, you are probably over-indexing on a preferred solution.

## Anti-Pattern Two: Capability-Driven Scoping

Capability-driven scoping is a subtle variant of solution-first thinking. It happens when you define the problem scope based on what the technology can do, rather than what users need.

The pattern looks like this: you prototype a model that performs well on task X, so you decide to ship a product that does task X, even though task X is only part of what users need to accomplish their goal. You tell yourself you will expand the scope later, but later never comes because the partial solution does not provide enough value to gain traction.

An example: a legal tech company built an AI system that could extract key dates from contracts with ninety-five percent accuracy. Impressive. They shipped it as a standalone feature: upload a contract, get back a list of dates. Adoption was dismal. The problem was not the accuracy. The problem was that extracting dates in isolation is not a valuable task. What legal teams actually needed was contract review: identify obligations, flag non-standard clauses, compare terms to a standard template. Dates were one small part of that workflow. Shipping date extraction as a standalone product was like selling a car that could only turn left.

Capability-driven scoping happens because it feels like progress. You have a model that works. You have metrics that prove it works. Shipping something feels better than admitting that what you built is not yet useful on its own. But shipping a partial solution that does not meet user needs is worse than shipping nothing, because it burns trust and creates the perception that AI does not work.

**How to avoid this:** Define the minimum useful scope before you prototype. What is the smallest end-to-end workflow that provides real value to users? If your model only addresses part of that workflow, you have two options: expand the model to cover the full workflow, or integrate it into a larger system that handles the rest. Do not ship a partial solution and hope users will figure out how to make it useful.

## Anti-Pattern Three: Demo-As-Spec

Demo-as-spec happens when a working demo becomes the implicit specification for the product, without anyone writing down the actual requirements, constraints, or success criteria.

This is seductive because demos are concrete and legible. You can show a demo to stakeholders and get immediate feedback. Demos create excitement and momentum. But demos are not specifications. A demo that works in controlled conditions often fails in production for reasons that were never considered: edge cases, data quality issues, latency constraints, integration requirements, compliance requirements.

An enterprise software company demoed an AI meeting assistant that could transcribe Zoom calls and generate action items. The demo was flawless: clean audio, articulate speakers, well-structured meeting, clear action items. Leadership loved it and greenlit production deployment. Three months later, the system was generating gibberish for half of all meetings. The problems: background noise, overlapping speakers, accents the model was not trained on, meetings where action items were implied rather than explicit. None of these issues appeared in the demo because the demo used cherry-picked data.

Demo-as-spec also creates a dangerous dynamic where stakeholders evaluate the system based on the best-case demo, not the average-case reality. When the production system underperforms relative to the demo, stakeholders assume the team shipped a worse version. In reality, the demo was never representative of what the system would face in production.

**How to avoid this:** Treat demos as hypothesis validation, not specifications. After a successful demo, the next step is to write down what success looks like for real-world data, what constraints apply, and what edge cases need to be handled. The demo proves that the approach is feasible; the framing document defines what "working" means in production. If the demo cannot be translated into a clear framing document with measurable success criteria, you are not ready to build.

## Anti-Pattern Four: Scope Creep Through Ambiguity

This anti-pattern happens when the problem framing is vague enough that everyone interprets it differently, and the scope expands quietly as people add their own assumptions.

A classic version: "Build an AI assistant for customer support." What does that mean? Does it mean a chatbot that handles tier-one questions? A co-pilot that suggests responses to human agents? A summarization tool that condenses ticket history? A routing system that assigns tickets to the right team? All of the above?

Without a clear problem statement and explicit non-goals, different stakeholders will assume different things. The product team assumes it is a customer-facing chatbot. The engineering team assumes it is an internal agent co-pilot. The customer support team assumes it will handle tier-one tickets but escalate complex cases. The legal team assumes it will not make binding commitments. The finance team assumes it will reduce headcount. None of these assumptions are written down, so nobody realizes they are working toward different goals until the project is well underway.

Scope creep through ambiguity is especially dangerous because it does not feel like scope creep. Nobody explicitly expands the scope. It just turns out that everyone had a different understanding of what "in scope" meant from the beginning.

**How to avoid this:** Write down the non-goals as explicitly as the goals. "We are not building a customer-facing chatbot; this is an internal co-pilot for support agents." "We are not trying to reduce headcount; we are trying to reduce resolution time." "We are not handling account security issues; only product questions." The more specific your non-goals, the harder it is for scope to creep through ambiguity.

## Anti-Pattern Five: Metric Gaming

Metric gaming happens when you optimize for a proxy metric that does not actually reflect user value, and you end up with a system that has great metrics but does not solve the problem.

An e-commerce company built a recommendation system optimized for click-through rate. CTR went up by twenty percent. Success, right? Not quite. Revenue did not increase. User satisfaction did not increase. What happened? The system learned to recommend clickbait: eye-catching products that users clicked on but did not buy. CTR was a proxy for engagement, but engagement was a proxy for purchase intent, and the system optimized for the proxy instead of the goal.

Metric gaming is not always intentional. Often it happens because the real goal is hard to measure, so you choose a proxy that is easier to measure, and over time you forget that it is a proxy. The team starts treating the proxy as the goal. Model improvements are judged by whether they increase the proxy metric, even if they do not increase the actual outcome you care about.

This is especially common with AI systems because model performance is easy to measure and business outcomes are hard to measure. Accuracy, precision, recall, F1 score—these are clean, objective, comparable. Customer satisfaction, revenue impact, operational efficiency—these are noisy, delayed, confounded by other factors. So teams optimize for the clean metrics and assume the messy metrics will follow. Often they do not.

**How to avoid this:** Define success criteria that include both proxy metrics and outcome metrics. For the recommendation system: CTR is a proxy metric, but conversion rate and revenue per user are outcome metrics. Track both. If proxy metrics improve but outcome metrics do not, your proxy is broken. The framing document should make explicit which metrics are proxies and which are outcomes, and it should specify the expected relationship between them. If that relationship does not hold in practice, you need to revisit your success criteria.

## Anti-Pattern Six: Stakeholder Capture

Stakeholder capture happens when one loud or influential voice defines the problem, and everyone else defers to them, even when their framing is incomplete or misaligned with other needs.

A common version: the CEO has a pet idea for an AI feature. The CEO is smart and well-informed, and the idea is not obviously wrong, so the team accepts it as the problem definition and starts building. Months later, the feature ships and adoption is low. Why? Because the CEO's use case was not representative of what most users needed. Or because the CEO did not understand operational constraints. Or because the CEO was optimizing for a strategic goal that was not clearly communicated to the team.

Stakeholder capture is not limited to executives. It can happen with any influential stakeholder: a key customer, a domain expert, a prolific engineer. The problem is not that their input is wrong. The problem is that their input becomes the only input, and the framing does not incorporate other perspectives.

**How to avoid this:** Institutionalize multi-stakeholder framing. The framing document should require input and sign-off from every group that will be affected by the system: product, engineering, data science, legal, compliance, finance, operations, customer support. If one stakeholder tries to define the problem unilaterally, point to the process: "We need input from X, Y, and Z before we finalize the framing." This is not about rejecting the influential stakeholder's input. It is about ensuring their input is balanced with other perspectives.

## Anti-Pattern Seven: Over-Framing

Over-framing happens when the framing process becomes an end in itself, and the team spends months refining the problem statement without ever moving to implementation.

This is the opposite failure mode from under-framing. Instead of jumping straight to code without thinking through the problem, the team gets stuck in analysis paralysis. They write and rewrite the framing document. They run stakeholder workshops. They commission user research. They debate edge cases and philosophical questions. Meanwhile, competitors ship and the market moves on.

Over-framing often happens in organizations with strong process culture or where stakeholders have been burned by poorly scoped projects in the past. The instinct is good—think before you build—but the execution is dysfunctional. Framing is meant to reduce uncertainty, but it cannot eliminate uncertainty. At some point, you have to make a decision with incomplete information and learn by doing.

**How to avoid this:** Timebox the framing process. A good rule of thumb: if you are spending more than two weeks on the framing document, you are over-framing. The framing document does not need to be perfect. It needs to be good enough to make an informed decision about whether to proceed, and to provide a shared reference point for implementation. You can always revise the framing as you learn more. The goal is to get to a "good enough" framing quickly, not to achieve a perfect framing eventually.

## Anti-Pattern Eight: The Just-Ship-It Anti-Pattern

The just-ship-it anti-pattern is the opposite of over-framing. It happens when the team skips framing entirely and goes straight to implementation, with the rationale that "we will figure it out in production."

This is the move-fast-and-break-things philosophy applied to AI. The idea is that you learn faster by shipping and iterating than by planning upfront. For some types of products, this works. For AI systems, it usually does not, because the failure modes are often invisible until the system is under real load with real users, and by that point the cost of fixing them is much higher.

An example: a social media company shipped an AI content moderation system without clear success criteria or failure mode analysis. The system was trained on historical moderation decisions and deployed to production with minimal testing. Within a week, users were complaining that legitimate content was being removed and harmful content was getting through. The team scrambled to diagnose the issues, but because there was no framing document, there was no shared understanding of what the system was supposed to do, what trade-offs were acceptable, or what constituted a failure. Every bug fix was a negotiation. Every metric was contested. The team spent months in reactive mode, firefighting issues that could have been anticipated and addressed during framing.

The just-ship-it anti-pattern is especially common in organizations that have succeeded with it for traditional software. But AI systems are not traditional software. They are probabilistic, context-dependent, and sensitive to data distribution. The faster you ship, the more likely you are to ship something that works in demo conditions but fails in production.

**How to avoid this:** Treat framing as a forcing function for risk assessment. You do not need to eliminate all risk before shipping, but you do need to identify the risks and decide which ones are acceptable. The framing document makes this explicit: here are the risks, here are the mitigations, here are the risks we are choosing to accept. If you cannot articulate the risks, you are not ready to ship.

## Why These Anti-Patterns Persist

These anti-patterns are not the result of incompetence. They persist because they serve short-term incentives that are misaligned with long-term outcomes.

Solution-first thinking happens because engineers and data scientists are excited about new technology and want to use it. Capability-driven scoping happens because shipping something feels better than admitting you are not ready. Demo-as-spec happens because demos create momentum and enthusiasm. Scope creep through ambiguity happens because being vague avoids conflict. Metric gaming happens because proxy metrics are easier to measure than outcomes. Stakeholder capture happens because deferring to authority is easier than challenging it. Over-framing happens because process provides cover for risk-averse organizations. Just-ship-it happens because urgency feels like progress.

The common thread is that framing requires slowing down, thinking rigorously, and making hard decisions before you have full information. All of these are uncomfortable. The anti-patterns are attractive because they let you avoid discomfort in the short term. The cost shows up later, when you have invested months of engineering effort into a system that does not solve the right problem.

The antidote is discipline. Not process for its own sake, but a shared commitment to asking the right questions before you start building. The framing document is the tool that enforces this discipline. It forces you to make explicit decisions, to get stakeholder alignment, and to define success before you invest in implementation.

In the next section, we will look at how the framing process differs depending on whether you are adding AI to an existing product or building something new from scratch.

