# 6.8 — Common Framing Failures That Surface During Evaluation

In late 2024, a fintech startup spent three months building a document processing system to extract structured data from financial statements. They framed the problem carefully, decomposed it into twelve tasks, defined success criteria, and hired a team of accountants to label evaluation data. When they ran their first evaluation, the results were baffling. Accuracy was inconsistent across evaluators. Some test cases passed every time. Others failed every time. And for about 30% of test cases, results flipped randomly between runs.

The engineering team assumed they had a model problem — maybe the LLM was non-deterministic, maybe the prompt needed tuning. They spent two weeks experimenting with temperature settings, different models, and prompt variations. Nothing improved. Finally, they brought in an outside consultant to review their evaluation setup. The consultant asked to see the success criteria. The document said: "Extract all relevant financial figures with high accuracy." The consultant asked: "What does 'relevant' mean?" The team did not have a clear answer. The accountants they hired to evaluate outputs were each using their own judgment about what counted as relevant. That was why evaluator agreement was low.

The problem was not the model. The problem was not the evaluation process. The problem was the **framing**. The success criteria were too vague to evaluate. Once the team went back and defined "relevant" precisely — listing the specific figures to extract and the format for each — evaluator agreement jumped to 92%, and they finally had reliable evaluation results.

This is the retrospective view of framing failures: when your evaluation results are confusing, inconsistent, or misleading, the root cause is often not your implementation or your evaluation setup. It is your framing. Framing failures are invisible during framing because everything looks reasonable in the abstract. They only surface when you try to evaluate, because evaluation is when abstract criteria meet concrete outputs.

This subchapter catalogs the six most common framing failures that surface during evaluation, how to diagnose them, how to fix them, and how to prevent them in future projects.

## Pattern 1: We Cannot Write Test Cases

**Symptom**: Your evaluation team is ready to start testing, but when they try to create test cases, they get stuck. They cannot generate concrete input examples. They cannot define what the expected output should be. They produce a handful of obvious test cases and then run out of ideas.

**Root Cause**: Your I/O specification is too vague. It describes inputs and outputs at a high level but does not provide enough detail to generate specific examples. For instance, your I/O spec says "Input: customer inquiry about billing." That is not specific enough. What kinds of billing inquiries? What level of detail? What edge cases? Without concrete examples in the I/O spec, evaluators are guessing.

**How to Diagnose**: Ask your evaluation team to generate ten test cases. If they struggle to get past three or four, your I/O spec is too vague. If the test cases they produce are all trivial happy-path scenarios with no edge cases, your I/O spec did not cover the input distribution.

**How to Fix**: Go back to your I/O specification and add **concrete examples**. For each input type, provide at least five real examples that cover the range of variation you expect. Include edge cases: empty inputs, malformed inputs, ambiguous inputs, adversarial inputs. For each output type, provide examples of what good outputs look like for each input scenario. Make the examples specific enough that someone who has never seen your system before could generate similar test cases.

**How to Prevent It**: During framing, require that every I/O spec include at least five input examples and five output examples. Review I/O specs with the evaluation team before you finish framing to confirm they are concrete enough to generate test cases.

## Pattern 2: Evaluators Disagree on What Is Correct

**Symptom**: You run evaluation with multiple human evaluators, and they disagree on what counts as a correct answer. Inter-rater agreement is low — 60% or worse. You look at specific examples where evaluators disagreed, and you see that each evaluator is using a different interpretation of the success criteria.

**Root Cause**: Your success criteria are ambiguous. They use subjective language like "accurate," "helpful," "professional," or "clear" without defining what those words mean in your specific context. Different evaluators interpret these terms differently. For example, one evaluator might think "helpful" means providing detailed explanations, while another thinks it means providing concise answers.

**How to Diagnose**: Measure inter-rater agreement. If it is below 80%, you likely have ambiguous criteria. Review the examples where evaluators disagreed. Ask each evaluator to explain why they scored the output the way they did. If their explanations reference different criteria or different interpretations of the same criterion, you have ambiguity.

**How to Fix**: Add **anchoring examples** to your success criteria. For each criterion, provide three to five examples of outputs that meet the criterion and three to five examples of outputs that do not. These examples serve as anchors that align evaluator interpretation. For instance, if your criterion is "response must be professional," show examples of professional responses and unprofessional responses with explanations of what makes them professional or not. Also, replace subjective terms with specific, measurable definitions. Instead of "accurate," say "contains the correct account number in the specified format." Instead of "helpful," say "answers the user's question within two sentences and offers one relevant follow-up action."

**How to Prevent It**: During framing, require that every success criterion is accompanied by at least three positive examples and three negative examples. Run a calibration session where multiple domain experts review the criteria and examples to confirm they all interpret them the same way.

## Pattern 3: We Are Testing the Wrong Thing

**Symptom**: Your evaluation results look good — you are hitting 90% accuracy on your test set — but when you deploy to production or run user testing, the system fails. Users complain that it does not work. You investigate and discover that the failures are happening in a part of the system you were not testing.

**Root Cause**: Your task decomposition was wrong. You decomposed the system into tasks based on a theoretical understanding of how it would work, but the real failure points in production are different. For example, you decomposed a support chatbot into three tasks: intent classification, knowledge retrieval, and response generation. You evaluated each task independently and they all looked good. But in production, the real failure point is **routing** — the system sends users to the wrong task because it misinterprets ambiguous queries. You were not testing routing because you did not identify it as a separate task during decomposition.

**How to Diagnose**: Compare evaluation failures to production failures. If production failures are concentrated in areas you did not evaluate, you decomposed wrong. Also, ask users or domain experts to describe failure modes. If they describe failures that do not map to any task in your registry, you missed something in decomposition.

**How to Fix**: Go back to task decomposition with production data. Look at real user interactions. Trace the path from input to output and identify every decision point. Each decision point is a potential task. Re-decompose based on where real failures occur, not where you theoretically think they might occur. Add the missing tasks to your task registry, define success criteria for them, and evaluate them.

**How to Prevent It**: Do not finalize task decomposition until you have real user data or realistic simulations. If you are building something net-new with no production data, run a prototype or wizard-of-oz study to observe real user interactions and identify failure points before you decompose.

## Pattern 4: Metrics Look Good but Users Complain

**Symptom**: Your automated metrics show 95% accuracy. Your human evaluation shows strong performance. But when you ship to users, they are unhappy. You get complaints that the system is "not helpful," "misses the point," or "does not understand what I need." You go back and look at your evaluation data and everything looks fine. What is happening?

**Root Cause**: Your success criteria do not capture what users actually care about. You defined criteria based on technical correctness or domain expert judgment, but you did not validate that those criteria align with user needs. For example, a legal document review system might achieve 98% accuracy on identifying relevant clauses according to expert evaluators, but users complain because the system does not explain **why** a clause is relevant, which is what they actually need to make decisions. The success criteria measured correctness but not usefulness.

**How to Diagnose**: Run user research. Interview users who are unhappy with the system. Ask them what is missing, what is wrong, and what they would change. Compare their feedback to your success criteria. If users are complaining about things that are not in your criteria, you have a gap.

**How to Fix**: Add **user-centric criteria** to your success criteria matrix. These are criteria defined from the user perspective, not the technical or domain expert perspective. Examples: "User can complete their task without asking follow-up questions." "User understands why the system gave this answer." "User feels confident in the system's recommendation." These criteria are harder to measure with automated metrics, but they are often what determines whether users adopt the system. Validate these criteria with actual users through interviews, surveys, or usability testing.

**How to Prevent It**: Involve users in framing. Do not rely solely on domain experts or product managers to define success. Run user research before you finalize success criteria to understand what users care about. Include at least one user-centric criterion for every task.

## Pattern 5: We Cannot Reproduce Failures

**Symptom**: Users report failures in production. You try to reproduce the failures in your evaluation environment, but you cannot. You do not have enough context to recreate the exact conditions that caused the failure. The input looks similar, but the system behaves differently in evaluation than it did in production.

**Root Cause**: Your I/O specification does not capture enough context to recreate production scenarios. You specified the primary inputs but not the contextual inputs. For example, your I/O spec for a chatbot says "Input: user message." But in production, the system also depends on conversation history, user profile data, time of day, and session state. When you try to reproduce a failure using only the user message, the system behaves differently because it does not have the same context.

**How to Diagnose**: Try to reproduce a production failure. If you cannot, list all the contextual factors that might differ between production and evaluation. Check whether those factors are in your I/O spec. If they are not, you have incomplete context capture.

**How to Fix**: Expand your I/O specification to include **all inputs that affect system behavior**, not just the primary inputs. This includes conversation history, user metadata, session state, time-based factors, external data sources, and tool outputs from previous steps. For each contextual input, define the schema and specify how to log it in production so you can recreate scenarios in evaluation. Update your instrumentation plan to capture these contextual inputs in production logs.

**How to Prevent It**: During framing, walk through the system end-to-end and identify every piece of data the system reads to make decisions. Add all of them to the I/O spec, even if they seem minor. Err on the side of over-specifying context rather than under-specifying.

## Pattern 6: Everything Is High Priority

**Symptom**: Your evaluation workload is overwhelming. You have 50 test cases for each of ten tasks, and you do not have enough time or budget to evaluate all of them thoroughly. Everything feels equally important. You try to evaluate everything, so you evaluate nothing well. You end up with shallow evaluation that misses critical failures.

**Root Cause**: You did not prioritize during framing. Your task registry lists all tasks with no indication of which are most critical. Your success criteria do not distinguish between must-have criteria and nice-to-have criteria. Without prioritization, the evaluation team defaults to treating everything equally, which means spreading resources too thin.

**How to Diagnose**: Ask your evaluation team how they are allocating effort. If they are spending equal time on every task, you have no prioritization. Look at your task registry. If every task is marked as high risk or high priority, you have no prioritization.

**How to Fix**: Go back to your task registry and apply the priority matrix from Chapter 6.5. Mark each task with risk tier and user impact. Focus evaluation effort on high-risk, high-impact tasks. For low-risk tasks, use lightweight evaluation methods like automated checks or sampling. For high-risk tasks, invest in thorough evaluation with multiple evaluators, adversarial testing, and edge case coverage. Update your success criteria to mark certain criteria as critical (must pass for launch) versus aspirational (nice to have but not blocking). Communicate priorities clearly to the evaluation team so they know where to focus.

**How to Prevent It**: Prioritization is not optional. During framing, require every task in the registry to have a risk tier and an impact score. Use these to drive evaluation planning. Review prioritization with leadership and domain experts to ensure it reflects business reality, not just engineering intuition.

## Why Framing Failures Are Expensive to Discover Late

All six of these failure patterns are fixable, but the cost of fixing them depends on when you discover them. If you discover vague I/O specs during framing review, you spend two hours adding examples. If you discover them during evaluation, you spend two weeks rewriting specs, regenerating test cases, and rerunning evaluation. If you discover them in production, you spend two months rebuilding parts of the system.

This is why framing review is so critical. The review is your chance to surface these failures before they become expensive. A good framing review asks the hard questions: Can we write test cases from this I/O spec? Do evaluators agree on what these criteria mean? Are we testing the right tasks? Do these criteria align with user needs? Can we reproduce production scenarios with this context? Have we prioritized ruthlessly?

If you cannot answer yes to all of these questions, you are not ready to start building or evaluating. Go back and fix the framing.

## How to Diagnose Framing Failures During Evaluation

When you encounter confusing or misleading evaluation results, use this diagnostic checklist to identify framing failures:

**Run a test case generation exercise**: Ask evaluators to generate ten test cases from your I/O spec without additional help. If they struggle, your I/O spec is too vague.

**Measure inter-rater agreement**: If it is below 80%, your success criteria are ambiguous.

**Compare evaluation failures to production failures**: If they do not align, your task decomposition is wrong.

**Run user interviews**: If users complain about things not in your success criteria, you are measuring the wrong things.

**Try to reproduce a production failure**: If you cannot, your I/O spec does not capture enough context.

**Check evaluation workload**: If the team is overwhelmed and treating everything equally, you have no prioritization.

Each of these diagnostics points to a specific framing failure. Fix the framing, then re-run evaluation.

## Practical Takeaways

**Expect framing failures to surface during evaluation**: This is normal. Evaluation is when abstract framing meets concrete reality. Use it as a feedback loop to improve framing.

**Diagnose the root cause**: When evaluation results are confusing, do not immediately blame the model or the evaluation process. Check the framing first.

**Fix framing, not symptoms**: If evaluators disagree, do not just hire more evaluators. Add anchoring examples to your success criteria. If you cannot write test cases, do not just generate random examples. Add concrete examples to your I/O spec.

**Run framing review before evaluation**: Use the six failure patterns as a checklist during framing review. Proactively look for vague I/O specs, ambiguous criteria, missing tasks, misaligned criteria, incomplete context, and lack of prioritization.

**Iterate quickly**: When you discover a framing failure during evaluation, fix it immediately and re-run. Do not keep evaluating a badly framed system hoping the results will get better.

**Document lessons learned**: When you discover a framing failure, document it and share it with other teams. Framing failures are learning opportunities. Make sure the organization benefits from your mistakes.

The next step is to prevent framing failures proactively by running a structured team exercise that reviews all framing artifacts with key stakeholders before evaluation begins — a framing review session designed to catch these patterns before they become expensive.
