# 3.2 â€” Working Memory for Tool-Using Agents

In August 2025, an e-commerce company deployed a customer service agent that could search the product catalog, check order status, and retrieve customer account details to answer support inquiries. The agent worked beautifully in testing with small product catalogs and simple orders. In production, the first customer inquiry about a bulk order triggered a catalog search that returned 47,000 tokens of product data. The agent dutifully injected all 47,000 tokens into its next LLM call to synthesize an answer, hitting the context limit and crashing. The fallback logic retried the call three times, each time crashing again, burning through $340 in API costs in under two minutes before the circuit breaker tripped. The engineering team discovered that the agent had no working memory management strategy. Every tool output, no matter how large, was kept in full and injected into every subsequent LLM call. The agent treated working memory like permanent memory, never discarding anything, never summarizing, never prioritizing. By the end of the first day, the circuit breaker had tripped eighteen times, and the agent had been taken offline.

Working memory is the scratchpad where agents store intermediate results that matter for the current task but not beyond it. A search result is working memory. A database query output is working memory. A file read is working memory. These are ephemeral artifacts that the agent needs to reason about right now but that lose relevance once the task completes. Managing working memory well is the difference between an agent that scales and an agent that crashes under real-world load.

## What Working Memory Is and What It Is Not

Working memory is distinct from both plan memory and long-term memory. **Plan memory** persists across the entire task lifecycle and stores the agent's intended approach, progress, and checkpoints. **Long-term memory** persists across tasks and stores knowledge, patterns, and historical context that inform future decisions. **Working memory** exists only within the current task and stores intermediate results generated by tool calls that the agent needs for subsequent reasoning steps.

When an agent calls a search tool, the search results go into working memory. When the agent calls a database query tool, the query results go into working memory. When the agent calls an API to retrieve external data, the API response goes into working memory. These results are inputs to the next reasoning step. The agent reads them, synthesizes them, and generates the next action. Once the task completes, working memory is discarded. There is no reason to keep search results from a completed task in memory for the next task. They are stale and irrelevant.

Working memory is also distinct from the context window, though the two are related. The context window is the total token budget available for a single LLM call. Working memory is the subset of that budget consumed by tool outputs. If your context window is 128,000 tokens and you allocate 100,000 tokens to working memory, you have only 28,000 tokens left for the system prompt, the plan, the progress log, and the user query. This is too tight. In practice, working memory should consume no more than 30 to 50 percent of the context window, leaving room for all the other context the agent needs.

The failure mode is straightforward. The agent calls a tool, the tool returns a large output, the agent injects the output into context, the context overflows, the LLM call fails. The naive fix is to increase the context window size, but this only delays the problem. If the tool can return 47,000 tokens today, it can return 94,000 tokens tomorrow. You cannot solve working memory bloat by buying a bigger context window. You solve it by managing what goes into working memory and what stays out.

## The Scratchpad Pattern for Agent Working Memory

The **scratchpad pattern** is the standard architecture for agent working memory. The scratchpad is a key-value store where each tool call writes its output under a unique key, and the agent reads from the scratchpad when it needs to reference previous results. The scratchpad is not a free-for-all. It has a fixed capacity measured in tokens, and when the capacity is exceeded, the agent must decide what to keep and what to discard.

The scratchpad is initialized at the start of the task and destroyed at the end of the task. During the task, the agent alternates between two modes: tool execution mode, where it calls tools and writes outputs to the scratchpad, and reasoning mode, where it reads from the scratchpad and decides the next action. In tool execution mode, the agent does not load the entire scratchpad into context. It only loads the specific keys it needs for the current reasoning step. In reasoning mode, the agent may load multiple keys if it needs to synthesize information from multiple tool calls.

The scratchpad is implemented as an external data structure, not as part of the LLM context. It lives in memory, in a Redis cache, or in a document store, depending on the agent's deployment architecture. The agent retrieves scratchpad entries on demand and injects them into the context window only when needed. This keeps the context window lean and focused.

The scratchpad key design determines how easy it is to retrieve relevant results later. Keys should be descriptive and scoped to the task structure. If the agent is executing a multi-step workflow, the key might include the step identifier: search_results_step_3, query_output_step_5. If the agent is answering a question that requires multiple lookups, the key might include the lookup type: catalog_search, order_status, customer_account. Descriptive keys make it easier to write retrieval logic that pulls the right data at the right time.

The scratchpad also tracks metadata for each entry: the timestamp when the result was written, the tool that generated it, the token count, and a summary of the content. The summary is a short human-readable description generated by the tool or by a lightweight LLM call immediately after the tool returns. When the agent needs to decide which scratchpad entries to load into context, it reads the metadata and summaries first, then selectively loads the full entries that are most relevant.

## What to Keep, What to Discard, and How to Decide

The scratchpad has a capacity limit measured in tokens. The limit is typically 30,000 to 50,000 tokens, representing the portion of the context window reserved for tool outputs. When the scratchpad exceeds this limit, the agent must evict entries to make room for new ones. The eviction policy determines which entries get discarded.

The simplest eviction policy is **least recently used (LRU)**. When the scratchpad is full and a new entry needs to be written, the agent discards the entry that was accessed least recently. LRU works well for tasks with temporal locality, where the agent tends to reference recent results more often than old results. LRU does not work well for tasks with semantic locality, where the agent may need to reference a result from ten steps ago because it is semantically relevant to the current step, even though it has not been accessed recently.

A better eviction policy is **relevance-based eviction**, where the agent scores each scratchpad entry based on its relevance to the current step and discards the lowest-scoring entries when capacity is exceeded. Relevance scoring requires the agent to understand the task structure and the dependencies between steps. If the current step is synthesizing an answer based on search results from step three and order status from step seven, those two entries have high relevance and should not be evicted. The catalog data from step two, which is no longer needed, has low relevance and can be evicted.

Relevance scoring can be manual or automatic. In manual relevance scoring, the task designer annotates each step with the scratchpad keys it depends on. The agent uses these annotations to compute relevance. In automatic relevance scoring, the agent uses an embedding model to compute semantic similarity between the current step's description and the summaries of scratchpad entries, then ranks entries by similarity. Automatic scoring is more flexible but more expensive because it requires embedding calls.

A hybrid policy combines time-based and relevance-based eviction. Entries older than N steps are candidates for eviction regardless of relevance. Among the candidates, the agent evicts the lowest-relevance entries first. This prevents the scratchpad from filling up with old results that are no longer needed while still protecting high-relevance recent results.

The eviction decision also depends on whether the tool output is recoverable. If the agent can re-run the tool call to regenerate the output, eviction is safe. If the tool call is expensive, rate-limited, or non-deterministic, eviction is risky because the agent may not be able to recover the result later. For non-recoverable results, the agent should either summarize them before eviction or mark them as protected and never evict them. Protected entries count against the scratchpad capacity but are exempt from eviction.

## Summarizing Tool Outputs Before Injection

Large tool outputs must be summarized before injection into the LLM context. Summarization reduces token consumption while preserving the information the agent needs to make decisions. A 47,000-token product catalog search result can be summarized into a 2,000-token list of product names, prices, and availability statuses. A 15,000-token database query result can be summarized into a 500-token table of key metrics. The summarization step happens immediately after the tool returns, before the output is written to the scratchpad.

Summarization can be rule-based or LLM-based. **Rule-based summarization** applies hard-coded logic to extract specific fields from the tool output. If the tool returns structured data like JSON, the rule extracts the fields the agent needs and discards the rest. Rule-based summarization is fast, cheap, and deterministic, but it requires the tool output format to be predictable. If the tool output format changes, the summarization rule breaks.

**LLM-based summarization** uses a small, fast model like GPT-5-mini or Claude Opus 4.5 Haiku to generate a natural language summary of the tool output. The summarization prompt asks the model to extract the key information relevant to the agent's current task and discard irrelevant details. LLM-based summarization is more flexible and handles unstructured or variable-format outputs, but it adds latency and cost. For every tool call, you now have two LLM calls: one to summarize the output and one to reason about the summary.

The summarization granularity depends on the task. For tasks where the agent needs precise details from the tool output, the summary should preserve those details. For tasks where the agent only needs high-level information, the summary can be aggressive. If the agent is answering a question about whether a product is in stock, the summary only needs the product name and the stock status. If the agent is generating a detailed product comparison, the summary needs specifications, pricing, and customer review scores.

Summarization also applies to multi-turn tool interactions. If the agent calls a search tool, gets back ten results, then calls a detail retrieval tool on each result, the scratchpad will accumulate ten detail outputs. Rather than keeping all ten in full, the agent summarizes each detail output as it arrives and keeps only the summaries in the scratchpad. When the agent needs to synthesize a final answer, it loads the ten summaries instead of the ten full outputs. This reduces token consumption by an order of magnitude.

The risk with summarization is information loss. The agent generates a summary that discards a critical detail, and later in the task, it needs that detail but no longer has access to it. The fix is to keep the full output in an auxiliary store outside the scratchpad, with the summary in the scratchpad. If the agent discovers it needs more detail, it can retrieve the full output from the auxiliary store on demand. This two-tier memory design separates frequently accessed summaries from rarely accessed full outputs, optimizing for the common case while still supporting the edge case.

## The Danger of Tool Output Bloat

Tool output bloat is the single biggest cause of context overflow in tool-using agents. A single tool call can return tens of thousands of tokens if the tool is querying a large dataset, scraping a webpage, or calling an API with verbose responses. The agent cannot simply dump these outputs into context and hope for the best. It must actively manage token budgets, summarize aggressively, and evict stale results.

The first defense against tool output bloat is tool design. Tools should be built with token limits in mind. A search tool should return at most N results, with N configurable based on the agent's token budget. A database query tool should include a row limit in every query. An API call tool should request only the fields the agent needs, not the full object graph. Tool developers must treat token consumption as a first-class design constraint, not an afterthought.

The second defense is pagination. If a tool query could return a large result set, the tool should support pagination: return the first page of results, give the agent an opportunity to evaluate whether it needs more, and only fetch additional pages if the agent requests them. Pagination prevents the agent from pulling 50,000 tokens of data when the answer is in the first 500 tokens. Pagination also enables the agent to bail out early if it determines the query is not productive.

The third defense is incremental retrieval. Instead of retrieving all the data upfront and then reasoning about it, the agent retrieves a small sample, reasons about whether it is on the right track, and then retrieves more if needed. This is a tight loop between retrieval and reasoning that prevents over-retrieval. Incremental retrieval requires tools that support filtering and refinement: the agent can call the tool with a broad query, get back a summary, then call the tool again with a narrower query to drill down.

The fourth defense is client-side filtering. The tool returns a large result set, but the agent applies filters before writing to the scratchpad. If the tool returned 500 products and the agent only cares about products under $50, the agent filters out everything over $50 before storing the results. Client-side filtering is less efficient than server-side filtering because it requires transferring data that will be discarded, but it is still better than storing and reasoning about irrelevant data.

The fifth defense is streaming and early termination. Some tools support streaming responses, where the output is delivered incrementally. The agent processes each chunk as it arrives and can terminate the stream early if it has enough information. Streaming is particularly useful for tools that generate long-form text or large lists. The agent reads the first few items, decides it has what it needs, and stops the stream, avoiding the token cost of the remainder.

## Working Memory in Multi-Agent Systems

When multiple agents collaborate on a task, working memory becomes shared infrastructure. Agent A calls a tool, writes the output to the shared scratchpad, and Agent B reads from the scratchpad to inform its next action. Shared working memory introduces concurrency challenges that do not exist in single-agent systems.

The primary challenge is consistency. If Agent A and Agent B both read the scratchpad at time T, make decisions based on the state at T, and then both write new entries to the scratchpad, the scratchpad state at time T+1 reflects both writes. But if the writes conflict or if one write invalidates the assumptions underlying the other write, the scratchpad ends up in an inconsistent state. The solution is to use transactional semantics for scratchpad updates: an agent acquires a lock before reading, holds the lock while making a decision, writes its updates, and releases the lock. This serializes access and ensures consistency.

The secondary challenge is capacity management. If each agent has its own token budget and writes to the shared scratchpad without coordination, the scratchpad can fill up with redundant or low-value entries. The solution is to assign each agent a quota within the shared scratchpad capacity. Agent A can write up to 10,000 tokens, Agent B can write up to 10,000 tokens, and so on. When an agent exceeds its quota, it must evict its own entries before writing new ones. This prevents any single agent from monopolizing the scratchpad.

The tertiary challenge is discoverability. In a multi-agent system, Agent B may not know what Agent A has written to the scratchpad unless there is a catalog or index. The solution is to maintain a scratchpad manifest: a lightweight data structure that lists all the keys currently in the scratchpad, the agent that wrote each entry, the timestamp, and the summary. Agent B reads the manifest to discover what information is available, then selectively loads the entries it needs.

Multi-agent working memory also requires garbage collection. If Agent A writes an entry and then terminates, the entry remains in the scratchpad even though no agent will ever read it again. The scratchpad manager must detect orphaned entries and evict them to reclaim capacity. Orphan detection can be time-based (evict entries older than N minutes) or reference-based (evict entries that have not been accessed by any agent in the last M steps).

## Instrumentation and Debugging for Working Memory

Working memory is invisible to the end user but critical to the agent's performance. When an agent fails, the root cause is often a working memory issue: the scratchpad was full, a critical result was evicted, a summary lost information, a tool output was too large to fit. You need instrumentation to diagnose these issues.

The first metric is scratchpad utilization: the percentage of the scratchpad capacity currently in use. If utilization is consistently above 90 percent, the agent is under memory pressure and will start evicting entries frequently. If utilization is consistently below 20 percent, the scratchpad capacity is over-provisioned and you can reduce it to save costs.

The second metric is eviction rate: the number of scratchpad entries evicted per task. A high eviction rate indicates that the scratchpad is too small, that tool outputs are too large, or that the task is too long. A zero eviction rate indicates that the scratchpad is adequately sized for the task.

The third metric is retrieval hit rate: the percentage of scratchpad reads that find the requested key. A low hit rate indicates that entries are being evicted before the agent needs them, which suggests the eviction policy is too aggressive or the scratchpad capacity is too small.

The fourth metric is summarization overhead: the latency and cost added by summarization calls. If summarization is taking longer than the tool calls themselves, the summarization strategy is too expensive and needs to be optimized.

Debugging working memory issues requires logging every scratchpad operation: every write, every read, every eviction, every summarization. The logs should include the key, the operation type, the timestamp, the token count, and the agent's current step. When a task fails, you reconstruct the scratchpad state at each step from the logs and identify where things went wrong. Did the agent evict a critical entry too early? Did a summarization lose essential information? Did a tool output exceed the scratchpad capacity? The logs tell the story.

You also need visibility into the scratchpad contents during task execution, not just after failure. A scratchpad inspector tool lets operators view the current scratchpad state: which keys are present, what their summaries are, how much capacity they consume, when they were last accessed. This visibility enables proactive intervention. If an operator sees the scratchpad filling up with low-value entries, they can manually evict them or adjust the task to reduce tool output volume.

## Working Memory Patterns Across Agent Frameworks

Agent frameworks in 2026 have varying levels of support for working memory management. LangGraph provides a state object that can hold working memory, but it does not enforce capacity limits or eviction policies. You must implement these yourself. LangGraph's state is also persisted to the database after every step, which means working memory ends up in long-term storage unless you explicitly clear it at task completion. For production use, you configure LangGraph to partition the state into persistent fields and transient fields, with working memory in the transient partition.

AutoGen treats working memory as part of the message history. Tool outputs are messages sent by the tool agent to the reasoning agent. This works for small outputs but fails for large outputs because the entire message history is injected into every LLM call. AutoGen's working memory is effectively unbounded unless you implement message pruning logic that removes old tool output messages from the history before they overflow the context window.

CrewAI uses a task-scoped context object for working memory. Each task has a context field, and tools write their outputs to the context. CrewAI automatically summarizes tool outputs using a configurable summarization strategy: you can choose rule-based extraction, LLM-based summarization, or no summarization. CrewAI's context capacity is configurable, and when capacity is exceeded, it evicts entries using LRU by default. This is the most out-of-the-box support for working memory among the major frameworks, though you still need to tune the capacity and eviction policy for your task.

Semantic Kernel provides memory plugins for working memory just as it does for long-term memory. You implement a working memory plugin that exposes write, read, and evict operations, and the agent calls the plugin as needed. Semantic Kernel does not prescribe a scratchpad structure or eviction policy. You design it based on your requirements. This flexibility is powerful but requires more implementation effort than the opinionated approaches in LangGraph or CrewAI.

Regardless of framework, the principles remain constant. Working memory is ephemeral. It has a fixed capacity. Large outputs must be summarized. Irrelevant entries must be evicted. The scratchpad is external to the context window and selectively injected. These are not framework features. They are architectural requirements for any agent that uses tools to retrieve external data.

The e-commerce company rebuilt their customer service agent with a working memory management layer in late 2025. They implemented a 40,000-token scratchpad with LLM-based summarization for all tool outputs, relevance-based eviction with a 10-step time window, and a two-tier memory design that kept full outputs in an S3 bucket for fallback retrieval. The rebuilt agent handled bulk order inquiries without crashing, synthesized answers from multiple tool calls without exceeding the context limit, and reduced API costs by 60 percent because it no longer injected redundant data into every LLM call. This is what working memory management enables: reliable, efficient, scalable tool use in production environments.

Conversation memory, which allows agents to reference prior interactions across sessions, requires different strategies than working memory and is covered in the next subchapter.

