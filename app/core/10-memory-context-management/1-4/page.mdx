# 1.4 â€” Memory Schemas: Structuring What Gets Remembered

In March 2025, a SaaS company launched a personal productivity agent that remembered user preferences, project context, and working patterns. The system used a simple architecture: every time the AI inferred something about the user, it wrote a text description to a vector database. "User prefers morning meetings." "User is working on Q2 planning." "User likes detailed summaries." The text was embedded and stored with a timestamp. Retrieval used semantic search to find relevant memories. The product team was proud of how flexible and low-effort this approach was. No rigid schemas, no database migrations, just natural language memories that evolved with usage.

Three months into production, the system became unusable. A user opened a project planning session and received reminders about a project that ended two months ago. Another user set their notification preference to "urgent only" but kept receiving routine alerts because the system also had a memory that said "user likes staying informed." A third user deleted their account, but their memories persisted in the vector database because there was no user ID field to key the deletion on. The company had 40,000 stored memories with no way to expire old ones, no way to resolve conflicts, no way to distinguish high-confidence facts from speculative inferences, and no way to audit what was stored about any individual user. The engineering team spent seven weeks building a migration pipeline to extract structured data from unstructured text memories, deduplicate conflicts, and implement proper expiration. They lost 30% of stored memories in the migration because the original text was too ambiguous to parse reliably.

The lesson is that **unstructured memory becomes unusable at scale**. Natural language memory descriptions seem easy when you have 50 memories. They break down completely at 50,000. Every memory item needs type, source, owner, confidence, timestamp, expiry, and access rules. This isn't over-engineering. It's the minimum viable structure that makes memory systems reliable, auditable, and maintainable. Teams that skip schema design in favor of "flexible" text-based memory spend months cleaning up the mess later.

## The Failure Modes of Schemaless Memory

Schemaless memory systems fail in predictable ways. The first failure is retrieval ambiguity. You store a memory as "user prefers concise responses." Six months later, you retrieve that memory via semantic search when the user asks for detailed analysis. The retrieval system finds it because "detailed analysis" and "concise responses" are semantically related to communication style. The AI now has conflicting signals: the current request asks for detail, the memory says prefer concise. There's no confidence score to indicate whether this is a strong preference or a passing mention. There's no timestamp to show it's six months old. There's no context about when this preference applies. The AI guesses, and sometimes it guesses wrong.

The second failure is conflict accumulation. A user's preferences change over time, but the schemaless system doesn't know how to handle updates. It stores the new preference as a new memory without deprecating the old one. Now both exist. Retrieval pulls both. The AI sees "user prefers concise responses" from January and "user prefers detailed explanations" from June. There's no version sequencing, no last-write-wins logic, no conflict resolution mechanism. The system either ignores one arbitrarily, or worse, tries to synthesize a middle ground that satisfies neither.

The third failure is unbounded growth. Without expiration rules, every memory persists forever. A user mentions they're working on a project in February. That memory stays in the system through March, April, and May, long after the project ended. The user starts a new project, and now the system has memories about both projects. By December, the user has memories about nine finished projects, three abandoned initiatives, two temporary preferences, and one question they asked out of idle curiosity. Retrieval returns an increasingly random mix of current and obsolete context. The system becomes less useful over time instead of more useful.

The fourth failure is ungoverned access. Without a structured owner field, you can't reliably filter memories by user, team, or organization. A semantic search for "project timeline" might return memories from other users if your retrieval isn't scoped correctly. Even if you scope by session or user context, there's no enforcement at the data layer. A coding error, a misconfigured retrieval, or a shared session ID can leak memories across users. This is a privacy violation that ruins trust and violates regulations.

The fifth failure is audit impossibility. A user files a data access request under GDPR asking to see all stored information about them. With schemaless text memories scattered across a vector database with no structured user ID field, you can't reliably answer. You can try to search for their username or email in memory text, but what if the memory says "the user working on climate analysis" without naming them? What if it uses a nickname or a session ID? You end up doing manual review of thousands of memories, hoping you find them all. This is not compliant, and it's not acceptable for production systems in 2026.

Schemaless memory is a prototype-phase convenience that becomes a production-phase liability. The engineering effort you save by avoiding schema design in week one, you pay back tenfold in months three through six when you're debugging retrieval conflicts, building expiration jobs, and migrating unstructured text to structured records.

## The Core Schema: Seven Required Fields

Every memory item, regardless of type or horizon, needs seven fields: memory type, source, owner, confidence, timestamp, expiry rule, and access level. These fields are not optional enhancements. They're the baseline that makes memory systems work.

**Memory type** defines what kind of information this is. The type determines retrieval logic, expiration policy, and conflict resolution rules. Common memory types include explicit preference (the user directly stated this), inferred preference (the system observed a pattern and the user confirmed it), behavioral pattern (the system observed this but the user hasn't confirmed), interaction history (the user took this action), project context (the user is working on this), and organizational policy (this is company-wide knowledge). Each type has different confidence levels and different handling rules. An explicit preference overrides an inferred one. A behavioral pattern gets lower weight than a confirmed preference. Project context expires when the project ends. Organizational policy doesn't expire based on user activity.

**Source** identifies where this memory came from. Did the user state it directly in a settings panel? Did the AI infer it from conversation and the user confirmed? Did the system observe it from behavioral patterns? Was it imported from another system? The source affects confidence and auditability. A preference imported from user settings has high confidence and clear provenance. A preference inferred from three conversations and confirmed by the user has medium-high confidence and conversation IDs as provenance. A preference inferred but not confirmed has low confidence and should be treated as a suggestion, not a fact. The source field lets you trace every memory back to its origin, which is essential for debugging conflicts and answering audit questions.

**Owner** defines who this memory belongs to. For user memory, the owner is a user ID. For team memory, it's a team ID. For organizational memory, it's an org ID or a content authority ID. The owner field is the primary filter for retrieval and the key for deletion, export, and access control. When you retrieve memories for a session, you filter by owner equals current user ID. When a user deletes their account, you delete all records where owner equals that user ID. When you audit data access, you query by owner. Without a structured owner field, none of these operations are reliable.

**Confidence** is a score between 0 and 1 that represents how certain you are that this memory is accurate and current. Explicitly stated preferences have confidence 1.0. Confirmed inferences have confidence 0.8 to 0.95 depending on how many observations support them. Unconfirmed behavioral patterns have confidence 0.4 to 0.6. Speculative inferences have confidence below 0.4. The confidence score affects retrieval weighting and conflict resolution. When two memories conflict, the higher confidence one wins. When retrieving memories for context, you filter out items below a confidence threshold. As memories age without being used or reconfirmed, confidence decays. A preference with confidence 0.9 that hasn't been relevant for four months drops to 0.7. This decay prevents stale memories from dominating current context.

**Timestamp** is actually three timestamps: created, last accessed, and last modified. Created timestamp tracks when the memory was stored. Last accessed tracks when it was last retrieved and used in a session. Last modified tracks when the content or confidence was updated. These timestamps drive expiration logic, staleness detection, and conflict resolution. A memory that hasn't been accessed in 90 days is a candidate for expiration. A memory created six months ago but accessed last week is still active. Two conflicting memories are resolved by last modified timestamp: the newer one wins.

**Expiry rule** defines when this memory should be deleted or archived. Expiration is not a single date. It's a policy: expire after 90 days of no access, expire when the project status changes to complete, expire when the user role changes, never expire unless user deletes, expire when superseded by a newer version. The expiry rule is evaluated by automated jobs that run daily or weekly. A job queries for memories where last accessed is older than the expiry threshold, confidence has decayed below 0.3, or the linked entity like a project has reached end state. Those memories are flagged for deletion and removed after a grace period. The expiry rule makes memory hygiene automatic instead of manual.

**Access level** defines who can read this memory. User-scoped memories are readable only by the owning user. Team-scoped memories are readable by team members. Organization-scoped memories are readable by anyone in the org with appropriate role. Public memories are readable by all users. The access level is enforced at retrieval time. When you query memories, you filter by owner equals current user OR access level allows current user's role. This prevents memory leakage across users and teams.

These seven fields are the minimum. Depending on your use case, you might add domain-specific fields: project ID for project memories, policy version for organizational memories, geographic scope for locale-specific memories. But the core seven are non-negotiable. Without them, you don't have a memory system. You have a pile of text.

## Schema Design Patterns by Memory Type

Different memory types need different schema extensions beyond the core seven fields. User preferences need a preference key, preference value, and applicability scope. The preference key is a structured identifier like "response_style," "notification_frequency," or "language." The preference value is the setting: "detailed," "urgent_only," "spanish." The applicability scope defines when this preference applies: always, only for work projects, only for mobile sessions, only during business hours. This scope prevents over-applying preferences. A user prefers brief responses on mobile but detailed responses on desktop. You store two preference records with the same key but different scopes and different values.

Behavioral patterns need an observation count, observation window, and pattern description. The observation count tracks how many times you've seen this behavior. A user declines calendar invitations after 5pm. You've observed this 12 times over three weeks. The observation count is 12, the observation window is three weeks, the pattern description is "declines late meetings." The confidence score is a function of observation count and consistency. Twelve observations with zero exceptions gives confidence 0.85. Twelve observations with three exceptions gives confidence 0.65. As you observe more instances, you update the observation count and recalculate confidence. If the pattern breaks (the user accepts a 6pm meeting), you decrease confidence or deprecate the memory entirely.

Interaction history needs an action type, entity ID, and outcome. The action type is what the user did: created document, ran query, executed workflow, opened project. The entity ID links to the thing they acted on. The outcome captures the result: success, failure, partial completion. Interaction history is usually not retrieved as semantic memory. It's queried as structured analytics data to inform suggestions or detect patterns. A user has created 15 documents in the past month, all related to Q2 planning. The system suggests templates or retrieves relevant Q2 planning documents. The interaction history schema supports this by making the action type and entity ID queryable fields.

Project context needs a project ID, project status, and role. The project ID links the memory to a project entity in your system. The project status tracks whether the project is active, paused, or completed. The role defines the user's involvement: owner, contributor, reviewer. When the project status changes to completed, all memories linked to that project ID are flagged for expiration. When the user's role changes from contributor to none, their project context memories are expired because they're no longer involved. This linkage prevents project memories from persisting after the project ends.

Organizational policy memories need a policy ID, version number, effective date, and retirement date. The policy ID is the stable identifier across versions. The version number increments with each update. The effective date defines when this version became active. The retirement date defines when it was superseded. When you retrieve organizational policy, you query for the record where policy ID matches, effective date is less than or equal to now, and retirement date is null or greater than now. This gives you the current active version. For audit purposes, you can query historical versions by specifying a past timestamp.

## Schema Evolution: Versioning and Migration

Schemas change as systems mature. You start with a simple preference schema: key, value, timestamp. Three months in, you realize you need applicability scope because users want different preferences for different contexts. You need to add a scope field to existing records and update the retrieval logic to filter by scope. This is a schema migration, and it needs to be handled without losing data or breaking running sessions.

The safe approach to schema migration is additive versioning. You add new fields with default values rather than modifying existing fields. The old schema has key, value, timestamp. The new schema has key, value, timestamp, scope, where scope defaults to "always" if not specified. You deploy the new schema and backfill existing records with scope equals "always," which maintains their current behavior. New records can specify scopes. Retrieval logic checks for scope and filters accordingly, treating missing scope as "always" for backwards compatibility. This migration is non-breaking.

The dangerous approach is in-place modification. You change the value field from a string to a structured object without versioning. Old records have string values. New records have object values. Retrieval logic breaks because it expects one type and gets another. You need a migration script to convert all old records, and during the migration window, the system is partially broken. This approach causes outages and data loss.

Production systems in 2026 use schema version numbers embedded in each record. Every memory item has a schema_version field that starts at 1. When you introduce a new schema, you increment to version 2. New records are written with schema version 2. Old records remain at version 1. Retrieval logic checks the schema version and applies the appropriate parsing and handling. A version 1 record gets default values for missing fields. A version 2 record uses the actual values. Over time, you backfill old records to the new version through batch jobs, but the system doesn't break during the transition.

Schema evolution also applies to field semantics. You initially store confidence as a boolean: confirmed or not confirmed. Later you realize you need a continuous score. You add a confidence_score field and map the boolean to scores: confirmed equals 0.9, not confirmed equals 0.5. New records use confidence_score. Old records use the boolean with mapping. Eventually you deprecate the boolean field and use only the score. This evolution happens incrementally without requiring a big-bang migration.

The key principle is that schema changes are treated as code changes: versioned, tested, deployed incrementally, and validated before full rollout. You don't change a production schema on Friday afternoon and hope it works. You test the new schema in staging, validate the migration script on a copy of production data, deploy during a maintenance window, and monitor for errors.

## Retrieval Logic: How Schemas Enable Reliable Memory Access

Structured schemas enable retrieval logic that schemaless systems can't support. When you need to retrieve user preferences for a session, you query where owner equals user ID, memory type equals "explicit_preference" or "inferred_preference," confidence greater than 0.7, expiry rule not triggered, and access level allows current user. This query returns exactly the high-confidence, current, relevant preferences. You don't get behavioral patterns that aren't confirmed. You don't get expired preferences from six months ago. You don't get low-confidence speculations. The schema enforces correctness.

When you need to resolve a conflict between two preferences, you query for records with the same preference key and owner, then sort by confidence descending and last modified descending. The top record is the winner. If two records have equal confidence, the newer one wins. If a user explicitly updates a preference, it comes in with confidence 1.0 and recent timestamp, so it automatically overrides older inferred preferences with confidence 0.8. The schema makes conflict resolution deterministic.

When you need to expire old memories, you query where last accessed is older than the expiry threshold defined in the expiry rule field. For a 90-day activity expiry, you query where last accessed is older than 90 days ago. You don't need to inspect the memory content or guess whether it's still relevant. The schema tracks access, and the expiry rule defines the policy. The job runs automatically and removes stale memories without human intervention.

When you need to audit what's stored about a user, you query where owner equals user ID and return all fields. The result is a structured table: memory type, source, confidence, timestamps, content. The user can see exactly what you've remembered, where it came from, how confident you are, and when it was last used. They can delete individual items or export the entire dataset. The schema makes auditing and compliance straightforward.

When you need to retrieve organizational policy, you query where policy ID matches the topic, effective date is less than or equal to now, retirement date is null or greater than now, and version number is maximum. This returns the single current active version of the policy. You don't get old versions unless you explicitly query for historical audit purposes. The schema enforces version control.

The contrast with schemaless retrieval is stark. Schemaless systems use semantic search over text. You embed the query "user communication preferences" and retrieve the top five semantically similar memories. You get a mix of current preferences, old preferences, speculative inferences, and tangential mentions of communication style from six months ago. You have no confidence scores to weight them, no timestamps to filter stale ones, no conflict resolution to pick the right one. You feed this mess into the model and hope it figures it out. Sometimes it does. Sometimes it doesn't. The system is unreliable because the retrieval is unreliable, and the retrieval is unreliable because the schema doesn't exist.

## Practical Implementation: Storage and Tooling

Implementing structured memory schemas in 2026 means using relational databases or document stores with enforced schemas, not dumping text into vector databases. A production memory system uses Postgres, MySQL, or DynamoDB with a defined schema for each memory type. User preferences are a table with columns for user ID, preference key, preference value, scope, confidence, created timestamp, last accessed timestamp, last modified timestamp, expiry rule, and schema version. Behavioral patterns are a separate table with columns for user ID, pattern description, observation count, observation window, confidence, and timestamps. Organizational policies are a third table with policy ID, version, content, effective date, retirement date, and governance metadata.

Vector embeddings are still useful for certain retrieval tasks, but they're computed from the structured data, not used as the primary storage. You store a user preference as a structured record, then optionally embed the preference description for semantic retrieval when needed. The structured record is the source of truth. The embedding is an index. When you retrieve, you query the structured database first to filter by owner, type, confidence, and expiry status, then optionally use embeddings to rank the filtered results by semantic relevance. This hybrid approach gives you the precision of structured queries and the flexibility of semantic search.

The tooling for schema-based memory includes migration frameworks like Alembic for Python or Flyway for Java, which version and apply schema changes safely. It includes ORM libraries like SQLAlchemy or Prisma, which map database schemas to application code and enforce type safety. It includes expiration job frameworks like Celery or Temporal, which run scheduled tasks to evaluate expiry rules and clean up stale data. It includes audit logging libraries that capture every memory write, update, and deletion with user ID, timestamp, and change description.

Teams that skip these tools and try to manage memory schemas manually through ad hoc scripts end up with broken migrations, inconsistent data, and no audit trail. The tools aren't overhead. They're the minimum infrastructure for reliable memory systems at production scale.

## The Schema-First Mindset: Design Before Store

The final lesson is that schema design happens before you store the first memory, not after you realize you need structure. When you're designing a memory system, the first question is not "how do we store text?" It's "what are the memory types, who owns them, how long do they live, and how do we retrieve them reliably?" You answer these questions by defining schemas. You identify the core fields every memory needs. You define type-specific extensions for preferences, patterns, history, and policies. You specify expiry rules, confidence scoring logic, and conflict resolution policies. You design the retrieval queries. You build the storage layer to support the schema.

This schema-first approach feels like more upfront work than just dumping text into a vector database. It is more upfront work. But it's also the difference between a memory system that works reliably for three years and a memory system that breaks after three months and requires a seven-week emergency migration. The teams that succeed with memory in 2026 are the ones that treat it as a data modeling problem with structured schemas, versioning, governance, and lifecycle management. The teams that fail are the ones that treat it as a "just store whatever" problem and hope semantic search makes up for the lack of structure.

The next subchapter covers how to implement expiration policies that keep memory current without manual cleanup, ensuring that what the system remembers stays relevant as users and contexts evolve.
