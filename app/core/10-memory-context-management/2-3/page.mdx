# 2.3 â€” Dynamic Context Assembly for Different Task Types

In late 2024, a legal technology company launched an AI contract analysis system that provided accurate clause extraction when given clean, isolated contracts. The model could identify termination clauses, liability limits, and payment terms with precision exceeding 92% when tested on individual documents. Six weeks after deployment to 140 corporate legal teams, the company faced a crisis. The system was failing on routine queries that required cross-referencing multiple documents. When a lawyer asked "What are my termination rights across all vendor agreements?", the system would return incomplete answers, miss critical cross-references, and occasionally contradict itself by citing clauses from the wrong contract. The problem was not model capability or retrieval quality. The problem was that the engineering team had built a single, fixed context assembly strategy. Every query received the same pattern: user question, then top-10 retrieved chunks, then generic instructions. The system could not distinguish between single-document analysis tasks that needed deep context from one contract versus multi-document comparison tasks that needed breadth across many contracts versus timeline reconstruction tasks that needed chronologically ordered correspondence. Different task types demand fundamentally different context compositions, and treating all queries with the same assembly strategy is architectural negligence.

## The Context Assembly Pattern

Your system receives hundreds or thousands of potential context elements for any given query. User message history spans dozens of turns. Your vector store returns fifty candidate chunks. You have structured metadata about the user, their account, their permissions, and their preferences. You have system state showing what documents are open, what filters are active, and what the user last modified. The question is not whether you have enough information. The question is which subset of this information belongs in the context window for this specific task at this specific moment, and in what order, and with what framing.

Dynamic context assembly is the practice of constructing the context window programmatically based on task type classification. You do not send the same context pattern to every request. You classify the incoming task, select a context template appropriate for that task type, populate that template with dynamically selected memory and documents, and validate that the assembled context fits within your budget before sending it to the model. This is not optional sophistication. This is the minimum viable pattern for production systems handling more than one type of interaction.

The alternative is what the legal technology company built: a universal context dumper that retrieves documents, appends conversation history, adds the user query, and hopes the model can figure out what matters. This approach works in demos. It fails in production because it wastes context budget on irrelevant information, creates interference between unrelated context elements, and forces the model to perform triage work that your system should handle. You pay for every token you send. Sending irrelevant tokens is operational waste. Sending contradictory tokens is quality risk. Sending unsorted tokens is model load you could have avoided.

## Task Type Classification as Context Strategy Selector

The first decision in dynamic context assembly is task type classification. You must identify what kind of work the user is asking the model to perform because different work requires different context. A customer support query needs recent conversation history and account status. A data analysis task needs schema definitions and sample data. A content generation task needs style guides and example outputs. A debugging task needs error logs and code context. These are not subtle differences. These are structural differences that require different information architectures.

Task type classification can be rule-based or model-based. Rule-based classification uses pattern matching on user queries, examining keywords, query structure, and conversation state. If the user message contains "error" or "not working" and you are in a technical support context, classify as debugging. If the user message asks for a comparison or contains "versus" or "difference between", classify as comparative analysis. If the user message requests generation of new content and does not reference existing documents, classify as creative generation. Rule-based classification is fast, deterministic, and easy to debug. It handles 70% to 85% of production queries reliably.

Model-based classification uses a small, fast model to analyze the user query and conversation context, returning a task type label. You send the last three conversation turns and the current user message to a GPT-4o mini or Claude 3.5 Haiku endpoint with a structured output request for task classification. The model returns a label from your predefined taxonomy: retrieval_qa, multi_document_comparison, timeline_reconstruction, code_generation, creative_writing, technical_debugging, account_inquiry, preference_update. Model-based classification handles ambiguous queries, understands implicit context, and adapts to phrasing variations. It adds 200 to 400 milliseconds of latency and costs $0.001 to $0.003 per classification. You use model-based classification when rule-based classification produces low confidence or when your task taxonomy is complex.

Many production systems use hybrid classification. Rule-based patterns handle obvious cases. Ambiguous queries fall through to model-based classification. You track classification accuracy by logging which classifier was used and whether the assembled context produced a successful response. If rule-based classification consistently misclassifies a pattern, you add a new rule. If model-based classification struggles with a query type, you refine your classification prompt or add examples to your few-shot set.

## Context Templates per Task Type

Once you have classified the task type, you select a context template. A context template is a structured specification of what information goes into the context window, in what order, and with what token budget allocation. Templates are not rigid. Templates are priority-ordered assembly instructions that adapt based on available information and token constraints.

A customer support query template might specify: system instructions with tone and policy guidance, 800 tokens; user account metadata including tier, join date, and recent activity, 200 tokens; last five conversation turns if they exist, up to 1500 tokens; retrieved knowledge base articles, up to 3000 tokens; current user query, variable. This template prioritizes recent conversation history because customer support is inherently conversational. It allocates significant budget to retrieved articles because answers must reference official policy. It includes account metadata because responses should acknowledge user tier and history.

A code generation template looks completely different: system instructions emphasizing code quality and security, 600 tokens; repository context showing file structure and dependencies, 400 tokens; relevant code files from the workspace, up to 5000 tokens; documentation for libraries being used, up to 2000 tokens; recent code edits if in an active session, up to 1000 tokens; current user request, variable. This template prioritizes actual code over conversation history because code generation is context-dependent on existing implementations. It includes repository structure because generated code must integrate correctly. It includes documentation because generated code should follow library conventions.

A legal analysis template prioritizes primary documents: system instructions with jurisdiction and analysis framework, 700 tokens; contract or legal document being analyzed, up to 8000 tokens; relevant regulatory references, up to 2500 tokens; prior analysis or annotations if they exist, up to 1500 tokens; user query, variable. This template gives the majority of the context budget to the primary legal document because legal analysis requires precise textual grounding. It limits conversation history because legal queries are often one-shot or short-threaded.

Your template library should contain eight to twenty templates covering your primary task types. Fewer than eight templates means you are underutilizing dynamic assembly. More than twenty templates means your task taxonomy is too granular and you should consolidate. Each template specifies component types, priority order, and token budget ranges. Priority order determines what gets included when you face token constraints. Token budget ranges are soft limits that can flex based on actual availability but prevent any single component from monopolizing the context window.

## Dynamic Selection of Memory and Documents

Templates specify component types, but components must be populated dynamically. When your template calls for "relevant code files", you must decide which files are relevant for this specific query. When your template calls for "recent conversation history", you must decide how many turns constitute recent and whether all turns are equally relevant. Dynamic selection is where retrieval, ranking, and context assembly intersect.

Selection strategies vary by component type. For conversation history, recency is usually the primary signal. You include the last N turns, working backward from the current query until you hit your token budget or reach a natural conversation boundary. Some systems implement semantic filtering, where turns are included only if they are semantically related to the current query, but this adds complexity and latency. For most conversational tasks, chronological recency is sufficient.

For retrieved documents or chunks, relevance scoring from your retrieval system determines selection. You retrieve the top K candidates, rank them by relevance score, and include them in order until you exhaust your template's document budget. You do not include all retrieved candidates. You include as many as fit within budget, prioritizing highest relevance. Some systems implement diversity re-ranking, where you avoid including multiple chunks from the same document or multiple chunks with high semantic similarity, preferring breadth over redundant depth. Diversity re-ranking improves answer quality when queries span multiple topics but adds 50 to 150 milliseconds to context assembly time.

For structured metadata like user account details or system state, selection is often all-or-nothing. You either include the complete account object or you exclude it. Partial inclusion creates risk of inconsistency or missing critical details. Metadata components are typically small, 100 to 300 tokens, so budget constraints rarely force exclusion. The decision is whether the task type benefits from this metadata. Customer support tasks benefit from account details. Creative writing tasks do not. Your template specifies which metadata components are relevant for each task type.

For multi-document scenarios, selection becomes ordering and interleaving. If your query is "Compare vendor contracts for termination clause differences", you need portions of multiple contracts in context. You cannot fit three complete 10,000-token contracts in a 128,000-token window alongside other context components. You must extract relevant sections from each contract, typically the sections your retrieval system identified as containing termination clauses, and interleave them with clear document boundaries. You might include 1500 tokens from Contract A, then 1200 tokens from Contract B, then 1800 tokens from Contract C, each with a header identifying the source document and section. This requires coordination between your retrieval system and your context assembly system, ensuring that retrieved chunks carry document metadata and that assembly can reconstruct document structure.

## Handling Task Type Shifts Mid-Conversation

Conversations do not maintain constant task types. A user starts with a factual question, transitions to asking for suggestions, then asks you to draft content based on those suggestions, then asks you to critique the draft. Each turn is a different task type. Your context assembly strategy must adapt mid-conversation without losing coherence.

The straightforward approach is to classify each turn independently and assemble context from scratch. This works when each turn is relatively independent, when conversation history is lightweight, and when your assembly latency is low. You classify turn seven as content generation, assemble a content generation context including the last three turns and relevant style guides, generate a response, then classify turn eight as critique, assemble a critique context including the generated content and quality criteria, and generate the critique. This approach is stateless, debuggable, and scales horizontally.

The sophisticated approach maintains conversation state and context continuity. You track what context was included in previous turns, identify deltas for the new turn, and decide whether to rebuild context from scratch or incrementally update. If turn six and turn seven are both content generation tasks and the user is iterating on the same piece of content, you preserve most of the context from turn six and append the new user feedback. This reduces redundant retrieval, lowers token costs, and maintains model continuity. But it requires state management, increases system complexity, and creates risk of context drift where your assembled context gradually diverges from what the current task actually needs.

Most production systems use hybrid approaches. Within a task type, maintain context continuity. When task type changes, rebuild context from scratch. You detect task type changes by comparing the classification of the current turn to the classification of the previous turn. If they match, preserve core context and append new elements. If they differ, discard task-specific context and assemble fresh. Conversation history always carries over, but retrieved documents, code files, or reference materials get refreshed based on the new task type.

Task type shifts create special challenges when the new task depends on outputs from the previous task. If turn six generates content and turn seven asks you to critique that content, the generated content from turn six must be available to turn seven. You have two options. First, you can include the assistant message from turn six in the conversation history, making the generated content part of the chronological conversation flow. This is simple and preserves conversational coherence but consumes history budget. Second, you can extract the generated content and treat it as a document component in the turn seven context, separate from conversation history. This preserves history budget for actual conversation but requires extraction logic and component management.

The legal technology company that failed initially rebuilt context from scratch on every turn, discarding all task-specific context and re-retrieving based solely on the latest query. When a lawyer asked "What are the payment terms?" followed by "And what happens if we are late?", the second query retrieved chunks about late payment penalties but lost the context of which contract was being discussed and what payment terms had just been reviewed. The fix required conversation state tracking, maintaining active document context across turns within a legal analysis session, and only rebuilding context when the user explicitly switched to a new document or task type.

## The Context Assembly Pipeline Architecture

Dynamic context assembly in production is a pipeline, not a function. You have stages that run sequentially, each stage producing inputs for the next stage, with monitoring and fallback at each stage. The pipeline typically consists of five stages: classification, retrieval, selection, assembly, and validation.

Classification receives the user query and conversation history, determines task type, and selects the appropriate context template. This stage is fast, typically 50 to 300 milliseconds depending on whether you use rule-based or model-based classification. Classification failures are rare but catastrophic. If classification produces an incorrect task type, the entire context assembly will be wrong. You log classification decisions, track classification confidence, and implement fallback to a default conservative template when confidence is below threshold.

Retrieval receives the task type and user query, executes retrieval operations against your vector store, knowledge base, code repository, or document store, and returns ranked candidates. Retrieval latency dominates your pipeline, typically 200 to 800 milliseconds depending on index size and query complexity. Retrieval failures are common: no results found, low relevance scores, timeout errors. You implement fallback strategies like broadening the query, retrieving from a secondary index, or proceeding with conversation history alone if retrieval fails completely.

Selection receives the context template, retrieval results, and available metadata, applies budget constraints and ranking logic, and produces an ordered list of context components with token counts. Selection is deterministic and fast, typically 20 to 80 milliseconds. Selection decisions are where your token budget is allocated. You log which components were included, which were excluded, and how much budget each consumed. This logging is critical for debugging context quality issues.

Assembly receives the selected components and constructs the final prompt string, adding delimiters, headers, and formatting. Assembly is string manipulation, typically 10 to 40 milliseconds. Assembly must handle encoding correctly, ensure no components are malformed, and produce a valid prompt structure. Assembly failures are rare but result in model errors or degraded outputs. You validate that assembled prompts parse correctly before sending to the model.

Validation receives the assembled prompt, counts tokens using the model's tokenizer, and confirms the total is within the context window limit with buffer. Validation is your final safety check, typically 30 to 100 milliseconds depending on prompt length. Validation failures mean your budget allocation in selection was wrong or a component exceeded its expected size. You implement overflow handling in validation, which we will explore in the next subchapter, covering detection, prevention, and graceful degradation when assembled context exceeds the model's window.

## Component Priority and Budget Allocation

Every context template must specify component priorities because you will frequently face situations where you cannot fit everything. Component priorities determine what gets included first and what gets dropped if you hit budget constraints. Priorities are not arbitrary preferences. Priorities reflect task requirements and quality dependencies.

System instructions always receive first priority. System instructions are non-negotiable because they define model behavior, set tone and style, and establish safety constraints. A system instruction component typically consumes 400 to 1200 tokens. You never drop system instructions to make room for other components. If your context window is so constrained that you cannot fit system instructions, you are using the wrong model.

The user query receives second priority. You cannot answer a question if you do not include the question. User queries vary widely in length, from 20 tokens for "Summarize this" to 2000 tokens for a query that includes pasted text or detailed requirements. You include the complete user query without truncation. Truncating user queries creates incomprehensible tasks.

After system instructions and user query, priority depends on task type. For retrieval QA tasks, retrieved documents receive third priority because you cannot answer questions without source material. Conversation history receives fourth priority because QA is often stateless or lightly stateful. For conversational support tasks, the priority flips: recent conversation history receives third priority because support is inherently conversational and context-dependent, and retrieved knowledge base articles receive fourth priority. For code generation tasks, existing code receives third priority because generated code must integrate with existing implementations, documentation receives fourth priority, and conversation history receives fifth priority.

You implement priority as an ordered list in your template. When the selection stage applies budget constraints, it walks the priority list, allocating budget to each component in order until the budget is exhausted. Components that do not fit are excluded. This ensures that the most critical information always makes it into the context, and only nice-to-have information gets dropped under pressure.

Some systems implement graduated inclusion, where low-priority components are included in compressed or summarized form when they do not fit at full size. If conversation history does not fit in its allocated budget, you summarize older turns and include recent turns verbatim. If retrieved documents do not fit, you include introductory paragraphs and key excerpts rather than full text. Graduated inclusion improves context completeness but increases assembly complexity and requires summarization infrastructure.

## Context Assembly Latency Budget

Dynamic context assembly adds latency to every request. Classification adds 50 to 300 milliseconds. Retrieval adds 200 to 800 milliseconds. Selection, assembly, and validation together add 60 to 220 milliseconds. Your total assembly latency budget is 310 to 1320 milliseconds before the request even reaches the model. For user-facing systems, this is a substantial portion of your total response time budget.

You manage assembly latency through parallelization and caching. Classification and retrieval can run in parallel if classification is rule-based and retrieval does not depend on task type. If your retrieval strategy is the same across all task types, you trigger retrieval immediately when the query arrives and trigger classification in parallel, then merge results in the selection stage. This cuts 50 to 300 milliseconds from your serial latency. If retrieval strategy varies by task type, you must classify first, then retrieve, accepting the serial latency cost.

Caching reduces latency for repeated operations. If your system instructions are identical across all requests of the same task type, you tokenize and cache them once rather than tokenizing on every request. If your retrieval results are stable for a given query, you cache retrieval results for 30 to 120 seconds and reuse them for duplicate queries. If your context templates are static, you compile them once at startup rather than parsing on every request. Caching is particularly effective for high-traffic systems where the same queries and task types recur frequently.

Latency budgets must be enforced with timeouts. If retrieval exceeds 800 milliseconds, you abort and proceed with fallback context. If model-based classification exceeds 400 milliseconds, you abort and use rule-based classification or default to a conservative template. Timeouts prevent tail latencies from cascading into user-visible delays. You monitor timeout rates and adjust retrieval indexes, classification models, or assembly logic when timeout rates exceed 2% to 5%.

The context assembly pipeline is not invisible infrastructure. Context assembly latency, token costs, and quality outcomes are observable metrics that inform system optimization. You log assembly time per stage, tokens consumed per component, task type distribution, and whether assembled context led to successful responses. You review these metrics weekly, identify bottlenecks, and iterate on templates, priorities, and selection logic. Context assembly is where your understanding of what information matters for which tasks is encoded, and getting it right is the difference between a system that feels intelligent and a system that feels like it is guessing.

Different tasks demand different context, and treating every query the same is negligence. You classify task type, select a template, populate it dynamically, assemble components in priority order, and validate before sending. This is not optional complexity. This is operational competence. The next subchapter covers context overflow, exploring detection, prevention, and graceful degradation when your assembled context exceeds the model's window.
