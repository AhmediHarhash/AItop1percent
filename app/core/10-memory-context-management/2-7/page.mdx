# 2.7 — Prompt-Context Interaction: How Memory Shapes Prompt Design

In June 2025, a healthcare scheduling platform deployed a new AI assistant designed to help patients reschedule appointments. The system prompt was carefully crafted over three weeks by the product team, with instructions to "prioritize the patient's preferred appointment times and locations based on their history." The prompt also included guidance to "confirm the patient's transportation needs" and "acknowledge any accessibility requirements mentioned in previous visits." The team launched with confidence, having tested the prompts extensively in their staging environment with manually curated example conversations. Within four days, customer support was flooded with complaints. Patients reported that the assistant kept asking them to repeat their preferences every single conversation, ignored their documented mobility restrictions, and never referenced their stated scheduling constraints. The root cause was not in the prompt—the prompt was well-written. The failure was architectural. The prompts assumed a memory system that did not exist. The instructions referenced "history" and "previous visits" and "mentioned accessibility requirements," but the actual system had no memory retrieval mechanism wired in. Every conversation started with an empty context window. The prompt was giving orders to consult information that was never present. This is the core lesson: your prompts and your context are not independent components. They form a coupled system, and designing one without the other guarantees failure.

## The Myth of Prompt Independence

Most teams treat prompt engineering as a standalone discipline. They iterate on instructions, refine tone, adjust specificity, and test outputs in isolation. This approach works only if your context window is static and predictable. The moment you introduce memory—session state, user history, retrieved documents, previous tool calls, injected knowledge—your prompt is no longer self-contained. It becomes a set of instructions that operate on dynamic context, and every assumption the prompt makes about what information is available becomes a potential failure point. A prompt that says "use the customer's communication preferences" assumes those preferences are in the window. A prompt that says "reference the conversation history to avoid repetition" assumes you are injecting that history. A prompt that says "if the user has a premium subscription, offer advanced features" assumes the subscription status is visible to the model. When these assumptions are violated, the model does not fail gracefully. It hallucinates, invents plausible-sounding details, or ignores the instruction entirely. The user experiences this as inconsistency, forgetfulness, or outright fabrication.

The problem is compounded by the fact that context composition varies across requests. One request might include three retrieved documents, another might include fifteen. One conversation might have two turns of history, another might have fifty. Your prompt is constant, but the data it operates on is variable. If your prompt is written as if the context is always complete and always structured the same way, it will fail when the context is sparse, when retrieval returns nothing, when memory items are excluded due to budget limits, or when the user starts a fresh session. You need prompt designs that adapt to the actual shape and content of the context window, not prompts that assume an ideal state.

## Prompt Sections That Reference Memory

The first pattern for managing prompt-context interaction is to structure your prompt into sections that explicitly reference memory sources. Instead of writing a monolithic system prompt that mixes general instructions with memory-dependent instructions, you separate them. Your prompt might have a General Instructions section that works regardless of context, a User Profile section that only activates when user profile data is present, a Conversation History section that only activates when history is injected, and a Retrieved Knowledge section that only activates when documents are retrieved. Each section is self-contained and includes conditional logic that acknowledges the presence or absence of its corresponding context.

For example, your User Profile section might begin with prose that says "if user profile data is available, prioritize the user's stated preferences for communication style, response length, and topic focus. If no profile data is available, default to neutral professional tone and medium-length responses." This is not implemented as branching code in the prompt itself—MDX cannot execute logic—but as clear instructions to the model about what to do in each scenario. The model is capable of recognizing whether the context contains user profile data, and it can follow conditional instructions based on that recognition. You make the condition explicit in the prompt text, and you structure the context injection to make the presence or absence of data obvious. If user profile data is present, you inject it with a clear delimiter: "User Profile Data:" followed by the structured fields. If it is not present, you omit that section entirely or include a line that says "User Profile Data: None available." The model sees the difference and adjusts behavior accordingly.

This approach requires coordination between your prompt authoring process and your context injection process. The team writing prompts must know exactly how memory is structured, where it appears in the context window, and what delimiters are used. The team building the memory retrieval system must commit to a stable format that the prompts can rely on. If retrieval returns user preferences as a bulleted list under a "Preferences" header, the prompt can reference "the Preferences section." If retrieval returns conversation history as timestamped turn pairs, the prompt can reference "the most recent exchange in the conversation history." The tighter the contract between prompt structure and context structure, the more reliably the model can follow instructions that depend on memory.

## Conditional Instructions Based on Available Context

Beyond structuring prompts into sections, you need conditional instructions that tell the model how to behave when context is incomplete. A common failure mode is the overconfident prompt: instructions that assume full context and provide no fallback. "Use the customer's purchase history to recommend products" is an overconfident instruction if purchase history is not always available. When the history is missing, the model either hallucinates a history, ignores the instruction, or defaults to generic recommendations without acknowledging the gap. A better instruction is: "If the customer's purchase history is available, use it to recommend products aligned with their past preferences. If purchase history is not available, recommend top-rated products in the requested category and inform the customer that personalized recommendations will be available once they make their first purchase." This version acknowledges both states and provides explicit guidance for each.

You apply this pattern to every memory-dependent instruction. If the prompt tells the model to "avoid repeating information already covered in the conversation," you pair it with "if no conversation history is available, treat this as the first interaction and provide complete context." If the prompt tells the model to "adjust tone based on the user's preferred formality level," you pair it with "if formality preference is unknown, default to neutral professional tone." If the prompt tells the model to "reference the uploaded document when answering technical questions," you pair it with "if no document is uploaded, inform the user that you can answer general questions but detailed technical answers require them to upload the relevant documentation." Each conditional instruction reduces the risk of the model making assumptions about unavailable data.

Writing these conditional instructions requires you to enumerate all the possible states of context incompleteness. This is not a trivial task. Your context window might include user profile, conversation history, retrieved documents, tool call results, session metadata, and real-time data feeds. Each of these can be present, absent, partial, or stale. You cannot write a conditional for every combination—that would produce an unmanageable prompt. Instead, you identify the high-impact memory sources—the ones that most influence model behavior—and write conditionals for those. For a customer support assistant, conversation history and user account status are high-impact. For a document analysis tool, retrieved document content is high-impact. For a personal productivity assistant, user preferences and calendar data are high-impact. You focus your conditional instructions on these sources and accept that lower-impact context will be handled by the model's general reasoning.

## Prompt Templates That Adapt to Context Composition

The next level of sophistication is prompt templates that adapt based on the actual composition of the context window. Instead of a single static prompt, you generate prompts dynamically based on what memory is available. If the user is in their first session, you use a First Session Prompt that emphasizes onboarding, explains capabilities, and avoids referencing history. If the user is in a returning session with conversation history, you use a Returning Session Prompt that acknowledges continuity, avoids redundant explanations, and references past interactions. If the user has uploaded a document, you use a Document Analysis Prompt that structures the interaction around the document. If the user is interacting in a high-priority support context, you use a Priority Support Prompt that emphasizes urgency and escalation paths.

This pattern is not about writing dozens of completely different prompts. It is about composing prompts from modular blocks based on context state. You have a core instruction set that appears in all prompts, and you have optional instruction blocks that are included conditionally. Your prompt generation logic checks the context window: Does it include user profile? Include the User Profile Instructions block. Does it include conversation history longer than five turns? Include the Conversation Continuity Instructions block. Does it include retrieved documents? Include the Document Reference Instructions block. The final assembled prompt is tailored to the actual context the model will see, which eliminates instructions that reference unavailable data and emphasizes instructions that are relevant to the current state.

Implementing this requires infrastructure. You cannot manually assemble prompts per request at scale. You need a prompt templating system that accepts context metadata as input and outputs the appropriate prompt. The metadata might be as simple as a set of boolean flags: has user profile, has conversation history, has retrieved documents, has tool call results. Your templating logic uses these flags to decide which instruction blocks to include. Some teams build this as a simple string concatenation system. Others use template engines with conditional rendering. Others use LLMs to generate the prompt based on a meta-prompt that describes the context state. The implementation matters less than the principle: the prompt must adapt to the context, not assume a fixed context.

## The Context-Aware Prompt Pattern

The context-aware prompt pattern formalizes this adaptive approach. A context-aware prompt is structured in three layers. The first layer is the invariant core: instructions that apply regardless of context state. This includes your role definition, your primary objective, your tone and style guidelines, and your safety and refusal policies. These instructions never change. The second layer is the context-dependent instructions: sections that activate based on available memory. These are the conditional blocks described above. The third layer is the context summary: a brief recap of what information is available to the model in this specific request. This summary appears at the start of the prompt and serves as a map for the model. It might say "Available context: user profile with communication preferences, conversation history covering the last two interactions, no retrieved documents, no uploaded files." This summary primes the model to understand the boundaries of its knowledge before it begins processing the user's input.

The context summary is particularly important for preventing hallucination. When the model knows what it does not have, it is less likely to invent details to fill gaps. If the summary says "no user purchase history available," the model is less likely to hallucinate a purchase history when answering product recommendation questions. If the summary says "conversation history limited to current session," the model is less likely to reference events from previous sessions that are not in the window. The summary creates explicit negative space—it names the absent information—and this reduces the model's tendency to confabulate.

Building a context-aware prompt requires tight integration between your memory system and your prompt system. Your memory retrieval process must output metadata about what it retrieved: how many items, from which sources, covering what time range, with what confidence scores. This metadata feeds into your prompt generation logic. If retrieval returned zero documents, the prompt includes no document reference instructions and the context summary says "no retrieved documents." If retrieval returned fifteen documents but they were all low-confidence, the prompt includes a caveat: "retrieved documents available but may not be directly relevant—verify key facts before stating them as certain." The prompt becomes a reflection of the actual context, not an idealized assumption.

## Common Failures When Prompts Assume Missing Context

The most frequent failure is the ghost reference: a prompt instruction that references data not in the context window. "Use the customer's preferred contact method" when no contact preference is in the window. "Follow the company's internal guidelines" when no guidelines are injected. "Continue the conversation from where we left off" when no conversation history is available. Ghost references produce two failure modes. The first is hallucination: the model invents plausible-sounding data to satisfy the instruction. It generates a fake contact preference, a fake set of guidelines, a fake conversation history. The second is instruction abandonment: the model ignores the instruction because it cannot fulfill it, and this produces inconsistent behavior where sometimes the model follows an instruction and sometimes it does not, depending on whether the required context is present.

The second common failure is the context overload prompt: instructions that assume unlimited context budget and tell the model to reference everything available. "Use all retrieved documents to provide a comprehensive answer." "Consider the user's entire conversation history when formulating a response." "Incorporate all available user preferences into your recommendation." These prompts work when context is small, but they fail when retrieval returns dozens of documents, when conversation history spans hundreds of turns, or when user preferences include fifty fields. The model cannot effectively process all of it within token limits, or it spends so many tokens on context processing that the response is truncated. A better prompt provides guidance on prioritization: "Use the most relevant retrieved documents, prioritizing those that directly address the user's question." "Reference recent conversation history, focusing on the last three exchanges." "Incorporate the user's top-priority preferences, specifically communication style and response format."

The third common failure is the brittle delimiter dependency: prompts that rely on exact formatting of injected context. "Find the user's email address in the User Profile section" assumes the context uses the exact header "User Profile" and structures email as a labeled field. If the context injection format changes—maybe the header becomes "Profile Data" or email is embedded in a paragraph instead of labeled—the prompt breaks. The model cannot find the expected structure and either fails to extract the information or extracts the wrong information. To avoid this, you make prompts resilient to formatting variation. Instead of "find the email address in the User Profile section," you write "locate the user's email address, which may appear in user profile data, account information, or contact details." You describe the semantic content, not the structural location, and you allow the model to find it wherever it appears.

The fourth failure is the context-prompt contradiction: a prompt that gives instructions that conflict with the available context. A prompt that says "be concise" but then injects ten pages of conversation history and fifteen retrieved documents. The model sees verbose context and interprets that as a signal that verbosity is expected, overriding the conciseness instruction. A prompt that says "assume the user is a beginner" but then injects advanced technical documentation into the context. The model sees expert-level material and adjusts its response to match, contradicting the beginner assumption. A prompt that says "prioritize speed" but then injects complex multi-step reasoning examples in the conversation history. The model sees detailed reasoning and mirrors it, producing slow responses. The lesson is that your context is part of the prompt. It sets expectations and provides examples, and those signals can overpower explicit instructions. If you want concise responses, inject concise context. If you want beginner-friendly responses, inject beginner-friendly context. If you want fast responses, inject examples of fast responses. The context and the instructions must reinforce each other, not contradict.

## Designing Prompts for Variable Memory Architectures

When your memory architecture varies by user tier, task type, or deployment environment, your prompts must account for that variation. A free-tier user might have no memory retrieval, a standard-tier user might have keyword-based retrieval of recent history, and a premium-tier user might have vector-based retrieval across their entire interaction history plus external knowledge sources. If you use the same prompt for all three tiers, the prompt will either under-serve premium users or over-promise to free users. Instead, you use tier-specific prompts. The free-tier prompt says "I can help with your request based on the information you provide in this conversation." The standard-tier prompt says "I can reference your recent activity to provide continuity across sessions." The premium-tier prompt says "I have access to your full interaction history and can retrieve relevant past conversations and external knowledge to give you deeply personalized assistance."

Similarly, task-specific memory architectures require task-specific prompts. A document Q&A task might inject the full document into the context window, and the prompt tells the model "answer based on the provided document—do not use external knowledge." A general conversation task might inject no documents, and the prompt tells the model "answer using your training knowledge and the current conversation." A research task might inject retrieved web results, and the prompt tells the model "synthesize information from the retrieved sources and cite them when making factual claims." Each prompt is written for the memory architecture that supports that task, and each prompt makes explicit what information sources the model should rely on.

This alignment between prompt and memory architecture is not automatic. It requires coordination between the teams that design memory retrieval and the teams that write prompts. In many organizations, these are separate teams with separate roadmaps. The memory team builds retrieval features based on technical feasibility and cost constraints. The prompt team writes instructions based on desired user experience and output quality. The two teams do not coordinate, and the result is prompts that reference memory that does not exist or memory systems that inject data the prompts do not know how to use. The fix is cross-functional design: memory architecture and prompt design must be co-developed. When the memory team decides to add a new retrieval source, the prompt team must update instructions to reference it. When the prompt team identifies a new instruction that requires context, the memory team must ensure that context is retrievable and injectable. This is not a one-time alignment—it is an ongoing collaboration that evolves as the system evolves.

## Testing Prompt-Context Interaction

Testing prompts in isolation is insufficient. You must test prompts against realistic context distributions. This means building test cases that cover the full range of context states your system will encounter: empty context, minimal context, typical context, maximal context, malformed context, and contradictory context. For each state, you verify that the prompt produces appropriate behavior. An empty context test checks that the model does not hallucinate missing information. A minimal context test checks that the model uses available information without demanding more. A typical context test checks that the model follows memory-dependent instructions correctly. A maximal context test checks that the model prioritizes effectively and does not get lost in volume. A malformed context test checks that the model degrades gracefully when context structure is unexpected. A contradictory context test checks that the model resolves conflicts between prompt instructions and context signals.

You run these tests continuously as both prompts and memory systems evolve. A change to retrieval logic that alters the format of injected documents can break prompts that expect a specific structure. A change to prompt instructions that adds a new memory reference can fail if the memory system does not retrieve that data. Regression testing must cover the interaction, not just the components. Many teams test prompts with synthetic context—manually crafted examples that represent idealized states. This misses the reality of production context: incomplete retrieval, noisy history, schema drift, and edge cases. You need test cases built from real production context snapshots, anonymized and curated, that capture the messy reality your prompts will face.

## Evolving Prompts as Memory Capabilities Expand

As you add memory capabilities—new retrieval sources, longer history windows, richer user profiles—your prompts must evolve to leverage them. A prompt written for a system with no memory will under-utilize a system with rich memory. A prompt written for keyword retrieval will not fully exploit vector retrieval. A prompt written for session-scoped history will miss opportunities when you extend to cross-session history. Each memory upgrade is an opportunity to enhance prompt instructions, but only if you actively revisit and revise prompts. Many teams treat prompts as static artifacts: write once, deploy, forget. This works only if memory architecture is static. When memory evolves, prompts must evolve with it, and this requires a prompt versioning and update process that tracks memory capabilities and ensures prompts stay aligned.

The discipline of prompt-context interaction design is the foundation of reliable memory-augmented AI systems. When prompts and context are designed in isolation, you get brittle systems that fail silently, hallucinate confidently, and frustrate users unpredictably. When they are designed together, with explicit contracts, conditional logic, and adaptive structures, you get systems that behave predictably across the full range of context states, degrade gracefully when context is incomplete, and scale reliably as memory capabilities grow. The next subchapter addresses the mechanism that prevents memory from consuming unbounded resources: hard caps on tokens, items, and retrieval operations that enforce budget discipline without sacrificing functionality.
