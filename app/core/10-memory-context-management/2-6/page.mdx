# 2.6 â€” Context Caching and Reuse Strategies

In late 2025, a legal technology company launched an AI contract analysis assistant that was technically impressive but economically unsustainable. The assistant helped lawyers review contracts against their firm's playbook: a 35,000-token document covering standard clauses, negotiation positions, and risk thresholds. Each contract review required sending the playbook plus the contract plus conversation history to the model. The contracts averaged 12,000 tokens. A typical review conversation was 15 turns. By turn 10, the system had sent the 35,000-token playbook to the model 10 times, re-processing 350,000 tokens of identical content. Across 200 daily reviews, the system processed 7 million tokens per day of repeated playbook content, costing 17.50 dollars per day at GPT-5 pricing just for re-sending the same static document. Over a month, that was 525 dollars in pure waste. The playbook changed quarterly. Between updates, every token spent re-processing it was unnecessary. The root cause was treating context as ephemeral when most of it was static and reusable across thousands of requests.

Context caching is the practice of storing processed context components and reusing them across multiple requests instead of re-processing identical content every time. Most production AI systems have substantial context reuse opportunities. System prompts are identical across requests. Retrieved knowledge documents are often shared across similar queries. Few-shot examples are static. Memory injections overlap heavily between turns in a conversation. Caching these components eliminates redundant processing, reduces costs, and improves latency. The challenge is identifying what to cache, when to invalidate caches, and avoiding the failure mode where stale cached content causes wrong answers.

## Prompt Caching: Provider-Native Reuse

**Prompt caching** is a feature offered by model providers that allows you to mark portions of your prompt as cacheable. When you send a request with cacheable content, the provider processes it once and stores the processed representation on their servers. On subsequent requests, if the cacheable content is identical, the provider reuses the stored representation instead of re-processing it. You pay full input token prices for the initial cache write and reduced prices for cache reads. Anthropic introduced prompt caching for Claude in mid-2024 and by 2025 it was a standard feature. OpenAI added similar functionality for GPT-5 and GPT-5.1 in 2025. By early 2026, prompt caching is table stakes for production AI systems operating at scale.

The mechanics are straightforward. You structure your prompt with cacheable sections marked explicitly. A typical structure is system prompt as the first cacheable block, static knowledge or examples as the second cacheable block, and dynamic user input as non-cached content. You send your first request with this structure. The provider processes the entire prompt, stores the cacheable blocks, and returns a cache ID or uses the content hash as the cache key. On your second request, you send the same cacheable blocks and new user input. The provider detects the cache hit, reuses the processed cacheable blocks, and only processes the new user input. You pay full price for the user input and a reduced rate for the cached blocks, typically 10% of normal input pricing.

The cost savings are dramatic for workloads with high context reuse. The legal tech company with the 35,000-token playbook switched to prompt caching in November 2025. On the first request of each day, they paid full price: 8.75 cents for the playbook at 2.50 dollars per million input tokens. On subsequent requests that day, they paid 0.25 dollars per million for cached reads: 0.875 cents per request for the playbook. Across 200 daily requests, they paid 8.75 cents once plus 0.875 cents times 199, totaling 1.82 dollars per day instead of 17.50 dollars per day, a 90% reduction in playbook processing costs. Over a month, savings were 469 dollars. Across a year, savings were 5,628 dollars, enough to fund additional features or absorb higher usage.

Prompt caching works best when your cacheable content is large relative to your dynamic content and when the same cacheable content is used across many requests. A system prompt that is 5,000 tokens and reused across 1,000 requests per day sees massive savings. A system prompt that is 200 tokens and changes frequently sees minimal benefit. The breakeven point depends on pricing, but generally you want at least 1,000 tokens of cacheable content and at least 10 requests reusing it to justify the complexity of implementing caching.

Cache duration is provider-specific. Anthropic's prompt caching in 2025-2026 stores caches for 5 minutes of inactivity: if you send another request within 5 minutes, the cache is still warm. After 5 minutes, the cache expires and the next request pays full processing costs to rebuild it. OpenAI's implementation in 2026 offers configurable cache TTLs from 5 minutes to 1 hour depending on your pricing tier. For high-traffic systems with steady request rates, caches stay warm indefinitely. For low-traffic systems or systems with bursty usage, caches expire frequently and savings are reduced.

You must design your prompt structure to maximize cache hits. If you include timestamps or request IDs in your system prompt, every request has unique cacheable content and you get zero cache hits. If you include user-specific information in cacheable blocks, each user gets separate caches and reuse is limited to per-user request sequences. The cacheable portions must be truly static across the requests you want to optimize. Dynamic content goes after the cacheable blocks in the prompt structure. This sometimes requires rethinking prompt design: you might move user context from the system prompt into a separate user message to keep the system prompt cacheable.

## Semantic Caching: Reusing Results for Similar Queries

**Semantic caching** goes beyond exact-match caching to reuse results for similar queries, even if the queries are not identical. Instead of caching the input context, you cache the output results and retrieve them based on semantic similarity to new queries. When a user asks "what are the risks of this contract," you generate an answer and store it with an embedding of the query. When another user asks "what risks does this contract have," you compute the embedding of the new query, find it is highly similar to the cached query, and return the cached answer instead of generating a new one. Semantic caching is effective when many users ask similar questions and when exact answers do not need to be regenerated each time.

Implementing semantic caching requires an embedding model, a vector database, and a similarity threshold policy. When a request comes in, you embed the query using a model like OpenAI's text-embedding-3-large or Anthropic's embedding endpoint. You search your cache database for entries with embeddings above a similarity threshold, typically 0.90 or higher. If you find a hit, you return the cached result. If you do not, you generate a fresh result, embed the query, store the result in the cache, and return it. On subsequent similar queries, you hit the cache and save generation costs.

The similarity threshold is critical. Too high and you get very few cache hits because queries must be nearly identical. Too low and you return cached results that do not actually answer the new query. A threshold of 0.95 is very conservative: only nearly identical queries hit the cache. A threshold of 0.85 is aggressive: even moderately similar queries hit the cache, which can cause mismatches. Most teams start at 0.92 and tune based on false positive rates. You must monitor cache hits that users immediately rephrase or reject, which indicates the cached answer did not satisfy their actual query.

Semantic caching is most effective for FAQ-style workloads where many users ask the same questions in different words. A customer support bot answering "how do I reset my password" versus "what's the process for password reset" versus "I forgot my password how do I change it" can reuse the same answer for all three. A documentation assistant answering "how does authentication work" versus "explain the authentication flow" versus "what's the auth mechanism" can cache one detailed answer and serve it for all variants. The savings compound quickly: if 30% of queries are semantic duplicates and you cache for 24 hours, you reduce generation volume by 30% and costs by a corresponding amount.

Semantic caching fails when queries are similar but require different answers due to context. Two users asking "what is my account balance" have semantically identical queries but need different answers based on their account data. Two users asking "what are the risks in this contract" are asking the same question but about different contracts. You must include context in the cache key to avoid serving wrong answers. One approach is to cache per-document: the cache key is the query embedding plus the document ID. "What are the risks" for contract A hits the cache only for future queries about contract A, not contract B. This reduces cache hit rates but prevents cross-contamination.

Cache invalidation is the hard problem in semantic caching. If the underlying knowledge changes, cached answers become stale. You update your documentation, but cached answers still reference the old version. You change your pricing, but cached answers still quote old prices. You need invalidation policies: time-based expiration, event-based purging, or version tagging. Time-based expiration is simple: cache entries expire after N hours. If your knowledge updates daily, set expiration to 24 hours. Event-based purging is more precise: when you update a document, you purge all cache entries associated with that document. Version tagging includes a version ID in the cache key: when the version increments, old caches are automatically obsolete.

A financial services company in 2025 used semantic caching for a retirement planning assistant. Users asked questions like "how much should I save for retirement" in hundreds of variations. The company cached answers with 24-hour expiration and 0.92 similarity threshold. Cache hit rate was 37% after two weeks of usage, meaning 37% of queries were answered from cache without calling the model. At GPT-5 pricing, this saved approximately 420 dollars per month on a system handling 50,000 queries monthly. The savings funded the vector database costs and left net savings of 280 dollars per month, a 12% total cost reduction.

## Component-Level Caching: Granular Reuse

**Component-level caching** treats context as composed of independent blocks and caches each block separately. Instead of caching the entire prompt or the final result, you cache individual context components: the system prompt, the few-shot examples, the retrieved documents, the user memory, the policy rules. When constructing a new prompt, you assemble it from cached components plus fresh dynamic content. This maximizes reuse because different requests often share some components but not all.

A customer support system might have a system prompt component, a product knowledge component, a policy rules component, a conversation history component, and a user query component. The system prompt is identical across all requests. The product knowledge is shared across all requests about a product category. The policy rules are shared across all requests in a region. The conversation history is unique per conversation. The user query is unique per request. You cache the first three components and dynamically load the last two. When a user in the US asks about product A, you load cached system prompt, cached product A knowledge, cached US policy rules, current conversation history, and current query. The next user in the US asking about product A reuses the first three cached components. A user in the EU asking about product A reuses the system prompt and product A knowledge but loads different cached policy rules.

Component-level caching requires a component catalog: a registry of available components with their cache keys, sizes, and update timestamps. When constructing a prompt, you look up the required components, check if cached versions exist, load from cache if available, and fetch fresh if not. This adds complexity but provides fine-grained control over what gets reused. You can implement component caching at the application level using Redis, Memcached, or DynamoDB, storing serialized components with appropriate TTLs.

The cache hit rate for component-level caching depends on component granularity. Very fine-grained components have higher individual hit rates but more overhead assembling many small pieces. Very coarse-grained components have lower hit rates because fewer requests share the exact combination. A middle ground is effective: 5 to 10 components per prompt, each representing a coherent reusable unit. System instructions are one component, not split across multiple. Few-shot examples are one component, not split per-example. Retrieved documents are one component per document, allowing partial overlap across requests retrieving some shared documents.

Component versioning is critical for correctness. If you update a component, cached copies must be invalidated. You can use version tags in cache keys: system-prompt-v3, policy-rules-us-v7, product-knowledge-a-v12. When you update the US policy rules, you increment to v8 and old v7 caches naturally expire. Requests start using v8 immediately without purging old caches, which is safer than in-place updates that might leave partial stale state.

A SaaS company in early 2026 implemented component-level caching for an AI sales assistant. The assistant used a 4,000-token system prompt, 8,000 tokens of product documentation, 3,000 tokens of pricing rules, 2,000 tokens of sales methodology, and 1,500 tokens of competitive intelligence, plus dynamic conversation history and user input. All five static components were cached with 7-day expiration. The system prompt and sales methodology were global. Product docs were per-product. Pricing rules were per-region. Competitive intelligence was per-competitor set. Across 10,000 daily requests, the cache hit rate for the system prompt was 99%, for sales methodology 99%, for product docs 73%, for pricing rules 82%, for competitive intelligence 45%. The blended savings were approximately 62% reduction in static content processing costs, translating to 890 dollars per month saved on a system spending 1,430 dollars monthly on model calls.

## Cache Invalidation Challenges and Strategies

Cache invalidation is famously one of the two hard problems in computer science, along with naming things and off-by-one errors. In AI context caching, invalidation is hard because the correctness impact of stale caches is often subtle and delayed. If you cache a retrieval result and the underlying document updates, requests using the cached result get outdated information. Users might not notice immediately, but over time the system gives progressively more wrong answers as more documents update and more caches go stale. You discover the problem when a user reports that the assistant is giving outdated information, you investigate, and you find caches that are weeks old.

The simplest invalidation strategy is time-based expiration. Set a TTL on every cache entry: 5 minutes, 1 hour, 24 hours, 7 days. The TTL should match your content update frequency. If your knowledge base updates hourly, cache for at most 1 hour. If your system prompt changes monthly, cache for days or weeks. Time-based expiration is easy to implement and guarantees eventual consistency, but it is inefficient: caches expire even when content has not changed, forcing unnecessary regeneration.

Event-based invalidation is more efficient but more complex. When content updates, you emit an invalidation event that purges related caches. You update a document, you purge caches for queries that retrieved that document. You update a system prompt, you purge all caches using that prompt. This requires tracking dependencies: which cache entries depend on which content. You can implement this with cache tags: when caching, you tag the entry with content IDs it depends on. When content updates, you purge all entries with that tag. Redis supports tag-based invalidation natively. DynamoDB requires secondary indexes on tags for efficient purging.

Versioned caching avoids invalidation entirely by including content version in the cache key. When content updates, you increment its version, and new requests use new cache keys automatically. Old caches persist but are never accessed because the version in the key is outdated. They expire naturally via TTL. This is clean and safe but uses more cache storage because multiple versions coexist temporarily. You mitigate this with short TTLs on versioned caches: 1 hour or less, so old versions evict quickly.

Lazy invalidation checks freshness on cache read instead of proactively purging. When you hit a cache entry, you check whether the cached content is still current by comparing a content hash or version number. If current, use the cache. If stale, regenerate and update the cache. This is efficient but adds latency to cache reads because you must verify freshness each time. It works well when freshness checks are cheap: a simple version number comparison or a fast hash lookup.

A common hybrid strategy is time-based expiration with event-based purging for critical updates. Most caches expire after 6 hours, but if you make a critical update to policy or pricing, you immediately purge related caches. This combines the simplicity of TTLs with the responsiveness of event-based invalidation for high-impact changes.

## Cost Savings from Effective Caching: Real Numbers from 2025-2026

The economic impact of context caching at scale is substantial. A mid-sized AI application in 2026 handling 500,000 requests per month with an average of 20,000 input tokens per request processes 10 billion input tokens monthly. At GPT-5 pricing of 2.50 dollars per million input tokens, that is 25,000 dollars per month in input costs. If 60% of those input tokens are cacheable and you achieve 80% cache hit rate on the cacheable portion, you reduce input token processing by 48%. That is 4.8 billion tokens monthly moving from full-price processing at 2.50 dollars per million to cached reads at 0.25 dollars per million. The cost for those tokens drops from 12,000 dollars to 1,200 dollars, saving 10,800 dollars per month. Over a year, that is 129,600 dollars saved through caching.

These numbers are not theoretical. A healthcare documentation company in late 2025 reported 11,200 dollars in monthly savings from implementing prompt caching on Claude Opus for clinical note generation. They had 55,000 tokens of medical coding guidelines and templates as cacheable content, reused across 80,000 notes per month. Before caching, they paid full input prices: 13,750 dollars per month just for the guidelines and templates. After caching, they paid full price on the first request each day per provider (approximately 400 providers) and cached prices on subsequent requests, totaling 2,550 dollars per month. The 81% cost reduction funded two additional engineers working on quality improvements.

A legal tech platform in early 2026 achieved 67% reduction in context processing costs through a combination of prompt caching and component-level caching. They cached a 28,000-token legal analysis framework, 15,000 tokens of jurisdiction-specific rules, and 12,000 tokens of firm-specific playbooks. Across 120,000 monthly requests, cache hit rates were 94% for the framework, 78% for jurisdiction rules, and 71% for playbooks. Total input token costs dropped from 18,400 dollars per month to 6,070 dollars per month, saving 12,330 dollars monthly. The savings exceeded the cost of the Redis cluster used for component caching by a factor of 20.

These case studies share a pattern: large static context components reused across many requests. The savings are proportional to the size of cacheable content and the request volume over which it is reused. A 50,000-token playbook reused across 100,000 requests saves vastly more than a 5,000-token system prompt reused across 1,000 requests. You maximize savings by identifying your largest static context components and ensuring they are cached effectively.

## When Caching Hurts: Stale Context and Wrong Answers

Caching introduces a correctness risk: serving stale cached content that causes wrong answers. This is the failure mode that prevents many teams from implementing caching despite clear economic benefits. The fear is justified. A customer support bot caching product documentation serves outdated answers after the product changes. A financial advisor caching market data gives recommendations based on yesterday's prices. A medical assistant caching treatment protocols follows outdated guidelines after a clinical update. These are not hypothetical risks. They happen in production when caching is implemented without proper invalidation.

The severity of stale content depends on your domain. In casual chatbots, stale caches are low-risk: maybe the assistant mentions an old feature or outdated pop culture reference, the user notices, the conversation continues. In regulated domains, stale caches are high-risk: wrong medical advice, wrong legal guidance, wrong financial recommendations can cause harm and liability. You must assess staleness risk before implementing caching. If stale content can cause harm, you need aggressive invalidation and staleness detection. If stale content is merely suboptimal, you can tolerate longer cache durations.

Staleness detection is hard because you often do not know content has updated until you check. If you are caching retrieved documents and the document is edited, you do not know unless you re-retrieve and compare. Some teams implement background freshness checks: periodically re-fetch cached content, compare to the cached version, and purge if different. This catches updates but adds load. Other teams rely on content publishers to emit update events that trigger invalidation. This requires integration with content management systems.

A hybrid approach is to cache with confidence scoring. When caching a result, you tag it with a confidence score based on content freshness. Recently retrieved documents get high confidence. Documents retrieved hours ago get medium confidence. When a cache hit occurs, you check the confidence score. High confidence hits are served directly. Medium confidence hits are served but trigger a background refresh to check freshness. Low confidence hits are not served; you regenerate instead. This balances cache efficiency with freshness guarantees.

Another risk is over-caching dynamic content. If you cache user-specific information across requests, you risk serving user A's cached data to user B. This is a catastrophic privacy and security failure. You must ensure cache keys include user identifiers or session identifiers for any user-specific content. Global caches must contain only truly global content. A healthcare company in mid-2025 had a caching bug where patient context was cached globally instead of per-patient. For six hours, approximately 80 patients received clinical summaries that included snippets from other patients' records before the bug was detected. This was a HIPAA violation, required disclosure to affected patients, and cost the company 340,000 dollars in remediation and legal fees. The bug was a simple error: using document ID as the cache key instead of document ID plus patient ID.

## Architecting for Cache Efficiency

Effective caching is not added after the fact; it is designed into your system architecture from the start. Cache-friendly architectures separate static and dynamic content, use consistent component boundaries, and design prompts for cache reuse. Cache-hostile architectures intermingle static and dynamic content, use inconsistent prompt structures, and generate unique prompts per request.

The first principle is to front-load static content in prompts. Model provider caching typically caches prefix content: the beginning of the prompt. If your static system prompt is at the start, cacheable. If you put user query first and system prompt after, not cacheable by provider caching. Structure prompts as: system prompt, static examples, static knowledge, dynamic context, user query. This maximizes the cacheable prefix.

The second principle is to use stable component identifiers. If you assemble prompts from components, use stable IDs and versions for each component. Do not generate new IDs per request. Do not include timestamps or request IDs in component keys. Use deterministic keys based on content or explicit version numbers. This ensures the same component has the same cache key across requests.

The third principle is to decouple content updates from deployment. If your system prompt is in your application code and you deploy code to update it, every deployment invalidates caches. If your system prompt is in a configuration store and you update it independently of code deployment, you can update the prompt without redeploying and control cache invalidation explicitly. This also enables A/B testing different prompts without code changes.

The fourth principle is to monitor cache performance continuously. Track cache hit rates, cache sizes, cache eviction rates, and staleness incidents. If hit rates are low, investigate why: are cache keys too specific? Are TTLs too short? If cache sizes are growing unbounded, you need better eviction policies. If staleness incidents occur, you need faster invalidation. Caching is not set-and-forget; it requires ongoing tuning based on production behavior.

A well-architected caching system in 2026 uses provider-native prompt caching for large static prompts, component-level caching for reusable context blocks, semantic caching for frequent similar queries, versioned cache keys to avoid invalidation complexity, and time-based expiration with event-based purging for critical updates. This multi-layer caching strategy routinely achieves 60% to 80% reduction in context processing costs while maintaining correctness through proper invalidation.

The next frontier in context management is handling multimodal context: images, audio, video, and structured data alongside text. Each modality has different token costs, caching characteristics, and compression strategies, requiring new approaches to context budgeting and reuse.
