# 6.6 — Cost of Memory: Token Spend Analysis and Optimization

In March 2025, a productivity software company launched a memory-enhanced AI assistant that personalized responses based on user history and preferences. Within three weeks, their monthly API bill jumped from $47,000 to $284,000—a six-fold increase that caught finance and engineering completely off guard. The culprit was not increased user traffic; active users had grown only 12% in that period. The explosion came from memory retrieval. Every conversation loaded an average of 4,200 tokens of context from past interactions, vectorized summaries, and stored preferences. With 890,000 conversations that month and an average of 3.4 memory retrievals per conversation, they were injecting over 12 billion tokens of memory context into requests—tokens that could have been avoided, compressed, or selectively filtered. By the time leadership recognized the pattern, they had burned through their quarterly infrastructure budget in 23 days.

The root cause was not a technical failure but an economic blind spot. The team understood that memory improved response quality, and they measured retrieval relevance and user satisfaction. What they never measured was the marginal cost per memory token, the ratio of memory tokens to response tokens, or the incremental value delivered per dollar of memory spend. They treated memory as a feature to maximize rather than a resource to optimize. Memory systems are not free. Every retrieved context fragment, every injected conversation summary, every personalization detail burns tokens that you pay for. If you do not measure the cost of memory separately from the cost of generation, you cannot optimize it. If you do not track how memory spend scales with usage, you will be surprised when the bill arrives. This subchapter teaches you how to analyze memory's contribution to token spend, identify optimization opportunities, and build cost-aware memory architectures that balance quality and economics.

## The Hidden Economics of Memory Retrieval

Memory increases token consumption in ways that are not immediately obvious. When a user sends a message, your system does not just process that message—it retrieves relevant memories, formats them into context, injects them into the prompt, and then generates a response. The memory retrieval step adds tokens to every request, and those tokens compound across millions of interactions. A typical memory-enhanced conversation might include 800 tokens of user input, 3,200 tokens of retrieved memory context, and 600 tokens of generated response. The memory tokens represent 71% of the total tokens processed, yet many teams never isolate that cost or question whether all of it is necessary.

The cost structure breaks down into several components. First, there is the cost of embedding user queries to perform vector search, which is usually negligible compared to the other costs. Second, there is the cost of retrieving and formatting memory from storage, which involves database queries and serialization but does not directly consume tokens. Third, and most significant, there is the cost of injecting retrieved memory into the prompt context window. Every token of memory you include in the prompt is billed as an input token. If you retrieve ten memories averaging 320 tokens each, you add 3,200 tokens to every request. Over a million requests, that is 3.2 billion tokens. At GPT-4o pricing as of January 2026—approximately five dollars per million input tokens—that memory context alone costs $16,000. If your system handles ten million requests per month, memory context could cost $160,000 monthly, separate from the cost of generating responses.

The economics worsen when memory grows over time. A new user might have 50 tokens of memory context. After three months of usage, that same user might have 4,000 tokens of memory context if you naively retrieve everything relevant. If you do not implement decay, summarization, or prioritization, the cost per conversation increases linearly with user tenure. Long-term users become exponentially more expensive to serve, and your unit economics degrade as your product succeeds. You are penalized for retention. This is the spread-too-thin anti-pattern applied to cost: you retrieve too much memory because you have not defined what is worth the marginal token spend.

The fourth cost component is processing memory with LLMs to summarize, compress, or reformat it before injection. Some architectures use a smaller, cheaper model to condense retrieved memories into compact summaries that are then injected into the main prompt. This adds a processing step and its own token cost, but if it reduces the memory payload from 3,200 tokens to 800 tokens, the net savings can be substantial. You spend tokens on compression to save tokens on injection. The tradeoff depends on the cost ratio between the compression model and the main model, the compression ratio you achieve, and the frequency of memory retrieval.

## Measuring Memory's Contribution to Total Token Spend

You cannot optimize what you do not measure. The first step in memory cost optimization is isolating how much of your token spend comes from memory context versus user input and generated output. Most LLM API bills aggregate all input tokens and all output tokens without distinguishing their sources. You must instrument your system to tag and track memory tokens separately. When you retrieve memories and inject them into the prompt, log the token count of that injection as a distinct metric. Track it per request, per user, per conversation, and per day. Aggregate it weekly and monthly. Compare memory token spend to total input token spend, and calculate the memory ratio.

A well-instrumented memory system exposes several key metrics. Memory tokens per request tells you the average token overhead of memory retrieval. If this number is 2,400, and your average user input is 600 tokens, memory is adding four times the token cost of the user's actual message. Memory ratio—memory tokens divided by total input tokens—tells you what percentage of your input spend is memory. If your memory ratio is 73%, nearly three-quarters of your input token bill is context you retrieved, not content the user provided. Memory cost per conversation is the dollar cost of memory tokens in a single interaction. If your memory tokens per request is 2,400 and input tokens cost five dollars per million, each conversation costs 1.2 cents in memory alone. Over ten million conversations, that is $120,000.

You should also measure memory cost per user per month. This metric reveals whether long-term users are more expensive to serve. If new users cost 40 cents per month in memory tokens and twelve-month users cost $3.20 per month, your cost is scaling eight-fold with tenure. That scaling is unsustainable. You need decay, summarization, or tiered memory strategies to flatten the curve. Similarly, measure memory cost by feature or memory type. If conversation history costs $80,000 per month and user preferences cost $12,000 per month, you know where to focus optimization efforts. You might discover that 60% of your memory spend comes from retrieving full message histories that could be summarized, or from storing low-value metadata that users never reference.

Cost anomaly detection is critical. Set thresholds for expected memory token spend per user and per conversation, and alert when actuals exceed those thresholds by a meaningful margin. If a single conversation suddenly injects 18,000 tokens of memory when the average is 2,400, investigate immediately. This could indicate a bug in retrieval logic, a user with pathological usage patterns, or a memory system runaway where recursive retrieval is pulling in far more context than intended. One SaaS company discovered that 4% of their users were generating 38% of their memory token spend because those users had conversation histories spanning hundreds of interactions, and the system was retrieving all of it every time. Flagging those outliers allowed the team to implement user-specific memory caps and reduce spend by 22% without impacting the other 96% of users.

## Cost Per Memory Retrieval and Scaling Dynamics

Memory systems incur costs at every retrieval, and those costs scale with the number of retrievals and the size of the retrieved payload. Cost per memory retrieval is the average token cost of a single memory fetch and injection. If you retrieve an average of 2.8 memories per conversation, each memory averages 850 tokens, and input tokens cost five dollars per million, the cost per retrieval is 0.425 cents. That sounds negligible, but over ten million conversations with 2.8 retrievals each, you perform 28 million retrievals at a total cost of $119,000. If you can reduce the average retrieval count from 2.8 to 1.9, you save $38,000 per month.

The number of retrievals per conversation is a design choice, not a fixed requirement. Some systems retrieve memories once at the start of a conversation. Others retrieve dynamically on every turn based on user input. Retrieval-per-turn architectures provide better contextual relevance but multiply token costs. If a conversation averages five turns and you retrieve 2.4 memories per turn, you perform twelve retrievals per conversation instead of 2.4. The token spend is five times higher. You must justify that cost with measurably better outcomes. If retrieval-per-turn increases user satisfaction by 4% but quintuples memory cost, you need to calculate whether the revenue or retention lift justifies the expense.

Retrieval size also scales cost. If your vector search returns the top ten most relevant memories and you inject all ten, you pay for all ten. If nine of those ten provide no marginal value, you are wasting 90% of your memory spend. Reducing retrieval count from ten to three can cut memory cost by 70% if the top three memories are sufficient for quality responses. You discover this through A/B testing. Run one variant with ten retrievals and one with three, and measure response quality, user satisfaction, and task success. If the three-retrieval variant performs within 2% of the ten-retrieval variant, you just reduced memory cost by two-thirds with negligible quality impact.

Token count per memory is the other lever. If each retrieved memory is a raw conversation transcript averaging 1,200 tokens, and you can compress it to a 300-token summary with 95% of the relevant information, you reduce memory cost by 75%. Compression techniques include summarization, entity extraction, and structured distillation. You can use a cheaper model like GPT-4o-mini or Claude 3.5 Haiku to compress memories before storing them, paying a small upfront processing cost to save much larger retrieval costs over the memory's lifetime. If a memory is retrieved fifty times over its lifespan, compressing it from 1,200 tokens to 300 tokens saves 45,000 tokens of injection cost—$0.225 at current pricing. The compression itself might cost $0.01. The return on investment is 22 to 1.

Scaling dynamics become critical as your user base grows. If your memory token spend is $50,000 per month with 100,000 active users, you might assume it will reach $500,000 per month at one million users. But if memory grows with user tenure, the scaling is not linear. Users who have been active for six months might carry three times the memory load of new users. As your user cohort ages, average memory per user increases, and cost per user increases. If 30% of your users are long-term, and they cost four times as much to serve, your blended cost per user is higher than the new-user baseline. Projecting future costs requires modeling both user growth and memory growth per user. You need cohort-based cost analysis: what does a user cost in month one, month three, month six, month twelve? If cost in month twelve is unaffordable, you need architectural changes now, not later.

## Optimization Strategies: Compress, Limit, and Substitute

The most direct optimization is compression. Before injecting memories into prompts, compress them to their essential information. Raw conversation logs are verbose and redundant. A ten-message exchange might contain 3,000 tokens, but the key facts—user preferences, decisions made, action items—might fit in 400 tokens. Use a lightweight LLM to extract and summarize. Prompt it with a directive like: "Extract the key facts, preferences, and decisions from this conversation. Omit greetings, repetition, and filler. Output a compact summary under 300 tokens." Store the compressed version and inject that instead of the raw transcript. You pay the compression cost once; you save the injection cost on every retrieval.

Structured extraction is another compression technique. Instead of storing free-form conversation text, extract entities, preferences, and facts into structured records. If a user says "I prefer morning meetings and dislike phone calls," store that as two preference records: "meeting time: morning" and "communication channel: avoid phone." When you retrieve, inject a formatted list: "User preferences: meetings in morning, avoid phone calls." This might be 12 tokens instead of 80 tokens from the original conversation context. Structured memory is more compact, more reliable, and cheaper to inject. The tradeoff is that extraction requires upfront processing and may lose nuance. Balance fidelity with cost.

Limiting retrieval count is the second major lever. If you currently retrieve the top ten memories, test whether the top five, three, or even one provides equivalent quality. Run controlled experiments. Measure task success, user satisfaction, response relevance, and error rates. If dropping from ten to five retrievals causes no measurable degradation, you just halved memory cost. If dropping from five to three causes a 3% drop in satisfaction but saves 40% of memory spend, calculate the dollar value of that 3% satisfaction loss. If retaining those users is worth more than the cost savings, keep five retrievals. If not, go to three. Make explicit tradeoffs based on data, not assumptions.

You can also implement tiered retrieval strategies. For simple queries, retrieve fewer memories. For complex or high-stakes queries, retrieve more. Use a classifier or heuristic to route requests. If a user asks "What is the weather today?" you probably need zero memories. If they ask "Summarize what we decided last week about the vendor selection," you need deep retrieval. Simple routing logic—query length, intent classification, or keyword matching—can cut average retrieval count by 30% by skipping memory retrieval for queries that do not need it.

Substituting cheaper models for memory processing is the third lever. If you use GPT-4o to compress or summarize memories before storage, consider switching to GPT-4o-mini or Claude 3.5 Haiku. As of January 2026, GPT-4o-mini input tokens cost approximately 15 cents per million compared to GPT-4o's five dollars per million—a 97% cost reduction. If the cheaper model produces 90% of the compression quality, the cost savings vastly outweigh the marginal quality loss. You can even chain models: use the cheap model for routine compression and the expensive model for complex or ambiguous memories. This hybrid approach optimizes cost without sacrificing quality on edge cases.

Another substitution strategy is using retrieval models instead of LLMs for memory ranking and filtering. Instead of using an LLM to decide which memories are relevant, use a fine-tuned embedding model or a reranker to score and filter memories before injection. Embedding and reranking costs are orders of magnitude lower than LLM inference. If you can reduce the candidate memory set from 50 to 5 using embeddings, you avoid injecting 45 irrelevant memories and save their token costs. The LLM only sees the top five, which are likely the only ones that matter.

## The Cost-Quality Tradeoff: More Memory, Better Responses, Higher Bills

Memory improves response quality. That is why you use it. More memory context gives the model more information to generate relevant, personalized, and accurate responses. But more memory context also costs more. Every additional memory you inject adds tokens, and tokens cost money. The relationship between memory quantity and response quality is not linear. The first memory you inject might improve response quality by 30%. The second might add another 15%. The fifth might add 2%. The tenth might add 0.5%. This is diminishing returns. At some point, adding more memory costs more than the incremental quality gain is worth.

Your job is to find the inflection point. Plot memory retrieval count on the X-axis and response quality on the Y-axis. Measure quality using task success rate, user satisfaction scores, or domain-specific metrics like factual accuracy or policy compliance. Also plot cost on a second Y-axis. You will see quality increase steeply at first, then flatten. Cost increases linearly. The optimal retrieval count is where the marginal quality gain no longer justifies the marginal cost increase. If three retrievals give you 85% of maximum quality at 30% of maximum cost, and ten retrievals give you 98% of maximum quality at 100% of maximum cost, the economically rational choice is three retrievals unless that extra 13% quality drives enough revenue or retention to justify the 70% cost increase.

This tradeoff is not one-size-fits-all. Different use cases have different quality requirements and different cost tolerances. A customer-facing chatbot that handles complaints might justify high memory spend because poor responses damage retention and brand reputation. An internal tool that drafts meeting summaries might tolerate lower memory context because the cost of suboptimal output is small. You should tier your memory budget by use case. Allocate more memory tokens to high-value interactions and fewer to low-value interactions. This is ruthless prioritization applied to cost.

You can also make the tradeoff dynamic. For users on premium plans, retrieve more memory and deliver higher quality. For free-tier users, retrieve less memory and deliver acceptable but not optimal quality. This aligns cost with revenue. You spend more to serve users who pay more, and you constrain spend on users who pay nothing. Some companies implement memory budgets per user: each user gets a monthly memory token allowance. Once they exhaust it, the system reduces retrieval count or switches to cheaper memory strategies. This prevents a small number of heavy users from driving runaway costs.

The cost-quality tradeoff also applies to memory freshness. Recent memories are usually more relevant than old memories, but maintaining recency requires more frequent retrievals and updates. If you retrieve memories from the last 30 days, you might inject 1,800 tokens. If you retrieve from the last 90 days, you might inject 5,400 tokens. The older memories might provide minimal marginal value. Test whether shortening the recency window from 90 days to 30 days impacts quality. If not, you just cut memory cost by 67%.

## Cost Anomaly Detection: Catching Spend Spikes Before They Escalate

Memory costs can spike suddenly due to bugs, configuration changes, or unexpected user behavior. A single code deployment that accidentally disables memory filtering can double your token spend overnight. A misconfigured retrieval parameter that changes top-k from 3 to 30 can increase memory cost tenfold. A user who discovers they can trigger unlimited memory retrievals by rephrasing questions can generate thousands of dollars in costs in a single session. If you do not detect these anomalies quickly, they can burn through your budget before anyone notices.

Cost anomaly detection requires real-time monitoring and alerting. Set baseline thresholds for expected memory token spend per hour, per day, and per user. Track actual spend continuously and alert when actuals exceed baselines by a defined margin—say, 50% above the seven-day moving average. If your typical hourly memory spend is 80 million tokens and you suddenly hit 160 million tokens in an hour, trigger an alert. Investigate immediately. Check recent deployments, configuration changes, and user activity. Identify the root cause and mitigate it before the next billing cycle.

User-level anomaly detection is equally important. If a single user generates 10,000 times the average memory token spend, that user is either abusing the system or triggering a bug. In March 2025, a healthcare AI assistant company discovered that one user account was generating 40% of their daily memory token spend. Investigation revealed that the user had written a script to automate question generation, and each question triggered full memory retrieval. The system was designed for interactive use, not bulk automation. The company implemented rate limiting and per-user memory budgets to prevent recurrence. They caught the anomaly within 48 hours, but it still cost them $11,000.

Feature-level anomaly detection tracks memory spend by memory type or retrieval pathway. If conversation history retrieval suddenly costs three times the baseline while preference retrieval remains stable, the issue is specific to conversation history logic. Narrow your investigation to that subsystem. If memory spend spikes only for users in a particular region or cohort, the issue might be localized to a feature rollout or experiment. Segmented monitoring allows faster root cause analysis and more targeted fixes.

Anomaly detection should also monitor memory growth rates. If your total stored memory is growing 20% per month and suddenly jumps to 60% per month, investigate why. This could indicate a bug that is duplicating memories, a feature that is generating excessive memory writes, or a viral user behavior that is creating more memory than expected. Memory growth drives future retrieval costs. If memory grows faster than users, your cost per user will increase over time. Detecting and addressing growth anomalies early prevents long-term cost escalation.

## Budgeting for Memory at Scale: Projecting Costs as You Grow

Memory costs scale with users, with usage, and with time. A memory system that costs $10,000 per month at 50,000 users might cost $200,000 per month at one million users, or it might cost $500,000 per month if long-term users carry more memory. Accurate cost projection requires modeling all three dimensions: user growth, usage intensity, and memory accumulation per user. You cannot extrapolate linearly from current spend. You must account for how memory compounds over time.

Start by calculating current cost per user per month. If your total memory token spend is $30,000 and you have 100,000 active users, your blended cost per user is 30 cents. But break this down by user cohort. New users might cost 10 cents per month. Users active for three months might cost 25 cents. Users active for twelve months might cost 80 cents. The blended average masks the underlying growth curve. Model this curve explicitly. If your user base ages—if the percentage of long-term users increases—your blended cost per user will rise even if total user count stays flat.

Next, project user growth. If you expect to grow from 100,000 users to 500,000 users over the next year, and your blended cost per user is 30 cents, naive extrapolation suggests memory cost will grow from $30,000 per month to $150,000 per month. But if 40% of your users will be long-term users by year-end, and long-term users cost 80 cents per month, your actual cost will be $220,000 per month. The difference between $150,000 and $220,000 is $70,000 per month, or $840,000 annually. Missing this in your budget leads to unexpected shortfalls and emergency cost cuts.

Usage intensity also scales cost. If average messages per user increases from 15 per month to 25 per month, and each message triggers memory retrieval, memory token spend increases 67% even if user count stays flat. Project usage trends based on historical data and product changes. If you are launching features that encourage more frequent interaction, factor the increased retrieval frequency into your cost model. If you are adding memory to new parts of the product, estimate the additional retrieval volume those features will generate.

Scenario planning helps you prepare for uncertainty. Build three cost models: conservative, expected, and aggressive. Conservative assumes slow user growth, flat usage, and effective memory optimization. Aggressive assumes fast user growth, increasing usage, and memory growth per user. Expected is your best guess. Calculate memory costs under all three scenarios. If even the conservative scenario exceeds your budget, you need architectural changes now. If the aggressive scenario is unaffordable, build contingency plans: memory caps, tiered retrieval, or compression strategies you can deploy quickly if growth accelerates.

Budgeting should also include memory infrastructure costs beyond tokens. Vector databases, storage, and retrieval infrastructure have their own costs. Pinecone, Weaviate, and Qdrant charge for storage, queries, and compute. As your memory store grows from one million vectors to 100 million vectors, storage and query costs scale. Factor these into your total memory budget. Some companies find that vector database costs exceed LLM token costs for memory retrieval. If your vector DB costs $40,000 per month and your memory token costs are $30,000 per month, your total memory expense is $70,000, not $30,000. Optimize both.

## Cost Comparison Across Memory Architectures

Different memory architectures have radically different cost profiles. A naive architecture that retrieves full conversation histories on every turn might cost ten times more than a summarized memory architecture. A vector database with high-dimensional embeddings and large-scale similarity search might cost five times more than a structured relational database storing key-value preferences. Understanding the cost implications of architectural choices allows you to design memory systems that fit your budget.

Vector database architectures incur costs for embedding generation, vector storage, and similarity search. Embedding generation costs depend on the embedding model and the volume of text you embed. OpenAI's text-embedding-3-large costs approximately 13 cents per million tokens as of January 2026. If you embed ten million tokens of memory content per month, that costs $1.30 in embedding fees—negligible. But vector storage and query costs scale with vector count and dimensionality. Storing 50 million 1536-dimension vectors in a managed vector database can cost $1,000 to $5,000 per month depending on the provider and query volume. Querying 50 million vectors for top-k similarity search adds per-query costs that compound with request volume.

Structured database architectures store memories as relational records—user preferences, facts, entities—and retrieve them via indexed queries. Storage costs are lower because you are not storing high-dimensional vectors, just text and metadata. Query costs are lower because indexed SQL lookups are cheaper than vector similarity search. A PostgreSQL database storing 50 million memory records with proper indexing might cost $200 per month in managed database fees and handle millions of queries per day without additional per-query charges. If your memory needs fit structured schemas—preferences, settings, facts—this architecture is far cheaper than vector search.

Hybrid architectures combine vector search for unstructured memory and relational databases for structured memory. You use vector search to retrieve relevant conversation snippets or documents, and you use relational queries to retrieve user preferences or settings. This allows you to optimize each memory type for its ideal storage and retrieval mechanism. Structured memories go in Postgres; unstructured memories go in a vector DB. You pay for both systems, but you use each only where it provides value. Total cost is often lower than using vector search for everything.

Another cost consideration is serverless versus dedicated infrastructure. Managed vector databases like Pinecone charge per query and per storage. Dedicated infrastructure like a self-hosted Qdrant instance has fixed costs—server rental, maintenance—but no per-query fees. If your query volume is high, dedicated infrastructure can be cheaper. If your query volume is low or unpredictable, serverless is cheaper because you pay only for what you use. Model both and choose based on your scale.

Caching is a cost-saving architecture that reduces redundant retrievals. If the same memory is retrieved multiple times in a short period, cache it in-memory and reuse it without querying the database or re-embedding. This cuts database query costs and embedding costs. A Redis cache layer can reduce vector database queries by 40% and save thousands per month. The cache itself costs money—Redis hosting fees—but less than the repeated queries it prevents.

## Projecting Memory Costs as User Base Grows

Projecting memory costs at scale requires modeling user cohorts, memory growth curves, and usage patterns. Start with cohort analysis. Track how much memory users accumulate over time. New users might have 200 tokens of memory. After one month, they might have 800 tokens. After six months, 3,000 tokens. After twelve months, 6,000 tokens. Plot this curve. Fit a regression model—linear, logarithmic, or exponential—to predict memory per user at any tenure. If memory growth is linear at 500 tokens per month, a user active for 24 months will carry 12,000 tokens. If memory growth is logarithmic, growth will slow over time and plateau at 8,000 tokens. The shape of this curve determines long-term cost scaling.

Next, project user cohort distribution. If you acquire 10,000 new users per month and retain 70% month-over-month, you can calculate how many users will be in each cohort at any future date. After twelve months, you will have 120,000 total users acquired, but only a fraction will still be active, and they will be distributed across twelve cohorts. Multiply each cohort's user count by that cohort's average memory token load. Sum across cohorts to get total memory tokens stored. Multiply by the average retrieval rate—say, 2.5 retrievals per user per month—to get total memory tokens retrieved per month. Multiply by token cost to get projected spend.

For example, assume you have 10,000 users in month one, each with 200 tokens, retrieving 2.5 times per month. That is 5 million tokens retrieved, costing $25 at five dollars per million tokens. In month twelve, you have 80,000 active users across twelve cohorts with an average memory load of 2,800 tokens, retrieving 2.5 times per month. That is 560 million tokens retrieved, costing $2,800. Your memory cost scaled from $25 to $2,800 in one year—a 112x increase—while your user count scaled only 8x. This is the compounding effect of memory growth. If you do not account for it, you will massively underestimate future costs.

Sensitivity analysis helps you understand which variables drive cost most. Vary user growth rate, retention rate, memory growth per user, and retrieval frequency independently, and observe the impact on projected costs. If a 10% increase in retention increases costs by 30%, retention is a high-leverage variable. If a 20% reduction in memory growth per user reduces costs by 40%, memory compression is a high-leverage optimization. Focus your efforts on the levers with the greatest cost impact.

You should also model the impact of optimization initiatives. If you implement memory summarization that reduces memory per user by 60%, recalculate your projections. If you implement tiered retrieval that cuts average retrievals from 2.5 to 1.7, recalculate again. Quantify the cost savings of each initiative and prioritize the ones with the highest return. Some optimizations require engineering effort but save millions annually. Make those investments early, before costs spiral.

## Real-World Cost Optimization Case Studies

In June 2025, an HR tech company offering an AI-powered employee assistant was spending $91,000 per month on memory token costs. Every interaction retrieved the employee's conversation history, preferences, organizational context, and policy summaries—an average of 4,700 tokens per request. With 1.2 million requests per month, they were injecting 5.64 billion memory tokens. The engineering team ran an experiment: they compressed conversation history into 400-token summaries and limited retrieval to the three most recent conversations instead of all conversations from the last 90 days. They also stopped retrieving organizational context for simple queries like policy lookups. The compressed architecture reduced memory tokens per request from 4,700 to 1,300—a 72% reduction. Monthly memory costs dropped from $91,000 to $25,000. User satisfaction scores dropped 1.8%, which the team deemed acceptable given the $66,000 monthly savings—$792,000 annually.

Another case: a legal research platform in September 2025 was spending $140,000 per month on memory for case history retrieval. They stored full case documents as embeddings and retrieved the top 15 most relevant cases per query. Each case averaged 2,200 tokens. Memory tokens per query averaged 33,000 tokens. The team implemented a two-stage retrieval strategy. First, they used embeddings to retrieve the top 15 cases. Then, they used a cheap reranker model to score those 15 and select the top 5. Only the top 5 were injected into the prompt—11,000 tokens instead of 33,000. The reranker added $1,200 per month in processing costs but reduced memory token costs from $140,000 to $47,000—a net savings of $91,800 per month. Response quality remained within 2% of the original system because the top 5 cases contained nearly all the relevant information.

A third example: a personal finance app in October 2025 noticed that memory costs were scaling faster than user growth. Investigation revealed that 8% of users—those who had been active for over a year—were generating 44% of memory costs. These users had extensive transaction histories and preference data. The team implemented a tiered memory strategy: users active for less than three months got full memory retrieval. Users active for 3-12 months got summarized memory. Users active for over a year got highly compressed memory with only critical preferences and recent transactions. This reduced memory tokens for long-term users by 68% while preserving full fidelity for new users still forming habits. Memory costs dropped 31% with no measurable impact on retention or satisfaction.

These cases illustrate the same principle: memory costs can be optimized dramatically without sacrificing quality if you measure, test, and iterate. The teams that succeed are the ones that treat memory as a constrained resource, not an unlimited one. They prioritize ruthlessly, compress aggressively, and validate that every token of memory they inject delivers value. The teams that fail are the ones that assume more memory is always better and discover too late that the bill is unsustainable.

You measure memory's token contribution, you optimize retrieval count and payload size, you balance cost against quality, you detect anomalies before they escalate, and you project costs as you scale. Memory is not free. Every token you inject costs money. If you do not manage that cost deliberately, it will manage you—by blowing your budget and forcing reactive cuts that degrade user experience. Build cost discipline into your memory architecture from day one, and you will scale sustainably. Ignore it, and you will spend your time explaining to finance why this month's bill is six times higher than forecast.

The next subchapter examines how to measure and reduce false recall confidence—when your memory system confidently retrieves or generates information that turns out to be wrong.
