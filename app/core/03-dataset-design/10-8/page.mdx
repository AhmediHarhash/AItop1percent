# 10.8 â€” Cross-Team Dataset Sharing Agreements

In mid-2025, a healthcare technology company ran into a legal problem. Team A had built a dataset of patient question transcripts for training a symptom triage model. Team B, working on a different product, discovered the dataset in the internal catalog and started using it to train a medication adherence model. Three months later, during a compliance audit, the auditor asked a simple question: what was the original consent language for these transcripts? Team A pulled the consent form. It specified use for symptom assessment only. Team B's medication adherence use case was not covered. The company had to shut down Team B's model, retrain it on different data, and delay the product launch by four months. The cost was not just the delay, it was the regulatory risk. Using data outside the scope of consent is a HIPAA violation.

The root cause was not malice or negligence. It was lack of a sharing agreement. Team B saw a dataset in the catalog, assumed it was available for any internal use, and integrated it. No one told them about the consent restrictions. No one asked them to document their use case. No one reviewed whether the new use case was compatible with the original purpose. The dataset flowed from one team to another with no governance, no tracking, and no constraints. This is the free-for-all anti-pattern, and it breaks the moment you operate in a regulated domain.

## The Components of a Data Sharing Agreement

A data sharing agreement is a formal contract between the team that owns a dataset and the team that wants to use it. The agreement defines scope, purpose, access controls, modification rights, attribution, service level expectations, and termination conditions. It is not a legal document in most cases, though legal may review it for high-risk datasets. It is an operational document that makes expectations explicit.

Scope defines what data is being shared. Is it the entire dataset or a subset? Is it the raw data, the processed data, or both? Is it the current version only, or does the agreement cover future versions? Scope prevents ambiguity. If Team A shares a customer conversation dataset but only the anonymized version, and Team B assumes they are getting the full version with metadata, the integration will fail. Scope must be written down and agreed upon before data moves.

Purpose defines what Team B is allowed to do with the data. Purpose is not "anything related to AI." Purpose is specific: "training a sentiment classification model for product review analysis" or "evaluating the performance of a new summarization model in a non-production research environment." Purpose limitation is a privacy principle and a legal requirement in many jurisdictions. Data collected for one purpose cannot be used for another purpose without consent or legal basis. The sharing agreement must state the purpose explicitly and restrict use to that purpose only.

Access controls define who on Team B can access the data. Is it the entire team, or only specific individuals? Do they need to complete data handling training first? Do they need to access the data through a specific environment with logging enabled? Access controls are technical and procedural. The agreement specifies the controls, and the infrastructure enforces them. If the agreement says only three named individuals can access the data, the storage system must enforce that rule through role-based access controls.

Modification rights define whether Team B can change the data. Can they clean it further, re-annotate it, augment it, or filter it? If they do, who owns the resulting derivative dataset? Modification rights are critical for dataset lineage. If Team B modifies the data and then shares it with Team C, Team A needs to know that the downstream dataset is a derivative, not the original. Modification rights must be explicit: no modifications allowed, modifications allowed but derivatives must be tracked, or modifications allowed and ownership transfers to Team B.

Attribution specifies how Team B must credit Team A when using the data. Attribution is not just courtesy, it is accountability. If a model trained on Team A's data causes a production incident, the post-incident review needs to trace the data lineage back to the source. Attribution requirements might include citing the dataset name and version in model documentation, listing Team A as a contributor in internal reports, or notifying Team A when the data is used in a production model. Attribution creates the paper trail that governance requires.

## Service Level Agreements for Shared Datasets

When Team B depends on Team A's dataset for a production model, the dependency creates an obligation. Team A is now responsible not just to themselves but to Team B. If Team A updates the dataset and introduces a quality regression, Team B's model breaks. If Team A stops maintaining the dataset, Team B's model becomes unsupported. Service level agreements make these obligations explicit.

Freshness guarantees define how often the dataset will be updated. If the dataset is updated weekly, the SLA might specify that updates are published every Monday by 9 AM. If the dataset is static, the SLA states that explicitly. Freshness guarantees let Team B plan their retraining schedules. A model that retrains monthly on a dataset that updates weekly has a predictable cadence. A model that retrains weekly on a dataset that updates unpredictably does not.

Quality guarantees define the minimum quality bar for the dataset. Quality might be measured as inter-annotator agreement, null rate, schema compliance, or domain coverage. The SLA specifies the threshold: "Inter-annotator agreement will remain above 0.90" or "Null rate for required fields will not exceed 2%." If quality drops below the threshold, Team A must notify Team B and either fix the issue or allow Team B to pause integration until quality is restored. Quality guarantees prevent silent degradation.

Availability guarantees define uptime for dataset access. If the dataset is served through an API or database, the SLA might specify 99.5% uptime during business hours. If the dataset is served as static files in object storage, availability might be 99.9%. Availability guarantees matter for production models that query the dataset in real time. A model that cannot access its reference data cannot serve predictions.

Breaking change policies define how Team A will handle changes that break downstream consumers. A breaking change might be a schema modification, a re-annotation that changes label definitions, or a filtering change that removes a substantial portion of the data. The SLA specifies notification requirements: Team A must notify all consumers at least two weeks before deploying a breaking change. Notification includes a migration guide and a timeline. Team B has the option to migrate immediately, request an extension, or fork the dataset at the pre-change version.

Support commitments define how Team A will respond to issues reported by Team B. Support might be best-effort, where Team A responds when they have time, or it might be guaranteed, where Team A commits to responding within one business day. Support commitments scale with criticality. A dataset used in a production model serving millions of users requires guaranteed support. A dataset used in a research experiment can rely on best-effort support.

## The Free-For-All Anti-Pattern and Its Consequences

The free-for-all anti-pattern happens when datasets flow between teams with no tracking, no agreements, and no constraints. A dataset gets published to a shared S3 bucket, and any team can download it and use it for anything. No one knows who is using it. No one knows what they are using it for. No one knows if they modified it. No one is accountable when something goes wrong.

The first consequence is compliance risk. If a dataset contains personally identifiable information and was collected under a specific consent or legal basis, using it for a different purpose may violate privacy law. The organization cannot assess compliance risk if it does not know who is using the data or for what purpose. Auditors ask for a list of all uses of sensitive data. In a free-for-all environment, the answer is "we don't know." That answer does not satisfy regulators.

The second consequence is lack of accountability. If Team B's model fails and the failure is traced to bad data, who is responsible? If no sharing agreement exists, Team B will blame Team A for providing bad data, and Team A will blame Team B for using it incorrectly. Without a written agreement, there is no ground truth. Finger-pointing replaces problem-solving. The incident review ends with "we need better communication," which solves nothing.

The third consequence is operational chaos. If Team A decides to deprecate the dataset or make a breaking change, they have no way to notify downstream consumers because they do not know who the consumers are. They make the change, and three weeks later Team B's model breaks in production. Team B files an urgent ticket asking why the data changed. Team A says "we deprecated that dataset a month ago, you should have migrated." Team B says "we never got a notification." Neither side is wrong. The system failed.

The fourth consequence is wasted work. If Team B modifies Team A's dataset to fix a quality issue, and Team C independently modifies the same dataset to fix the same issue, both teams waste time. If Team A had tracked consumers and published a fixed version, both teams could have used the fix. Lack of visibility into dataset usage prevents coordination and leads to duplicate work.

## Purpose Limitation and Consent Boundaries

Purpose limitation is the principle that data collected for one purpose should not be used for another purpose without additional consent or legal basis. It is a core tenet of GDPR, HIPAA, and most modern privacy laws. Purpose limitation applies to datasets just as it applies to raw data. If a dataset was built for fraud detection, using it for credit scoring is a purpose change. If a dataset was built for symptom triage, using it for medication adherence is a purpose change. Purpose changes require review.

The sharing agreement must document the original purpose and the new purpose. If the new purpose is compatible, the agreement states that and proceeds. If the new purpose is incompatible, the agreement is blocked until legal or privacy review is complete. Compatibility is a judgment call. Training a model to classify customer support tickets by urgency is compatible with the original purpose of improving customer support. Training a model to predict which customers are likely to cancel is not compatible, because it changes the purpose from support to retention.

Consent boundaries define the limits of use based on what users agreed to. If a dataset was collected with a consent form that said "your data will be used to improve our customer support chatbot," using it to train a sales lead scoring model violates the consent boundary. The consent form is the contract. Violating it is not just unethical, it is illegal in many jurisdictions. The sharing agreement must reference the consent language and verify that the new use case falls within the consent scope.

Anonymization does not eliminate purpose limitation. Even if a dataset is fully anonymized and contains no personally identifiable information, it may still carry purpose restrictions based on the original consent or terms of service. A dataset of anonymized medical records collected for research cannot be repurposed for commercial product development if the consent form specified research use only. Anonymization removes identity, but it does not remove purpose constraints.

## Modification Rights and Derivative Dataset Ownership

When Team B receives a dataset from Team A and modifies it, a derivative dataset is created. The derivative may be a cleaned version, a re-annotated version, a filtered subset, or an augmented version. Ownership of the derivative must be defined in the sharing agreement. Three models are common: Team A retains ownership, Team B gains ownership, or joint ownership is established.

Under the Team A retains ownership model, Team B can modify the dataset for their own use, but they cannot share the derivative with other teams or publish it externally. Team A remains the sole owner and controller. This model is used when the original dataset is highly sensitive or when Team A wants to maintain tight control over all derivatives. The downside is that it limits reuse. If Team B improves the dataset, other teams cannot benefit from the improvement unless they go through Team A.

Under the Team B gains ownership model, Team B can modify the dataset and treat the derivative as their own. They can share it with other teams, publish it internally, and make decisions about its use without consulting Team A. This model encourages innovation and reuse. The downside is that Team A loses visibility into how their data is being used and modified. If the derivative is low quality, it may reflect poorly on Team A even though they did not create it.

Under the joint ownership model, both teams own the derivative and must agree on how it is used and shared. This model is used when both teams contribute significant value. Team A provides the original data, and Team B provides extensive cleaning, re-annotation, or augmentation. Joint ownership requires ongoing coordination, which adds overhead but ensures both teams have a say in how the derivative is managed.

The sharing agreement must specify which model applies. If it does not, conflicts arise. Team B assumes they own the derivative and shares it with Team C. Team A discovers this and objects, saying the derivative includes their proprietary data. Team C is caught in the middle. The conflict could have been avoided with a clear ownership clause in the original sharing agreement.

## Breaking Changes and Migration Support

Breaking changes are inevitable. Schemas evolve, label definitions are refined, source data changes, and quality standards increase. When Team A makes a breaking change to a shared dataset, Team B's integration may break. The sharing agreement must define how breaking changes are handled.

Notification is mandatory. Team A must notify all consumers at least two weeks before deploying a breaking change. Notification includes a description of the change, the reason for it, the deployment date, and migration guidance. Migration guidance explains how to adapt to the change: which fields are renamed, which labels are redefined, which filters are added. Without migration guidance, Team B is left to reverse-engineer the change by comparing old and new versions.

Versioning supports migration. Team A should publish the breaking change as a new major version and keep the old version available for a transition period. Team B can migrate to the new version on their own schedule, as long as the migration is complete before the old version is deprecated. The transition period is typically four to twelve weeks, depending on the criticality of the downstream models. A dataset used in production models serving millions of users needs a longer transition period than a dataset used in research experiments.

Migration support may include tooling. If the schema change requires a data transformation, Team A can provide a migration script that converts Team B's existing integration code or data pipeline to work with the new schema. Migration tooling reduces the burden on consumers and increases the likelihood that they migrate successfully. If migration is too hard, consumers will stay on the old version indefinitely, forcing Team A to maintain both versions forever.

Rollback plans are necessary when breaking changes cause unexpected problems. If Team A deploys a breaking change and Team B's production model breaks despite following the migration guidance, Team A must be prepared to roll back to the previous version while the issue is investigated. Rollback plans are part of the SLA. If rollback is not possible, the breaking change should be tested with a subset of consumers in a canary deployment before being rolled out to all consumers.

## The Organizational Politics of Data Sharing

Data sharing is not just technical, it is political. Teams guard their data because data is power. A team that controls a valuable dataset controls which projects get access to it and which do not. Access is currency. Denying access to a competitor team can slow their project down and give your team a strategic advantage. This dynamic is toxic, but it is real.

The dataset marketplace and sharing agreements formalize access and remove some of the political friction. If the policy says that any team with a legitimate business need can request access, and the decision is based on documented criteria rather than personal relationships, the process becomes fair. Transparency reduces politics. When access requests and approvals are logged and visible, arbitrary denials are harder to justify.

Incentives must align with sharing. If teams are rewarded for hoarding data and penalized for sharing it, sharing will not happen. If teams are rewarded for publishing high-quality datasets that other teams use, sharing becomes desirable. Track dataset reuse as a positive metric. Celebrate teams whose datasets are used by many other teams. Make data sharing a part of performance reviews and team goals.

Executive support is necessary when political conflicts arise. If Team A refuses to share a dataset with Team B for reasons that are clearly political rather than legitimate, leadership must intervene. The decision cannot be left to the teams to resolve on their own, because the power imbalance is too great. Leadership must enforce the principle that data created with company resources is a company asset, not a team asset, and sharing is the default unless there is a documented reason to restrict it.

## Building a Culture of Dataset Sharing Without Losing Control

The goal is not unrestricted sharing. The goal is governed sharing. Governed sharing means that datasets are shared widely, but always with agreements, tracking, and accountability. Control is maintained through documentation and process, not through gatekeeping.

The sharing agreement is the control mechanism. It specifies what data is shared, for what purpose, under what conditions, and with what obligations. It creates a record of the transaction. If something goes wrong, the agreement is the reference point for determining what was supposed to happen and what actually happened. The agreement is not a barrier to sharing, it is the infrastructure that makes sharing safe.

Publication to the dataset marketplace is the forcing function. If sharing requires publication, and publication requires metadata, ownership, and access policies, then sharing becomes a formal process. Ad hoc sharing through Slack messages or email attachments is discouraged because it bypasses the governance layer. Formal sharing through the marketplace is encouraged because it creates visibility and accountability.

Usage tracking provides feedback. The marketplace tracks which teams are using which datasets and for what purpose. This data is visible to dataset owners and to governance teams. If a dataset is being used in an unexpected way, the owner can reach out and verify that the use case is legitimate. If usage patterns change suddenly, the owner can investigate. Tracking does not prevent misuse, but it makes misuse visible.

Sharing agreements are not one-time documents. They are reviewed and renewed periodically, typically every six or twelve months. The review asks: Is this dataset still being used? Is the use case still the same? Are the access controls still appropriate? Are there any issues or concerns? Periodic review prevents sharing agreements from becoming stale and ensures that the relationship between teams remains aligned.

When Team A builds a dataset and Team B wants to use it, the sharing agreement is the mechanism that makes it safe, legal, and sustainable. Without it, sharing is a gamble. With it, sharing becomes a managed process that scales across the organization. But even with agreements in place, things go wrong. Datasets get contaminated, corrupted, or leaked. When that happens, the organization needs an incident management process designed specifically for dataset failures.
