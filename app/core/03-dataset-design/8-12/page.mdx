# 8.12 â€” Regulatory Requirements: EU AI Act Dataset Obligations

What happens when your dataset does not meet the EU AI Act's data governance requirements? Your product cannot legally operate in Europe. Not "operates with restrictions." Not "operates pending compliance review." Cannot operate. The Act became enforceable in 2024 for prohibited systems, in 2025 for general-purpose AI models, and in August 2026 for high-risk systems. If you are deploying a high-risk AI system in the European Union right now, you must comply with Article 10's data and data governance requirements. Compliance is not aspirational. It is the precondition for market access.

The EU AI Act represents the most comprehensive AI regulation in the world. It categorizes AI systems by risk level and imposes obligations accordingly. Prohibited systems cannot be deployed at all. High-risk systems face strict requirements including data governance, transparency, human oversight, and accuracy obligations. General-purpose AI models face transparency and documentation requirements. Limited-risk systems face minimal requirements. For most commercial AI products, the relevant category is high-risk, and the relevant article for dataset engineering is Article 10.

Understanding Article 10 requires understanding what the Act considers high-risk. High-risk systems include those used in biometrics, critical infrastructure, education and vocational training, employment, essential services, law enforcement, migration and border control, and administration of justice. If your AI system screens resumes, scores creditworthiness, determines access to benefits, predicts recidivism, or identifies individuals in public spaces, it is high-risk. The obligations apply regardless of whether you are a European company. If you operate in Europe, you comply.

## EU AI Act Article 10: Data and Data Governance Requirements

Article 10 establishes that training, validation, and test datasets for high-risk AI systems must be subject to appropriate data governance and management practices. The datasets must be relevant, representative, free of errors, and complete. They must also meet quality criteria with regard to the purpose for which they are used. In practice, this means you must demonstrate that your data is fit for purpose and that you have processes to ensure ongoing quality.

Relevance means your data reflects the actual use case. If you train a hiring screener on resumes from a specific industry and deploy it across all industries, your training data is not relevant to the deployment context. If you train a fraud detection system on transaction data from one country and deploy it in another country with different fraud patterns, your data is not relevant. The Act requires that the data used to build the system aligns with the domain, context, and population where the system will operate.

Representativeness means your data reflects the diversity of the population the system will encounter. If you train on data from urban areas and deploy in rural areas, your data is not representative. If you train on data from younger demographics and deploy across all ages, your data is not representative. The Act does not define a specific threshold for representativeness, but it requires that you assess it and document where representation fails. You must be able to explain which groups are underrepresented and what impact that has on system performance.

Freedom from errors means your data is labeled correctly and consistently. If your training data contains mislabeled examples, annotation errors, or inconsistencies in how categories are defined, it does not meet the standard. The Act requires that you implement quality assurance processes for data labeling and that you audit your data for errors. If you outsource annotation, you remain responsible for quality. You cannot delegate liability to your annotation vendor.

Completeness means your data covers the range of scenarios the system will encounter. If your training data includes only straightforward cases and your system will face edge cases in production, your data is not complete. If your data excludes certain languages, dialects, or contexts that users will present, it is not complete. Completeness does not mean exhaustive. It means sufficient to support reliable performance across the deployment context.

The Act also requires that datasets are examined for possible biases that may affect the health and safety of persons or lead to discrimination. You must implement measures to detect, prevent, and mitigate these biases. This is not optional. This is not "bias testing if feasible." This is a mandatory requirement. If you deploy a high-risk system without testing for bias, you are in violation.

Data governance practices must include protocols for data collection, annotation, storage, and versioning. You must document where your data came from, how it was collected, who annotated it, what instructions they received, and how you verified quality. You must maintain records of dataset versions and be able to trace which dataset version was used to train which model version. When an incident occurs, you must be able to reconstruct what data the model learned from.

## High-Risk AI System Dataset Obligations

High-risk AI systems face the strictest dataset obligations. If your system falls into a high-risk category, you must meet all of the following requirements before deployment. These requirements apply to systems placed on the market after the enforcement date, but they also apply as best practices for systems deployed earlier. If you cannot demonstrate compliance, you face enforcement action.

You must document the design choices for your training, validation, and test datasets. This includes explaining how you sourced the data, how you split it into training, validation, and test sets, and why you chose those splits. If you used stratified sampling to ensure balanced representation in the test set, you document that. If you used temporal splits to simulate deployment conditions, you document that. The rationale for design choices must be technically sound and documented in writing.

You must ensure that your datasets are relevant to the intended purpose. If your system will be used for hiring in healthcare, your training data should include healthcare resumes. If your system will be used for credit scoring in rural areas, your training data should include applicants from rural areas. Relevance is assessed against the specific deployment context, not a generic task definition.

You must ensure that your datasets are representative of the population the system will serve. This requires demographic analysis. You must identify the protected attributes relevant to your use case: race, ethnicity, gender, age, disability, religion, sexual orientation, and others. You must measure the distribution of these attributes in your training data. You must compare that distribution to the expected distribution in deployment. If mismatches exist, you must document them and explain how they affect system performance.

You must test for bias before deployment. The Act does not prescribe specific bias metrics, but it requires that you assess whether the system treats different groups fairly. You must disaggregate performance metrics by protected attributes. You must measure whether false positive rates, false negative rates, precision, recall, or other relevant metrics differ across groups. If disparities exist, you must explain whether they are justified by legitimate differences in the underlying data or whether they represent bias that must be mitigated.

You must implement measures to mitigate bias. If testing reveals disparities, you cannot simply document them and ship anyway. You must take action. This might include rebalancing the training data, applying fairness constraints during training, adjusting decision thresholds by group, or redesigning the task to avoid bias-prone predictions. The Act does not require perfect fairness. It requires that you make reasonable efforts to reduce bias and that you document what you did.

You must maintain records of your data governance processes. This means keeping logs of data collection activities, annotation instructions, quality audits, versioning decisions, and bias testing results. These records must be accessible to regulators upon request. If you cannot produce them, you are not compliant.

A recruiting technology company in 2025 prepared for EU AI Act compliance by implementing a comprehensive data governance framework. They documented every stage of their dataset pipeline: source selection, demographic sampling targets, annotation protocols, quality control procedures, and bias testing methodology. When they submitted their conformity assessment documentation to a notified body, the auditor found their data governance records exemplary. The system was certified compliant within six weeks. A competitor who lacked documentation spent nine months retrofitting governance processes and missed their market window.

## Training, Validation, and Testing Data Requirements

The Act distinguishes between training data, validation data, and test data, and it imposes obligations on all three. Many teams focus on training data quality and neglect validation and test data. This is a mistake. Validation and test data are where you assess whether your system meets the Act's requirements. If your test data is not representative, you cannot demonstrate that your system performs fairly across the deployment population.

Training data must be relevant, representative, free of errors, and complete. You must document its sources, its demographic composition, and any known limitations. If you used data augmentation, synthetic generation, or other techniques to supplement real data, you must document those techniques and explain why they were necessary. If you applied filtering or preprocessing, you must document what you removed and why.

Validation data must be independent of training data. You cannot tune hyperparameters on the training set and claim the model generalizes. You must hold out a validation set that the model does not see during training. The validation set must also be representative. If your validation set skews toward one demographic, you may optimize for that group's performance and miss problems in other groups.

Test data must be independent of both training and validation data. The test set is where you assess final performance and compliance with fairness requirements. The Act requires that you evaluate your system on test data before deployment. You cannot deploy and then test in production. You must test first, document the results, and only deploy if the results meet your safety and fairness criteria.

The test data must reflect deployment conditions. If your system will encounter adversarial inputs, your test set should include adversarial examples. If your system will face distribution shift over time, your test set should include data from multiple time periods. If your system will serve multiple languages or regions, your test set should cover all of them. The test set is not a random sample of historical data. It is a carefully constructed dataset designed to reveal whether the system will fail in deployment.

You must also document how you split the data. If you used random splits, you explain why random splits are appropriate for your use case. If you used stratified splits to ensure demographic balance in each set, you document the stratification variables. If you used temporal splits to avoid data leakage, you document the time boundaries. The split methodology affects whether your evaluation is trustworthy, and regulators will scrutinize it.

A credit scoring company preparing for compliance discovered during internal audit that their test set was not representative. It oversampled applicants from metropolitan areas and undersampled rural applicants. Their reported performance metrics were inflated. They rebuilt their test set with geographic stratification, reran all evaluations, and discovered a 9 percentage point accuracy gap for rural applicants. They retrained the model with augmented rural data before launch. Without the test data audit, they would have shipped a non-compliant system and faced regulatory penalties.

## Bias Testing and Monitoring Obligations

The Act requires that you test for bias before deployment and monitor for bias after deployment. Testing is a one-time gate before launch. Monitoring is an ongoing process throughout the system's lifecycle. Both are mandatory for high-risk systems.

Bias testing involves disaggregating performance metrics by protected attributes. You measure precision, recall, false positive rate, false negative rate, accuracy, and any other relevant metrics separately for each demographic group. You compare these metrics across groups. If one group has materially worse performance, you investigate why. If the disparity is due to biased training data, you fix the data. If the disparity is due to legitimate differences in base rates, you document that and explain why it does not constitute unfair discrimination.

The Act does not specify what counts as a material disparity. Different use cases have different fairness requirements. A 2 percentage point difference in accuracy may be acceptable in some contexts and unacceptable in others. What matters is that you assess the disparity in the context of potential harm. If a 2 percentage point difference in false positive rates means that one group is wrongly denied credit at a significantly higher rate, that is material. If it has no practical impact on outcomes, it may not be.

You must also test for intersectional bias. It is not sufficient to test gender and race separately. You must test combinations: Black women, older Hispanic men, young disabled users, and so on. Intersectional groups often experience compounded disparities that are invisible when you test attributes in isolation. If your sample sizes are too small to reliably measure performance for intersectional groups, you acknowledge that limitation and explain how you will address it.

Monitoring means tracking performance by demographic group in production. You cannot test once at launch and assume performance remains stable. Distribution shift happens. User populations change. Adversaries adapt. You must implement monitoring infrastructure that measures disaggregated performance continuously. You set alert thresholds: if performance for any group drops by more than a specified amount, you investigate. If a new bias emerges, you trigger a review and determine whether mitigation is needed.

Monitoring also includes collecting user feedback. The Act grants users the right to lodge complaints about AI systems. You must have a process for receiving complaints, investigating them, and taking corrective action when warranted. If users report that your system is biased, you cannot ignore those reports. You must investigate, document your findings, and respond.

An employment screening platform implemented automated bias monitoring that calculated demographic parity and equalized odds metrics daily for each client deployment. When metrics for one client deviated beyond acceptable thresholds, the system triggered an alert. Investigation revealed that the client had changed their job posting language in ways that attracted a less diverse applicant pool, which the AI then learned from. The platform notified the client, paused the deployment, and recommended changes to recruitment practices. The proactive monitoring prevented a discrimination incident that could have resulted in regulatory action against both the client and the platform provider.

## Documentation and Transparency Requirements

The Act requires that you document your data governance practices and make certain information available to regulators, users, and in some cases the public. Documentation is not internal record-keeping. It is a transparency obligation.

For high-risk systems, you must produce technical documentation that includes a description of the training, validation, and test datasets. This description must cover the sources of data, the methods for data collection, the demographic composition, any known biases, and the measures taken to mitigate bias. The documentation must be detailed enough that a regulator or auditor can assess whether you complied with Article 10.

You must also provide information to users. Users deploying your high-risk system must be informed about the data used to train it, including any limitations or biases. This information must be clear, accessible, and sufficient to allow users to make informed decisions about whether to deploy the system and how to use it safely. If your system has known performance gaps for certain demographics, you disclose that.

For general-purpose AI models, transparency obligations extend further. You must make publicly available a summary of the training data, including the sources, the scope, and the main characteristics. If your model was trained on copyrighted content, you must disclose that. If your model was trained on web scrapes, you disclose the domains and the date ranges. The goal is to allow downstream deployers to assess whether your model is appropriate for their use case.

Documentation must be maintained throughout the system's lifecycle. When you update the dataset, you update the documentation. When you discover new biases, you update the documentation. When you change data sources, you update the documentation. The documentation is a living artifact, not a one-time deliverable.

You must also maintain logs and records that demonstrate compliance. If a regulator requests evidence that you tested for bias, you must be able to produce test results, methodology descriptions, and decision logs. If you cannot, you are presumed non-compliant. The burden of proof is on you.

A medical AI company maintained a compliance documentation repository that tracked every dataset version, every bias test, every mitigation experiment, and every approval decision. When a national competent authority conducted a surprise audit in 2026, the company provided complete documentation within 48 hours. The auditor verified compliance across all Article 10 requirements and closed the audit with no corrective actions. Companies without comparable documentation faced prolonged audits, suspension of market authorization, and significant legal costs.

## GPAI Model Obligations for Dataset Disclosure

General-purpose AI models, especially those with systemic risk, face specific dataset disclosure obligations. If you provide a foundation model, a large language model, or another GPAI system, you must document the data you used for training and make that documentation publicly accessible.

You must disclose the data sources. If you scraped the web, you list the major domains. If you used licensed datasets, you name them or describe them if licensing prohibits naming. If you used proprietary internal data, you describe its nature and scope. The disclosure does not require releasing the data itself, but it requires describing it in enough detail that others can assess its quality and bias profile.

You must disclose the methods used to curate and filter the data. If you applied toxicity filters, you describe the thresholds and the filtering mechanisms. If you deduplicated, you describe the deduplication strategy. If you removed certain content categories, you describe what you removed and why. These choices shape what the model learns, and downstream deployers need to understand them.

You must disclose known biases. If your training data overrepresents certain languages, regions, or viewpoints, you state that. If your data includes problematic content that you could not fully filter out, you acknowledge that. The goal is not to present the data as perfect. The goal is to allow others to make informed decisions about whether your model is suitable for their application.

For models with systemic risk, you must also conduct adversarial testing and red-teaming to identify serious risks, including bias-related risks. You must document the results of this testing and the measures you took to address identified risks. If you cannot eliminate a risk, you document it as a residual limitation.

GPAI obligations are stricter than high-risk system obligations in some respects because GPAI models are used as components in many downstream systems. A bias in a foundation model propagates to every application built on it. Transparency about dataset choices allows downstream developers to assess whether they need additional mitigation for their specific use case.

A foundation model provider in 2025 published a detailed data transparency report that listed all major training data sources, described filtering and curation processes, disclosed known demographic and linguistic skews, and reported the results of bias red-teaming exercises. Downstream deployers cited this transparency as a key factor in selecting the model. Enterprise customers building high-risk applications on the foundation model used the transparency report as part of their own compliance documentation. The provider's transparency created a competitive advantage in the enterprise market.

## Practical Compliance: What Teams Must Do Before August 2026

Compliance with the EU AI Act is not a legal exercise. It is an engineering exercise. Legal teams can help you interpret the requirements, but engineering teams must implement them. Here is what you must do if you are deploying a high-risk AI system in Europe and the August 2026 deadline is approaching.

First, classify your system. Determine whether it is high-risk under the Act. If it is used for hiring, credit scoring, benefit eligibility, law enforcement, biometrics, or another high-risk application, it is subject to Article 10. Do not assume your system is low-risk because it seems benign. Read the Annex III list carefully. Consult with legal counsel if you are unsure.

Second, audit your datasets. Assess whether your training, validation, and test data meet the relevance, representativeness, accuracy, and completeness requirements. Document the demographic composition of your data. Identify underrepresented groups. If your data does not meet the standard, plan remediation: collect more data, reweight existing data, or apply synthetic augmentation.

Third, test for bias. Disaggregate your performance metrics by every protected attribute you can measure. Look for disparities. Investigate root causes. If the disparities are due to biased data, fix the data. If they are due to task design, redesign the task. If they are unavoidable, document them and explain why mitigation is not feasible.

Fourth, implement monitoring. Build infrastructure to track disaggregated performance in production. Set alert thresholds. Assign responsibility for reviewing alerts and taking action. Monitoring is not optional. It is a requirement.

Fifth, document everything. Produce technical documentation that covers data sources, data governance practices, bias testing results, mitigation measures, and known limitations. Keep this documentation up to date. Be prepared to share it with regulators.

Sixth, train your team. Engineers, product managers, and data scientists must understand the Act's requirements. They must know what representativeness means, what bias testing involves, and what documentation is required. Compliance is not something Legal handles alone. It is something the entire team must internalize.

Seventh, establish accountability. Assign someone to own compliance. That person should have authority to delay launches if compliance is incomplete. They should report to senior leadership. Compliance is a gate, not a suggestion.

If you are not doing these things, you are not compliant. If you are not compliant and you are operating in Europe, you are exposed to enforcement action. Fines under the Act can reach 6 percent of global annual revenue. For a large company, that is hundreds of millions of dollars. For a startup, it is existential.

A startup offering AI-powered legal document analysis realized in early 2026 that they were not compliant. They had three months until the August deadline. They halted feature development, assigned half their engineering team to compliance work, hired external auditors, and implemented bias testing and monitoring infrastructure. They met the deadline but at significant cost: a four-month feature freeze, 600,000 dollars in external audit fees, and lost competitive ground. A competitor who started compliance work in 2024 met the deadline with minimal disruption and captured market share while the startup was distracted.

## Why Compliance Is Not Optional

Some companies treat regulation as a game of probabilities. They calculate the likelihood of enforcement, the expected fine, and the cost of compliance. If the expected fine is lower than the cost of compliance, they choose non-compliance. This is a catastrophic miscalculation for the EU AI Act.

First, enforcement is not probabilistic. It is systematic. The Act establishes national competent authorities in each member state with the power to audit AI systems. These authorities are staffed, funded, and politically motivated to enforce the Act. If you deploy a high-risk system, you should assume you will be audited at some point.

Second, fines are not the only consequence. Non-compliance can result in your system being banned from the European market. It can result in criminal liability for executives. It can result in reputational damage that destroys your business in Europe and beyond. The Act is not a cost of doing business. It is a market access requirement.

Third, compliance builds competitive advantage. The companies that comply early can market their systems as EU AI Act certified. They can win contracts with European enterprises and governments that require compliance. They can expand into regulated industries that demand fairness. Compliance is not just risk mitigation. It is differentiation.

Fourth, the Act sets a global standard. Other jurisdictions are watching Europe. Similar regulations are under consideration in the United States, Canada, the United Kingdom, and elsewhere. If you build compliance into your dataset engineering practices now, you are prepared for future regulations. If you defer, you will face costly retrofits every time a new jurisdiction imposes requirements.

The EU AI Act's dataset obligations are the most rigorous in the world, but they are not unreasonable. They require you to do what you should already be doing: building fair, representative datasets, testing for bias, documenting limitations, and monitoring performance. These practices are not unique to Europe. They are professional standards. The Act simply makes them mandatory.

A multinational corporation with AI systems deployed across multiple jurisdictions used EU AI Act compliance as the baseline for all markets. They reasoned that meeting the strictest standard simplified operations and reduced risk globally. When similar regulations passed in California and Canada in late 2026, the corporation was already compliant. Competitors who built separate compliance frameworks for each jurisdiction faced escalating costs and complexity. The corporation's unified approach to compliance became a strategic advantage.

## What Success Looks Like

Successful compliance is not about checking boxes. It is about building systems that work for everyone, documenting what you built honestly, and maintaining quality over time. When you achieve compliance, you have not just met a legal standard. You have built a better product.

Your datasets are representative, which means your system performs well across the populations it serves. Your bias testing infrastructure catches problems before they reach users. Your monitoring systems detect distribution shift and performance degradation early. Your documentation provides transparency that builds trust with users and customers. Your governance processes ensure that quality does not degrade as the team grows and the product evolves.

The companies that treat compliance as a burden build minimal systems that barely meet requirements. The companies that treat compliance as an opportunity build robust systems that exceed requirements and create competitive advantage. The difference is mindset. Compliance is not overhead. It is investment in quality.

---

Chapter 8 concludes here. You now understand how to assess, measure, and mitigate bias in datasets. You know how to document what you built and what limitations remain. You understand the business case for fairness and the regulatory obligations that enforce it. The next chapter addresses privacy, PII, and data security, where the dataset engineering challenge shifts from fairness to confidentiality and where the stakes are equally high.
