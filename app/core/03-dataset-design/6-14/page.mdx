# 6.14 — Eval Set Reviews: The Cross-Functional Approval Process

In early 2025, a legal technology startup spent four months building what their engineering team believed was a production-ready contract clause generation system. They had constructed a 1,200-example evaluation set, achieved 89% accuracy on their metrics, and were preparing to launch to their first enterprise customer—a Fortune 500 financial services firm. Two days before the scheduled launch, the customer's legal team requested a review of the evaluation dataset to understand how quality was being measured. The review meeting lasted six hours and ended with the launch being postponed indefinitely. The legal team identified 147 examples where the ground truth annotations were legally incorrect, 89 examples that tested outdated regulatory requirements, and 34 examples where the definition of success was ambiguous enough that no reliable evaluation was possible. The engineering team had built the eval set in isolation, without input from domain experts, compliance officers, or the lawyers who would ultimately be accountable for the system's outputs. The eval set was not measuring legal quality. It was measuring what engineers thought legal quality meant, and that gap made the entire evaluation process worthless.

This failure is not rare. It is the default outcome when evaluation datasets are treated as engineering artifacts rather than cross-functional product requirements. An eval set encodes decisions about what quality means, what errors are acceptable, what contexts matter, and what trade-offs are appropriate. These are not technical decisions. They are product decisions, domain decisions, compliance decisions, and risk decisions. If the people who understand those domains are not involved in defining, reviewing, and approving the eval set, you are building on a foundation of unvalidated assumptions. The eval set review process is where those assumptions get challenged, corrected, and formalized into an organizational standard that engineering, product, domain experts, legal, and leadership all agree represents the quality bar for the system.

## Who Reviews Eval Sets and Why Their Perspectives Matter

The composition of the eval set review team determines whether the eval set reflects real-world quality or just engineering intuition. At minimum, you need representation from four groups: engineering, product, domain experts, and compliance or legal. Larger organizations or higher-risk systems may also need representation from trust and safety, user research, operations, and executive leadership. Each group brings a different lens on what quality means and what failure looks like, and the eval set must satisfy all of them.

Engineering reviews the eval set for technical correctness and feasibility. They validate that the eval cases are actually testable, that the ground truth annotations are consistent with how the system is designed to work, that the scoring metrics are implementable, and that the eval set can run within acceptable time and cost budgets. Engineering catches problems like eval cases that assume capabilities the model does not have, ground truth that requires information not available in the input, or scoring rubrics that cannot be automated and would require prohibitive human evaluation costs. Engineering also validates that the eval set avoids contamination with training data and that it represents realistic production inputs rather than adversarial edge cases that will never occur.

Product reviews the eval set for alignment with user needs and business requirements. They validate that the eval cases cover the most important use cases, that the quality dimensions being measured are the ones users actually care about, that the pass-fail thresholds align with acceptable user experience, and that the eval set balances coverage across different user segments and scenarios. Product catches problems like over-indexing on rare edge cases at the expense of common workflows, measuring dimensions that do not affect user satisfaction, or setting quality bars so high that the system will never launch or so low that it will disappoint users.

Domain experts review the eval set for correctness of ground truth and realism of scenarios. They validate that the examples are representative of real-world complexity, that the annotations reflect true domain knowledge rather than common misconceptions, that the terminology is used correctly, and that the eval cases test the nuances that matter in practice. Domain experts catch problems like ground truth based on outdated standards, eval cases that oversimplify real-world ambiguity, incorrect labeling of what constitutes a right or wrong answer, and missing coverage of domain-specific edge cases that occur frequently in production.

Compliance and legal review the eval set for risk, regulatory adherence, and liability implications. They validate that the eval cases test compliance with relevant regulations, that the ground truth does not encode legally problematic behaviors, that the system is being evaluated on its ability to avoid prohibited outputs, and that the eval set documents the organization's due diligence in assessing safety and compliance. Legal catches problems like eval cases that would encourage the model to produce legally risky outputs, missing coverage of regulatory requirements, ground truth that conflicts with legal standards, and insufficient testing of how the system handles adversarial or boundary cases that could create liability.

A healthcare AI company building a clinical decision support tool assembled a review team that included two physicians, one nurse practitioner, one pharmacist, one HIPAA compliance officer, one product manager, two ML engineers, and one VP of engineering. Each evaluator reviewed a sample of 100 eval cases from the full 800-case set and flagged issues. The physicians identified 23 cases where the ground truth medical guidance was outdated or incomplete. The pharmacist identified 17 cases where medication interaction warnings were wrong. The compliance officer identified 12 cases that tested scenarios that would violate patient privacy regulations if answered. The product manager identified 31 cases that were technically correct but did not reflect how real clinicians would use the system. The engineers identified 8 cases that were impossible to score automatically. The combined review resulted in 91 eval cases being rewritten, 34 being removed entirely, and 48 new cases being added to cover gaps the review identified.

## The Eval Set Review Meeting: Structure and Objectives

The eval set review meeting is not a presentation where engineering shows the eval set and stakeholders nod approval. It is a working session where the team examines the eval set in detail, debates ground truth, identifies gaps, and makes binding decisions about what quality means. A well-run review meeting has a clear agenda, pre-work requirements, decision-making authority, and documented outcomes.

Pre-work is essential. All reviewers must receive the full eval set—or a representative sample if the full set is too large—at least one week before the meeting. The eval set should be shared in a reviewable format, such as a spreadsheet or annotation tool, where reviewers can see the input, the expected output, the quality dimensions being measured, and the scoring rubric. Reviewers should be asked to flag cases where they disagree with the ground truth, where they think the scenario is unrealistic, where they see coverage gaps, or where they think the scoring criteria are unclear. This pre-work ensures that the meeting is spent resolving substantive issues rather than reading the eval set for the first time.

The meeting itself typically runs two to four hours, depending on eval set size and system complexity. The agenda should cover five topics: eval set coverage, ground truth correctness, scoring rubric clarity, edge case handling, and sign-off criteria. The facilitator—usually the product manager or engineering lead—should drive the discussion, but every reviewer should have the opportunity to raise issues and challenge assumptions.

Coverage review comes first. The team walks through the distribution of eval cases across use cases, user types, input complexity, and quality dimensions. The goal is to confirm that the eval set tests the scenarios that matter most and does not have large gaps. Product leads this discussion by presenting the intended use case breakdown and asking domain experts and other stakeholders whether critical scenarios are missing. A financial services eval set review might reveal that 60% of cases test stock trading questions but only 10% test retirement planning questions, even though retirement planning is 40% of production traffic. This imbalance is a coverage gap that needs to be corrected.

Ground truth correctness review is where domain experts scrutinize the annotations. The team samples eval cases—often starting with cases that reviewers flagged during pre-work—and debates whether the expected output is actually correct. This is where you discover that your ground truth is based on outdated information, reflects common misconceptions, or oversimplifies complex judgments. A legal contract review meeting might debate whether a specific clause is compliant with a recent regulatory update. A medical eval set review might debate whether a recommended treatment is standard of care or represents one school of thought among several valid approaches. These debates are not distractions. They are the core value of the review process. If your team cannot agree on what the right answer is, your eval set cannot measure quality reliably.

Scoring rubric clarity review ensures that the criteria for passing or failing are well-defined and consistently applicable. The team examines the rubric for each quality dimension and asks whether a different evaluator would score the same output the same way. If the rubric says "the response must be empathetic," that is too vague. If it says "the response must acknowledge the user's stated emotion and avoid dismissive language," that is clearer. If the rubric says "the response must be concise," that is subjective. If it says "the response must be under 150 words unless additional length is required to fully answer the question," that is measurable. Vague rubrics lead to inconsistent evaluation and unresolvable disagreements about whether the system is meeting the quality bar.

Edge case handling review focuses on the adversarial, ambiguous, and boundary cases in the eval set. These are the cases where multiple answers might be defensible, where the system is most likely to fail, or where failure has the highest cost. The team debates how many edge cases to include, how to score outputs that are partially correct, and what to do when the model produces an answer that is different from the ground truth but still acceptable. A customer support eval set review might debate whether a response that solves the user's problem in an unconventional way should pass even if it does not match the expected response template.

Sign-off criteria review is the final discussion. The team defines what it means for the eval set to be approved and ready for use. This typically includes agreement on coverage thresholds, inter-rater reliability targets for human-scored dimensions, documented resolution of all flagged ground truth disputes, and assignment of ownership for ongoing eval set maintenance. The meeting should end with explicit commitments: who will implement the agreed-upon changes, when the revised eval set will be ready, and who has final authority to approve it.

## Review Checklists: What to Validate Before Approval

A structured checklist ensures that eval set reviews are comprehensive and consistent across systems and teams. The checklist should cover coverage, balance, contamination, freshness, scoring reliability, and documentation. Each item should have clear pass-fail criteria so that reviewers can objectively assess whether the eval set is ready.

Coverage checklist items include: Do the eval cases span all major use cases identified in the product requirements? Are there eval cases for each user segment the system is designed to serve? Are there eval cases representing low, medium, and high input complexity? Are there eval cases for each quality dimension the system is being measured on? Are there eval cases for the edge cases and failure modes identified in risk assessments? A checklist item passes if the eval set includes at least a minimum number of cases—often five to ten—for each category. If any category is missing or under-represented, the eval set does not pass coverage review.

Balance checklist items include: Is the distribution of eval cases proportional to expected production distribution, or is there a documented justification for oversampling certain categories? Are positive and negative examples balanced, or is there a documented justification for imbalance? Are easy, medium, and hard cases balanced? Does the eval set avoid over-indexing on any single scenario or user type? Imbalanced eval sets lead to misleading metrics. If 90% of your eval cases are easy and 10% are hard, a system that passes 85% of cases might be failing 50% of hard cases, which is the only number that matters if hard cases are common in production.

Contamination checklist items include: Are the eval cases distinct from training data? If training data is not fully documented, has the team done sampling to check for overlap? Are the eval cases based on real production data, synthetic data, or manually written examples, and is the source documented? If the eval set includes production data, has it been scrubbed of PII and other sensitive information? Contamination is the silent killer of eval set validity. If your model has seen the eval cases during training, your metrics are meaningless, and this is often invisible until someone explicitly checks.

Freshness checklist items include: Are the eval cases based on current domain knowledge, or do they reflect outdated standards? Are there eval cases that test recently added features or recently discovered failure modes? If the system has been in production, does the eval set include examples of recent production failures? How often is the eval set reviewed and updated, and is there a documented schedule? Stale eval sets measure the wrong thing. A medical eval set based on 2023 treatment guidelines will give you false confidence in a system that needs to reflect 2026 standards.

Scoring reliability checklist items include: For dimensions that require human judgment, what is the inter-rater reliability? Is it above the acceptable threshold, typically 80% agreement or a Cohen's kappa above 0.7? For dimensions that are automated, has the scoring function been validated against human judgment on a sample? Are the scoring rubrics clear enough that a new evaluator can apply them consistently? Can the eval set be scored within acceptable time and cost constraints? Unreliable scoring means you cannot trust your metrics, and if scoring is too expensive, you cannot run the eval set frequently enough for it to be useful during development.

Documentation checklist items include: Is there a document that explains the purpose of the eval set, the intended use cases, the quality dimensions being measured, and the pass-fail criteria? Is each eval case annotated with metadata like use case category, difficulty level, and quality dimensions at risk? Is there a change log that documents when the eval set was created, who contributed, what major revisions have been made, and why? Is there ownership assigned for maintaining and updating the eval set? Without documentation, the eval set becomes a black box that only the original creators understand, and it cannot be maintained or evolved as the system changes.

## Disagreements About Ground Truth and Resolution Frameworks

Ground truth disagreements are inevitable and valuable. They surface the ambiguities, edge cases, and domain complexities that your eval set needs to handle. The question is not whether disagreements will occur but how you resolve them in a way that improves the eval set rather than papering over legitimate uncertainty.

The first category of disagreement is factual errors. A reviewer identifies that the ground truth is simply wrong—it contains outdated information, reflects a common misconception, or makes a mistake in logic or calculation. These disagreements are the easiest to resolve. The team validates the correct information, updates the ground truth, and documents the correction. A financial services eval set review might catch that the ground truth assumes a tax rate that changed two years ago. A medical eval set review might catch that the ground truth recommends a medication dosage that is no longer standard of care. These are clean fixes.

The second category is ambiguity in the task. The input is unclear, incomplete, or could reasonably be interpreted in multiple ways, leading to multiple valid outputs. These disagreements reveal that the eval case itself is poorly designed. The resolution is not to pick one interpretation and call it ground truth but to rewrite the eval case to remove the ambiguity or to explicitly test how the system handles ambiguous inputs. If the eval case is "What is the best investment for retirement," that is ambiguous because best depends on risk tolerance, time horizon, and financial goals. The eval case should either specify those parameters or test whether the system asks clarifying questions rather than assuming.

The third category is domain judgment calls. Domain experts disagree about what the right answer is because there are multiple valid approaches, schools of thought, or acceptable standards. A medical eval set review might have two physicians disagree about whether a specific treatment is first-line or second-line therapy for a condition. A legal eval set review might have two lawyers disagree about whether a contract clause is enforceable in a specific jurisdiction. These disagreements are not errors. They reflect real-world uncertainty, and the eval set must handle them.

The resolution framework for judgment calls depends on how conservative your system needs to be. If your system is advisory and users are expected to apply their own judgment, you can label these cases as having multiple acceptable answers and score the system as passing if it produces any of the acceptable options. If your system is authoritative and users are expected to trust its output, you need to define a canonical answer even when experts disagree, and you should choose the most conservative, lowest-risk option. A medical system should default to the safer treatment. A legal system should default to the more defensible contract language. A financial system should default to the lower-risk investment recommendation.

A clinical decision support company developed a three-tier framework for resolving ground truth disagreements. Tier one disagreements were factual errors where one answer was objectively correct and the other was wrong. These were resolved by the domain expert with the most relevant expertise, and the ground truth was updated immediately. Tier two disagreements were ambiguous inputs where the task was under-specified. These were resolved by rewriting the eval case to add necessary context or to explicitly test ambiguity handling. Tier three disagreements were judgment calls where multiple answers were defensible. These were resolved by the clinical director, who chose the answer that aligned with the organization's risk posture and clinical philosophy, and the decision was documented in the eval set metadata so future reviewers understood why that ground truth was chosen.

Another resolution approach is to flag uncertain cases and track them separately. Instead of forcing a single ground truth, you annotate the case as having high uncertainty or multiple valid answers, and you score the system differently on these cases. A response that matches the primary ground truth gets full credit. A response that matches an alternative acceptable answer gets partial credit. A response that does not match any acceptable answer fails. This approach acknowledges uncertainty without pretending it does not exist, and it prevents the eval set from penalizing the model for producing outputs that are different but defensible.

## Sign-Off Authority and Decision Rights

Eval set approval is not consensus-driven. It is authority-driven. Someone must have the final decision right to approve the eval set, resolve unresolved disputes, and declare that the eval set is ready for use. Without clear authority, eval set reviews devolve into endless debates where no decision is final and the eval set is never approved.

The most common model is that product management has final sign-off authority, with input from engineering, domain experts, and legal. Product owns the quality bar and the user experience, so product should decide whether the eval set measures the right things. Engineering has veto authority on technical feasibility—if the eval set cannot be implemented or scored reliably, engineering can block approval. Domain experts have veto authority on ground truth correctness—if the domain experts say the annotations are wrong, the eval set cannot be approved. Legal has veto authority on compliance and risk—if legal says the eval set does not adequately test regulatory requirements, the eval set cannot be approved. This multi-veto model ensures that no single function can push through an eval set that fails on critical dimensions, but it still gives product the final authority to make trade-offs and prioritize.

A healthcare AI company formalized decision rights as follows: the product manager for the AI system owns eval set approval and has final decision authority on coverage, use case prioritization, and pass-fail thresholds. The clinical director has veto authority on ground truth correctness for medical content. The engineering director has veto authority on technical feasibility and scoring reliability. The compliance officer has veto authority on regulatory coverage and risk testing. Any of the veto holders can block eval set approval, but only the product manager can declare it approved. This structure prevented both the problem of one function railroading an inadequate eval set and the problem of endless debate with no resolution.

For lower-risk systems, decision rights can be simpler. The engineering team lead might have full approval authority with advisory input from product and domain experts. For higher-risk systems, decision rights might escalate to executive leadership. A legal tech company required that eval sets for systems advising on regulated matters be signed off by the VP of product, the chief legal officer, and the CTO jointly. No single leader could approve, and all three had to agree that the eval set was adequate.

Sign-off should be documented in writing. The approval document should list who reviewed the eval set, what issues were raised, how they were resolved, who has final authority, and what the sign-off criteria were. This documentation creates accountability and ensures that future changes to the eval set go through the same rigorous process. If the eval set is later found to be inadequate, the sign-off record shows who was responsible and what information they had at the time.

## Documentation Requirements for Approved Eval Sets

An approved eval set is not just a collection of examples and annotations. It is a specification of quality that must be documented thoroughly enough that anyone on the team can understand what it measures, why it was designed that way, and how to use it. Minimum documentation includes purpose, scope, construction process, ground truth sources, scoring rubrics, known limitations, maintenance plan, and change history.

Purpose documentation answers why the eval set exists and what decisions it informs. Is this eval set for pre-launch validation, ongoing regression testing, model comparison, or performance monitoring? What quality dimensions does it measure, and what quality dimensions does it explicitly not measure? What pass-fail threshold is required for launch, and what threshold is required for ongoing production use? Purpose documentation ensures that the eval set is not misused to make decisions it was not designed to support.

Scope documentation defines what inputs and scenarios are covered and what is out of scope. What use cases are included, and which are tested by other eval sets? What user segments are represented? What input complexity range is tested? What edge cases are included? Scope documentation prevents teams from assuming the eval set provides coverage it does not have.

Construction process documentation describes how the eval set was built. Were examples sourced from production data, written by domain experts, generated synthetically, or pulled from public datasets? How many annotators labeled the ground truth, and what was their inter-rater reliability? What quality checks were performed during construction? Who reviewed and approved the eval set, and when? This documentation establishes the provenance and trustworthiness of the eval set.

Ground truth sources documentation specifies where the correct answers came from. Were they based on domain expert judgment, regulatory text, published standards, or production outcomes? If multiple sources were used, how were conflicts resolved? If ground truth required judgment calls, what principles guided those calls? This documentation is critical for understanding whether the eval set measures the right kind of correctness.

Scoring rubrics documentation defines exactly how each quality dimension is measured. For each dimension, the rubric should specify whether scoring is automated or human, what the scoring scale is, what constitutes passing versus failing, and how edge cases are handled. The rubric should be clear enough that two independent scorers would produce the same results on the same output. Vague rubrics make evaluation unreliable and unreproducible.

Known limitations documentation acknowledges what the eval set does not cover and where it might be misleading. Does it under-represent certain user segments? Does it over-represent easy cases? Are there known gaps in coverage that are planned for future expansion? Are there quality dimensions that are important but not measured because reliable scoring is not feasible? Documenting limitations prevents overconfidence and sets expectations for what the eval set can and cannot tell you.

Maintenance plan documentation specifies how the eval set will be kept current. How often will it be reviewed for freshness? Who is responsible for adding new cases when the product adds features or when production failures reveal gaps? What triggers a major revision versus incremental updates? What is the process for proposing and approving changes? Without a maintenance plan, eval sets decay and become obsolete.

Change history documentation logs every significant change to the eval set. When were cases added, removed, or modified? Who made the change, and why? What was the impact on eval metrics, and were model performance numbers adjusted to account for the change? Change history creates an audit trail and prevents the problem of eval metrics improving not because the model got better but because the eval set got easier.

A legal contract analysis company maintained a 47-page eval set documentation package that included all of these components. The package was version-controlled, reviewed quarterly, and updated whenever the eval set changed. New team members were required to read the documentation as part of onboarding. Product reviews referenced the documentation to understand what quality commitments the eval set enforced. External auditors reviewed the documentation as part of compliance assessments. The documentation turned the eval set from a tool that only the ML team understood into an organizational asset that everyone could trust and use.

## Handling Review Conflicts and Escalation Paths

Not all eval set review disagreements can be resolved through discussion and consensus. Some disagreements reflect fundamental differences in risk tolerance, product philosophy, or domain interpretation. You need clear escalation paths and decision frameworks for when the review process reaches an impasse.

The first category of unresolvable disagreements is when product and engineering have different risk tolerances. Product might want to launch with an eval set that engineering considers too lenient because the pass-fail thresholds do not adequately test edge cases. Engineering might want an eval set that product considers too strict because it would delay launch by months to achieve the required pass rate. These disagreements are not technical. They are business decisions about acceptable risk, and they must be escalated to leadership with clear framing of the trade-offs.

A SaaS company faced this conflict when their engineering team wanted 95% pass rate on a comprehensive eval set before launching an AI email writing feature, while product wanted to launch at 85% pass rate to capture market opportunity. The escalation to the VP of Product and VP of Engineering resulted in a hybrid decision: launch at 88% on a reduced-scope eval set covering only the most common use cases, with a commitment to expand coverage and raise the bar to 92% within three months post-launch. Both teams had to compromise, but the decision was made explicitly with documented trade-offs rather than through attrition or passive-aggressive undermining.

The second category is when domain experts disagree with each other. Two physicians might have different views on appropriate treatment recommendations. Two lawyers might interpret regulatory requirements differently. Two financial advisors might disagree on what constitutes prudent investment advice. These disagreements cannot be resolved by technical means. They require domain leadership to make a call about which interpretation the organization will adopt as its standard.

A medical AI company faced repeated conflicts between a cardiologist and an internist on their review team about how aggressively to recommend specialist referrals. The cardiologist wanted the system to recommend cardiology referral for any chest pain or cardiac risk factor. The internist wanted the system to triage more conservatively to avoid overwhelming specialists with unnecessary referrals. The conflict was escalated to the chief medical officer, who made the call that the system should err toward over-referral in the first version and that they would tune down referral rates based on production data about which referrals were clinically appropriate.

The third category is when legal or compliance requirements conflict with user experience. Legal might require disclosures that make responses so hedged and verbose that users stop reading them. Compliance might require verification steps that make the system too slow or cumbersome for practical use. These conflicts pit regulatory risk against product risk, and they must be resolved by leadership with input from legal, compliance, product, and sometimes external counsel.

A financial services company building an investment recommendation system faced this conflict when their legal team required that every recommendation include comprehensive disclaimers about market risk, past performance, and regulatory disclosures. The user research team demonstrated that users stopped reading after the first 50 words and that burying the actual recommendation in 200 words of disclosures made the feature unusable. The resolution was to implement layered disclosure: a short, clear recommendation with a one-sentence risk summary, followed by expandable detailed disclosures for users who wanted them. Legal accepted this approach after confirming it met regulatory requirements for disclosure availability even if users did not read it.

The escalation framework should specify who has decision authority at each level. For technical disagreements about eval set implementation, the engineering lead decides. For product scope and prioritization disagreements, the product manager decides. For domain correctness disagreements, the most senior domain expert or a designated domain authority decides. For legal and compliance disagreements, legal has veto authority. For cross-functional conflicts that involve trade-offs between these areas, the decision escalates to the executive team or a designated cross-functional leader.

## Version Control and Change Management for Eval Sets

Eval sets must be version-controlled with the same rigor as code, and changes must go through a formal change management process that prevents accidental degradation and ensures traceability. Treating eval sets as informal collections of examples that anyone can modify leads to eval set drift, unexplained metric changes, and loss of institutional knowledge about why cases were designed the way they were.

Every eval set should have a version number or commit hash that uniquely identifies its exact state. When you report eval metrics, you must report which eval set version those metrics were computed against. When you compare model performance across time, you must either use the same eval set version for all comparisons or explicitly account for eval set changes. A report that says "model accuracy improved from 84% to 91%" is incomplete. It should say "model accuracy improved from 84% on eval set v2.3 to 91% on eval set v2.7, with eval set changes contributing an estimated 3 percentage points of the improvement."

Changes to the eval set should go through pull request or equivalent review processes. A proposed change should include the rationale, the specific cases being added, removed, or modified, the expected impact on metrics, and the approval from relevant stakeholders. The change should be reviewed by at least one other person before merging. For high-stakes eval sets, changes should require review by domain experts and approval by the eval set owner.

A legal tech company implemented a three-tier change process. Tier one changes were adding new cases to expand coverage without modifying existing cases. These required review by one engineer and approval by the eval set owner. Tier two changes were modifying ground truth or scoring rubrics for existing cases. These required review by one engineer and one domain expert and approval by the eval set owner and the legal director. Tier three changes were removing cases or making changes that would materially affect metrics. These required review by engineering, domain experts, legal, and product, and approval by the VP of Product. This tiered process balanced the need for continuous improvement with the need for stability and oversight.

When eval sets change, you must re-evaluate all model checkpoints against the new eval set to understand how much of the metric change is due to model improvement versus eval set changes. If your production model was evaluated at 89% on eval set v2.5 and you update to eval set v2.6, you need to re-evaluate the production model on v2.6 to establish a new baseline. Otherwise you will not know whether future changes improve the model or just align better with the new eval set.

Version control also enables rollback when eval set changes turn out to be wrong. If you update the eval set and discover that the changes made it less predictive of production quality, you can roll back to the previous version while you investigate what went wrong. Without version control, you cannot roll back because you do not have a precise record of what the eval set looked like before the change.

## Cross-Functional Ownership and Accountability

Eval set quality is not solely an engineering responsibility. It is a cross-functional responsibility that requires ongoing commitment from product, domain experts, legal, and leadership. Unclear ownership leads to eval sets that decay over time as everyone assumes someone else is maintaining them.

The eval set owner is the single person accountable for eval set quality, currency, and fitness for purpose. This is typically a product manager or senior engineer with deep context on the system and the authority to drive cross-functional alignment. The owner is responsible for scheduling reviews, incorporating production feedback, coordinating with domain experts, ensuring documentation is current, and making the final call on changes. The owner does not do all the work, but they are accountable for ensuring it gets done.

Domain experts are responsible for reviewing ground truth correctness, validating that eval cases reflect real-world scenarios, and updating the eval set when domain knowledge changes. They are not responsible for building the eval set infrastructure or running evaluations, but they are responsible for ensuring that what is being measured is actually correct and meaningful. Domain expert responsibility must be formalized in role expectations and performance objectives, or it will be deprioritized in favor of their primary responsibilities.

Engineering is responsible for the technical implementation of scoring, the reliability and performance of evaluation pipelines, and the tooling that makes it feasible to build and maintain the eval set. They are not responsible for defining what quality means, but they are responsible for measuring it accurately and efficiently. Engineering should push back when eval set requirements are technically infeasible and propose alternative approaches that achieve the same quality goals with less implementation complexity.

Product is responsible for defining the quality bar, prioritizing coverage across use cases, and ensuring that the eval set aligns with user needs and business requirements. They are not responsible for implementing the eval set or validating domain correctness, but they are responsible for ensuring that passing the eval set actually means the product is ready to ship. Product should push back when the eval set measures the wrong things or sets a quality bar that is misaligned with market expectations.

Legal and compliance are responsible for ensuring that the eval set adequately tests regulatory requirements and risk scenarios. They are not responsible for building or running the eval set, but they have veto authority on launch if the eval set does not demonstrate adequate testing of legal and compliance dimensions. Legal should be involved in eval set design from the beginning, not brought in at the end to rubber-stamp an eval set they had no input on.

A healthcare AI company formalized cross-functional ownership by creating an eval set council that met monthly. The council included the product manager as owner, two physicians as domain experts, the engineering lead for the AI system, a compliance officer, and a representative from user research. The council reviewed eval set metrics, discussed production failures, prioritized new eval cases, and approved major changes. This formalized structure ensured that eval set maintenance was not an afterthought and that all stakeholders had a regular forum to raise concerns and propose improvements.

## Continuous Improvement: Treating Eval Sets as Living Artifacts

Eval set review is not a one-time gate before launch. It is a continuous process that runs throughout the system lifecycle. Production teaches you what your eval set missed, what it over-weighted, and what changed in the real world. The eval set must evolve in response to that learning, or it stops being a useful quality measure and becomes an obsolete snapshot of what you thought mattered six months ago.

The most important input to eval set improvement is production failure analysis. Every time the system fails in production in a way that the eval set did not predict, you have discovered an eval set gap. A customer support system might pass 92% of eval cases but produce a production response that is factually wrong and makes a user angry. If that failure mode was not represented in the eval set, you need to add cases that test it. A medical documentation system might score 89% on completeness but generate a note that is missing a required billing code. If the eval set did not test billing code completeness, it needs to be updated.

A fintech company ran a monthly production failure review where support, product, engineering, and data science examined the worst user-reported issues and the lowest-rated AI outputs from the previous month. For each failure, they asked whether the eval set would have caught it. In the first six months of production, they identified 73 failure modes that the eval set did not test. They added 114 new eval cases covering those modes and removed 41 existing cases that tested scenarios that never occurred in production. The updated eval set caught 68% of the next month's production failures before they reached users, compared to 41% before the update.

Another source of eval set improvement is user feedback and usage patterns. If users consistently rate certain types of responses poorly even though the system passes eval cases for those scenarios, your eval set is measuring the wrong thing. If users consistently work around the system for certain tasks, your eval set might not be testing those tasks at the right level of difficulty or realism. User research and product analytics should feed directly into eval set updates.

Regulatory and domain knowledge changes also require eval set updates. When new regulations come into effect, when domain standards change, when new research invalidates old practices, your eval set must be updated to reflect current correctness. A healthcare eval set built in 2024 might contain ground truth that is outdated by 2026 clinical guidelines. A financial services eval set built before a regulatory change might contain ground truth that is no longer compliant. Eval set maintenance plans should include regular reviews against current domain standards and regulatory requirements.

The process for proposing eval set changes should be lightweight enough to encourage continuous improvement but rigorous enough to prevent ad hoc degradation. A common pattern is that anyone on the team can propose adding, removing, or modifying eval cases by submitting a documented proposal that explains what is changing, why, and what the expected impact on metrics is. Proposals are reviewed by a designated eval set owner—often the product manager or a senior engineer—and approved or rejected within a defined time frame, typically one week. Approved changes are implemented in a staging version of the eval set, metrics are recomputed, and if the change is validated, it is promoted to the production eval set.

## The Cultural Shift: From Engineering Artifact to Organizational Standard

The most important outcome of rigorous eval set review is not technical. It is cultural. When eval sets are reviewed cross-functionally, documented thoroughly, and maintained continuously, they stop being obscure engineering artifacts that only the ML team understands. They become organizational standards that define shared expectations for quality. Product can point to the eval set when stakeholders ask what quality bar the system must meet. Legal can reference the eval set in compliance audits. Leadership can use eval set metrics to make launch and investment decisions with confidence.

This cultural shift requires consistent investment in making eval sets legible and accessible to non-technical stakeholders. That means documentation that explains what is being tested and why in plain language. That means dashboards that show eval set coverage and pass rates without requiring SQL queries. That means regular communication about eval set updates and what they mean for product quality. That means treating eval set reviews as important meetings that merit senior leadership attendance, not routine check-the-box formalities that junior engineers run on their own.

When eval sets are treated as organizational standards, they create accountability that extends beyond the engineering team. Product cannot claim the system is ready to launch if it is failing eval set requirements without explaining why the eval set is wrong or why the launch should proceed despite failing the agreed-upon quality bar. Leadership cannot pressure the team to ship faster without acknowledging that they are choosing to ship at lower quality than the eval set defines. The eval set becomes the shared ground truth that aligns the organization around what quality means and prevents political pressure from overriding technical reality.

Your eval set is the contract between your team and your stakeholders about what quality means. That contract must be negotiated, documented, and maintained with the same rigor as any other product specification. When that process breaks down—when eval sets are built in isolation, approved without real review, or allowed to go stale—the eval set stops being a quality measure and becomes a false sense of security. The next chapter of eval set failure is recognizing when your eval set itself is wrong, and that is the final discipline we turn to next.
