# 2.12 â€” Collection Cadence: One-Shot vs Continuous Pipelines

In late 2024, a fintech startup built a fraud detection model trained on a carefully curated dataset of 200,000 labeled transactions collected over six months. The model performed exceptionally well during offline evaluation, achieving 96% precision and 94% recall on their held-out test set. They deployed it to production in January 2025 with confidence. By March, precision had dropped to 78% and customer complaints about false positives had tripled.

The problem was not model degradation or deployment bugs. It was data drift. The fraud landscape had shifted dramatically in the two months after dataset collection ended. New attack patterns emerged, legitimate user behavior changed due to seasonal shopping patterns, and payment method preferences evolved. The model was still answering last year's questions while production was asking this year's questions.

The team had treated dataset collection as a one-time project, shipping the model without a plan for continuous data refresh. They spent four months rebuilding the pipeline for continuous collection, retraining monthly, and implementing drift monitoring. The lag between problem recognition and solution cost them an estimated $1.8 million in fraud losses and damaged trust with their payment processing partners.

The failure was not a lack of ML expertise. It was a fundamental mismatch between collection cadence and product requirements. Some datasets can be collected once and remain valid for months or years. Others become stale within days or weeks and require continuous refresh.

The choice between one-shot and continuous collection is not a technical detail. It is a core product architecture decision that determines whether your model stays aligned with reality or drifts into irrelevance. Collection cadence defines the shelf life of your dataset and the operational complexity of keeping your model current.

## One-Shot Collection for Static Benchmarks and Evaluation Sets

One-shot collection assembles a dataset once, freezes it, and uses the same dataset version indefinitely. This makes sense when the underlying distribution is stable, when the dataset serves as a benchmark for comparing approaches over time, or when the cost and complexity of continuous collection outweighs the benefit of freshness.

Evaluation datasets for research benchmarks are almost always one-shot. You collect a representative sample, apply rigorous quality control, publish the dataset with a version number, and never change it. This allows researchers to compare results across papers and reproduce experiments years later.

One-shot collection also makes sense for tasks where the ground truth is inherently stable. A dataset of historical documents for named entity recognition does not become stale because the documents do not change. A dataset of images for object detection remains valid as long as the object categories are stable.

A dataset of code snippets for syntax error detection does not drift because programming language syntax is fixed by specification. If your task involves content that was created in the past and is not being updated, one-shot collection is appropriate.

The advantage of one-shot collection is simplicity. You run the collection pipeline once, invest heavily in quality, labeling, and validation, and produce a definitive dataset version. You do not need orchestration for recurring runs, monitoring for pipeline failures, or infrastructure for incremental updates.

You version the dataset once, archive the collection code, and move on to model development. The dataset becomes a stable artifact that you can trust to remain unchanged.

The disadvantage is obsolescence. If the task distribution changes, if user behavior evolves, if the domain shifts, your one-shot dataset becomes less representative over time. You will not notice this immediately. Model performance in offline evaluation will remain stable because you are evaluating on the same frozen test set.

The degradation appears only in production, where the model encounters data from the current distribution that no longer matches the dataset it was trained on. By the time you detect the problem, the gap may be large enough to require complete dataset rebuilding and retraining.

## Continuous Collection for Production Data and Evolving Distributions

Continuous collection runs the pipeline on a recurring schedule, pulling new data from production systems and refreshing the dataset regularly. This keeps the dataset aligned with the current distribution and allows the model to learn from recent examples.

Continuous collection is essential when your task distribution drifts over time, when production data contains patterns not present in historical data, or when freshness is a competitive advantage.

For fraud detection, continuous collection is non-negotiable. Fraud patterns evolve rapidly as attackers adapt to model defenses. A model trained only on historical fraud examples will miss new attack vectors within weeks. Continuous collection pulls recent transactions daily or weekly, labels new fraud cases as they are confirmed, and retrains the model to incorporate the latest patterns.

This creates a feedback loop where production data continuously improves the model.

For content moderation, continuous collection adapts to evolving language, memes, and adversarial techniques. Harmful content creators actively evade detection by inventing new coded language and visual tricks. A static dataset becomes obsolete as adversaries learn what the model blocks and route around it.

Continuous collection captures new evasion techniques as moderators flag them, keeping the model current with adversarial innovation.

For recommendation systems, continuous collection tracks shifting user preferences and catalog changes. User interests drift seasonally and in response to external events. New products are added and old products go out of stock. A static dataset cannot teach the model to recommend items that did not exist when the dataset was collected.

Continuous collection ingests new user interactions and catalog updates daily, allowing the model to learn from the freshest signals.

The advantage of continuous collection is alignment with reality. Your dataset represents the current state of the world, not a historical snapshot. Model retraining on fresh data reduces drift and keeps performance stable in production. You can detect distribution shifts early by comparing recent data to historical baselines and trigger retraining or intervention before performance degrades visibly.

The disadvantage is complexity. Continuous collection requires orchestration to schedule pipeline runs, monitoring to detect failures and data quality issues, and infrastructure to handle incremental updates and storage growth. You need policies for how much historical data to retain, how to deduplicate across pipeline runs, and how to version datasets when you retrain weekly or daily.

Continuous collection transforms dataset engineering from a project into an operational discipline.

## Hybrid Approaches: Combining Static and Continuous Components

Many production systems use hybrid collection strategies that combine static and continuous components. You maintain a stable core dataset that changes rarely and augment it with a continuously refreshed recent data slice. This balances the stability of one-shot collection with the freshness of continuous updates.

The static core provides coverage of rare cases, edge conditions, and historical diversity that may not appear in recent data. For a customer support classification model, the static core includes examples of uncommon issue types, regional variations, and seasonal edge cases accumulated over years.

This prevents the model from forgetting rare but important patterns just because they have not appeared recently.

The continuous slice provides fresh examples of current patterns, emerging issues, and recent language or behavior shifts. For the same customer support model, the continuous slice includes the last 30 days of production tickets, capturing new product features, recent policy changes, and current customer concerns.

This keeps the model aligned with what customers are asking about today.

You control the mix ratio between static core and continuous slice based on how fast your distribution drifts. For slow-drifting tasks, the static core might represent 80% of the dataset with a 20% continuous slice. For fast-drifting tasks, you might use a 50-50 split or even a smaller static core.

You monitor model performance on recent production data to determine if the continuous slice is large enough to prevent drift.

Hybrid approaches also apply to evaluation datasets. You maintain a static held-out test set for long-term trend analysis and reproducibility, and you continuously collect a recent test set for drift detection. You report model performance on both test sets.

If performance on the static test set remains stable but performance on the recent test set degrades, you know the issue is distribution drift, not model regression. This diagnostic capability is impossible with purely one-shot evaluation.

## Scheduling and Orchestration for Continuous Pipelines

Continuous collection pipelines run on schedules defined by your data freshness requirements and source system update frequencies. You use workflow orchestration tools like Apache Airflow, Prefect, or cloud-native schedulers to trigger pipeline runs daily, weekly, or hourly.

The schedule aligns with when new data becomes available in source systems and when you need updated datasets for retraining.

For daily pipelines, you schedule collection to run after source systems complete their daily batch processing, typically early morning hours when production traffic is low. The pipeline extracts yesterday's data, applies deduplication and validation, merges it with the existing dataset, and writes a new dataset version.

Model retraining runs after dataset updates complete, producing a new model version that incorporates the latest data.

For weekly pipelines, you schedule collection on weekends or during planned maintenance windows when you can afford longer processing times and when source systems are stable. Weekly cadence makes sense when daily updates provide diminishing returns, when labeling throughput cannot keep up with daily data volume, or when retraining is expensive enough that daily runs are not cost-effective.

For hourly or real-time pipelines, you use streaming ingestion rather than batch extraction. Data flows continuously from production systems into the dataset, and incremental updates trigger model retraining or online learning updates. This is rare outside of high-stakes, high-velocity domains like ad bidding or real-time fraud detection, because the infrastructure and operational complexity is substantial.

Orchestration also manages pipeline dependencies. Dataset collection may depend on upstream data exports completing, labeling workflows finishing, or quality checks passing. Your orchestrator monitors these dependencies and triggers the next stage only when prerequisites are met.

If an upstream dependency fails, the orchestrator retries, alerts on-call engineers, and prevents downstream stages from processing incomplete data.

## Incremental Updates vs Full Rebuilds

Continuous pipelines can update datasets incrementally, adding new records to the existing dataset, or rebuild the entire dataset from scratch on each run. Incremental updates are faster and cheaper but introduce complexity in deduplication and versioning. Full rebuilds are slower and more expensive but guarantee consistency and simplify testing.

Incremental updates append new records to the existing dataset after running deduplication against the existing records to avoid introducing duplicates. This requires maintaining an index of existing records and comparing new records against that index, as discussed in the previous subchapter.

Incremental updates preserve the full history of collected data, which grows unbounded unless you implement retention policies to prune old records.

Full rebuilds reprocess all source data from scratch on each pipeline run, applying the latest deduplication and conflict resolution logic to the entire dataset. This is computationally expensive but guarantees that every dataset version is produced by the same logic, making results reproducible and easier to debug.

Full rebuilds also allow you to change merge policies or deduplication thresholds and retroactively apply them to historical data without complex migration logic.

The choice depends on dataset size and processing cost. For datasets under a few hundred thousand records, full rebuilds are fast enough to run daily. For datasets in the millions, incremental updates are necessary to keep pipeline runtime reasonable.

You can also use a hybrid approach: incremental updates daily and full rebuilds weekly or monthly to reset the dataset to a known clean state.

Versioning strategies differ between incremental and full rebuilds. With full rebuilds, each pipeline run produces a new dataset version with a unique identifier. With incremental updates, you either version on each update or version only when the dataset changes materially, such as when a new source is added or when the schema changes.

You store version metadata alongside the dataset so that model training runs can reference the exact dataset version they used.

## Retention Policies and Dataset Growth Management

Continuous collection causes datasets to grow unbounded unless you implement retention policies that prune old data. Retention policies define how long records remain in the dataset before being archived or deleted. The retention period balances the value of historical data for coverage and diversity against the cost of storage and processing.

For tasks where recent data is most predictive, you retain only the last N days or N records. A fraud detection dataset might retain the last 90 days of transactions, assuming that fraud patterns older than 90 days are no longer relevant.

A content moderation dataset might retain the last 180 days of flagged content, balancing the need for recent evasion techniques with coverage of rare violation types that appear infrequently.

For tasks where rare historical cases are valuable, you retain a stratified sample of old data while keeping all recent data. You might keep 100% of the last 30 days, 50% of data from 30 to 90 days ago, and 10% of data older than 90 days, sampled to preserve rare classes and edge cases.

This prevents the dataset from forgetting rare patterns while controlling growth.

You implement retention policies in the pipeline logic, not as manual cleanup. Each pipeline run checks record timestamps and removes or archives records outside the retention window. Archived records are stored separately in cheaper storage, accessible for analysis but not included in model training.

This keeps the active dataset size stable over time while preserving a complete historical archive for audit or retrospective analysis.

Retention policies interact with deduplication. When you remove old records, you may create duplicates if a previously deduplicated record is deleted and a duplicate from a later source remains. You rerun deduplication after applying retention policies to ensure the dataset remains duplicate-free.

## Mapping Collection Cadence to Product Requirements

The appropriate collection cadence is determined by your product requirements, not by what is technically easiest. You analyze how fast your task distribution drifts, how quickly user behavior changes, and what performance degradation is acceptable before retraining. This analysis drives the cadence decision.

If your product serves a domain where ground truth changes weekly, you need at least weekly collection. If your product operates in a stable domain where changes happen over months, quarterly collection may suffice. If your product competes on being current with the latest trends, daily or hourly collection is a competitive requirement.

If your product values consistency and reproducibility over recency, one-shot collection with infrequent updates is appropriate.

You validate cadence decisions empirically by measuring model performance over time with different collection schedules. You run an experiment where you train models on datasets collected daily, weekly, and monthly, deploy them in production, and measure performance degradation over time.

The cadence that keeps performance within acceptable bounds while minimizing operational cost is the correct choice.

Product requirements also determine how much latency you can tolerate between data generation and dataset inclusion. For real-time fraud detection, you might need data from yesterday to be in today's training set, requiring daily collection with overnight processing.

For a research benchmark, you can tolerate months of latency because the dataset serves long-term comparisons, not current production needs.

Collection cadence is not fixed forever. You revisit the decision when product requirements change, when the task distribution starts drifting faster or slower, or when new data sources become available. You treat cadence as a tunable parameter that you monitor and adjust based on performance metrics and operational cost.

## Measuring Distribution Drift to Inform Cadence

Distribution drift is the enemy that continuous collection fights against. You measure drift explicitly to determine whether your current collection cadence is sufficient or whether you need to increase frequency. Drift detection compares the distribution of recent production data to the distribution of your training dataset.

Statistical drift metrics quantify how much the distribution has changed. Population stability index measures how categorical feature distributions have shifted. Kolmogorov-Smirnov tests detect changes in continuous feature distributions. Kullback-Leibler divergence measures the difference between probability distributions.

You compute these metrics weekly or monthly on recent production data compared to your training data baseline.

When drift metrics exceed predetermined thresholds, you trigger alerts and investigate whether the drift represents real world changes that require retraining or data quality issues that need fixing. Real world drift means your model is operating on a distribution it was not trained for, and performance will degrade.

Data quality drift means something broke in your data pipeline or source systems, and you need to fix the root cause before retraining.

Performance-based drift detection monitors production metrics like precision, recall, accuracy, or business KPIs over time. If metrics degrade steadily without code changes or deployment issues, distribution drift is the likely cause. You correlate performance degradation with the time elapsed since the last dataset refresh to estimate how quickly your distribution drifts.

If performance degrades 10% after two months, you know you need to refresh at least every two months to maintain acceptable quality.

Slice-based drift analysis breaks down drift by data slices or segments. You might find that the overall distribution is stable but a specific geographic region, user demographic, or product category is drifting rapidly. This informs targeted collection strategies where you refresh specific slices more frequently than others, rather than refreshing the entire dataset uniformly.

## Monitoring Collection Pipelines for Failures and Drift

Continuous pipelines fail in ways that one-shot pipelines do not. Sources go offline, schemas change, extraction queries break, and labeling backlogs accumulate. You monitor pipeline health continuously and alert when failures occur or when data quality degrades.

You monitor pipeline execution metrics: runtime duration, records processed, deduplication rate, and schema validation pass rate. Deviations from baseline trigger alerts. If a pipeline that normally processes 10,000 records per day suddenly processes 500, a source may have failed or a date filter may be misconfigured.

If deduplication rate jumps from 5% to 40%, a source may be producing duplicate exports or your deduplication logic may have regressed.

You monitor data quality metrics: null rate per field, distribution of categorical values, numeric value ranges, and text length distributions. If null rates spike, a source schema may have changed. If categorical value distributions shift suddenly, new categories may have been introduced or data may be corrupted.

You compare these metrics to historical baselines and alert when they exceed thresholds.

You monitor data drift by comparing recent data to a reference distribution from the initial dataset collection. You compute distribution metrics like KL divergence, population stability index, or simple summary statistics like mean, median, and quantile values for key features.

Significant drift indicates that the current data no longer matches the distribution your model was trained on, signaling a need for retraining or for investigation into whether the drift represents real distribution change or data quality issues.

Pipeline monitoring is not passive dashboarding. It triggers automated responses. When a pipeline run fails, the orchestrator retries with exponential backoff. When retries are exhausted, it alerts on-call engineers and pauses downstream stages to prevent incomplete data from reaching production.

When data quality metrics degrade, the pipeline quarantines the affected batch and routes it for manual review rather than merging it into the dataset automatically.

## Cost and Complexity Trade-offs in Collection Cadence

Continuous collection is more expensive than one-shot collection in both infrastructure cost and engineering effort. You pay for compute and storage on every pipeline run. You pay for engineering time to build orchestration, monitoring, and failure recovery.

You pay for operational overhead to investigate failures and maintain the pipeline as sources evolve. These costs are justified when the value of fresh data exceeds the cost of continuous operation.

Infrastructure cost scales with pipeline frequency and dataset size. Daily pipelines cost roughly 30 times more than monthly pipelines in compute alone, though incremental processing and caching reduce the multiplier. Storage cost grows linearly with retention period.

If you retain 90 days of data, you store three times as much as if you retain 30 days. You balance cost against freshness by tuning retention policies and pipeline frequency to the minimum cadence that maintains acceptable model performance.

Engineering effort to build continuous pipelines is front-loaded but ongoing. You invest weeks to build the orchestration, monitoring, and incremental update logic. Once built, you invest hours per week to maintain the pipeline as schemas change, as sources are added or removed, and as quality issues arise.

This ongoing effort is the price of keeping your model current. You amortize it across the value of the model. A model that drives millions of dollars in revenue justifies substantial pipeline investment. A model that supports a low-stakes internal tool does not.

Operational complexity increases with pipeline frequency. Daily pipelines mean daily opportunities for failure and daily monitoring burden. Hourly pipelines mean you need 24/7 on-call coverage for pipeline failures. You reduce complexity by designing pipelines to be self-healing: retries on transient failures, graceful degradation when sources are partially unavailable, and automatic rollback when data quality checks fail.

You also reduce complexity by consolidating pipelines: one pipeline that collects from multiple sources on a common schedule is simpler than multiple independent pipelines with different schedules.

## Labeling Throughput as a Cadence Constraint

Your collection cadence cannot exceed your labeling throughput. If you collect 10,000 new examples daily but your labeling team can only label 2,000 examples per day, unlabeled data accumulates faster than you can process it. This creates a growing backlog that delays model updates and wastes collection effort.

You must match collection volume to labeling capacity or implement strategies to increase labeling throughput.

Labeling capacity depends on whether you use human annotators, automated labeling, or a hybrid approach. Human annotators have fixed throughput determined by task complexity and available labor hours. If each annotation takes 2 minutes and you have 10 annotators working 8 hours per day, your maximum throughput is 2,400 annotations per day.

You cannot sustain daily collection of more than 2,400 examples without hiring more annotators or simplifying the labeling task.

Automated labeling using models or heuristics scales more easily but introduces quality trade-offs. You might use a model to predict labels automatically for high-confidence cases and route low-confidence cases to human review. This increases effective throughput by handling easy cases automatically while preserving quality on difficult cases.

The automation rate determines your effective labeling capacity. If 70% of cases can be automated, your effective throughput increases by 3.3 times.

Active learning prioritizes which examples to label based on informativeness. Instead of labeling every new example, you label only the examples that are most likely to improve model performance: cases where the current model is uncertain, cases that represent underrepresented slices, or cases that are dissimilar to existing training data.

This increases the value extracted per labeled example, allowing you to maintain model performance with lower labeling throughput.

Weak supervision and programmatic labeling use rules, heuristics, or crowd-sourced signals to generate noisy labels at scale. You trade label precision for label volume. If your task can tolerate 80% label accuracy instead of 95%, weak supervision might provide 10 times the labeling throughput.

You combine weak labels with a small set of high-quality labels to calibrate the noise and denoise during training.

## Testing Continuous Pipelines: Reproducibility and Determinism

Continuous pipelines must produce deterministic, reproducible results despite running repeatedly over time. You test reproducibility by running the pipeline multiple times on the same source data and asserting that the output is identical. You test determinism by verifying that pipeline behavior does not depend on execution time, network conditions, or transient state.

Reproducibility requires deterministic ordering of records, deterministic deduplication logic, and deterministic conflict resolution. If your pipeline uses timestamps or random sampling, you seed the random number generator or replace timestamps with stable values during testing.

If your pipeline depends on external services for enrichment or labeling, you mock those services in tests to eliminate nondeterministic responses.

You build integration tests that simulate continuous operation by running the pipeline on synthetic data with controlled updates. You create an initial dataset, run the pipeline to simulate day one, add new records, run the pipeline again to simulate day two, and assert that the resulting dataset matches expectations.

You test retention policies by advancing the simulated date and verifying that old records are pruned correctly. You test deduplication across pipeline runs by introducing duplicates in later runs and verifying they are detected and removed.

You also test pipeline failure scenarios. You simulate source failures, network timeouts, schema changes, and disk space exhaustion to verify that the pipeline handles errors gracefully and does not corrupt the dataset. You verify that partial runs do not leave the dataset in an inconsistent state and that retries after failure produce the same result as a successful first attempt.

Testing continuous pipelines is harder than testing one-shot pipelines because you must account for state changes over time. You version test fixtures alongside pipeline code and regenerate expected outputs whenever pipeline logic changes, just as you version datasets in production.

## When to Switch from One-Shot to Continuous

You start with one-shot collection when building a new model or exploring a new task. One-shot collection is faster to implement and sufficient for initial model development and evaluation. You switch to continuous collection when one of three conditions appears: production performance degrades over time despite stable offline evaluation, new data arrives regularly from production that is materially different from the initial dataset, or product requirements demand freshness as a feature.

The switch from one-shot to continuous is a planned engineering project, not a reactive patch. You design the continuous pipeline, build the orchestration and monitoring, test it on historical data, and run it in parallel with your existing dataset for a validation period.

You compare models trained on the one-shot dataset versus models trained on the continuous dataset and verify that the continuous dataset produces equal or better performance. Once validated, you retire the one-shot dataset and make the continuous pipeline the source of truth.

You do not switch to continuous collection just because it sounds more sophisticated. Continuous collection is operationally expensive and justified only when the value of freshness exceeds the cost of operation. If your model performance is stable over months with a static dataset, if your domain does not drift rapidly, and if your product does not require recency, one-shot collection with periodic manual refreshes is a valid long-term strategy.

Collection cadence is not a one-time decision. You monitor model performance and data drift continuously and revisit the cadence decision when conditions change. You start with one-shot, switch to continuous when drift appears, and tune the frequency as you learn how fast your distribution evolves.

The right cadence is the one that keeps your model aligned with reality at acceptable operational cost, and that balance shifts as your product and domain mature.

Your collection pipeline now handles multi-source merging with robust deduplication and conflict resolution, and it runs on a cadence aligned with your product's freshness requirements. The next step is managing the volume and velocity of incoming data with smart sampling strategies that preserve signal while controlling cost.
