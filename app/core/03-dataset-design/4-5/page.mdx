# 4.5 — Handling Missing, Incomplete, and Contradictory Records

In March 2025, a healthcare analytics company launched a clinical decision support tool trained on electronic health records from eleven hospital systems. The model was designed to predict patient deterioration risk based on vital signs, lab results, and medication history. During the first month of deployment, clinicians reported that the model frequently flagged low-risk patients as high-risk and missed several patients who actually deteriorated. The data science team investigated and found that roughly 35% of records in their training data had missing values for one or more critical fields — blood pressure readings were absent, lab results were pending, medication lists were incomplete. The team had used simple mean imputation to fill missing values during training, replacing missing blood pressure with the average blood pressure across all patients. This introduced systematic bias because missing values were not random — they were more common in emergency admissions, in patients transferred between units, and in records from older systems with incomplete digitization. The model had learned patterns from imputed values that did not reflect reality. The team spent three months rebuilding their pipeline to handle missingness explicitly, using indicator variables to flag missing fields and training the model to make predictions even when data was incomplete. The revised model improved risk prediction accuracy by 11 percentage points and reduced false positives by 40%.

Missing data is not a data quality failure — it is a structural characteristic of real-world datasets that you must handle deliberately. Fields are missing because users skip optional form fields, because systems fail to record data, because integration pipelines drop fields during transformation, because legacy systems do not collect certain data types, and because data is genuinely unavailable at the time of prediction. Your model will encounter missing data in production, so your training data should reflect realistic missingness patterns, and your model should learn to handle incomplete information rather than relying on synthetic completeness created through imputation.

The distinction between training-time handling and inference-time handling is critical. At training time, you have the luxury of analyzing missingness patterns, deciding which records to keep, and choosing imputation strategies that preserve signal. At inference time, you must make predictions on whatever data you receive, missing fields and all. If your training data contains no examples with missing fields because you imputed everything, your model has never learned to handle missingness and will behave unpredictably when fields are missing in production. You should preserve realistic missingness in your training data so the model learns robust patterns.

## Patterns of Missingness

Missing completely at random (MCAR) occurs when the probability that a value is missing is independent of both observed and unobserved data. A sensor fails randomly and drops 2% of readings with no pattern. A user accidentally skips a form field with no correlation to any other variable. MCAR is the least problematic form of missingness because it does not introduce bias — the missing data is a random sample of the complete data. You can often ignore MCAR missingness or impute it with simple strategies without harming model performance.

In practice, true MCAR is rare. Most missingness has some pattern, even if subtle. A sensor that fails randomly in theory may actually fail more often under certain environmental conditions. A form field that appears to be skipped randomly may be skipped more often by users on mobile devices. You should test the MCAR assumption by comparing the distribution of observed variables between records with and without missing values. If records with missing field A have a different distribution of field B than records where A is present, the missingness is not completely random.

Missing at random (MAR) occurs when the probability that a value is missing depends on observed data but not on the missing value itself. A patient's blood pressure reading is more likely to be missing if they were admitted through the emergency department, but conditional on admission route, missingness is random. A user's income field is more likely to be missing if they are younger, but conditional on age, missingness is random. MAR is more common than MCAR and is often manageable because you can model the missingness using observed features.

You handle MAR by including the variables that predict missingness as features in your model, often using indicator variables that explicitly flag when a field is missing. If blood pressure is MAR conditional on admission route, you include admission route as a feature and add a binary indicator variable that is true when blood pressure is missing. The model can then learn different patterns for emergency admissions with missing BP versus emergency admissions with observed BP. This allows the model to account for the selection bias introduced by non-random missingness.

Missing not at random (MNAR) occurs when the probability that a value is missing depends on the unobserved value itself. A patient's pain score is more likely to be missing when pain is very severe because the patient cannot complete the assessment. A user's satisfaction rating is more likely to be missing when satisfaction is very low because dissatisfied users disengage. A financial transaction amount is more likely to be missing when the amount is unusually large because it triggers additional review. MNAR is the most problematic form of missingness because the missing data is systematically different from the observed data, and you cannot fully correct for this bias using observed variables alone.

You cannot definitively prove that data is MNAR from the data itself, because by definition the information you need is missing. You identify MNAR through domain knowledge — understanding the data collection process and the mechanisms that cause missingness. If you know that nurses are instructed to skip recording vital signs when a patient is unstable and being rushed to surgery, you know that missing vitals are MNAR. If you know that users abandon surveys when they become frustrated, you know that missing survey responses are MNAR.

Handling MNAR requires explicit modeling of the missingness mechanism. One approach is to use domain knowledge to create proxy variables that correlate with the missing values. If pain scores are missing when pain is severe, you might use other indicators of severity — such as opioid administration or emergency interventions — as proxies. Another approach is to model the missingness directly using a selection model or pattern-mixture model, though these require strong assumptions and are more complex to implement. In some cases, the most honest approach is to acknowledge that you cannot reliably predict outcomes for records with MNAR missingness and to flag those predictions as uncertain.

Systematic missingness occurs when entire fields or data sources are missing for specific subsets of your data. Records from one hospital system lack medication history because that system does not integrate pharmacy data. Records before a specific date lack a field because the field was added later. Records from international users lack geographic detail because privacy regulations prohibit collection. Systematic missingness is different from random or MAR missingness because it creates subpopulations with fundamentally different data availability.

You handle systematic missingness by training separate models for different data availability regimes, by using model architectures that can handle variable input schemas, or by explicitly including data source or time period as features so the model learns context-specific patterns. If 30% of your data comes from a source that never provides field X, you should not impute X for those records — you should train the model to make predictions without X for that subpopulation. You may need to evaluate model performance separately for each subpopulation to ensure that performance is acceptable even when data is incomplete.

## Imputation Strategies for Different Data Types

Simple imputation strategies include mean imputation, median imputation, and mode imputation. Mean imputation replaces missing numeric values with the mean of observed values. Median imputation uses the median. Mode imputation uses the most common value for categorical variables. These strategies are fast and easy to implement, but they distort the distribution of the variable, eliminate variance, and break correlations between features. Mean imputation makes all imputed values identical, which is unrealistic and can introduce artifacts that the model learns.

You should use simple imputation only when missingness rates are very low — under 5% — and when the variable is not critical to the prediction task. If a minor feature has 3% missing values and you replace them with the mean, the distortion is minimal. If a critical feature has 30% missing values and you replace them with the mean, you have fundamentally altered the dataset and introduced a strong artificial signal. Simple imputation should never be your default strategy for important features with substantial missingness.

Indicator variable augmentation is a better approach for many cases. Instead of replacing the missing value, you add a binary indicator variable that flags whether the value was missing, and you replace the missing value with a neutral default — such as zero for numeric variables after normalization, or a special "missing" category for categorical variables. The model can then learn separate patterns for records with observed values versus records with missing values. This preserves the information that the value was missing, which is often informative in itself.

For example, if you have a feature for "number of previous purchases" and it is missing for some users, you add an indicator variable "previous purchases missing" and set the number of previous purchases to zero for those users. The model learns to use the indicator variable to distinguish new users — where zero purchases is expected — from users where purchase history is unavailable for other reasons. This approach works well for MAR and MNAR missingness where the fact that a value is missing carries information.

Model-based imputation uses statistical or machine learning models to predict missing values based on observed values. You train a regression model to predict missing numeric values or a classification model to predict missing categorical values, using the other features in the record as predictors. This preserves correlations between features and produces more realistic imputed values than mean imputation. You can use linear regression, decision trees, k-nearest neighbors, or more sophisticated methods like multiple imputation by chained equations (MICE).

Model-based imputation is more accurate than simple imputation but introduces complexity and risk. The imputation model itself can be wrong, and errors in imputation propagate into your downstream model. If you impute missing income using a model that has learned biased patterns, you inject that bias into your final predictions. You should evaluate imputation accuracy separately and should avoid imputing values that are critical to your prediction task — if the target variable is highly correlated with a feature, imputing that feature is risky.

Multiple imputation is a principled statistical approach where you generate multiple plausible values for each missing entry, creating multiple complete datasets. You train your model on each imputed dataset separately, then combine the predictions. This accounts for uncertainty in the imputed values and produces more honest confidence intervals. Multiple imputation is computationally expensive and complex to implement, but it is the gold standard for handling missing data in high-stakes applications where uncertainty quantification matters.

For categorical variables with missing values, you should consider treating "missing" as its own category rather than imputing. If a user did not provide their job title, "job title missing" is a legitimate category that may have predictive value. Users who skip optional fields may differ systematically from users who complete them. Creating an explicit missing category preserves this signal. This approach works well when missingness is common and when the fact that a user did not provide a value is informative.

For text fields with missing values, imputation is rarely appropriate. You cannot meaningfully impute a missing product review or a missing clinical note. Instead, you should use model architectures that handle variable-length inputs gracefully, such as attention mechanisms that can operate on whatever text is available. If a text field is missing, you pass an empty string or a special token indicating absence, and the model learns to make predictions without that field.

## Handling Contradictions Between Sources

Data contradictions occur when you have multiple sources of information for the same fact, and those sources disagree. One system records a patient's medication as prescribed on March 3, another records it as March 5. One user profile lists the account creation date as January 2024, another lists it as February 2024. One product review says "shipped quickly," the user's order history shows a three-week delay. Contradictions arise from data entry errors, synchronization delays between systems, different definitions of the same field, and genuine ambiguity in the real world.

The first step in handling contradictions is to determine whether the sources are actually measuring the same thing. If two systems use different definitions of "account creation date" — one recording the date the email was verified, the other recording the date the first login occurred — they are not contradictory, they are measuring different events. You should rename the fields to reflect their true semantics and treat them as separate features. If the sources are genuinely measuring the same underlying fact and disagree, you have a true contradiction.

Source prioritization is the simplest resolution strategy. You designate one source as authoritative and always use its value when conflicts arise. The system of record for medication data is the pharmacy system, so you always use pharmacy records over clinical notes when they disagree. The authoritative source for user profile data is the user's most recent update, so you always use the latest value. Source prioritization works well when you have a clear hierarchy of data quality and when one source is reliably more accurate than others.

You should document your source priority rules explicitly and apply them consistently. If you sometimes prioritize source A and sometimes prioritize source B based on ad hoc judgments, you introduce inconsistency and make your data pipeline fragile. A clear rule — "for field X, always use source A; if source A is missing, use source B; if both are missing, use source C" — is auditable and reproducible.

Recency-based resolution uses the most recent value when sources disagree. If two systems have different values for a user's address, you use the value from the system that was updated most recently. This assumes that the most recent information is most accurate, which is often true for fields that change over time. Recency-based resolution works well for user profiles, account settings, and other data that is actively maintained. It works poorly for historical data where the most recent source may be an error or a correction that should not be back-propagated.

Voting-based resolution uses majority agreement when you have three or more sources. If five annotators labeled an example and four agree on label A while one says label B, you use label A. If three data sources provide a patient's weight and two agree on 180 pounds while one says 280 pounds, you use 180 pounds. Voting works well when sources are independent and when errors are random. It works poorly when errors are systematic — if four sources all derive from a single upstream system and one source is independent, the independent source may be correct even though it is outvoted.

Confidence-weighted resolution uses metadata about source reliability to resolve conflicts. Each source has an associated confidence score based on historical accuracy, and when sources disagree, you use the value from the highest-confidence source. If source A is correct 95% of the time and source B is correct 70% of the time, you trust source A when they conflict. You can estimate confidence scores from validation data by comparing each source against a ground truth set and computing accuracy.

This approach requires maintaining and updating confidence scores, which adds complexity. Confidence scores should be computed separately for different fields and different contexts — a source may be highly reliable for one type of data and unreliable for another. You should re-evaluate confidence scores periodically as data quality changes over time.

Preserving conflicts as features is an alternative to resolving them. Instead of choosing one value, you include both values as separate features and let the model learn how to handle the disagreement. You add features like "medication date source A," "medication date source B," and "medication date sources agree." The model can learn that when sources agree, the date is reliable, and when they disagree, the data is uncertain. This approach works well when the contradiction itself is informative — for example, disagreement between systems may indicate data quality issues that correlate with other problems.

Escalation to manual review is the safest approach for high-stakes contradictions. If two medical systems disagree on a patient's allergy status, you do not resolve this automatically — you flag it for review by a clinician. If two financial systems disagree on a transaction amount by more than a threshold, you escalate to a human auditor. Automatic resolution is efficient, but it can propagate errors in safety-critical domains. You should define clear escalation rules based on the severity of the field and the magnitude of the disagreement.

## When Missing Data Is a Signal

Missingness is often informative — the fact that a field is missing tells you something about the record or the user. A missing phone number may indicate a privacy-conscious user. A missing previous employer may indicate a recent graduate. A missing lab result may indicate that the test was not ordered, which itself suggests something about the patient's condition. You should not automatically impute away this signal — you should preserve it and let the model learn from it.

In credit risk modeling, missing employment history is a strong signal. Applicants with stable employment provide complete histories. Applicants with gaps or frequent job changes are more likely to leave fields blank. If you impute missing employment fields, you erase a feature that predicts default risk. You should instead use indicator variables to flag missingness and include the fact of missingness as a feature.

In medical prediction tasks, missing test results often indicate that the test was not clinically indicated. If a patient does not have a recent cholesterol measurement, it may mean their physician assessed them as low-risk. If you impute missing cholesterol with the population mean, you treat low-risk patients as if they have average cholesterol, which is misleading. You should train the model to recognize that absent tests are informative, not random.

In user behavior modeling, missing interactions indicate disengagement. A user who has not logged in for 90 days is different from a user who logs in daily but has missing data due to a tracking failure. If you impute missing login timestamps, you lose the ability to distinguish active users from disengaged users. You should preserve the absence of events as a signal.

The key question is whether missingness is predictive of your target variable. If records with missing field X have a systematically different distribution of your target than records where X is present, missingness is informative. You should test this by computing target statistics separately for records with and without missing values. If patients with missing blood pressure have a 12% mortality rate while patients with observed blood pressure have a 4% mortality rate, missingness is highly informative and should be preserved.

When missingness is informative, you should use indicator variables, treat missing as a category, or use model architectures that explicitly handle sparsity. You should not use imputation strategies that erase the information that a value was missing. Your evaluation metrics should separately report performance on records with and without missing values to ensure the model performs well across both groups.

## Incomplete Records and Minimum Viable Data

Some records are so incomplete that they provide no useful signal for training. A customer support ticket with no text, no category, and no metadata is not trainable. A medical record with only a patient ID and no clinical information is not useful. You should define minimum viable data requirements for each task — the set of fields that must be present for a record to be included in training data.

Minimum viable data depends on your task definition. For a document classification task, the minimum viable record might be "at least 20 characters of text and a valid label." For a medical risk prediction task, it might be "at least two of the following: vital signs, lab results, or medication list, plus age and admission type." You should determine these requirements based on what information a human would need to make the same judgment you are asking the model to make.

Records that do not meet minimum viable data thresholds should be excluded from training. They add noise without adding signal, and they force the model to learn from uninformative examples. You should track the exclusion rate — if you are excluding more than 20% of records due to incompleteness, you have a data collection problem that should be addressed upstream. You should not relax your minimum viable data requirements to avoid discarding data — that leads to training on garbage.

At inference time, you cannot exclude records — you must make predictions on whatever data arrives. If a production record does not meet minimum viable data requirements, you should flag the prediction as low-confidence, return a default response, or escalate to human review. Your model should be trained on some incomplete records so it learns to handle them, but the training distribution should reflect realistic production incompleteness, not extreme cases that rarely occur.

Handling missing, incomplete, and contradictory records is a structural part of dataset engineering. You analyze missingness patterns to understand whether data is missing randomly or systematically. You choose imputation strategies that preserve signal rather than distorting distributions. You resolve contradictions using source prioritization, voting, or confidence weighting. You preserve missingness as a feature when it is informative. You exclude records that do not meet minimum data requirements while tracking exclusion rates. The next challenge is ensuring that the text itself — the language in your dataset — meets quality standards for grammar, fluency, and register consistency.
