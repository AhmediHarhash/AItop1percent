# Section 3 — Dataset Design & Curation

## Chapter 1

### Plain English

A dataset, in this system, is not "training data."

A dataset here means:

**A carefully chosen set of real examples that represent what your AI system must handle, so you can measure quality, catch regressions, and improve safely.**

If Section 02 defined what "good" means, Section 03 defines:

**What situations we will test "good" on.**

In 2026, the fastest teams ship safely because they have strong datasets.
The fastest teams that crash and burn usually don't.

---

### Why This Section Exists

AI systems fail on the cases you did not include.

Without a real dataset strategy:
- you only test happy paths
- you overfit to demos
- regressions slip through
- safety issues appear in production first (worst case)
- "it worked yesterday" becomes a weekly disaster

Dataset design exists to:
- represent reality, not theory
- cover normal + edge cases
- create repeatable measurements
- make improvements measurable
- protect against silent quality decay

---

### What "Dataset" Means for Evals (Not ML Textbooks)

In eval systems, datasets are **evaluation sets** (decision sets), not training sets.

They are used to:
- test changes (prompts/models/tools)
- compare versions
- define release gates
- monitor ongoing quality

You may later create training datasets (fine-tuning), but that is not the first purpose here.

In 2026, many teams ship great products with little or no fine-tuning,
because their eval datasets drive the right improvements.

---

### Types of Datasets You Need (The Full Map)

You do not need one dataset. You need a **portfolio**.

#### 1) Golden Dataset (Core Coverage)
The golden dataset is:
- small to medium
- high quality
- stable
- representative of core tasks

Purpose:
- release gating
- regression detection
- "if this fails, we do not ship"

This is the dataset you protect like gold.

---

#### 2) Long-Tail Dataset (Real World Pain)
This dataset targets:
- rare edge cases
- weird formatting
- ambiguous requests
- multi-step complexity
- unusual user behavior

Purpose:
- prevent embarrassing failures
- build robustness
- reduce "it breaks in production" events

Long-tail is where quality leadership lives.

---

#### 3) Adversarial / Red-Team Dataset (Attack Reality)
This dataset contains:
- prompt injection attempts
- tool abuse attempts
- data exfiltration patterns
- jailbreak-like instructions
- "act as admin" tricks
- malicious instructions hidden in documents

Purpose:
- safety gates
- security posture
- enterprise trust

In 2026, this is non-negotiable for serious products.

---

#### 4) Drift Dataset (Change Over Time)
This dataset evolves from production:
- new user intents
- new vocabulary
- new failure patterns
- seasonal / trend shifts

Purpose:
- keep the system relevant
- prevent decay over time
- adapt to changing reality

If you don't have drift capture, your dataset becomes a museum.

---

#### 5) Per-Customer / Per-Tenant Dataset (Multi-Tenant Reality)
If you serve multiple customers:
- each customer has different policies
- different data patterns
- different "good" definitions

Purpose:
- ensure you don't optimize for one customer and break another
- allow customer-specific gates where needed

Even if you start with one product, design for this early.

---

#### 6) Tool-Execution Dataset (Action Correctness)
If your system calls tools:
- wrong tool calls are worse than wrong text
- wrong arguments cause real damage

This dataset includes:
- correct tool choice
- correct argument schema
- correct sequencing
- correct confirmations

Purpose:
- measure tool success rate
- prevent dangerous autonomy
- make agents reliable

---

#### 7) Voice / Real-Time Dataset (Latency + Flow)
If voice is involved, include:
- interruptions
- barge-in
- noisy audio conditions (conceptual scenarios)
- fast turn-taking
- misunderstanding recovery

Purpose:
- measure real-time behavior, not just text quality
- prevent "it works in chat but fails on calls"

---

### Dataset Sourcing (Where Examples Come From)

A 2026-grade dataset portfolio typically comes from four sources:

#### Source A — Production Logs (Best Source)
- real user inputs
- real failure cases
- real distribution of requests

This is the strongest source when you have it.

#### Source B — SME / Expert Crafted Examples
- domain experts provide gold-standard cases
- good for early stage or regulated domains

#### Source C — Synthetic Data (Use Carefully)
AI-generated examples can help cover:
- rare edge cases
- adversarial patterns
- scenario variety

But synthetic data can also introduce unrealistic patterns.

Rule:
Use synthetic to widen coverage, not to replace reality.

#### Source D — Historical Support Tickets / FAQs / Knowledge Base
Excellent for:
- real customer questions
- common pain points
- "what users actually ask"

---

### Dataset Quality Principles (Non-Negotiable)

#### 1) Represent the Real Distribution
Your dataset must match:
- the frequency of common tasks
- the reality of messy inputs
- the real mix of short/long queries
- different user styles

If your dataset is too "clean," your AI will look great and fail fast.

---

#### 2) Coverage of Task Taxonomy
Every dataset entry must be associated with a task type:
- chat
- RAG
- tool calling
- agent workflow
- voice

If you don't label task type, you cannot evaluate properly.

---

#### 3) Clear Expected Outcome
Each entry must have:
- what success means (from Section 02)
- what counts as failure
- what uncertainty behavior is acceptable

Even when answers vary, you must define what "acceptable" means.

---

#### 4) Balance Stability vs Freshness
- Golden dataset should be stable to detect regressions.
- Drift dataset should evolve to stay relevant.

You need both.

---

### Dataset Unit of Record (What One "Example" Contains)

A single dataset item in 2026 often includes:

- input (user query)
- context (conversation state, if relevant)
- retrieved documents (if RAG is involved)
- tool availability (which tools exist)
- expected tool calls (if tools are required)
- constraints (policies, tenant rules)
- expected outcome (pass criteria)
- scoring rubric reference
- risk tier (Tier 0–3)
- metadata (source, timestamp, domain)

This is what allows:
- reproducible evaluation
- fair comparisons across versions
- enterprise defensibility

---

### Dataset Versioning (Critical)

Datasets are living assets.
You must version them like code.

A minimal versioning practice:
- each dataset has a version number
- each change has a reason
- each added case links to:
  - a production failure
  - a new requirement
  - a new risk

Rule:
**Never silently redefine reality.**

Versioning prevents "moving goalposts."

---

### Sampling Strategy (How Big Should It Be?)

You do not need millions of examples to start.

A practical 2026 starting point:

- Golden dataset: 100–500 high-quality cases
- Long-tail dataset: 100–300 cases
- Adversarial dataset: 50–200 cases
- Tool dataset: 50–200 cases
- Voice dataset: 50–200 cases (if voice)
- Drift dataset: continuously growing

Quality beats quantity early.

As you scale, you grow coverage gradually.

---

### Enterprise Needs (What Enterprises Expect)

Enterprises expect:
- traceability (why this example exists)
- auditability (who approved it)
- tenant separation (customer-specific rules)
- compliance awareness (PII handling)
- consistency over time

A strong dataset system supports:
- regulated deployments
- legal defensibility
- procurement confidence

This is why eval datasets become strategic assets.

---

### Founder Needs (What Startups Need)

Founders need:
- speed without breaking trust
- learning loops from production
- datasets that drive iteration

A good dataset portfolio gives you:
- faster improvement
- fewer incidents
- better product-market fit feedback
- stronger demos (because you can show measurable progress)

---

### Common Failure Modes (What to Avoid)

- dataset is only happy paths
- dataset is too small and never grows
- dataset is full of synthetic fluff
- no long-tail coverage
- no adversarial coverage
- no tool-call correctness tests
- no versioning
- mixing tasks without tagging them
- using "accuracy" without defining what accuracy means

These are top causes of AI products failing at scale.

---

### Interview-Grade Talking Points

You should be able to explain:

- the difference between eval datasets and training datasets
- why you need a portfolio (golden, long-tail, adversarial, drift)
- how production failures feed dataset growth
- why versioning matters
- how datasets connect to release gates

If you can explain this cleanly, you're in the top tier.

---

### Completion Checklist

You are done with this section when you can:

- design a dataset portfolio for any AI product
- explain why each dataset type exists
- define what one dataset item must contain
- explain dataset versioning and why it matters
- explain how datasets connect to release gates and monitoring

Do not move on until this feels obvious.

---

### How This Connects Forward

Now that we have datasets, the next question is:

**How do we label and annotate them so humans and machines agree?**

That is Section 04 — Labeling & Annotation.
