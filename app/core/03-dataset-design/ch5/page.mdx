# Chapter 5 — Dataset Versioning, Lineage, and Infrastructure

Datasets change. Models trained on different dataset versions produce different results, yet most teams have no way to know which version a model was trained on or whether they can recreate that exact dataset state six months later. Without versioning, lineage tracking, and proper storage infrastructure, you cannot reproduce results, debug failures, or roll back safely. When a data pipeline breaks, you lose weeks to detective work instead of minutes to remediation. When a feature turns out to be poisoned or incorrect, you cannot isolate which trained models are affected. When regulations require audit trails, you have nothing.

This chapter covers versioning strategies (snapshot, delta, and semantic approaches), tooling for dataset versioning in 2026 (DVC, LakeFS, and cloud-native options), and how to build lineage tracking systems that connect source tables to pipelines to model artifacts. We also cover storage architecture, lakehouse formats, and the practical mechanics of reproducibility, compaction, small-file problems, and partition strategies. Finally, we address data contracts that prevent breaking changes, pipeline reliability engineering, late-arriving data handling, and cost management at scale.

The stakes are high. A team without dataset versioning cannot scale beyond a handful of models. A team with poor lineage tracking cannot audit compliance. A team with unreliable pipelines ships trained models they cannot reproduce. This chapter removes that liability.

---

- **5.1** — Why Dataset Versioning Is Non-Negotiable
- **5.2** — Versioning Strategies: Snapshot, Delta, and Semantic
- **5.3** — Tooling for Dataset Versioning in 2026: DVC, LakeFS, and Cloud-Native Options
- **5.4** — Lineage Tracking: From Source to Model to Production
- **5.5** — Reproducibility: Recreating Any Dataset State on Demand
- **5.6** — Storage Architecture: Object Stores, Data Lakes, and Feature Stores
- **5.7** — Lakehouse Formats and Table Layers: Parquet, Delta, Iceberg Concepts
- **5.8** — Compaction, Small-File Problem, and Partition Strategy
- **5.9** — Dataset Registries: Cataloging What You Have
- **5.10** — Data Contracts: Fields, Allowed Values, Null Rules, and Latency SLAs
- **5.11** — Breaking Change Governance: Who Approves, Rollout Plans, Migration Windows
- **5.12** — Pipeline Reliability Engineering: Idempotency, Backfills, and Reprocessing
- **5.13** — Late-Arriving Data and Poison Message Handling
- **5.14** — Pipeline SLAs and SLOs: Freshness, Completeness, and Delay Budgets
- **5.15** — Access Control, Permissions, and Security for Sensitive Datasets
- **5.16** — Cost Management and Disaster Recovery for Dataset Storage at Scale

---

*We start with why dataset versioning is non-negotiable and what happens to teams that skip it.*
