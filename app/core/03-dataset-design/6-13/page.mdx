# 6.13 — Multi-Dimensional Eval Sets: Testing Multiple Criteria Simultaneously

In mid-2025, a healthcare technology company launched an AI-powered clinical documentation assistant that had passed all its evaluation benchmarks with flying colors. The accuracy eval set showed 94% precision on clinical terminology extraction. The safety eval set demonstrated zero HIPAA violations across 500 test cases. The tone eval set confirmed appropriate professional language in 98% of outputs. Three weeks after launch, the system was pulled offline. The problem was not that any single dimension failed, but that the combinations failed catastrophically. A discharge summary could be clinically accurate, legally compliant, and professionally worded while simultaneously being so cold and bureaucratic that it terrified patients. A medication instruction could pass safety checks and use correct medical terms while being incomprehensible to anyone without a medical degree. The team had optimized three dimensions independently and discovered too late that real-world quality lives at the intersection of all dimensions simultaneously.

This is the fundamental limitation of single-dimension evaluation. You cannot assess quality by testing one criterion at a time and hoping they compose well in production. They do not compose well. A system that scores 95% on factual accuracy and 95% on empathetic tone does not produce outputs that are 95% good on both dimensions. It produces outputs where accuracy sometimes undermines empathy, where safety constraints sometimes destroy clarity, where formality sometimes conflicts with accessibility. The only way to know whether your system handles these tensions is to build evaluation sets that test multiple criteria on the same outputs simultaneously. This is not an advanced technique for mature teams. This is baseline professional practice for anyone shipping AI systems to real users.

## Why Single-Dimension Eval Sets Miss Real-World Failure Modes

Single-dimension evaluation creates an illusion of safety. When you test accuracy separately from safety, tone separately from completeness, latency separately from correctness, you are measuring components that never exist in isolation in production. Your users do not experience accuracy and then separately experience tone. They experience a single output that either succeeds or fails on the combination of dimensions that matter to them. A customer support response that is factually perfect but takes 45 seconds to generate has failed on the latency dimension in a way that undermines the accuracy win. A legal contract clause that is legally sound but uses ambiguous pronouns has failed on precision in a way that negates the legal correctness.

The problem is not just that you miss combined failures. The problem is that optimizing for single dimensions often creates trade-offs that you never measure. When you tune a system to maximize safety scores, you often increase refusal rates in ways that hurt user experience. When you tune for conciseness, you often sacrifice clarity. When you tune for formality, you lose accessibility. These trade-offs are invisible in single-dimension evals because each dimension is measured on different examples with different ground truth. Your safety eval set contains adversarial prompts that should be refused. Your user experience eval set contains legitimate requests that should be answered helpfully. The system learns to refuse aggressively to ace the safety eval and answer permissively to ace the experience eval, and you never see the tension until production users encounter the grey areas where both dimensions apply.

A financial services company discovered this in late 2024 when their investment advice chatbot passed accuracy evals with 91% correctness on financial facts and passed compliance evals with zero regulatory violations across 800 test cases. In production, the system told a 67-year-old user asking about retirement accounts that "you should maximize tax-advantaged contributions to your 401k and consider a Roth conversion ladder strategy." This statement was factually accurate and legally compliant. It was also utterly useless to someone who did not know what a Roth conversion ladder was and was looking for plain-language guidance on whether to retire next year or wait two more years. The accuracy eval tested whether facts were correct. The compliance eval tested whether advice stayed within regulatory bounds. Neither eval tested whether the combination of accurate, compliant information was actually helpful to the user asking the question.

This is not a problem you can solve by running more single-dimension evals. You can have ten separate eval sets covering ten separate quality dimensions, and you will still miss the interaction effects. The only solution is to design eval cases where each example is scored on multiple dimensions simultaneously, forcing you to confront the trade-offs and combinations that define real-world quality.

## Designing Eval Cases That Test Multiple Criteria Simultaneously

Multi-dimensional eval cases start with a single input and a single expected output, but the evaluation measures that output against multiple quality criteria at once. For a customer support response, you might score accuracy, tone, completeness, compliance, and latency on the same generated text. For a code generation task, you might score correctness, readability, security, performance, and maintainability on the same generated function. The key is that you are not comparing different outputs to different ground truths. You are comparing one output to one reference and asking multiple questions about how well it matches.

This requires rethinking how you write ground truth annotations. Instead of labeling an example with a single correct answer, you label it with a reference output and a set of criteria that define quality for that specific case. For a medical Q&A example, your ground truth might include the factually correct answer, the appropriate tone for the patient's age and condition, the required safety disclaimers, the maximum acceptable length, and the reading level target. Your evaluation then scores the model output on all five dimensions and tracks whether it succeeds on all of them, some of them, or none of them.

A legal technology company building contract review tools developed a multi-dimensional eval set where each contract clause was annotated with six criteria: legal accuracy, plain-language clarity, completeness of coverage, consistency with other clauses, formatting compliance, and flagged ambiguities. A generated clause could score perfectly on legal accuracy and fail on clarity, or score well on completeness and fail on consistency. The eval set tracked these combinations and revealed that the model was excellent at generating legally sound language but terrible at ensuring that new clauses did not contradict existing ones. This was invisible in their earlier single-dimension evals because legal accuracy and cross-clause consistency were tested on completely different examples.

The design process for multi-dimensional eval cases involves three steps. First, identify the quality dimensions that matter for your task. Do not start with ten dimensions because you think more is better. Start with the three to five dimensions that users actually care about and that can realistically fail independently. For customer support, that might be accuracy, empathy, actionability, safety, and conciseness. For content moderation, that might be policy compliance, false positive rate, explanation quality, cultural sensitivity, and decision latency. Choose dimensions that represent real trade-offs your system has to navigate.

Second, write eval cases where multiple dimensions are at risk simultaneously. Do not write easy cases where all dimensions align. Write cases where being accurate might require being lengthy, where being safe might require being vague, where being empathetic might require bending formality. A good multi-dimensional eval case is one where a naive system will optimize one dimension and fail another. For example, a customer asking "why was I charged twice" is a case where accuracy requires investigating transaction history, empathy requires acknowledging frustration, actionability requires offering a refund path, safety requires not admitting fault prematurely, and conciseness requires not drowning the user in policy explanations. A system that optimizes any single dimension will fail the combination.

Third, define scoring rubrics for each dimension that can be applied to the same output. This is harder than it sounds because different dimensions often require different evaluation methods. Accuracy might be scorable with exact match or semantic similarity. Tone might require human judgment or a fine-tuned classifier. Latency is objective and measurable. Completeness might require checklist-based evaluation. You need to design a scoring pipeline that can apply all these methods to the same output and aggregate the results into a multi-dimensional quality profile.

## Scoring Matrices and Multi-Criteria Evaluation

A scoring matrix is a structured representation of how a single output performs across multiple quality dimensions. For each eval case, you produce a vector of scores rather than a single pass-fail judgment. If you have five quality dimensions, each output gets five scores, and your eval set produces a matrix where rows are examples and columns are dimensions. This matrix becomes your diagnostic tool for understanding system quality.

The simplest scoring approach is binary pass-fail per dimension. For each quality criterion, you define a threshold or a set of conditions that constitute passing. An output passes accuracy if it contains the required facts, passes tone if it uses appropriate language, passes safety if it avoids prohibited content, passes completeness if it addresses all parts of the question, and passes latency if it generates in under three seconds. An eval case passes overall only if it passes all dimensions. This gives you a strict combined quality bar and makes trade-offs explicit. If 80% of cases pass accuracy but only 60% pass all dimensions, you know that 20% of cases are failing on non-accuracy criteria even when the facts are correct.

A more granular approach uses scaled scores per dimension. Instead of pass-fail, you score each dimension on a range, such as zero to five or zero to 100. Accuracy might be scored based on what percentage of required facts are present. Tone might be scored on a scale from highly inappropriate to perfectly appropriate. Completeness might be scored based on how many required elements are included. This produces a richer quality profile and lets you track partial successes. An output that scores 95 on accuracy, 70 on tone, and 85 on completeness is different from one that scores 70 on accuracy, 95 on tone, and 85 on completeness, and both are different from one that scores 85 on all three. The matrix reveals these patterns.

The challenge with multi-dimensional scoring is aggregation. How do you combine five separate scores into a single quality judgment? There is no universal answer because the right aggregation depends on your product requirements. For some tasks, all dimensions are equally important, and you should use the minimum score across dimensions as your overall quality metric. A medical advice system cannot afford to be excellent on accuracy but poor on safety. The lowest dimension defines the failure. For other tasks, dimensions have different weights, and you should use a weighted average. A creative writing assistant might weight tone and creativity more heavily than factual accuracy. A data extraction system might weight accuracy and completeness far above tone.

A SaaS company building email draft generation developed a three-tiered aggregation system. Tier one dimensions were accuracy, compliance, and safety. Any failure on a tier one dimension was an automatic overall failure regardless of other scores. Tier two dimensions were tone, completeness, and clarity. Failures on tier two dimensions were serious but could be tolerated if tier one was perfect and tier three was strong. Tier three dimensions were conciseness, creativity, and formatting. These were nice-to-haves that improved user experience but did not define core quality. This tiered approach made trade-offs explicit and prevented the system from compensating for critical failures with superficial successes.

Another aggregation approach is to track correlation patterns. Instead of combining scores into a single number, you analyze how dimensions co-vary. Do cases that score high on accuracy also score high on clarity, or is there a negative correlation? Do cases that score low on latency score higher on completeness, suggesting the model needs more time for thorough answers? A healthcare documentation company discovered that their system had a strong negative correlation between clinical accuracy and patient readability. Cases that scored above 90 on medical terminology correctness scored below 60 on eighth-grade reading level. This correlation was invisible in single-dimension evals and only emerged when both dimensions were measured on the same outputs.

## The Explosion Problem and Managing Combinatorial Complexity

Multi-dimensional evaluation creates a combinatorial explosion in the number of cases you need to achieve coverage. If you want to test five quality dimensions and each dimension has three levels of difficulty, you are looking at 243 combinations if you want full factorial coverage. If you add context variables like user type, domain, and language, the combinations multiply into the thousands. This is not feasible for most teams, and attempting exhaustive coverage leads to eval sets that are too large to run frequently, too expensive to label, and too complex to interpret.

The solution is not to test every combination but to test the combinations that matter. Start by identifying which dimensions interact and which are independent. If accuracy and tone are independent—meaning high accuracy does not predict high or low tone—you do not need to test every accuracy level with every tone level. You can test accuracy variations with average tone and tone variations with average accuracy. But if safety and clarity interact—meaning safer outputs tend to be vaguer—you need to explicitly test cases that require both high safety and high clarity to see how the system navigates the trade-off.

A legal contract generation company mapped their dimension interactions and discovered that four of their seven quality dimensions were independent, but three formed a tightly coupled cluster. Legal accuracy, regulatory compliance, and liability mitigation were independent of tone, formatting, and reading level. But legal accuracy, regulatory compliance, and liability mitigation all interacted with each other because increasing any one often required trade-offs on the others. They restructured their eval set to test the independent dimensions separately and the coupled dimensions jointly, reducing their required eval case count from over 2,000 to under 400 while maintaining coverage of the interactions that actually occurred in production.

Another strategy is to use boundary cases rather than exhaustive combinations. Instead of testing every level of every dimension, test the extremes where trade-offs are most acute. Test cases where accuracy and conciseness are both maximally important and see which one the system sacrifices. Test cases where safety and helpfulness are in maximum tension and see how the system balances them. Test cases where formality and accessibility pull in opposite directions. These boundary cases reveal the system's priorities and failure modes more clearly than mid-range cases where all dimensions are moderately important.

Prioritization based on product requirements is the most important strategy for managing complexity. Not all dimension combinations are equally likely or equally damaging in production. If your system is used primarily by domain experts, clarity for non-experts is a lower-priority dimension. If your system operates in a highly regulated industry, compliance is a higher-priority dimension than creativity. You should over-sample the dimension combinations that align with your highest-risk scenarios and under-sample or skip the combinations that represent edge cases your users will rarely encounter.

A customer support platform analyzed six months of production conversations and found that 80% of multi-dimensional failures involved just three dimension pairs: accuracy plus empathy, completeness plus conciseness, and safety plus actionability. Failures involving creativity, formality, or humor were vanishingly rare because the domain did not require those dimensions. They restructured their eval set to focus 70% of cases on the three common dimension pairs and only 30% on other combinations, which let them double their coverage depth on the cases that mattered without exploding the total eval set size.

## Prioritizing Dimensions Based on Product Requirements

Not all quality dimensions are created equal, and treating them as equally important in evaluation leads to misallocated effort and misleading metrics. Your product requirements define a hierarchy of dimensions, and your eval set must reflect that hierarchy. A medical diagnosis assistant must prioritize accuracy and safety above tone and conciseness. A creative writing tool must prioritize originality and engagement above factual accuracy. A legal contract reviewer must prioritize completeness and precision above reading ease. Failing to encode these priorities in your eval set means you will optimize for the wrong things and miss the failures that actually matter to your users.

The first step in dimension prioritization is to map product requirements to quality dimensions explicitly. For each core user need, identify which dimensions are critical, which are important, and which are optional. If your primary user need is "help customer service agents resolve billing disputes quickly," your critical dimensions are accuracy of billing information and actionability of suggested resolutions. Important dimensions are empathy and clarity. Optional dimensions are creativity and humor. This mapping ensures that your eval set spends the most effort on the dimensions that define product success.

A fintech startup building an AI financial advisor went through this mapping exercise and identified three critical dimensions: regulatory compliance, factual accuracy on financial products, and appropriateness of advice for user risk profile. They identified two important dimensions: clarity for non-expert users and completeness of consideration of user goals. They identified one optional dimension: engagement and rapport-building tone. They then structured their eval set so that 60% of scoring weight went to critical dimensions, 30% to important dimensions, and 10% to optional dimensions. This meant that a response could score perfectly on tone and still fail overall if it gave incorrect financial information, but it could have mediocre tone and still pass if it nailed the critical dimensions.

Dimension prioritization also affects how you handle trade-offs in multi-dimensional cases. When two dimensions conflict, your eval set should reward the system for choosing the higher-priority dimension. If safety and helpfulness conflict, and safety is higher priority, your ground truth should reflect that the correct answer is the safer one even if it is less helpful. If accuracy and conciseness conflict, and accuracy is higher priority, your ground truth should favor the longer, more accurate response. This is not about ignoring lower-priority dimensions. It is about teaching the system that when forced to choose, it should choose correctly.

A healthcare chatbot team faced a recurring conflict between empathy and clinical precision. Empathetic responses often required softening clinical language in ways that reduced precision, while clinically precise responses often sounded cold. Their product requirements defined clinical precision as critical and empathy as important. They annotated their eval set to reflect this: cases where precision and empathy aligned were expected to score high on both, but cases where they conflicted were expected to prioritize precision. A response that was clinically precise but lacked empathy was marked as acceptable. A response that was empathetic but clinically imprecise was marked as a failure. This taught the system the correct trade-off hierarchy.

Another aspect of prioritization is temporal and contextual. Some dimensions become more important in certain contexts or at certain points in the user journey. For a customer support system, empathy is more critical in the first message when a user is upset and less critical in follow-up messages focused on technical resolution. Completeness is more critical in final resolution messages and less critical in initial acknowledgments. Your eval set should reflect this by including context annotations that indicate which dimensions are most important for each case. A question from an angry user should be evaluated with higher weight on empathy. A question from a user seeking detailed technical information should be evaluated with higher weight on completeness and accuracy.

## Examples from Production Systems

A customer support platform serving a global e-commerce company built a multi-dimensional eval set with five dimensions: factual accuracy, policy compliance, empathetic tone, actionability, and response latency. Each of their 600 eval cases was scored on all five dimensions, and overall quality was defined as passing all critical dimensions and at least three of five total dimensions. They discovered that their system had a systematic failure mode where responses to refund requests were factually accurate and policy-compliant but failed on actionability because they explained policy without telling users what specific steps to take. This failure was invisible when actionability was evaluated separately because the actionability eval set used different examples that did not involve complex policy explanations.

They restructured their eval cases to focus on scenarios where multiple dimensions were at risk. Refund requests became a major category because they required accuracy on order details, compliance with return policies, empathy for user frustration, actionable next steps, and fast response times. By testing all five dimensions on the same refund request examples, they caught the actionability gap and retrained the system to include explicit step-by-step instructions even when explaining policy limitations. Their production customer satisfaction scores increased by 18 percentage points within two months.

A medical documentation assistant serving hospital systems used a four-dimensional eval set: clinical accuracy, regulatory compliance, readability for non-clinical staff, and completeness of required documentation elements. Their most challenging cases were discharge summaries, which had to be medically accurate for receiving physicians, compliant with insurance reporting requirements, understandable for patients and caregivers, and complete with all mandatory fields for legal and billing purposes. Early versions of their system optimized for clinical accuracy and produced summaries that were excellent for physicians but incomprehensible to patients and missing required billing codes.

They built 200 discharge summary eval cases annotated with all four dimensions and realistic examples of cases where dimensions conflicted. A summary for a diabetic patient needed to be clinically accurate about medication changes, compliant with Medicare reporting requirements, readable by a patient with a tenth-grade education, and complete with ICD-10 codes for billing. They scored each generated summary on all four dimensions and required a minimum score of 80 out of 100 on each dimension for the case to pass. This forced the system to balance clinical precision with readability and revealed that the model could achieve both by using plain-language explanations alongside technical terms rather than choosing one or the other.

A legal contract review system used a five-dimensional eval set: legal correctness, completeness of coverage, internal consistency, clarity for non-lawyers, and flagged risk areas. Their highest-value cases were merger and acquisition agreements, where a single missed clause could cost millions and where contracts often ran to hundreds of pages with complex interdependencies. They built 150 M&A contract eval cases where each case tested whether the system could identify legally problematic clauses, ensure all required topics were covered, flag contradictions between sections, explain risks in plain language, and highlight the highest-priority issues for attorney review.

Their multi-dimensional scoring revealed that the system was excellent at identifying legally problematic language and terrible at assessing completeness because it evaluated each clause in isolation without checking whether all necessary topics were present. A contract could pass legal correctness on every clause and still fail completeness by omitting an entire required section. They added a completeness-checking module that compared generated reviews against a checklist of required topics and rescored all eval cases. The combined system passed 91% of multi-dimensional cases compared to 73% when dimensions were tested separately.

A code generation tool for enterprise developers used a six-dimensional eval set: functional correctness, security, performance, readability, maintainability, and adherence to team style guides. Each of their 500 eval cases included a coding task and a reference implementation, and generated code was scored on all six dimensions. They discovered that optimizing for functional correctness led to code that worked but was often insecure, and optimizing for security led to code that was overly defensive and slow. Multi-dimensional evaluation forced them to find implementations that balanced correctness, security, and performance simultaneously.

They built a tiered scoring system where correctness and security were mandatory pass dimensions—any failure meant overall failure—and performance, readability, maintainability, and style were weighted secondary dimensions. A generated function had to work correctly and have no security vulnerabilities to pass, and then it was scored on how well it balanced the remaining dimensions. This prevented the system from shipping insecure code even if it was fast and readable, while still rewarding code that excelled on multiple dimensions.

## Building Multi-Dimensional Eval Sets in Practice

The practical workflow for building multi-dimensional eval sets starts with dimension selection. Convene your cross-functional team—engineering, product, domain experts, legal, support—and identify the three to seven quality dimensions that define success for your system. Avoid the temptation to include every conceivable dimension. Focus on the dimensions that users notice, that can realistically fail, and that require trade-offs. Document why each dimension matters and what failure on that dimension looks like in production.

Next, write eval cases that exercise multiple dimensions simultaneously. Start with real production examples where quality was good or bad for multiple reasons. A support response that succeeded did not just have correct information; it also had the right tone, provided actionable steps, and arrived quickly. A response that failed did not just have incorrect facts; it also lacked empathy, was too long, or violated policy. Use these real examples as templates and write variations that test different dimension combinations.

Annotate each eval case with ground truth for all dimensions. This is labor-intensive, and you cannot cut corners by annotating some dimensions on some cases and other dimensions on other cases. Every case needs every dimension scored, or you lose the ability to see interactions. For dimensions that require human judgment, use multiple annotators and measure inter-rater reliability. For dimensions that can be automated, build scoring functions and validate them against human judgment on a sample.

Build a scoring pipeline that applies all dimension evaluations to each generated output and produces a score matrix. This pipeline should output per-dimension scores, overall pass-fail status, and aggregate metrics like percentage of cases passing all dimensions, average score per dimension, and correlation between dimensions. Run this pipeline on every model checkpoint and track how dimension scores evolve during development.

Analyze dimension interactions in your results. Look for negative correlations that indicate trade-offs the model is making. Look for cases that pass some dimensions and fail others. Look for clusters of failures that share common dimension patterns. Use this analysis to guide model improvements and to refine your eval set. If a dimension never fails, it might not be at risk and can be de-prioritized. If two dimensions always fail together, they might be measuring the same underlying issue and can be consolidated.

## Interpreting Multi-Dimensional Results and Making Decisions

Multi-dimensional eval results are harder to interpret than single-dimension pass rates, but they are far more informative. Instead of a single number that tells you the system is 87% accurate, you get a matrix that tells you the system is 94% accurate, 89% compliant, 76% empathetic, 91% actionable, and 82% concise, and only 68% of cases pass all five dimensions. This matrix reveals where the system is strong, where it is weak, and where trade-offs are being made badly.

The first interpretive question is whether dimension scores are acceptable in absolute terms. If your product requirements say empathy must be above 85% and you are at 76%, you have a gap to close regardless of how other dimensions look. Set minimum acceptable thresholds for each dimension based on product and user needs, and flag any dimension that falls below threshold as a blocking issue.

The second question is whether the multi-dimensional pass rate is acceptable. This is the percentage of cases that meet minimum thresholds on all critical dimensions and acceptable performance on important dimensions. If only 68% of cases pass all dimensions, you are shipping a system that fails multi-dimensional quality on nearly a third of inputs. This is often far worse than single-dimension metrics suggest because single-dimension evals hide the cases that pass on the tested dimension but fail on others.

The third question is where the multi-dimensional failures are concentrated. Are there specific input types, user contexts, or task categories where multi-dimensional quality collapses? A healthcare chatbot might have 85% multi-dimensional pass rate overall but only 60% on questions about medication interactions, where accuracy, safety, and clarity are all critical and all difficult. Identifying these concentration points lets you focus improvement effort on the highest-impact areas.

The fourth question is what trade-offs the system is making. If accuracy and empathy are negatively correlated, the model is sacrificing one to achieve the other. If compliance and actionability are negatively correlated, the model is choosing between being safe and being helpful. These trade-offs are product decisions, not just technical issues. You need to decide whether the trade-offs are acceptable, and if not, whether you need better prompts, better training data, better model selection, or different product requirements.

A financial services company found that their investment advice system had 92% accuracy, 96% compliance, but only 71% clarity for non-expert users, and the multi-dimensional pass rate was 68%. They analyzed failures and found that 80% of multi-dimensional failures were cases where compliance and clarity conflicted. The model was generating legally compliant disclosures that were incomprehensible to users without finance backgrounds. They made a product decision to prioritize clarity within compliance constraints, which meant simplifying disclosures even if it required longer explanations, and they retrained the model on examples that demonstrated how to be both compliant and clear.

## Dimension Weighting Strategies and Trade-Off Resolution

Beyond simply measuring multiple dimensions, you need explicit strategies for how to weight them when making quality judgments and how to resolve conflicts when dimensions pull in opposite directions. The weighting strategy you choose has profound implications for what your system optimizes for and what failures it tolerates.

The equal-weight approach treats all dimensions as equally important and requires passing scores on all of them. This is appropriate when no single dimension can be sacrificed for others. A medical diagnosis system might require passing scores on accuracy, safety, completeness, and clarity with no dimension weighted higher. A failure on any dimension is an overall failure. The advantage is simplicity and clear expectations. The disadvantage is that it can be overly rigid when real-world quality involves nuanced trade-offs.

The threshold-then-optimize approach sets minimum thresholds on critical dimensions and then optimizes a weighted combination of remaining dimensions. You might require that all outputs meet minimum standards for safety and compliance, and then among outputs that meet those minimums, optimize for the best balance of clarity, empathy, and conciseness. This approach acknowledges that some dimensions are non-negotiable while others involve trade-offs. A financial services company used this approach with hard thresholds on regulatory compliance and factual accuracy, and then optimized for user experience dimensions among compliant responses.

The tiered-priority approach ranks dimensions in strict order and resolves conflicts by always favoring higher-priority dimensions. If safety conflicts with helpfulness, safety wins. If accuracy conflicts with conciseness, accuracy wins. This creates clear decision rules for the system and for human reviewers. A healthcare chatbot implemented a five-tier priority: patient safety first, clinical accuracy second, scope appropriateness third, clarity fourth, and empathy fifth. When dimensions conflicted, the system was trained to sacrifice lower-priority dimensions to preserve higher-priority ones. This prevented situations where the system was empathetic but medically wrong.

The context-dependent approach varies dimension weights based on the input context, user type, or task category. Empathy might be weighted higher for customer complaints and lower for technical inquiries. Completeness might be weighted higher for expert users who want comprehensive information and lower for novice users who want simple answers. Latency might be weighted higher for real-time chat interactions and lower for asynchronous email responses. This approach acknowledges that quality is not uniform across all use cases and that the same output might be excellent in one context and poor in another.

A customer support platform implemented context-dependent weighting by classifying incoming queries into emotional support, technical troubleshooting, account management, and policy questions. Emotional support queries weighted empathy at 40%, actionability at 30%, accuracy at 20%, and conciseness at 10%. Technical troubleshooting weighted accuracy at 50%, completeness at 30%, and clarity at 20%. Account management weighted compliance at 40%, accuracy at 35%, and security at 25%. This context-aware evaluation revealed that the system was strong on technical queries but weak on emotional support, which would have been hidden in a one-size-fits-all eval.

## Common Multi-Dimensional Failure Patterns

Production experience reveals recurring patterns in how multi-dimensional quality fails. Recognizing these patterns helps you design eval sets that specifically test for them and helps you interpret eval results when they occur.

The compensatory failure pattern is when a system scores well overall by excelling on easy dimensions to compensate for failing hard dimensions. A response might be beautifully formatted, grammatically perfect, and appropriately formal while being factually wrong or missing the point entirely. The overall score looks acceptable because formatting, grammar, and tone are easy to get right, but the response fails on the dimensions that actually matter. Multi-dimensional eval sets must prevent this by requiring minimum scores on critical dimensions rather than allowing averaging.

The regression-to-safe failure pattern is when a system optimizes for safety or compliance dimensions so aggressively that it becomes useless. The system refuses to answer questions it should answer, gives overly hedged responses that provide no actionable information, or demands unnecessary clarifications before responding. It passes safety and compliance evals perfectly but fails user experience catastrophically. A legal chatbot passed 100% of compliance eval cases by refusing to answer any question that touched on legal interpretation, but in production it was useless because users needed legal guidance, not refusal.

The precision-recall trade-off failure pattern is when a system optimizes precision at the expense of recall or vice versa across different quality dimensions. A content moderation system might achieve high precision on policy violations by being extremely conservative, but this increases false positives where legitimate content is incorrectly flagged. Or it might achieve high recall by flagging anything remotely questionable, but this decreases precision and creates user frustration. Multi-dimensional evals must test both precision and recall and measure the cost of favoring one over the other.

The context-insensitive failure pattern is when a system applies the same quality strategy to all inputs regardless of context. It might be verbose when brevity is needed, formal when casualness is appropriate, or simplified when technical depth is required. The system has a default mode that works well in some contexts but fails in others, and single-dimension evals miss this because they test one context at a time. Multi-dimensional evals should include context variation as an explicit dimension.

The interaction-effect failure pattern is when two dimensions that individually score well create problems in combination. A response might be accurate and empathetic separately, but the empathetic framing undermines confidence in the accuracy, or the clinical accuracy makes the empathy sound patronizing. A legal contract clause might be complete and clear separately, but the clarity comes from oversimplification that makes the completeness misleading. These interaction effects only emerge when you evaluate both dimensions on the same output.

## Practical Tooling for Multi-Dimensional Evaluation

Building and running multi-dimensional eval sets requires tooling that can handle complex scoring pipelines, aggregate results across dimensions, and surface patterns in multi-dimensional failure. The tools you use shape how effectively you can implement multi-dimensional evaluation.

Your evaluation framework must support per-dimension scoring functions that can be mixed and matched. Some dimensions are scored by exact match, some by semantic similarity, some by fine-tuned classifiers, some by rule-based checks, and some by human judgment. The framework should let you compose these scoring methods into a pipeline that runs all of them on each output and collects the results into a structured format. A well-designed framework treats each dimension as a plugin with a standard interface and allows you to add, remove, or modify dimensions without rewriting the entire evaluation logic.

Your results storage must preserve the full multi-dimensional score matrix, not just aggregated metrics. You need to be able to query for cases that passed dimension A but failed dimension B, cases where dimensions A and B both failed, cases where dimension scores were below threshold even though overall pass-fail was acceptable, and cases where dimension scores had unusual patterns. Storing results in a structured database or data warehouse enables this analysis. Storing only summary statistics loses the information you need to debug multi-dimensional failures.

Your visualization tooling should support multi-dimensional analysis. Heatmaps showing dimension correlations, scatter plots showing how two dimensions trade off, histograms showing score distributions per dimension, and breakdowns showing pass rates by dimension and by dimension combination all help you understand multi-dimensional quality. A dashboard that shows only overall pass rate is insufficient. You need views that let you drill into specific dimension combinations and understand where quality is strong and where it collapses.

Your annotation tooling should support multi-dimensional ground truth entry. Annotators should be able to score each eval case on all relevant dimensions in a single workflow rather than having to annotate the same cases multiple times in different tools. The tooling should enforce that all dimensions are scored for all cases and should track inter-annotator reliability per dimension. Good annotation tools also let you flag cases where dimensions conflict and document the trade-off decision for future reference.

A healthcare AI company built a custom evaluation platform that supported pluggable scoring functions per dimension, stored all results in a PostgreSQL database with full dimension detail, generated interactive dashboards showing dimension correlations and failure patterns, and provided annotation interfaces where clinical reviewers could score medical accuracy, safety, completeness, and clarity in a single pass. The platform reduced their eval execution time by 60% and made multi-dimensional analysis so accessible that product managers used it directly rather than relying on data science to run custom analyses.

## Moving Beyond Eval Sets to Continuous Multi-Dimensional Monitoring

Multi-dimensional eval sets are not a one-time construction. They are living artifacts that must evolve as your product requirements shift, as your user base grows, and as your model changes. The same principles that apply to building multi-dimensional eval sets apply to monitoring production quality on multiple dimensions continuously. You need instrumentation that tracks not just whether responses are accurate but whether they are accurate, safe, empathetic, actionable, and fast simultaneously. You need dashboards that show dimension-by-dimension performance and multi-dimensional pass rates. You need alerting that fires when any critical dimension drops below threshold or when multi-dimensional pass rates degrade.

Production monitoring should measure the same dimensions as your eval set and use the same scoring methods where feasible. This creates a direct comparison between eval performance and production performance. When eval says the system is 89% accurate and production monitoring says it is 76% accurate, you have either an eval set problem or a production distribution problem. Both are critical to investigate. When eval and production align, you have confidence that your eval set is predictive.

Continuous monitoring also reveals dimension drift over time. You might launch with strong performance on all dimensions, but as your user base grows or as the product evolves, certain dimensions degrade while others stay stable. A customer support system might maintain 92% accuracy over six months but see empathy scores drop from 87% to 71% as traffic increases and responses become more templated. Multi-dimensional monitoring catches this drift and triggers intervention before users revolt.

The discipline of multi-dimensional evaluation is the discipline of acknowledging that quality is not one thing. It is the intersection of many things, and optimizing each thing separately does not produce systems that work well on all things together. The only way to build systems that succeed on the combinations of dimensions that matter to users is to evaluate those combinations directly, explicitly, and continuously.

Your next step is to formalize how those multi-dimensional eval sets get reviewed, approved, and maintained across your organization. Multi-dimensional evaluation is powerful, but it is only as good as the cross-functional alignment on what dimensions matter and how to measure them. That cross-functional review process is where eval sets move from engineering artifacts to organizational standards, and that is the subject we turn to next.
