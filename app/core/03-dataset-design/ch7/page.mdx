# Chapter 7 — Training and Fine-Tuning Dataset Construction

Training datasets serve a fundamentally different purpose than evaluation datasets, and the mistakes teams make in constructing them are different too. You do not evaluate with training data. You do not use the same sampling strategy, the same quality bars, or the same measurement approach. Yet many teams blur this line, creating instruction-tuning datasets that are too small, too homogeneous, or contaminated with held-out eval examples. Other teams build preference datasets for RLHF or DPO without understanding the cost-quality tradeoffs, or they apply curriculum design poorly and watch training plateau because data is ordered wrong.

This chapter covers instruction-tuning dataset design, preference and ranking datasets for RLHF and DPO, dataset size planning (how much you actually need versus how much vendors tell you to buy), curriculum design that stages data properly, data mixing across tasks and domains, hard negatives, filtering for training quality, and the integration loop that connects production failures back to training priorities. We also cover special cases: datasets for RAG systems (retrieval pairs and passage sets), datasets for agent systems (trajectories and tool-use patterns), transfer and multi-task design, and multimodal training data alignment. Finally, we address the practical handoff to fine-tuning providers and their format requirements.

Training data is where models learn. Eval data is where you measure. The separation between them is sacred. Violate it and your metrics become meaningless.

---

- **7.1** — Training Data vs Eval Data: The Separation Principle
- **7.2** — Instruction-Tuning Dataset Design
- **7.3** — Preference and Ranking Dataset Design for RLHF and DPO
- **7.4** — Dataset Size Planning: How Much Data You Actually Need
- **7.5** — Curriculum Design: Ordering and Staging Training Data
- **7.6** — Data Mixing: Combining Tasks, Domains, and Difficulty Levels
- **7.7** — Negative Examples and Hard Negatives
- **7.8** — Dataset Filtering for Training: Quality Thresholds That Matter
- **7.9** — Training Data for RAG Systems: Retrieval Pairs and Passage Sets
- **7.10** — Training Data for Agent Systems: Trajectory and Tool-Use Datasets
- **7.11** — Transfer and Multi-Task Dataset Design
- **7.12** — The Integration Loop: Active Learning Triggers, Failure Routing, and Labeling Prioritization
- **7.13** — Multimodal Training Data: Alignment Across Audio, Image, Video, and Text
- **7.14** — The Handoff to Fine-Tuning: Format Requirements by Provider

---

*We start with the separation principle: why training data and eval data must never mix, and why this is harder than it sounds.*
