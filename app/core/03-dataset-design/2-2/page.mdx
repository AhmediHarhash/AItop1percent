# 2.2 â€” Harvesting Production Data: Logging, Sampling, and Consent

In late 2025, a SaaS company discovered that their content moderation model was failing to detect a new category of policy-violating content that had emerged over the prior three months. The company had production logging infrastructure in place and collected millions of user-submitted posts daily. The problem was not a lack of data. The problem was that their logging system captured only the final post content and the model's binary decision, not the intermediate signals, user metadata, or contextual features that would have revealed the new pattern. When the trust and safety team tried to investigate the failures, they found that the logs contained insufficient information to diagnose the root cause, reproduce the failure cases, or build a training set to fix the problem. They had logged faithfully for 18 months, but they had logged the wrong things.

The root cause was not a technical failure of the logging infrastructure. The infrastructure worked perfectly. The root cause was that the team had never asked what they needed to learn from production data, only what they needed to store for compliance. They logged post content because they might need to review it for legal reasons. They logged model decisions because they needed to track error rates. They never logged the feature vectors, the confidence scores, the user's prior history, or the time-of-day signals that the model actually used to make decisions. When a new failure mode appeared, the logs could not explain it. The team had built a compliance archive, not a learning system.

Harvesting production data is not the same as storing production data. Logging is an active discipline that requires you to decide what information is worth capturing, how to sample it without introducing bias, and how to obtain and manage consent under privacy regulations. The difference between teams that learn from production and teams that merely survive production is whether they treat logging as a strategic investment or as a compliance checkbox. Production data is only valuable if you log what matters, sample intelligently, and handle consent properly.

## What to Log: Signal, Context, and Outcomes

The most common logging mistake is logging only the final output. You log the user query and the model response, but not the intermediate reasoning, the retrieved documents, the confidence scores, or the contextual metadata that influenced the decision. This is sufficient for compliance and basic error tracking, but insufficient for learning. When the model fails, you cannot diagnose why it failed, because you do not have the intermediate signals that led to the failure. When the model succeeds in unexpected ways, you cannot understand what worked, because you do not have the features that drove the success.

Comprehensive production logging captures three categories of information: the input signal, the contextual metadata, and the outcome. The input signal is what the user provided: the query, the document, the image, the audio, or whatever raw data the model processed. The contextual metadata is everything else that influenced the model's behavior: the user's account type, their prior interactions, the time of day, the geographic location, the session history, the A/B test variant, the model version, the retrieval results, the prompt template used, and any other features the model considered.

The outcome is what happened: the model's response, the confidence scores, the user's feedback signal, and any downstream actions taken based on the model's output. A customer service chatbot should log the user query, the conversation history, the user's account tier, the time of interaction, the model's generated response, the confidence score, the retrieval context if applicable, and whether the user clicked thumbs up, thumbs down, or requested a human agent. All of these signals together allow you to reproduce the interaction and understand what drove the outcome.

## Why Intermediate Signals Matter for Learning

Logging all three categories allows you to reproduce model behavior, debug failures, and build training sets that reflect production reality. If a model produces a low-quality response, you can examine the input signal to see if the user's query was ambiguous. You can examine the contextual metadata to see if the user's prior interactions created misleading context. You can examine the outcome to see if the model's confidence score indicated uncertainty. Without all three categories, you are guessing.

A financial services chatbot logged user queries and model responses but not the account balance, transaction history, or account status that the model accessed to generate answers. When the model started giving incorrect advice to users with pending transactions, the team could not diagnose the problem because the logs did not show what account state the model had seen. They had to manually reproduce the failures in a staging environment, which took days. If they had logged the account state as contextual metadata, they could have diagnosed the issue in hours.

You also need to log metadata about the logging process itself. You need to know when each example was logged, what sampling rate was used, what software version was running, and whether the logging pipeline was operating normally or experiencing errors. If your logging system drops 10% of examples due to network failures, but you do not log that fact, your dataset will be silently corrupted by selection bias. Logging metadata is not overhead. It is the foundation of data quality.

## Capturing Feature Representations and Model Internals

For systems that rely on retrieval, ranking, or multi-step reasoning, you need to log the intermediate artifacts. If your model retrieves documents from a knowledge base, log the retrieval query, the top documents returned, their relevance scores, and which documents were actually used in the final response. If your model ranks candidates, log the candidate set, the ranking scores, and the final selection. If your model performs multi-step reasoning, log each reasoning step and the intermediate conclusions.

A legal research assistant retrieved case law to answer user questions. The team logged user queries and final answers but not the retrieved cases or relevance scores. When users complained that answers were missing recent precedent, the team could not determine whether the retrieval system was failing to find the cases or whether the model was failing to incorporate them. They eventually added retrieval logging and discovered that the retrieval system was working fine, but the model was ignoring low-scoring results that were actually relevant. Without logging the retrieval artifacts, this diagnosis would have been impossible.

Logging internal model state can also be valuable for fine-tuning and debugging. Some teams log embedding vectors, attention weights, or hidden layer activations for a sample of production traffic. This allows them to analyze what representations the model learned, whether certain inputs cluster together in embedding space, and whether attention patterns match expectations. This level of logging is expensive in storage and compute, so it is typically applied to a small sample rather than all traffic.

## Sampling Strategies: Avoiding Bias While Controlling Volume

Logging everything is usually infeasible. If you serve a million requests per day, logging every request generates terabytes of data per month, most of which is redundant. The solution is sampling: you log a fraction of traffic instead of all traffic. Sampling reduces storage costs and processing overhead, but introduces the risk of bias. If you sample naively, you may oversample common cases and undersample rare cases, creating a training set that does not reflect the true distribution of production traffic.

The simplest sampling strategy is uniform random sampling. You log each request with probability p, where p is chosen to control volume. If you want to log 10% of traffic, you set p to 0.1. Uniform random sampling is unbiased in expectation, meaning that the logged distribution matches the true distribution on average. The problem with uniform random sampling is that rare events remain rare in the logged data. If a failure mode occurs in 0.1% of production traffic, and you log 10% of traffic, you expect to capture only 0.01% of your logged data as examples of that failure mode. This may not be enough to learn from.

## Stratified Sampling for Rare Event Coverage

A better strategy is stratified sampling, where you sample different subpopulations at different rates. You might log 5% of successful requests, 50% of requests where the model expressed low confidence, and 100% of requests where the user provided explicit negative feedback. This ensures you capture rare but important events at higher rates than common events. Stratified sampling introduces intentional bias: your logged data overrepresents rare events relative to their true frequency.

This is acceptable as long as you track the sampling rates and adjust for bias during training and evaluation. If you train on stratified data without reweighting, the model will overfit to rare events. If you reweight by the inverse of the sampling probability, the model learns the correct distribution. A medical diagnosis system that logs 100% of rare disease cases but only 2% of common cold cases must reweight examples during training so the model does not overpredict rare diseases.

Another strategy is reservoir sampling, which allows you to sample a fixed number of examples uniformly from an infinite stream without knowing the total population size in advance. Reservoir sampling is useful when you want to maintain a fixed-size sample of recent production traffic for quick analysis. You maintain a buffer of n examples and replace each example with decreasing probability as new examples arrive. Reservoir sampling guarantees that at any point in time, the buffer is a uniform random sample of all examples seen so far.

## Temporal Sampling and Shift Detection

You also need to sample over time to capture temporal dynamics. If you log heavily during business hours but rarely during nights and weekends, your dataset will be biased toward daytime usage patterns. If you log heavily during launch periods but rarely during steady state, your dataset will be biased toward early adopters. Time-stratified sampling ensures that you capture representative examples from all time periods, even if traffic volume varies.

A simple approach is to log a fixed number of examples per hour or per day, regardless of total traffic. This ensures temporal coverage even if absolute volumes fluctuate. An e-commerce search system logs 1,000 queries per hour during peak shopping periods and 1,000 queries per hour during off-peak periods, even though peak traffic is 20 times higher. This ensures the logged data includes off-peak patterns that might otherwise be drowned out by peak traffic.

Temporal sampling is particularly important for detecting distribution drift. If you log continuously over months, you can compare the distribution of queries, errors, and user feedback across time periods. A sudden shift in query distribution might indicate a new product feature launched, a marketing campaign started, or external events changed user behavior. A gradual shift might indicate long-term changes in user demographics or preferences. Without temporal coverage, you cannot detect these shifts.

## Tracking Sampling Metadata to Enable Reweighting

All sampling strategies require you to log the sampling metadata: what sampling strategy was used, what sampling rate applied to each example, and what stratum the example belongs to. Without this metadata, you cannot correct for sampling bias during training or evaluation. Sampling metadata is not optional. It is part of the logged data.

For stratified sampling, you log a stratum identifier and sampling rate for each example. An example might have stratum set to low confidence and sampling rate set to 0.5, indicating it came from a stratum where 50% of examples were logged. During training, you apply an importance weight of 2.0 to this example to correct for the oversampling. An example from the success stratum with sampling rate 0.05 receives a weight of 20.0.

You also log the timestamp and any time-based sampling metadata. If you applied hourly quotas, log which hour the example came from and what the quota was. If you applied day-of-week sampling, log the day and the sampling rate for that day. All of this metadata allows you to reconstruct the true production distribution from the biased logged sample.

## Consent Under GDPR and the EU AI Act

Production data almost always includes personal data: user identifiers, queries that reveal personal information, uploaded documents that contain names or addresses, or interaction patterns that can be linked to individuals. Under GDPR, you cannot collect, store, or process personal data without a legal basis. The most common legal basis for production logging is consent: the user explicitly agrees that their data may be used for purposes including AI training and improvement. Consent must be informed, specific, and freely given. Burying consent in a general terms of service is not sufficient. You need explicit opt-in for AI training purposes.

GDPR also grants users the right to access their data, the right to correct it, the right to delete it, and the right to object to processing. This means you cannot simply log production data and forget about it. You must maintain a mapping from logged data back to user identities so you can honor deletion requests. You must provide users with access to their logged data if they request it. You must stop processing their data if they object.

These requirements have significant implications for dataset engineering. You cannot treat logged data as immutable. You must support deletion, which means you must either rebuild models after each deletion request or use techniques like machine unlearning to remove the influence of deleted data without full retraining. A customer service company receives an average of 40 GDPR deletion requests per month. They maintain a deletion log and rebuild their training dataset weekly to exclude deleted users. Models are retrained monthly to incorporate the updated dataset.

## The EU AI Act and High-Risk System Requirements

The EU AI Act adds additional obligations for high-risk AI systems. You must document the data sources used to train and validate the model, including provenance, quality characteristics, and consent status. You must ensure that datasets are representative and free from bias to the extent possible. You must log sufficient information to enable audits and investigations. These requirements are not purely legal compliance. They are good engineering practice. If you cannot document where your data came from and whether consent was obtained, you cannot defend your system against legal challenges or regulatory scrutiny.

Consent is not a one-time checkbox. User expectations change, regulations change, and your use of data may change as the product evolves. You need to re-obtain consent when you change how data is used. If you initially collected data for error monitoring and later decide to use it for training, you need new consent. If you initially trained a model for one purpose and later repurpose it, you need to verify that the original consent covers the new use. Consent is an ongoing relationship, not a permanent license.

## Implementing Consent in Production Systems

The practical implementation of consent for production logging is to provide users with clear, specific language at the point of interaction. When a user submits a query or uploads a document, you inform them that their input may be logged and used to improve the AI system, and you provide an opt-out mechanism. Users who opt out are not logged, or their data is logged but flagged as not eligible for training. You maintain a consent ledger that tracks which users consented, when they consented, and what they consented to.

When you build a training set, you filter out any data from users who have not consented or who have withdrawn consent. This is not optional infrastructure. It is a legal and ethical prerequisite for using production data. A financial services company that logged customer service conversations without obtaining explicit consent for AI training faced GDPR enforcement action and fines totaling 1.2 million euros. The company had consent for customer service purposes but not for AI training, and the regulator ruled that general consent was insufficient.

Your consent interface should be clear and non-coercive. Consent cannot be a condition of using the product unless data collection is strictly necessary for the service. If users can opt out without losing access to core functionality, the opt-out must be easy to find and easy to use. A prominent toggle in settings, a checkbox during onboarding, or an opt-out link in the interaction interface are all acceptable. Burying the opt-out in legal terms or requiring users to email customer support is not acceptable.

## Anonymization and Pseudonymization: Reducing Privacy Risk

Even with consent, you should minimize privacy risk by anonymizing or pseudonymizing logged data. Anonymization means removing or replacing personally identifiable information so that the data cannot be linked back to individuals. Pseudonymization means replacing direct identifiers with pseudonyms, while retaining the ability to re-identify individuals if necessary for legal or operational reasons. GDPR encourages pseudonymization as a privacy-protective measure, though pseudonymized data is still considered personal data and subject to GDPR obligations.

Anonymization is difficult to do correctly. Simply removing names and email addresses is not sufficient if other fields like timestamps, location, or interaction patterns can be combined to re-identify individuals. A query like "where is the nearest hospital to my house" contains no direct identifiers but may be uniquely identifiable when combined with timestamp and location metadata. True anonymization requires either aggregating data to the point where individuals are not distinguishable or applying differential privacy techniques to add noise that prevents re-identification. Both approaches reduce data utility, so there is a tradeoff between privacy and learning.

## Pseudonymization as a Practical Middle Ground

Pseudonymization is easier to implement and often sufficient for production logging. You replace user identifiers with randomly generated pseudonyms and store the mapping in a separate, access-controlled database. The logged data contains pseudonyms instead of real identifiers, so engineers working with the data cannot see who the users are. If a user requests deletion, you look up their pseudonym in the mapping and delete all logged data associated with that pseudonym. Pseudonymization does not eliminate privacy risk, but it reduces it significantly by limiting who can link logged data back to individuals.

You also need to anonymize or redact free-text content that may contain personal information. User queries, uploaded documents, and model-generated responses may include names, addresses, phone numbers, credit card numbers, health information, or other sensitive data. Simple redaction replaces sensitive entities with placeholders. More sophisticated techniques use named entity recognition models to detect and redact personal information automatically.

No redaction technique is perfect, so you should combine automated redaction with manual review for high-risk data and clear user warnings that sensitive information should not be included in inputs. A healthcare chatbot warns users at the start of each session: "Do not enter your social security number, credit card information, or other sensitive identifiers. This conversation may be logged to improve our service."

## The Privacy-Utility Tradeoff in Redaction

The tradeoff with anonymization and redaction is that you lose information that may be relevant for learning. If you redact all names, the model cannot learn how to handle names correctly. If you redact all numbers, the model cannot learn numeric reasoning. The solution is to apply the minimum redaction necessary to meet privacy requirements, not blanket redaction of all potentially sensitive content. If your task does not require learning from personal information, redact it. If your task does require learning from personal information, ensure you have proper consent and access controls.

A legal document analysis system needs to learn from contract party names, dollar amounts, and dates. Redacting all of this information would make the data useless for training. The system obtains explicit consent from users to log full contract text for training purposes, applies strict access controls so only authorized ML engineers can access the data, and applies retention limits so contracts are deleted after 18 months. This balances privacy protection with learning utility.

## Storage Considerations: Cost, Retention, and Retrieval

Production logs accumulate quickly. A system serving a million requests per day, logging 10% of traffic at an average of 2 kilobytes per logged example, generates 200 megabytes per day or 73 gigabytes per year. That is manageable for a single year, but if you retain logs for five years, you are storing 365 gigabytes, and if traffic grows 10x, you are storing 3.65 terabytes. Storage costs are not negligible, especially when you account for redundancy, backups, and compliance requirements that may mandate long retention periods.

The first decision is how long to retain logged data. Compliance requirements may mandate minimum retention periods. GDPR does not mandate retention, but other regulations like financial services rules or healthcare rules may require you to retain data for years. Even without legal mandates, you may want to retain data to detect long-term trends, perform retrospective analyses, or rebuild models using historical data. The tradeoff is that longer retention increases storage costs and increases privacy risk. The longer you retain data, the more opportunities for breaches or misuse.

## Tiered Retention Policies

A common retention policy is to retain full-fidelity logs for a short period, such as 30 to 90 days, and then downsample or archive older data. Full-fidelity logs are kept for recent traffic to support debugging, incident response, and rapid iteration. After the retention window, you either delete the data, downsample it to a smaller random sample, or archive it to cheaper cold storage. Archived data is still accessible for long-term analysis but costs less to store and is retrieved less frequently. This tiered retention strategy balances cost, utility, and privacy risk.

A SaaS company retains 100% of production logs for 60 days in hot storage for rapid access. After 60 days, they downsample to 10% and move to warm storage. After one year, they downsample to 1% and move to cold storage. After three years, they delete all data except for a 0.1% sample retained for long-term trend analysis. This policy reduces storage costs by 90% compared to retaining all data indefinitely, while still preserving sufficient data for learning and debugging.

You also need to design for efficient retrieval. Logged data is not useful if you cannot query it. You need to index logs by relevant fields: user pseudonym, timestamp, model version, error status, confidence score, or any other metadata you logged. You need to support queries like "show me all examples where the model had low confidence and the user provided negative feedback in the past 30 days" or "show me all examples from users in the EU who consented to training in the past 90 days." Without indexing and query capabilities, logged data is a write-only archive that cannot be used for learning.

## Compliance Infrastructure for Logged Data

Storage infrastructure also needs to support compliance obligations. You need to be able to delete specific users' data in response to GDPR deletion requests. You need to be able to export data for audits or regulatory investigations. You need to be able to prove that consent was obtained and that data was handled according to policy. This requires maintaining metadata about consent status, logging access to the data, and tracking modifications or deletions. Compliance infrastructure is not separate from logging infrastructure. It is part of the same system.

A financial services company maintains an audit log of every access to production training data. The audit log records who accessed the data, when, what query they ran, and what data was returned. If a regulator asks for evidence that data was handled properly, the company can produce a complete record of all data access. If a security incident occurs, the audit log shows whether any unauthorized access took place. Audit logging adds overhead but is essential for regulated industries.

## The Difference Between Logging Everything and Logging What Matters

Logging everything is seductive because it avoids the need to make decisions about what to log. You log every field, every request, every response, and every intermediate signal, and you figure out later what you need. The problem with logging everything is cost, noise, and complexity. Storage costs scale with volume. Query performance degrades with data size. Engineers drown in irrelevant data when trying to find the signal that matters. Logging everything is not a substitute for logging thoughtfully.

Logging what matters requires you to think ahead about what you need to learn, what failures you need to diagnose, and what analyses you plan to perform. This does not mean you can predict every future need. It means you log the inputs, outputs, context, and outcomes that are most likely to be useful, and you iterate as you learn what additional signals matter. A good heuristic is to log everything the model used to make a decision, everything the user did in response, and enough metadata to reproduce the decision later. This is far less than everything, but far more than the minimum.

## Schema Versioning for Evolving Logs

You also need to version your logging schema. As the product evolves, the model changes, the features change, and the logged fields change. If you do not version the schema, you cannot interpret historical logs correctly. A field that meant one thing in version 1 may mean something different in version 2. A field that was always populated in version 1 may be missing in version 2. Schema versioning allows you to handle these changes gracefully. You log the schema version with each example, and when you query historical data, you know which schema applies.

A document classification system added a new confidence score field in version 2 of the logging schema. Logs from version 1 do not have this field. When querying historical data, the system checks the schema version and applies default values for missing fields in old logs. When training models, the system filters to examples from schema version 2 or later when confidence scores are needed, or uses all versions when confidence is not required. Without schema versioning, the team would not know which logs are missing the field.

Logging what matters also means logging failures and edge cases at higher rates than successes. Common, successful interactions teach the model less than rare, problematic interactions. If you sample uniformly, your logs will be dominated by successes, and you will have limited data to learn from failures. Stratified sampling, as discussed earlier, ensures you capture failures at high rates. This requires you to define what counts as a failure: low confidence scores, negative user feedback, timeouts, exceptions, or any other signal of poor performance. Logging failures is not just for debugging. It is for learning.

## Building the Logging Pipeline: Infrastructure and Instrumentation

Harvesting production data requires dedicated infrastructure. You need instrumentation in the production service to emit logs, a transport layer to move logs from the service to storage, a storage layer to persist logs efficiently, and a query layer to retrieve logs for analysis and training. Each component must be reliable, scalable, and maintainable. Logging is not a side project. It is production infrastructure that must meet the same standards as the rest of your system.

Instrumentation is the code that captures data and emits log events. Instrumentation must be lightweight, non-blocking, and robust to failures. If logging adds latency to user requests, users will suffer. If logging crashes when storage is unavailable, the entire service crashes. Instrumentation must handle failures gracefully by dropping logs rather than propagating errors. It must also handle high throughput without overwhelming downstream systems. Asynchronous logging, buffering, and rate limiting are standard techniques.

## Transport, Storage, and Query Layers

The transport layer moves logs from the service to storage. Common transport mechanisms include message queues like Kafka or RabbitMQ, streaming services like AWS Kinesis, or direct writes to cloud storage. The transport layer must handle backpressure, retries, and ordering guarantees. If the transport layer drops logs, you lose data. If it reorders logs, you lose temporal relationships. If it duplicates logs, you introduce bias. Transport reliability is critical.

The storage layer persists logs for long-term retention and query. Common storage systems include data warehouses like Snowflake or BigQuery, data lakes like S3 or GCS, or specialized logging systems like Elasticsearch. The storage layer must support efficient writes for high-throughput logging and efficient queries for analysis. It must also support compliance features like deletion, encryption, and access controls. Storage is not a commodity. It is a design decision that affects cost, performance, and compliance.

The query layer allows engineers to retrieve logged data for analysis, training, and debugging. The query layer may be SQL for structured data, full-text search for unstructured data, or custom APIs for complex queries. The query layer must be fast enough to support interactive exploration and scalable enough to handle large batch exports. It must also enforce access controls so that only authorized users can query sensitive data. The query layer is the interface between logged data and the people who learn from it. If querying is slow or difficult, logged data will not be used.

## Monitoring Logging Health: Detecting Gaps and Failures

Logging infrastructure can fail silently. A network partition may cause logs to be dropped. A schema change may cause logs to be misinterpreted. A sampling bug may cause logs to be biased. If you do not monitor logging health, you will not notice these failures until you try to use the data and discover it is incomplete or corrupted. Logging health monitoring is as important as monitoring the production service itself.

The simplest logging health metric is volume: how many examples were logged per hour or per day. A sudden drop in logged volume indicates a failure. A sudden spike may indicate a sampling bug or a traffic anomaly. You should set alerts on volume thresholds and investigate when volume deviates from expected patterns. Volume monitoring does not tell you if the logged data is correct, but it tells you if data is being logged at all.

## Schema Compliance and Sampling Bias Monitoring

A more sophisticated metric is schema compliance: what percentage of logged examples match the expected schema. If fields are missing, have unexpected types, or contain null values when they should not, the logged data is corrupted. Schema compliance monitoring detects instrumentation bugs, schema version mismatches, and data quality issues. You should set alerts when schema compliance drops below a threshold and investigate the root cause.

You should also monitor sampling bias by comparing logged distributions to expected distributions. If you log 10% of traffic uniformly, the distribution of user types, timestamps, and request types in the logs should match the distribution in production traffic. If the logged distribution is skewed, your sampling is biased. Sampling bias monitoring detects bugs in sampling logic, selection effects, and infrastructure failures that cause non-uniform logging.

Finally, you should monitor consent status: what percentage of logged examples have valid consent. If consent rates drop suddenly, it may indicate a user interface bug, a change in user behavior, or a compliance issue. Consent monitoring ensures that you are not logging data you do not have permission to use. All of these monitoring metrics should be tracked continuously, visualized in dashboards, and used to trigger alerts when anomalies occur. Logging is infrastructure, and infrastructure must be monitored.

The next subchapter covers how to clean, filter, and deduplicate logged data to prepare it for training and evaluation.
