# 4.12 â€” Data Quality SLOs: Availability, Freshness, Correctness, and Completeness

In March 2025, a machine learning platform at a logistics company ran a postmortem on a model deployment that had gone badly wrong. The new route optimization model had been tested thoroughly on historical data and passed all evaluation benchmarks with flying colors. Two weeks into production deployment, customer complaints about late deliveries spiked 40%. The investigation revealed that training data refreshes had been failing intermittently for six weeks before model training. The training dataset was 60% complete, mixing fresh data with weeks-old cached data, and missing entire geographic regions due to upstream API timeout issues. No alerts had fired. No dashboards showed red. The team had assumed data was fine because there was no explicit failure signal. The company spent $680,000 on service credits to affected customers and three months rebuilding trust with major accounts. The root cause was not a broken pipeline. It was the absence of explicit data quality commitments. The team had no SLO defining how available, fresh, correct, or complete their training data needed to be.

Data quality needs service level objectives just like API services do. You define explicit numerical targets for availability, freshness, correctness, and completeness. You measure actual data quality against those targets continuously. You maintain error budgets that track how much quality degradation you can tolerate before taking action. You alert when SLO violations occur. You treat data quality SLO violations with the same urgency you treat service uptime SLO violations. This is not bureaucratic overhead. It is operational discipline that separates professional data engineering from hope-driven development.

## The Four Core Data Quality Dimensions

Data quality SLOs cover four fundamental dimensions that collectively determine whether your data is usable for training and inference. Availability measures whether data exists and is accessible when needed. Freshness measures how recent your data is relative to the real-world events it represents. Correctness measures whether data values are accurate and conform to expectations. Completeness measures what percentage of expected data is actually present. Each dimension requires separate measurement, separate SLOs, and separate monitoring.

Availability is the foundation. If data does not exist or cannot be accessed, nothing else matters. Availability SLOs define what percentage of time your data pipeline must successfully produce usable datasets. A 99.9% availability SLO means your pipeline can be down or producing unusable data for at most 43 minutes per month. A 99% availability SLO allows up to 7.2 hours of downtime per month. You set availability SLOs based on how frequently you need fresh training data and how much delay you can tolerate in model updates.

Freshness measures the time lag between when real-world events occur and when data representing those events becomes available in your training dataset. A freshness SLO might specify that 95% of training data must be no more than 24 hours old, or that 99% of inference data must be no more than five minutes old. Freshness SLOs prevent training on stale data that no longer reflects current patterns. They ensure your models learn from recent examples and adapt quickly to changing conditions.

Correctness measures whether data values are accurate, valid, and conform to expected formats and constraints. A correctness SLO might specify that 99.5% of records must pass all validation checks, or that critical fields must have error rates below 0.1%. Correctness SLOs prevent training on corrupted, malformed, or nonsensical data that would degrade model quality. They enforce data integrity end-to-end from source systems through transformation pipelines to training datasets.

Completeness measures what fraction of expected data is actually present. A completeness SLO might specify that 98% of expected daily records must be successfully ingested, or that no more than 2% of critical fields can be null. Completeness SLOs prevent training on partial datasets that miss important patterns, underrepresent critical subgroups, or create biased models due to missing data. They ensure that data pipelines reliably capture the full scope of phenomena you are trying to model.

These four dimensions are not interchangeable. You can have highly available data that is stale. You can have fresh data that is incomplete. You can have complete data that is incorrect. You need explicit SLOs for all four dimensions to ensure your data is actually usable for production model training.

## Defining SLO Targets: Balancing Rigor and Realism

Setting appropriate SLO targets requires balancing the ideal data quality you want with the realistic data quality your sources and infrastructure can deliver. SLOs that are too strict create constant alerts and encourage teams to game metrics or ignore violations. SLOs that are too lax provide false confidence and allow quality degradation that hurts model performance. You set targets based on empirical observation, model sensitivity analysis, and business impact assessment.

You start by measuring current data quality without SLOs. Run your pipeline for several weeks while collecting metrics on availability, freshness, correctness, and completeness. Compute percentiles, identify typical ranges, and understand natural variation. This baseline data tells you what quality your current system delivers and where your biggest challenges are.

You analyze model sensitivity to each quality dimension. How does model performance degrade as training data gets older? What happens to precision and recall when you train on 95% vs 98% vs 99.9% correct data? How much does completeness matter for different features? You run controlled experiments varying each quality dimension and measuring impact on model metrics. This analysis tells you which quality dimensions matter most and what minimum quality levels maintain acceptable model performance.

You assess business impact of quality degradation. What does it cost when your fraud model misses transactions due to stale training data? What customer experience damage happens when your recommendation model trains on incomplete interaction data? What compliance risk emerges when your risk model uses incorrect attribute data? Business impact analysis converts abstract quality metrics into concrete cost and risk numbers that justify SLO targets.

You set SLO targets that are achievable with current infrastructure but require sustained operational discipline. A good SLO target is one that you meet 90-95% of the time with normal operations but occasionally violate when things go wrong. If you never violate your SLOs, they are too loose and provide little signal. If you constantly violate your SLOs, they are too tight and create alert fatigue. You tune targets over time based on observed violation rates and business impact.

You define separate SLOs for different datasets and use cases. Your production training dataset might have a 99.5% correctness SLO because model quality directly affects customer experience. Your experimental training dataset might have a 95% correctness SLO because it supports exploratory research where some noise is acceptable. Your inference data might have a 99.9% freshness SLO because stale features degrade real-time predictions. Different use cases have different requirements. Your SLO framework accommodates this variation explicitly.

## Measuring Against SLOs: Instrumentation and Calculation

SLO measurement requires continuous instrumentation that captures data quality metrics at every pipeline stage. You emit structured metrics, aggregate them into SLO calculations, and expose results through dashboards and alerts. SLO measurement is not a manual audit process. It is automated infrastructure that runs continuously alongside your data pipelines.

Availability measurement tracks successful pipeline executions versus total expected executions. You define what constitutes a successful execution: the pipeline completed, produced output data, and passed all quality gates. You track success and failure over rolling time windows. Your availability SLO calculation is successful executions divided by total expected executions over the measurement window. A 99% availability SLO measured over 30-day windows requires at least 29.7 successful daily executions out of 30 attempts.

Freshness measurement tracks the age of data at consumption time. You extract event timestamps from source data, compare them to current time, and compute age distributions. You calculate the percentage of records within your freshness threshold. If your SLO specifies that 95% of data must be less than 24 hours old, you measure the actual percentage meeting that threshold continuously. Freshness violations occur when the percentage drops below your SLO target.

Correctness measurement tracks validation failure rates across all quality checks. You count records that pass validation versus total records processed. You track error rates for critical fields separately from error rates for optional fields. You aggregate validation results into overall correctness percentages. If your SLO specifies 99.5% correctness, you measure whether at least 99.5% of records pass all validation checks in each pipeline run.

Completeness measurement tracks expected versus actual data volumes and field population rates. You establish expected record counts based on historical patterns, upstream system metrics, or business logic. You compare actual ingested records to expected counts. You measure null rates for critical fields. You calculate completeness as actual records divided by expected records and as populated fields divided by total field opportunities. Completeness violations occur when actual completeness falls below SLO targets.

You compute SLO compliance over defined measurement windows. A 30-day measurement window means you evaluate whether you met your SLO target over the last 30 days. A 7-day window provides faster feedback but is more sensitive to short-term variations. You choose measurement windows that match your operational cadence and business planning cycles. Monthly windows align with business reporting. Weekly windows support rapid iteration and tuning.

## Error Budgets: Managing Quality Degradation Allowances

Error budgets formalize how much quality degradation you can tolerate before taking action. If your availability SLO is 99%, your error budget is 1%. You can be unavailable for up to 1% of the measurement window without violating your SLO. Error budgets create explicit trade-offs between velocity and reliability. They let you make informed decisions about when to slow down and fix quality issues versus when to keep shipping features.

You track error budget consumption in real time. Your monitoring dashboards show current SLO compliance, error budget remaining, and burn rate. If you are consuming error budget faster than expected, you see this immediately and can decide whether to take corrective action. Error budget visibility turns quality management from reactive firefighting into proactive planning.

You use error budget policies to guide operational decisions. When error budget is healthy, you can take more risks with pipeline changes, deploy experimental features, and prioritize velocity. When error budget is depleted, you freeze non-critical changes, focus on stability, and invest in quality improvements. Error budget policies create clear decision criteria that balance innovation and reliability.

Error budget exhaustion triggers escalation and investment in reliability improvements. If you violate your availability SLO three months in a row, you do not just keep apologizing. You allocate engineering time to improve pipeline reliability until you have healthy error budget again. If freshness violations consume all your error budget, you invest in faster processing infrastructure or more frequent refresh schedules. Error budgets convert quality goals into resource allocation decisions.

You negotiate error budget allocation across teams and use cases. If your pipeline serves both production models and experimental research, you might allocate more error budget to production data and less to experimental data. If you support multiple business units with different quality needs, you might set different error budgets for each. Error budget allocation makes trade-offs explicit and ensures that critical use cases get the reliability they need.

## Alerting on SLO Violations: Fast Detection and Response

SLO violations need immediate visibility and clear escalation paths. You configure alerts that fire when SLO compliance drops below targets or when error budget burn rate exceeds safe thresholds. Alerts route to responsible teams with enough context to investigate and remediate quickly. SLO alerting is not optional monitoring. It is your primary mechanism for ensuring data quality commitments are met.

You implement both threshold alerts and burn rate alerts. Threshold alerts fire when SLO compliance drops below your target. If your availability SLO is 99% and actual availability drops to 98.5%, a threshold alert fires immediately. Threshold alerts provide fast detection of SLO violations but can be noisy if quality fluctuates around the threshold.

Burn rate alerts fire when error budget is being consumed faster than sustainable. If your 30-day error budget is 1% and you consume 0.5% in the first three days, you are on track to exhaust your budget in six days and violate your SLO. Burn rate alerts provide early warning before SLO violations occur and give you time to take corrective action. They are less noisy than threshold alerts because they focus on trends rather than instantaneous values.

You set multiple alert severity levels based on violation magnitude and duration. A brief dip below your SLO target might generate a low-severity warning that creates a ticket for investigation. A sustained violation over several hours might generate a medium-severity alert that pages during business hours. A catastrophic drop to 90% availability when your SLO is 99% might generate a high-severity page to on-call engineers immediately. Severity levels ensure appropriate urgency for different violation patterns.

Alert messages include SLO context and investigation starting points. An availability SLO violation alert includes current availability percentage, SLO target, error budget remaining, recent failure patterns, and links to relevant logs and dashboards. A freshness SLO violation alert includes current freshness percentile, SLO target, staleness trends, and affected data sources. Alerts answer what violated, by how much, and where to start debugging.

You track SLO violation response times and resolution times. How long does it take from alert firing to engineer acknowledgment? How long from acknowledgment to root cause identification? How long from identification to remediation? You use this data to improve incident response processes and validate that your alert routing reaches the right teams quickly.

## Connecting Data Quality SLOs to Model Quality SLOs

Data quality SLOs and model quality SLOs are not independent. Violations of data quality SLOs predict degradation in model quality SLOs. You establish explicit relationships between data quality metrics and model performance metrics. You use data quality SLO compliance as a leading indicator of model quality risk. You coordinate response to data quality violations with model quality monitoring.

You measure correlation between data quality SLO compliance and model performance metrics. When freshness SLO violations occur, does model recall drop? When correctness SLO violations occur, does precision degrade? When completeness SLO violations occur, do fairness metrics worsen? You analyze historical incidents to quantify these relationships. This analysis lets you predict model impact from data quality degradation and prioritize data quality improvements that matter most for model performance.

You incorporate data quality SLO compliance into model release criteria. Before deploying a new model to production, you verify that training data met all quality SLOs during the training window. If data quality SLOs were violated, you investigate whether the violations affected model quality and decide whether to proceed with deployment or retrain with higher-quality data. Data quality gates prevent deploying models trained on degraded data.

You trigger model retraining or rollback based on data quality SLO violations. If production data freshness violates SLOs for an extended period, your models are learning from stale patterns. You retrain with fresh data once freshness SLOs are restored. If correctness violations contaminated training data, you roll back to a model trained on clean data and retrain once correctness is fixed. Data quality SLO violations inform model lifecycle decisions.

You coordinate incident response across data and model teams. When a data quality SLO violation occurs, Data Engineering investigates pipeline failures while Data Science assesses model impact. You do not wait to see if models degrade before responding to data quality issues. You assume that data quality violations will affect model quality and prepare accordingly. Coordinated response reduces time to detection and mitigation when data quality issues cascade to model quality problems.

## SLO Review and Evolution: Tuning Based on Outcomes

Data quality SLOs are not set once and forgotten. You review SLO compliance regularly, analyze violation patterns, assess whether targets remain appropriate, and tune SLOs based on observed outcomes and changing business needs. SLO review is a continuous improvement process that keeps quality standards aligned with operational reality and business requirements.

You conduct monthly SLO reviews that examine compliance trends, violation patterns, error budget consumption, and business impact. You identify SLOs that are consistently met with margin to spare and consider tightening targets. You identify SLOs that are frequently violated despite best effort and consider loosening targets or investing in infrastructure improvements. SLO reviews turn quality management into a data-driven practice.

You track false positive and false negative rates for SLO alerts. A false positive is an alert that fired but did not indicate a real problem requiring action. A false negative is a real quality issue that did not trigger an alert. High false positive rates indicate SLO targets are too tight or measurement is too noisy. False negatives indicate SLO targets are too loose or measurement is incomplete. You tune targets and measurement logic to minimize both.

You analyze the relationship between SLO violations and business outcomes. Did data quality SLO violations predict customer complaints, revenue impact, or compliance issues? Did you meet all data quality SLOs but still experience model quality problems? This analysis validates whether your SLOs are measuring the right things and whether targets are calibrated appropriately.

You update SLOs when business requirements change. A new regulatory requirement might mandate fresher data, requiring tighter freshness SLOs. A shift to real-time inference might require higher availability SLOs. A move upmarket to enterprise customers might require higher correctness and completeness SLOs. You treat SLO updates as formal changes that go through review and approval, not informal adjustments made ad-hoc.

You document SLO rationale and change history. Each SLO includes why it was set at its current level, what analysis informed the target, and what business requirements it supports. When you change an SLO, you document what prompted the change and what you expect the impact to be. This documentation preserves institutional knowledge and prevents future teams from repeating past mistakes or abandoning hard-won quality standards without understanding why they exist.

## SLOs as Organizational Contracts

Data quality SLOs are not just technical metrics. They are organizational contracts that define expectations between data producers and data consumers. Data Engineering commits to delivering data that meets defined quality standards. Data Science commits to using data responsibly within the bounds of those quality standards. Product commits to accepting limitations when data quality constraints make certain features infeasible. SLOs create shared accountability and shared understanding.

SLOs enable productive conversations about trade-offs. Instead of arguing whether data quality is good enough, you refer to SLOs. If data meets SLOs, quality is acceptable by definition. If it does not meet SLOs, quality improvement is required. If new use cases need higher quality than current SLOs provide, you negotiate SLO changes with clear understanding of the infrastructure investment required. SLOs turn subjective quality debates into objective metric discussions.

SLOs document capabilities for external stakeholders. When Product asks if you can launch a new feature next week, you can point to data availability SLOs and show that you can deliver training data within required timelines. When Legal asks about data accuracy for compliance audits, you can show correctness SLO compliance and error budget tracking. SLOs provide evidence of data quality that supports business planning and risk assessment.

SLOs create accountability for reliability investment. If you consistently violate data quality SLOs, leadership can see this in SLO compliance reports and allocate resources to fix it. If you consistently meet SLOs with healthy error budgets, you demonstrate that current investment is sufficient and can redirect resources to new capabilities. SLOs make quality investment decisions transparent and data-driven.

SLOs align incentives across teams. Data Engineering is measured on data quality SLO compliance. Data Science is measured on model quality metrics that depend on data quality. Product is measured on features that require reliable data. When everyone is aligned on SLO targets and compliance tracking, teams naturally collaborate to maintain quality standards. SLOs create shared ownership of quality outcomes.

Data quality SLOs in 2026 are foundational infrastructure for professional AI development organizations. You define explicit numerical targets for availability, freshness, correctness, and completeness. You measure continuously against those targets. You track error budgets. You alert on violations. You connect data quality to model quality. You review and evolve SLOs based on outcomes. This is not overhead. This is how you build reliable AI systems that serve real users with real consequences. The teams shipping production AI successfully are the teams that treat data quality with the same rigor they treat service reliability. They set SLOs, measure compliance, and respond to violations with urgency. You should too.

The infrastructure for ensuring data quality does not stop at observability and SLOs. The next critical component is systematic versioning that lets you reproduce any training run, trace quality issues to specific data versions, and coordinate changes across teams safely.
