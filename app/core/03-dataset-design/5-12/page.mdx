# 5.12 â€” Pipeline Reliability Engineering: Idempotency, Backfills, and Reprocessing

In November 2024, a customer support AI platform experienced a catastrophic data pipeline failure that took 11 days to recover from and cost the company $1.7 million in lost revenue and emergency engineering time. The incident began when a routine infrastructure upgrade caused a transient network failure that interrupted their nightly dataset generation pipeline. The pipeline had processed 60 percent of the day's data when it crashed. The on-call engineer restarted it. The pipeline reprocessed the entire day from scratch, but it did not clean up the partial output from the failed run. The dataset now contained duplicate records for 60 percent of the day's data and single records for the remaining 40 percent. Downstream model training jobs ingested the corrupted dataset and learned distorted patterns. Quality metrics dropped by 22 percent. Customer escalations spiked. The team tried to fix the issue by running the pipeline a third time, but they still did not remove the duplicates. The dataset now had triplicates for 60 percent of records, duplicates for 40 percent, and singlets for the remainder. Five days into the incident, they realized they needed to scrap the corrupted dataset entirely and rebuild from source. But they had no backfill tooling. Their pipeline was designed to process today's data, not reprocess historical data. Engineers spent six days writing custom backfill scripts, validating outputs, and verifying that duplicates were eliminated before they could resume normal operations. The root cause was not the infrastructure failure. Infrastructure fails. The root cause was that the pipeline was not designed to handle failure. It was not idempotent. It did not clean up partial outputs. It could not reprocess data without creating corruption. It had no backfill capability. The team had built for the happy path and paid the price when reality diverged.

Pipeline reliability is not about preventing failures. Failures are inevitable. Reliability is about recovering from failures cleanly, quickly, and without manual heroics.

## Idempotency as a Design Principle

A pipeline is idempotent if running it multiple times on the same input produces the same output without side effects. This is the foundational principle of reliable data engineering. If your pipeline is not idempotent, every retry, every rerun, and every reprocessing attempt risks introducing corruption. If your pipeline is idempotent, you can rerun it as many times as needed without fear.

Non-idempotent pipelines append data. They write outputs to a table or file without checking whether the output already exists. When you rerun the pipeline, it writes the data again, creating duplicates. This is the most common failure mode. Your pipeline processes a batch of records, crashes halfway through, and on restart, processes some records a second time. Your output dataset now has duplicates for the records that were processed twice. Downstream consumers see inflated counts, duplicate training examples, or double-counted metrics.

Idempotent pipelines overwrite data. They define the output path or table partition based on the input batch identifier, and they write to that path using a mode that replaces existing data. If the pipeline runs once, it writes the output. If the pipeline runs a second time on the same input, it overwrites the previous output with identical data. The result is the same regardless of how many times you run it.

The simplest implementation is partition-based idempotency. Your pipeline processes data in batches defined by time windows, such as hourly or daily partitions. The output path includes the partition identifier. For a daily pipeline processing data for 2025-11-15, the output path is `s3://bucket/dataset/date=2025-11-15/`. When the pipeline writes, it deletes any existing data in that partition and writes new data. If the pipeline runs twice for the same date, the second run replaces the first run's output. No duplicates. No corruption.

File-level idempotency requires atomic writes. You write output to a temporary path, then atomically move it to the final path. If the pipeline crashes before the move completes, the temporary path contains partial data but the final path is unchanged. On restart, the pipeline writes to a new temporary path and moves it to the final path, overwriting any previous output. Most cloud storage systems support atomic move operations for single files. For multi-file outputs, you use manifest files or write to staging directories and promote them atomically using directory renames.

Database-level idempotency uses upsert semantics. Your pipeline writes records with unique identifiers. The database replaces existing records with matching identifiers and inserts new records. If the pipeline processes the same input twice, it writes the same records with the same identifiers, and the database state does not change. This works well for row-oriented datasets in Postgres, MySQL, or DynamoDB. It is less efficient for columnar datasets in data warehouses but still feasible using merge operations.

Idempotency is not optional. It is the minimum bar for production data pipelines. If your pipeline is not idempotent, you will corrupt data. It is only a matter of time. Every transient failure, every deployment bug, every infrastructure hiccup becomes a data quality incident. You cannot build reliable systems on non-idempotent foundations.

## Exactly-Once Semantics in Data Pipelines

Exactly-once processing is a stronger guarantee than idempotency. Idempotency means you can rerun the pipeline and get the same result. Exactly-once means that even if the pipeline crashes and restarts, every input record is processed exactly one time, never zero times and never more than once. This is critical for pipelines that perform aggregations, joins, or stateful transformations where processing a record twice produces incorrect results.

Exactly-once is hard to achieve. It requires coordination between the data source, the processing engine, and the data sink. The most common implementation uses distributed transactions. The processing engine tracks which input records have been processed, writes outputs and updates the processing state in a single atomic transaction, and commits the transaction only if both the output write and the state update succeed. If the transaction fails, the engine retries the entire batch. If the transaction succeeds, the engine moves to the next batch. This ensures that each input record contributes to the output exactly once.

Kafka provides exactly-once semantics for stream processing. When you read from a Kafka topic, process records, and write to another Kafka topic or a transactional sink, you can configure the pipeline to use Kafka's transactional API. The pipeline reads a batch of records, processes them, writes outputs, and commits the Kafka consumer offset in a single transaction. If the pipeline crashes before the transaction commits, the consumer offset is not updated, and the pipeline reprocesses the batch on restart. If the transaction commits, the consumer offset advances, and the pipeline never reprocesses those records. This works well for streaming pipelines with Kafka as both source and sink.

Batch pipelines use checkpoint files for exactly-once semantics. The pipeline reads input records, processes them, and writes outputs. After each batch completes successfully, the pipeline writes a checkpoint file recording which input batches have been processed. On restart, the pipeline reads the checkpoint file and skips any input batches that are already complete. This ensures that each batch is processed exactly once even across restarts. The checkpoint file must be written atomically after outputs are written. If you write the checkpoint before the output completes, you risk skipping batches that were never processed. If you write the output before the checkpoint, you risk reprocessing batches and creating duplicates unless your pipeline is also idempotent.

Idempotency plus deduplication achieves exactly-once in practice. If your pipeline is idempotent and you add a deduplication step at the end of processing, you get exactly-once behavior even if the pipeline reruns. The deduplication step removes duplicate records based on unique identifiers before writing the final output. This is simpler to implement than distributed transactions and works well for batch pipelines where the cost of deduplication is acceptable.

Not all pipelines need exactly-once semantics. If your pipeline is generating training data and a few duplicate examples do not meaningfully affect model quality, idempotency alone is sufficient. If your pipeline is aggregating metrics and double-counting a record causes incorrect dashboards, you need exactly-once. The distinction matters because exactly-once is more complex and more expensive. You pay for it in latency, infrastructure cost, and engineering time. You only implement it when the correctness requirement justifies the cost.

## Backfill Strategies and Historical Reprocessing

A backfill is reprocessing historical data to generate new outputs or update existing outputs. You need backfills when you fix a bug in your pipeline, when you add a new feature to your dataset, when you change your data model, or when you recover from a data corruption incident. Backfill capability is not optional. If you cannot backfill, you cannot fix mistakes. Your datasets are append-only, and every bug is permanent.

The first requirement for backfills is that your pipeline can process arbitrary date ranges. Many pipelines are hardcoded to process today's data. They read from a source table, filter for records with today's timestamp, process them, and write to an output partition named with today's date. This works for daily production runs. It does not work for backfills. To backfill, you need to parameterize the date. Your pipeline accepts a start date and end date as input, processes all data in that range, and writes to output partitions corresponding to those dates. This is a simple code change but it must be designed in from the start. Retrofitting date parameterization into a pipeline that was not built for it is painful.

The second requirement is source data retention. You cannot backfill data you no longer have. If your pipeline reads from a Kafka topic with a seven-day retention policy and you need to backfill data from 30 days ago, you are out of luck unless you archived the source data elsewhere. Reliable pipelines archive raw input data to long-term storage like S3 or GCS before processing. The archive is partitioned by date and retained for at least as long as you might need to backfill. A common retention policy is one year for active datasets and indefinite retention for compliance-critical datasets. The archive does not need to be optimized for query performance. It can be compressed JSON or Avro files that are cheap to store and read sequentially during backfills.

The third requirement is backfill orchestration. You do not run backfills manually. You build tooling that automates the process. The tool accepts a dataset name, a start date, and an end date as input. It generates a DAG of pipeline runs, one per day or one per hour depending on your partition granularity. It submits the runs to your scheduler, monitors their progress, retries failures, and reports completion. It verifies that the output partitions were written successfully and that they contain the expected number of records. It handles dependency ordering if multiple datasets need to be backfilled in sequence. This tooling is reusable across all datasets and becomes part of your platform infrastructure.

Backfills can be slow. Reprocessing a year of data for a pipeline that runs nightly takes 365 pipeline runs. If each run takes one hour, the backfill takes 365 hours, or 15 days. You cannot afford this latency for critical fixes. You parallelize. Your backfill tool submits runs for multiple dates in parallel, up to your cluster capacity. If you can run ten dates in parallel, the backfill completes in 36.5 hours instead of 365. You configure the parallelism based on the urgency of the backfill and the available compute resources.

Incremental backfills reduce cost. If you are fixing a bug that only affects data from the last 30 days, you do not reprocess the last year. You reprocess only the affected range. If you are adding a new feature to your dataset and downstream consumers do not need historical data for that feature, you start the backfill from today and let it run forward. Incremental backfills are faster and cheaper, but they require that you understand the scope of the issue and the requirements of consumers.

## Handling Pipeline Failures and Partial Outputs

Pipelines fail in the middle of processing. The instance crashes. The network times out. The disk fills up. The source system becomes unavailable. When this happens, your pipeline may have written partial outputs. You must handle partial outputs safely or you will corrupt your dataset.

The simplest approach is atomic partition replacement. Your pipeline writes all outputs for a partition to a staging path. Only after all outputs are written and validated does the pipeline move the staging path to the final path. If the pipeline crashes before the move, the staging path contains partial data but the final path is unchanged. On restart, the pipeline writes to a new staging path and discards the old one. The final path is always complete or absent, never partial.

Staging paths are named with unique identifiers to avoid collisions. If your pipeline processes the partition for date 2025-11-15, the staging path is `s3://bucket/dataset-staging/date=2025-11-15/run=20251115-143022/`. The run identifier includes a timestamp or UUID. If the pipeline crashes and restarts, the new run writes to a different staging path like `s3://bucket/dataset-staging/date=2025-11-15/run=20251115-150311/`. After the new run completes, it moves `run=20251115-150311/` to the final path `s3://bucket/dataset/date=2025-11-15/` and deletes the abandoned `run=20251115-143022/` staging path. This prevents partial data from leaking into the final path.

For multi-file outputs, you use manifest files. Your pipeline writes multiple output files to the staging path. After all files are written, it writes a manifest file listing the filenames and their checksums. The final atomic operation is moving the manifest file to the final path. Consumers read the manifest to discover which files are part of the complete output. If the manifest is absent, consumers know the output is incomplete and do not read it. This pattern works well for datasets stored as many small Parquet or Avro files.

Cleanup jobs delete abandoned staging paths. Your pipeline writes staging paths but crashes before moving them. Over time, staging paths accumulate. You run a periodic cleanup job that scans the staging directory, identifies runs older than 24 hours that were never promoted to the final path, and deletes them. This prevents storage bloat from failed runs.

Retries must be safe. When your pipeline fails, your orchestration system retries it. The retry must not assume the previous run left the system in a clean state. It must not append to partial outputs. It must overwrite or delete partial outputs. This is why idempotency matters. An idempotent pipeline can be retried safely. A non-idempotent pipeline requires manual intervention to clean up partial state before retry.

## Reprocessing Workflows and Version Control

Reprocessing is running your pipeline again on data it has already processed. You reprocess to fix bugs, to apply updated logic, to regenerate outputs with a new schema, or to recover from corruption. Reprocessing is distinct from backfilling. Backfilling processes historical data that was never processed before. Reprocessing replaces existing outputs with new outputs.

Reprocessing workflows require output versioning. You do not overwrite production outputs until you have verified that the reprocessed outputs are correct. Your pipeline writes reprocessed data to a versioned path like `s3://bucket/dataset/date=2025-11-15/version=v2/`. Consumers continue reading from `version=v1` while you validate `version=v2`. You compare record counts, run quality checks, spot-check examples, and verify that downstream consumers can parse the new version. Once validation passes, you promote `version=v2` to the default path and deprecate `version=v1`. If validation fails, you delete `version=v2` and debug the issue without disrupting production.

Versioned outputs consume more storage. You are storing both `v1` and `v2` until the transition is complete. For large datasets, this doubles your storage cost temporarily. You accept this cost because the alternative is deploying broken reprocessed data and causing incidents. Storage is cheaper than incidents.

Reprocessing pipelines are scheduled separately from production pipelines. You do not run reprocessing in the same DAG as your nightly production pipeline. You create a separate DAG for the reprocessing job, configure it to write to the versioned path, and run it on-demand or on a separate schedule. This isolates reprocessing failures from production and allows you to control resource allocation. If reprocessing is urgent, you allocate more resources. If it is background cleanup, you run it during low-traffic windows.

Reprocessing can be parallelized across partitions. If you are reprocessing a year of data, you submit runs for all 365 partitions in parallel. Each run writes to the versioned path for its partition. You validate all partitions in aggregate and promote them together. This reduces reprocessing time from weeks to days or hours.

Diff tools help validate reprocessing. You compare the old version and the new version of each partition and surface the differences. You check record counts, schema changes, field value distributions, and example records. If the reprocessing was intended to fix a bug, you verify that the bug is fixed. If it was intended to add a new field, you verify that the new field is populated. If it was intended to change a calculation, you verify that the calculation changed as expected. Automated diff tools catch issues that manual spot-checks miss.

## Monitoring and Alerting for Pipeline Health

Reliability requires visibility. You cannot fix failures if you do not know they happened. You need monitoring that detects pipeline failures, data quality issues, and performance degradation in real time.

Pipeline run status is the first metric. Your orchestration system tracks whether each pipeline run succeeded, failed, or timed out. You alert if a production pipeline fails. You alert if a pipeline that normally completes in one hour takes three hours. You alert if a pipeline retries more than twice. These alerts go to the on-call engineer and to the dataset owner team's Slack channel. They include links to logs, links to the orchestration UI showing the failed run, and the error message from the failure.

Data quality metrics are the second layer. Your pipeline emits metrics after processing each batch: record count, null rate per field, value distribution per categorical field, min-max-mean per numeric field, schema validation pass rate, and duplicate count. You compare these metrics to historical baselines. If today's record count is 30 percent lower than the 30-day average, you alert. If the null rate for a required field jumps from 0.1 percent to 5 percent, you alert. If duplicates appear when the pipeline is supposed to be idempotent, you alert. These alerts catch data quality regressions that would not trigger pipeline failures but would corrupt downstream consumers.

Latency metrics track how long pipelines take to complete. You measure total runtime, time per stage, and time per partition. You alert if latency increases significantly. Latency increases indicate performance regressions, infrastructure issues, or data volume growth that requires scaling. Catching latency increases early allows you to investigate and fix them before they cause SLA breaches.

Dependency metrics track upstream and downstream health. Your pipeline depends on source datasets produced by other teams. You monitor whether those source datasets are available and fresh. If the upstream dataset is late or missing, you alert the upstream team and delay your pipeline run rather than processing incomplete data. You also monitor downstream consumers. If your pipeline writes to a dataset that feeds a production model serving endpoint, you track whether the model successfully loaded the new dataset. If the load fails, you investigate whether the schema changed unexpectedly or the data is corrupted.

Alerting fatigue is a real problem. If your alerts are noisy and frequently false positives, engineers ignore them. You tune thresholds to minimize false positives. You suppress alerts during known maintenance windows. You implement smart alerting that escalates only if multiple metrics degrade simultaneously. You route low-severity alerts to Slack and high-severity alerts to PagerDuty. You regularly review alert history and disable alerts that never led to actionable incidents.

## Disaster Recovery and Dataset Reconstruction

Despite all precautions, catastrophic failures happen. Your dataset is corrupted beyond repair. Your source data is lost. Your pipeline logic has a bug that affects months of historical data. You need disaster recovery plans that allow you to rebuild datasets from scratch.

The foundation of disaster recovery is immutable source data. You never modify or delete source data. You only append to it. Your pipeline reads from source data, transforms it, and writes to output datasets. If output datasets are corrupted, you can always reprocess from source. If you have been modifying source data in place, you cannot recover. Immutable source data is non-negotiable.

Source data backups provide additional insurance. You replicate source data to a secondary region or a separate storage account. You enable versioning on your S3 buckets so deleted or overwritten objects can be recovered. You periodically test restores from backups to verify they work. Backups do not replace immutable source data, but they protect against accidental deletions or bucket misconfigurations.

Pipeline code versioning allows you to reproduce historical outputs. Your pipeline code is stored in Git with tags for every production deployment. If you need to rebuild a dataset from six months ago, you check out the code version that was running six months ago, configure it to process the date range you need, and run it. This reproduces the exact transformations that generated the original dataset. If your pipeline code is not versioned or if you have been making untracked manual edits, you cannot reproduce historical outputs.

Rebuild procedures are documented and tested. You write runbooks explaining how to rebuild each critical dataset from source data. The runbook includes which source tables to read, which code version to use, which configuration parameters to set, how to validate the rebuilt output, and how to promote it to production. You test the runbook by performing a rebuild drill quarterly. The drill verifies that the documentation is accurate, that engineers know how to execute it, and that the rebuild completes within your recovery time objective.

Recovery time objectives vary by dataset. For a dataset feeding a production model that serves customer traffic, your RTO might be four hours. For a dataset feeding internal analytics dashboards, your RTO might be 48 hours. You design your disaster recovery procedures to meet the RTO. If you cannot rebuild within the RTO using normal backfill procedures, you pre-provision standby infrastructure or build expedited rebuild pipelines that parallelize aggressively.

Point-in-time recovery is critical for compliance datasets. If you are subject to regulatory audit and the auditor asks for the state of a dataset as it existed on a specific historical date, you must be able to produce it. This requires either retaining historical snapshots of the dataset or retaining enough source data and code versioning to reproduce the snapshot. Snapshot retention is simpler but more expensive. Code versioning is cheaper but requires discipline to maintain reproducibility. Most teams use a hybrid: retain snapshots for the last 90 days and rely on code versioning for older dates.

## Capacity Planning and Resource Scaling

Reliable pipelines do not just handle failures. They also handle growth. Your data volume increases. Your processing logic becomes more complex. Your pipeline starts missing SLAs because it cannot scale. You need capacity planning to stay ahead of growth.

You measure data volume trends. You track input record count, input data size, and output data size per partition over time. You fit trend lines and project future growth. If your data volume is growing 15 percent per month, you know you will need to scale your pipeline within six months. You plan infrastructure upgrades before you hit capacity limits, not after your pipeline starts failing.

You measure compute utilization. You track CPU, memory, disk I/O, and network I/O during pipeline runs. If your pipeline is CPU-bound, you scale up to larger instance types or scale out to more instances. If it is memory-bound, you add RAM or refactor to process data in smaller chunks. If it is I/O-bound, you optimize data formats, enable compression, or use faster storage classes. You cannot optimize what you do not measure.

You implement backpressure and rate limiting. If your pipeline reads from a source system that cannot handle high query rates, you rate-limit your reads to avoid overwhelming the source. If your pipeline writes to a downstream system with throughput limits, you batch writes and implement retries with exponential backoff. If you do not implement backpressure, your pipeline becomes the denial-of-service attack that takes down shared infrastructure.

Autoscaling reduces cost and improves reliability. Your pipeline uses cloud-native autoscaling to add compute resources when data volume spikes and release them when volume drops. You configure autoscaling based on queue depth, partition count, or expected runtime. This keeps your pipeline responsive during peak loads without over-provisioning during off-peak times. Autoscaling is not fire-and-forget. You monitor autoscaling behavior and tune scaling policies based on observed performance.

Capacity planning is not a one-time exercise. You revisit it quarterly. You review data volume trends, processing latency trends, cost trends, and infrastructure utilization. You adjust scaling policies, upgrade instance types, and optimize pipeline logic to stay ahead of growth. Teams that neglect capacity planning discover the problem when their pipeline misses SLAs during a critical business event. Teams that plan ahead scale smoothly and avoid incidents.

Reliability engineering is not glamorous. It does not involve cutting-edge algorithms or novel architectures. It involves designing pipelines that handle mundane failures gracefully, that can reprocess data without corruption, that can recover from disasters without data loss, and that scale predictably as data volume grows. These are engineering fundamentals, not optional enhancements. The difference between production-grade dataset infrastructure and prototypes is reliability. Next, we examine another failure mode that reliability engineering must address: data that arrives late or corrupted, and how to handle it without poisoning your entire dataset.
