# 1.8 â€” Dataset Requirements Documents: What to Define Before You Collect

In early 2025, a financial services company decided to build an AI system for detecting fraudulent wire transfers. The product manager kicked off the project with a clear vision: reduce fraud losses by forty percent within six months while maintaining a false positive rate below two percent. The ML team immediately began collecting data.

They pulled three years of wire transfer records from the transaction database. Roughly eighteen million transfers in total. They sampled fifty thousand transfers and sent them to a labeling vendor with instructions to mark each as fraudulent or legitimate.

Four weeks later, the labels came back. The team split the data into training and validation sets, trained an initial model, and achieved ninety-two percent accuracy on the validation set. Everyone celebrated.

## The Production Reality Check

Two weeks into production testing with a limited rollout to one branch, the system flagged eleven percent of all transfers as suspicious. Ten times the target false positive rate. The fraud team was overwhelmed. Legitimate business customers were furious about delays. The product manager demanded to know what went wrong.

The ML lead reviewed the data and discovered several critical issues. First, the historical data was heavily imbalanced: only zero-point-three percent of transfers were actually fraudulent. But the random sample sent to labelers included a fifty-fifty mix, leading to a model that was calibrated for a world that did not exist.

Second, the labeling instructions had not defined what constituted fraud for edge cases. Transfers later reversed by the customer, transfers flagged by compliance but not confirmed as fraud, transfers to high-risk jurisdictions that were legitimate. Different labelers had made different judgment calls, introducing label noise that the model learned.

## The Missing Pieces

Third, the dataset included no examples of certain fraud patterns that had emerged in the past six months. Those patterns were rare and did not appear in the random sample. Fourth, the team had not collected metadata about transfer context, customer history, or time of day. Features that fraud experts knew were critical but that were not in the transaction table the ML team had pulled.

The entire effort had to be restarted. The team spent five weeks writing a detailed dataset requirements document. It specified exactly what data to collect, how much, from what sources, with what features, labeled according to what definitions, stratified to ensure coverage of rare fraud types, and validated to meet specific quality bars.

The second attempt took longer to execute but produced a dataset that actually worked. The product launched four months later than originally planned. But it met its performance targets within two weeks of production deployment.

## The Lesson in Lost Time

The project manager later told the executive team that the four months lost in the beginning could have been avoided. If they had spent two weeks defining dataset requirements before collecting a single example, they would have shipped on time with higher confidence.

This is the pattern repeated across hundreds of AI projects. Teams rush to collect data without defining what they need. They discover the gaps weeks or months later. They restart with proper requirements and succeed the second time.

## Why Requirements Documents Exist

A **dataset requirements document** specifies what data you need, why you need it, how much you need, in what format, with what labels, at what quality level, and on what timeline before you begin data collection or labeling. It serves the same function as a product requirements document or a technical design document.

It forces you to think through the problem deeply. It creates alignment across stakeholders. It establishes a shared understanding of success before committing resources.

Teams that skip this step waste weeks or months collecting data that turns out to be the wrong data. Labeled in the wrong way, at the wrong level of quality, or insufficient in volume or coverage.

## The Predictable Mistakes

The financial services team made every classic mistake of skipping requirements definition. They collected data without understanding the class distribution they needed to support. They labeled data without defining labeling criteria for ambiguous cases.

They sampled data without stratifying for rare but critical patterns. They pulled features without consulting domain experts about what actually mattered for the task.

Each of these mistakes was predictable and avoidable. But only if the team had paused to write down what they needed before starting work.

## Why Requirements Matter More in AI

Requirements documents are especially critical in AI because data collection and labeling are expensive, time-consuming, and often irreversible. If you collect the wrong data, you cannot go back in time and collect different data.

If you label data according to vague instructions, you cannot retroactively apply consistent criteria without relabeling. If you discover six weeks into labeling that you need additional features, you have to go back to the data source, re-extract data, and potentially relabel everything.

The cost of mistakes compounds. A two-week investment in writing requirements can save three months of wasted execution.

## The Contract Function

The document also serves as a contract between the team building the dataset and the stakeholders who will use it. Product needs to know what the dataset will enable and when it will be ready. ML engineering needs to know what data and labels they will receive and at what quality.

Domain experts need to know what scenarios and edge cases the dataset will cover. Leadership needs to know how much the effort will cost and what the expected return is.

Without a written requirements document, these expectations remain implicit, misaligned, and subject to misunderstanding. With a requirements document, everyone can review, debate, and agree before resources are committed.

## Who Writes Dataset Requirements Documents

Responsibility for writing the dataset requirements document typically falls to the ML engineer or applied scientist leading the project. Often in collaboration with the product manager and domain experts.

This is not a task that can be delegated to junior team members or outsourced to vendors. The person writing the requirements must understand both the product goals and the technical constraints of model training.

They must be able to translate a product objective like "reduce fraud losses by forty percent" into concrete data specifications. Like "we need at least five thousand labeled examples of confirmed fraud, stratified across the twelve most common fraud patterns, with metadata including customer tenure, average transaction volume, and transaction time."

## The Requirements Process

The process usually begins with the product or business stakeholder articulating the problem and the success criteria. The ML lead then works backward to determine what data would be required to build a model that could meet those criteria.

This often involves consulting domain experts to understand what features and scenarios matter. Reviewing existing data sources to see what is available. Estimating how much labeled data will be needed based on task complexity and desired performance. Identifying gaps that will require new data collection or feature engineering.

In organizations with dedicated data engineering or ML infrastructure teams, those teams often contribute to or review the requirements document. This ensures that the specified data can actually be collected and processed within the proposed timeline and budget.

## Review and Sign-Off

The document should be reviewed by all key stakeholders before data collection begins. Product reviews to confirm that the specified data will support the intended use case. Domain experts review to confirm that the labeling schema and coverage requirements make sense.

Engineering reviews to confirm that the data pipeline and storage approach are sound. Leadership reviews to confirm that the cost and timeline are acceptable.

This review process often surfaces misunderstandings or gaps that would otherwise not be discovered until much later. It is the last cheap opportunity to course-correct before expensive execution begins.

## The Anatomy of a Dataset Requirements Document

A well-structured dataset requirements document includes several standard sections. The first section states the **objective**: what product or model capability this dataset is intended to enable, what success looks like, and how the dataset will be evaluated.

For the fraud detection example, the objective might be: "Enable a production fraud detection model that reduces fraud losses by forty percent while maintaining a false positive rate below two percent on wire transfers within our retail banking division."

This objective grounds all subsequent decisions. Every choice about data sources, sample sizes, and labeling criteria must connect back to this objective.

## Data Sources Section

The second section specifies **data sources**: where the data will come from, what systems or databases will be queried, what time range will be covered, and what access or permissions are required.

For fraud detection, this might specify: "Wire transfer records from the core banking transaction database, covering January 2022 through December 2024, approximately eighteen million records. Requires read access to transaction table and customer profile table. Data extraction will be coordinated with data engineering team to avoid impacting production database performance."

This section makes operational dependencies explicit. If you need access you don't have, if extraction will take longer than expected, if the data source has quality issues, you discover these problems during requirements review, not during execution.

## Dataset Size and Composition

The third section defines the **dataset size and composition**: how many total examples are needed, how the data should be split across training, validation, and test sets, and how examples should be stratified or sampled to ensure coverage of important scenarios.

For fraud detection: "Target fifty thousand labeled examples. Stratified sampling to ensure at least five thousand confirmed fraud cases, covering the twelve primary fraud typologies identified by the fraud investigations team. Remaining examples sampled to reflect true class distribution in production, approximately zero-point-three percent fraud rate. Training set seventy percent, validation set fifteen percent, test set fifteen percent, with stratification maintained in each split."

This section prevents the class imbalance mistake the team made in their first attempt. It forces you to think about sampling strategy before you sample.

## Data Schema and Features

The fourth section specifies the **data schema and features**: what fields or attributes each example will include, what format each field will take, and what preprocessing or feature engineering will be applied.

For fraud detection: "Each example includes transaction amount, sender account ID, receiver account ID, timestamp, sender account tenure in days, sender average monthly transaction volume, receiver institution type, transaction country code, and whether the transaction was flagged by existing rule-based systems. All monetary amounts in USD. Timestamps in UTC. Account tenure calculated as of transaction date."

This section prevents the missing features problem. It forces consultation with domain experts about what actually matters for the task.

## Labeling Schema and Guidelines

The fifth section defines the **labeling schema and guidelines**: what labels will be applied, what each label means, how ambiguous cases should be handled, and what instructions labelers will receive.

For fraud detection: "Binary label: fraud or legitimate. Fraud is defined as a transaction confirmed by the fraud investigations team as unauthorized or part of a known fraud scheme. Transactions flagged but not confirmed are labeled legitimate. Transactions later reversed by the customer are labeled legitimate unless confirmed as fraud by investigations. Labeling will be performed by in-house fraud analysts following a detailed labeling guide to be developed in collaboration with the fraud investigations lead."

This section prevents label noise from inconsistent judgments. It establishes clear definitions before labeling begins.

## Quality Requirements

The sixth section establishes **quality requirements**: what accuracy or agreement rate is required for labels, what validation checks will be applied to ensure data quality, and what process will be used to handle quality failures.

For fraud detection: "Target ninety-five percent inter-rater agreement on a sample of five hundred examples labeled independently by two analysts. Any example with disagreement will be reviewed by the fraud investigations lead for adjudication. Automated validation will check for missing fields, out-of-range values, and duplicate transaction IDs. Examples failing validation will be flagged for manual review."

This section makes quality expectations concrete and measurable. It establishes the bar before work begins.

## Timeline and Milestones

The seventh section outlines the **timeline and milestones**: when data collection will begin and end, when labeling will be completed, when dataset versions will be delivered, and what the critical path dependencies are.

For fraud detection: "Data extraction: weeks one to two. Labeling guide development and analyst training: weeks two to three. Labeling execution: weeks four to seven. Quality validation and adjudication: week eight. Final dataset delivery: end of week eight. Critical path dependency: access to fraud investigations team for labeling guide review and adjudication, requires commitment of twenty hours over four weeks."

This section surfaces scheduling dependencies and resource constraints. If the fraud investigations team cannot commit twenty hours, you know that during planning, not during execution.

## Cost and Resources

The eighth section estimates **cost and resources**: how much the data collection and labeling will cost, what internal resources will be required, and what vendors or contractors will be engaged.

For fraud detection: "Estimated forty hours of data engineering time for extraction and preprocessing. Estimated two hundred hours of fraud analyst time for labeling. Estimated twenty hours of fraud investigations lead time for guideline development and adjudication. No external vendor costs. Total internal cost approximately thirty thousand dollars at blended rates."

This section forces realistic budgeting. If leadership is not willing to commit thirty thousand dollars, you know before you start work.

## Completeness Over Length

Not every dataset requirements document needs all eight sections. But the best documents address each of these areas explicitly. The goal is not to produce a bureaucratic artifact but to ensure that the team has thought through every dimension of what they need.

The document becomes a forcing function for completeness. If a section is difficult to write, that difficulty often reveals gaps in your understanding that must be addressed.

## Examples of Good Versus Bad Requirements

A bad dataset requirement is vague, unverifiable, or ambiguous. "Collect enough data to train a good model" is bad because it does not specify how much is enough or what good means.

"Label the data accurately" is bad because it does not define what accurate means or how accuracy will be measured. "Get data from production" is bad because it does not specify what production system, what time range, or what fields.

These requirements cannot be costed, scheduled, or validated. They leave too much room for interpretation and misalignment.

## Specificity and Measurability

A good dataset requirement is specific, measurable, and actionable. "Collect fifty thousand labeled examples, stratified to include at least five thousand fraud cases" is good because it specifies an exact number and a stratification requirement.

"Achieve ninety-five percent inter-rater agreement on a validation sample of five hundred examples" is good because it specifies a measurable quality bar and how it will be validated.

"Extract wire transfer records from the transaction database covering January 2022 through December 2024, including fields for amount, sender ID, receiver ID, timestamp, and country code" is good because it specifies the exact source, time range, and schema.

## Labeling Schema Example

Consider two labeling schema definitions for a content moderation task. Bad version: "Label content as safe or unsafe based on community guidelines."

Good version: "Label content as safe, unsafe, or ambiguous. Unsafe content includes explicit violence, credible threats, graphic sexual content, or hate speech targeting protected characteristics as defined in the company content policy document version three-point-two. Ambiguous content includes borderline cases where the labeler is uncertain; these will be escalated for review by the trust and safety lead. Safe content is everything else. Labelers will complete a two-hour training module and pass a qualification test with ninety percent accuracy before labeling production data."

The good version specifies three labels instead of two. It defines what each label means with reference to a specific policy document. It provides guidance on handling uncertainty. It specifies labeler training and qualification requirements.

## How Requirements Connect to Product Requirements

Dataset requirements are not created in a vacuum. They derive from product requirements. If the product requirement is "reduce fraud losses by forty percent," the dataset requirement must specify data sufficient to train a model capable of meeting that bar.

If the product requirement is "support content moderation in fifteen languages with consistent policy enforcement," the dataset requirement must specify labeled examples in all fifteen languages with labeling done by native speakers following a unified policy.

The connection is often mediated by a **model performance target**. The product manager specifies a business objective. The ML lead translates that objective into a model performance target, such as "detect ninety percent of fraud while keeping false positives below two percent."

## The Estimation Challenge

The ML lead then estimates what dataset size and quality would be required to achieve that target. Based on the complexity of the task, the number of features, the expected class distribution, and historical experience with similar problems. That estimate becomes the dataset requirement.

This translation is not mechanical. It requires judgment and often involves iteration. The initial dataset requirement might be "one hundred thousand labeled examples." After reviewing cost and timeline, the team might negotiate down to fifty thousand examples and accept a slightly lower model performance target.

Or negotiate up to seventy-five thousand examples by extending the timeline. The requirements document makes these trade-offs explicit and allows stakeholders to make informed decisions.

## Surfacing Infeasibility

In some cases, the dataset requirement reveals that the product requirement is not feasible. If the product team wants a model that detects a rare event type with high precision and recall, but there are only fifty historical examples of that event in existence, the dataset requirement process will surface this constraint.

Before months are spent on a doomed effort. The conversation can then shift to whether the product requirement should be revised, whether synthetic data or data augmentation can fill the gap, or whether an alternative approach is needed.

This is one of the most valuable functions of requirements documents: failing fast when a product goal is unachievable given data constraints.

## Common Mistakes and How to Avoid Them

The most common mistake is writing requirements that are too vague to be actionable. "Collect enough data" is not a requirement. "Collect ten thousand labeled examples" is a requirement.

Vague requirements cannot be costed, scheduled, or validated. They leave too much room for interpretation and misalignment. The fix is to force specificity: write down exact numbers, exact schemas, exact definitions.

## The Domain Expert Gap

The second most common mistake is not involving domain experts in requirements definition. ML engineers often write dataset requirements in isolation, based on what they think matters, without consulting the people who actually understand the domain.

This leads to datasets that miss critical edge cases. Use features that are not predictive. Or label according to definitions that do not match real-world decision criteria.

The fix is to make domain expert review a mandatory step in the requirements process. Incorporate their feedback seriously, even when it complicates the plan.

## Underestimating Cost and Timeline

The third common mistake is underestimating the cost and timeline. Teams frequently write requirements that call for one hundred thousand labeled examples without checking how much that will cost or how long it will take.

When reality hits, the plan collapses, and corners get cut. The fix is to estimate cost and timeline as part of the requirements process. Get leadership sign-off on those estimates before committing to the plan.

If leadership is unwilling to fund the full plan, negotiate scope reduction during requirements review, not during execution.

## The Living Document Problem

The fourth common mistake is not versioning or updating the requirements document. The initial document might be solid, but as the project progresses, new information emerges. You discover that a certain feature is not available in the data source. You realize that the labeling schema needs an additional category.

You learn that the target dataset size is insufficient based on early model experiments. If these changes are not reflected in an updated requirements document, the document becomes obsolete. Its value as a source of truth disappears.

The fix is to treat the requirements document as a living artifact. Update it as decisions are made and review it periodically to ensure it reflects current plans.

## When to Write Requirements and When to Skip Them

Not every data collection effort requires a formal requirements document. If you are collecting a few hundred examples for an early prototype to test feasibility, writing a ten-page requirements document is overkill.

In the early exploration phase, speed and iteration matter more than process rigor. You can collect some data, label it loosely, see if the approach has any promise, and then write formal requirements if you decide to proceed.

But once you are committing significant resources, formal requirements are essential. If you are planning to collect ten thousand or more labeled examples, if you are spending more than ten thousand dollars on labeling, if you are coordinating across multiple teams or vendors, you need a requirements document.

## The High-Stakes Scenarios

If you are building a dataset that will be used for a production model, if you are working in a regulated domain where data provenance matters, or if you are in an environment where mistakes are costly and hard to reverse, you need a requirements document.

The document does not have to be long. A good dataset requirements document is often three to six pages. The length matters less than the clarity and completeness.

Every key stakeholder should be able to read the document and understand what data will be collected, why, how, when, and at what cost. If the document achieves that, it has done its job.

Writing dataset requirements is the foundation of disciplined dataset engineering. It is the moment when you define what your dataset is your moat, the subject we turn to next.
