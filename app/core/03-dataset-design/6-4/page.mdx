# 6.4 — Stratified Sampling for Representative Eval Sets

In March 2025, a legal technology company deployed a contract analysis system that had scored 91% accuracy in pre-launch evaluation. Within two weeks of production deployment, the Trust and Safety team escalated: the system was failing catastrophically on commercial lease agreements, achieving only 63% accuracy on a document type that represented 18% of actual production traffic. The evaluation dataset had been constructed through random sampling of 2,000 historical contracts, which seemed rigorous. The problem was that commercial leases represented only 4% of the historical archive—they were disproportionately recent additions to the company's practice areas. Random sampling had selected only 80 lease contracts for evaluation, and by chance, 71 of those were simple residential leases. The nine commercial leases in the eval set were all from a single client in the retail sector. The system had never been tested against office leases, industrial leases, or ground leases. The company spent six weeks rebuilding their evaluation infrastructure and retraining their model, delaying a $4.2 million contract expansion. The root cause was not insufficient eval data volume—it was the naive assumption that random sampling produces representative coverage.

Random sampling treats all examples as equally likely to be selected. This is statistically unbiased in the abstract, but it produces systematically unrepresentative evaluation sets when your production distribution is imbalanced, heterogeneous, or structured. If 90% of your examples belong to Category A and 10% belong to Category B, random sampling gives you roughly 1,800 Category A examples and 200 Category B examples in a 2,000-example set. That seems proportional until you realize that Category B might be the high-value, high-risk segment—your pricing engine, your fraud detector, your medical triage system. You need sufficient statistical power to measure performance on Category B with confidence, and 200 examples rarely provides that power, especially if Category B itself contains subcategories with different difficulty profiles. Stratified sampling solves this by dividing your population into meaningful strata and sampling within each stratum according to a deliberate allocation strategy. You define what makes examples different in ways that matter for performance, and you ensure your evaluation set represents those differences with adequate coverage.

## Stratification Dimensions: Category, Difficulty, Source, and Segment

The first decision in stratified sampling is what dimensions to stratify on. You stratify on dimensions where you expect performance to vary and where you need independent measurement of that performance. The most common dimension is task category or document type. If your system processes invoices, receipts, and purchase orders, you stratify on document type. If your system answers questions, you might stratify on question type—factual, inferential, opinion, comparison. Category stratification ensures you measure performance on each task type independently rather than allowing dominant categories to mask failures in rare ones.

The second dimension is difficulty. Not all examples within a category are equally hard. Some invoices are clean, single-page, standard formats from major vendors. Others are handwritten, multi-page, non-standard layouts from international suppliers with currency conversions and tax complications. Some questions are simple lookups with one unambiguous answer. Others are multi-hop reasoning problems requiring synthesis across multiple contradictory sources. If you sample only by category, you might get 500 invoice examples that are all easy and 50 that are hard, or vice versa. You cannot assess whether your system handles edge cases if your eval set undersamples them. Difficulty stratification typically requires a difficulty annotation or heuristic—manual labels from domain experts, rule-based proxies like document length or query complexity, or difficulty scores from a pilot model. You then sample within each difficulty tier to ensure adequate representation of easy, medium, and hard examples.

The third dimension is source or provenance. Examples from different sources often have different characteristics. User-generated data differs from synthetic data. Data from Customer A differs from Customer B. Data from the web differs from data from internal databases. Data from 2024 differs from data from 2026. If your training data is 70% synthetic and 30% real user data, but your eval set is 95% synthetic because synthetic data is easier to label and access, you will overestimate production performance. Source stratification ensures you evaluate on data that reflects the true distribution of where production inputs come from. This often requires metadata tracking—tagging each example with its source system, customer ID, collection date, and generation method.

The fourth dimension is user segment or use case. Different users use your system differently. Enterprise customers submit different queries than free-tier users. Nurses use your clinical assistant differently than physicians. Users in North America phrase requests differently than users in Europe or Asia. If you stratify by user segment, you can measure whether your system performs equitably across segments and detect when one segment is systematically underserved. This is especially critical for systems subject to fairness or compliance requirements. You cannot claim equitable performance if you never measured performance by segment.

You do not stratify on every possible dimension. Stratification increases complexity and reduces sample size per stratum. If you stratify on five dimensions with three levels each, you create 243 strata. If you have 2,000 evaluation examples, that is eight examples per stratum—too few to measure anything reliably. You choose the two to four dimensions where performance variation is largest, where independent measurement is most important, and where you have the metadata to stratify reliably. You combine or ignore other dimensions.

## Proportional vs Equal Allocation

Once you have defined your strata, you must decide how many examples to sample from each. The two primary strategies are proportional allocation and equal allocation. Proportional allocation samples from each stratum in proportion to that stratum's size in the production population. If Category A is 80% of production traffic and Category B is 20%, you allocate 80% of your eval budget to Category A and 20% to Category B. Proportional allocation ensures that your overall eval score reflects the production-weighted average performance. If you evaluate on 1,600 Category A examples and 400 Category B examples, and you weight those scores by stratum size, you get an overall accuracy that matches what you would see on a random production sample.

Proportional allocation is appropriate when you care primarily about overall system performance and when stratum-level performance does not have independent significance. If your system processes a mix of task types and you measure only a single aggregated SLA—95% accuracy overall—then proportional allocation makes sense. You allocate eval resources where they have the most impact on your overall metric.

Equal allocation samples the same number of examples from each stratum regardless of stratum size. If you have five categories, you sample 400 examples from each, giving you 2,000 examples total with balanced coverage. Equal allocation ensures you have sufficient statistical power to measure performance within each stratum independently. If Category E represents only 2% of production traffic but is high-risk or high-value, you might need 300 examples to detect a 5-point accuracy drop with confidence. Proportional allocation would give you only 40 examples—too few for reliable measurement. Equal allocation solves this by oversampling rare strata and undersampling common strata.

Equal allocation is appropriate when you have stratum-specific SLAs, when rare strata are high-stakes, or when you need to detect performance regressions in small segments. If your contract says you will deliver 90% accuracy on every document type, not 90% overall, you need equal allocation to measure each type independently. If your fraud detection system processes 98% legitimate transactions and 2% fraud, you need to oversample fraud cases to evaluate fraud detection performance—proportional allocation would give you 40 fraud examples, which is useless.

The tradeoff is that equal allocation distorts your overall eval score. If you evaluate on 400 examples from each of five categories, your overall accuracy is an unweighted average of five category-level accuracies. That does not reflect production performance if categories have different prevalence. You can correct this by computing a weighted average using production prevalence as weights, but then you are reweighting your eval results, which introduces complexity. You can also run two eval sets—one proportionally allocated for overall measurement and one equally allocated for stratum-specific measurement. This doubles your labeling cost but gives you both views.

A middle-ground strategy is minimum-threshold allocation. You define a minimum sample size for each stratum—typically 200 to 500 examples depending on your precision requirements—and then allocate the remaining budget proportionally. If you have 2,000 examples and five strata, you allocate 200 to each stratum first, consuming 1,000 examples. You allocate the remaining 1,000 proportionally. This ensures every stratum has enough examples for independent measurement while still weighting your eval toward common strata. This is the most common approach in production systems with heterogeneous traffic.

## Handling Rare Categories and Long-Tail Distributions

Most production systems have long-tail distributions. A small number of categories account for most traffic, and a large number of categories account for very little. If you have 50 document types but 5 types represent 80% of volume and the other 45 represent 20%, stratifying on all 50 types creates 45 rare strata with very few examples each. You cannot evaluate on 10 examples per stratum and get meaningful signal.

The first strategy is collapsing rare strata. You group low-volume categories into a single "other" or "rare" stratum and sample from it as a unit. If 45 document types collectively represent 20% of traffic, you treat them as one stratum and sample 400 examples from it. This gives you adequate coverage of the long tail without fragmenting your eval budget. The downside is you lose category-specific measurement for rare types. You know your system performs at 87% accuracy on rare documents overall, but you do not know which rare types are failing. You accept this tradeoff if rare types are low-stakes or if you plan to investigate failures manually.

The second strategy is adaptive stratification. You start with coarse strata—major categories and "other." You evaluate performance. If the "other" stratum performs poorly, you decompose it into sub-strata and resample to identify which rare categories are failing. You iterate until you isolate the problem categories. This is more expensive because it requires multiple eval rounds, but it avoids over-investing in rare categories that perform fine.

The third strategy is risk-based prioritization. You do not treat all rare categories equally. You identify which rare categories are high-risk, high-value, or compliance-critical, and you oversample those while ignoring the rest. If 40 of your 45 rare document types are low-stakes internal forms and 5 are regulated financial disclosures, you stratify on the 5 regulated types and collapse the other 40 into "other." You allocate eval resources where risk is concentrated.

The fourth strategy is exclude-and-monitor. You explicitly exclude very rare categories from your evaluation set and rely on production monitoring to detect failures. If a document type appears fewer than 100 times per month in production, you cannot build a statistically robust eval set for it anyway. You instrument production to log every instance of that type, you review failures manually, and you add adversarial examples to your eval set if you find systematic issues. This is pragmatic for categories that are truly rare and where pre-deployment eval is infeasible.

## Stratification by Difficulty

Stratifying by difficulty requires a difficulty definition and a difficulty labeling process. The simplest approach is rule-based heuristics. For document extraction, difficulty might correlate with page count, text density, or presence of tables. For question answering, difficulty might correlate with query length, number of entities, or syntactic complexity. For classification, difficulty might correlate with label ambiguity or distance from prototypical examples. You define a heuristic, score every example, bin examples into easy, medium, and hard tertiles, and sample from each bin.

Heuristics are fast and scalable but often wrong. A short question is not always easy. A long document is not always hard. If your heuristic correlates only weakly with true difficulty, stratification does not help. The better approach is human-labeled difficulty. Domain experts review a sample of examples and rate difficulty on a three- or five-point scale. You use those ratings to stratify. This is expensive but accurate. You can also use a pilot model to estimate difficulty—run a baseline model on all examples, compute confidence scores, and treat low-confidence examples as hard. This assumes low confidence correlates with difficulty, which is often but not always true.

Once you have difficulty labels, you decide on allocation. Equal allocation across difficulty bins ensures you measure easy, medium, and hard performance independently. Proportional allocation weights your eval toward the difficulty distribution in production. If 60% of production inputs are easy, 30% medium, and 10% hard, proportional allocation gives you 1,200 easy, 600 medium, and 200 hard examples. This reflects production impact but might undersample hard cases. Many teams use minimum-threshold allocation—at least 300 examples per difficulty bin, with remaining budget allocated proportionally.

Difficulty stratification also informs where to focus model improvement. If your system achieves 96% on easy examples, 89% on medium, and 71% on hard, you know the performance gap is in hard cases. You can prioritize data collection, prompt engineering, or model selection for hard cases specifically. Without difficulty stratification, an overall 91% accuracy score tells you nothing about where the gap is.

## Stratification by Source and Temporal Distribution

Source stratification ensures your eval set reflects where production data comes from. If 50% of production inputs come from mobile app, 30% from web, and 20% from API integrations, you sample 1,000 from mobile, 600 from web, and 400 from API. This is straightforward if your data warehouse tags every example with source metadata. If it does not, you must infer source from other fields or backfill metadata before stratification is possible.

Source matters because different sources often have different quality profiles. Mobile inputs are shorter and more likely to contain typos. Web inputs are more verbose. API inputs are more structured but might come from a single customer with idiosyncratic formatting. If you evaluate only on web data, you overestimate mobile performance. If you evaluate only on API data from Customer A, you have no idea how your system performs on Customer B.

Temporal stratification ensures your eval set reflects recent data. Production distributions shift over time. User behavior changes. Product features evolve. External events introduce new topics or phrasings. If your eval set is frozen from 2024 and it is now 2026, it does not represent current production. Temporal stratification typically means sampling from recent time windows—most examples from the last three months, some from the last year, a few from older periods to maintain continuity. You avoid sampling too heavily from a single week or month to prevent seasonal or event-driven skew.

One common approach is rolling windows. Every quarter, you refresh 25% of your eval set with new examples from recent production. You maintain 75% continuity for trend tracking and introduce 25% novelty to keep the set current. This requires a versioning system for your eval set and careful tracking of which examples are in which version. It also requires re-labeling new examples, which is ongoing cost.

## Metadata Requirements and Stratification Feasibility

Stratified sampling is only feasible if you have the metadata to stratify on. If you want to stratify by document type but your data warehouse does not record document type, you cannot stratify. You must either infer document type from content using a classifier, manually label a sample, or skip that dimension. Metadata gaps are one of the most common reasons teams fall back to random sampling—not because random sampling is better, but because they lack the infrastructure to do anything else.

Building stratification-ready infrastructure means tagging every example with relevant metadata at ingestion time. When a user submits a query, you log query text, user ID, session ID, source platform, timestamp, and inferred query type. When a document is uploaded, you log document type, page count, source system, upload timestamp, and customer ID. You store this metadata in your data warehouse alongside the raw data. You design your schema to support filtering and grouping by these dimensions. This infrastructure pays off not only for evaluation but also for monitoring, debugging, and analytics.

If you lack metadata for historical data, you can backfill it for a sample. You manually label 5,000 examples with document type and difficulty, you train a classifier on those labels, and you predict labels for the remaining million examples. You treat predicted labels as noisy metadata and stratify accordingly. This is not as good as ground-truth metadata but better than nothing.

## Stratification and Eval Signal Quality

Stratified sampling improves eval signal quality in three ways. First, it ensures adequate coverage of important subpopulations. If rare categories, hard examples, or specific user segments are undersampled in random sampling, stratified sampling corrects that. You measure what you need to measure, not just what is common.

Second, it increases statistical power for stratum-specific metrics. If you need to detect a 5-point accuracy drop in Category B, you need enough Category B examples to make that drop statistically significant. Proportional sampling might give you too few; equal or minimum-threshold sampling gives you enough.

Third, it reduces variance in eval scores across different random samples. If you draw two random samples from an imbalanced population, the number of rare-category examples varies significantly between samples. One sample might have 30 fraud cases, another 60. Your overall accuracy estimate fluctuates depending on which sample you drew. Stratified sampling fixes the number of examples per stratum, eliminating that source of variance. Your eval scores are more stable across resampling.

The cost of stratification is complexity. You must define strata, label examples with stratum metadata, choose allocation strategy, and track stratum membership in your eval dataset. You must also recompute overall metrics as weighted averages if you use non-proportional allocation. This is more work than random sampling, but it is necessary work if you want representative, reliable evaluation. Random sampling is only appropriate when your production distribution is uniform, which it almost never is.

Your next step is ensuring that the subset of evaluation data you rely on most—your golden set—is constructed and maintained with even higher rigor than your general eval set, so you have a stable, trusted benchmark that anchors all model and prompt improvements.
