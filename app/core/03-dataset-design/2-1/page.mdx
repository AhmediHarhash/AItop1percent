# 2.1 â€” The Three Sources: Production Logs, Synthetic Generation, External Procurement

In mid-2025, a healthcare technology company burned through $340,000 in external data procurement licenses before discovering that 60% of their purchased medical dialogue dataset contained patterns their actual users never exhibited. The company had built a patient intake chatbot trained primarily on licensed conversational data from a vendor specializing in telehealth interactions. The vendor's data came from text-based telehealth platforms serving primarily younger, tech-savvy patients. The company's actual user base skewed older, with different communication patterns, different chief complaints, and different expectations about formality and medical terminology. The model performed beautifully on benchmark evaluations using held-out portions of the licensed dataset. It failed immediately in production because real patients asked questions the purchased data never anticipated, used phrasing the model had never seen, and expected responses calibrated to a different demographic profile.

The root cause was not that external data is inherently bad. The root cause was treating data sourcing as a procurement decision rather than a strategic architecture decision. The team chose external data because it was immediately available, properly formatted, and came with quality guarantees from the vendor. They never asked whether the distribution of that data matched the distribution they would see in production. They never built infrastructure to capture their own production logs. They never considered synthetic generation as a way to fill specific gaps. They treated data sourcing as a one-time vendor selection problem instead of an ongoing engineering discipline that required understanding the quality profiles, cost structures, and distribution characteristics of three fundamentally different data sources.

Every dataset you build will draw from one or more of three sources: production logs from your own system's real usage, synthetic generation using language models to create new examples, or external procurement through purchase or licensing. Each source has different strengths, different failure modes, and different implications for cost, quality, distribution match, legal compliance, and long-term maintainability. Understanding when to use each source, how to combine them, and how to avoid over-reliance on any single source is foundational to dataset engineering. You cannot build production-grade AI systems without making deliberate, informed choices about where your training and evaluation data comes from.

## Production Logs: The Gold Standard for Distribution Match

Production logs are records of real interactions between your system and real users in real contexts. When you log user queries, model responses, user feedback signals, and contextual metadata from actual production traffic, you capture the true distribution of what your system encounters. Production data is the gold standard for distribution match because it represents reality by definition. If you are building a customer service chatbot, production logs show you exactly what customers actually ask, exactly how they phrase their questions, exactly what ambiguities and edge cases occur in practice. No other data source can match this fidelity to real-world usage.

Production data also captures shifts over time. User behavior changes, product features evolve, business context shifts, and external events create new patterns. When you continuously log production traffic, your dataset automatically reflects these changes. You do not need to predict what future queries will look like. You simply capture them as they happen.

This makes production logs essential for detecting distribution drift, identifying new failure modes, and maintaining model performance as the world changes around you. A customer service system in early 2024 might have seen queries dominated by account access issues. By mid-2025, after the company launched new product features, the query distribution shifted toward feature troubleshooting and integration questions. Production logs captured this shift automatically. External data from 2023 did not.

The primary limitation of production logs is that you cannot collect them until you have a production system. If you are building a new feature, launching a new product, or entering a new market, you have no production traffic to log. You must bootstrap with other data sources first, then transition to production logs once real usage begins.

The secondary limitation is volume and coverage. Early in a product's life, production traffic may be sparse, skewed toward early adopters, or concentrated in a narrow set of use cases. You may have plenty of data for common queries but almost nothing for edge cases, failure scenarios, or less frequent interaction patterns. Production logs give you distribution match, but they do not automatically give you comprehensive coverage.

## Why Production Data Captures Real Complexity

Production logs include the messiness that synthetic and external data often lack. Users make typos, use non-standard phrasing, switch languages mid-sentence, provide incomplete information, or reference context from previous interactions. They ask ambiguous questions that require clarification. They express frustration, urgency, or uncertainty in ways that affect how the model should respond. Real production data contains all of this complexity because real users are not following a script.

A financial services company found that 22% of their production queries included references to prior conversations that were not present in their external training data. Users would ask questions like "what about the account we discussed yesterday" or "can you check on that transfer from last week" without providing account numbers or transaction details. The model trained on clean external data had no idea how to handle these contextually dependent queries. The model fine-tuned on production logs learned to recognize when context was missing and ask clarifying questions.

Production data also reveals edge cases that are difficult to anticipate. A legal document analysis system discovered through production logs that 8% of uploaded contracts included handwritten annotations scanned as part of the PDF. The external training data consisted of clean, digital-only contracts. The model had never seen handwritten text overlaid on printed documents. Performance on these real-world edge cases was abysmal until the team incorporated production examples.

## The Infrastructure Prerequisites for Production Logging

The quality of production logs depends entirely on what you choose to log, how you sample, and how you handle consent and privacy. Logging everything is expensive, often legally problematic, and generates massive volumes of redundant data. Logging too little means you miss critical patterns. Intelligent logging infrastructure is not optional. It is a prerequisite for using production data effectively.

You need to instrument your production systems to capture not just the final user query and model response, but also the intermediate signals, feature vectors, confidence scores, retrieval results, and contextual metadata that influenced the model's decision. When a failure occurs, you need enough information in the logs to reproduce the model's behavior, understand why it made the choice it did, and identify what should have happened instead.

You need sampling strategies that balance volume with coverage. Uniform random sampling captures the overall distribution but may miss rare events. Stratified sampling over-samples failures, low-confidence predictions, and negative user feedback to ensure you have sufficient examples of problematic cases. You need to track sampling metadata so you can correct for sampling bias during training.

You need consent and privacy infrastructure that complies with GDPR, HIPAA, and other regulations. Production logs often contain personally identifiable information that requires explicit user consent before you can use it for training. You need mechanisms to obtain consent, track consent status, honor deletion requests, and anonymize or pseudonymize sensitive data. We cover all of these logging infrastructure requirements in depth in the next subchapter.

## Synthetic Generation: Controlled Diversity at Scale

Synthetic generation uses language models to create training and evaluation examples programmatically. You provide the model with instructions, few-shot examples, or structured prompts, and it generates new data that matches the pattern you specified. Synthetic generation is valuable because it gives you control over what you create. If you need 500 examples of customer service queries about password resets, you can generate them. If you need adversarial examples testing edge cases in medical diagnosis, you can generate them. If you need multilingual variants of existing data, you can generate them.

Synthetic data lets you fill gaps, balance distributions, and create coverage for scenarios that rarely appear in production logs. A fraud detection system might see millions of legitimate transactions in production but only hundreds of actual fraud cases. Generating synthetic fraud examples allows you to train the model on a more balanced dataset without waiting years to accumulate enough real fraud cases.

Synthetic generation is also fast and cheap relative to human annotation. Generating 10,000 synthetic examples might cost a few hundred dollars in API calls and a few hours of engineering time. Annotating 10,000 real production examples with high-quality labels might cost tens of thousands of dollars and weeks of annotator time. For tasks where human judgment is expensive or slow, synthetic generation provides a scalable alternative. You still need human review to verify quality, but the initial generation step is far cheaper than manual creation.

## The Distribution Mismatch Risk of Synthetic Data

The primary risk of synthetic generation is distribution mismatch. Language models generate data that reflects their training distributions, not necessarily your production distribution. If you ask GPT-4 to generate customer service queries, it will produce queries that sound plausible based on its training data, but those queries may not match the actual phrasing, tone, complexity, or demographic signals of your real users. Synthetic data can be too clean, too grammatically correct, too polite, or too focused on common cases.

It can lack the messiness, ambiguity, and edge cases that define real-world usage. A content moderation system trained exclusively on synthetic examples of policy-violating content performed poorly in production because real violators used slang, misspellings, deliberate obfuscation, and cultural references that the synthetic data generator never produced. The synthetic examples were textbook violations. The real violations were creative attempts to evade detection.

If you train exclusively on synthetic data, you risk building a model that performs well on synthetic evaluations but fails on real production traffic. This is the synthetic-to-real gap, and it can be large. A sentiment analysis system trained on synthetic product reviews achieved 94% accuracy on a synthetic test set but only 71% accuracy on real customer reviews. The synthetic reviews used standard phrasing and clear sentiment indicators. Real reviews were sarcastic, included mixed sentiments, or buried the key opinion in tangential commentary.

## Model Collapse and Bias Amplification

The secondary risk is model collapse or amplification of biases. If you use a language model to generate training data, then train a new model on that synthetic data, you are training on the outputs of the first model. If the first model has biases, errors, or stylistic quirks, the second model may inherit and amplify them. If you then use the second model to generate more synthetic data and train a third model, the problems compound.

This is not a theoretical concern. Multiple research groups have documented performance degradation when models are trained iteratively on synthetic data without sufficient real-world grounding. The models become increasingly confident in their own patterns and increasingly disconnected from real data distributions. The technical term is model collapse, and it happens when synthetic data feedback loops dominate training.

A financial forecasting company used GPT-4 to generate training data for a text-to-SQL model, then trained a fine-tuned model on that data, then used the fine-tuned model to generate additional training examples. By the third iteration, the model had developed a strong preference for certain SQL syntax patterns that GPT-4 favored but that were inefficient for the company's database schema. The model was fluent but impractical.

## Legal and Ethical Considerations for Synthetic Data

You also need to be careful about legal and ethical implications. Synthetic data generated by a language model may reflect copyrighted patterns, personally identifiable information from the model's training set, or biased stereotypes. Using synthetic data in regulated domains like healthcare or finance requires the same scrutiny as real data. Just because data is synthetic does not mean it is free from legal or ethical obligations.

If your synthetic generation prompt includes proprietary information or copyrighted examples, the generated outputs may constitute derivative works subject to the original copyright. If your synthetic data includes realistic names, addresses, or personal details that happen to match real individuals, you may have inadvertently created privacy violations. If your synthetic data reflects gender, racial, or cultural biases present in the generation model's training data, you are baking those biases into your training set.

Synthetic generation is a tool, not a replacement for real data. It works best when combined with production logs to fill specific gaps, not as the sole data source. The rule is to use synthetic data to augment, not to replace. Generate synthetic examples for scenarios underrepresented in production logs. Generate synthetic adversarial examples to test robustness. Generate synthetic multilingual variants to expand language coverage. But always ground your training in real production data as the foundation.

## External Procurement: Speed and Coverage with Distribution Risk

External procurement means purchasing or licensing datasets from third-party vendors, research institutions, or open-source communities. External data is valuable because it provides immediate volume and coverage. If you are building a sentiment analysis system, you can license a dataset of millions of labeled reviews instead of annotating your own. If you are building a medical coding system, you can purchase a dataset of clinical notes with ICD-10 codes instead of negotiating data-sharing agreements with hospitals.

External data accelerates development by giving you a starting point without requiring months of data collection infrastructure. A startup building a document classification system can license a dataset of pre-labeled business documents and have a baseline model running in two weeks. Building the same dataset from scratch would require hiring annotators, developing annotation guidelines, building quality control processes, and spending three to six months on data collection.

External data also provides access to expertise and scale you may not have in-house. Specialized vendors invest heavily in data quality, annotation guidelines, and domain expertise. A vendor focused on legal document analysis may have better-quality legal training data than you could produce internally without hiring legal experts. A vendor focused on multilingual data may have native speakers for 50 languages, which would be impractical for most companies to assemble. External procurement lets you buy expertise and scale that would be prohibitively expensive to build yourself.

## The Distribution Mismatch Problem with External Data

The primary risk of external procurement is distribution mismatch, just as with synthetic data, but for different reasons. External datasets are collected in different contexts, from different user populations, with different incentives and constraints than your production environment. The telehealth dialogue data in the opening story came from a different demographic, a different product interface, and a different set of chief complaints than the healthcare company's actual users. The data was high quality by objective measures, but it was the wrong distribution for the task.

This is the most common failure mode of external procurement. The data looks good in isolation but does not match the reality of your production system. A customer support chatbot trained on external customer service conversations from e-commerce companies performed poorly when deployed in a B2B SaaS context. The vocabulary, the question patterns, the resolution expectations, and the technical depth were all different. The external data was high quality for e-commerce support, but the company was not in e-commerce.

Distribution mismatch is insidious because it does not show up in standard evaluation metrics if your evaluation data comes from the same external source as your training data. The model looks great on held-out external test examples. It fails when real users interact with it. The mismatch only becomes visible when you evaluate on production data, which you may not have when you are first building the system.

## Licensing Restrictions and Legal Risks

The secondary risk is licensing and usage restrictions. External datasets often come with restrictions on redistribution, commercial use, modification, or publication of results. You may be prohibited from sharing the data with contractors, using it for certain applications, or publishing research that reveals details about the dataset. Violating these terms can result in legal action, loss of access, or reputational damage.

You need to read and understand licensing agreements before committing to external data. You also need to ensure that downstream use of models trained on external data complies with those agreements. If your license prohibits commercial use, but you deploy a model in a commercial product, you have violated the terms. If your license prohibits redistribution, but you fine-tune an open-source model on the data and publish the fine-tuned weights, you may have violated the terms depending on how the agreement defines redistribution.

Some licenses allow research use but not commercial deployment. Some allow internal use but not customer-facing products. Some allow use for training but not for publication of benchmarks. The terms vary widely, and many are written in legal language that requires careful interpretation. When in doubt, consult your legal team before purchasing or using external data.

## Data Quality and Provenance Due Diligence

The tertiary risk is data quality and provenance. Not all vendors are equally rigorous about annotation quality, data sourcing, or consent. Some datasets are scraped from public websites without verifying copyright or privacy compliance. Some datasets are annotated by low-paid crowdworkers with minimal training and no quality control. Some datasets contain personally identifiable information, protected health information, or other sensitive data that was not properly anonymized.

Purchasing external data does not transfer legal liability away from you. If you use a dataset that violates GDPR, HIPAA, or copyright law, you are responsible for the violation, not the vendor. Due diligence is required. Ask vendors how the data was collected, whether consent was obtained, whether the data complies with GDPR and other regulations, and what indemnification they provide if legal issues arise.

Ask for documentation of annotation guidelines, inter-annotator agreement scores, and quality control processes. Ask for demographic information about annotators to assess whether their backgrounds match your target domain. Ask for provenance documentation showing where the data originated and what transformations were applied. Reputable vendors provide this information. Vendors who cannot or will not provide documentation should be treated with skepticism.

## Combining Sources: The Hybrid Strategy

The most effective dataset engineering strategies combine all three sources strategically. You use production logs as the foundation because they provide distribution match. You use synthetic generation to fill gaps, balance classes, and create adversarial examples. You use external procurement to bootstrap before production data is available or to add coverage for domains where you lack expertise. The hybrid approach gives you the strengths of each source while mitigating the weaknesses.

A typical hybrid pipeline starts with external procurement during initial development. You license a general-purpose dataset that covers the broad contours of your task. You use this data to build a baseline model and validate that the problem is solvable. Once you launch to production, you begin logging real user interactions.

You use production logs to fine-tune the model, identify distribution gaps, and measure performance on real traffic. When you discover edge cases or underrepresented scenarios in production logs, you use synthetic generation to create targeted examples that address those gaps. Over time, the proportion of production data increases, the proportion of external data decreases, and synthetic data is used surgically to address specific deficiencies.

## Managing Data Provenance in Hybrid Datasets

This hybrid approach requires infrastructure to manage data provenance. You need to track which examples came from which source, when they were collected, and what filtering or transformations were applied. You need to be able to analyze model performance separately on production data, synthetic data, and external data to detect distribution mismatches. You need to be able to update the mix of data sources over time as production traffic grows and priorities shift.

Data provenance is not optional metadata. It is essential for debugging, compliance, and continuous improvement. When a model fails on a specific class of inputs, you need to know whether your training data included examples from that class, which source those examples came from, and whether the examples were representative. Without provenance tracking, you are flying blind.

You implement provenance tracking by adding source metadata fields to every training example. Every record includes a source type field indicating production, synthetic, or external. It includes a source identifier indicating the specific vendor, generation job, or production logging pipeline. It includes a timestamp indicating when the data was collected or generated. It includes a schema version indicating what validation rules applied. All of this metadata travels with the data through your pipeline.

## Setting Weight Policies Across Data Sources

You also need to set policies for how much weight to give each source. A common mistake is to treat all data equally. If you have 10,000 external examples, 5,000 synthetic examples, and 500 production examples, and you sample uniformly during training, the model will be dominated by external and synthetic distributions. The 500 production examples will have minimal influence.

A better approach is to oversample production data, downsample external data, or use importance weighting to ensure the model prioritizes the distribution that matters most. The exact weighting depends on your task, your evaluation criteria, and how much production data you have, but the principle is constant. Production data should drive the model's behavior, not be drowned out by larger volumes of mismatched external or synthetic data.

One practical policy is to ensure production data represents at least 30% of each training batch, regardless of the overall dataset proportions. Another policy is to use production data for validation and early stopping, so the model optimizes for production performance even if external data dominates training volume. A third policy is to separate training into stages: pre-train on external data, fine-tune on a balanced mix, then fine-tune again on production-only data.

## Cost Structures: What You Pay and When

The cost structures of the three sources are fundamentally different. Production logs have upfront infrastructure costs and ongoing storage costs, but marginal cost per example is near zero once logging is in place. You pay to build the logging pipeline, the data warehouse, the sampling logic, and the consent mechanisms. After that, collecting an additional million examples costs only incremental storage, which is cheap. Production logs are expensive to start but scale efficiently.

Synthetic generation has low upfront costs and moderate marginal costs. You pay per API call or per token generated. Generating 10,000 examples might cost a few hundred dollars. Generating a million examples might cost tens of thousands. The cost scales linearly with volume, which makes synthetic generation economical for filling small gaps but expensive as a primary data source at scale. You also pay engineering time to write prompts, validate outputs, and filter low-quality generations. This is not free, but it is typically cheaper than human annotation.

External procurement has high upfront costs and often recurring license fees. Purchasing a dataset might cost tens of thousands to hundreds of thousands of dollars, depending on size, domain, and exclusivity. Some vendors charge annual licensing fees. Some charge per model trained or per deployment. Some charge based on revenue generated by the product using the data. These costs can add up quickly, especially if you need multiple datasets or if licensing terms restrict how you use the data.

External procurement is expensive upfront but may be the only option when you lack production traffic or domain expertise. For a medical AI startup with no existing user base, licensing clinical datasets from established healthcare institutions is the only way to train a credible model before launch. The $200,000 licensing fee is a necessary investment. Once production traffic begins, the startup can shift to production logs and reduce reliance on external data.

## Lifecycle Cost Accounting for Datasets

The total cost of a dataset is not just the cost to acquire the data. You must also account for labeling, cleaning, filtering, storage, versioning, and compliance infrastructure. Production logs may be cheap to collect but expensive to label. Synthetic data may be cheap to generate but expensive to validate. External data may come pre-labeled but require extensive filtering to match your distribution.

When comparing costs across sources, account for the full lifecycle, not just acquisition. A synthetic dataset that costs $500 to generate but requires $8,000 in human review and filtering is not actually cheaper than external data that costs $6,000 but arrives clean and validated. A production logging pipeline that costs $30,000 to build but collects a million examples over two years at near-zero marginal cost is cheaper in the long run than repeatedly purchasing external data.

You also need to account for opportunity cost. Time spent negotiating external data licenses, evaluating vendors, and integrating external datasets is time not spent building production features or improving models. Time spent debugging synthetic generation prompts and filtering low-quality outputs is time not spent on other priorities. Time spent building logging infrastructure pays dividends over years, but it is upfront investment that delays other work.

## Legal Implications: Ownership, Consent, and Compliance

The legal implications of data sourcing vary by source. Production logs are data you generate from your own users, which means you own the data but must comply with consent and privacy obligations. Under GDPR, users must consent to data collection and processing. Under the EU AI Act, high-risk AI systems must document data provenance and quality. You cannot simply log everything and assume you have the right to use it for training.

You need explicit consent for AI training purposes, not just general terms of service. You need to anonymize or pseudonymize data to protect privacy. You need to provide users with the right to access, correct, or delete their data. Production logs are your data, but they come with legal obligations. A financial services company that logged customer service conversations without obtaining explicit consent for AI training faced GDPR enforcement action and fines totaling 1.2 million euros.

Synthetic data is data you generate, which means you own it and face no consent obligations to real individuals. However, you may face copyright or licensing obligations related to the language model you used to generate the data. If you use a proprietary API like OpenAI or Anthropic, the terms of service may impose restrictions on how you use generated outputs. Some models prohibit using outputs to train competing models. Some models prohibit using outputs in certain regulated domains. Read the terms carefully.

Synthetic data is legally simpler than production data, but it is not entirely free from restrictions. A company that used OpenAI's API to generate training data for a competing language model violated OpenAI's terms of service and faced contract termination and potential legal action. The synthetic data itself was not the problem. The use of it to build a competing product violated the agreement.

## External Data Licensing and Liability Transfer

External data is owned by the vendor or licensor, and you are subject to the licensing agreement. You do not own external data. You have permission to use it under specific terms. Violating those terms is breach of contract and potentially copyright infringement. You also inherit any legal risks associated with how the vendor collected the data. If the vendor scraped data without consent, violated copyright, or failed to anonymize sensitive information, you may be liable when you use that data.

Due diligence is essential. Ask vendors how the data was collected, whether consent was obtained, whether the data complies with GDPR and other regulations, and what indemnification they provide if legal issues arise. Some vendors provide indemnification clauses that protect you if the data turns out to violate laws or third-party rights. Other vendors disclaim all liability, leaving you fully responsible.

A document processing company licensed a dataset of scanned contracts from a vendor, only to discover later that the contracts contained personally identifiable information that had not been redacted and that the original contract parties had not consented to their data being sold to third parties. The company faced potential GDPR violations and had to halt use of the dataset, wasting six months of development work.

## Compliance Across Combined Sources

When you combine data sources, you must comply with the most restrictive terms across all sources. If your external data license prohibits commercial use, you cannot deploy a model trained on a mix of external and production data in a commercial product. If your production data requires GDPR compliance, you must apply GDPR protections to the entire dataset, even if synthetic or external portions are not subject to GDPR.

Legal compliance is not modular. It applies to the entire pipeline. You cannot train a model on a mix of permissively licensed and restrictively licensed data and then claim the permissive portions allow commercial use. The restrictive license taints the entire model. You must track licensing terms for every data source and ensure your use complies with the intersection of all constraints.

This has practical implications for dataset architecture. You may need to maintain separate datasets for different licensing regimes. You may need to train separate models on data with compatible licenses. You may need to exclude certain data sources entirely because their terms are incompatible with your product plans. These decisions must be made early, ideally before acquiring external data or before launching production logging.

## The Over-Reliance Anti-Pattern: Single-Source Brittleness

The most common mistake in data sourcing is over-reliance on a single source. Teams that rely exclusively on external data build models that fail in production because the distribution does not match. Teams that rely exclusively on synthetic data build models that fail on real-world messiness because synthetic data is too clean. Teams that rely exclusively on production logs build models that cannot handle new scenarios because production data is biased toward existing user behavior and may not cover edge cases or future use cases.

Single-source brittleness is dangerous because it is invisible during development. If you train and evaluate on the same source, performance looks good. The problems only appear in production when the model encounters data from a different distribution. By that time, you have already shipped, users are experiencing failures, and fixing the problem requires collecting new data, retraining, and redeploying.

The cost of fixing single-source brittleness in production is 10 to 100 times higher than the cost of designing a hybrid strategy from the beginning. A SaaS company that launched a feature classification model trained exclusively on external data spent eight weeks debugging production failures, collecting production logs, retraining, and redeploying. If they had started with a hybrid strategy that included even a small amount of synthetic data designed to mimic expected production patterns, they would have caught the distribution mismatch before launch.

## Measuring Performance Separately by Source

The solution is to diversify data sources deliberately and measure performance on each source separately. Track precision, recall, and other metrics on production data, synthetic data, and external data independently. If performance is strong on external data but weak on production data, you have a distribution mismatch problem. If performance is strong on synthetic data but weak on production data, your synthetic data is not realistic enough. If performance is strong on production data but weak on adversarial synthetic data, you lack robustness.

Separate measurement by source reveals these problems early, when they are cheap to fix. You implement this by tagging evaluation examples with their source and computing metrics stratified by source. Your evaluation dashboard shows overall performance plus performance breakdowns for each source. When you see divergence, you investigate. You examine the examples where performance differs, identify what distributional characteristics explain the gap, and adjust your data sourcing strategy accordingly.

You also need to set explicit policies for minimum production data coverage. A reasonable policy is that at least 30% of your evaluation set must come from production logs, and the model must meet performance targets on that production subset. This ensures you are optimizing for the distribution that matters. Another reasonable policy is that production data must represent at least 50% of training data once production traffic is available. This ensures the model is not dominated by external or synthetic distributions.

The exact thresholds depend on your task and risk tolerance, but the principle is universal. Production data is the tiebreaker. When in doubt, trust production data over other sources. If production and external data disagree about what constitutes good performance, production wins. If synthetic evaluations look great but production evaluations look poor, production is telling you the truth.

## Strategic Sourcing: Aligning Data Sources to Product Stages

The best data sourcing strategy aligns with your product lifecycle. In the pre-launch phase, you have no production data, so you rely on external procurement and synthetic generation. You use external data to build a baseline model and validate feasibility. You use synthetic data to fill gaps and create adversarial examples. Your evaluation set should include both external data and synthetic data designed to mimic your expected production distribution as closely as possible, even though you have no real production examples yet.

In the launch phase, you begin collecting production logs but volumes are still low. You continue using external and synthetic data as the majority of training data, but you start using production logs for evaluation and fine-tuning. You measure performance on production data separately to detect distribution mismatches early. You use production data to identify gaps in your external and synthetic datasets and generate new synthetic data to fill those gaps. The balance shifts gradually from external-heavy to production-heavy.

In the growth phase, production data becomes the dominant source. You have enough volume and coverage to train primarily on production logs. You use synthetic data to fill specific gaps, handle edge cases, and create adversarial examples. You use external data only for domains where you still lack production coverage or need specialized expertise. Your evaluation set is primarily production data, with synthetic and external data used to test robustness and generalization. The model is now optimized for your actual production distribution, not a proxy.

## Mature Systems and Continuous Production Learning

In the mature phase, production data is the foundation, and external and synthetic data are used surgically. You continuously log production traffic, retrain regularly, and monitor for distribution drift. When new features launch, new markets open, or new use cases emerge, you may temporarily increase synthetic or external data to bootstrap coverage, but you quickly transition back to production data as the primary source.

This lifecycle is not rigid. Some products remain in the pre-launch phase for years if they are research prototypes. Some products skip directly to growth phase if they launch with high traffic. The principle is to match your data sourcing strategy to the availability and quality of production data, not to follow a fixed schedule. A high-traffic consumer application can transition to production-dominant training within weeks. A specialized B2B tool with low traffic may rely on external and synthetic data for years.

The key is intentionality. Know which phase you are in, know what data sources are appropriate for that phase, and have a plan for transitioning to the next phase. Do not get stuck in pre-launch patterns after you have production traffic. Do not prematurely abandon external data if your production logs still lack coverage. Align your data sourcing to your product reality, and evolve it as your product evolves.

The next subchapter covers the infrastructure, policies, and practices required to harvest production data effectively. Production logs are the most valuable data source, but only if you log the right things, sample intelligently, and handle consent properly.
