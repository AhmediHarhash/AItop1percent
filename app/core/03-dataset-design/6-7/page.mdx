# 6.7 — Train, Val, Test Split Strategies: Random vs Time-Based vs User-Based

In mid-2025, a financial services company launched a fraud detection system that achieved 94% precision on their test set. The model went into production in July, and within two weeks, precision dropped to 71%. The fraud team escalated to Engineering, who re-ran the evaluation pipeline and confirmed: the test set still showed 94%. The model had not changed. The evaluation had not changed. But production performance was catastrophically different.

The root cause was invisible in the metrics: they had used a random split. Their dataset contained six months of transaction data from January through June 2025, shuffled and split 70-15-15 for train, validation, and test. The test set contained transactions from January, February, March, April, May, and June, randomly sampled. The model learned patterns that spanned the entire six-month window. Fraudsters changed tactics in July. The test set, built from historical data, had no examples of the new patterns. The evaluation was accurate for the past, but useless for predicting the future.

This is the central failure mode of random splits: they test interpolation, not generalization. How you split your evaluation dataset determines whether your metrics reflect the performance your users will experience. Random splits work when the future resembles the past. Time-based splits work when the future evolves. User-based splits work when user identity matters. Choosing the wrong split strategy does not just inflate your metrics. It hides the failure modes your system will encounter in production, and you discover them only when users do.

## Random Splits: When the Future Looks Like the Past

Random splitting is the default. You shuffle your dataset, assign 70% to training, 15% to validation, 15% to test. Every example has an equal probability of appearing in any split. The test set is statistically representative of the full dataset. If the underlying distribution is stable, this works.

Random splits are appropriate when your task has no temporal structure and no user structure. A sentiment classification task built from product reviews spanning multiple years, where the reviews are independent and the language of sentiment does not drift, can use a random split. A toxicity detection task built from forum posts, where toxicity manifests in consistent linguistic patterns regardless of when the post was written or who wrote it, can use a random split. A translation task where the source and target languages have stable grammar and vocabulary, and the examples are drawn from diverse domains with no time-dependent trends, can use a random split.

Random splits break when the distribution shifts. If you are building a medical diagnosis assistant and your dataset contains clinical notes from 2023 through 2025, a random split mixes 2023 notes into your test set. If treatment protocols changed in 2024, or if new diagnostic criteria were introduced in 2025, your test set does not reflect current medical practice. Your model might perform well on the outdated 2023 cases in the test set, but fail on the 2025 cases your users actually submit. The evaluation is accurate for the mixture, but the mixture does not match production.

Random splits also break when examples are not independent. If your dataset contains multiple paraphrases of the same user query, and those paraphrases are randomly split across train and test, the model can memorize the query and achieve artificially high scores on the test paraphrases. If your dataset contains multiple outputs generated from the same input, and those outputs appear in both train and test, the model can overfit to the input structure. Random splits assume independence. When that assumption is violated, the split leaks information.

You can detect inappropriate random splits by checking for temporal trends in your error distribution. If your model performs significantly better on test examples from certain months or certain years, your random split is hiding temporal drift. If your model performs significantly worse on the most recent examples in your test set, your random split has given you an optimistic estimate of future performance. You should re-split using time, and measure the gap. That gap is the generalization penalty your users will pay.

## Time-Based Splits: Testing Generalization to the Future

Time-based splitting reserves the most recent data for testing. You sort your dataset by timestamp, assign the earliest 70% to training, the next 15% to validation, and the final 15% to test. The test set represents the future relative to the training set. If your model generalizes to the test set, it has demonstrated the ability to generalize forward in time.

Time-based splits are the correct choice for any task where the data distribution changes over time. Fraud detection, content moderation, question answering over current events, recommendation systems, search ranking, customer support triage, and financial forecasting all have temporal structure. The patterns in January differ from the patterns in June. The patterns in June differ from the patterns in December. A random split lets the model learn from all months and test on all months. A time-based split forces the model to learn from the past and test on the future.

A customer support automation system built by a SaaS company in late 2024 used a random split on two years of support tickets. The model achieved 89% accuracy at predicting the correct resolution category. They deployed in January 2025. Accuracy dropped to 76% within three weeks. The product had launched two new features in December 2024, and those features generated new categories of support requests that did not exist in the historical data. The random split had mixed December tickets into the training set, so the model had seen a few examples of the new categories. But the test set also contained December tickets, so the evaluation showed high accuracy. A time-based split, holding out December and January for testing, would have revealed the accuracy drop before deployment.

Time-based splits require you to define what "time" means. For some systems, time is the timestamp when the example was created. For others, time is the timestamp when the example was labeled. For still others, time is the timestamp when the input data was published or became available. If you are building a news summarization system, the relevant timestamp is when the article was published, not when you scraped it or labeled it. If you are building a clinical decision support tool, the relevant timestamp is when the patient visit occurred, not when the note was entered into your dataset. The wrong timestamp definition can leak future information into your training set.

You must also decide how much data to reserve for testing. A 15% test set on a six-month dataset reserves approximately three weeks of the most recent data. If your distribution shifts weekly, three weeks may be enough to detect the shift. If your distribution shifts monthly or quarterly, you need a longer test window. A healthcare technology company building a patient intake assistant used a time-based split on eight months of data, reserving the final two weeks for testing. Their test metrics were strong. They deployed, and within a month, performance degraded. The degradation was seasonal: patient volume and complaint patterns changed as flu season began in October. Two weeks of test data in August did not capture the October shift. They should have reserved at least one full seasonal cycle for testing, or tested on multiple non-contiguous time windows to detect seasonal variation.

Time-based splits also interact with model training dynamics. If your training set ends in May and your test set begins in June, you are testing the model's ability to generalize one month forward. If you retrain your model weekly, the time gap between training and test in production is one week, not one month. Your evaluation overstates the generalization challenge. Conversely, if you retrain monthly and your test set represents one week of data, you are understating the challenge. The time gap in your split should match the retraining cadence you plan to use in production.

## User-Based Splits: Preventing Identity Leakage

User-based splitting assigns all examples from a given user to the same split. If user A appears in the training set, all of user A's examples are in the training set. If user B appears in the test set, all of user B's examples are in the test set. No user appears in multiple splits. This prevents the model from memorizing user-specific patterns and overfitting to user identity.

User-based splits are necessary when user behavior is consistent within a user but diverse across users. A personalized email generation system, where each user has a distinct writing style, should use a user-based split. If the training set contains five emails from user A and the test set contains three more emails from user A, the model can memorize user A's style and achieve high accuracy on those three test emails without learning general principles of email writing. A user-based split tests whether the model can generalize to new users, which is the actual production requirement.

A legal technology startup built a contract clause extraction tool in early 2025. Their dataset contained 8,000 contracts from 140 law firms. They used a random split, achieved 92% extraction accuracy on the test set, and launched. Accuracy in production was 78%. The failure was user leakage. Some law firms contributed dozens of contracts, all written with similar templates and clause structures. The random split placed some contracts from each firm in train and some in test. The model learned firm-specific templates and performed well on test contracts from firms it had seen during training. But new firms, with new templates, were out-of-distribution. A user-based split, holding out entire firms for testing, would have revealed this.

User-based splits require you to define what a "user" is. In some systems, user identity is explicit: a logged-in account, an email address, a customer ID. In others, user identity is implicit: a device ID, an IP address, a session cookie. If your system serves anonymous users and you cannot link examples to stable identities, user-based splitting may not be feasible. But if you can define user identity, even approximately, you should split on it when user behavior is heterogeneous.

The challenge with user-based splits is that they can reduce your test set size if users contribute unequal numbers of examples. If 10% of your users contribute 60% of your examples, a user-based split that reserves 15% of users for testing may reserve only 8% of examples for testing. You lose statistical power. The alternative is to stratify by user activity level: reserve some high-activity users, some medium-activity users, and some low-activity users for testing, ensuring that your test set has both breadth across users and depth within high-activity users. This is more complex to implement, but it preserves test set size while preventing user leakage.

User-based splits also interact with cold-start performance. If your system is expected to perform well on new users from day one, a user-based split is the correct evaluation. If your system is allowed a warm-up period where it collects user data before making predictions, your evaluation should include both cold-start performance on held-out users and warm-start performance on users with partial training data. The split strategy must match the production scenario.

## Choosing the Right Split for Your Product

The correct split strategy is determined by the generalization requirement your product must satisfy. If your product must generalize to future time periods, use a time-based split. If your product must generalize to new users, use a user-based split. If your product must generalize to both future time and new users, use a time-and-user-based split, where the test set contains only recent examples from users not seen during training.

A recommendation system built by a media company in 2025 faced both temporal shift and user diversity. Content popularity changed weekly. User preferences varied widely. A random split would overstate performance. A time-only split would test temporal generalization but allow user leakage. A user-only split would test user generalization but allow temporal leakage. They used a combined split: the test set contained only users who joined in the final month of the dataset period, evaluated on content published in that same month. This tested the hardest case: new users, new content. Test metrics were lower than a random split would have shown, but they reflected production performance accurately.

You can also use multiple test sets with different split strategies. A fraud detection system might maintain three test sets: a random split from the full dataset, testing overall performance; a time-based split from the most recent month, testing temporal generalization; and a user-based split from users with no training examples, testing cold-start performance. Each test set answers a different question. The random split establishes a performance ceiling. The time-based split measures temporal degradation. The user-based split measures cold-start degradation. Together, they give you a complete picture of where your model will fail.

The size of each split also depends on your data volume and retraining cadence. If you have millions of examples and retrain daily, you can afford to reserve a large test set representing multiple weeks or months of recent data. If you have thousands of examples and retrain monthly, you must balance test set size against training set size. A test set that is too small has high variance in metrics. A test set that is too large starves the training set and degrades model quality. The typical 70-15-15 split is a reasonable default, but you should adjust based on how much data you have and how stable your metrics are.

If your dataset has strong seasonal or cyclical patterns, your test set should span a full cycle. A hiring automation tool built by an HR technology company used three months of training data and two weeks of test data. They launched in September. Performance collapsed in November. The training data was from May through July, summer months with lower hiring volume. The test data was from late July, still summer. November was the start of year-end hiring pushes, with different role types, different urgency levels, and different candidate pools. The model had never seen this distribution. A test set spanning a full year, or at minimum a test set from the same calendar period as the expected deployment, would have caught this.

## Validation Set Strategy and Hyperparameter Tuning

The validation set must follow the same split strategy as the test set. If you use a time-based test set, you must use a time-based validation set, with the validation period earlier than the test period but later than the training period. If you use a user-based test set, you must use a user-based validation set, with distinct users in validation and test. The validation set is where you tune hyperparameters, select prompt templates, and choose model versions. If the validation set uses a different split strategy than the test set, you will tune for one distribution and test on another.

A common mistake is to use a random validation set with a time-based test set. You tune your model on the random validation set, achieving strong performance because the validation set mixes past and future data. You then evaluate on the time-based test set, where performance is lower because the test set only contains future data. You conclude that you have overfit to the validation set, but the real problem is that the validation set was not representative of the test set. The split strategies must align.

Another mistake is reusing the test set for hyperparameter tuning. You run an experiment, check the test set metrics, adjust hyperparameters, check again, adjust again. After multiple iterations, your hyperparameters are overfit to the test set. The test set is no longer a held-out evaluation; it has become part of the training loop. You must reserve the test set for final evaluation only. All tuning happens on the validation set. If you need to run many experiments and you are worried about overfitting the validation set, you can split the validation set further into multiple folds and use cross-validation, but you never touch the test set until you have locked in your final model.

## Detecting Split Strategy Failures in Production

If you chose the wrong split strategy, you will see a performance gap between your test metrics and your production metrics. The gap has a pattern. If you used a random split and performance degrades steadily over time after launch, you have temporal drift that the random split did not detect. If you used a time-based split and performance is strong for returning users but weak for new users, you have user leakage that the time-based split did not prevent. If you used a user-based split and performance degrades after a product update or a seasonal shift, you have temporal drift that the user-based split did not capture.

You can diagnose the failure by re-splitting your evaluation dataset and re-running metrics. Take your existing test set and split it further by time, by user, or by both. If metrics vary significantly across sub-splits, your overall test set is hiding heterogeneity. If metrics are much lower on the most recent sub-split, you have temporal drift. If metrics are much lower on the sub-split of new users, you have user leakage. The sub-split with the lowest metrics is your best estimate of future production performance.

A content moderation system built by a social media platform in late 2024 used a random split and reported 91% precision on their test set. They segmented the test set by month and found that precision was 94% on examples from January through June, but 84% on examples from October through December. They had temporal drift. They re-split using time, holding out December for testing, and found precision dropped to 81%. That was the real number. They adjusted their launch timeline, collected more recent training data, and retrained. Post-launch precision was 83%, closely matching the time-based test estimate.

## When to Use Hybrid and Stratified Splits

Some products require testing multiple generalization dimensions simultaneously. A healthcare diagnostic assistant must generalize to new patients, new time periods, and new clinical sites. A user-based, time-based, site-based split would reserve recent patients from held-out sites for testing. This is a three-way split. It is complex to implement, and it dramatically reduces the size of the test set, but it is the only way to test the hardest generalization case your system will face.

You can implement hybrid splits by defining a composite key. For a user-time split, the key is user ID and time bucket. You assign users to splits first, then within each user split, you assign time buckets to splits. For a user-time-site split, the key is user ID, time bucket, and site ID. You assign sites to splits first, then users within held-out sites, then time buckets within held-out users. The result is a test set where every example is out-of-distribution on all three dimensions.

This is expensive. A three-way split with 15% reserved for each dimension leaves you with approximately 0.15 cubed, or 0.3%, of your data in the test set if the dimensions are independent. If your dataset has 10,000 examples, your test set has 30 examples. That is too small for stable metrics. You must either collect more data, relax one of the split dimensions, or accept high variance in your test metrics. There is no free lunch. Testing the hardest case requires data.

Stratified splits are an alternative when you want to preserve distributional balance across splits. If your dataset has three user activity levels—low, medium, high—and you want all three levels represented in train, validation, and test, you split within each level. You randomly assign 70% of low-activity users to train, 15% to validation, 15% to test, and do the same for medium and high. The result is that each split has the same proportion of low, medium, and high activity users as the overall dataset. This prevents the test set from being unrepresentatively skewed toward one activity level.

Stratified splits are useful when you have known subgroups with different performance characteristics and you want to test performance on all subgroups. A loan approval assistant might stratify by applicant credit score range, ensuring that the test set includes applicants from all credit tiers. A resume screening tool might stratify by years of experience, ensuring that the test set includes both junior and senior candidates. Stratification ensures that your test metrics are not dominated by the largest subgroup.

## The Split Strategy You Choose Defines the Generalization You Measure

Your evaluation dataset does not measure model quality in the abstract. It measures model quality on the distribution defined by your test set. If your test set is a random sample of historical data, your metrics reflect performance on historical data. If your test set is recent data from new users, your metrics reflect performance on recent data from new users. The split strategy is a statement of what you are testing.

This means you must choose the split strategy before you build the dataset, not after. The split strategy determines what data you need to collect. If you need a time-based split with a three-month test window, you must collect data spanning at least three months beyond your training data. If you need a user-based split with 15% of users held out, you must collect data from enough users that 15% gives you a statistically stable test set. The split strategy is a requirements document for data collection.

It also means you must document the split strategy and share it with stakeholders. Product managers need to know whether your 89% accuracy metric is from a random split, a time-based split, or a user-based split, because those three numbers mean different things. A random split accuracy of 89% might translate to a time-based accuracy of 81% and a user-based accuracy of 76%. If the product roadmap assumes 89%, and production delivers 76%, you have a planning failure. The split strategy must be visible in every metrics report, every launch review, and every model card.

The evaluation dataset is not just a benchmark. It is a contract between you and your stakeholders about what performance your system will deliver in production. The split strategy defines the terms of that contract. Random splits promise performance on data that looks like the past. Time-based splits promise performance on data that comes in the future. User-based splits promise performance on users you have never seen. Choose the promise you can keep, and build the split that tests it. The next step is to prevent the most common form of leakage in user-based splits: when the same user appears in both train and test because you did not link their identities correctly.
