# 5.9 â€” Dataset Registries: Cataloging What You Have

In early 2025, a financial services company with over 400 data science practitioners discovered that eleven different teams had independently built near-identical customer transaction datasets over the previous eighteen months, each spending between three and eight weeks on feature engineering, validation, and compliance review. The datasets differed only in minor feature definitions and date ranges. None of the teams knew the others existed. The cumulative wasted effort totaled 63 engineer-weeks, equivalent to over 1.2 million dollars in fully-loaded labor cost. The discovery happened accidentally when a new team lead conducted an informal survey asking what datasets people were using. When the results were compiled, the duplication was immediately obvious. The company had no dataset registry, no searchable catalog, and no process for publishing dataset metadata. Each team worked in isolation, assumed their requirements were unique, and rebuilt from scratch.

The root cause was not communication failure or organizational dysfunction. It was infrastructure failure. Without a centralized registry, discovering existing datasets requires tribal knowledge, Slack searches, or luck. Teams default to building new datasets because finding existing ones is harder than starting over. The cost of duplication is invisible until someone aggregates it, and by then the waste is sunk. Dataset registries exist to make discovery cheaper than creation. They catalog what datasets exist, who owns them, what schema they expose, when they were last updated, and how to access them. Organizations that treat datasets as first-class artifacts catalog them like code repositories, API endpoints, or database tables. Organizations that treat datasets as ephemeral byproducts lose millions to redundant work. Understanding how to design, populate, and maintain a dataset registry is not optional for any organization running production AI at scale.

## What a Dataset Registry Is and Why It Matters

A dataset registry is a metadata catalog that tracks datasets as named, versioned, documented artifacts. It stores information about each dataset's schema, ownership, lineage, access patterns, quality metrics, and physical location. It provides search and discovery interfaces so teams can find datasets by keyword, tag, schema attribute, or owner. It integrates with access control systems to enforce permissions and with orchestration systems to track usage.

The registry is not a data warehouse or a feature store. It does not store the data itself. It stores pointers to data and metadata about data. The data remains in object storage, data lakes, or databases. The registry is an index, not a repository.

The value proposition is search and reuse. When a team needs a dataset of customer transactions, they search the registry for transaction-related datasets, review the schemas and documentation, assess quality metrics, and request access. If a suitable dataset exists, they use it. If not, they build a new one and register it so the next team can find it. Over time, the registry accumulates the organization's collective dataset inventory, reducing duplication and increasing leverage.

The failure mode is the unregistered dataset. Every dataset created outside the registry is invisible to search and unavailable for reuse. If registration is optional, most datasets never get registered because the team that builds a dataset has already solved their problem and has no immediate incentive to document it for others. Registration must be mandatory and automated, integrated into the dataset creation workflow so that publishing to the registry is the default path, not an optional extra step.

## Metadata Schemas: What Information the Registry Must Capture

The registry schema defines what metadata is tracked for each dataset. Insufficient metadata makes search ineffective. Excessive metadata makes registration burdensome and discourages adoption. The optimal schema captures the information necessary for discovery, access, and quality assessment without requiring essay-length documentation.

At minimum, the registry must capture name, description, owner, schema, location, and created timestamp. Name is a unique identifier, typically namespaced by team or domain. Description is a human-readable summary of what the dataset contains and what it is used for, one to three sentences. Owner is the team or individual responsible for maintaining the dataset, with contact information. Schema is the column names, types, and descriptions. Location is the storage path or table identifier. Created timestamp is the date the dataset was first registered.

Extended metadata includes last updated timestamp, update frequency, access control policy, lineage upstream sources, lineage downstream consumers, quality metrics, row count, and tags. Last updated timestamp indicates freshness. Update frequency indicates whether the dataset is static, updated daily, or streaming. Access control policy specifies who can read, write, or delete the dataset. Lineage tracks what datasets or pipelines feed into this dataset and what datasets or models consume it. Quality metrics might include null rates, duplicate rates, or schema compliance scores. Row count provides a quick sense of scale. Tags are free-form labels like customer-data, transactions, or pii-detected.

The schema should be extensible. As teams discover additional metadata that aids discovery or quality assessment, they should be able to add it without restructuring the entire registry. JSON or semi-structured formats work well for this. Start with required fields, add optional fields as needed, and allow teams to define custom metadata within a reserved namespace.

## Search and Discovery: Making Datasets Findable

The registry's primary user interface is search. Teams search by keyword, filter by schema attributes, sort by recency or popularity, and browse by tag or owner. The quality of the search experience determines whether the registry gets used or ignored.

Keyword search must index dataset names, descriptions, column names, and column descriptions. If a team searches for revenue, the registry should return datasets with revenue in the name, datasets described as containing revenue data, and datasets with a revenue column. Full-text search with ranking is essential. Simple prefix matching is insufficient.

Faceted filtering allows users to narrow results by owner, schema attributes, tags, or update frequency. If a user searches for transactions and gets 50 results, they filter to datasets owned by the payments team, updated daily, and tagged as production-ready. The result set narrows to three datasets, all of which are relevant.

Schema-based search enables queries like show me all datasets with a user-id column and a timestamp column. This is particularly useful for finding datasets that can be joined or for discovering datasets that conform to a particular contract. Implementing schema search requires indexing the schema metadata and supporting queries over structured fields, not just free text.

Popularity metrics improve relevance. If ten teams use a particular transaction dataset and only one team uses another, the heavily-used dataset should rank higher in search results. Popularity can be measured by query count, number of downstream consumers, or explicit endorsements. Registries that surface popular datasets reduce the risk that users pick an abandoned or low-quality dataset over a well-maintained one.

## Ownership Tracking: Who Maintains What and How to Reach Them

Every dataset must have an owner. Ownership is not optional. A dataset without an owner is a dataset that no one maintains, no one documents, and no one can ask questions about. When schema changes, when quality degrades, or when access issues arise, the owner is the point of contact.

Ownership should be assigned to a team, not an individual, to survive turnover. Individual ownership works early in a project but fails as the organization scales and people change roles. Team ownership ensures continuity. The registry stores the team name and a contact method, typically a Slack channel, email alias, or ticketing queue.

The registry should enforce that every dataset has an owner at registration time. If the owner field is blank, registration fails. This forces the registering team to decide who is responsible before the dataset is published. Deferred ownership assignment never happens.

Ownership also implies accountability. If a dataset is registered but not maintained, its quality metrics degrade, and downstream consumers file issues, the owner is responsible for remediation or deprecation. If the dataset is no longer needed, the owner deprecates it and removes it from active search results. If the dataset is critical but under-resourced, the owner escalates to leadership.

Registries that do not enforce ownership devolve into graveyards of orphaned datasets. Search results fill with stale, unmaintained datasets that waste users' time. Ownership is the mechanism that keeps the registry useful.

## Lineage: Tracking Upstream Sources and Downstream Consumers

Lineage metadata tracks where data comes from and where it goes. Upstream lineage records the source datasets, pipelines, or external systems that feed into a dataset. Downstream lineage records the datasets, models, dashboards, or APIs that consume the dataset. Together, upstream and downstream lineage form a dependency graph that enables impact analysis, root cause diagnosis, and change management.

When a dataset's schema changes, downstream lineage tells you which models, pipelines, and teams are affected. You notify them before deploying the change, reducing the risk of breaking production systems. When a dataset's quality degrades, upstream lineage tells you which source datasets or pipelines are responsible. You trace the issue to its root cause rather than treating symptoms.

Lineage can be tracked manually or automatically. Manual lineage requires dataset owners to declare dependencies when registering a dataset. This is simple but error-prone. Teams forget to update lineage when dependencies change, and lineage becomes stale. Automatic lineage extraction parses pipeline code, query logs, or orchestration metadata to infer dependencies. This is accurate but requires integration with the execution environment.

The best approach is hybrid. Automatic extraction provides baseline lineage, and manual annotation adds context that cannot be inferred, such as business logic dependencies or informal data flows. For example, if a feature engineering pipeline reads from a dataset and writes to another, automatic extraction captures that. If the pipeline also relies on an external API that is not tracked as a dataset, manual annotation documents it.

Lineage should be versioned. As datasets evolve and pipelines change, the dependency graph changes. Tracking historical lineage allows you to understand what dependencies existed when a specific model was trained or when a specific incident occurred. This is critical for reproducibility and post-incident analysis.

## Integration with Access Control and Governance

The registry is the natural integration point for access control and governance policies. When a dataset is registered, the owner specifies who can access it. The registry enforces permissions by integrating with identity providers and authorization systems. Users cannot query a dataset unless the registry confirms they have read access.

This centralization simplifies governance. Instead of managing permissions per table, per bucket, or per pipeline, you manage permissions per registered dataset. The registry becomes the single source of truth for who has access to what. Auditors query the registry to produce access reports. Compliance teams define policies in the registry, and the registry enforces them across all storage backends.

Sensitive datasets require additional metadata. If a dataset contains personally identifiable information, the registry tags it as PII-detected. If a dataset is subject to GDPR or HIPAA, the registry tags it accordingly. These tags enable automated policy enforcement, such as requiring encryption at rest, restricting access to specific roles, or mandating data retention limits.

The registry can also track access logs. Each time a user queries a dataset, the registry logs the event with user identity, timestamp, and query details. This provides an audit trail for compliance and enables usage analytics that drive registry improvements. If a dataset is rarely queried, it may be a candidate for deprecation. If a dataset is queried frequently by many teams, it may warrant additional documentation or quality investment.

## Preventing Duplicate Work Through Proactive Search Workflows

The registry prevents duplication only if teams search it before building new datasets. If search is manual and optional, teams skip it. The solution is to make search a required step in the dataset creation workflow.

When a team begins a new project, the first task is to search the registry for existing datasets that might meet their needs. The search is documented in the project plan or ticket. If suitable datasets exist, the team uses them. If no suitable datasets exist, the team documents why existing datasets are insufficient and proceeds to build a new one. The new dataset is registered upon creation.

This workflow is enforceable through tooling. The dataset creation script or CLI prompts the user to search the registry and confirm that no suitable datasets exist before proceeding. The confirmation is logged. Teams that skip search see friction. Teams that perform search see streamlined access to existing datasets. Over time, search becomes habitual.

Proactive search is particularly effective when combined with schema-based search and recommendations. If a team is building a dataset with columns user-id, timestamp, and event-type, the registry searches for existing datasets with similar schemas and surfaces them as recommendations. This reduces the burden on the user to formulate the perfect search query. The registry assists discovery.

## Deprecation and Lifecycle Management

Datasets do not live forever. Source schemas change, business requirements evolve, and better datasets replace old ones. The registry must support deprecation and lifecycle management to prevent clutter and guide users toward current, high-quality datasets.

Deprecation is a multi-stage process. First, the owner marks the dataset as deprecated in the registry, providing a reason and pointing to a replacement dataset if one exists. The registry continues to serve the dataset but displays a warning to users. Downstream consumers are notified and given a migration timeline. After the migration period, the dataset is archived. Archived datasets are removed from active search results but remain queryable for reproducibility. After a retention period, archived datasets are deleted.

This staged approach balances stability and hygiene. Immediate deletion breaks downstream systems. Permanent retention clutters search results and increases storage costs. Deprecation with migration periods and archival strikes the balance.

The registry should track dataset lifecycle stage as metadata: active, deprecated, archived, or deleted. Search defaults to active datasets. Users can opt in to search deprecated or archived datasets if they need historical data or are debugging legacy systems. This keeps the primary search experience focused on current, maintained datasets.

## Building and Maintaining the Registry: Tooling and Process

The registry can be built on top of existing metadata catalog systems like Apache Atlas, Amundsen, DataHub, or cloud-native solutions like AWS Glue Data Catalog or Unity Catalog. These systems provide schema storage, search indexing, lineage tracking, and access control integration out of the box. The implementation effort is configuration and integration, not building from scratch.

Alternatively, teams build lightweight registries on top of relational databases or document stores, exposing a REST API and a web UI for search. This approach works for smaller organizations or teams with specific requirements not met by existing platforms. The trade-off is maintenance burden. Off-the-shelf catalog systems are maintained by vendors or open-source communities. Custom registries are maintained by your team.

Whichever approach you choose, the registry must integrate with the dataset creation and orchestration tools your teams already use. If teams create datasets with Spark, the Spark write path should trigger registry updates. If teams orchestrate pipelines with Airflow, Airflow task completion should update lineage. Integration reduces registration friction and increases compliance.

The registry also requires operational discipline. Ownership must be enforced. Metadata must be validated for completeness. Stale datasets must be deprecated. These are process issues, not technical issues. Assign a team or individual as registry steward, responsible for auditing compliance, curating high-quality datasets, and evangelizing best practices. The registry is infrastructure, and infrastructure requires maintenance.

The next step is formalizing the expectations between dataset producers and consumers through data contracts, which define schema guarantees, allowed values, and latency SLAs, covered in 5.10.
