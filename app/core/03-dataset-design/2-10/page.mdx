# 2.10 â€” Domain Expert Elicitation: Structured Knowledge Capture

In late 2024, a medical AI startup building a clinical decision support system hired eight board-certified physicians to review and annotate patient cases for six months. Each physician was paid $300 per hour, working 10 hours per week, for a total investment of $960,000. The goal was to build a training dataset of 10,000 annotated cases covering common diagnostic scenarios. After six months, the team had collected 3,200 cases, less than one-third of the target.

Worse, when they analyzed inter-annotator agreement, they found that the eight physicians disagreed on diagnosis or treatment recommendations in 34% of cases. The disagreements were not random noise. They reflected genuine clinical judgment differences, different specialty backgrounds, and different interpretations of ambiguous evidence. The startup had no protocol for resolving disagreements, no calibration process to align physician perspectives, and no quality assurance beyond basic completeness checks.

They spent nearly a million dollars on expert time but produced a dataset too inconsistent to train a reliable model. They spent another four months and $400,000 hiring a medical director to design structured elicitation protocols, re-annotate disputed cases, and build consensus guidelines.

This failure illustrates the central challenge of domain expert elicitation. For specialized domains including medical, legal, financial, scientific, and technical fields, the most valuable training and evaluation data comes from experts who possess deep domain knowledge. But experts are expensive, scarce, and hard to scale. A single expert hour costs $200 to $500. Experts have limited availability because they are busy practicing their profession.

And experts disagree with each other, not because they are wrong, but because expertise involves judgment, interpretation, and context-specific reasoning that does not reduce to simple rules. You cannot simply hire experts, point them at data, and expect consistent, high-quality annotations. You must design structured elicitation protocols that capture expert knowledge efficiently, calibrate experts to align their perspectives, resolve disagreements systematically, and build sustainable pipelines that scale beyond the initial cohort.

This subchapter teaches you how to extract maximum value from every expert hour while respecting the complexity and variability inherent in expert judgment.

## Defining What You Need From Experts

Before you recruit a single expert, you must define precisely what knowledge you need and why only experts can provide it. Expert time is your most expensive dataset resource. Wasting it on tasks that could be done by non-experts or automated systems is professional negligence.

You need experts for tasks that require specialized knowledge, professional judgment, domain-specific pattern recognition, or interpretation of ambiguous evidence. You do not need experts for tasks that involve applying clear rules, finding information in reference materials, or making binary classifications based on surface features.

A common mistake is asking experts to annotate data that could be annotated by trained non-experts following detailed guidelines. For example, a legal AI system might ask attorneys to label whether a contract contains a specific clause type. If the clause definition is clear and the identification task is mechanical, a paralegal or even a college graduate with a two-hour training session can do this work at one-tenth the cost.

You only need attorneys when the task requires interpreting clause meaning, assessing enforceability, or making judgment calls about ambiguous language. Misallocating expert time to non-expert tasks is the fastest way to blow your budget.

You must also distinguish between knowledge elicitation and data labeling. Knowledge elicitation means extracting the conceptual frameworks, decision criteria, and reasoning patterns that experts use. Data labeling means applying those frameworks to specific examples. Both are valuable, but they require different processes.

Knowledge elicitation happens through structured interviews, think-aloud protocols, and iterative refinement sessions. You ask experts to explain how they approach a task, what factors they consider, what edge cases they watch for, and how they resolve ambiguity. Data labeling happens through annotation interfaces, batch review workflows, and quality control checks. You ask experts to apply their judgment to examples following established protocols.

Most expert elicitation projects need both phases. You start with knowledge elicitation to understand expert reasoning, then use that understanding to design annotation protocols, and finally engage experts in data labeling. Skipping the knowledge elicitation phase and jumping straight to labeling produces the inconsistency the medical startup experienced.

Without shared frameworks and calibration, each expert applies their own personal heuristics, and the resulting dataset reflects individual variability rather than domain consensus.

## Recruiting and Qualifying Experts

Not all credentials are created equal. A board certification, law degree, or PhD demonstrates that someone has completed formal training, but it does not guarantee they have the specific expertise your project needs. A cardiologist is an expert in cardiovascular medicine but not in dermatology. A corporate attorney is an expert in M&A transactions but not in criminal defense.

A machine learning researcher is an expert in neural network architectures but not in compiler optimization. You must recruit experts whose specialization aligns with your data domain.

Qualification goes beyond checking credentials. You need to assess practical expertise through work samples, reference checks, and trial tasks. For a medical AI project, you want physicians who currently practice in the relevant specialty, not researchers who left clinical practice a decade ago.

For a legal AI project, you want attorneys who have handled the specific case types your system will encounter, not recent law school graduates. For a financial AI project, you want analysts who work with the instruments and markets your system covers, not generalists with broad but shallow knowledge.

Trial tasks are your best qualification tool. Before committing to a long-term contract, you give each candidate expert a small set of examples to annotate or review. You then compare their responses to ground truth if available, or to responses from other candidate experts.

You look for accuracy, consistency, attention to detail, and the ability to articulate reasoning. Experts who provide thoughtful explanations for their judgments are more valuable than experts who produce labels without explanation, because their reasoning helps you understand the task and design better protocols.

You also assess availability and reliability. An expert who commits to 10 hours per week but consistently delivers only 6 hours is worse than an expert who commits to 5 hours and delivers 5 hours reliably. An expert who takes three weeks to respond to questions or return annotated data creates bottlenecks that delay your entire project.

During the recruitment process, you set clear expectations about time commitment, response times, and communication norms, and you select experts who can meet those expectations.

## The Cost Structure of Expert Time

Expert compensation requires careful planning because hourly rates vary dramatically by specialty and geography. Medical specialists command $250 to $500 per hour. Attorneys range from $200 to $600 per hour depending on experience and practice area. Financial analysts specializing in derivatives or structured products may charge $300 to $400 per hour.

These rates reflect opportunity cost, not arbitrary pricing. When a cardiologist spends an hour annotating cases for your dataset, they forgo an hour of clinical practice that might generate $500 in professional fees.

You reduce effective cost per annotation by maximizing the value extracted from each expert hour. This means preparing batches efficiently so experts spend time on judgment rather than navigating interfaces. It means designing annotation tasks that capture not just labels but reasoning, so each annotation produces multiple training signals.

It means batching related cases together so experts can leverage context across examples rather than cold-starting on every case.

You also reduce cost by tiering tasks appropriately. Junior experts or specialists-in-training can handle straightforward cases at lower rates, reserving senior experts for genuinely complex or ambiguous cases. A second-year medical resident can label clear-cut pneumonia cases. A board-certified pulmonologist should review the edge cases where imaging is ambiguous or where multiple diagnoses are plausible.

Budget planning for expert elicitation requires multiplying hourly rates by realistic productivity estimates. A physician might annotate 8 to 12 medical cases per hour if each case requires reviewing patient history and imaging. An attorney might review 4 to 6 contracts per hour if each requires analyzing multiple clauses.

These rates determine your total budget. For 10,000 annotated cases at 10 cases per hour and $300 per hour, you need $300,000 in expert fees. Add 20% for project management, calibration sessions, and dispute resolution, and your total expert cost reaches $360,000.

## Designing Elicitation Protocols

Unstructured expert interviews produce valuable insights but not scalable datasets. If you ask three experts to "review these cases and tell me what you think," you will get three different formats, three different levels of detail, and three incompatible annotation schemes. Structured elicitation protocols give experts a consistent framework that channels their expertise into usable data.

A well-designed protocol starts with a clear task definition. You provide experts with a written description of what they are being asked to do, what information they should provide, and what format their responses should take. For example, in a medical diagnosis task, the protocol might specify: "For each patient case, provide a ranked list of up to five possible diagnoses in order of likelihood. For each diagnosis, assign a confidence level from 1 to 5.

If the case information is insufficient to make a diagnosis, indicate what additional information you would need." This structure ensures every expert provides comparable data.

The protocol must also define how to handle ambiguity and uncertainty. Real-world cases are often ambiguous. Two diagnoses may be equally plausible. A legal contract may be open to multiple interpretations. A financial projection may depend on unknowable future events.

Your protocol needs to give experts a way to express uncertainty rather than forcing them to make artificial binary choices. Options include confidence scores, probability distributions, multiple plausible answers, or explicit "insufficient information" labels. These representations preserve the nuance of expert judgment instead of collapsing it into false certainty.

You must provide experts with the right amount of context. Too little context and experts cannot make informed judgments. Too much context and you bias their responses or overwhelm them with irrelevant information. In medical case review, experts need patient symptoms, medical history, and test results, but they do not need the patient's name, address, or insurance information.

In legal contract review, experts need the full contract text and information about the transaction type, but they do not need the parties' financial statements unless those are directly relevant to the clause being analyzed. You determine the right context level through pilot testing with a small number of experts.

Your protocol should include calibration materials. Before experts begin annotation, you give them a set of 10 to 20 examples that have been pre-annotated by your team or by a senior expert who helped design the protocol. Experts review these examples, compare their own judgments to the reference annotations, and discuss any differences with the project team.

This calibration process aligns expert understanding of the task, reveals ambiguities in the protocol, and sets quality expectations. Skipping calibration is the primary cause of inter-annotator disagreement.

## Managing Expert Panels and Achieving Consensus

A single expert provides one perspective. An expert panel provides coverage of different specializations, viewpoints, and approaches. Panels are essential for tasks where reasonable experts can disagree, where the domain has multiple schools of thought, or where you need to capture the range of acceptable professional judgment.

But panels introduce new challenges: how many experts do you need, how do you select them, and how do you handle disagreements?

Panel size depends on task complexity and required confidence. For straightforward tasks with high inter-expert agreement, three to five experts provide sufficient coverage and allow majority voting to resolve minor disagreements. For complex tasks with inherent judgment variability, you may need eight to twelve experts to capture the range of professional opinion.

Beyond twelve, you hit diminishing returns because you are adding experts with similar perspectives to those already in your panel. The medical startup's eight-physician panel was a reasonable size, but they failed to design the panel for diversity of perspective.

Panel composition should be deliberate. You want diversity across relevant dimensions: specialty, experience level, practice setting, and geographic location. For a medical AI covering primary care, you might recruit family medicine physicians, internists, and pediatricians from academic medical centers, community hospitals, and private practices across urban and rural settings.

This diversity ensures your dataset captures different clinical approaches and does not over-represent the perspective of one institution or practice style. Panels composed entirely of experts from the same organization or training program will have artificially high agreement because they share institutional norms and practices.

You must distinguish between disagreements that represent genuine uncertainty and disagreements that represent mistakes. When expert annotations diverge, you first check for obvious errors: did an expert misread the case information, misunderstand the task, or make a simple mistake? If so, you ask them to review and revise.

If the disagreement persists, you determine whether it reflects different but defensible judgments or whether one expert is clearly wrong. For defensible disagreements, you have several options: keep all expert labels and train your model to predict the distribution of expert opinion, escalate to a senior expert or panel vote to establish ground truth, or exclude ambiguous cases from training data and use them only for evaluation.

Consensus-building mechanisms include structured discussion and Delphi methods. In structured discussion, experts who disagree on a case are brought together to present their reasoning and attempt to reach agreement. This works well for small numbers of high-stakes cases where the investment in discussion time is justified.

Delphi methods involve multiple rounds of independent annotation with summary feedback between rounds. After the first round, you show experts the distribution of responses without revealing individual judgments, and ask them to reconsider their annotations. Often experts will converge toward consensus once they see how their peers approached the task.

If disagreement persists after two or three rounds, you accept that the case is genuinely ambiguous and treat it accordingly.

## Capturing Reasoning and Rationale

Expert labels are valuable, but expert reasoning is transformative. When an expert labels a medical case as "likely pneumonia," that label is a single data point. When the expert explains "the presence of fever, productive cough, and focal infiltrate on chest X-ray are classic signs of pneumonia, though the lack of elevated white blood cell count is atypical and warrants follow-up," you now have structured reasoning that explains not just the conclusion but the evidence weighting and caveats.

This reasoning can be used to train models that generate explanations, to validate model outputs, and to identify patterns in expert decision-making.

Capturing reasoning requires changing your annotation interface from simple classification to structured explanation. After an expert provides a label, you prompt them to explain their reasoning. The prompt must be specific: "What evidence supports this diagnosis?" "What alternative interpretations did you consider and why did you rule them out?"

"What additional information would increase your confidence?" Open-ended "explain your reasoning" prompts produce inconsistent responses because different experts interpret the question differently.

You can structure reasoning capture through templates. For medical diagnosis, a reasoning template might include: key positive findings, key negative findings, alternative diagnoses considered, differentiating factors, and confidence level. For legal contract analysis, a template might include: relevant contract provisions, applicable legal principles, ambiguities or risks, and recommended actions.

These templates ensure reasoning is captured consistently across experts and cases.

Voice annotation is faster than typing for capturing detailed reasoning. Experts record verbal explanations while reviewing cases, and you transcribe the recordings into structured text. This works especially well for complex tasks where typing would interrupt expert workflow. A radiologist can narrate their interpretation while examining a medical image more naturally than they can type structured notes.

You must balance the efficiency gain against transcription costs and the risk of losing structure in free-form narration.

Reasoning data has quality issues distinct from label quality issues. An expert may provide an accurate label but a poor explanation, or they may provide a detailed explanation that does not actually support the label they gave. You need separate quality control for reasoning, checking that explanations are specific rather than generic, that they reference the actual case evidence, and that they logically support the provided label.

Generic explanations like "based on my clinical experience" or "this is standard practice" provide no usable information. Specific explanations like "the elevated troponin level combined with ST-segment changes indicates acute myocardial infarction" are training gold.

## Building Annotation Interfaces for Expert Workflows

Experts work differently than crowdworkers. They multitask across professional responsibilities, they work on varied devices, and they expect tools that respect their time and expertise. Annotation interfaces designed for crowdworkers frustrate experts and reduce engagement.

Expert interfaces must minimize friction. Loading times over two seconds cause experts to abandon sessions. Navigation that requires more than two clicks to complete common actions wastes expert time. Forms that do not save partial progress lose work when experts are interrupted by clinical duties or client calls.

You design interfaces that feel like professional tools, not mechanical task platforms.

Mobile and tablet support is non-negotiable for many expert workflows. Physicians annotate cases between patient appointments on tablets. Attorneys review contracts on laptops during travel. Financial analysts work from multiple devices throughout the day. Your interface must function identically across devices and screen sizes, preserving annotation state and supporting the same interactions whether on a phone, tablet, or desktop.

Keyboard shortcuts and power-user features improve expert productivity. Experts who annotate hundreds of cases develop muscle memory for common actions. Providing shortcuts for frequent tasks, allowing batch operations, and supporting keyboard-only navigation enables experts to work faster.

A radiologist reviewing chest X-rays should be able to cycle through images, mark findings, and submit annotations without touching a mouse.

Context preservation across sessions matters for expert retention. When an expert returns to the interface after hours or days, they should land exactly where they left off, with the same cases queued and the same context visible. Forcing experts to re-navigate or re-load context every session wastes time and creates frustration.

Session state should persist automatically without requiring manual saves.

## Scaling Expert Pipelines

Expert elicitation does not scale through parallelization the way crowdsourced labeling does. You cannot throw 100 medical experts at a dataset and expect linear speedup. Experts need calibration, oversight, and quality control, which limits how many you can manage simultaneously.

A realistic expert pipeline has tiers: a small core team of highly engaged senior experts who design protocols and resolve disputes, a larger pool of practicing experts who perform the bulk of annotation, and automated or semi-automated quality control to catch errors without requiring expert review of every annotation.

Your core team typically consists of two to four senior experts who commit significant time to the project. They participate in protocol design, review challenging cases, provide feedback to the annotation pool, and serve as final arbiters for disputed annotations. They are paid more than pool annotators, often as project consultants with equity or advisory roles rather than hourly contractors.

Their deep engagement ensures quality and consistency as the project scales.

The annotation pool consists of ten to thirty experts who work part-time on annotation tasks. They are trained on the protocol through calibration sessions, annotate batches of examples, and receive regular feedback on quality metrics. You rotate the pool over time as experts' availability changes, maintaining continuity through strong onboarding processes and documentation.

Pool experts are typically paid hourly at market rates for their specialty.

Quality control for expert annotations cannot rely solely on inter-annotator agreement, because experts often legitimately disagree. Instead, you track expert-specific quality metrics: accuracy on cases with known ground truth, consistency over time on similar cases, and completion rate of required annotation fields.

Experts whose quality metrics decline receive targeted feedback and retraining. Experts whose quality remains low despite feedback are removed from the pool. You also implement spot checks where core team members review a random sample of each expert's annotations monthly to catch systematic errors that metrics might miss.

Automation accelerates expert pipelines by handling tractable cases and routing complex cases to experts. If your model can handle 70% of cases with high confidence, you only need experts to review the remaining 30%, reducing expert time by two-thirds. This requires confidence calibration so you can trust the model's self-assessment of when it needs expert review.

You validate this by comparing model confidence to expert agreement: cases where the model is highly confident should have high expert agreement, while cases where the model is uncertain should show more expert disagreement. If this correlation holds, you can automate routing.

## Sustaining Expert Engagement Over Time

Experts who start with enthusiasm often burn out after months of repetitive annotation. Unlike crowdworkers who expect routine tasks, professionals expect intellectual engagement and impact. Sustaining expert participation requires making the work intellectually rewarding, demonstrating impact, and respecting expert time.

Intellectual engagement comes from variety and challenge. Rotate experts across different case types rather than having each expert annotate the same case type repeatedly. Prioritize challenging cases that require real judgment over straightforward cases that feel mechanical. Involve experts in protocol refinement so they are shaping the process, not just following it.

Create opportunities for experts to learn from each other through case discussion sessions or shared reasoning reviews.

Demonstrating impact means showing experts how their annotations improve the product. Share metrics on model performance improvements driven by expert data. Provide access to the product so experts can see their knowledge operationalized. When the model successfully handles a case type that experts spent weeks annotating, tell them.

When user feedback indicates the model is providing value in their domain, share that feedback. Experts who see their work making a difference stay engaged. Experts who feel like anonymous data laborers quit.

Respecting expert time means removing friction from the annotation process. Provide intuitive annotation interfaces that work on the devices experts actually use. If physicians annotate cases between patient appointments on tablets, your interface must work on tablets, not just desktop browsers.

Minimize coordination overhead through asynchronous workflows, clear expectations, and responsive support. When an expert has a question about an ambiguous case, answer within hours, not days. When payment or administrative issues arise, resolve them immediately. Experts who feel their time is valued stay engaged. Experts who fight with clunky tools or slow bureaucracy leave.

Recognition and professional development create long-term engagement. Acknowledge expert contributions in public communications, research publications, or product documentation where appropriate and where it aligns with professional ethics. Offer experts opportunities to present their work at conferences or co-author papers based on insights from the annotation project.

Provide learning opportunities by sharing aggregate findings, interesting edge cases, or model behavior patterns that emerge from expert data. These benefits transform the annotation engagement from transactional labor into professional development.

## Ethical and Professional Obligations

Domain experts occupy a position of trust in their professional communities. When they contribute to an AI system, they risk that trust if the system produces harmful outputs or is used inappropriately. You have ethical obligations to experts that go beyond the contractual relationship.

Transparency about use cases is fundamental. Experts deserve to know what the system they are building will be used for, who will have access to it, and under what constraints. A physician who helps build a clinical decision support system for emergency departments has different ethical considerations than one building a system for insurance claims processing.

If the use case changes, you must inform experts and give them the opportunity to withdraw if the new use violates their professional ethics.

You must also protect expert reputation. Annotations and reasoning provided by experts should not be attributed to them publicly without consent. If your system generates explanations based on expert reasoning, those explanations should not claim "Dr. Smith says..." without Dr. Smith's permission.

This is both an ethical requirement and often a professional licensing requirement. Medical boards, bar associations, and professional standards bodies have rules about public statements and endorsements that experts must follow.

Experts must not be pressured to compromise professional judgment. If your business model depends on achieving certain annotation outcomes, and you communicate this to experts explicitly or implicitly, you create pressure to bias annotations in your favor. This is unethical and undermines the value of expert input.

You must create a firewall between business interests and annotation processes, making clear to experts that their job is to provide honest professional judgment regardless of business implications.

Finally, you must consider the systemic impact of the system experts help you build. If experts contribute to a medical AI that later denies necessary care, a legal AI that produces biased outcomes, or a financial AI that enables fraud, those experts bear some moral responsibility even if they had no control over deployment decisions.

You have an obligation to use expert contributions responsibly and to engage experts in governance decisions about appropriate use, deployment safeguards, and impact monitoring. Treating experts as disposable data sources rather than partners in responsible AI development is both ethically wrong and strategically shortsighted.

The next subchapter addresses a different frontier in data collection: synthetic data generation. Where expert elicitation extracts knowledge from human specialists, synthetic data uses models to generate training examples at scale, raising new questions about quality, diversity, and model collapse.
