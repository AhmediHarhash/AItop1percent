# 6.3 â€” Sizing Your Eval Set: Statistical Power and Confidence Intervals

In early 2025, a financial services company was evaluating two models for a transaction categorization system. Model A was their current GPT-4-based solution running at $0.03 per 1,000 transactions. Model B was a newer Claude-based alternative at $0.018 per 1,000 transactions. The team ran both models on their 120-example evaluation set. Model A achieved 86.7% accuracy, 104 correct out of 120. Model B achieved 90.0% accuracy, 108 correct out of 120. The delta was 3.3 percentage points in favor of Model B. Based on this result, they switched to Model B to capture both the performance improvement and the 40% cost reduction. Three weeks after launch, production metrics showed Model B performing at 84.2% accuracy compared to Model A's historical 87.1%. The system had regressed by nearly three percentage points. The root cause was not that the models performed differently in production than in evaluation. The root cause was that a 3.3-point difference on 120 examples was well within random noise. The 95% confidence interval for that difference was roughly plus or minus six percentage points. The team had made a high-stakes decision based on a measurement that could not distinguish signal from noise.

This failure reveals a fundamental truth about evaluation: the size of your eval set determines the precision of your measurements, and measurements without adequate precision produce decisions based on noise. Most teams choose eval set sizes based on intuition, convenience, or what feels like enough. A hundred examples seems substantial. Five hundred seems comprehensive. But these intuitions have no connection to statistical reality. The right eval set size depends on the magnitude of differences you need to detect, the confidence level you require, and the baseline performance of your system. Understanding these relationships is not optional statistical sophistication. It is basic engineering discipline that separates principled measurement from guessing.

## Statistical Power and Detectability

Statistical power is the probability that your eval set will detect a real performance difference when one exists. If you are comparing two systems and one is truly better than the other, power is the chance your eval will correctly identify which one is better. High power means you reliably detect real differences. Low power means real differences are often missed or misattributed to noise.

Power depends on three factors: the size of the difference you are trying to detect, the size of your eval set, and the variance in your measurements. The relationship is straightforward. Larger differences are easier to detect and require smaller eval sets. Smaller differences are harder to detect and require larger eval sets. More examples give you more power to detect smaller differences. Higher variance in your task makes differences harder to detect and requires more examples.

Consider a binary classification task where your baseline system achieves 80% accuracy. You make a change and want to know whether accuracy has improved. If the true improvement is ten percentage points, bringing accuracy to 90%, you can detect that improvement with very few examples. Even with 50 examples, the difference between 40 correct and 45 correct is likely to be statistically significant. If the true improvement is one percentage point, bringing accuracy to 81%, you need hundreds of examples to distinguish that from random variation. With 50 examples, you are comparing 40 correct to 40.5 expected correct, and random noise will dominate.

The standard framework for power analysis assumes you are testing a null hypothesis that two systems perform equally well against an alternative hypothesis that they differ by some amount. You specify the significance level, typically 5%, meaning you accept a 5% chance of falsely concluding there is a difference when there is not. You specify the desired power, typically 80% or 90%, meaning you want an 80% or 90% chance of detecting the difference if it exists. Given these parameters and an estimate of baseline performance, you can calculate the required sample size.

For comparing two proportions such as accuracy rates, the formula involves the baseline proportion, the effect size, the significance level, and the desired power. The details are beyond the scope here, but the intuition is simple. If you want 80% power to detect a three-percentage-point difference at 5% significance with a baseline accuracy of 85%, you need approximately 800 examples per system, or 1,600 total if you are evaluating both on the same set. If you want to detect a five-point difference, you need approximately 300 examples per system. If you want to detect a one-point difference, you need several thousand examples per system.

These numbers make clear why small eval sets are dangerous. A 100-example eval set gives you virtually no power to detect differences smaller than about eight percentage points. A 200-example set can detect differences around five to six points with reasonable power. A 500-example set can reliably detect three to four point differences. A 1,000-example set can detect two to three point differences. If you are trying to choose between two prompts that differ by two percentage points, and you are using a 150-example eval set, you are flipping a coin.

This also explains why many teams see large metric swings during development that disappear in production. With a small eval set, random variation can easily produce five or ten point swings. You think you have made a breakthrough when you have just gotten lucky with sampling. Conversely, you might reject a genuinely good change because you got unlucky. Without adequate sample size, you cannot distinguish signal from noise, and your entire optimization process becomes random walk.

## Confidence Intervals for Eval Metrics

A confidence interval quantifies the uncertainty in your measurement. If your eval set shows 87% accuracy, the confidence interval tells you the range in which the true accuracy likely falls. A narrow confidence interval means your measurement is precise. A wide confidence interval means your measurement is noisy.

For a proportion such as accuracy, the confidence interval depends on the observed proportion, the sample size, and the desired confidence level. The most common approach is the normal approximation to the binomial distribution. For an observed accuracy of p based on n examples, the standard error is the square root of p times one minus p divided by n. The 95% confidence interval is approximately p plus or minus 1.96 times the standard error.

Consider an eval set of 200 examples where your system achieves 85% accuracy, meaning 170 correct. The standard error is the square root of 0.85 times 0.15 divided by 200, which is approximately 0.025 or 2.5 percentage points. The 95% confidence interval is 85% plus or minus 4.9 percentage points, or roughly 80% to 90%. This means if you repeated the evaluation many times with different random samples, 95% of the confidence intervals you computed would contain the true accuracy.

Now consider the same 85% accuracy on an eval set of 1,000 examples. The standard error drops to approximately 1.1 percentage points, and the 95% confidence interval is 85% plus or minus 2.2 percentage points, or roughly 83% to 87%. The larger sample size gives you much more precision.

Confidence intervals make clear whether observed differences are meaningful. If System A has 85% accuracy on 200 examples with a confidence interval of 80-90%, and System B has 88% accuracy on the same 200 examples with a confidence interval of 83-93%, the intervals overlap substantially. You cannot confidently say System B is better. The three-point observed difference could easily be noise. If you increase to 1,000 examples and System A is at 85% with interval 83-87% while System B is at 88% with interval 86-90%, the intervals barely overlap, and you have much stronger evidence that System B is truly better.

This also applies to tracking performance over time. If your baseline eval metric is 84% with a confidence interval of 81-87%, and after a code change it is 86% with an interval of 83-89%, you cannot confidently say you improved performance. The change might be real, or it might be noise. If after several changes you are at 91% with an interval of 89-93%, you can confidently say you have improved because the new interval does not overlap the baseline interval.

Many teams report point estimates without confidence intervals and make decisions based on differences that are well within measurement noise. You see 85.3% versus 87.1% and ship the second option because it is 1.8 points higher. But if your eval set is small, that 1.8-point difference has a confidence interval that spans negative five to positive eight points. You have no idea which system is better. Reporting confidence intervals alongside point estimates is not statistical pedantry. It is honest communication about measurement uncertainty.

## Eval Set Size by Task Type and Performance Level

The required eval set size is not universal. It depends on your task type, your baseline performance, and the magnitude of improvements you care about. Different tasks have different variance characteristics, and different performance levels have different statistical properties.

For binary classification tasks at moderate performance levels around 70-85% accuracy, detecting a three-percentage-point improvement with 80% power typically requires 500-800 examples. Detecting a five-point improvement requires 200-350 examples. Detecting a one-point improvement requires several thousand examples. These numbers assume balanced classes. If classes are imbalanced, you need more examples to achieve the same power because most of your examples provide information about the majority class.

For multi-class classification tasks, the required sample size depends on the number of classes and the distribution of examples across classes. If you have ten classes and want to measure per-class performance, you need enough examples of each class to make those measurements reliable. If you want 80% power to detect a five-point improvement within a class, and you have roughly balanced classes, you need 200-300 examples per class, or 2,000-3,000 total examples. This is why multi-class eval sets tend to be larger than binary eval sets.

For extraction tasks where you measure field-level precision and recall, the variance is higher because you have multiple fields per example and each field can be correct or incorrect independently. You typically need larger eval sets, often 800-1500 examples, to achieve the same statistical power as a classification task. For summarization or generation tasks where you measure aggregate quality scores, the variance depends on your scoring rubric. Human-scored tasks with subjective criteria tend to have higher variance and require more examples.

Baseline performance also affects required sample size. Tasks with very high baseline performance, say 95% accuracy or higher, have lower variance and require fewer examples to detect absolute differences. But they also have less room for improvement, so relative differences become more important. A three-point improvement from 70% to 73% is a 4.3% relative gain. A three-point improvement from 95% to 98% is a 3.2% relative gain but represents eliminating 60% of remaining errors, which is often more significant. For high-performance tasks, you often care about smaller absolute differences, which drives sample size back up.

For tasks with very low baseline performance, say below 50%, the variance is also relatively low because performance is near chance. But such tasks are often poorly defined or too difficult, and you need to revisit the task framing before worrying about eval set size.

As a rough heuristic, for classification tasks at typical performance levels of 70-90%, aim for 500-1000 examples to detect three to five point differences reliably. For multi-class or extraction tasks, aim for 800-1500 examples. For generation tasks with human evaluation, aim for 400-800 examples if each example is scored by multiple raters. For high-stakes decisions like model selection or launch readiness, increase these numbers by 50-100% to get tighter confidence intervals.

## Minimum Viable Eval Sizes and When to Expand

The minimum viable eval set size is the smallest set that gives you enough signal to make the decisions you need to make. This is not a single number. It depends on what you are trying to decide. Different decisions require different levels of precision.

For rapid iteration during prompt engineering, a 100-200 example eval set is often sufficient. You are not trying to detect one-point improvements. You are trying to catch obvious breaks and get directional signal on whether a change is promising. A five-point swing on 150 examples is likely real signal. You can use that to guide iteration. This small eval set should be fast to run, ideally under ten seconds, so you can iterate quickly.

For validating a release candidate or deciding whether to ship a change, you need more precision. A 500-800 example eval set is appropriate. You want confidence that observed improvements are real and that you have not introduced regressions. You want confidence intervals tight enough that a three-point improvement is distinguishable from noise. This eval set might take 30 seconds to a few minutes to run, which is acceptable for lower-frequency decisions.

For comparing models, especially when switching involves cost or vendor changes, you need even more precision. A 1,000-2,000 example eval set is appropriate. You want to detect two to three point differences reliably because those differences might translate to significant cost or performance impacts at scale. This eval set might take several minutes to run, which is fine for infrequent high-stakes decisions.

For launch readiness or compliance validation, you may need 1,500-3,000 examples to achieve the confidence levels required by stakeholders or regulators. This is especially true in regulated domains like healthcare or finance where you need to demonstrate with high statistical confidence that your system meets performance thresholds.

You should also expand your eval set when you detect anomalies or unexpected results. If a small eval set shows a surprising result, validate it on a larger set before making decisions. If your 200-example set shows a seven-point improvement from a minor prompt change, run a 800-example set to confirm. If the improvement holds, ship it. If it disappears, it was noise.

Another reason to expand is when you need to measure performance on rare slices. If a particular intent or edge case appears in 3% of production but is high-stakes, you need enough examples of that slice to measure performance reliably. With a 500-example eval set, you would have only 15 examples of that slice, which is insufficient. You might expand to 1,500 examples to get 45 examples of the rare slice, or you might over-sample the rare slice to ensure adequate coverage.

You should also expand when your confidence intervals are too wide for your decision. If you measure 84% accuracy with a confidence interval of 79-89%, and your product requirements specify 85% minimum accuracy, you cannot confidently claim you meet requirements because the lower bound of your interval is below the threshold. You need a larger eval set to narrow the interval.

Conversely, you can sometimes use smaller eval sets if you only care about large differences. If you are comparing a prototype to a baseline and you only want to ship the prototype if it is at least ten points better, a 200-example set may be sufficient to detect such a large difference. If the prototype shows only a five-point improvement, you do not need to expand the eval set to determine whether the difference is real. It is not large enough to matter even if real.

## When You Need More Data Versus Better Data

Expanding your eval set is not always the solution to measurement uncertainty. Sometimes the problem is not sample size but data quality, and adding more low-quality examples does not help. Knowing when to add more data versus when to improve data quality is critical.

You need more data when your confidence intervals are too wide for your decision, when you are trying to detect small differences, or when you need to measure performance on many slices. You need more data when statistical power calculations indicate your current sample size is insufficient. These are symptoms of a sample size problem, and the solution is more examples.

You need better data when your eval set is unrepresentative, when your labels are noisy, or when your eval metrics do not correlate with production outcomes. If your eval set shows 90% accuracy but production shows 78%, the problem is not sample size. The problem is that your eval set does not match production. Adding more unrepresentative examples will not fix this. You need to revisit coverage, sampling strategy, and annotation quality.

You need better data when annotator agreement is low. If two expert annotators disagree on 30% of examples, adding more examples annotated by the same process will just add more noisy labels. You need to improve annotation guidelines, provide more training, or reconcile disagreements before expanding.

You need better data when your eval set is contaminated. If you have been iterating on eval examples during development, adding more contaminated examples does not help. You need to set aside a clean eval set and stop looking at individual examples during tuning.

You need better data when your difficulty calibration is off. If your eval set is all easy examples and you need to measure performance on hard cases, adding more easy examples does not help. You need to source or generate hard examples and include them in your eval set.

In practice, many teams face both problems simultaneously. Their eval set is both too small and of mediocre quality. In this situation, prioritize quality first. Build a smaller, high-quality eval set with clean labels, good coverage, and proper difficulty calibration. Validate that this set correlates with production outcomes. Then expand it systematically to increase statistical power. A 300-example high-quality eval set is far more valuable than a 1,000-example low-quality set.

## Statistical Significance Testing in Practice

When comparing two systems, formal statistical significance testing tells you whether the observed difference is likely to be real or likely to be noise. The most common test for comparing proportions is the two-proportion z-test. You compute the difference in proportions, the standard error of the difference, and the z-score. If the z-score exceeds a threshold corresponding to your chosen significance level, typically 1.96 for 5% significance, you conclude the difference is statistically significant.

For example, suppose System A achieves 420 correct out of 500 examples, or 84.0% accuracy. System B achieves 445 correct out of 500 examples, or 89.0% accuracy. The difference is five percentage points. The pooled standard error is approximately 2.0 percentage points. The z-score is five divided by two, or 2.5. This exceeds 1.96, so the difference is statistically significant at the 5% level. You can confidently say System B performs better.

Now suppose System A achieves 168 correct out of 200, or 84.0%, and System B achieves 178 correct out of 200, or 89.0%. The difference is still five percentage points, but the standard error is now approximately 3.2 percentage points because the sample size is smaller. The z-score is five divided by 3.2, or about 1.56. This does not exceed 1.96, so the difference is not statistically significant. You cannot confidently say System B is better despite the five-point observed difference.

Significance testing prevents you from over-interpreting noise, but it also introduces a common misconception. Statistical significance does not mean practical significance. A one-percentage-point difference can be statistically significant if your sample size is large enough, but it might not matter for your product. Conversely, a five-point difference might fail to reach statistical significance if your sample size is small, but it might still be worth pursuing.

The right approach is to consider both statistical significance and practical significance. Define in advance what magnitude of improvement you care about. If a change must deliver at least three percentage points to justify the engineering cost, test whether the observed difference exceeds three points and whether it is statistically significant. If the observed difference is six points and significant, ship it. If the observed difference is two points even if significant, do not ship. If the observed difference is four points but not significant, expand your eval set to get more precision before deciding.

You should also be cautious about multiple comparisons. If you test ten different prompts against a baseline and use a 5% significance level for each test, you have a roughly 40% chance of finding at least one false positive. This is the multiple comparisons problem. The more tests you run, the more likely you are to see apparently significant results by chance. The standard correction is to adjust your significance threshold based on the number of tests. If you are running ten tests, you might use a significance threshold of 0.5% per test instead of 5%. This reduces false positives but also reduces power, requiring larger sample sizes.

In practice, most teams do not perform formal significance testing during rapid iteration. You run experiments, look at eval metrics, and make judgment calls based on whether differences seem large and consistent. This is fine for fast iteration. But for high-stakes decisions like model selection, launch readiness, or A/B test interpretation, formal significance testing is appropriate. It forces you to be explicit about uncertainty and prevents overconfident decisions based on noise.

## Practical Workflow for Eval Set Sizing

When starting a new project, use the following workflow to determine eval set size. First, define the minimum performance difference you care about. This might be driven by product requirements, cost considerations, or user experience thresholds. If you are comparing models and switching costs $50,000 per year, you might decide you need at least a three-point accuracy improvement to justify switching. If you are iterating on prompts and iteration is cheap, you might care about two-point improvements.

Second, define your desired confidence level. For routine development decisions, 80% power and 5% significance is standard. For high-stakes decisions, you might want 90% power or tighter significance thresholds. For launch readiness, you might need 95% confidence that you meet performance thresholds.

Third, estimate your baseline performance. If you have historical data or production metrics, use those. If you are building something new, run a quick baseline on a small sample to get an estimate. Baseline performance feeds into the power calculation.

Fourth, use a sample size calculator or formula to compute the required eval set size given your desired power, significance level, baseline performance, and effect size. Many online calculators exist for two-proportion tests. Alternatively, you can use statistical software or libraries.

Fifth, add a buffer. Sample size calculations assume certain statistical properties that real eval sets may not perfectly satisfy. Add 10-20% to the calculated size to account for annotation errors, edge cases, and other sources of noise.

Sixth, validate feasibility. If the calculated size is 3,000 examples and annotation costs $5 per example, you are looking at $15,000 in annotation cost. Decide whether that is justified by the decision being made. If not, revisit your requirements. Can you accept lower power? Can you tolerate wider confidence intervals? Can you care about larger effect sizes?

Finally, build and validate. Construct your eval set using the principles from 6.1 and 6.2. Run baseline systems and check that confidence intervals match your expectations. If you computed you need 800 examples for a certain confidence interval width, and you build an 800-example set and the intervals are much wider than expected, investigate. You may have higher variance than anticipated and need more examples, or you may have annotation quality issues that need to be addressed.

This workflow ensures your eval set is appropriately sized for the decisions you need to make. It prevents both under-powered eval sets that waste time measuring noise and over-powered eval sets that waste resources measuring with unnecessary precision.

In 6.4, we will examine how to source and annotate evaluation data to ensure your eval set is not only properly sized but also representative, uncontaminated, and reliably labeled.
