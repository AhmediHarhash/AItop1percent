# 2.9 — Synthetic Augmentation of Golden Sets

When to use synthetic data to expand golden set coverage is one of the most misunderstood decisions in regression testing. Synthetic data is not inherently inferior to human-curated cases. It is not a shortcut. It is not a compromise. It is a tool that serves specific purposes well and other purposes poorly. The error most teams make is not using too much synthetic data — it is using synthetic data for the wrong reasons or without understanding what synthetic augmentation can and cannot accomplish.

**Synthetic augmentation** means generating additional test cases programmatically or with AI assistance to expand coverage beyond what you can curate manually. Done correctly, it increases coverage of edge cases, stress tests model robustness, and fills gaps in distributions that are rare in production but critical to test. Done incorrectly, it dilutes signal with noise, creates test cases that do not represent real user needs, and gives false confidence that your golden set is comprehensive when it is merely large.

The principle is this: synthetic augmentation extends human judgment, it does not replace it. Your core golden set — the cases that represent your most critical use cases, your most dangerous failure modes, and your most important correctness criteria — must be human-curated. Synthetic cases expand from that core to cover variations, stress conditions, and adversarial inputs that would be infeasible to collect or curate manually.

## When Synthetic Augmentation Adds Value

Synthetic augmentation solves three problems that manual curation cannot solve efficiently. The first is scale of variation. Real production data clusters around common patterns. Most users ask similar questions, follow similar workflows, and encounter similar edge cases. But long-tail variations matter. A customer support system might see thousands of password reset requests daily, but only one request every few weeks that combines a password reset with account recovery, two-factor authentication bypass, and a disputed charge. That rare combination is critical to test, but waiting to collect it organically from production could take months. Synthetic augmentation generates it on demand.

The second problem is adversarial coverage. Attackers do not behave like typical users. They craft inputs designed to break your system — prompt injections, data exfiltration attempts, social engineering attacks. Collecting real adversarial examples from production means waiting for attacks to succeed. Synthetic generation lets you create adversarial test cases preemptively. You define the attack pattern — jailbreak prompts, instruction override attempts, sensitive data leakage triggers — and generate test cases that probe for those vulnerabilities before attackers find them.

The third problem is distribution balance. Production data is imbalanced. Common cases dominate. Rare cases disappear. If you sample randomly from production to build your golden set, you will oversample the common cases and undersample the rare ones. Synthetic augmentation rebalances the distribution. You generate synthetic examples of underrepresented but important cases — low-frequency languages, uncommon medical conditions, niche product categories — to ensure your golden set covers the full range of cases your model must handle, not just the cases it sees most often.

Synthetic augmentation does not replace human curation in any of these scenarios. You still need human-curated examples of password resets, adversarial inputs, and rare cases. The synthetic cases extend from those examples, generating variations that share the same structure, risk profile, or distributional characteristics but differ in specifics. The human-curated cases anchor the golden set in reality. The synthetic cases expand coverage beyond what reality provides frequently enough to curate manually.

## Generation Techniques for Synthetic Golden Cases

The simplest synthetic generation technique is rule-based variation. You take a human-curated test case and apply transformations that preserve its essential characteristics while changing surface details. A customer support case about canceling a subscription becomes a case about pausing a subscription, downgrading a subscription, or transferring a subscription to another user. The underlying intent — account management — remains constant. The specific action varies. This technique works well for generating coverage of workflow variations where the correctness criteria are similar but the inputs differ.

Rule-based variation requires carefully designed transformation rules. Do not apply arbitrary changes. Define which aspects of a test case can vary without changing the case's purpose. For a sentiment classification case, you might vary word choice and sentence structure while preserving sentiment polarity. For a content moderation case, you might vary the context surrounding a policy violation while keeping the violation itself constant. The transformed cases must remain valid test cases. If your transformations produce nonsensical inputs or ambiguous correctness criteria, you are generating noise, not coverage.

The second technique is model-assisted generation. You use a language model to generate synthetic test cases based on prompts that specify the structure, domain, and risk profile you need. This works well for generating adversarial cases, long-tail examples, and distribution-balanced variants. You provide a few human-curated seed cases as examples, describe the characteristics of the cases you want to generate, and let the model produce candidates. The critical step is validation. Do not add model-generated cases to your golden set without human review. Model-generated cases often contain subtle errors — incorrect factual claims, ambiguous phrasing, edge cases that miss the edge. A human reviewer must validate each generated case, confirm its correctness criteria, and approve it for inclusion.

Model-assisted generation is most valuable when generating adversarial cases. A prompt injection attack has infinite variations. Manually writing all of them is infeasible. But you can prompt a model to generate variations based on known attack patterns — instruction override, role confusion, context manipulation, encoding tricks. The model produces dozens of candidate adversarial cases. You review them, keep the ones that represent genuine threats, and discard the ones that are trivial or unrealistic. This process scales adversarial coverage far beyond what manual curation allows, while maintaining the quality control that human review provides.

The third technique is data augmentation through backtranslation or paraphrasing. You take a human-curated case, translate it to another language and back, or paraphrase it using automated tools. The result is a semantically similar case with different surface form. This technique is particularly useful for testing robustness — ensuring your model handles rephrased inputs, informal phrasing, or multilingual variations without degrading. Backtranslation introduces natural variation in word choice and sentence structure that mirrors how different users express the same intent. The generated cases are synthetic, but they reflect real linguistic diversity.

Paraphrasing requires caution. Not all paraphrases preserve meaning. Automated paraphrasing tools sometimes introduce factual errors, change sentiment, or alter intent. Every paraphrased case must be validated to ensure it still tests what you intended to test. But when done correctly, paraphrasing is one of the lowest-cost ways to expand coverage of linguistic variation without manually writing hundreds of rephrasings.

## Quality Validation for Synthetic Additions

Synthetic test cases are not automatically valid. They require validation before inclusion in your golden set. The validation process depends on the generation method, but the principle is consistent: a synthetic case must meet the same quality standard as a human-curated case. If you would not manually write the case because it is ambiguous, unrealistic, or poorly defined, do not include it because a generator produced it.

For rule-based variations, validation focuses on transformation correctness. Did the transformation preserve the case's purpose? Does the transformed case still test the intended correctness dimension? If you transformed a refund request into a chargeback dispute, and the correctness criteria for refunds do not apply to chargebacks, the transformation failed. Discard the case. If the transformation preserved intent and correctness criteria, validate the expected output. If the transformation changed the input in a way that should change the output, update the expected output accordingly. A rule-based variation is only valid if both input and output remain consistent with the case's purpose.

For model-assisted generations, validation is more intensive. A human reviewer must read every generated case, verify that it represents a realistic scenario, confirm that the expected output is correct, and ensure that the case adds meaningful coverage rather than duplicating existing cases. Model-assisted generation often produces plausible-sounding but subtly flawed cases. A generated customer support case might reference a product feature that does not exist. A generated adversarial case might use an attack pattern that is theoretically possible but practically irrelevant. These cases waste evaluation budget and dilute signal. Reject them during validation.

Track your rejection rate during validation. If you generate 100 synthetic cases and reject 60 during validation, your generation process is inefficient. Improve your prompts, tighten your transformation rules, or provide better seed examples. If you reject fewer than 10 percent, you might be under-validating. Synthetic cases should fail validation at a moderate rate — not so high that generation is wasteful, but high enough that validation is actually catching flawed cases.

For augmentation through paraphrasing or backtranslation, validation focuses on semantic preservation. Does the paraphrased case mean the same thing as the original? If you paraphrased "cancel my subscription immediately" to "I would like to discontinue my service at your earliest convenience," did you preserve urgency, intent, and expected response? If the paraphrase introduced ambiguity or changed meaning, discard it. If it preserved meaning but uses different phrasing, validate that the expected output remains appropriate. Sometimes paraphrasing legitimately changes the expected output. "Cancel now" and "cancel eventually" are semantically different despite appearing similar. The expected outputs must diverge accordingly.

Label all synthetic cases in your golden set metadata. Mark each case with a field indicating it was generated synthetically and specifying the generation method. This metadata serves two purposes. First, it enables analysis of how synthetic cases perform compared to human-curated cases. If synthetic cases consistently have higher error rates or lower discrimination power, you know your generation process needs improvement. Second, it enables selective exclusion. If you want to report performance on human-curated cases only, you can filter out synthetic cases. If you want to analyze performance on adversarial cases specifically, you can filter for synthetically generated adversarial cases. Metadata turns your golden set into a queryable dataset, not just a list of test cases.

## When Not to Use Synthetic Augmentation

Synthetic augmentation is not appropriate for every coverage gap. Some gaps must be filled with real, human-curated examples. Using synthetic data where real data is required creates the illusion of coverage without the substance.

Do not use synthetic augmentation for your core correctness criteria. The cases that define what "good" looks like for your system must come from real user needs, real stakeholder priorities, and real production patterns. Synthetic cases do not capture stakeholder intent. They reflect the generator's assumptions about what matters. If your golden set's core is synthetic, you are testing against a generated version of correctness, not the actual correctness criteria your users care about.

Do not use synthetic augmentation to avoid doing the hard work of curating production cases. If you have production data that represents the cases you need to test, use it. Synthetic generation is for filling gaps that production data does not provide, not for substituting synthetic cases because manual curation is time-consuming. A golden set built entirely from synthetic data is not a golden set. It is a benchmark disconnected from reality.

Do not use synthetic augmentation when domain expertise is required. Generating synthetic medical cases requires medical knowledge. Generating synthetic legal cases requires legal expertise. Generating synthetic compliance cases requires compliance domain knowledge. A language model cannot reliably generate valid domain-specific test cases without domain-expert oversight. If you lack domain expertise in-house, do not use synthetic generation as a substitute. Partner with domain experts to curate real cases. The cost of including a flawed synthetic case in a safety-critical or compliance-sensitive domain is far higher than the cost of curating real cases.

Do not use synthetic augmentation if you cannot validate it. If you lack the capacity to review and approve generated cases before including them in your golden set, do not generate them. Unvalidated synthetic cases are worse than no cases. They create false confidence. Your golden set appears comprehensive, but it tests for unrealistic scenarios, ambiguous correctness, or irrelevant edge cases. You pass your regression tests while real regressions slip through. Validation is not optional. It is the step that turns generated noise into validated signal.

## The Right Ratio of Synthetic to Human-Curated Cases

There is no universal ratio. The right balance depends on your domain, your use case, and the quality of your synthetic generation process. But a useful heuristic is this: your human-curated core should represent at least 60 to 70 percent of your golden set. Synthetic augmentation should account for no more than 30 to 40 percent. If synthetic cases exceed half your golden set, you are over-relying on generation and under-investing in curation.

Within the synthetic portion, adversarial and stress-test cases can represent a larger share. If your golden set has 800 cases, and 300 are synthetic, it is reasonable for 200 of those 300 to be adversarial or edge-case stress tests. These are the cases where synthetic generation adds the most value. For core use cases, common workflows, and stakeholder-prioritized correctness checks, the ratio should flip. These should be 80 to 90 percent human-curated, with only a small synthetic tail covering minor variations.

Track the contribution of synthetic cases to your regression detection rate. If synthetic cases catch 40 percent of regressions, they are pulling their weight. If they catch less than 10 percent, they are not adding enough value to justify the maintenance burden. If they catch more than 60 percent, you might be under-investing in human-curated cases that represent the core failure modes users experience.

Synthetic augmentation is a tool. It extends coverage, balances distributions, and scales adversarial testing. But it is not a replacement for the hard work of understanding what correctness means to your users, curating cases that represent real needs, and validating that your golden set measures what actually matters. Use it to fill gaps, not to avoid the foundational work that makes a golden set golden.
