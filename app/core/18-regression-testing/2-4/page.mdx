# 2.4 — Sourcing Golden Set Examples

Where do golden examples actually come from? Most teams treat this as a creative exercise — they sit in a conference room, imagine what matters, and write test cases that feel representative. Three months later, their golden set has almost no overlap with what actually breaks in production. The golden set passes every release. Production incidents happen weekly. The disconnect is not accidental. It reveals a fundamental misunderstanding: golden sets are not invented. They are excavated from the places where your system has already been tested by reality.

The best golden sets are built from archaeological work. You go back through your system's history and extract the moments that mattered — the queries that broke your model, the edge cases that exposed gaps in your prompts, the user behaviors you never anticipated, the regulatory scenarios you barely caught in time. These are not hypothetical. They are real examples that already proved themselves important enough to cause a problem, generate a support ticket, or require an incident post-mortem. When you compile these into a golden set, you are building a test suite that is guaranteed to be relevant because it already failed you once.

## Production Logs as Primary Source Material

The richest source of golden examples is production traffic. Real users generate queries, inputs, and interaction patterns that no internal team would ever imagine. They abbreviate in unexpected ways. They make typos that still convey intent. They code-switch between languages. They reference cultural contexts your team does not share. They phrase sensitive questions with euphemisms your model was never trained to recognize. They chain multiple intents into a single request. They assume capabilities your product does not have. Every one of these behaviors is a test case.

Start by pulling a representative sample of production logs — say, ten thousand recent interactions. Sort them by response latency, by user feedback signals, by downstream conversion rates, by the presence of fallback behavior. You are looking for the queries where your system struggled. A 500ms response instead of 150ms. A user who rephrased their question three times. A session that ended without completing the task. These are not random samples. They are diagnostic samples that reveal where your system is fragile.

From this set, manually review the top hundred edge cases. You will see patterns immediately. Certain query structures consistently cause longer latency because your retrieval step is expensive for multi-clause questions. Certain phrasings trigger your safety filter even though the user intent was benign. Certain domain-specific abbreviations fail to match your knowledge base. Each pattern becomes a golden example. You extract one representative instance of the pattern, annotate it with the expected behavior, and add it to your golden set. Now your regression suite tests for the thing that already broke.

But do not stop at edge cases. Also sample from your highest-volume, most common queries. These are the queries your system must never regress on. If your chatbot handles password resets, and 40 percent of all traffic is password reset requests, you need at least a dozen variations of password reset queries in your golden set. If your model summarizes contracts, and most contracts are NDAs, your golden set needs comprehensive NDA coverage. The golden set protects both your known strengths and your known weaknesses. Production logs give you both.

## Incident Archaeology and Failure Catalogs

Every production incident is a gift. It tells you exactly what your system could not handle and exactly what the consequences were. Yet most teams treat incidents as isolated problems to be fixed and forgotten. The post-mortem documents the root cause, the team deploys a patch, and everyone moves on. Six months later, a different change reintroduces the same failure mode because there was no regression test preventing it.

Build a failure catalog. Go back through every incident in the last twelve months. Extract the input that triggered the failure. If it was a prompt that caused the model to hallucinate patient data, save that prompt. If it was a retrieval query that returned incorrect documents, save that query. If it was a tool call that executed the wrong action, save the conversation history that led to it. These become mandatory golden examples. Any new release that causes your system to fail on a past incident should be automatically blocked.

The annotation for incident-derived examples is straightforward. The expected behavior is whatever would have prevented the incident. If the model hallucinated, the expected behavior is to refuse to answer or retrieve real data. If the retrieval was wrong, the expected behavior is the correct document set. If the tool call was wrong, the expected behavior is no tool call or the correct one. You are not aiming for perfect behavior. You are aiming for "not repeating this specific failure."

Treat your incident history as your most valuable testing asset. Some teams maintain a dedicated "hall of shame" subset within their golden set — twenty to fifty examples that represent the most costly, most embarrassing, or most dangerous failures the system has ever produced. Every release must pass the hall of shame. If a change causes any of these examples to regress, the release is blocked immediately. No discussion. The team already paid the price for these failures once. They do not pay it again.

## Support Tickets and User Pain Points

Your support team sees a version of your product that your engineering team never does. They see the accumulated frustration of users who tried your system, found it lacking, and cared enough to reach out for help. They see the same complaints over and over. They see the specific phrasings users choose when your model misunderstands them. They see the edge cases your documentation does not cover. They see the tasks users assume your product can do but cannot.

Mine your support ticket system for golden examples. Filter for tickets tagged with keywords like "incorrect response," "model did not understand," "wrong information," "not working as expected." Pull the top hundred by volume. Half of them will be user error or misunderstanding of your product's scope. The other half are real system failures. Those failures are golden examples.

For each valid support ticket, extract the user's original input and the response your system gave. Then determine what the correct response should have been. Sometimes the correct response is a direct answer the model failed to provide. Sometimes it is a clarifying question the model should have asked. Sometimes it is a graceful refusal because the request is out of scope. Annotate the example accordingly. Add it to your golden set. Now your regression suite tests for the thing your users already told you was broken.

Support-derived examples have a unique property: they represent the user's mental model of your system, not your team's. Users phrase questions in ways your team would never write them. They make assumptions about capabilities you never claimed to have. They expect behaviors that feel obvious to them but were never in your requirements. These examples force your golden set to be user-centric rather than engineer-centric. That is exactly what you want. Your regression suite should test whether your system works for your users, not whether it works for your internal team's idealized understanding of your users.

## Red-Team Prompts and Adversarial Coverage

Not all important examples come from organic user behavior. Some come from deliberate adversarial testing. Red-teaming is the process of intentionally trying to break your system — probing for prompt injections, data leakage, policy violations, unsafe outputs, biased responses. Every red-team session generates dozens of adversarial prompts. The ones that succeed in breaking your system become mandatory golden examples.

Go back through your red-teaming archives. Identify every prompt that successfully jailbroke your model, leaked sensitive information, bypassed a safety filter, or produced an output your team would never want a user to see. Extract those prompts. Annotate them with the expected behavior: refusal, sanitized output, fallback response, or policy-compliant alternative. Add them to your golden set. Now your regression suite tests whether your defenses remain intact across releases.

The value of red-team-derived examples is that they test the boundaries of your system's behavior. Organic user traffic mostly stays within expected use cases. Adversarial traffic deliberately explores the edges. It tests what happens when someone tries to trick your model into revealing training data. It tests what happens when someone phrases a harmful request as a hypothetical. It tests what happens when someone embeds instructions in user data. These are low-probability, high-consequence scenarios. They may never appear in production logs. But if they do appear, the consequences are severe. Your golden set must cover them.

Red-teaming also reveals capability gaps. Sometimes an adversarial prompt does not break your model — it just reveals that your model cannot do something it should. A user tries to bypass your content filter by asking the same question in five different languages, and your model only has safety coverage in English. That is not a security failure. It is a capability gap. But it still belongs in your golden set because it represents a known limitation that future changes might accidentally make worse.

## Compliance Cases and Regulatory Requirements

If your system operates in a regulated domain — healthcare, finance, legal, hiring, credit — your golden set must include compliance test cases. These are examples that test whether your system adheres to regulatory requirements. They are not optional. They are the difference between a compliant release and a compliance violation that costs your company millions in fines.

Compile a list of every regulatory requirement your system must satisfy. GDPR data minimization. HIPAA de-identification. Fair lending standards. Non-discrimination in hiring. Each requirement maps to one or more golden examples. For GDPR, you need examples that test whether your model refuses to answer questions that would require revealing unnecessary personal data. For HIPAA, you need examples that test whether your model redacts protected health information in its responses. For fair lending, you need examples that test whether your model's recommendations are independent of protected attributes like race or gender.

These examples are different from organic user queries. They are adversarial in structure but regulatory in purpose. A GDPR compliance example might be a prompt that asks your model to list all personal data it has about a user, when the correct response is to refuse and direct the user to a formal data subject access request process. A HIPAA compliance example might be a prompt that asks your model to summarize a patient record, when the correct response is to strip all identifying information before responding. These are not natural user interactions. They are test cases designed to prove compliance.

Compliance cases also include positive examples — scenarios where your system should take a specific action to satisfy a regulatory requirement. If your system must provide an explanation for any automated decision that affects a user, your golden set needs examples of decisions paired with compliant explanations. If your system must allow users to opt out of data processing, your golden set needs examples of opt-out requests paired with confirmation responses. The golden set does not just test that your system avoids violations. It tests that your system actively fulfills its obligations.

## The Earned-Not-Invented Principle

Golden sets are not creative writing exercises. They are artifacts of reality. Every example should trace back to a real event: a production query, a past failure, a support ticket, a red-team finding, a compliance obligation. If you cannot answer "where did this example come from," it does not belong in your golden set. It belongs in your broader eval suite, where speculative coverage is fine. But the golden set is reserved for examples that have already proven their importance.

This discipline prevents golden set bloat. Teams that invent examples from scratch end up with hundreds of hypothetical test cases that feel important but never actually correspond to system failures. The golden set grows to two thousand examples. Regression tests take an hour to run. Releases slow to a crawl. And when a real production issue happens, the team discovers their golden set did not include the relevant scenario because no one thought to invent it.

The earned-not-invented principle keeps your golden set lean, relevant, and fast. You only add examples that have already justified their existence. Your regression suite stays focused on protecting the behaviors that actually matter to your users, your business, and your regulators. And when your golden set grows over time, it grows in proportion to your system's real-world complexity — not in proportion to your team's imagination.

Your golden set is a living archive of everything your system has learned the hard way. Treat it accordingly. Every example is a scar from a past failure or a shield against a known threat. Sourcing examples carefully is not just about test coverage. It is about ensuring that your regression suite protects the things that have already cost you something to learn.

The next challenge is ensuring that the ground truth annotations for these examples are actually correct — because a golden set with weak labels creates false regressions and erodes trust in your release process.

