# 5.4 â€” Branch-Based Testing Strategies

Feature branches get fast tests. Main gets the full suite. Release gets the golden set. This is not just convention. It is the architecture that keeps development velocity high while keeping production quality non-negotiable. Teams that run the same tests on every branch waste money and slow down development. Teams that run different tests without a clear strategy ship regressions. The branch-based testing strategy encodes your quality requirements into your branch protection rules so the CI system enforces them automatically.

Different branches serve different purposes. Feature branches are where engineers experiment, iterate, and get fast feedback. Main branch is your stable baseline where only tested, reviewed, and approved code lives. Release branches are your production candidates where you validate everything one final time before deploy. Each branch has a different risk profile, different velocity requirements, and different quality gates. Your testing strategy should reflect this.

## Feature Branch Testing: Speed Over Completeness

Feature branches exist for rapid iteration. Engineers push multiple commits per hour, trying different approaches, debugging failures, and refining implementations. The testing strategy for feature branches must prioritize fast feedback over comprehensive validation. If tests take 45 minutes to run, engineers will stop waiting for them.

The feature branch test suite should complete in under five minutes and cost less than two dollars per run. This means running smoke tests and targeted regression tests only. Smoke tests validate that basic functionality still works. Targeted regression tests validate the specific component the engineer is modifying. Everything else waits until the pull request stage.

For a feature branch modifying prompt templates, run the smoke suite plus prompt-specific regression tests for the affected prompts only. If the engineer is working on customer service prompts, do not run legal document analysis tests. If they are iterating on the system prompt, do not run RAG retrieval tests. Scope tests to what changed.

For a feature branch modifying retrieval logic, run the smoke suite plus RAG evaluation tests. Do not run end-to-end scenario tests or adversarial tests yet. The engineer needs to know whether their retrieval changes improved relevance, not whether the system handles prompt injection correctly. That validation comes later.

For a feature branch refactoring internal utilities that do not touch inference paths, run unit tests only. Skip model-based tests entirely. If the refactor changes model behavior, the engineer made a mistake and needs different feedback than "your tests are slow."

Feature branch testing should never block development velocity. If an engineer wants to push ten exploratory commits in an hour to test different prompt phrasings, the CI system should handle that gracefully. Fast, cheap, targeted tests enable this workflow. Slow, expensive, comprehensive tests kill it.

Some teams configure CI to skip tests entirely on feature branches and only run tests when the engineer opens a pull request. This works if your team is disciplined about opening draft PRs early to get test feedback. It fails if engineers push code for days before opening a PR and then discover their changes broke everything.

## Main Branch Testing: The Full Regression Suite

Main branch is your source of truth. Every commit on main should be production-ready, fully tested, and approved by at least one other human. The testing strategy for main branch enforces this through comprehensive validation that nothing is allowed to bypass.

When a pull request merges to main, the CI system runs your full regression suite against the merged code. This is not the same test run that validated the PR. The PR tests ran against the feature branch, which may have diverged from main during code review. The post-merge test run validates that the integration between the new code and current main did not introduce regressions.

The main branch test suite includes smoke tests, component-specific regression tests, integration tests, and golden set validation. It runs every test you trust enough to block a deploy. If a test is flaky, it should not be in the main branch suite. If a test is too expensive to run on every merge, it belongs in time-based triggers or release validation, not main branch CI.

Main branch tests typically take 20 to 45 minutes and cost 50 to 150 dollars per run. This is acceptable because merges to main are infrequent relative to feature branch commits. A team of ten engineers might merge 15 pull requests per day, which means 15 full test runs. That is manageable. Running full tests on every feature branch commit across ten engineers would mean hundreds of runs per day, which is not.

Main branch test failures are treated as critical incidents. If a merge to main causes test failures, the team stops and fixes it immediately. You do not merge more code on top of a broken main branch. You do not wait until tomorrow to investigate. A broken main branch means nobody can cut a release, nobody can merge their PR, and nobody knows what good looks like anymore.

Some teams configure main branch protection to require that tests pass before the merge completes. This prevents broken code from ever landing on main. Other teams allow the merge to complete but immediately revert it if post-merge tests fail. Both approaches work. The first is safer. The second is faster when you trust your PR-level testing.

## Release Branch Testing: Golden Set Plus Safety

Release branches are your final gate before production. Code on a release branch has already passed feature branch testing, pull request validation, and main branch regression. The release branch testing strategy adds one more layer: the golden set plus any additional safety tests required for deploy.

The golden set is your highest-confidence test collection. These are the 200 to 500 test cases that represent your most critical user scenarios, your most important quality dimensions, and your biggest production failure risks. Running the golden set on the release branch is the final validation that this code is ready to serve users.

Release branch testing also includes safety tests that are too expensive or too slow to run on every merge. Adversarial testing, bias audits, compliance validation, and stress testing all belong here. These tests might take two hours and cost 300 dollars, which is prohibitive for main branch CI but acceptable for release candidates that deploy once per week or once per sprint.

For regulated industries, release testing includes compliance-specific validation. If you operate under HIPAA, your release tests validate that the model does not leak protected health information. If you operate under GDPR, your release tests validate that the model respects data subject rights. If you operate under the EU AI Act, your release tests validate that your system meets transparency and accuracy requirements for your risk classification.

Release branch tests run when you create the release branch and again immediately before deploy. The gap between branch creation and deploy might be hours or days, during which the model API could have changed, the knowledge base could have been updated, or infrastructure could have shifted. Re-running tests immediately before deploy catches environment-driven regressions.

Release branch test failures block deploy. No exceptions. If your golden set passes but adversarial tests fail, you do not deploy. If compliance tests pass but bias audits reveal issues, you do not deploy. The entire point of release testing is to give you confidence that production will behave as expected. A test failure destroys that confidence.

## Hotfix Branch Testing: Targeted Regression on Affected Areas

Hotfix branches exist to fix production incidents quickly. The normal development workflow is too slow when users are experiencing an outage or data corruption. Hotfix branches bypass the usual feature branch, PR review, and main branch merge process in favor of a direct path from fix to production.

The testing strategy for hotfix branches must balance speed with safety. You need fast feedback to get the fix deployed quickly. But you also need sufficient validation to ensure the hotfix does not introduce new regressions. The solution is targeted regression testing scoped to the affected area.

If the hotfix modifies prompt logic, run smoke tests plus prompt-specific regression tests. If it changes retrieval configuration, run RAG evaluation tests. If it adjusts model routing rules, run routing accuracy tests. Skip unrelated test suites. The goal is validating that the hotfix actually fixes the issue and does not break adjacent functionality.

Hotfix branch tests should complete in under ten minutes. Anything longer delays the fix and extends the production incident. If your targeted regression suite takes longer than ten minutes, you need faster tests or better scoping.

After the hotfix deploys and the incident resolves, merge the hotfix back to main and run the full regression suite. This post-incident validation ensures the hotfix did not introduce subtle regressions that targeted testing missed. If post-incident tests fail, you now have a new incident to manage, but at least it is not compounding the original problem.

Some teams skip hotfix testing entirely under the logic that production is already broken, so any fix is better than no fix. This is professional negligence. Untested hotfixes frequently make incidents worse by introducing new failure modes, cascading failures, or data corruption. The few minutes spent running targeted regression tests prevent hours of extended outages.

## Branch Protection Rules for AI Quality Gates

Branch protection rules enforce your testing strategy at the infrastructure level. They prevent code from merging unless it meets specific quality requirements. For AI systems, branch protection rules should require that regression tests pass, that golden set scores stay above thresholds, and that at least one human reviewed the changes.

Main branch protection should require passing tests, code review approval, and linear history. Passing tests means the full regression suite completed successfully. Code review approval means at least one other engineer reviewed the changes and explicitly approved them. Linear history means no merge commits, which keeps your git history clean and makes rollbacks easier.

Release branch protection should require passing golden set tests, passing safety tests, and approval from a tech lead or release manager. This ensures that someone with production responsibility explicitly signed off on the release.

Feature branch protection is typically minimal. Feature branches are personal workspaces where engineers experiment. Requiring tests to pass before pushing commits to a feature branch creates friction without adding safety. Tests should run automatically, but failures should not block commits.

Branch protection rules also prevent force pushes to protected branches. Force pushing rewrites git history, which breaks traceability and makes rollbacks dangerous. If you need to undo a merge to main, use a revert commit, not a force push.

Some teams configure branch protection to require that the branch is up to date with main before merging. This prevents race conditions where two PRs both pass tests against an old version of main, but merging them together causes failures. The merge queue pattern solves this more elegantly.

## The Merge Queue Pattern for AI Systems

The merge queue pattern solves a specific race condition. Two pull requests both target main. Both run regression tests against the current state of main. Both pass. Engineer A merges first. Engineer B merges second. The combination of both changes breaks tests even though each change individually passed.

This happens because the two PRs tested against the same baseline. PR A passed tests against main at commit X. PR B also passed tests against main at commit X. But after A merges, main is at commit X plus A. PR B never tested against X plus A, only against X.

The merge queue prevents this by serializing merges and re-testing immediately before merge. When Engineer A approves their PR, it enters the merge queue. The CI system merges the PR to main and runs the full regression suite. If tests pass, the merge completes. If tests fail, the merge is aborted and the PR goes back to Engineer A for fixes.

When Engineer B approves their PR, it enters the merge queue behind Engineer A. The queue waits for A to complete. Once A succeeds, the CI system rebases B on top of the new main and runs tests. If tests pass, B merges. If tests fail, B goes back for fixes. This ensures every merge is tested against the actual current state of main.

The merge queue adds latency. Instead of merging immediately when a PR is approved, you wait for your turn in the queue. For teams with high merge volume, this can mean waiting 30 to 60 minutes. But the tradeoff is worth it. The merge queue prevents the race condition that causes main branch breakage despite passing PR tests.

For AI systems with expensive regression suites, the merge queue becomes essential. If your full regression tests cost 80 dollars and take 25 minutes, you cannot afford to run them twice per PR: once when the PR opens and again immediately before merge. The merge queue solves this by only running full tests once, immediately before the actual merge, against the actual current main.

## Preventing Regression from Feature Branch Merges

Feature branch merges are the highest-risk moment for regressions. The code was developed in isolation, possibly over days or weeks, while main continued to evolve. Even with a merge queue, regressions slip through when feature branches diverge too far from main.

The solution is frequent rebasing. Engineers should rebase their feature branch on main at least once per day. This keeps the feature branch close to main, reduces merge conflicts, and ensures regression tests run against code that resembles what will actually be merged.

Frequent rebasing also surfaces integration issues early. If your feature branch modifies prompt logic and main branch introduced a new prompt template format, you want to discover that conflict while you are still developing, not when you open a PR. Rebasing daily gives you continuous integration feedback even while working on a long-lived feature branch.

Some teams enforce rebase frequency through automation. A bot comments on pull requests that are more than three days behind main, warning the engineer to rebase. If the PR falls more than seven days behind, the bot blocks merging until the engineer rebases and re-runs tests.

Another approach is to require that PRs are no more than 500 lines of code. Large PRs are hard to review, hard to test, and more likely to introduce regressions. They also indicate that the feature branch lived too long without merging. Breaking work into smaller PRs keeps branches short-lived, reduces divergence from main, and makes regressions easier to isolate when they occur.

## Test Results as Merge Requirements

Branch protection rules can require more than just passing tests. They can require that specific metrics stay above thresholds, that no new failures were introduced, and that golden set scores remain stable.

For example, you might configure main branch protection to require that accuracy on the golden set stays above 94 percent, that no task-specific test scores drop by more than two percentage points, and that adversarial test pass rate stays above 88 percent. If a PR causes accuracy to drop from 96 to 93 percent, the merge is blocked even though 93 percent is above your production minimum. The requirement is that you do not introduce regressions, not just that you stay above a floor.

This approach catches subtle degradation. If every PR causes a one-point accuracy drop, you will slowly drift from 96 percent to 85 percent without any single PR looking obviously bad. Requiring no regression relative to current main prevents this drift.

The tradeoff is that legitimate changes sometimes reduce one metric while improving another. If you refactor prompts to be more concise and accuracy drops slightly but latency improves significantly, blocking the merge might be wrong. The solution is making regression requirements configurable per PR with mandatory justification. The engineer can override the requirement by explaining why the tradeoff is acceptable, and the code reviewer must agree.

Test results should be visible in the PR interface. Engineers and reviewers should see accuracy scores, pass rates, and metric deltas directly in GitHub, GitLab, or whatever code review tool you use. This makes quality part of the code review conversation, not an afterthought.

## Cross-Environment Branch Testing

Different branches often deploy to different environments. Feature branches deploy to personal dev environments. Main branch deploys to staging. Release branches deploy to production. Your testing strategy should account for environment-specific behavior.

Some regressions only appear in production because production has different data, different traffic patterns, or different infrastructure than staging. You cannot catch these with branch-based testing alone. But you can reduce their frequency by running environment-aware tests.

For example, your release branch tests might include production traffic replay. You take real user queries from the past week, anonymize them, and run them through the release candidate. If the model behavior changes significantly from current production, you investigate before deploying. This catches regressions that only appear on production data.

Similarly, your main branch tests might include staging environment validation. After merging to main and running standard regression tests, deploy the new code to staging and run a live test against the staging environment. If staging tests fail but CI tests passed, you have an environment-specific issue to fix before cutting a release.

Environment-aware testing is expensive and slow, so it belongs on branches close to production, not on feature branches. But it is the only way to catch environment-specific regressions before users see them.

The next subchapter covers test runner infrastructure, where you will learn how to build CI systems that execute thousands of model-based tests daily without breaking your budget or your patience.
