# 1.10 â€” Building Your First Regression Infrastructure

Minimum viable regression infrastructure is not optional. It is survival. Every AI system that reaches production without it is one prompt change, one model update, one vendor patch away from silent degradation that no one detects until customers complain. The difference between a team that catches breaking changes before they ship and a team that discovers them in production is not sophistication or budget. It is the presence of six specific components, correctly connected, running automatically on every change. Anything less is hope dressed up as process.

Most teams believe they need complex machinery to test AI systems properly. They imagine days of infrastructure work, specialized tooling, dedicated teams. They delay building regression tests because the full vision feels overwhelming. This is the wrong mental model. Minimum viable regression infrastructure is not a scaled-down version of enterprise testing. It is a complete system at the smallest possible scale. You can build it in a day. You can run it in CI within a week. You can enforce hard gates that block deployments before your next sprint ends. The question is not whether you have time to build it. The question is whether you can afford not to.

## The Six Essential Components

The minimum viable stack has exactly six components. Each one is necessary. None of them are optional. Together they form a closed loop that detects regression automatically, quantifies it objectively, and prevents it from reaching production without human override. The components are: a golden dataset, baseline outputs stored in version control, metric computation that runs automatically, statistical comparison against stored baselines, a CI trigger that runs on every pull request, and gate enforcement that blocks merges when regression is detected.

**Golden dataset** is the first component. This is your curated set of test cases that represent the quality and behavior you cannot afford to lose. In minimum viable form, this is thirty to one hundred examples. Not thousands. Not representative of your full traffic distribution. Just the critical cases where failure is unacceptable. A healthcare chatbot might have thirty cases covering medication interactions, dosage calculations, and emergency triage. A legal assistant might have fifty cases covering contract clause extraction, regulatory citations, and ambiguous term interpretation. A customer support agent might have forty cases covering refund policies, escalation triggers, and angry customer de-escalation. These are not random samples. These are the cases you review in every incident post-mortem. The cases that, when they fail, cause immediate customer harm or legal exposure or revenue loss. The cases where you already know what good looks like.

Store the golden dataset in a structured format. JSONL works well. Each line is one test case with input, expected behavior description, and any context the model needs. The format matters less than the discipline of version-controlling it. When your product manager says a behavior must never regress, that requirement goes into the golden set. When an incident reveals a failure mode you missed, you add a test case for it. The golden set is not static. It grows as your understanding of critical quality deepens. But it starts small. Thirty cases is enough to catch most regressions. One hundred cases is enough to catch nearly all of them.

**Baseline outputs stored** is the second component. Every time you promote a model or prompt to production, you run the golden dataset through it and save the full output for every case. Not just the final answer. The full output including any intermediate steps, tool calls, retrieved context, reasoning traces. Store these in version control alongside the golden dataset. Tag them with the model version, the prompt hash, the timestamp. These outputs are your ground truth for what production behavior looks like right now. When you test a candidate change, you compare its outputs to these stored baselines. The comparison is not subjective. It is a diff between two sets of stored artifacts.

Most teams skip this step. They store test cases but not baseline outputs. Then when they run regression tests, they have no objective reference point. They eyeball results and guess whether they seem better or worse. This is not testing. This is theater. Storing baseline outputs transforms regression testing from a subjective ritual into an objective measurement. You ran the same inputs through two different systems. You stored both outputs. Now you compare them with metrics. The human judgment happened once, when you chose what to put in the golden set. After that, the process is mechanical.

**Metric computation** is the third component. You need at least two metrics: one for correctness and one for safety. Correctness measures whether the model still does the job it is supposed to do. Safety measures whether it avoids the harms it is supposed to avoid. In minimum viable form, correctness can be exact match on structured outputs, semantic similarity on open-ended text, or LLM-as-judge scoring for nuanced reasoning. Safety can be absence of specific phrases, presence of required disclaimers, or refusal rate on adversarial inputs. The metrics do not need to be perfect. They need to be automated, quantitative, and sensitive enough to catch real regressions before customers do.

The computation runs on every candidate output against every baseline output. You end up with two sets of metric values: one for the baseline system, one for the candidate system. Store both. You will need them for the next step. The computation itself is usually a Python script that loads the golden dataset, loads the baseline outputs, generates candidate outputs by calling your staging model or prompt, computes metrics for both, and writes the results to a file. This script runs in CI. It should complete in under five minutes for a hundred cases. If it takes longer, your golden set is too large or your metric computation is too expensive. Simplify until it is fast enough to run on every pull request.

## Statistical Comparison and Gate Logic

**Statistical comparison** is the fourth component. You have metric values for baseline outputs and metric values for candidate outputs. Now you need to decide: is the candidate better, worse, or equivalent? This is not a subjective judgment. It is a statistical test. For every metric, you compute the difference between baseline and candidate performance. If the difference is negative and exceeds a threshold, you flag regression. If the difference is positive and exceeds a threshold, you flag improvement. If the difference is within threshold bounds, you call it equivalent.

The threshold is not zero. Metrics have variance. Semantic similarity scores fluctuate by a point or two even when the underlying model behavior is identical. LLM-as-judge scores vary based on randomness in the judge model. Setting the threshold to zero means flagging noise as regression. This creates false positives that train your team to ignore the gate. Instead, set thresholds based on observed variance in your baseline metrics. Run the golden dataset through your production system three times. Compute metrics each time. The variation you see is your noise floor. Set your regression threshold at twice that variation. This gives you a buffer that tolerates normal fluctuation while catching real degradation.

For a golden set of one hundred cases, you can use simple aggregate statistics. Compute mean performance across all cases for baseline and candidate. If candidate mean is lower than baseline mean by more than your threshold, flag regression. For more sensitivity, compute per-case differences and count how many cases regressed by more than the threshold. If more than ten percent of cases regress, flag it even if the aggregate mean is stable. This catches the pattern where a change improves average performance but breaks a critical minority of cases. The logic is simple. The discipline is what matters. You defined the threshold in advance. You computed the metrics automatically. You applied the comparison mechanically. There is no room for "it looks fine to me" or "I think this is better." The data decides.

**CI trigger** is the fifth component. Regression tests must run automatically on every change that could affect model outputs. This means every prompt edit, every model version update, every retrieval pipeline modification, every tool definition change, every system prompt adjustment. If a pull request touches any of these, the CI system runs the regression suite before allowing merge. You do not rely on developers remembering to run tests. You do not trust manual testing. You enforce it in CI.

The trigger is a GitHub Actions workflow, a GitLab CI job, a Jenkins pipeline. The implementation does not matter. What matters is that it runs automatically, runs on every relevant pull request, completes in under ten minutes, and reports results as a pass-fail status check. The workflow checks out the code, installs dependencies, loads the golden dataset, loads baseline outputs, runs candidate outputs through the staging system, computes metrics, performs statistical comparison, and writes results to the CI logs. If regression is detected, the workflow exits with a non-zero status. This blocks the merge until someone overrides it or fixes the regression.

## Gate Enforcement and Override Protocol

**Gate enforcement** is the sixth component. This is the step that separates real regression testing from gate theater. If your CI detects regression but allows the merge anyway, you have built a monitoring system, not a gate. Monitoring is valuable. Gates are necessary. Enforcement means the regression check is a required status check in your repository settings. Pull requests with failing regression tests cannot be merged. Period. This is not negotiable. This is not "we should probably look at this before shipping." This is a hard block.

The enforcement has one exception: human override. There are legitimate reasons to ship a change that regresses some metrics. You are trading accuracy for speed. You are intentionally removing a capability to improve safety. You are accepting temporary regression because you plan to fix it in the next iteration with a technique that is not ready yet. These decisions are valid. But they must be explicit, documented, and approved by someone with authority to accept the trade-off. The override mechanism is not "ignore the test failure and merge anyway." The override mechanism is "add a label to the pull request that says override-regression-gate, write a comment explaining why this regression is acceptable, get approval from a designated approver, and then merge."

Every override gets logged. At the end of every sprint, someone reviews all overrides and asks: are the trade-offs we accepted still valid? Have we fixed the regressions we said we would fix? Are we overriding too often, which suggests our thresholds are miscalibrated or our golden set is wrong? The log creates accountability. It makes the cost of regression visible. It forces teams to confront the gap between the quality they claim to enforce and the quality they actually ship.

## What Good Looks Like Versus What Most Teams Have

Good regression infrastructure runs silently in the background. Developers submit pull requests. CI runs regression tests. Most of the time, tests pass and the pull request merges. Occasionally, tests fail and the developer investigates. They discover their prompt change broke a critical case. They revise the change or adjust the prompt or update the golden set because the failure revealed a misaligned expectation. The regression was caught before it reached staging. The customer never saw it. The team did not need an incident to learn the change was dangerous. The infrastructure did its job.

What most teams have is a folder called tests that no one runs. Or a script that runs regression tests but takes forty minutes and only works on one developer's laptop. Or a CI job that runs regression tests but never blocks merges because the tests are flaky and no one trusts them. Or a process where someone manually reviews a sample of outputs before each deploy and decides subjectively whether the new version is better. These are not regression gates. These are gestures toward testing that create the appearance of rigor without the substance.

The difference between good infrastructure and the illusion of infrastructure is not complexity. It is completeness. The six components form a closed loop. The golden dataset defines what quality means. Baseline outputs anchor that definition to specific examples. Metrics quantify whether candidates meet the definition. Statistical comparison removes subjectivity. CI trigger ensures the comparison happens every time. Gate enforcement turns the comparison into a decision that blocks bad changes. Remove any one component and the loop breaks. You can still catch some regressions manually. But you will miss others. And the ones you miss will be the ones that matter.

## Building It in One Day

You can build minimum viable regression infrastructure in eight hours. Hour one: define your golden dataset by listing the thirty most critical test cases you already know about. Write them down in JSONL format. Hour two: run those cases through your current production system and save the outputs. Commit the golden set and the baseline outputs to git. Hour three: write a Python script that loads the golden set, runs the cases through a candidate system, and computes metrics. Test it locally. Hour four: write the statistical comparison logic that compares candidate metrics to baseline metrics and flags regression. Test it locally. Hour five: wrap the script in a CI workflow that triggers on pull requests. Test it on a branch. Hour six: add the regression check as a required status check in your repository settings. Test that it blocks merges. Hour seven: document the override protocol and add it to your team wiki. Hour eight: run a fire drill where you intentionally introduce a regression and verify the gate catches it.

The first version will have gaps. The golden set will miss cases you later realize are critical. The metrics will not capture all the nuances of quality. The thresholds will produce occasional false positives or false negatives. This is expected. Minimum viable infrastructure is not perfect infrastructure. It is infrastructure that catches most regressions most of the time, which is infinitely better than catching no regressions until customers complain. You improve it iteratively. Every incident that slips through prompts you to add a test case. Every false positive prompts you to refine your threshold. Every override prompts you to revisit whether your golden set reflects actual priorities.

The teams that fail to build regression infrastructure do not fail because they lack skills or tools. They fail because they convince themselves that anything short of comprehensive testing is not worth building. They wait for the perfect design. They wait for dedicated headcount. They wait for a testing framework to mature. They wait until a major incident forces them to build something reactive. Then they build it under pressure, poorly, and half the team resents it because it feels like overhead imposed from outside. The teams that succeed are the ones who recognize that imperfect infrastructure built today is worth more than perfect infrastructure built never. They start small. They enforce it immediately. They improve it continuously. Within three months, they cannot imagine shipping without it.

The next subchapter covers the most common anti-patterns teams fall into when building and operating regression infrastructure, including the single most dangerous mistake: building gates that never actually block deploys.
