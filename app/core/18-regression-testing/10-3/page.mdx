# 10.3 — Task Completion and Conversion Regression

In September 2025, a travel booking platform deployed a new model version that achieved 94 percent accuracy on their routing benchmark, up from 89 percent. The engineering team celebrated. The eval suite showed improvements across every dimension they measured: intent classification was sharper, entity extraction was cleaner, response coherence was better. The model shipped to production on a Friday afternoon.

By Monday morning, the business metrics told a different story. Task completion rate had dropped from 68 percent to 61 percent. Users were starting booking flows but abandoning them at higher rates. Session length increased, but not in a good way—users were spending more time in conversations that went nowhere. The support ticket queue grew by 40 percent over the weekend. Revenue from AI-assisted bookings dropped 11 percent.

The root cause took three days to find. The new model was more cautious about making recommendations. When a user asked for hotel options, the old model would present three solid choices immediately. The new model asked clarifying questions first: budget range, preferred neighborhood, amenities needed. The questions were reasonable. The logic was sound. But users interpreted the questions as the model not understanding them. They abandoned the conversation and went to a competitor's site that gave instant answers.

The engineering metrics improved. The user experience degraded. The business outcome collapsed. This is task completion regression—when your model gets better at answering questions but worse at helping users accomplish what they came to do.

## Task Completion as the Ultimate Business Metric

Task completion is the bridge between model performance and business value. A user does not come to your product to have a conversation. They come to book a flight, get a question answered, complete a purchase, resolve an account issue, find a document, schedule an appointment. The conversation is the means. Task completion is the end.

Task completion rate is binary: did the user accomplish what they set out to do, or did they not? Every other metric—response quality, hallucination rate, latency, user satisfaction—matters only insofar as it influences this outcome. A perfectly accurate model that does not help users complete tasks is a failure. A messier model that gets users to their goals faster is a success.

Task completion is also the metric that executives care about. Product leaders do not ask how your F1 score changed. They ask how many more users completed purchases this week compared to last week. Finance does not track your BLEU score. They track revenue per session. When you deploy a new model, the question that determines whether you keep your job is not whether the model is smarter—it is whether users are more successful.

Measuring task completion requires defining what completion means for your use case. For a customer support bot, completion might mean the user's issue was resolved without escalating to a human agent. For a document search assistant, completion might mean the user found the document they needed and opened it. For a shopping assistant, completion might mean the user added items to their cart and checked out. For a medical triage bot, completion might mean the user received a clear next step and booked an appointment or decided to self-care.

The definition must be specific, measurable, and aligned with user intent. Vague definitions like "user was satisfied" or "conversation ended positively" are not sufficient. You need a concrete signal: a transaction completed, a document opened, a ticket closed, an appointment scheduled, a form submitted. This signal becomes your completion event. Everything before it is the funnel. Everything after it is retention.

Task completion regression happens when a model change reduces the percentage of sessions that reach the completion event. The model might be better in every technical dimension. The regression is behavioral, not technical. The model does something—asks more questions, provides more context, uses different phrasing, recommends different actions—that causes more users to abandon the task before finishing.

## Defining Task Completion for Your Use Case

The first challenge is identifying the completion event. In some domains, it is obvious. An e-commerce assistant's completion event is checkout. A travel booking assistant's completion event is a confirmed reservation. A technical support bot's completion event is ticket resolution without human escalation. But in many domains, completion is ambiguous.

Consider a legal research assistant. A lawyer asks for case law related to a specific issue. The model retrieves five relevant cases and summarizes them. Did the task complete? It depends. If the lawyer reads the summaries and moves on to drafting their brief, the task completed. If the lawyer reads the summaries, realizes none of them are relevant, and starts a new search with different terms, the task did not complete. The model responded, but it did not help the user accomplish their goal.

To define completion, you need to understand user intent at the start of the session. Intent determines what success looks like. A user who starts a session with "I need to book a flight to Boston next Tuesday" has clear intent. Completion is booking the flight. A user who starts with "Tell me about flights to Boston" has exploratory intent. Completion might be viewing flight options, or it might be nothing—they are just browsing. You need to distinguish between committed intent and exploratory intent. Only committed intent has a clear completion event.

One approach is to classify sessions by intent category at the start. High-intent sessions have explicit goals: book, buy, cancel, change, resolve. Medium-intent sessions have research goals: compare, explore, learn. Low-intent sessions have no clear goal: browse, chat, experiment. Task completion applies primarily to high-intent sessions. For medium-intent, you measure engagement and depth. For low-intent, you measure retention and return rate.

Another approach is to use implicit signals to infer intent. If a user asks a follow-up question within ten seconds, they are still engaged. If they leave the session open but stop responding for two minutes, they might be reading your response—or they might have abandoned. If they close the session within thirty seconds of the model's response, they either got exactly what they needed or gave up immediately. The challenge is distinguishing between "satisfied and done" and "frustrated and gone."

The most reliable signal is the action the user takes after the AI interaction. Did they click a link the model provided? Did they fill out a form? Did they add an item to their cart? Did they schedule a call? Did they escalate to a human agent? Did they close the session and never come back? These downstream actions reveal whether the model moved the user toward their goal or left them stranded.

For task completion regression testing, you need a baseline. Before deploying a new model, measure task completion rate for the current model over a representative time window—ideally two weeks to smooth out day-of-week and time-of-day variance. This baseline becomes your threshold. The new model must meet or exceed this rate. If task completion drops by more than two percentage points, you do not ship. If it drops by more than five percentage points, you roll back immediately.

## Measuring Task Completion Rates

Task completion rate is the percentage of high-intent sessions that reach the completion event. The denominator is all sessions where the user had a clear goal. The numerator is sessions where the user accomplished that goal. If 1,000 users started booking flows and 680 completed bookings, your task completion rate is 68 percent.

Measuring this requires instrumenting your application to track both the start of intent and the completion event. The start of intent is usually the first user message in a session, tagged with intent classification. The completion event is the downstream action: a transaction ID, a confirmation screen view, a ticket status change, a document download. You log both events with session IDs and timestamps, then join them in your analytics pipeline.

Timing matters. A user might start a booking flow, get distracted, and complete the booking four hours later. Do you count that as task completion? It depends on your product. For a synchronous assistant where users expect instant help, a four-hour gap suggests the user abandoned the AI interaction and completed the task through a different channel. For an asynchronous assistant where users might research and return, a four-hour gap is normal. You need a completion window—the maximum time between intent start and completion event during which you credit the AI interaction. For most products, thirty minutes is reasonable. Beyond that, other factors dominate.

Segmentation reveals where regression happens. Aggregate task completion rate might hold steady while specific user segments collapse. Break down task completion by intent type, user cohort, session length, time of day, and model response characteristics. A model change might improve task completion for simple intents while degrading it for complex ones. It might help new users while confusing power users. It might work well during business hours but fail at night when users have different expectations.

You also need to measure partial completion. Not every abandoned task represents total failure. A user who starts a flight search, views options, adds a flight to their cart, but does not check out has made partial progress. They might return later to complete the purchase. They might have decided the prices were too high. They might have been interrupted. Tracking partial completion—milestones within the funnel—helps you identify where users drop off.

Funnel analysis is the standard tool. Map the steps from intent to completion: intent expressed, model responds, user engages, model provides options, user selects, user confirms, task completes. Each step is a funnel stage. Measure drop-off rate between stages. A model change that increases drop-off between "model provides options" and "user selects" suggests the options are less relevant or less clearly presented. A model change that increases drop-off between "user confirms" and "task completes" suggests confusion about the final step.

Task completion rate variance across sessions reveals model consistency. If task completion rate is 70 percent but some sessions have 90 percent completion and others have 40 percent, your model is inconsistent. High-variance models are risky. A model that helps most users but catastrophically fails a minority might have a decent average completion rate but terrible user trust. Low-variance models are more predictable. You would rather have a model with 68 percent completion across all segments than a model with 75 percent average but wild swings by segment.

## Conversion Regression: When Users Stop Completing Goals

Conversion regression is the business term for task completion regression. It happens when a model change reduces the percentage of users who convert—who buy, book, subscribe, sign up, or otherwise complete a monetizable action. Conversion regression is the nightmare scenario for product leaders. You deployed a better model. Revenue dropped.

The root cause is usually a mismatch between model behavior and user expectations. The model optimizes for accuracy, helpfulness, safety, or some other dimension that your eval suite measures. Users optimize for speed, simplicity, and getting to their goal with minimal friction. When these optimizations conflict, the model wins the eval suite and loses the user.

A common pattern is the over-helpful model. The model is trained to provide thorough, complete answers. It explains context, anticipates follow-up questions, provides caveats and disclaimers. Users interpret this as the model not being confident or not understanding them. They want a direct answer, not a lecture. They abandon the conversation.

Another pattern is the over-cautious model. The model is tuned to avoid errors, so it asks clarifying questions before committing to a response. The questions are reasonable. But users interpret questions as the model stalling or not knowing the answer. They lose confidence and leave. The old model guessed more often and was wrong more often, but it felt faster and more decisive. Users preferred it.

A third pattern is the complexity creep model. The new model has access to more tools, more data sources, more retrieval capabilities. It can do more. But doing more takes longer. Users wait three seconds instead of one second. They see "thinking" indicators. They perceive the system as slower, even if the final answer is better. Speed is a feature. Complexity is a tax.

Conversion regression often shows up in A/B tests even when offline evals look perfect. You run the new model on ten percent of traffic. Task completion rate drops three percentage points. Revenue per session drops five percent. User satisfaction scores drop. But your eval suite shows the new model outperforming the old model on every metric. The disconnect is that your eval suite measures model outputs. Users judge model outcomes. The path from output to outcome includes user perception, trust, and behavior—none of which your eval suite captures.

Detecting conversion regression requires production metrics wired into your release gates. Before you scale a new model from ten percent to 100 percent of traffic, you check: Is task completion rate stable or improving? Is revenue per session stable or improving? Is escalation rate stable or decreasing? If any of these metrics degrade by more than your threshold, you do not scale. You investigate, iterate, or roll back.

The investigation starts with session replay. Find sessions where the old model led to task completion and the new model led to abandonment. Watch what happened. Did the new model ask more questions? Did it provide different options? Did it use different phrasing? Did it take longer to respond? The differences reveal what changed. The user behavior reveals why it mattered.

The fix is usually not reverting to the old model. It is tuning the new model's behavior to preserve the improvements while avoiding the regression. You might keep the better retrieval system but reduce the number of follow-up questions. You might keep the improved accuracy but simplify the phrasing. You might keep the additional tool access but cache results to reduce latency. The goal is to decouple the technical improvement from the behavioral change that caused the regression.

## The Path Length Problem: Model Makes Users Work Harder

Path length is the number of turns required for a user to reach task completion. A shorter path is almost always better. If the old model completed a booking in three turns—user states intent, model provides options, user confirms—and the new model requires five turns because it asks clarifying questions first, the new model increases path length. Longer paths create more opportunities for users to abandon.

Path length regression happens when a model change increases the average number of turns to task completion. The model is not slower per turn. It is not less accurate per turn. It just requires more turns to get to the same outcome. Users perceive this as the model being harder to use. Task completion drops.

The root cause is often a shift in conversational strategy. The old model was trained to make assumptions and move forward quickly. The new model is trained to gather information before acting. The new model is technically more correct—it asks for details the old model should have asked for. But users do not want to be interviewed. They want to get their task done. The increased path length feels like bureaucracy.

Measuring path length requires tracking turns per session for sessions that reached task completion. If the median path length was three turns and it increases to four turns after a model change, you have path length regression. Even if task completion rate holds steady, longer paths mean more user effort. Users who tolerate four turns today might not tolerate five turns tomorrow. Path length creep is a leading indicator of future abandonment.

Path length varies by task complexity. Simple tasks should complete in one to three turns. Medium-complexity tasks might require four to six turns. Complex tasks might need eight to ten turns. If a simple task starts requiring five turns, something is wrong. The model is asking unnecessary questions, providing incomplete answers that require follow-ups, or failing to use context from earlier in the conversation.

One mitigation is to optimize for first-turn resolution. If the user's first message contains enough information to complete the task, the model should attempt completion immediately. It should not ask for clarification unless genuinely ambiguous. This requires training the model to extract maximum information from the initial prompt and to make reasonable default assumptions when details are missing. The trade-off is occasional errors when assumptions are wrong. The benefit is faster task completion for the 80 percent of cases where assumptions are correct.

Another mitigation is to batch questions. If the model needs three pieces of information, it should ask for all three in one turn, not ask them sequentially across three turns. "To book your flight, I need your preferred departure time, seating preference, and whether you want checked baggage" is better than three separate turns asking for each item. The user provides all three answers in one response, reducing path length from four turns to two.

Path length regression testing requires a baseline median path length for each task type. If the new model increases median path length by more than one turn for any task type that represents more than ten percent of sessions, you do not ship. You investigate whether the increased path length is necessary—are you asking for information that genuinely improves outcomes?—or whether it is avoidable friction.

## Task Abandonment Signals

Not all abandoned tasks are equal. A user who abandons after one turn had a different experience than a user who abandons after ten turns. A user who abandons during business hours might come back later. A user who abandons at night might never return. Measuring abandonment requires understanding why users leave.

Explicit abandonment is when the user takes a clear exit action: they close the session, they click a "cancel" button, they navigate away from the page. Implicit abandonment is when the user stops responding. The session stays open, but the user has left. After two minutes of inactivity, you classify the session as abandoned. The challenge is distinguishing between "user is reading a long response" and "user has given up and walked away."

Abandonment timing reveals user frustration. If users abandon in the first turn, the model's initial response failed to engage them. If users abandon after three to four turns, the conversation was not progressing toward their goal. If users abandon after ten turns, they invested significant effort but the model could not close the loop. Late-stage abandonment is the most damaging—the user tried hard, the model failed anyway.

Abandonment rate by turn number shows where friction occurs. Plot abandonment rate as a function of turn count. If abandonment spikes at turn three, something about the third model response or the third user response creates friction. Maybe the model asks a question users do not want to answer. Maybe the model provides options users do not find relevant. Identifying the spike lets you focus improvements on that specific turn.

Abandonment rate by intent type shows which tasks your model struggles with. If users completing simple tasks abandon at five percent but users completing complex tasks abandon at thirty percent, your model is not equipped for complexity. You need better tools, better retrieval, or better handoff to human agents for complex intents.

Abandonment rate by user cohort shows whether the regression is universal or segment-specific. New users might abandon more because they do not understand how to interact with the model. Power users might abandon more because the new model changed behavior they had learned. Mobile users might abandon more because the new model's responses are too long for small screens. Segmenting abandonment reveals where to focus.

One subtle abandonment signal is session restart. A user abandons a session, then starts a new session within five minutes with the same or similar intent. This suggests the user did not give up on their goal—they gave up on that conversation and tried again. High session restart rates indicate that users blame the specific conversation, not the product. This is recoverable. You can improve session restart outcomes by detecting when a user is repeating an intent and routing them to a different conversational strategy or a human agent immediately.

Another signal is escalation after abandonment. A user abandons the AI interaction, then contacts support through a different channel—phone, email, chat with a human. This is the worst outcome. The user invested time in the AI interaction, got no value, and now costs you additional support expense. Tracking escalation-after-abandonment rate shows the true cost of task completion regression. It is not just lost conversion—it is increased operational cost.

Task completion regression and user satisfaction tracking are two sides of the same outcome. Task completion measures whether the model helped the user achieve their goal. User satisfaction measures whether the user felt good about the experience. Both must be monitored, both must be protected, and both must be tested before every release—starting with the satisfaction signals that reveal what users really think about your model's behavior.

---

*Next: 10.4 — User Satisfaction Signal Tracking*
