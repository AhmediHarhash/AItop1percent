# 1.2 â€” How AI Regression Differs from Software Regression

Traditional software regression testing is deterministic. You have a function that takes input A and returns output B. You write a test that calls the function with input A and asserts that the output equals B. The test passes or fails. Binary. Exact. Reproducible. You change the code, run the test again, and if the output is still B, you have not regressed. If the output is C, you have regressed. The detection is automatic, the threshold is zero tolerance, and the fix is clear: revert the change or fix the code until the test passes again. This model does not work for AI systems.

AI systems are probabilistic. You send the same input twice and get two different outputs. You cannot assert exact equality because exact equality does not exist. You cannot write a test that says "given this input, the model must produce this exact output" because the model will produce a different output on the next run. You cannot rely on binary pass-fail because most outputs fall into a gray zone: not perfect, not broken, somewhere in between. The detection is statistical, the threshold is negotiable, and the fix is often a trade-off rather than a correction. This fundamental difference breaks every assumption that traditional regression testing makes.

## The Problem of Nondeterminism

The first and most disruptive difference is nondeterminism. Set the temperature to 0.7, send the same prompt twice, and you will get two different responses. Sometimes the responses are semantically identical but syntactically different. Sometimes they are both correct but take different approaches. Sometimes one is correct and the other is not. This is not a bug. This is how the model works. The sampling process is stochastic by design. You cannot eliminate the variance without setting temperature to zero, and even at temperature zero, different model versions, different infrastructure, or different random seeds can produce different outputs.

In traditional software, nondeterminism is a bug. If a function returns different outputs for the same input, you file a ticket and fix it. In AI systems, nondeterminism is a feature. It makes the model more creative, more diverse, more human-like. But it also makes regression testing vastly more complex. You cannot compare outputs for exact equality. You cannot assert that the model will always produce a specific string. You need a way to measure whether the distribution of outputs has shifted, not whether a single output matches.

This requires a mental model shift. You are not testing whether the model produces the correct output on a single example. You are testing whether the model produces correct outputs at the same rate as baseline across a large set of examples. If your baseline model was correct 91 percent of the time and your new model is correct 89 percent of the time, you have a regression. But you cannot detect that regression by running three examples and eyeballing the results. You need hundreds or thousands of examples. You need statistical confidence. You need to distinguish between real regression and random variance.

## The Problem of Output Spaces

The second difference is the size and complexity of output spaces. In traditional software, the set of possible outputs is usually small and enumerable. A function might return true or false. A method might return an integer between zero and one hundred. A REST API might return one of twelve predefined error codes. You can write tests that cover every possible output or at least every meaningful category of output. In AI systems, the output space is effectively infinite. A model that generates text can produce any sequence of tokens. A model that generates embeddings can produce any point in a high-dimensional vector space. You cannot enumerate the possibilities. You cannot write a test case for every potential output.

This makes regression detection a classification problem rather than a comparison problem. You are not asking "did the model produce the same output as baseline?" You are asking "is the new output in the same category of correctness as the baseline output?" If the baseline output was correct, is the new output also correct? If the baseline output was incorrect, is the new output also incorrect, or did the model fix a previous failure? This requires a judge. The judge might be a human reviewer, a rubric-based classifier, an LLM evaluator, or a deterministic function that checks structural properties. But you cannot escape the need for judgment. Exact comparison does not work when outputs are open-ended.

The challenge is that judgment is expensive. If you have ten thousand examples in your regression suite and you need a human to judge whether each new output is correct, you have just committed ten thousand human judgments to every release candidate. At one minute per judgment, that is 166 hours of work. If you pay reviewers 25 dollars per hour, that is 4,150 dollars per release. If you ship weekly, that is 215,800 dollars per year just for regression testing. This is why most teams automate judgment using LLM evaluators or heuristic classifiers. But automation introduces its own risks. The evaluator might miss regressions that a human would catch. The evaluator might flag false positives that waste engineering time. You are trading cost for accuracy, and you need to know where the break-even point is.

## The Problem of Noise Floors

The third difference is the existence of noise floors. In traditional software, if a test fails, you have a problem. The failure is signal. In AI systems, if a model produces a bad output on one example, you might have a problem, or you might just be seeing random variance. The model has a baseline error rate. If your baseline model is correct 91 percent of the time, it is incorrect 9 percent of the time. Those incorrect outputs are noise. They do not indicate a bug. They indicate the limits of the model's capabilities given the data, architecture, and training process. When you ship a new model, you expect to see some incorrect outputs. The question is: are you seeing more incorrect outputs than baseline?

This is the noise floor. It is the rate of errors you expect to see even when the model is performing as intended. To detect regression, you need to distinguish between variance within the noise floor and true degradation. If your baseline model was correct on 910 out of 1,000 examples and your new model is correct on 905 out of 1,000 examples, is that a regression or random variance? You need statistical significance testing. You need confidence intervals. You need to understand the distribution of error rates across multiple runs. This is not something traditional regression testing prepares you for.

The noise floor varies by task, by domain, and by model architecture. A model that classifies intents might have a noise floor of 5 percent. A model that generates creative content might have a noise floor of 20 percent because "correctness" is subjective. A model that answers factual questions might have a noise floor of 2 percent. You cannot set a universal regression threshold. You need to measure the noise floor for your specific system, on your specific task, with your specific eval set. Then you set regression thresholds relative to that noise floor. A 2 percent drop in accuracy might be significant if your noise floor is 1 percent. It might be insignificant if your noise floor is 5 percent.

## From Exact Matching to Distribution Matching

In traditional regression testing, you compare outputs for exact equality. In AI regression testing, you compare output distributions. The baseline model produces a distribution of outputs. The new model produces a distribution of outputs. Regression occurs when the new distribution is measurably worse than the baseline distribution according to the dimensions you care about. This is a fundamentally different mental model.

Distribution matching requires aggregate metrics. You cannot look at a single example and declare success or failure. You need to look at hundreds or thousands of examples and calculate summary statistics: mean accuracy, median latency, 95th percentile token cost, variance in response length, rate of safety violations. These metrics describe the shape of the distribution. When you ship a new model, you recalculate the metrics and compare them to baseline. If accuracy drops by 2 percent, latency increases by 50 milliseconds, cost rises by 0.003 dollars per request, and variance in response length doubles, you have four measurable regressions. The question is whether any of them exceed your acceptable thresholds.

Distribution matching also requires sample size discipline. If you test on ten examples, random variance dominates. You cannot distinguish between a real 2 percent drop in accuracy and random fluctuation. If you test on one thousand examples, you can detect a 2 percent drop with high confidence. If you test on ten thousand examples, you can detect a 0.5 percent drop. The larger your sample size, the smaller the regressions you can detect. But larger sample sizes cost more to generate, annotate, and evaluate. You are balancing sensitivity against cost. Most teams find that 500 to 2,000 examples per regression suite provides a reasonable balance: enough to detect meaningful regressions, not so many that the eval takes hours to run or costs thousands of dollars.

## The Role of Statistical Thresholds

Because outputs are probabilistic, you cannot use binary pass-fail thresholds. You need statistical thresholds. A statistical threshold defines the minimum size of a regression you care about and the confidence level you require before declaring that regression real. For example, you might say: if accuracy drops by more than 1.5 percent with 95 percent confidence, that is a regression. If accuracy drops by 0.8 percent, that might be noise, and you accept it. If accuracy drops by 1.2 percent but the confidence interval spans zero, you run more examples to increase confidence.

Statistical thresholds vary by stakeholder. Engineering might accept a 2 percent latency increase if it comes with a 3 percent accuracy gain. Finance might reject a 10 percent cost increase regardless of accuracy gains. Trust and Safety might reject any increase in toxicity rate above 0.1 percent. You need to negotiate these thresholds with stakeholders before you run your first regression test. Otherwise, you will find yourself in endless debates about whether a 1.4 percent drop in accuracy is acceptable or catastrophic. The threshold should be decided based on user impact, business priorities, and regulatory constraints, not based on post-hoc rationalization of whatever number you happened to observe.

The discipline here is to document your thresholds, test against them automatically, and enforce them as release gates. If your threshold for accuracy regression is 1.5 percent and your new model drops accuracy by 2.1 percent, the model does not ship. No exceptions, no overrides, no "but it is better in other ways" arguments. The threshold exists to protect you from making bad trade-offs under deadline pressure. If you override the threshold, you undermine the entire system. If the threshold is wrong, change the threshold through a formal stakeholder review process. Do not change it on a Friday afternoon because you want to ship before the weekend.

## Why Traditional Testing Discipline Still Matters

Everything about AI regression testing is harder, slower, and more expensive than traditional regression testing. But the discipline is the same. You define what correct looks like. You measure whether you have maintained correctness. You block releases that fail the test. You automate the process so it runs on every change. You treat regression detection as a first-class part of your development workflow, not as an optional step you skip when timelines are tight.

The teams that succeed at AI regression testing are the teams that bring traditional testing discipline to a probabilistic domain. They write eval suites. They maintain baselines. They run tests before shipping. They track metrics over time. They escalate when thresholds are breached. They roll back when production behavior diverges from tested behavior. The difference is that they do all of this with statistical thresholds, distribution comparisons, and aggregate metrics instead of exact matching and binary pass-fail. The mechanics change. The discipline does not.

The teams that fail at AI regression testing are the teams that treat nondeterminism as an excuse to skip testing. They say "the model is probabilistic, so we cannot test it" and ship changes based on intuition. They say "we ran it on three examples and it looked good" and call that validation. They say "users will tell us if something breaks" and treat production as their test environment. These teams ship regressions constantly. They spend half their time rolling back changes, apologizing to users, and debugging issues that could have been caught with a proper regression suite.

The lesson from traditional software testing is this: you cannot ship quality without measurement, and you cannot maintain quality without regression detection. That lesson does not change when you move from deterministic code to probabilistic models. The implementation details change. The principle does not.

The next subchapter covers the baseline challenge: how to define, capture, and maintain the validated baseline that all regression testing depends on.
