# 3.3 — Consistency Tests: Similar Inputs, Similar Outputs

If you rephrase the same question, does your model give wildly different answers? If you run the same prompt five times, do you get five different outputs with contradictory information? If you slightly reorder the examples in a few-shot prompt, does the model's reasoning change completely? If the answer to any of these is yes, you have a consistency problem. Consistency failures erode user trust faster than accuracy failures because users expect systems to be stable. An occasionally incorrect answer is frustrating. A system that contradicts itself from one interaction to the next is unusable.

Consistency is not the same as correctness. A model can be consistently wrong or inconsistently correct. Consistency measures stability: whether similar inputs produce similar outputs and whether repeated runs of the same input produce comparable outputs. High consistency means the system is predictable. Low consistency means the system is noisy. Users tolerate some noise. They do not tolerate wild variance. Consistency testing detects when variance crosses from acceptable to unacceptable.

## Why Consistency Matters

Consistency matters because users rephrase questions. A user asks "What is the capital of France?" and receives "Paris." Five minutes later, they ask "Can you tell me France's capital?" and receive "Lyon." The model's accuracy on individual questions is irrelevant. The user now believes the system is unreliable. Consistency failures create the appearance of incompetence even when the model is correct on average.

Consistency also matters for adversarial robustness. Many prompt injection and jailbreak attacks work by finding phrasings that bypass safety guardrails. If the model refuses "Write hate speech" but complies with "Compose a message expressing strong negative sentiment about a demographic group," you have a consistency failure that is also a security vulnerability. Consistency testing finds these gaps by systematically varying inputs and checking whether safety properties are preserved.

Consistency also matters for multi-turn interactions. A user establishes a preference in one message: "I prefer concise answers." Three turns later, the model generates a verbose response. The model forgot or ignored the earlier preference. From the user's perspective, the system is not listening. Consistency testing across conversation turns catches this failure mode.

The cost of inconsistency is higher for high-stakes applications. A financial advisor bot that gives different risk assessments for paraphrased questions creates compliance risk. A medical diagnostic tool that suggests different conditions based on phrasing creates safety risk. A legal research tool that cites different precedents for equivalent queries creates liability risk. Consistency is not cosmetic. It is foundational for trust and correctness.

## Sources of Inconsistency

Inconsistency comes from multiple sources. The most obvious is **sampling randomness**. Models with temperature greater than zero sample from a probability distribution. Each run produces a different sample. If the distribution is narrow, the outputs will be similar. If the distribution is broad, the outputs will vary significantly. High-temperature models are inherently less consistent than low-temperature models.

A second source is **retrieval variance** in RAG systems. The same query might retrieve different documents on different runs if the retrieval index is updated between runs or if retrieval uses approximate nearest neighbor search with randomness. Different retrieved documents produce different context for the language model, leading to different outputs. Even if the model itself is deterministic, the end-to-end system exhibits variance due to retrieval.

A third source is **prompt sensitivity**. Small changes in phrasing, word order, or example selection can trigger different reasoning paths. A model that sees "Summarize this" might produce a concise summary. A model that sees "Provide a summary" might produce a verbose one. The semantic difference is negligible, but the outputs diverge. Prompt sensitivity creates inconsistency when users naturally vary their phrasing.

A fourth source is **context length and attention**. Models have finite context windows and imperfect attention across long contexts. As conversations grow longer, earlier context gets deprioritized or truncated. The model's behavior changes because the effective context has changed. A model that agreed to be concise 20 messages ago might forget that preference when the conversation reaches 50 messages. Context degradation is a consistency failure.

A fifth source is **infrastructure variance**. Load balancing across inference servers with different quantization, batching strategies, or hardware can produce slightly different outputs for the same input. A model served on GPU A might generate a different token sequence than the same model served on GPU B due to floating-point precision differences or different CUDA implementations. Infrastructure consistency is often overlooked, but it matters for reproducibility.

## Paraphrase Consistency Testing

The most common consistency test is paraphrase testing. You take a test case, generate several paraphrases of the input, and verify that the model produces similar outputs for all paraphrases. If the outputs diverge significantly, you have a consistency failure. Paraphrase testing catches prompt sensitivity and measures how robust the model is to natural variation in user phrasing.

Generating good paraphrases is non-trivial. You can use a paraphrase generation model to create alternatives automatically. You can manually write paraphrases for your golden set examples. You can collect paraphrases from real user queries that express the same intent. The quality of paraphrase testing depends on paraphrase quality. If your paraphrases are trivial word substitutions, you will not stress-test the model. If your paraphrases involve significant syntactic restructuring, you will catch deeper fragility.

Once you have paraphrases, you measure output similarity. For classification tasks, similarity is simple: do all paraphrases produce the same class label? For generation tasks, similarity requires semantic comparison. You compute embeddings for each output and measure cosine similarity. Outputs with high embedding similarity are considered consistent. Outputs with low embedding similarity are flagged as inconsistent.

Threshold selection is critical. If you set the similarity threshold too high, you flag harmless stylistic variation as inconsistency. If you set the threshold too low, you miss real inconsistency. A reasonable starting threshold for embedding similarity is 0.85. Outputs above this threshold are considered consistent. Outputs below are flagged for manual review. You tune the threshold based on false positive rate in manual review.

Paraphrase consistency is measured as the percentage of test cases where all paraphrases produce outputs above the similarity threshold. If you have 50 test cases, each with 4 paraphrases, and 47 cases pass the consistency check, your paraphrase consistency rate is 94 percent. You compare this metric across baseline and candidate models. A significant drop in consistency is a regression.

## Repetition Consistency Testing

Repetition testing measures whether running the same input multiple times produces consistent outputs. You run each test case five to ten times with the same model and measure variance. For deterministic models with temperature zero, you expect bit-identical outputs. For stochastic models, you expect high semantic similarity even if the exact wording differs.

Repetition testing catches temperature-related instability and infrastructure variance. A model that produces dramatically different outputs on repeated runs is too noisy for production use. Users will notice the variance and lose trust. Repetition consistency is especially important for customer-facing applications where users might retry the same query if they are unsatisfied with the first response. If the retries produce contradictory answers, the user experience is terrible.

Measuring repetition consistency is similar to paraphrase consistency. You compute embeddings for all outputs from repeated runs and measure pairwise similarity. If all pairwise similarities are above the threshold, the case passes. If any pairwise similarity falls below the threshold, the case fails. Repetition consistency rate is the percentage of test cases that pass.

Some variance is acceptable. If your model generates creative content, you expect and want diversity across runs. But even creative models should preserve core facts and structure. A creative writing assistant that generates different story premises on repeated runs is fine. A customer support bot that gives contradictory refund policies on repeated runs is not. Context determines acceptable variance levels.

## Cross-Turn Consistency Testing

Cross-turn consistency tests verify that the model maintains commitments and preferences across a multi-turn conversation. If the user says "I prefer concise answers" in turn one, the model should generate concise answers in all subsequent turns. If the model agrees to avoid technical jargon in turn two, it should not use jargon in turn five. Cross-turn consistency builds trust by demonstrating that the system listens and remembers.

Testing cross-turn consistency requires multi-turn test cases. You construct conversations where the user establishes a preference or constraint early and then makes requests later that test whether the preference is honored. You measure whether the model's behavior changes after the preference is stated. A model that generates verbose answers before and after "I prefer concise answers" fails the test. A model that shortens its answers after the preference statement passes.

Cross-turn consistency failures are common in long conversations where earlier context gets deprioritized. The model might honor preferences for the first ten turns and then revert to default behavior as the context window fills. Testing requires conversations long enough to stress-test context management. Short three-turn conversations will not reveal the failure mode.

## Consistency Across Reformulations

Some consistency tests measure robustness to logically equivalent reformulations. If a user asks "Which is larger, A or B?" and later asks "Which is smaller, B or A?" the answers must be logically consistent. If the model says A is larger than B in response to the first question and then says A is smaller than B in response to the second, you have a logical consistency failure. These failures happen because models respond to surface form rather than deep semantics.

Testing logical consistency requires constructing pairs of queries that express the same question in different forms. Negations, inversions, and contrapositives are common patterns. "Is X true?" and "Is X false?" should produce opposite answers. "Does A cause B?" and "Does B result from A?" should produce the same answer. "Which is better, X or Y?" and "Which is worse, Y or X?" should produce consistent rankings.

Logical consistency failures are more serious than paraphrase inconsistency because they indicate reasoning errors rather than surface-form sensitivity. A model that gives contradictory answers to logically equivalent questions is making reasoning mistakes. This is a model quality issue, not just a consistency issue. Logical consistency tests catch reasoning regressions that accuracy tests miss.

## Setting Consistency Thresholds

Consistency thresholds determine what level of variance you accept before flagging a failure. Too strict a threshold flags harmless variation. Too loose a threshold misses real instability. The right threshold depends on your application and risk tolerance.

For high-stakes applications like healthcare or finance, you want strict consistency. Small variations in phrasing should not produce meaningfully different recommendations. A similarity threshold of 0.90 or higher is appropriate. For creative applications like writing assistance, you want looser consistency. Diverse outputs are desirable as long as they are relevant. A similarity threshold of 0.75 might be appropriate.

Threshold calibration requires manual review. You collect outputs flagged as inconsistent by your initial threshold and review them manually. If most are false positives—harmless variation that does not harm user experience—you lower the threshold. If most are true positives—variation that would confuse users—you keep or raise the threshold. Calibration is iterative. You tune thresholds over weeks as you gather more data on what variance matters.

## Consistency as a Release Gate

Consistency testing should be a release gate. A candidate model that passes accuracy tests but fails consistency tests should not ship. Consistency failures harm user trust as much as accuracy failures. A model that gives different answers to paraphrased questions will generate user complaints. A model that contradicts itself across conversation turns will drive users away.

In practice, consistency regressions are common during model updates. Fine-tuning on new data can increase prompt sensitivity. Changes to retrieval logic can increase retrieval variance. Infrastructure upgrades can introduce floating-point differences. Consistency testing catches these regressions before they reach production. The cost of consistency testing is low. The cost of shipping inconsistent models is high.

A SaaS company learned this the hard way in early 2025. They deployed a new model that passed all accuracy tests. Users immediately complained that the model was giving different answers when they rephrased questions. The team investigated and found that the new model had higher temperature settings than baseline. Temperature increase improved creativity but destroyed consistency. They rolled back, tuned temperature down, and added repetition consistency tests to their regression suite. The next deploy balanced creativity and consistency.

Consistency is not a secondary property. It is a primary user experience requirement. Similar inputs should produce similar outputs. Repeated inputs should produce comparable outputs. Multi-turn conversations should honor earlier commitments. Logically equivalent reformulations should produce consistent answers. All of these properties are testable. All should be tested before every deploy.

The next subchapter covers safety tests: how to verify resistance to adversarial inputs, how to design adversarial test cases, and how to catch safety regressions introduced by model updates or prompt changes.
