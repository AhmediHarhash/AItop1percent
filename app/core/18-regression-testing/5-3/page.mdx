# 5.3 â€” Trigger Strategies: When to Run Which Tests

In late 2025, a fintech company ran their full regression suite on every commit. The suite included 2,400 test cases covering RAG retrieval, multi-step reasoning, and financial compliance validation. Each run cost $180 in model API fees and took 22 minutes. The team averaged 140 commits per day across twelve engineers. Within three weeks, their CI bill hit $86,000. The VP of Engineering demanded an explanation. The team had no answer beyond "we wanted to catch regressions early." They had conflated thoroughness with intelligence. Every test was important, but not every test needed to run on every change.

The trigger decision is the most consequential cost-quality tradeoff in AI testing infrastructure. Run too many tests too often and you burn budget without proportional safety gains. Run too few tests too rarely and regressions slip through. The right trigger strategy matches test scope to change risk. A one-line prompt adjustment does not require the same validation as a model swap. A documentation update does not warrant the same scrutiny as a retrieval algorithm change. Your CI system should enforce this hierarchy automatically.

## The Trigger Decision Matrix

Every code change carries a different regression risk profile. A modification to your RAG chunking logic can break retrieval across your entire application. A typo fix in a README file cannot. The trigger strategy encodes this risk assessment into automated rules that determine which tests run when.

**Code changes to core inference paths** trigger the full regression suite. If someone modifies your prompt construction logic, your model router, your tool-calling orchestration, or your output parser, every downstream behavior is potentially affected. These changes require comprehensive validation before merge. The cost is justified by the blast radius.

**Prompt template changes** trigger task-specific regression tests plus baseline quality checks. If your customer service prompt changes, you need to validate customer service scenarios and ensure the model still handles edge cases correctly. But you do not need to re-run your legal document analysis tests unless those prompts share template components. Scope the validation to what actually changed.

**Configuration changes** trigger targeted validation based on what the config controls. If you adjust temperature from 0.7 to 0.9, run tests that validate output diversity and consistency. If you change a retrieval threshold, run RAG evaluation tests. If you modify a feature flag, run tests for the features it gates. Configuration often changes more frequently than code, so efficient triggering here saves substantial cost.

**Data changes** trigger domain-specific regression tests. If you update your knowledge base, run retrieval and generation tests against queries that should surface the new information. If you add synthetic training examples, run tests that validate the distribution shift did not introduce bias or capability loss. Data changes often fly under the radar because they do not touch code, but they affect model behavior just as directly.

**Dependency updates** trigger smoke tests first, then escalate based on results. If you upgrade your model API client library, run a smoke test to verify basic functionality. If it passes, proceed with normal development. If it fails, escalate to full regression before allowing the update to merge. Most dependency updates are safe. The few that are not need to fail fast.

The matrix should be encoded in your CI configuration as path-based triggers. When a pull request includes changes to files matching `src/inference/**`, run the full suite. When it touches `prompts/**`, run prompt regression tests. When it only modifies `docs/**` or `README.md`, skip model-based tests entirely. Most CI systems support this natively through path filters.

## The Escalation Pattern

The most effective trigger strategy uses escalation: tests get more comprehensive as code moves closer to production. This balances fast feedback during development with thorough validation before release.

**Smoke tests run on every commit**. These are fast, cheap tests that validate basic functionality. Can the model generate a response? Does the API return a 200 status? Does the output parse correctly? Smoke tests typically complete in under two minutes and cost less than one dollar. They catch catastrophic regressions immediately. If smoke tests fail, the developer knows within seconds that their change broke something fundamental.

**Regression tests run on pull request creation**. When a developer opens a PR, the CI system runs the full regression suite scoped to the affected components. For a prompt change, this means running all task-specific tests for that prompt. For a retrieval change, this means running the RAG evaluation suite. For a model swap, this means running everything. Regression tests take 10 to 30 minutes and cost 10 to 50 dollars depending on scope. They provide the evidence needed for code review.

**Golden set tests run on merge to main**. Once a PR is approved and merged, the CI system runs your golden set against the updated main branch. The golden set is your highest-confidence test collection, typically 200 to 500 cases that represent your most critical scenarios. Golden set tests take 15 to 45 minutes and cost 30 to 100 dollars. They serve as the final gate before the change becomes part of your stable codebase.

**Release validation tests run before deploy**. When you cut a release candidate, run the golden set again plus any additional safety tests required for production. This might include adversarial tests, bias audits, or compliance validation. Release validation is the most expensive test run, sometimes costing several hundred dollars, but it is the last opportunity to catch regressions before users see them.

The escalation pattern ensures that every change passes progressively stricter validation as it moves through your development pipeline. Early stages prioritize speed to keep developers productive. Later stages prioritize thoroughness to protect production quality.

## Path-Based Triggers

Path-based triggering is the most precise way to scope tests to relevant changes. Your CI configuration specifies which file paths, when modified, should trigger which test suites.

A typical configuration might look like this structure in concept: when files under the prompts directory change, run the prompt regression suite. When files under the retrieval module change, run RAG evaluation tests. When files under the model router change, run routing accuracy tests. When files under tests or docs change, skip model-based tests entirely.

The key is maintaining accurate path mappings as your codebase evolves. If you refactor your prompt management system from a single file to a directory structure, update your trigger configuration accordingly. If you consolidate multiple retrieval implementations into a unified module, adjust which tests run when that module changes.

Path-based triggers prevent waste. A documentation update should not cost you 80 dollars in eval runs. A unit test fix should not trigger 30 minutes of model inference. The CI system should be smart enough to know the difference.

Some changes touch multiple paths and should trigger the union of relevant test suites. If a pull request modifies both prompt templates and retrieval logic, run both prompt regression and RAG evaluation tests. If it changes the model router and updates several prompts, run routing tests and prompt tests. Most CI systems handle this automatically when you define triggers with path filters.

## Time-Based Triggers

Not all regression testing should be event-driven. Some tests are too expensive to run on every PR but too important to skip. Time-based triggers solve this by running comprehensive test suites on a schedule regardless of code changes.

**Nightly regression runs** execute your full test suite against the current main branch every night. This catches regressions that slipped through PR-level testing, validates that cumulative changes across multiple PRs did not interact poorly, and provides a daily quality baseline. Nightly runs typically cost 200 to 500 dollars but give you high confidence that your main branch is production-ready at the start of each workday.

**Weekly golden set validation** runs your most critical tests against production-like data. This might include running your golden set against production traffic patterns, validating model behavior on recent user queries, or testing against updated knowledge base snapshots. Weekly validation ensures your regression tests stay aligned with reality.

**Monthly comprehensive audits** run every test you have, including expensive adversarial evaluations, bias audits, and edge case testing that is too slow for regular CI. Monthly audits provide periodic deep validation that your system still meets all quality requirements.

Time-based triggers also serve as a safety net. If someone bypasses CI, merges without approval, or introduces a configuration change outside version control, the next scheduled run will catch it. You should treat time-based test failures with the same urgency as PR failures. They indicate that your main branch is not in a known-good state.

## Manual Triggers

Automated triggers handle 95 percent of regression testing, but humans still need the ability to run tests on demand. Manual triggers are essential for investigation, validation, and pre-release confidence.

**On-demand full regression** lets any team member run the complete test suite against any branch at any time. This is critical when investigating a production incident, validating a complex refactor before opening a PR, or getting fast feedback on a prototype. Manual triggers should be available through your CI interface, command-line tools, and deployment dashboards.

**Targeted test execution** allows running specific test suites or individual test cases. If you are debugging a retrieval failure, you should be able to run just the RAG evaluation suite without waiting for 2,000 unrelated tests to complete. If you are validating a prompt change, you should be able to run just the tests for that prompt. Manual targeting gives developers precision when they need it.

**Cross-branch validation** runs regression tests comparing two branches against each other. This is useful when evaluating whether a major refactor maintained behavioral parity, when deciding between two implementation approaches, or when validating that a hotfix did not introduce side effects. Cross-branch validation produces a diff report showing which test cases changed behavior between branches.

Manual triggers should have the same access control and audit logging as automated runs. You do not want a junior engineer accidentally triggering 50 full regression runs and costing the company 10,000 dollars. You do want visibility into who ran what tests when, especially when investigating why a regression reached production.

## Avoiding Over-Triggering

Over-triggering is the most common and most expensive trigger strategy mistake. Teams configure CI to run everything on every change out of fear. The result is slow feedback loops, high costs, and developer frustration.

The symptoms of over-triggering are obvious. Pull requests take 45 minutes to validate when most changes only affect one component. Your CI bill grows faster than your team size. Developers start merging code without waiting for tests to complete because they cannot afford the time. The regression suite becomes something the team works around instead of something they trust.

The fix requires discipline. Start by auditing your current trigger configuration. For each test suite, identify which file paths should actually trigger it. If your RAG evaluation suite runs when someone updates the authentication module, that is over-triggering. If your prompt regression tests run when someone changes the deployment configuration, that is over-triggering.

Establish a cost budget for CI testing and instrument your pipeline to track spending per test suite, per trigger type, and per team member. When you see that 60 percent of your CI budget goes to running full regression on documentation changes, you have the data needed to justify fixing it.

Implement test suite dependencies. If your prompt regression tests already cover certain scenarios, do not also run those same scenarios in your end-to-end tests unless the end-to-end context adds meaningful validation. Redundant testing burns money without improving quality.

## Avoiding Under-Triggering

Under-triggering is harder to detect but just as dangerous. Tests that do not run often enough fail to catch regressions until they reach production. The symptoms appear as user-reported bugs, production incidents, and post-deployment rollbacks.

The most common under-triggering mistake is treating configuration changes as low-risk. A temperature adjustment from 0.7 to 1.2 can dramatically change model behavior, but many teams do not trigger regression tests when configuration files change. A feature flag flip can route traffic to an untested code path, but teams often do not validate flag changes the same way they validate code changes.

Data changes also suffer from under-triggering. When you update your knowledge base, add synthetic examples, or refresh your retrieval corpus, those changes affect model behavior just as much as prompt changes. But many CI systems do not monitor data files or trigger tests when data changes. The result is regressions that appear to come from nowhere.

The fix is comprehensive trigger coverage. Every file that affects model behavior should be in your CI trigger configuration. Prompt templates, configuration files, data manifests, feature flags, model router rules, tool definitions, and output parsers all need triggers. If modifying a file can change what your AI system does, modifying that file should trigger tests.

Audit your production incidents to identify under-triggering gaps. If you shipped a regression that existing tests could have caught, ask why those tests did not run. Often the answer is that the change did not match any trigger rule. Add the rule, document why it matters, and move on.

## Trigger Configuration as Code

Your trigger strategy should be version-controlled, reviewed, and tested just like application code. Hard-coding trigger rules in a CI dashboard makes them invisible, unreviewed, and fragile. Storing them in configuration files in your repository makes them explicit, auditable, and refactorable.

Most CI systems support defining triggers in YAML configuration files committed to your repository. These files specify which paths trigger which workflows, what conditions must be met for tests to run, and how test results gate merges. When triggers are code, changing them requires a pull request, which means code review, which means someone else validates that your trigger logic makes sense.

Version-controlling triggers also creates a history. When you investigate why a regression reached production, you can trace back through trigger configuration changes to see if someone inadvertently disabled a critical test. When you optimize CI costs, you can compare trigger configurations before and after to quantify the impact.

Test your triggers the same way you test application code. Add a test case that verifies modifying a prompt file actually triggers prompt regression tests. Add a test that verifies documentation changes do not trigger model-based tests. These meta-tests catch trigger configuration bugs before they cause outages.

The next subchapter covers branch-based testing strategies, where different branches enforce different quality gates based on their role in your development workflow.
