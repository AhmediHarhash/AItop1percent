# 3.5 — Performance Tests: Latency, Throughput, Cost

In March 2025, a customer service AI platform upgraded from GPT-4o to GPT-5.1 and immediately deployed to production. The quality metrics looked good. Accuracy improved by 4 percentage points. Refusal rates stayed stable. User satisfaction scores were neutral. Then the monthly cloud bill arrived. Inference costs had increased by 340 percent. The team investigated. GPT-5.1 was generating 60 percent more tokens per response than GPT-4o for the same queries. The model was more verbose. Users did not notice. The CFO did. The team rolled back the upgrade and spent two weeks fine-tuning output length before trying again. They had tested quality. They had not tested cost.

Performance is a regression surface. Latency, throughput, and cost are not constants. They shift with every model update, every prompt change, every inference parameter tweak. A model that responded in 800 milliseconds can start responding in 2.1 seconds. A system that handled 500 requests per minute can drop to 320. A deployment that cost 4 cents per query can jump to 11 cents. If your regression suite does not measure performance, you discover these regressions in production, after users complain or after the budget is blown. By then, the damage is done.

## Latency Percentiles: Why Averages Lie

Most teams track average latency. Baseline: 1.2 seconds. New version: 1.3 seconds. The regression seems minor. But averages hide distribution. If 95 percent of requests complete in 900 milliseconds and 5 percent complete in 18 seconds, the average is still around 1.2 seconds. Users do not experience the average. They experience the distribution. The 5 percent who wait 18 seconds perceive the system as broken. If a new model version shifts that tail, user experience degrades even if the average barely moves.

**Latency percentile tracking** measures response time at multiple points in the distribution. P50 is the median—half of requests are faster, half are slower. P95 means 95 percent of requests are faster than this threshold. P99 means 99 percent are faster. The tail percentiles—p95, p99, p99.9—reveal the outliers that average latency hides. A model upgrade that keeps p50 stable but doubles p99 latency is a regression. Most users see no difference. A small minority experiences catastrophic slowdowns. If those users are high-value enterprise customers or appear in public-facing demos, the regression is a crisis.

Your baseline must capture all percentiles. Run your existing model on a representative query set—ideally production traffic samples or a synthetic test set that mirrors production distribution. Record p50, p75, p90, p95, p99, and p99.9 latency. These become your regression thresholds. When you test a new model version, you run the same query set and compare percentiles. If p99 increases by more than 20 percent, you investigate. If p99.9 increases by more than 30 percent, you block the release. The thresholds are context-dependent. A backend batch processing system can tolerate higher tail latency than a real-time voice assistant. But every system has a threshold. If you do not measure it, you cannot enforce it.

The most common cause of latency regression is **token generation slowdown**. A new model version generates tokens at a slower rate, either because the model architecture changed or because inference optimizations broke. If your previous model generated 40 tokens per second and your new model generates 28 tokens per second, every response takes longer. The effect is proportional to output length. Short responses are barely affected. Long responses become noticeably slower. This shows up in the percentiles. P50 moves slightly. P95 and p99 jump significantly because those represent longer responses.

The second cause is **time-to-first-token regression**. This is the delay between sending a request and receiving the first token of the response. For streaming responses, time-to-first-token determines how quickly the user sees the model start working. If it increases from 400 milliseconds to 1.1 seconds, the system feels unresponsive even if total latency stays the same. Users perceive lag. Some abandon the request before the response starts. Time-to-first-token regression is especially damaging in conversational interfaces where responsiveness signals that the system understood the input. A delay feels like confusion.

Your performance regression suite must measure both. Total latency percentiles and time-to-first-token percentiles. If either regresses, you investigate. If both regress, you do not ship.

## Throughput Regression: How Many Queries Survive

Throughput is how many queries your system processes per unit of time under load. Baseline: 500 requests per minute at 90 percent success rate. A new model version drops to 380 requests per minute at 85 percent success rate. This is throughput regression. Fewer successful queries, more failures, longer queues. Your users experience timeouts, retries, and degraded service. Your infrastructure costs increase because you need more instances to handle the same load. Throughput regression is often invisible in small-scale testing. You test a single query and it works. You test ten queries and they work. You test 500 concurrent queries and 20 percent fail.

**Load testing for AI systems** requires simulating production-scale request volume against the new model version. The goal is not to test whether the model can handle a single request. The goal is to test whether it can handle realistic concurrent load without degradation. You need a test harness that sends hundreds or thousands of requests per minute, measures success rates, latency under load, and resource utilization. If throughput drops, you investigate. If success rates drop, you block the release.

The most common cause of throughput regression is **increased inference time per request**. A model that takes twice as long to respond can only handle half the throughput on the same infrastructure. If your baseline model completes requests in 1.2 seconds and your new model completes them in 2.4 seconds, your throughput just halved. You either accept lower throughput or double your infrastructure spend. Neither is acceptable. If the latency increase is from improved quality, you have a trade-off to evaluate. If the latency increase is from an unintended change, you have a regression to fix.

The second cause is **memory pressure and resource contention**. Some model updates increase memory consumption. A larger model, a different quantization format, a change in KV-cache management—any of these can push your inference servers closer to memory limits. Under low load, this is fine. Under high load, the servers start swapping, garbage collection increases, and requests start failing or timing out. You do not notice in single-query testing. You notice when production load hits and throughput collapses.

Your regression suite must include load tests. Not every release needs a full stress test. But every release candidate needs a load test that simulates realistic concurrency. If throughput drops by more than 10 percent, you investigate. If throughput drops by more than 25 percent, you block the release. Throughput regression is a production reliability failure waiting to happen. Catch it before deployment, not after.

## Token Count Regression as a Cost Proxy

Model inference costs are usually billed per token—input tokens and output tokens. If a new model version generates more tokens for the same query, your costs increase proportionally. A model that averaged 180 tokens per response now averages 290 tokens. You just increased costs by 61 percent. Users may not notice. The CFO will. If your regression suite does not track token counts, you discover cost regressions when the bill arrives. By then, you have burned through the monthly budget.

**Token count regression testing** tracks input tokens, output tokens, and total tokens per query type. Your baseline is the current model's token distribution across a representative query set. When you test a new model version, you run the same queries and compare token counts. If median output tokens increase by more than 15 percent, you investigate. If p95 output tokens increase by more than 25 percent, you investigate. If total token cost per query increases by more than 20 percent, you block the release unless the quality improvement justifies the cost increase.

The most common cause is **verbosity drift**. The new model is more helpful. It provides longer explanations, more examples, more context. This is often an improvement in isolation. But in aggregate, it is a cost explosion. A customer service bot that previously answered in 40 words now answers in 95 words. Users are slightly more satisfied. The system is three times more expensive per query. At 10 million queries per month, that is the difference between a sustainable business and a financial crisis.

The second cause is **formatting changes**. A new prompt template adds structured output requirements. The model now wraps every response in JSON formatting, includes metadata fields, or adds conversational preambles that were not there before. The user does not need these. The product requirements do not specify them. But the model generates them anyway because the prompt says to. Each response carries 30 extra tokens of formatting overhead. Across millions of queries, this is real money.

Your regression suite must measure token counts at query-type granularity. Short factual queries should not generate 200-token responses. Long analytical queries can justify 600-token responses. But if a query type that previously averaged 85 tokens now averages 140 tokens, you need to understand why. If the increase is intentional and valuable, you budget for it. If the increase is accidental, you fix the prompt or model configuration before deploying.

Some teams set hard token limits per query type. A simple FAQ query cannot exceed 100 tokens. A technical support query cannot exceed 300 tokens. A creative brainstorming query cannot exceed 600 tokens. If a model version exceeds these limits, the release is blocked automatically. This prevents verbosity drift from becoming a cost disaster. The limits are enforced in regression testing, not discovered in production.

## Time-to-First-Token Versus Total Latency

For streaming responses, users perceive two types of latency. Time-to-first-token is how long they wait before seeing anything. Total latency is how long they wait for the complete response. A model that has low total latency but high time-to-first-token feels slower than a model with higher total latency but instant first-token. The user sees nothing happening for two seconds, then the response streams quickly. The perception is that the system is slow, even if total latency is acceptable.

**Time-to-first-token regression** is especially critical in conversational interfaces. A chatbot that previously started responding in 300 milliseconds now takes 1.4 seconds to produce the first token. Users perceive this as lag. Some abandon the conversation. Some refresh the page. Some submit the query again, creating duplicate requests and increasing load. The user experience degrades even if the model is technically correct and total latency is within bounds.

The most common cause of time-to-first-token regression is **prompt bloat**. A new system prompt is longer. The prompt now includes 1,200 tokens of instructions instead of 400 tokens. The model must process all of those input tokens before generating the first output token. Latency increases proportionally. If the additional instructions improve quality, this may be worth the trade-off. But often, the bloat is accidental. Instructions are added over time. No one removes the obsolete ones. The prompt grows to 2,000 tokens. Time-to-first-token climbs. Users notice.

The second cause is **retrieval latency**. For RAG systems, time-to-first-token includes the time to retrieve context from the vector database, rerank results, and construct the augmented prompt. If the retrieval step slows down—because the vector database grew, because reranking logic became more expensive, because network latency increased—time-to-first-token suffers. The model has not changed. The infrastructure changed. But users perceive it as model regression.

Your regression suite must track time-to-first-token separately from total latency. Measure both on every release. If time-to-first-token increases by more than 30 percent, investigate. If it exceeds your user-experience threshold—typically 500 milliseconds for real-time applications, 1 second for async applications—block the release. A system that feels unresponsive will lose users faster than a system that is technically slow but feels instant.

## Setting Performance Baselines

A regression test suite without baselines is meaningless. You measure latency. You measure throughput. You measure token counts. But you have no reference point. Was 1.8 seconds good or bad? Was 340 requests per minute high or low? Was 210 tokens per response acceptable or excessive? Without baselines, you cannot detect regression. You can only measure current performance. The two are not the same.

**Performance baselines** are the reference values for latency percentiles, throughput, token counts, and cost per query from your current production model. You establish baselines by running your existing model on a representative query set under realistic load conditions. The query set must cover all major use cases. The load conditions must reflect actual production concurrency. If production handles 600 requests per minute at peak, your baseline test runs at 600 requests per minute. If production sees bursts of 1,200 requests per minute, your baseline test includes that scenario.

Once you have baselines, you set regression thresholds. Most teams use percentage-based thresholds. Latency p50 can increase by up to 10 percent. Latency p99 can increase by up to 20 percent. Throughput can decrease by up to 10 percent. Token count per query can increase by up to 15 percent. Cost per query can increase by up to 20 percent. These thresholds are configurable. A cost-sensitive product may set tighter cost thresholds. A latency-sensitive product may set tighter latency thresholds. But every product must have thresholds. Without them, regression testing becomes advisory. With them, it becomes a gate.

Some teams use absolute thresholds instead of percentage-based ones. Latency p99 cannot exceed 2 seconds, regardless of baseline. Throughput cannot drop below 400 requests per minute. Token count per query cannot exceed 250 tokens. Absolute thresholds are simpler to enforce and easier to communicate to stakeholders. The downside is that they do not adapt to baseline changes. If you optimize your system and reduce p99 latency from 1.8 seconds to 1.2 seconds, the absolute threshold of 2 seconds becomes too loose. You lose the ability to detect smaller regressions. Percentage-based thresholds adapt automatically.

The hybrid approach is best. Use percentage-based thresholds for detecting regression relative to the previous version. Use absolute thresholds for enforcing user-experience and cost constraints. If latency p99 increases by more than 20 percent, investigate. If latency p99 exceeds 2 seconds, block the release. Both conditions matter. The first catches regression. The second enforces product requirements.

## Load Testing for AI Systems

Load testing for traditional web services is well understood. You simulate concurrent users, measure response times and error rates, identify bottlenecks. Load testing for AI systems is harder because the bottleneck is not the web server or the database. The bottleneck is inference. A single model request consumes GPU memory, compute time, and bandwidth in ways that vary by input length, output length, and model complexity. Load testing must account for all of this.

**AI-specific load testing** requires simulating realistic query patterns under realistic concurrency. You cannot send the same query 10,000 times and call it a load test. Real production traffic has diverse query lengths, diverse output requirements, diverse user behaviors. Your test set must reflect that distribution. Short queries and long queries. Simple questions and complex multi-turn conversations. Queries that retrieve context and queries that do not. The distribution matters because different query types have different performance profiles.

Your load test must measure latency under load. A model that responds in 800 milliseconds at 10 requests per minute may respond in 3.2 seconds at 600 requests per minute. This is not a model problem. This is an infrastructure problem. You do not have enough GPU capacity or inference throughput to handle production load. If you do not load-test before deploying, you discover this when production traffic arrives and users start experiencing timeouts. By then, your options are to scale infrastructure immediately—expensive and slow—or to throttle traffic—bad for users. Both are avoidable with load testing.

The most critical metric is **degradation rate under load**. At 100 requests per minute, p99 latency is 1.1 seconds. At 300 requests per minute, it is 1.6 seconds. At 600 requests per minute, it is 4.8 seconds. This is nonlinear degradation. Throughput increases by 6x, but tail latency increases by 4x. The system is approaching its breaking point. If production traffic spikes to 800 requests per minute, users will experience unacceptable latency or timeouts. You need to know this before deploying. Load testing reveals it.

Your regression suite must include a standard load test for every release candidate. The test does not need to simulate maximum theoretical load. It needs to simulate realistic peak load plus 20 percent headroom. If production peaks at 500 requests per minute, your load test runs at 600 requests per minute. If the new model version cannot handle that load with acceptable latency and success rates, it is not ready to deploy. If it can, you have confidence that performance will not degrade in production.

## The Cost of Ignoring Performance Regression

In late 2025, a logistics AI platform upgraded its routing model and deployed to production without performance testing. The model produced better routes—3 percent more efficient on average. But it also took 40 percent longer to compute those routes. At low query volumes, no one noticed. At production scale, the system could not keep up. Queries backed up. Latency climbed from 1.2 seconds to 9 seconds. Drivers waited for route updates. Deliveries were delayed. The company reverted to the old model within six hours. The 3 percent efficiency gain cost them a day of operational chaos and a six-figure revenue hit from delivery penalties.

Performance regression is not a technical inconvenience. It is a business failure. Latency regression degrades user experience. Throughput regression limits scale. Cost regression destroys margins. If your regression suite does not enforce performance baselines, you are gambling that every model update will coincidentally maintain performance. That gamble fails eventually. When it does, you have already deployed to production. Rolling back is expensive. Staying deployed is worse.

The fix is simple. Treat performance as a first-class regression surface. Measure latency percentiles. Measure throughput under load. Measure token counts and cost per query. Set baselines. Set thresholds. Enforce them before deployment. If a model version improves quality but regresses on performance, you have a decision to make. Sometimes the trade-off is worth it. Sometimes it is not. But you cannot make that decision if you do not measure the regression in the first place.

Performance tests are not optional. They are not nice-to-have. They are the difference between shipping a model that works in testing and shipping a model that works in production. The next step is understanding how to run these regression tests systematically—baseline comparison is the foundation.
