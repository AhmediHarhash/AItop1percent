# 4.7 — Context Window Overflow Regression

**Context window overflow** is what happens when the total input to your model — retrieved documents, conversation history, system instructions, and the user's query — exceeds the maximum tokens the model can process. The model does not error gracefully. It truncates, and truncation changes behavior. A retrieval system that worked perfectly at ten documents per query breaks silently when you add three more. A conversation that was coherent for fifteen turns degrades on turn sixteen. The overflow does not announce itself. The model simply stops seeing part of its input, and your users see responses that ignore critical information.

Context overflow is not a rare edge case. It is the predictable consequence of systems that grow. You add more retrieval chunks to improve coverage. You extend conversation memory to support longer sessions. You expand system instructions to handle new edge cases. Each change independently seems reasonable. Together they consume the context budget, and when the budget runs out, the model behavior changes. Testing for context overflow means testing the interaction between retrieval depth, conversation length, and instruction complexity — not individually, but in combination.

## The Truncation Strategies and Their Failure Modes

When input exceeds the context window, something must be removed. The strategies for removal determine what breaks. The most naive strategy is **tail truncation** — drop everything past the token limit. If your system instructions are at the beginning and the user query is at the end, tail truncation removes the query. The model answers a question it never saw. This is not hypothetical. A customer support system in early 2025 used tail truncation and discovered that long conversations resulted in responses that had nothing to do with the most recent customer message. The model was responding to something from five turns ago because the current turn was truncated away.

The second strategy is **head truncation** — drop the oldest content first. This preserves recent conversation turns but removes system instructions. A sales assistant built in mid-2025 used head truncation to manage long conversations. After twenty turns, the system instructions that enforced compliance guardrails were gone. The model started making pricing commitments it was explicitly instructed never to make. The team discovered the problem when Legal reviewed transcripts and found the model quoting prices that had been deprecated for six months. The instructions forbidding those quotes had been truncated twelve turns earlier.

The third strategy is **middle truncation** — keep the system instructions at the start and the user query at the end, and truncate conversation history in the middle. This is the default behavior in many frameworks. It works well until the retrieved context is too large. A research assistant in late 2025 retrieved fifteen documents per query and used middle truncation to manage context. The system instructions were preserved. The user query was preserved. But the retrieved documents — the entire reason the system existed — were truncated. The model answered based on two documents instead of fifteen, and the answers were incomplete. The eval suite missed this because the test cases used small retrieval sets. Production queries retrieved more, and quality silently degraded.

## The Lost in the Middle Problem

Even when nothing is truncated, context window utilization affects model behavior. The **lost in the middle** phenomenon is well-documented by 2026: models pay more attention to content at the beginning and end of the context window, and less attention to content in the middle. A RAG system that retrieves ten documents and places them in the middle of the context window will see the model rely heavily on the first and last document, even if the most relevant document is the fifth one.

This is not a hypothetical pattern. It has been measured across GPT-5, Claude Opus 4.5, and Gemini 3 models. A legal research system in 2025 retrieved ten case law documents and placed them between the system instructions and the user query. The most relevant case was consistently ranked third or fourth by the retrieval system. The model cited the first and tenth cases far more often than the third, even when the third was the strongest precedent. The team discovered the problem when attorneys noticed the model was citing weaker cases. They reordered retrieval results to place the highest-ranked document first, and citation quality improved immediately.

Testing for lost-in-the-middle requires more than checking whether the model can access any retrieved document. You must test whether the model can access the most relevant document when it is not in a privileged position. A regression test suite for a RAG system should include cases where the correct answer is in the first retrieved document, the last retrieved document, and the middle retrieved documents. If performance degrades for middle-position documents, you have a lost-in-the-middle regression, and the fix is not more retrieval — it is better placement or fewer retrieved documents.

## Context Budget Regression

Every token of retrieved context is a token unavailable for generation. This is the **context budget** — the allocation of tokens between input and output. A model with a 128,000 token context window and a conversation that consumes 120,000 tokens has 8,000 tokens available for the response. If the user asks for a detailed analysis, the model runs out of space mid-answer. The response truncates, and the user sees an incomplete thought.

Context budget regression happens when changes increase input size without accounting for output needs. A financial analysis system in mid-2025 retrieved five documents per query and generated responses averaging 2,000 tokens. The team improved retrieval by increasing the number of retrieved documents to twelve. Input size increased from 15,000 tokens to 36,000 tokens. Output size remained at 2,000 tokens on average, but edge cases that required 6,000-token responses now hit the context limit. The model would start a detailed analysis and stop mid-sentence. The team discovered the problem when users reported incomplete reports. The fix was not increasing the context window — it was retrieval pruning to preserve output budget.

Testing for context budget regression requires output-length aware test cases. Your eval suite should include queries that require long responses — summaries of twenty-page documents, analyses with multiple examples, comparisons across five dimensions. If those queries produce truncated output after a retrieval change, you have a context budget regression. The metric to track is **output truncation rate** — the percentage of responses that hit the maximum output token limit. If this rate increases after a change, the change consumed output budget, and you need to reclaim it.

## Testing for Overflow Conditions

Overflow testing requires synthetic load beyond production norms. Production queries average eight retrieved documents. Your overflow tests should include fifteen, twenty, thirty. Production conversations average twelve turns. Your overflow tests should include thirty turns, fifty turns. The goal is not to test typical behavior — it is to test the boundary where typical becomes failure.

A customer support system in late 2025 tested with conversations up to twenty turns, which covered 95 percent of production traffic. The remaining 5 percent — the longest, most complex support cases — were untested. When a product change increased retrieval from six documents to nine, the longest conversations started exhibiting strange behavior. The model would forget the user's account details mid-conversation. The team traced the problem to context overflow at turn twenty-three. The test suite never exercised that depth. The fix was expanding the test suite to include fifty-turn conversations, which exposed overflow issues before deployment.

Overflow test cases should include combinations of factors: long conversations with high retrieval depth, complex queries with large system instructions, multi-turn dialogues with dense conversation history. Each factor individually may be safe. The combination is where overflow occurs. A good overflow test suite includes at least ten cases at the 99th percentile of context utilization, not just the median.

## Dynamic Context Allocation Strategies

Static context allocation — always retrieve ten documents, always keep fifteen conversation turns — works until it does not. **Dynamic context allocation** adjusts input components based on available budget. If the conversation is short, retrieve more documents. If the conversation is long, retrieve fewer. If the query is simple, allocate more tokens to output. If the query is complex, allocate more to input.

A medical diagnosis system in 2025 used dynamic allocation. For simple queries with no conversation history, the system retrieved twelve research papers. For complex multi-turn consultations, it retrieved six. The allocation strategy prioritized conversation context over retrieval depth for returning users, and retrieval depth over conversation context for first-time users. This prevented overflow while maintaining quality. The strategy was tuned through A/B testing, not guesswork. The team measured quality at different allocation levels and chose the allocation that maximized correctness without hitting overflow.

Testing dynamic allocation requires test cases that trigger different allocation modes. Your eval suite should include short single-turn queries, long multi-turn conversations, queries with minimal retrieval needs, and queries that require maximum retrieval. Each mode should have regression tests that verify the allocation strategy makes the right trade-off. If a change breaks the allocation logic, quality degrades in a specific mode, and the regression test catches it.

## Monitoring Context Utilization

Context overflow is invisible without instrumentation. The model does not emit a warning when it truncates. The user does not see an error message. Quality degrades, and no one knows why. **Context utilization monitoring** tracks how much of the context window is consumed by each component: system instructions, retrieved documents, conversation history, user query. When utilization approaches the limit, the monitoring system alerts before overflow occurs.

A legal research system in late 2025 implemented context utilization dashboards that tracked average utilization, 95th percentile utilization, and maximum observed utilization per session. When a retrieval change increased average utilization from 60 percent to 82 percent, the dashboard flagged the increase before users reported issues. The team rolled back the change, investigated why utilization spiked, and discovered that the new retrieval system was including full document text instead of excerpts. The fix was reverting to excerpt-based retrieval, which restored utilization to 60 percent.

The metrics to track are **average context utilization** across all queries, **context utilization distribution** to find the tail cases, and **overflow rate** — the percentage of queries where input exceeded the context limit and truncation occurred. If overflow rate is above zero, you have a problem. If context utilization is above 80 percent, you are at risk. If utilization is increasing over time, something is consuming budget, and you need to find it before overflow begins.

## The Model Upgrade Trap

When you upgrade from one model to another, the context window size may change, but so does tokenization. GPT-5 and Claude Opus 4.5 both support 200,000-token context windows, but they tokenize text differently. A conversation that consumes 30,000 GPT-5 tokens may consume 33,000 Claude tokens. If your system was tuned to use 90 percent of GPT-5's context window, upgrading to Claude Opus 4.5 with the same input will overflow.

A translation service in mid-2025 migrated from GPT-5 to Claude Opus 4.5 to improve translation quality. The migration was tested with single-document translations, which worked perfectly. Multi-document translations — ten source documents translated in one batch — started failing silently. The model would translate the first seven documents completely and the last three partially. The team traced the problem to tokenization differences. The same ten documents consumed 8 percent more tokens in Claude than in GPT-5, and the batch translation requests that were at 92 percent utilization in GPT-5 were at 100 percent in Claude. The fix was reducing batch size from ten documents to nine.

Testing for tokenization regression requires running the same test cases against both the old and new model and comparing context utilization. If utilization increases by more than 5 percent, you need to adjust input size or allocation strategy. This is not optional. Model upgrades that ignore tokenization differences will overflow in production, and the overflow will be silent.

A regression test that passed with 90 percent context utilization is not a safe test. It is a test waiting to fail when you upgrade models. Safe tests use 70 percent utilization or less, leaving headroom for tokenization changes, retrieval improvements, and conversation length growth. If your tests routinely exceed 80 percent utilization, you are testing the edge of failure, not the center of safety.

---

The next subchapter addresses what happens when retrieval changes do not cause overflow but do cause hallucinations — and how to detect the causal link between retrieval quality and hallucination rate before users encounter fabricated information.
