# 8.6 — Input Distribution Monitoring

Model performance depends on input characteristics. A summarization model trained and evaluated on news articles performs differently on legal contracts. A sentiment classifier tuned on product reviews performs differently on social media posts. A question-answering system evaluated on factual queries performs differently on opinion-based questions. The relationship between input characteristics and output quality is not incidental — it is foundational. Your eval suite measures performance on a specific distribution of inputs. If production traffic shifts to a different distribution, your eval results no longer predict production performance. Input distribution monitoring is the system that detects when this shift happens and triggers re-evaluation before quality degrades.

This is not optional. Every production AI system experiences input drift. Users find new ways to interact with the system. Product changes introduce new use cases. External events shift the topics users ask about. Seasonal patterns change the language users employ. Marketing campaigns bring in users with different expectations. Competitors launch features that change user behavior. A model that performed at 91 percent accuracy on January traffic may perform at 83 percent accuracy on April traffic — not because the model changed, but because the inputs changed. If you do not monitor input distribution, you will not know this is happening until users complain, metrics degrade visibly, or a regression becomes a public incident.

## Why Input Distribution Matters

The reason input distribution matters is that models are optimized for the data they were trained on and evaluated against. A model learns patterns — the statistical relationships between input features and correct outputs. When production inputs match the distribution of training and eval inputs, the model applies those learned patterns successfully. When production inputs diverge from that distribution, the model encounters scenarios it was not optimized for. It guesses. Sometimes the guesses are correct. Often they are not.

The divergence can be subtle. A customer support model trained on queries submitted via web form encounters queries submitted via voice assistant. The language is less formal, sentences are shorter, and users assume more context. The model was never evaluated on this input style. It still generates responses, but the responses are less relevant because the model is pattern-matching against training examples that used different phrasing. The eval suite shows 89 percent quality. Production quality is 76 percent. The gap is invisible until you measure input distribution and realize that 30 percent of production traffic no longer matches eval distribution.

The divergence can also be dramatic. A content moderation system trained on English text encounters a sudden influx of Spanish content after a marketing campaign launches in Latin America. The model was not trained on Spanish. It still produces predictions, but those predictions are unreliable. The eval suite never included Spanish examples. No alarm fires. Users report that harmful content is not being removed, and safe content is being blocked incorrectly. The root cause is input distribution shift, but the symptom is quality degradation.

Input distribution monitoring answers a single question: is the traffic my model sees in production statistically similar to the traffic it was evaluated on? If the answer is yes, your eval results remain valid. If the answer is no, you need to re-evaluate on the new distribution and potentially retrain or fine-tune the model to handle it.

## Detecting Input Distribution Shift

Input distribution shift is detected by comparing production inputs to a baseline. The baseline is the distribution of inputs in your eval set — the test data you used to validate model performance. For every production input, you extract the same features you track in your eval set — input length, language, topic, complexity, entity density, sentiment, and any domain-specific features relevant to your task. You aggregate these features over a time window — hourly, daily, or weekly depending on traffic volume — and compare the aggregated production distribution to the baseline distribution.

The comparison uses statistical tests designed to detect distribution differences. The most common test is the **Kolmogorov-Smirnov test**, which measures the maximum difference between two cumulative distribution functions. If the KS statistic exceeds a threshold, the distributions are considered significantly different. Another common test is the **chi-squared test**, which compares observed frequencies to expected frequencies across bins. For continuous features like input length, you use KS. For categorical features like language or topic, you use chi-squared.

The threshold for significance is calibrated to your tolerance for false positives and false negatives. A low threshold triggers alerts frequently, which increases false positives but ensures you catch real shifts early. A high threshold reduces false positives but increases the risk of missing gradual shifts until they become severe. Most teams start with a significance level of 0.05 — the standard threshold in statistical testing — and adjust based on observed alert frequency and the cost of investigating false positives.

The time window over which you aggregate inputs matters. A short window — one hour — detects shifts quickly but is sensitive to normal traffic fluctuations. A long window — one week — smooths out fluctuations but delays detection. The correct window depends on your traffic volume and how quickly shifts occur. A high-traffic system with 100,000 requests per hour can use a one-hour window and still have enough samples for statistical significance. A low-traffic system with 1,000 requests per day needs a 24-hour window to accumulate sufficient data.

## Baseline Input Distributions

The baseline is not static. It is updated periodically to reflect intentional changes in your system. When you launch a new feature, expand to a new user segment, or add support for a new language, the input distribution changes by design. You want the monitoring system to detect unexpected shifts, not expected shifts. This requires updating the baseline whenever you make a planned change that affects input characteristics.

Baseline updates happen in two ways. The first is manual update: when you deploy a new feature, you collect inputs from the first week of production traffic, verify that the feature is working as intended, and replace the baseline with the new distribution. The monitoring system now compares future traffic to this updated baseline. The second is automatic update: the system incrementally incorporates new data into the baseline over time, using a sliding window or exponential decay. Automatic updates prevent the baseline from becoming stale, but they also risk incorporating gradual drift that should have triggered an alert. Most teams use manual updates for major changes and automatic updates with long decay periods for minor changes.

The baseline must represent the traffic you intend to support. If your eval set contains only English queries but you plan to support Spanish, your baseline should include Spanish examples before you launch Spanish support. If your eval set contains only short inputs but you expect users to submit long inputs in production, your baseline should include long inputs. A baseline that does not match your intended use cases will trigger false positives when users behave as expected.

## Input Feature Monitoring

The features you monitor depend on your task and your model architecture. Every system should monitor basic features: input length, language, and any task-specific structure like query type or entity presence. Many systems also monitor derived features like semantic topic, syntactic complexity, and sentiment.

**Input length** is the most universal feature. It is measured in tokens, characters, or words depending on your tokenization strategy. Length matters because many models degrade on inputs far outside the length distribution they were trained on. A summarization model trained on 500-token articles may fail on 3,000-token articles. A question-answering model trained on single-sentence queries may fail on multi-paragraph queries. Length shifts are common — users discover that the system accepts longer inputs and start submitting them, or a product change increases character limits without corresponding model updates.

**Language** is critical for multilingual systems. You detect language using a language identification library and track the proportion of inputs in each language. If your eval set is 95 percent English and 5 percent Spanish, but production traffic shifts to 70 percent English and 30 percent Spanish, your model is now handling a distribution it was not evaluated on. Language shift is one of the most dramatic distribution changes and one of the easiest to detect. It also has the clearest mitigation — you add Spanish examples to your eval set, re-evaluate, and fine-tune if necessary.

**Topic** is harder to measure but highly predictive of model performance. You can use a topic modeling library, a zero-shot classifier, or a keyword-based heuristic to assign inputs to categories. If your eval set contains 40 percent product questions, 30 percent account questions, and 30 percent billing questions, but production traffic shifts to 60 percent billing questions, your model is seeing more of a specific topic than it was optimized for. Topic shift often correlates with external events — a billing system outage causes a spike in billing questions, or a new feature launch causes a spike in feature-specific questions. Detecting topic shift lets you re-evaluate on the shifted distribution and determine whether performance has degraded.

**Complexity** is a proxy for how difficult an input is to process. You can measure syntactic complexity using dependency parse depth, lexical complexity using vocabulary diversity, or semantic complexity using embedding-based metrics. Complexity shift is common when your user base evolves — early adopters submit simple queries, mainstream users submit more complex queries. A model that performs well on simple inputs may fail on complex inputs. Monitoring complexity lets you detect this shift and respond before quality degrades visibly.

## Statistical Tests for Distribution Shift

The statistical test you use depends on the feature type. For continuous features like input length, you use the **Kolmogorov-Smirnov test** or the **Wasserstein distance**. For categorical features like language or topic, you use the **chi-squared test** or **Jensen-Shannon divergence**. For high-dimensional features like embeddings, you use **maximum mean discrepancy** or **kernel two-sample tests**.

The KS test measures the maximum vertical distance between two cumulative distribution functions. If baseline input lengths range from 10 to 500 tokens with a median of 120, and production input lengths range from 10 to 800 tokens with a median of 180, the KS test quantifies how different these distributions are. A p-value below 0.05 indicates a statistically significant shift. The advantage of the KS test is that it is nonparametric — it makes no assumptions about the shape of the distributions. The disadvantage is that it is sensitive to sample size. With very large samples, even trivial differences become statistically significant.

The chi-squared test compares observed frequencies to expected frequencies across categorical bins. If your baseline has 80 percent English, 10 percent Spanish, and 10 percent French, but production traffic shifts to 60 percent English, 25 percent Spanish, and 15 percent French, the chi-squared test detects this shift. The test is straightforward to interpret and fast to compute. The disadvantage is that it requires discrete categories. Continuous features must be binned, which loses information.

For embeddings — high-dimensional vector representations of inputs — you use kernel-based tests like maximum mean discrepancy. You embed both baseline and production inputs into a shared vector space, then measure the distance between the mean embeddings. A large distance indicates that the semantic content of inputs has shifted. This approach is powerful because it captures semantic drift that simple features like length or keyword frequency would miss. The disadvantage is computational cost. Embedding every input and computing MMD over large samples is expensive.

## Shift Severity: When to Alert

Not every shift requires immediate action. Small shifts happen constantly due to natural traffic variation. Large shifts indicate that your model is operating outside its validated range and performance may be degraded. The severity of a shift determines the response.

**Minor shift** is defined as a statistically significant difference that is small in magnitude. Input length increases by 10 percent, or the proportion of Spanish inputs increases from 5 percent to 8 percent. Minor shifts are logged but do not trigger alerts. They accumulate over time, and if multiple minor shifts occur in the same direction, they become a moderate shift.

**Moderate shift** is defined as a difference large enough to potentially affect model performance but not large enough to constitute an emergency. Input length increases by 30 percent, or the proportion of a specific topic doubles. Moderate shifts trigger alerts to the engineering team. The alert includes a summary of the shift, a comparison chart showing baseline versus production distributions, and a recommendation to re-evaluate the model on recent production traffic. The team investigates within 48 hours.

**Severe shift** is defined as a difference large enough that model performance is likely degraded. Input length triples, or a language not present in the baseline suddenly represents 20 percent of traffic. Severe shifts trigger immediate alerts and may trigger automatic mitigations like routing traffic to a fallback model or increasing human review rates. The team investigates within hours.

The thresholds that define minor, moderate, and severe shifts are calibrated based on historical data. You analyze past incidents where model performance degraded and determine what magnitude of input shift preceded the degradation. Those magnitudes become your thresholds. If a 50 percent increase in input length historically correlates with a 5 percent drop in quality, then a 50 percent increase in length is classified as a moderate shift.

## Input Drift vs Seasonal Variation

Not all distribution changes are drift. Some are seasonal variation — predictable patterns that recur on a schedule. Retail systems see input shifts during Black Friday. Tax preparation systems see input shifts in April. Customer support systems see input shifts after product launches. These shifts are expected. The monitoring system should account for them.

Seasonal variation is handled by maintaining multiple baselines — one for each season or event type. During Black Friday, the system compares production traffic to the Black Friday baseline, not the annual baseline. During tax season, it compares to the tax season baseline. This prevents false positives during predictable periods of high variance.

Drift is variation that is not explained by season or known events. A gradual increase in average input length over six months, or a sudden appearance of queries in a new language that no product change introduced. Drift indicates that user behavior is evolving in ways you did not anticipate, and your model may no longer be optimized for the traffic it is receiving.

The distinction matters because the response is different. Seasonal variation requires no action — your system is behaving as expected. Drift requires investigation and potentially re-evaluation or retraining. The monitoring system should flag drift and ignore seasonal variation, which requires historical data to train the model of what normal seasonal patterns look like.

## Input Monitoring Dashboards

Input distribution monitoring is not useful if no one looks at it. The monitoring data must be surfaced in dashboards that engineers and product managers check regularly. The dashboard shows baseline versus production distributions for every monitored feature, statistical test results, shift severity, and trend lines over time.

The key visualizations are histograms showing baseline versus production distributions overlaid on the same axis, time series charts showing how each feature has evolved over the past week or month, and heatmaps showing shift severity across all features simultaneously. The dashboard also includes a summary panel that lists the features with the largest shifts and the features that triggered alerts.

The dashboard is reviewed during weekly model health reviews. The team examines shifts, determines whether they are expected or unexpected, and decides whether re-evaluation is needed. If a shift is expected and performance metrics remain stable, no action is taken. If a shift is unexpected or performance metrics have degraded, re-evaluation is triggered.

## Triggering Re-Evaluation When Inputs Shift

When a moderate or severe input shift is detected, the next step is re-evaluation. You sample recent production inputs that caused the shift, construct an eval set from those inputs, label them or generate reference outputs, and run the model through the eval pipeline. The result is a performance metric on the shifted distribution.

If performance on the shifted distribution matches performance on the baseline distribution, the shift is benign — the model generalizes well to the new input characteristics. No further action is needed. If performance on the shifted distribution is significantly lower, the shift is harmful. You now have three options: fine-tune the model on examples from the shifted distribution, route the shifted traffic to a different model that handles it better, or increase human review rates for inputs that match the shifted distribution.

Re-evaluation triggered by input shift is more targeted than scheduled re-evaluation. You are not evaluating the model on a static test set — you are evaluating it on the exact distribution that production traffic has shifted toward. This gives you precise information about whether the shift is affecting quality and how severe the impact is.

Input distribution monitoring closes the loop between eval and production. Your eval suite tells you how the model performs on known distributions. Input monitoring tells you when production diverges from those known distributions. Together, they ensure that your understanding of model performance stays current even as user behavior evolves.

The next critical component is **output distribution shift detection** — monitoring the model's predictions for changes that indicate degradation even when input distributions remain stable.
