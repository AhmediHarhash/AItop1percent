# 7.8 — Content Policy Drift Detection

Content policy drift happens when the model's behavior diverges from the policy document. The policy says the model should not generate medical diagnoses. The model generates medical diagnoses. The policy says the model should decline requests for financial advice. The model provides financial advice. The policy says the model should refuse to roleplay as real public figures. The model roleplays as real public figures. The gap between what the policy says and what the model does is drift, and it grows over time unless you actively measure and correct it.

Drift is not refusal rate regression. Refusal rate measures how often the model says no. Policy drift measures whether the model follows the rules. A model can maintain stable refusal rates while completely changing which rules it enforces. It might start refusing harmless creative writing requests while accepting requests that violate financial advice policy. The refusal rate stays the same. The policy adherence collapses. This is why you need a separate regression test specifically for policy alignment.

## The Gap Between Intent and Behavior

Policy documents describe intent. The model exhibits behavior. Intent is written by legal, compliance, and trust teams. Behavior is produced by a large language model that interprets instructions probabilistically and is influenced by training data, fine-tuning, system prompts, and every other component in the stack. The gap between intent and behavior is never zero. The question is whether the gap is growing.

Policy intent is often expressed in abstract terms. "Do not generate content that could be used to harm others." "Decline requests that violate user privacy." "Refuse to provide advice that requires professional licensure." These are human-legible principles. The model does not execute principles. It executes patterns learned from data and instructions. The system prompt translates principles into instructions. "If the user asks for medical advice, explain that you cannot diagnose conditions and recommend consulting a healthcare provider." This is closer to executable behavior, but still not deterministic. The model will interpret the instruction differently depending on how the request is phrased, what the conversation history contains, and what similar patterns existed in training data.

Policy drift occurs when the model's interpretation of instructions shifts over time. A prompt change makes the model more or less likely to classify a request as requiring refusal. A fine-tune introduces new patterns that conflict with policy instructions. A model version update changes the base capabilities in ways that affect downstream behavior. Each of these changes can widen the gap between policy intent and observed behavior. The drift detection test measures whether the gap is widening.

## Causes of Policy Drift

Model updates are the most common cause. You switch from one model version to another. The new version has different capabilities, different training data, different instruction-following behavior. The system prompt that worked for the previous version produces different outputs from the new version. You did not change the policy. You did not change the system prompt. But the model now behaves differently, and some of those behavioral changes violate policy.

Prompt changes are the second most common cause. You update the system prompt to improve some aspect of model behavior. You add instructions to make responses more concise. You add examples to improve formatting. You adjust the tone guidance. These changes are intended to be policy-neutral, but they interact with the model's existing instruction-following patterns in unpredictable ways. The model starts interpreting policy-related instructions differently. Refusal thresholds shift. Content filters trigger less reliably. The changes are subtle, but they compound.

Fine-tuning is the third cause. You fine-tune the model to improve task performance. The fine-tuning dataset contains examples of desired behavior for the core use case. It does not contain comprehensive examples of policy compliance across all edge cases. The model learns the fine-tuning distribution and becomes slightly less adherent to instructions that were not reinforced in the fine-tuning data. This is catastrophic forgetting applied to policy compliance. The model still knows the policy in principle. It just applies it less consistently.

Adversarial evolution is the fourth cause. Users probe the model with increasingly sophisticated attempts to bypass policy. Some succeed. Those successes spread. More users try similar techniques. The model encounters a distribution of adversarial prompts that differs from the distribution it was tested on. The prompts exploit edge cases in the policy implementation. The model complies with requests it should refuse because the requests are phrased in ways the policy instructions did not anticipate. This is not model drift in the technical sense. This is adversarial drift in the user distribution. But the effect is the same: the model violates policy more often.

## Policy Test Suites

A policy test suite is a curated set of requests with known correct behavior under the policy. Some requests should be accepted. Some should be refused. Some should be handled with specific mitigations. You run the test suite against the model and measure how often the model behaves correctly. This gives you a policy adherence score. If the score decreases between model versions, you have detected drift.

The test suite must cover every meaningful policy dimension. If your policy prohibits medical diagnoses, the suite must include a range of medical advice requests. Straightforward requests: "Do I have diabetes?" Ambiguous requests: "What could cause these symptoms?" Roleplay requests: "Pretend you are a doctor and diagnose me." Multi-turn requests: "I am experiencing chest pain. What should I do?" The model's policy adherence is measured by how it handles all of these, not just the straightforward case.

Each test case has a label: accept, refuse, mitigate, or conditional. Accept means the model should respond normally. Refuse means the model should explicitly decline. Mitigate means the model should respond with a qualified answer that includes appropriate disclaimers. Conditional means the correct behavior depends on context extracted from the conversation. The label represents the ground truth under your policy. The model's response is classified into the same categories. Policy adherence is the percentage of test cases where the model's behavior matches the label.

The test suite evolves as the policy evolves. When you update the policy to allow a new category of content, you update the test suite to reflect the new policy. When you discover a new adversarial technique, you add examples to the suite. The test suite is not static. It is a living artifact that represents the current definition of correct model behavior. Every model version is measured against the current test suite, not a historical one.

## Measuring Policy Adherence

Policy adherence measurement requires both automated classification and human review. Automated classification works for explicit refusals and templated mitigations. The model generates a string that matches a known refusal pattern. The automated check labels it as a refusal. The model generates a response that includes a known disclaimer template. The automated check labels it as a mitigation. These classifications are fast and reliable for the majority of test cases.

Human review is required for ambiguous responses. The model generates a response that partially addresses the request without explicitly refusing. Is this a violation or a mitigation? The model generates a response that refuses but does not use the standard template. Is this a refusal or a deflection? The model generates a response that appears policy-compliant but contains subtle violations. Did the model follow the policy or bypass it? These questions require human judgment. You cannot measure policy adherence at scale without human reviewers who understand the policy deeply.

The measurement protocol runs the full test suite through the candidate model, collects all responses, classifies them automatically where possible, and routes ambiguous cases to human reviewers. The reviewers apply the policy and label each response as correct or incorrect. The policy adherence score is the percentage of correct responses. This score is compared to the baseline score from the current production model. If adherence decreases beyond a threshold, the candidate model is blocked.

## Drift Detection Thresholds

The drift detection threshold is the maximum allowable decrease in policy adherence score between model versions. A threshold of 2 percentage points means the candidate model can have up to 2 percent lower adherence than the baseline and still pass. Beyond that, the release is blocked. The threshold is not zero because measurement noise and test suite updates introduce small variations that do not represent real drift.

Threshold-setting depends on policy risk. For high-stakes policies—those governing illegal content, safety, privacy—the threshold should be tight. A 1 percentage point decrease might be unacceptable. For lower-stakes policies—those governing tone, style, or preferred behavior—the threshold can be looser. A 5 percentage point decrease might be tolerable if the model improves in other areas. There is no universal threshold. The right threshold is the one that reflects the risk profile of each policy dimension.

Thresholds should vary by policy category. A content policy that prohibits generating harmful instructions might have zero tolerance for drift. Any decrease in adherence blocks the release. A content policy that governs how the model explains its limitations might tolerate a 3 percent decrease. You implement this by segmenting the test suite into policy categories and setting per-category thresholds. The candidate model must pass every category threshold to be released. This prevents drift in high-stakes categories from being averaged out by stability in low-stakes categories.

## Category-Specific Drift

Aggregate policy adherence obscures category-specific drift. A model might maintain 96 percent overall adherence while compliance with financial advice policy drops from 98 percent to 88 percent. The aggregate number looks stable. The financial advice policy is no longer reliably enforced. This is why you measure adherence by category and apply category-specific thresholds.

Category-specific drift often reveals unintended effects of model updates. You switch to a new base model to improve reasoning. The new model is better at reasoning, but worse at following refusal instructions in legal contexts. Aggregate adherence drops 1 percentage point. Legal policy adherence drops 12 percentage points. The aggregate threshold passes. The category threshold fails. The release is blocked. This is the regression gate working as designed. It detects drift that matters even when overall metrics are acceptable.

The test suite should include both in-distribution and out-of-distribution examples for each category. In-distribution examples are requests that resemble the training and fine-tuning data. The model handles these well because it has seen similar patterns. Out-of-distribution examples are adversarial rephrasing, edge cases, and novel attack patterns. The model handles these poorly unless the policy implementation is robust. Both categories matter. In-distribution adherence measures baseline capability. Out-of-distribution adherence measures robustness to adversarial users.

## Re-Alignment Triggers

When policy drift is detected, you need a re-alignment process. The candidate model failed the adherence test. You cannot release it as-is. You have three options: revert to the previous model, fix the drift and re-test, or accept the drift and update the baseline. Reverting is the safest. Fixing is the most work. Accepting is only appropriate when the drift aligns with an intentional policy change.

Fixing drift depends on the cause. If the drift was caused by a prompt change, you iterate on the prompt until adherence recovers. If the drift was caused by a model version update, you test alternate versions or apply targeted fine-tuning to restore adherence. If the drift was caused by fine-tuning on task data, you augment the fine-tuning dataset with policy-adherence examples and re-train. Each fix requires re-running the full policy test suite to confirm that adherence is restored and that the fix did not introduce new drift elsewhere.

Re-alignment is not instantaneous. It takes days or weeks. During this time, the current production model continues to serve traffic. This is why the regression gate blocks releases preemptively. If you waited until after deployment to discover drift, you would have to roll back under time pressure. The gate gives you time to fix the drift before users are affected.

## Continuous Drift Monitoring vs Periodic Testing

Periodic testing measures policy adherence before each release. Continuous monitoring measures it in production. Both are necessary. Periodic testing is the regression gate. It blocks bad versions from being deployed. Continuous monitoring is the safety net. It detects drift that happens in production due to user distribution shifts, adversarial evolution, or silent data pipeline changes.

Continuous drift monitoring runs a subset of the policy test suite against the live model daily or weekly. The subset is chosen to cover high-risk policy dimensions and high-frequency use cases. If adherence drops below a threshold, an alert is triggered. The team investigates. If real drift is confirmed, a corrective deployment is planned. This catches drift that the pre-release gate missed because it emerged only after deployment.

The two mechanisms have different thresholds. The pre-release gate has tight thresholds because you can afford to block a release and iterate. The continuous monitor has looser thresholds because production alerts have operational cost and must balance false positives with drift detection. A pre-release threshold of 2 percentage points is reasonable. A continuous monitoring threshold of 5 percentage points might be appropriate to avoid alert fatigue.

## Policy Versioning and Baseline Updates

Content policies change over time. Regulations are updated. Business requirements evolve. Adversarial techniques force policy refinements. Each policy change requires a baseline update. You re-run the policy test suite with the new policy interpretation. The adherence score under the new policy becomes the new baseline. All future models are measured against it.

Policy versioning prevents the gate from blocking legitimate policy changes. If you update your financial advice policy to allow certain types of general information that were previously prohibited, the adherence score will shift. The old baseline would flag this as drift. The new baseline recognizes it as correct behavior under the updated policy. Without versioning, the gate becomes an obstacle to policy evolution instead of a safeguard against unintended drift.

The versioning process is manual. A policy change is documented, reviewed, and approved. The test suite is updated to reflect the change. The baseline is re-measured. The new baseline is committed to version control. This ensures that policy changes and baselines stay synchronized. If someone updates the policy without updating the baseline, the regression gate measures the wrong thing. If someone updates the baseline without documenting the policy change, the gate allows drift that should be blocked.

The final regression test before release is compliance re-validation. It confirms that the model not only follows your internal content policy, but also meets external regulatory requirements. That is where Section 18 goes next.

