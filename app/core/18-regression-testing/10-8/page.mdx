# 10.8 â€” Regression Test Generation from Bugs

In June 2025, a legal AI company deployed a fix for a bug where their contract review model was incorrectly flagging standard indemnification clauses as high-risk. The fix worked. The bug disappeared. Three months later, during a routine model update, the bug returned. A different engineer, unaware of the previous fix, had refactored the prompt in a way that reintroduced the exact same failure mode. The model started flagging standard clauses again. Customers noticed immediately. The fix took two days because the original engineer who solved it was on vacation, and the new engineer had to re-debug the problem from scratch. The root cause was simple: when the first engineer fixed the bug, they did not write a regression test. The codebase had no memory of the fix. The bug came back because nothing prevented it from coming back.

Bugs that come back are not accidents. They are failures of process. Every bug fix is a piece of knowledge: this specific thing was broken, and this is how we made it correct. If that knowledge lives only in a commit message or in the memory of the engineer who fixed it, it will be lost. Code changes. Engineers leave. Prompts get refactored. The only way to ensure a bug stays fixed is to encode the fix as a test that runs forever. This is not optional. This is professional baseline.

## Every Bug Fix Needs a Regression Test

The rule is absolute: no bug fix is complete until a regression test exists that would have caught the bug before it was fixed and now catches it if anyone reintroduces it. This is not a suggestion for high-severity bugs. This applies to every bug that made it past your development process and into a build, a staging environment, or production.

The regression test serves two purposes. First, it proves the bug is actually fixed. If you cannot write a test that fails before the fix and passes after the fix, you do not understand the bug well enough to be confident it is resolved. The test is the proof of fix. Second, it prevents recurrence forever. Once the test is in your regression suite, no code change, prompt refactoring, or model update can reintroduce the bug without breaking the build. The bug becomes architecturally impossible to reintroduce silently.

A customer support AI company made regression test creation mandatory for every bug fix. The definition of done for a bug was: code fix committed, test committed, test passing in CI, pull request approved. If a bug fix pull request did not include a new test, the CI pipeline flagged it with a blocking error. Engineers could not merge without either adding a test or getting an explicit exemption from a staff engineer with written justification. The exemption rate was less than 2 percent per quarter. The bug recurrence rate dropped from 15 percent to less than 1 percent within six months.

The test-first approach is even stronger. Before fixing the bug, write a test that reproduces it. The test should fail, confirming that the bug exists. Then fix the bug. The test should pass, confirming the fix works. Then commit both the fix and the test together. This approach forces clarity. If you cannot write a failing test that captures the bug, you do not understand the failure mode. The discipline of test-first bug fixing eliminates vague fixes that paper over symptoms without addressing root causes.

## Test Case Design from Bug Reports

A bug report is raw material for a test case, but it is not yet a test case. The conversion requires extracting the essential conditions that trigger the failure and encoding them as assertions.

Test case design starts with bug reproduction. The bug report describes what went wrong, but it often lacks precision. A vague bug report says "model gave wrong answer for medical query." A precise bug report says "model recommended acetaminophen for a patient with liver disease, where the patient context explicitly stated 'history of hepatitis C, avoid hepatotoxic drugs.'" The first bug report cannot be converted into a test. The second one can.

If the bug report is vague, your first step is to reproduce the bug with specific inputs. A healthcare AI team required every bug report to include a minimal reproduction case: the exact input, the exact model configuration, the exact observed behavior, and the exact expected behavior. If a bug was reported without this data, it was sent back to the reporter with a template to fill out. This seemed bureaucratic, but it had a hidden benefit: the act of creating a minimal reproduction case often revealed the root cause immediately. Half of all bugs were resolved by the reporter before engineering even looked at them, simply because writing the reproduction case clarified the problem.

The test case must capture the minimal conditions that trigger the bug. Not the full context, not the entire user session, just the essential inputs. A fintech company had a bug where their fraud detection model incorrectly flagged transactions from a specific merchant ID as suspicious. The bug report included 45 minutes of user session logs. The test case extracted three things: the merchant ID, the transaction amount, and the user account type. Those three conditions were sufficient to reproduce the bug. The test ran in 200 milliseconds instead of simulating a 45-minute session. Minimal reproduction cases make tests fast, reliable, and maintainable.

The assertion in a bug-based test has two parts: what must not happen (the bug behavior) and what must happen (the correct behavior). A medical AI bug involved the model hallucinating a drug interaction that did not exist. The test asserted two things: the output must not mention the hallucinated interaction (negative assertion), and the output must correctly state that no interactions are known for this drug combination (positive assertion). Both assertions are necessary. The negative assertion prevents the specific bug from recurring. The positive assertion ensures the fix did not just silence the output entirely.

## Bug Severity and Test Priority

Not all bugs warrant the same level of regression testing. Bug severity determines test priority, which determines where the test runs and how often.

Critical bugs are those that cause data exposure, safety risk, financial loss, or regulatory violation. Every critical bug becomes a Tier 1 regression test that runs on every commit before merge. No negotiation. If the bug was severe enough to trigger an incident response, it is severe enough to block future commits that reintroduce it.

High-severity bugs are those that cause visible incorrect behavior for users but do not involve safety or compliance risk. These become Tier 2 regression tests that run nightly or before deployment. A customer support AI bug that caused the model to give incorrect refund policies was high-severity. It did not risk user safety, but it created operational problems and customer confusion. The regression test ran nightly to catch regressions before staging deployment.

Medium-severity bugs are those that affect edge cases, rare inputs, or non-critical features. These become Tier 3 regression tests that run weekly or before major releases. A bug that caused the model to format timestamps incorrectly for users in a single timezone was medium-severity. It needed a regression test, but it did not need to run on every commit. The test ran weekly as part of the full regression suite.

Low-severity bugs are cosmetic issues, minor UX glitches, or issues that have straightforward workarounds. These might not warrant automated regression tests at all. Instead, they go into a manual test plan or a lower-priority test suite that runs before major version releases. A bug that caused the model to use inconsistent capitalization in a non-customer-facing log message was low-severity. The fix was committed, but no automated test was added. Engineering judgment determined that the cost of maintaining the test exceeded the value of preventing recurrence.

A legal tech company used a bug severity matrix that mapped directly to test priority. Severity 1 bugs always became Tier 1 tests. Severity 2 bugs became Tier 2 tests unless test complexity was prohibitive. Severity 3 bugs became Tier 3 tests if test creation was straightforward, otherwise they were logged but not tested. Severity 4 bugs were fixed but not tested. This matrix removed ambiguity from the test generation decision.

## Automated Test Generation from Bug Patterns

Some classes of bugs occur repeatedly with slight variations. Instead of writing individual tests for each instance, you can automate test generation for the entire bug pattern.

Bug pattern recognition starts with clustering. If multiple bugs share a common root cause, they belong to the same pattern. A content moderation AI had a series of bugs where the model failed to detect policy violations when the violating content was embedded in long paragraphs with surrounding benign text. Each bug involved a different policy and different content, but the pattern was the same: the model's attention degraded for violations buried in verbose context. Instead of writing individual tests for each bug, the team automated test generation. They created a test generator that took any policy rule and any violation example, embedded it in paragraphs of benign filler text at various positions, and asserted that the violation must still be detected. This single test generator covered 200 potential bugs.

Automated test generation also applies to input variation bugs. A medical AI had multiple bugs involving medication names with non-standard formatting: brand names with unusual capitalization, generic names with hyphens, medications with numeric suffixes. Each bug was fixed individually, but the underlying issue was that the prompt assumed standard formatting. The team built a test generator that took a list of known medications and generated test cases for every common formatting variation: all caps, all lowercase, mixed case, hyphenated, unhyphenated, with suffixes, without suffixes. The generator produced 3000 test cases from 200 base medications. Every subsequent prompt change was validated against all 3000 variations. Formatting-related medication bugs went to zero.

Automated test generation requires investing in test infrastructure upfront. You need parameterized test frameworks, test case generators, and tooling that can produce hundreds or thousands of test cases from a single pattern specification. The investment pays off when a single generator prevents dozens of individual bugs. A fintech company invested two engineer-weeks building a test generator for currency and date formatting bugs. The generator produced 5000 test cases covering every currency symbol, every date format, and every locale combination in their supported regions. Over the next year, the generator caught 30 bugs before they reached production. The return on investment was clear.

## Bug Clustering for Test Coverage

Bug clustering is the process of grouping bugs by common characteristics to identify systematic test coverage gaps. Instead of treating each bug as an isolated fix, clustering reveals which categories of bugs are escaping your test suite most frequently.

Clustering by failure mode groups bugs based on what went wrong. Hallucination bugs form one cluster. Context handling bugs form another. Formatting bugs form a third. Instruction-following bugs form a fourth. When you cluster by failure mode, you see which types of failures your test suite is weakest at catching. A legal tech company clustered six months of bugs and discovered that 40 percent of all bugs involved the model ignoring specific constraints in the prompt. Their test suite had extensive coverage for correctness but almost no coverage for constraint compliance. They added a test category for constraint-following and saw bugs in that cluster drop by 70 percent.

Clustering by input characteristics groups bugs based on what triggered them. Long inputs form one cluster. Multilingual inputs form another. Inputs with special characters form a third. Inputs with ambiguous phrasing form a fourth. When you cluster by input characteristics, you see which types of inputs are underrepresented in your test suite. A customer support AI clustered their bugs and found that 30 percent involved user inputs with profanity, sarcasm, or aggressive tone. Their test suite had polite, well-formed inputs but almost no adversarial or emotional inputs. They generated 500 test cases with challenging tone and phrasing. Bugs triggered by user frustration dropped by 60 percent.

Clustering by model component groups bugs based on where in the system the failure occurred. Retrieval bugs form one cluster. Prompt formatting bugs form another. Output parsing bugs form a third. When you cluster by component, you see which parts of your pipeline are most fragile. A healthcare AI found that 50 percent of bugs involved the retrieval step pulling incorrect or irrelevant documents. The model itself was working correctly, but it was being fed wrong context. They added component-specific regression tests that validated retrieval quality independently from model output quality. Retrieval-related bugs dropped by 80 percent.

The clustering analysis should happen quarterly. Review all bugs from the past three months, assign each bug to one or more clusters, and calculate the percentage of bugs in each cluster. Any cluster that accounts for more than 15 percent of bugs represents a systematic test coverage gap. Add tests for that cluster until its share drops below 10 percent, then reanalyze next quarter.

## The Bug-Test Registry

The bug-test registry is a database that links every bug to the regression test that prevents its recurrence. This is not just documentation. This is the institutional memory that ensures you never fix the same bug twice.

The registry has five fields. First, bug ID: the unique identifier from your bug tracking system. Second, bug description: a human-readable summary of what was broken. Third, test ID: the unique identifier of the regression test that covers this bug. Fourth, test location: the file path and function name where the test lives. Fifth, test status: whether the test is passing, failing, or disabled, and why.

A fintech company built their bug-test registry as a table in their internal wiki. Every time a bug was fixed, the engineer who fixed it added a row to the table with the bug ID and the test ID. The table was searchable, so if a similar bug was reported later, engineers could search the registry to see if a related test already existed. The registry prevented duplicate test creation and surfaced patterns. When three different bugs all linked to tests in the same module, it signaled that the module had weak coverage and needed broader regression testing.

The registry also tracks test coverage completeness. For every closed bug in your bug tracking system, the registry shows whether a corresponding regression test exists. The coverage percentage is the number of bugs with linked tests divided by the total number of closed bugs. The target coverage depends on bug severity. For critical bugs, coverage should be 100 percent. For high-severity bugs, coverage should be above 90 percent. For medium-severity bugs, coverage should be above 60 percent. If coverage is lower, it means bugs are being fixed without regression tests, and recurrence risk is high.

The registry enables test archaeology. When a test fails, you can look up why it was created. The test name might be obscure, but the registry links it to the original bug with full context. A customer support AI engineer encountered a failing test called "test_constrained_refund_policy_GB." The test name did not explain what it was testing or why. The engineer looked it up in the bug-test registry and found that it was created to prevent a bug where the model gave incorrect refund windows for UK customers. The context transformed the test from a mysterious failure into a clear requirement. The registry turned every test into a documented piece of institutional knowledge.

## Bug Recurrence Metrics

Bug recurrence is the ultimate measure of regression test effectiveness. If bugs keep coming back, your regression suite is not doing its job.

The recurrence rate is calculated per bug severity tier. For critical bugs, the recurrence rate should be zero. If a critical bug recurs, it means either the original fix was incomplete or the regression test is not running correctly. For high-severity bugs, the recurrence rate should be below 5 percent. A small number of high-severity bugs may recur due to complex interactions or edge cases, but the rate should be very low. For medium-severity bugs, the recurrence rate can be as high as 10 percent, depending on how aggressively you test edge cases.

A healthcare AI company tracked recurrence rate monthly. They defined recurrence as any bug that was marked fixed, then reopened with the same root cause within six months. They measured recurrence rate separately for each severity tier. When the recurrence rate for high-severity bugs exceeded 5 percent, they triggered a review of the regression testing process. The review asked three questions: Are regression tests being created for every bug? Are the tests running in the right environments? Are the tests asserting the right conditions? The answer was usually no to one of these three, and the fix was straightforward.

Recurrence rate also measures test maintenance quality. A test that was created but is now skipped, disabled, or ignored does not prevent recurrence. A legal tech company discovered that 15 percent of their regression tests were disabled due to flakiness or environment issues. Their recurrence rate for bugs covered by disabled tests was 40 percent. Bugs were returning not because tests did not exist, but because the tests were not running. They launched a test reliability initiative to fix or remove every disabled test. Once the test suite was fully enabled again, recurrence rate dropped to 3 percent.

The time-to-recurrence metric measures how long it takes for a fixed bug to come back. Bugs that recur within days indicate that the fix was not tested thoroughly before merge. Bugs that recur within weeks indicate that the regression test is not running frequently enough. Bugs that recur within months indicate that the test exists but is not catching all variants of the failure mode. A fintech company found that bugs with a time-to-recurrence under one week almost always lacked regression tests entirely. They made regression test creation a merge-blocking requirement, and recurrence within one week dropped to zero.

## Bug Recurrence as a Leading Indicator of Test Debt

Bug recurrence is not just a quality metric. It is a leading indicator of test debt. When the same bugs keep coming back, it means your regression suite is not keeping pace with codebase complexity.

Test debt accumulates when fixes are deployed faster than tests are created. A customer support AI team was fixing 50 bugs per quarter but only creating 30 regression tests per quarter. The gap meant that 20 bugs per quarter were fixed without regression coverage. Over time, those 20 bugs accumulated into test debt. By the end of the year, 80 bugs had been fixed without tests. The recurrence rate climbed from 5 percent to 18 percent. The team realized they were accumulating debt and made a policy change: no bug could be marked as resolved until a regression test was merged. The recurrence rate dropped back to 6 percent within two quarters.

Test debt also accumulates when tests become outdated. A model update might change expected behavior in ways that make old regression tests irrelevant. If those tests are not updated or removed, they create maintenance burden without providing value. A healthcare AI company audited their regression suite and found that 25 percent of tests were validating behavior that no longer applied to the current model. Those tests were removed. The test suite became faster and more maintainable, and the recurrence rate did not change because the removed tests were not preventing real bugs.

Paying down test debt requires deliberate investment. A fintech company allocated 20 percent of each sprint to test debt reduction. Engineers reviewed the bug-test registry, identified bugs without regression tests, and wrote the missing tests. Over six months, they eliminated 200 untested bugs. The recurrence rate dropped from 12 percent to 4 percent. The investment was not glamorous, but it was the difference between chronic instability and reliable production behavior.

## From Bug Fix to Permanent Guard Rail

The regression test generation process is what turns every bug fix from a temporary patch into a permanent guard rail. The bug happens once. The test ensures it never happens again. This is the simplest and highest-leverage quality practice in software engineering.

A legal tech company tracked what they called the "bug immunity rate": the percentage of historical bugs that, if reintroduced into the codebase, would be caught by the current regression suite before reaching production. When they started systematically generating regression tests from bugs, their immunity rate was 30 percent. Most bugs that had been fixed in the past had no corresponding test. Over two years, they raised the immunity rate to 92 percent. Nearly every bug that had ever occurred was now a permanent test case. The recurrence rate dropped to less than one per year. The test suite had become immune to its own history.

Your regression suite is only as good as the bugs it has learned from. Every bug that escapes to production without generating a test is wasted learning. The bug taught you something about what can go wrong, and you threw that knowledge away by not encoding it as a test. The teams that build reliable systems do not throw knowledge away. They capture it, encode it, and ensure it compounds over time. One approach creates chronic bug recurrence. The other creates reliability that gets stronger with every fix.

Next, we examine how release cadence and regression test execution interact to determine your deployment velocity and quality.
