# 5.7 — Budget-Aware CI Execution

Every CI run has a cost. If you do not track it, it will track you. In early 2026, teams are running hundreds or thousands of CI jobs daily. Each job invokes evaluation pipelines. Each pipeline makes model calls. Each call burns tokens. Without cost tracking, your CI bill spirals from manageable to catastrophic in weeks. Budget-aware CI execution means knowing what every pipeline costs before you run it, enforcing limits that prevent waste, and routing evaluation traffic to the cheapest models that still meet your quality bar. This is not about penny-pinching. This is about sustainability at scale.

## The Cost Explosion Problem

AI CI is fundamentally more expensive than traditional software CI. A standard software CI job compiles code, runs unit tests, and checks linting. It costs cents per run. An AI CI job evaluates model outputs against hundreds of test cases. Each test case generates a prompt, invokes a model, captures a response, and runs multiple LLM-as-judge evaluations. At 2026 pricing, a single comprehensive eval run can cost five to fifty dollars. Multiply that by twenty pull requests per day, and you are spending three hundred to a thousand dollars daily on CI alone. Add in nightly regression suites, multi-model comparisons, and A-B testing, and the monthly bill reaches tens of thousands. Teams that do not track this spend discover it when finance asks why the model API bill tripled in a quarter.

The problem compounds because costs are hidden. Developers do not see a dollar amount when they trigger a CI run. They see a green check or a red X. The actual cost is buried in API logs, aggregated across teams, billed weeks later. Without real-time visibility, there is no feedback loop. A developer adds fifty new test cases to improve coverage, not realizing they just increased the per-run cost by fifteen dollars. Another developer switches from a fast model to a reasoning model for judge evaluations, tripling the cost without noticing. By the time someone realizes the budget is blown, hundreds of runs have already executed. Budget-aware CI makes costs visible at decision time, not invoice time.

## Per-Run Cost Tracking

Every CI job must report its total cost before exiting. This means instrumenting your pipeline to track tokens consumed, API calls made, and models invoked at each step. Modern LLM observability platforms provide this out of the box. You tag each request with metadata identifying the CI run, the test suite, and the engineer who triggered it. The platform aggregates token counts in real time and multiplies by model pricing to produce a per-run total. At the end of the job, the CI system logs the final cost and surfaces it in the pull request or build summary. The developer sees immediately: this run cost twelve dollars. The next time they modify the test suite, they know the baseline and can predict the delta.

Token-level tracking is non-negotiable. Input tokens and output tokens have different costs. Prompt caching, if available, reduces input token charges. Some models charge per million tokens, others per request. If your pipeline mixes models—GPT-5-mini for generation, Claude Opus 4.5 for judging, GPT-5-nano for summarization—you must track each model separately and sum the totals. Approximations fail. Estimating cost based on test case count or pipeline duration introduces error margins of fifty percent or more. The only reliable method is direct token measurement from the provider API response.

Metadata enrichment makes cost data actionable. Tag each API call with the test case ID, the evaluation dimension, the model used, and the CI job identifier. When a run exceeds budget, you can drill down to see which test cases drove the overage. Maybe the factual accuracy suite accounts for sixty percent of the cost because it uses the most expensive judge model. Maybe a single test case with a ten-thousand-token context dominates the spend. Without metadata, you see a total. With metadata, you see opportunities to optimize.

## Budget Limits and Enforcement

Budget limits prevent runaway costs before they happen. You define a maximum cost per pull request, per branch, per day, or per team. When a CI run approaches the limit, the system warns. When it exceeds the limit, the system halts further execution and fails the build with a budget error. This is harsh but necessary. A single runaway eval suite—perhaps someone accidentally committed a test case with a hundred-thousand-token document—can cost hundreds of dollars in minutes. Hard limits are the safety net.

Per-PR budgets are the most common pattern. You allow each pull request a budget of twenty dollars. This covers smoke tests, fast evals, and a reasonable subset of regression checks. If the developer needs more—maybe they are testing a major prompt rewrite—they can request a budget increase through a configuration file or a bot command. The key is that the default is constrained. Most PRs stay under budget. The ones that blow the limit are intentional, reviewed, and justified.

Per-day and per-team budgets add another layer of control. Even if individual PRs stay within limits, the aggregate can spiral if your team merges thirty PRs per day. A per-day budget caps total CI spend regardless of how many builds run. When the daily budget is exhausted, further builds queue or skip non-critical eval steps. This forces prioritization. Engineers merge the highest-priority changes first. Lower-priority PRs wait until the next day or use cheaper eval strategies.

Budget enforcement requires real-time tracking. You cannot wait until the run completes to check the cost. By then, the tokens are burned. Instead, your CI orchestration layer tracks cumulative cost as the pipeline progresses. After each eval step, it checks the running total against the limit. If the limit is exceeded mid-run, it skips remaining steps and fails the build. This requires instrumentation at the orchestration layer—your CI framework must integrate with the observability platform that tracks token costs.

## Cost-Quality Tradeoffs in CI

Not every CI stage requires the most expensive model. Smoke tests can use the cheapest available model that meets a minimum quality threshold. Fast evals prioritize speed and cost over perfect accuracy. Full regression suites, run nightly or before release, justify premium models. The trick is matching model cost to the decision being made. A smoke test that catches syntax errors or catastrophic failures does not need Claude Opus 4.5. GPT-5-nano works fine and costs a fraction. A release gate that blocks production deploys demands the highest-quality evaluation, regardless of cost.

Model routing for cost optimization is a deliberate architectural choice. Your eval framework supports multiple model tiers. Tier one uses the cheapest models—small, fast, inexpensive—for initial sanity checks. Tier two uses mid-range models for standard regression checks. Tier three uses frontier models for high-stakes evaluations. CI jobs declare which tier they need per stage. A typical PR pipeline runs tier one on every commit, tier two on approval, and tier three only on the release branch.

Budget-aware eval strategies adjust test coverage based on available budget. If a full regression suite costs fifty dollars and the PR budget is twenty dollars, you run a sampled version. Instead of evaluating all five hundred test cases, you evaluate a stratified sample of a hundred cases that cover the same distribution of inputs, task types, and risk categories. The sample is deterministic—hashed based on commit SHA or test case ID—so the same cases run on every commit to the same branch. This provides consistency while cutting cost by eighty percent. Full coverage runs nightly when budget constraints are relaxed.

Another strategy is dynamic batching. Instead of running eval as each test case completes, you batch ten or twenty cases into a single prompt and process them together. This reduces per-request overhead and takes advantage of prompt caching for shared context. Some models charge less per token at higher volumes. Batching trades latency for cost. Individual test feedback is delayed, but the total spend drops by twenty to forty percent.

## Budget Alerts and Attribution

Cost alerts prevent surprises. When a CI run exceeds seventy-five percent of its budget, the system posts a warning to the pull request. The developer sees it before the run completes and can cancel or adjust. When a team exceeds seventy-five percent of its weekly budget, engineering leadership gets a notification. This creates visibility before the problem becomes critical. Alerts should include actionable data: which test suites are most expensive, which models dominate the spend, which developers or PRs contributed the most.

Cost attribution assigns spend to the teams and projects that generated it. Every CI run is tagged with team identifier, project identifier, and cost center if applicable. At the end of the month, finance receives a breakdown: Team A spent twelve thousand dollars on CI, Team B spent eight thousand. Within Team A, Project X accounted for seventy percent of the cost. This level of granularity enables chargeback models in large organizations. It also creates accountability. Teams that optimize their eval strategies see their costs drop. Teams that run wasteful pipelines see their allocations scrutinized.

Attribution metadata flows from CI to billing. When your CI orchestration layer makes an API call, it tags the request with `team=platform-ai` and `project=assistant-v2`. The LLM provider logs these tags alongside token counts. Your observability platform aggregates by tag and produces per-team, per-project cost reports. This requires standardized tagging conventions across the organization. Without consistency, attribution becomes guesswork.

## Budget Rollover and Burst Capacity

Budget systems need flexibility for legitimate spikes. A team preparing a major release might need triple their normal CI budget for a week. A research project evaluating a new model architecture might burn budget rapidly during active development. Strict daily limits without rollover or burst capacity become a blocker. Budget-aware systems support two mechanisms: rollover and burst allowances.

Rollover allows unused budget from one period to carry forward. If your team has a weekly budget of five hundred dollars and only spends three hundred in week one, the remaining two hundred rolls into week two. This rewards efficiency and provides a buffer for high-activity periods. Rollover typically caps at one or two periods to prevent hoarding. You cannot accumulate six months of unused budget. But you can smooth out week-to-week variation.

Burst allowances permit temporary overages with approval. A team can exceed their daily budget by fifty percent if they provide justification and get manager or tech lead sign-off. The overage is deducted from future allocations. This enables urgent work without waiting for the next budget period. Burst requests are logged and reviewed quarterly to identify patterns. If a team bursts every week, their baseline allocation is too low. If they burst never, it might be too high.

## Model Routing for CI Cost Optimization

Model routing is the most effective cost lever. Instead of using a single model for all eval tasks, you route each task to the cheapest model that meets the quality requirement. Syntax validation uses a nano model. Coherence checks use a mini model. Factual accuracy and reasoning tasks use premium models. The routing logic lives in your eval framework. Each test case declares its required quality tier. The framework selects the model from that tier based on current availability, cost, and latency.

Cost-based routing evolves with pricing changes. In early 2026, GPT-5-nano is the cheapest option for simple tasks. If a competitor releases an even cheaper model with comparable quality, your routing rules update automatically. You maintain a model registry that tracks per-token costs and quality benchmarks. When a new model enters the registry, the routing engine evaluates whether it should replace an existing model in any tier. This keeps costs optimized as the market shifts.

Quality thresholds prevent over-optimization. You do not route to the absolute cheapest model if it fails to meet a minimum accuracy bar. Each tier has a quality floor measured against a validation set. If a model drops below the floor, it is removed from the tier. This prevents cost optimization from degrading eval reliability. The trade-off is explicit: tier one must achieve at least eighty-five percent agreement with human judgment. Tier three must achieve at least ninety-five percent. Any model meeting the threshold qualifies. The cheapest one wins.

## When to Use Cheaper Evaluation Strategies

Cheaper strategies are not a compromise. They are the right choice for specific contexts. Smoke tests do not need comprehensive coverage. They need fast feedback on catastrophic failures. A smoke test that runs in thirty seconds and costs ten cents is better than a full regression suite that runs in ten minutes and costs ten dollars—if both catch the same deploy-blocking bugs. The question is not whether to use cheap strategies, but when.

Deterministic checks should never invoke a model. Syntax validation, schema conformance, length checks, profanity filters—all of these can run as code-based assertions. They cost nothing except compute time. If your CI pipeline routes these checks to an LLM, you are burning money for no reason. The first optimization is to classify which checks are deterministic and remove them from the LLM eval path entirely.

Cached evaluations eliminate redundant cost. If you evaluate the same prompt-response pair multiple times across different CI runs, cache the result. The cache key includes the prompt hash, the model identifier, the evaluation criteria, and the judge model. When a test case reruns with identical inputs, the cached result is returned instantly at zero marginal cost. Cache hit rates of sixty to eighty percent are common in active development. This cuts eval costs nearly in half.

Sampling strategies reduce coverage to fit budget. Stratified sampling ensures you test a representative subset. Adaptive sampling increases coverage for high-risk areas and decreases it for stable, low-risk areas. Hash-based sampling provides determinism—the same commit always evaluates the same sample. These techniques trade exhaustive coverage for fiscal sustainability. The key is ensuring the sample still provides enough confidence to make the merge-or-block decision.

## The Monday Morning Question

When a pull request fails the budget check, the developer should know exactly why and what to do about it. The error message includes the total cost, the budget limit, the most expensive test cases, and suggested optimizations. It does not just say "budget exceeded." It says: "This run cost twenty-eight dollars. Your PR budget is twenty dollars. The top three test cases accounted for fifteen dollars. Consider reducing context size or using a cheaper judge model for non-critical dimensions." Actionable feedback turns budget enforcement from a blocker into a teaching moment.

Budget-aware CI is not about restricting quality. It is about making cost visible so teams can make informed trade-offs. The most disciplined organizations treat CI budget like production cost—tracked, analyzed, and optimized continuously. The result is sustainable evaluation practices that scale without surprise invoices.

Next, you need to store every result, compare it against baselines, and trace regressions back to the commit that introduced them—that is the role of result storage and comparison systems.
