# 5.1 — Why CI/CD for AI Is Different

Most teams think CI/CD for AI systems is just traditional CI with different tests. They are wrong. The fundamental assumptions that make traditional continuous integration work—tests are fast, tests are cheap, tests are deterministic—all collapse when the system under test is an LLM application. Teams that apply conventional CI patterns without adaptation end up with pipelines that either run too slowly to provide feedback or cost so much to operate that developers start bypassing them.

## The Determinism Problem

Traditional software testing assumes repeatability. Run the same test twice on the same code, get the same result. This assumption enables every optimization in modern CI systems. You can cache test results. You can skip tests when files haven't changed. You can parallelize without worrying about coordination. You can treat test failures as signal, not noise.

AI systems produce different outputs on identical inputs. The same prompt with the same model at the same temperature setting can return different responses. This is not a bug. It is how the systems work. Temperature above zero means sampling from a probability distribution, not selecting a deterministic answer.

The immediate consequence: traditional test result caching does not work. You cannot skip running an eval because the code hasn't changed, because the eval might produce a different result this time. You cannot assume a test that passed yesterday will pass today, because the model might sample differently.

This creates what looks like flakiness. A PR shows green tests. Someone re-runs the pipeline an hour later without changing code. Two tests fail. The developer assumes the tests are broken and merges anyway. Three days later, a user reports the exact issue the flaky tests caught.

The deeper problem is that flaky tests train teams to ignore test failures. When tests fail unpredictably, "just re-run it" becomes standard procedure. That habit persists even when a test is catching a real regression. The CI system loses credibility one false alarm at a time.

## The Speed Problem

A typical unit test suite for a backend service runs in seconds. Integration tests might take a minute or two. The full suite completes in under five minutes. Developers get feedback while the change is fresh in their minds. This tight feedback loop is what makes test-driven development practical.

A comprehensive eval suite for an LLM application takes minutes to hours. You are not checking if a function returns the correct integer. You are running hundreds of prompts through a model, calling an LLM-as-judge to score each response, aggregating results, comparing against baseline, and flagging regressions. Each eval case might take five to thirty seconds. A 500-case regression suite takes 40 minutes to two hours.

That time lag changes developer behavior. When tests take two hours to run, developers do not run them locally before pushing. When CI takes two hours to complete, developers move on to other work while waiting for results. When they finally get feedback, they are deep in a different problem. Context switching back to fix the regression costs another 30 minutes. The fix triggers another two-hour wait.

The result: developers batch changes. Instead of small, frequent commits that are easy to review and easy to revert, you get large PRs that accumulate three days of work because running evals after every small change is too slow. The batch increases risk. A regression could be in any of the twelve commits in the PR. Debugging which commit introduced the problem takes longer than if you had caught it immediately.

## The Cost Problem

Traditional tests cost nothing except compute time. Running a million unit tests costs a few dollars of cloud compute. You can run the full suite on every commit without worrying about budget.

AI evals cost real money per execution. You are calling production LLM APIs. If you evaluate with GPT-5, every eval case costs you a fraction of a cent to several cents depending on prompt and output length. A 1,000-case regression suite might cost five to twenty dollars per run.

Run that suite on every commit across a team of 20 developers making five commits per day, and you are spending 500 to 2,000 dollars per day on CI. Scale that to a 200-person engineering org and the cost becomes unsustainable. Teams either disable the comprehensive evals or add so many restrictions on when they run that they stop catching regressions effectively.

The cost pressure creates perverse incentives. Developers learn which file changes trigger expensive evals and start bundling unrelated changes to avoid multiple runs. Product managers start questioning whether evals are worth the cost. Finance asks Engineering to justify the CI budget. The pipeline that was supposed to enforce quality becomes a political problem.

## The Baseline Problem

Traditional tests compare output to a known-correct answer. You call a function with specific inputs and assert that the output matches the expected value. The expected value is written in the test. The comparison is exact. Pass or fail is unambiguous.

AI evals compare output to a baseline that might have changed. Your baseline is usually "whatever the current production system returns" or "whatever the model returned last week." But production systems change. Models get updated. Prompt templates evolve. The baseline you recorded last Tuesday might not be valid today.

This creates a question that does not exist in traditional testing: when you detect a difference, is that a regression or is the baseline stale? If the baseline was recorded three months ago and the model has improved since then, the new output might actually be better. Flagging it as a regression is wrong. But if you assume all differences are improvements, you will miss real regressions.

The problem compounds when you have multiple engineers working on different features. One team updates the prompt template, which changes baseline outputs for eval cases their change does not touch. Another team's PR now shows dozens of false regressions because the baseline does not reflect the template change. They do not know if they should update the baseline or fix their code. Neither does the CI system.

## The Judge Problem

Traditional tests use deterministic assertions. You check if a string equals another string, if a number is greater than a threshold, if an array contains an element. The comparison logic is simple, fast, and unambiguous.

AI evals often use LLMs to judge LLM outputs. You send the model's response to another model and ask "is this answer correct?" The judge model is itself probabilistic and fallible. It might rate an incorrect answer as correct or a correct answer as incorrect. It might rate the same answer differently on two evaluations.

This introduces circular dependency. You are using an AI system to validate an AI system. When the judge disagrees with human judgment, which one is wrong? When the judge's rating changes between runs, is that noise in the judge or signal about the output? When you update the judge model to a newer version, all your historical eval results become incomparable.

The cost problem also hits harder. Every eval case now requires at least two LLM calls: one to generate the output, one to judge it. If your judge uses chain-of-thought reasoning to improve accuracy, that is even more tokens. The cost per eval case doubles or triples. The time per eval case increases proportionally.

## The Flakiness Spiral

All these problems combine to create flaky tests. A test passes on one run and fails on the next with no code changes. The output was different this time. The judge rated it differently. The baseline comparison threshold was borderline and stochastic variation tipped it over.

Flaky tests destroy trust in CI. When developers see tests fail and pass randomly, they stop treating failures as blocking issues. They re-run the pipeline until it passes. They mark flaky tests as known-flaky and stop investigating. They start merging PRs with test failures, assuming the failures are noise.

This cultural shift is the most dangerous outcome. Once the team stops trusting the CI system, regressions slip through. The pipeline that was supposed to catch problems before they reach production becomes a checkbox exercise. Developers run it because it is required, not because they believe the results.

The irony is that AI systems need stricter quality gates than traditional software, not looser ones. An incorrect function return value might break one feature. A regressed LLM prompt can generate wrong answers for thousands of users, expose PII, violate content policy, or produce legally problematic outputs. The stakes are higher, but the CI system is less reliable.

## Why Adaptation Is Required

You cannot port traditional CI practices directly. The patterns that work for deterministic, fast, cheap tests fail for probabilistic, slow, expensive evals. Pretending AI CI is just regular CI with different test commands results in pipelines that developers bypass, tests that lose credibility, and regressions that reach production.

The solution is not to abandon CI principles. It is to adapt them. Fast feedback is still critical, but you achieve it with tiered testing strategies, not by running the full suite on every commit. Test reliability is still essential, but you achieve it with statistical thresholds and baseline management, not with exact-match assertions. Cost control is still necessary, but you achieve it with smart triggering and resource management, not by skipping tests.

The next subchapter covers how to structure a pipeline that accounts for these constraints—one that provides fast feedback where speed matters, comprehensive coverage where risk is high, and stays within budget while maintaining developer trust.

## What It Looks Like When You Get It Right

A team at a financial services company rebuilt their AI CI pipeline in late 2025 after six months of fighting flaky tests and slow feedback. They implemented three-tier testing: fast smoke tests on every commit, comprehensive regression on PR approval, full golden set validation before production deploy. They added statistical result comparison to handle stochastic outputs. They instrumented cost tracking and set per-pipeline budgets.

The results were immediate. Developer velocity increased because feedback time dropped from 90 minutes to eight minutes for the fast tier. Regression catch rate improved because the team started trusting test failures again. CI costs dropped by 60 percent because they stopped running expensive evals unnecessarily. The pipeline became the quality gate it was supposed to be, not the obstacle developers tried to bypass.

The principle: CI/CD for AI systems requires different architecture, different triggering strategies, different result comparison, and different cost management. Adapting these elements lets you keep the benefits of continuous integration while accounting for the realities of probabilistic systems.

Pipeline architecture is where adaptation starts.
