# 1.8 — Reproducibility and Determinism Controls

Why do your tests show a regression on Monday but pass on Tuesday with no code changes? You re-run the exact same test suite against the exact same system version. Monday's results show three failures. Tuesday's results show zero failures. Nothing changed. The git commit is identical. The prompt is identical. The model is the same. The data is the same. Yet the results differ. You have discovered variance. And variance is the enemy of regression detection.

**Variance** is any source of randomness that causes the same test to produce different results across runs. Temperature sampling makes the model choose different tokens. Top-p sampling changes which tokens are even considered. Random seeds affect sampling order. Retrieval systems may return documents in different orders if scores are tied. Tool-calling systems may choose different tools if multiple tools are equally valid. Distributed systems may process requests in different orders due to network timing. Every source of variance adds noise to your test results. Noise hides regressions. A test that fails due to a real regression might pass on the next run due to lucky sampling. A test that should pass might fail due to unlucky sampling. You cannot trust the results. You cannot act on the failures. Your regression suite becomes unreliable.

The solution is to enforce determinism wherever possible and measure variance wherever determinism is impossible. **Determinism** means the system produces identical outputs given identical inputs. Deterministic tests are reproducible. You run the test Monday. You run the test Tuesday. You get the same result. If the result changes, you know something in the system changed — the code, the prompt, the model, the config. You have a real signal. You can act on it.

Reproducibility is not just a debugging convenience. It is the foundation of regression testing. If your tests are not reproducible, you cannot distinguish between signal and noise. A 5-point accuracy drop might be a real regression or it might be sampling variance. You do not know. You cannot know. Without reproducibility, regression testing is just expensive coin flipping.

## The Variance Tax

A fintech company ran a regression suite of 120 tests every morning. Each test called GPT-5 with temperature set to 0.7. The suite took 40 minutes to run. The results varied from day to day. Some mornings, 110 tests passed. Other mornings, 115 tests passed. Some tests flipped between pass and fail across consecutive runs with no system changes. The team responded by re-running failed tests three times. If a test passed on any of the three attempts, they marked it as passing. If it failed all three times, they investigated. This strategy reduced false alarms. It also tripled test runtime to two hours and introduced a 14 percent false-negative rate — real regressions that failed once, passed on retry, and were ignored.

The variance tax is the cost of operating a non-deterministic test system. You pay it in three currencies: **time**, **confidence**, and **missed regressions**. Time costs come from re-running flaky tests. Confidence costs come from not trusting your test results. Missed regressions come from ignoring failures that might be real because they might also be noise. A team running 500 tests with 10 percent variance will see 50 tests flip randomly. If you investigate every flip, you waste engineering time. If you ignore flips, you miss real problems.

The variance tax compounds with test volume. A team running ten tests can manually review every failure. A team running 500 tests cannot. They need automation. They need alerting. They need confidence that a failure means something. Variance breaks confidence. When 10 percent of your failures are noise, you stop trusting the other 90 percent. You start ignoring alerts. You start assuming failures are false positives. Eventually, a real regression reaches production because everyone assumed the alert was noise.

The alternative is to eliminate variance. Set temperature to zero. Fix random seeds. Lock sampling parameters. Make retrieval ordering deterministic. Remove every source of randomness you can control. The result is a test suite where every failure means something. Engineers trust the alerts. They investigate immediately. They catch regressions before they reach production. The variance tax disappears.

## Temperature Zero and the Determinism Floor

Most language models support a temperature parameter that controls output randomness. Temperature zero means deterministic sampling — the model always picks the highest-probability token at each step. Temperature above zero means stochastic sampling — the model samples from a probability distribution, and the same input can produce different outputs. For exploratory use cases, stochastic sampling is valuable. For regression testing, it is poison.

Setting temperature to zero is the single most effective determinism control for language model systems. It eliminates the primary source of variance in most AI systems. If your test suite runs with temperature zero, the model's output becomes deterministic given the same input, prompt, and model version. You run the test Monday. You run the test Tuesday. You get the same output. If the output changes, you know the prompt, model, or input changed. You have a reliable signal.

Some models do not support perfect determinism even at temperature zero due to floating-point precision, parallel processing, or non-deterministic hardware acceleration. But the variance at temperature zero is orders of magnitude lower than the variance at temperature 0.7. A system running at temperature 0.7 might produce ten different outputs for the same input. A system running at temperature zero might produce two outputs that differ by a single token due to floating-point rounding. The first is unusable for regression testing. The second is acceptable.

The objection teams raise is that production traffic runs at non-zero temperature and testing at temperature zero does not reflect real behavior. This objection is valid but misunderstands the purpose of regression testing. Regression testing does not simulate production traffic. It detects whether your changes broke something that used to work. If your prompt change causes quality to drop at temperature zero, it will also cause quality to drop at temperature 0.7 — the variance will just make the drop harder to measure. Temperature zero removes noise. It makes regressions visible. Once you confirm the change is safe at temperature zero, you can sample at production temperature to confirm the change is also safe under variance. But the first gate is determinism.

## Seed Locking and Sampling Control

Even at temperature zero, some models exhibit minor variance due to random seed differences. The seed controls the random number generator used for sampling, and different seeds can produce different outputs even when temperature is zero if the model uses any stochastic operations internally. Setting a fixed seed ensures that the random number generator produces the same sequence of values across test runs. Combined with temperature zero, seed locking provides the strongest determinism guarantee most models can offer.

Seed locking works by passing a seed parameter to the model API: you set the seed to a fixed value like 42 or 12345, and every test run uses the same seed. The model's sampling behavior becomes reproducible. If the output changes, you know the model version, prompt, or input changed. You have eliminated sampling variance as a confounding variable. Some model APIs do not expose seed control. In those cases, temperature zero is your only determinism lever. Use it.

**Top-p sampling** is another variance source. Top-p, also called nucleus sampling, restricts sampling to the smallest set of tokens whose cumulative probability exceeds a threshold. If top-p is set to 0.9, the model only samples from tokens that together account for 90 percent of the probability mass. This introduces variance because the set of tokens under consideration changes depending on the model's confidence. Two test runs might sample from different token sets even with the same seed. For regression testing, set top-p to 1.0 or disable it entirely. This forces the model to consider all possible tokens, reducing variance.

Some models also support frequency penalties, presence penalties, or repetition penalties. These parameters adjust token probabilities to discourage repetition. They introduce variance because the penalty depends on what the model has generated so far, and small differences in early tokens can cascade into large differences later. For regression testing, set all penalties to zero. You want the model's raw behavior, unmodified by anti-repetition heuristics.

## When Determinism Is Impossible

Some systems cannot be made fully deterministic. Retrieval systems that query external databases may return results in different orders if multiple documents have identical scores. Tool-calling systems that dispatch to external APIs may receive different responses depending on API state, rate limits, or network conditions. Multi-agent systems may exhibit different execution orders depending on timing and parallelism. Distributed systems may process requests in different orders due to thread scheduling or network latency. In these cases, perfect determinism is impossible. The question becomes: how do you test a system that is inherently non-deterministic?

The answer depends on whether the non-determinism is in the AI component or in the surrounding infrastructure. If the non-determinism is in infrastructure — retrieval ordering, API response times, database race conditions — you can often isolate it by mocking or stubbing external dependencies. Replace the live retrieval system with a deterministic mock that always returns documents in the same order. Replace the live API with a fixture that always returns the same response. Replace the live database with an in-memory store that guarantees ordering. This removes infrastructure variance and isolates the AI behavior you want to test.

If the non-determinism is in the AI component itself — a model that does not support temperature zero, a model that has internal stochasticity even at temperature zero, a multi-model system where different models race to respond — you cannot eliminate variance. You can only measure it. Run each test multiple times, measure the distribution of outcomes, and track whether the distribution changes over time. If your test used to pass 95 out of 100 runs and now passes 70 out of 100 runs, you have detected a regression even though individual runs are noisy. This approach trades speed for reliability. Testing each case 100 times is slow. But it is better than ignoring variance and shipping regressions.

Some teams use **golden outputs** to handle non-deterministic systems. They run the system once, manually verify the output is correct, and store it as the golden reference. Future tests compare new outputs to the golden output using fuzzy matching or semantic similarity rather than exact equality. If the new output is sufficiently similar to the golden output, the test passes. This tolerates minor variance while still catching major regressions. The tradeoff is that fuzzy matching can miss subtle regressions and can flag false positives when the output changes in harmless ways. Golden outputs work best for high-level integration tests where you care about semantic correctness, not token-level precision.

## The Retrieval Ordering Hazard

Retrieval-augmented generation systems introduce a specific determinism challenge: document ordering. Your retrieval system ranks documents by relevance score. If two documents have identical scores, the system may return them in arbitrary order depending on database internals, index structure, or tie-breaking heuristics that are not specified. The model receives the top five documents. If the order changes, the model sees different context. The output changes. You have variance.

To make retrieval deterministic, enforce strict ordering with secondary tie-breakers. If two documents have the same relevance score, break the tie by document ID, timestamp, or lexicographic order. The specific tie-breaker does not matter. What matters is consistency: the same query always returns documents in the same order. Some retrieval systems support deterministic ordering modes. Others require you to implement tie-breaking in application logic. If your retrieval system does not support deterministic ordering and you cannot modify it, log the retrieved document IDs with every test result. If a test fails, check whether the retrieved documents changed. If they did, you know retrieval variance is a factor.

Another retrieval variance source is **index staleness**. If your retrieval index is rebuilt or updated during a test run, the same query may retrieve different documents before and after the rebuild. This is not a bug in your test. It is a real behavior change caused by index updates. But if the index update happens between a passing test run and a failing test run, you may incorrectly attribute the failure to a code or prompt change. To avoid this, lock the retrieval index version during regression testing. Run all tests against the same index snapshot. If you update the index, re-run the full regression suite and compare results. Treat index updates as a system change, not as invisible infrastructure churn.

## The Tool-Calling Stochasticity Problem

Agentic systems that call tools introduce variance through tool selection and parameter generation. The model decides which tool to call based on the user request. If multiple tools are equally valid, the model may choose different tools across runs. Even if the model chooses the same tool, it may generate different parameter values, especially if the parameters are generated at non-zero temperature. The tool executes. The result is fed back to the model. The model generates a response. Every stochastic choice point cascades into downstream variance.

To reduce tool-calling variance, enforce deterministic tool schemas and parameter constraints. Define tools with narrow, unambiguous interfaces. If a tool accepts a date parameter, require a specific format. If a tool accepts a search query, log the exact query used. Run the model at temperature zero when generating tool calls. Some model APIs support structured output modes that force the model to generate valid JSON conforming to a schema. Use these modes. They reduce the chance that the model generates malformed tool calls that the system rejects and retries, which introduces variance.

For tools that call external APIs, mock the API responses during regression testing. If your tool calls a weather API, replace the live API with a fixture that always returns the same weather data. If your tool calls a database, replace the live database with a deterministic in-memory store. This removes external variance and isolates the model's behavior. You are no longer testing whether the weather API is reachable. You are testing whether your system calls the right tool with the right parameters and handles the response correctly.

## Variance Measurement and Statistical Testing

When determinism is impossible, you need statistical methods to detect regressions in the presence of noise. Run each test case multiple times — ten runs, fifty runs, or a hundred runs depending on how much variance you observe. Compute the mean accuracy, pass rate, or quality score across runs. Track the mean over time. If the mean drops significantly, you have detected a regression even though individual runs are noisy.

**Statistical significance testing** helps distinguish real regressions from random fluctuations. If your test used to pass 90 percent of the time and now passes 85 percent of the time, is that a real drop or just sampling noise? A t-test or a chi-squared test can tell you. If the difference is statistically significant at p less than 0.05, you have strong evidence of a regression. If the difference is not significant, you do not have enough data to conclude anything. You need more test runs or a larger sample size.

Some teams track variance as a first-class metric. They measure the standard deviation or coefficient of variation for each test across runs. If variance increases, something changed — not necessarily the quality, but the stability. A test that used to pass 95 out of 100 times with low variance and now passes 90 out of 100 times with high variance has become less reliable even if the mean only dropped five points. High variance is a signal. It means the system is sensitive to some uncontrolled factor. You should investigate.

Variance measurement is slower than deterministic testing. If you run each test 50 times instead of once, your test suite is 50 times slower. This is acceptable for critical tests that guard high-risk changes. It is not acceptable for every test in a 5,000-test suite. The pragmatic approach is to run most tests deterministically at temperature zero and run a smaller subset of tests stochastically at production temperature to measure variance. The deterministic tests catch most regressions. The stochastic tests validate that your system is stable under variance. You need both.

## The Determinism Trade-Off

Determinism makes tests reliable. It also makes them less realistic. Production traffic is not deterministic. Real users receive non-zero temperature outputs. Real systems experience retrieval variance, API latency, and race conditions. If your regression tests run in a perfectly deterministic sandbox, they may miss problems that only appear under variance. A prompt that works perfectly at temperature zero may produce gibberish at temperature 0.9. A retrieval system that works when documents are always in the same order may break when tie-breaking is random.

The answer is not to abandon determinism. The answer is to test in layers. **Layer one** is deterministic regression testing. Temperature zero, fixed seeds, mocked dependencies, locked index versions. These tests run fast, produce reliable results, and catch most regressions. They are your safety net. You run them on every pull request. If they fail, you do not merge. **Layer two** is stochastic integration testing. Production temperature, live dependencies, real variance. These tests run slower, produce noisier results, and catch problems that only appear under realistic conditions. You run them nightly or before major releases. If they show concerning variance increases or quality drops, you investigate.

Most regressions are caught by layer one. Layer one is deterministic, fast, and trustworthy. Layer two catches the edge cases — the prompt that works at temperature zero but degrades at temperature 0.8, the retrieval system that breaks when documents tie, the tool-calling system that fails when the API is slow. You need both layers. Determinism is not about making tests unrealistic. It is about making tests reliable. Once you have reliability, you add realism in controlled doses.

## Reproducibility as a Release Gate

Reproducibility is not just a debugging tool. It is a quality standard. If your test results are not reproducible, your system is not ready to ship. Non-reproducibility means your system's behavior is unpredictable. Unpredictable systems cannot be trusted in production. Before you deploy a change, you should be able to run your regression suite twice and get the same results both times. If you cannot, you have uncontrolled variance. Uncontrolled variance means you cannot confidently predict what your system will do when it reaches users.

Some teams enforce reproducibility as a release gate. A change cannot be deployed until the regression suite has been run three times with identical results. If any test produces different results across the three runs, the change is blocked. The team investigates the variance source, adds determinism controls, and re-runs. This gate prevents flaky tests from reaching production. It also creates pressure to fix variance sources rather than tolerating them.

Reproducibility gates work best for systems that can achieve near-perfect determinism. If your system is inherently stochastic, a strict reproducibility gate is not practical. But even stochastic systems can enforce statistical reproducibility: the mean and variance across multiple runs must be stable. If the mean or variance shifts across test batches, the change is blocked. The principle is the same: before you ship, you need evidence that your system behaves predictably.

Reproducibility is the foundation of trust in regression testing. Without it, you are flying blind. With it, you can detect regressions early, attribute failures to specific changes, and deploy with confidence. The next step is to lock down the specific technical parameters that create reproducibility — seed control, temperature settings, and sampling determinism configurations.
