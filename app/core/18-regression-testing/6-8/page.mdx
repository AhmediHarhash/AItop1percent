# 6.8 — Token Explosion Detection

In October 2025, a legal tech company deployed a prompt change to improve citation formatting. The change added one instruction: "Include all relevant case citations with full context." The intention was to make citations more complete. The result was catastrophic. Output tokens increased from an average of 450 per response to 2,300 per response. The model began including not just case names but full paragraph excerpts from each cited case, sometimes four or five paragraphs per citation, sometimes eight citations per response. Within 18 hours, the system had processed 34,000 requests at the new token rate. The cost for those 18 hours exceeded the cost for the previous two weeks. The bill was 67 thousand dollars higher than it should have been. The team discovered the issue when their usage alert fired. By then, the damage was done. They rolled back immediately. The post-mortem question was simple: why did this deploy without a token gate?

Token explosion is the sudden, unexpected increase in token usage caused by a prompt change, a model behavior change, or a loop condition failure. It is the single most common source of cost regression in LLM-based systems. It is also the most preventable. Token explosions do not happen silently. They leave clear signals. Input token counts spike. Output token counts spike. The total tokens per request doubles or triples or increases by 5 times. These signals are detectable before deployment. The teams that catch token explosions in CI never experience them in production. The teams that skip token gates experience them every three to six months, always with the same shock, always with the same question: how did this get through?

## What Token Explosion Looks Like

Token explosion manifests in three patterns. First, input token explosion. Your prompt gets longer. You add new instructions. You add more examples. You add more context from retrieval. You add system messages. Each addition seems reasonable. The sum is catastrophic. Your baseline prompt was 800 tokens. The new prompt is 2,400 tokens. You just tripled your input cost. If you handle 1 million requests per month, you went from 800 million input tokens to 2.4 billion input tokens. At 10 dollars per million tokens, you went from 8,000 dollars per month to 24,000 dollars per month. The feature you added cost you 16,000 dollars per month in additional input tokens. No one calculated this before shipping.

Second, output token explosion. The model generates longer responses. Sometimes this is intentional. You changed the prompt to ask for more detail, and the model complied. Sometimes this is unintentional. You changed one instruction, and the model interpreted it as a request for verbosity. You added "be thorough," and the model started writing essays. You added "include examples," and the model started generating three examples per point. You added "provide context," and the model started explaining background information that users never asked for. Output tokens increased from 400 to 1,200. You tripled your output cost. Output tokens are more expensive than input tokens. If output tokens cost 30 dollars per million and you handle 1 million requests per month, you went from 12,000 dollars per month to 36,000 dollars per month. The single word "thorough" cost you 24,000 dollars per month.

Third, combined explosion. Input and output both increase. This is the worst case. Your prompt got longer and your responses got longer. You added retrieval context, which increased input tokens by 60 percent. You added instructions for detail, which increased output tokens by 80 percent. Your total cost-per-request went from 0.05 dollars to 0.14 dollars. You increased cost by 180 percent. If you process 500,000 requests per month, you went from 25,000 dollars per month to 70,000 dollars per month. Your quarterly budget was 75,000 dollars. You will exceed it in five weeks. The explosion is not a spike. It is the new normal until someone notices and rolls back.

Token explosions are not always obvious in aggregate metrics. A 5 percent increase in median tokens is normal variance. A 10 percent increase might be traffic mix shifting toward longer queries. A 50 percent increase is definitely a problem. But the explosion often appears first in percentiles. The median might increase by 15 percent while the 95th percentile increases by 200 percent. This happens when the prompt change affects only certain query types. Simple queries are unaffected. Complex queries trigger the new behavior. The median does not move much because most queries are simple. The tail explodes because complex queries are now catastrophically expensive. You need gates at multiple percentiles. Monitoring only the median misses tail explosions.

## Causes: Prompt Changes, Model Behavior Changes, Loop Conditions

Prompt changes are the most common cause. You add an instruction, you reorder instructions, you rephrase a constraint, you add examples. Each change affects token usage. The change is usually intended to improve quality. It often succeeds at improving quality. It also often succeeds at increasing cost. The team focuses on the quality improvement. They celebrate the accuracy gain. They ship the change. Three days later, Finance asks why the AI budget is 40 percent over forecast. The answer is that the quality improvement came with a cost increase, and no one checked the cost before shipping.

Model behavior changes are the second most common cause. You switch from GPT-5-mini to GPT-5. You switch from Claude Sonnet 4.5 to Claude Opus 4.5. You switch from Gemini 3 Flash to Gemini 3 Pro. The new model is more capable. It is also more verbose. The same prompt produces longer responses. For some models, the verbosity is dramatic. A prompt that produced 300 tokens of output on GPT-5-mini might produce 800 tokens on GPT-5. A prompt that produced 500 tokens on Claude Sonnet 4.5 might produce 1,400 tokens on Claude Opus 4.5. You switched models to improve quality. You succeeded. You also tripled your output token cost. If you did not measure token usage in staging before deploying the model switch, you discovered the cost increase in production.

Loop condition failures are the rarest cause but the most catastrophic. Your system uses a loop to refine responses. Generate a response, check if it meets criteria, regenerate if it does not, repeat until success or max iterations. This is a common pattern for structured output generation, for validation-heavy tasks, for multi-step reasoning. The loop has a maximum iteration count. The baseline is that 90 percent of requests succeed on the first try, 8 percent succeed on the second try, 2 percent succeed on the third try. The loop rarely reaches max iterations. Then you change the validation criteria. Suddenly, 40 percent of requests fail the first attempt. 25 percent fail the second attempt. 15 percent reach max iterations. Your token usage increased by 60 percent because every request now requires more attempts. The cost increase is invisible in per-request logs because each individual LLM call looks normal. The cost increase is visible in aggregate logs because you are making more calls per request.

Model updates from providers can also cause token explosions, though less commonly. OpenAI, Anthropic, Google, and other providers occasionally update their models. The model name stays the same, but the behavior changes. A model update might make responses longer, more detailed, or more verbose. You did not change anything in your code. The model changed underneath you. Your token usage increased by 20 percent overnight. This is rare because providers test updates carefully. But it happens. The defense is to monitor token usage continuously and to have alerts for sudden increases. If your median output tokens increase by more than 10 percent in one day, investigate. It might be a traffic shift. It might be a provider model update. Either way, you need to know.

## Input Token Regression and Output Token Regression

Input token regression happens when your prompts get longer. The most common driver is retrieval. You add a RAG component. Each query now includes 1,200 tokens of retrieved context. Your baseline input was 600 tokens. Your new input is 1,800 tokens. You tripled input tokens. This might be acceptable. Retrieval reduces hallucinations, improves accuracy, provides source attribution. The cost increase might be worth it. But you need to measure it. You need to know that retrieval adds 1,200 tokens. You need to know that tripling input tokens costs X dollars per month. You need to decide whether the quality gain is worth X dollars. If you do not measure input tokens before deploying retrieval, you cannot make this decision. You discover the cost after the fact.

Input token regression also happens through instruction creep. Your prompt starts as three sentences. Over six months, you add one instruction per week. Each instruction is small. Each solves a real problem. After six months, your prompt is 47 instructions. Your input tokens increased from 80 to 940. You increased input cost by 11 times. No single change was dramatic. The sum was catastrophic. This is why you need input token gates. Each prompt change is checked against baseline. If input tokens increase by more than 10 percent, the gate fails. You are forced to justify the increase or to find a more concise way to express the instruction. Instruction creep is invisible without gates. With gates, it is visible and controllable.

Output token regression happens when responses get longer. The most common driver is prompt phrasing. You change "answer concisely" to "provide a detailed answer." Output tokens double. You change "list the key points" to "explain each point with examples." Output tokens triple. You change "yes or no" to "yes or no, and explain your reasoning." Output tokens increase by 10 times for queries where the reasoning is complex. Every word in the prompt affects output length. "Thorough" makes responses longer. "Detailed" makes responses longer. "Comprehensive" makes responses longer. "Brief" makes responses shorter. "Concise" makes responses shorter. "Summarize" makes responses shorter. Prompt engineering is token engineering. You cannot optimize for quality without considering token cost.

Output token regression also happens when you remove constraints. Your baseline prompt includes "respond in 3 sentences or less." You remove this constraint to allow the model more flexibility. Output tokens increase by 150 percent because the model now generates 8 sentences instead of 3. The quality might improve. The responses might be more complete. But the cost increased by 150 percent. You need to measure this. You need to decide whether the flexibility is worth the cost. Some teams set soft constraints instead of hard constraints. Instead of "respond in 3 sentences or less," they use "respond concisely, typically in 2 to 4 sentences." This guides the model without enforcing a strict limit. Output tokens increase by 20 percent instead of 150 percent. The flexibility is there, but the cost is controlled.

## Token Count Gates: Max Input, Max Output, Max Total

Token count gates enforce hard limits on token usage. You define a maximum input token count, a maximum output token count, and a maximum total token count per request. Any request that exceeds these limits is rejected or truncated. These gates protect you from runaway token usage. They are the circuit breakers of token management. They ensure that no single request can consume 10,000 tokens or 50,000 tokens or 100,000 tokens. They cap your per-request cost at a known value.

The maximum input token limit is set based on your prompt design and your retrieval strategy. If your baseline prompt is 800 tokens and your retrieval adds up to 1,500 tokens, your expected input is 2,300 tokens. You set your max input limit at 3,000 tokens. This allows for variance. It allows for longer queries. It allows for occasional retrieval spikes. But it prevents a query from including 10,000 tokens of retrieved context due to a bug or a misconfiguration. If input tokens exceed 3,000, you truncate the retrieval context or you reject the request with an error. The user sees a message: "Your query is too complex. Please simplify." This is better than processing the request and paying for 10,000 input tokens.

The maximum output token limit is set based on your use case. If your baseline output is 500 tokens and your 95th percentile is 900 tokens, you set your max output limit at 1,500 tokens. This allows for longer responses when needed. It prevents the model from generating 5,000 tokens of output due to a prompt misinterpretation or a loop failure. Most LLM APIs accept a max tokens parameter. You set it to 1,500. The model stops generating after 1,500 tokens. The response might be incomplete. That is acceptable. An incomplete response costs 1,500 tokens. A complete response that runs to 5,000 tokens costs 5,000 tokens. You save 3,500 tokens. Over 1 million requests, you save 3.5 billion tokens. At 30 dollars per million tokens, you save 105,000 dollars. The incomplete response is the lesser evil.

The maximum total token limit is the sum of max input and max output. If max input is 3,000 and max output is 1,500, max total is 4,500. This is your per-request token budget. No request can exceed this budget. If a request tries to exceed it, you stop it. This budget protects your monthly spend. If you process 2 million requests per month and your max total is 4,500 tokens, your maximum possible token usage is 9 billion tokens. If your token cost is 20 dollars per million tokens average (blended input and output), your maximum possible monthly cost is 180,000 dollars. This number is calculable. It is predictable. It is defensible to Finance. Without token limits, your maximum possible monthly cost is unbounded. A single bug can consume your entire annual budget.

## Detecting Gradual Token Creep vs Sudden Explosions

Gradual token creep is the slow increase in token usage over weeks or months. Each week, tokens increase by 1 or 2 percent. No single change is dramatic. No single deploy triggers an alert. But over 12 weeks, tokens increase by 25 percent. Your cost-per-request went from 0.08 dollars to 0.10 dollars. Your monthly cost went from 80,000 dollars to 100,000 dollars. You are spending 20,000 dollars more per month, and no one noticed. Token creep happens because teams add features, add instructions, add context, and add retrieval without tracking cumulative token impact. Each addition is justified. The sum is expensive.

You detect token creep with trend analysis. You track median input tokens, median output tokens, and median total tokens per day. You compute a 7-day moving average. You set an alert for when the moving average increases by more than 5 percent compared to the previous month. If median tokens were 1,200 last month and they are 1,260 this week, your alert fires. You investigate. You review recent changes. You identify which changes contributed to the increase. You decide whether the increase is acceptable or whether you need to optimize. Token creep is invisible without trend tracking. With trend tracking, it is visible and addressable.

Sudden token explosions are the dramatic increase in token usage within hours or days. You deploy a change. Tokens double or triple. The explosion is immediate. It is obvious in dashboards. It is obvious in cost alerts. The challenge is detecting it before it reaches production. You detect explosions with pre-deployment token gates. You run your candidate release against your test suite. You measure tokens per request. You compare to baseline. If tokens increase by more than 20 percent at any percentile, the gate fails. You investigate. You identify the root cause. You fix the regression or you justify the increase. The gate ensures that explosions never reach production. They are caught in CI or in staging.

Some explosions are traffic-driven, not code-driven. Your code did not change. Your traffic changed. Users started asking longer queries. Users started using a feature that generates longer responses. Your tokens increased by 30 percent, and it is not a regression. It is a usage pattern shift. You cannot gate this in CI because CI tests your code, not your users. You detect this in production monitoring. You set alerts for sudden token increases. When the alert fires, you investigate. If the increase is code-driven, you roll back. If the increase is traffic-driven, you decide whether to optimize the system, to increase the budget, or to add user-facing token limits. Traffic-driven explosions are rare. Code-driven explosions are common. The gate stops the common case. Monitoring catches the rare case.

## Token Budgets Per Request Type

Different request types have different token budgets. A chat message should use 1,500 tokens total. A document summarization should use 8,000 tokens total. A code generation request should use 4,000 tokens total. A yes-no classification should use 200 tokens total. You set budgets for each request type separately. You enforce budgets separately. You monitor separately. Aggregating all request types into a single token budget hides the regressions.

The token budget for a request type is based on three factors. First, the baseline token usage. You measure current token usage for that request type. You compute the 95th percentile. That is your baseline. Second, the business value. A high-value request can afford a higher token budget. A low-value request needs a lower budget. If a document summarization request drives 50 dollars of revenue and costs 0.40 dollars in tokens, you can afford a token budget that costs 0.60 dollars if it improves quality. If a chat message drives 2 dollars of revenue and costs 0.05 dollars in tokens, you cannot afford a token budget that costs 0.15 dollars. Third, the user experience requirement. Some request types require completeness. A legal document summary cannot be truncated. A chat message can be. You set higher token budgets for request types where truncation harms UX.

You enforce token budgets at runtime. Each request type has a max token limit. The limit is passed to the LLM API as the max tokens parameter. If the request type is a chat message, max tokens is 800. If the request type is a document summary, max tokens is 3,000. The API enforces the limit. The model stops generating when it reaches the limit. This ensures that no request exceeds its budget. It caps your cost per request. It makes your total monthly cost predictable. Without per-request-type budgets, one expensive request type can consume your entire budget while other request types stay cheap. With per-request-type budgets, you control cost at the granular level.

## Emergency Token Limits

Emergency token limits are the absolute maximum token count you will allow under any circumstances. They are higher than your normal token budgets. They are the failsafe. They protect you from catastrophic bugs, from infinite loops, from prompt injection attacks that try to generate 100,000 tokens of output. You set emergency limits at 10 times your normal budget. If your normal max output is 1,500 tokens, your emergency limit is 15,000 tokens. If a request reaches the emergency limit, you terminate it immediately. You log the event. You alert the on-call engineer. You investigate.

Emergency limits are enforced at the infrastructure level, not just at the application level. You configure your API client to enforce a hard token limit. You configure your load balancer to terminate requests that exceed a duration threshold. You configure your monitoring system to alert on any request that uses more than 10,000 total tokens. This multilayer enforcement ensures that a bug in the application code cannot bypass the limit. Even if the application passes a max tokens parameter of 1 million to the API, the infrastructure limit stops it at 15,000. The request fails. The cost is capped. The damage is contained.

Emergency limits are rare triggers. In a healthy system, you might see one emergency limit trigger per month, usually caused by a malformed query or a rare edge case. If you see emergency limit triggers daily or weekly, your normal token budgets are too loose or your system has a bug. The emergency limit is not a substitute for proper token budgeting. It is the last line of defense. It catches the cases that should never happen. When it fires, you investigate immediately. You treat it as a production incident. You find the root cause. You fix it. The emergency limit is not normal. It is emergency.

## Token Explosion Post-Mortems

When a token explosion reaches production, you run a post-mortem. The post-mortem answers five questions. First, what changed? You identify the code change, the prompt change, or the model change that caused the explosion. You pull the commit. You review the diff. You understand what was added or modified. Second, why did it pass review? You identify whether the change was reviewed and whether token impact was considered during review. If token impact was not considered, you identify why. Was there no token gate? Was the gate disabled? Was the gate too loose? Third, how much did it cost? You calculate the total excess cost. You compare actual spend to expected spend. You present the number in dollars.

Fourth, how was it detected? You identify whether the explosion was detected by monitoring, by cost alerts, by user reports, or by accident. You measure the time from deployment to detection. If detection took 18 hours, you identify why it took so long. Was there no alert? Was the alert threshold too high? Was the alert ignored? Fifth, what prevents recurrence? You identify the systemic fix. You add a token gate if one did not exist. You tighten the gate threshold if it was too loose. You improve test coverage if the test suite missed the regression. You improve monitoring if detection was too slow. The post-mortem is not about blame. It is about learning. It is about making the next explosion impossible.

The highest-value output of a token explosion post-mortem is the updated runbook. The runbook documents the token budgets for each request type, the token gate thresholds, the monitoring alert thresholds, and the emergency response procedure. When the next engineer joins the team, they read the runbook. They understand that token explosions are a known risk. They understand how to prevent them. They understand how to detect them. They understand how to respond. The runbook is the institutional memory. It ensures that the lesson learned from the 67,000 dollar token explosion in October 2025 is not forgotten by the team that ships the next prompt change in March 2026.

Token explosions are preventable. They are detectable. They are measurable. The team that treats token usage as a first-class metric never experiences an explosion in production. The team that skips token gates experiences them repeatedly, always with surprise, always with the same question: how did this happen? The answer is always the same: you did not measure it before you shipped it. The next subchapter covers prompt length creep — the gradual inflation of prompt size that increases cost by 40 percent over six months without a single dramatic incident, and how to detect it before it consumes your margin.

