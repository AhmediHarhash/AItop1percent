# 6.7 — Cost-Per-Request Regression Thresholds

Quality gates without cost gates protect quality while bankrupting the business. Your model accuracy improved by 2 percentage points. Your precision went up. Your recall held steady. Every quality metric moved in the right direction. And your cost-per-request tripled. You shipped a change that will consume 47 thousand dollars more per month than the previous version. Finance discovered this three weeks after deployment when the invoice arrived. By then, you had processed 2.3 million requests at the new cost structure. The rollback conversation was excruciating.

Cost is not a secondary concern that you monitor after quality. Cost is a first-class regression metric that gets the same rigor, the same gates, the same enforcement as accuracy or latency. When a change makes requests more expensive, that is a regression. It might be an acceptable regression. It might be a worthwhile tradeoff. But it is never invisible. It is never discovered after the fact. You measure cost before deployment, you set thresholds before deployment, and you enforce gates before deployment. The release engineer who treats cost as optional is the one explaining to the CFO why the AI product line just blew through its quarterly budget in six weeks.

## Calculating Cost-Per-Request

Cost-per-request is not a vague directional metric. It is a specific dollar amount that you calculate for every request type in your system. For an LLM-based application in 2026, the calculation has three primary components. First, the API call cost. If you are using GPT-5, Claude Opus 4.5, or Gemini 3 Pro through a provider API, you pay per input token and per output token. The pricing is public and stable enough to use in gates. GPT-5 might cost 10 dollars per million input tokens and 30 dollars per million output tokens. Claude Opus 4.5 might cost 15 dollars per million input tokens and 75 dollars per million output tokens. For a request that sends 2,000 input tokens and receives 800 output tokens, the GPT-5 cost is 0.02 dollars plus 0.024 dollars, which equals 0.044 dollars per request. The Claude Opus 4.5 cost is 0.03 dollars plus 0.06 dollars, which equals 0.09 dollars per request. This is the baseline number. It is calculable. It is measurable. It is enforceable.

Second, the surrounding infrastructure cost. If you run retrieval before the LLM call, you pay for vector database queries. If you run an embedding model to encode the user query, you pay for that embedding call. If you run a classifier to route the request to the right model, you pay for that classification call. If you run post-processing to validate or transform the output, you pay for that compute. Each of these steps adds fractional cents to the request. Individually they seem negligible. Collectively they matter. A vector database query might cost 0.0001 dollars. An embedding call might cost 0.0002 dollars. A classifier call might cost 0.0005 dollars. The total infrastructure cost per request might be 0.001 dollars. When you handle 10 million requests per month, that infrastructure cost is 10 thousand dollars. It is not noise. It is budget.

Third, the hidden cost of failure. When a request fails and you retry it, you pay twice. When your prompt includes unnecessary context that you could have filtered, you pay for tokens you did not need. When your model generates a response that your validation layer rejects and you regenerate, you pay for both attempts. These failure costs are harder to track but they compound quickly. A system with a 5 percent retry rate effectively multiplies its cost by 1.05. A system with a 10 percent regeneration rate multiplies its cost by 1.10. A system with both multiplies by 1.155. If your base cost-per-request is 0.05 dollars, your effective cost is 0.058 dollars. Over 10 million requests, that is 80 thousand dollars in waste. Failure costs are invisible in per-request accounting but brutal in aggregate accounting.

The cost-per-request number you enforce in gates is the sum of these three components. API cost plus infrastructure cost plus expected failure cost. For a GPT-5-based system handling a standard RAG query, the total might be 0.05 dollars. For a Claude Opus 4.5-based system handling a complex reasoning task, the total might be 0.12 dollars. For a Gemini 3 Flash-based system handling a simple classification, the total might be 0.008 dollars. These numbers vary by request type. A chat message costs less than a document summarization. A yes-no classification costs less than a multi-turn dialogue. You calculate cost-per-request separately for each request type in your system. You set thresholds separately. You enforce gates separately. Aggregating all request types into a single cost metric hides the regressions.

## Setting Cost Baselines and Thresholds

The cost baseline is what you spend today. You run your current production system for one week. You log every request type. You calculate the cost for each request. You compute the median cost, the 95th percentile cost, and the 99th percentile cost for each request type. These three numbers form your baseline. The median tells you what a typical request costs. The 95th percentile tells you what an expensive request costs. The 99th percentile tells you what an outlier request costs. For a document summarization task, the baseline might be median 0.08 dollars, 95th percentile 0.14 dollars, 99th percentile 0.22 dollars. For a chat message, the baseline might be median 0.03 dollars, 95th percentile 0.06 dollars, 99th percentile 0.10 dollars. These baselines are not theoretical. They are empirical. They reflect real traffic, real prompts, real model behavior.

Once you have baselines, you set thresholds. The threshold is how much cost increase you will tolerate before blocking a deployment. The most common threshold is 10 percent above baseline median. If your baseline median is 0.08 dollars, your threshold is 0.088 dollars. If a candidate release produces requests that cost 0.09 dollars at the median, the gate fails. A 10 percent threshold is aggressive. It catches most meaningful regressions. It forces the team to justify cost increases. It prevents cost from drifting upward one percent at a time until you have doubled your spend over six months without noticing. Some teams use a 15 percent threshold. Some use 20 percent. The right number depends on your margin structure and your cost sensitivity. A high-margin product can tolerate 20 percent cost variance. A low-margin product needs 5 percent or tighter.

You also set absolute caps. An absolute cap is the maximum cost-per-request you will ever allow, regardless of baseline. For a product with a 10 dollar customer lifetime value, you might set an absolute cap of 0.50 dollars per request. For a product with a 100 dollar customer lifetime value, you might set an absolute cap of 2.00 dollars per request. For a product with a 1,000 dollar customer lifetime value, you might set an absolute cap of 10.00 dollars per request. The absolute cap prevents a catastrophic change from shipping. If someone accidentally switches the entire system from GPT-5-mini to GPT-5 or from Gemini 3 Flash to Gemini 3 Deep Think, the median cost might jump 10 times. A 10 percent threshold would not catch this. An absolute cap would. The cap is the circuit breaker. It stops the disaster before it reaches production.

You enforce thresholds at multiple percentiles. The median threshold catches changes that make all requests more expensive. The 95th percentile threshold catches changes that make expensive requests even more expensive. The 99th percentile threshold catches changes that create new outlier costs. A prompt change might leave the median untouched but increase the 95th percentile by 30 percent because it adds context that only matters for complex queries. A model switch might leave the 95th percentile untouched but increase the median by 12 percent because the new model is slower at common tasks. You need gates at all three percentiles. Monitoring only the median misses tail cost explosions. Monitoring only the tail misses systemic cost increases.

## Cost Regression Detection

Cost regression detection runs in CI before deployment. You take the candidate release. You run it against your cost regression test suite. The test suite is a collection of representative requests, sampled from production traffic. For each request type, you have 100 to 500 examples. These examples cover typical requests, edge cases, and historical problem cases. You run the candidate release against all examples. You measure the cost for each request. You compute the median, 95th percentile, and 99th percentile for each request type. You compare these numbers to your baseline. If any percentile exceeds its threshold, the gate fails.

The comparison is not just absolute cost. It is also cost attribution. When the gate fails, you need to know why. You decompose the cost into input tokens, output tokens, API calls, and infrastructure calls. You compare each component to baseline. If input tokens increased by 40 percent, that is the root cause. If output tokens increased by 5 percent, that is not. If API calls doubled because the new version retries more often, that is the root cause. If infrastructure calls held steady, that is not. Cost attribution tells you what to fix. Without it, the gate failure is just a number. With it, the gate failure is a diagnosis.

Cost regression detection also tracks cost variance. A release might have the same median cost as baseline but much higher variance. Some requests get cheaper, some get more expensive, and the median stays flat. This is a regression. High variance means unpredictable cost. Unpredictable cost means unpredictable budget. If your baseline median is 0.08 dollars with a standard deviation of 0.015 dollars, and your candidate median is 0.08 dollars with a standard deviation of 0.035 dollars, you have a problem. The median gate passes. The variance gate should fail. You set a variance threshold the same way you set a cost threshold. If baseline variance is 0.015 dollars, you might allow up to 0.018 dollars. Beyond that, the release is too unpredictable to ship.

Some cost regressions are justified. You switched from a smaller model to a larger model because the quality improvement was worth it. You added a retrieval step because it reduced hallucinations by 15 percentage points. You increased the output token limit because cutting off responses mid-sentence was hurting user experience. These are conscious tradeoffs. The cost gate still fails. The difference is that you have a justification ready. You document the quality improvement. You present the tradeoff to stakeholders. You get approval to increase the budget. Then you override the gate and deploy. The gate is not a veto. It is a checkpoint. It forces the conversation. It prevents silent cost increases.

## Cost Gates in CI/CD

Cost gates run in continuous integration the same way quality gates do. When a developer opens a pull request, the CI system runs the cost regression suite. It measures cost-per-request for the candidate code. It compares to baseline. It reports the result in the pull request. If the gate passes, the cost impact is green. If the gate fails, the cost impact is red. The developer sees the failure immediately. They investigate before the code is reviewed. They fix the regression or they document the justification. By the time the code reaches a reviewer, the cost question is already answered.

The cost gate is not a blocker in the same sense as a test failure. A test failure means the code is broken. A cost gate failure means the code is expensive. Broken code cannot merge. Expensive code can merge if the expense is justified. The workflow is different. When tests fail, you fix the bug. When cost gates fail, you assess the tradeoff. If the tradeoff is worth it, you add a comment to the pull request explaining why. If the tradeoff is not worth it, you reduce the cost. The gate ensures the tradeoff is visible. It ensures the decision is deliberate.

Cost gates also run in staging before production deployment. The CI cost gate runs against a fixed test suite. The staging cost gate runs against live traffic replay. You take one hour of production traffic from the previous week. You replay it against the candidate release in staging. You measure the actual cost for actual requests. You compute percentiles. You compare to baseline. This is the final check. If the staging cost gate passes, you have high confidence that production cost will be within threshold. If the staging cost gate fails, you have caught a regression that the CI suite missed. This happens when the CI test suite is not representative of current traffic patterns. Traffic evolves. Queries get longer. Users start using features you did not anticipate. The CI suite becomes stale. The staging gate catches what the CI suite misses.

Some teams run cost gates in production with automatic rollback. You deploy the candidate release to 5 percent of traffic. You measure cost-per-request in real time. If the cost exceeds threshold for more than 10 minutes, you automatically roll back. This is the most aggressive form of cost gating. It catches regressions that only appear under real load, real traffic distribution, real model behavior. It is also the riskiest. A transient cost spike caused by a few expensive requests can trigger a rollback even if the median cost is fine. You need careful threshold tuning and careful percentile selection to avoid false positives. But for high-cost, high-sensitivity systems, production cost gates with automatic rollback are the final defense.

## Cost-Quality Frontier

The cost-quality frontier is the set of tradeoffs where improving quality requires increasing cost. You cannot improve quality without spending more, and you cannot reduce cost without degrading quality. This is the hard part of cost regression gates. A release that increases cost and increases quality is not a regression. It is a tradeoff. The cost gate fails. The quality gate passes. Someone has to decide whether the tradeoff is worth it. That decision depends on margin, customer sensitivity, competitive position, and budget.

You formalize this decision with a cost-quality matrix. The matrix has four quadrants. Quadrant one: cost decreases, quality improves. This is a free win. Ship immediately. Quadrant two: cost increases, quality improves. This is a tradeoff. Evaluate the margin math. If the quality improvement increases revenue or retention by more than the cost increase, ship. If not, reject. Quadrant three: cost increases, quality degrades. This is a double regression. Block immediately. Quadrant four: cost decreases, quality degrades. This is also a tradeoff. Evaluate the quality impact. If the degradation is imperceptible to users and the cost savings are material, ship. If the degradation is noticeable, reject.

The cost-quality matrix is not subjective. You operationalize it with dollar values. You assign a dollar value to each percentage point of quality improvement. If improving accuracy by 1 percentage point reduces refund requests by 50 per month, and each refund costs 20 dollars, then 1 percentage point of accuracy is worth 1,000 dollars per month. If your system handles 100,000 requests per month and the cost-per-request increases by 0.01 dollars, the cost increase is 1,000 dollars per month. The tradeoff is neutral. If the quality improvement is 2 percentage points, the value is 2,000 dollars per month and the tradeoff is positive. If the quality improvement is 0.5 percentage points, the value is 500 dollars per month and the tradeoff is negative. This math lets you automate quadrant two decisions. If value exceeds cost, approve. If cost exceeds value, reject.

Not all quality improvements have calculable dollar values. Improving response tone, improving empathy, improving creativity — these matter but they are hard to monetize. For these improvements, you use stakeholder judgment. Product decides whether the improvement is worth the cost. The cost gate still runs. The cost increase is still visible. The difference is that Product makes the call instead of the math. The cost gate serves the same function: it forces the conversation. It ensures the tradeoff is not invisible. It ensures someone with budget authority says yes before the cost increase ships.

## Cost Monitoring vs Cost Gating

Cost monitoring and cost gating are different disciplines. Cost monitoring tells you what you spent. Cost gating tells you what you are about to spend. Monitoring is reactive. Gating is proactive. Monitoring discovers cost regressions after deployment. Gating prevents cost regressions before deployment. Both are necessary. Neither is sufficient alone.

Cost monitoring tracks aggregate spend over time. You measure total cost per day, per week, per month. You measure cost by feature, by endpoint, by user segment. You set up alerts for cost spikes. You investigate anomalies. You build dashboards that show cost trends. This is operational hygiene. It ensures you know where the money goes. But monitoring does not stop regressions. By the time the alert fires, you have already spent the money. By the time you investigate, you have already processed thousands or millions of requests at the new cost structure. Monitoring gives you visibility. It does not give you control.

Cost gating gives you control. You enforce cost thresholds before code reaches production. You block deployments that exceed thresholds. You force cost conversations during code review. You make cost a first-class release criterion alongside quality, latency, and safety. Gating prevents the spend. It stops the regression before it reaches production. But gating does not give you visibility into aggregate trends. A system can pass every cost gate and still see total spend increase by 30 percent over six months because traffic doubled. Gating protects cost-per-request. Monitoring protects total cost.

You need both. Gating is the defense against regressions. Monitoring is the defense against drift. Gating catches the pull request that accidentally switches from GPT-5-mini to GPT-5. Monitoring catches the gradual increase in average query length that makes every request 8 percent more expensive over three months. Gating is your pre-deployment control. Monitoring is your post-deployment control. The mature team runs both. The immature team runs monitoring and wonders why their bill keeps growing despite no major changes. They are missing regressions that happened one percent at a time.

Cost gating without monitoring is blind to macro trends. Cost monitoring without gating is blind to micro regressions. The release engineer who has blocked a bad deploy understands this viscerally. They know that the gate is not enough. They know that the dashboard is not enough. They know that you need both to keep the system solvent. The cost-per-request threshold that saves the business today is the threshold that lets you scale to 10 million requests tomorrow without blowing the budget.

Token explosion is the most common cause of sudden cost regression. One prompt change. One loop condition. One model behavior shift. And your output tokens increase by 5 times overnight. The next subchapter covers token explosion detection — how to catch it before it reaches production, how to set token budgets per request type, and how to build emergency token limits that stop runaway generation before it consumes your entire monthly budget in 48 hours.

