# 3.12 — Test Suite Maintenance and Pruning

The Engineering Manager stares at the dashboard. "Why are we running 487 test cases on every deployment when only twelve have failed in the past eighteen months?" The Release Engineer pulls up the execution logs. Total runtime: six hours and forty minutes. Total cost per run: $340. Total number of times anyone has looked at this problem: zero. The team built the test suite over three years. Every new feature got new tests. Every regression got a prevention test. Every edge case got coverage. Nobody ever removed anything. The test suite became a museum of every concern anyone ever had about the model—valid, outdated, redundant, or completely irrelevant. Now it takes longer to run the tests than to deploy the change.

The silent killer of test automation is not bad tests. It is too many tests that nobody maintains. Your test suite started as a careful collection of critical validations. It became an archaeological site where every layer represents a past team's concerns, most of which no longer matter. The cost is not just runtime and dollars. It is cognitive load—engineers stop trusting a test suite that mostly passes, execution delays that push releases to the next day, and the growing temptation to skip testing entirely because it takes too long. Test suite maintenance is not optional housekeeping. It is the difference between tests that protect you and tests that slow you down until you stop using them.

## The Maintenance Tax

Every test in your suite has a carrying cost. It consumes execution time—anywhere from 200 milliseconds for a simple output format check to thirty seconds for a multi-turn conversation evaluation. It consumes compute dollars—model inference, embeddings, judge calls, retrieval operations. It consumes engineering attention when it fails—someone has to investigate, determine if it is a real regression or a flaky test, and decide whether to fix the model or fix the test. When you have 50 tests, the tax is manageable. When you have 500 tests, the tax becomes the budget.

A customer service AI team ran 623 tests on every deployment candidate. Execution time: eight hours. Cost per run: $890. The team deployed twice per week, so the monthly test bill was $7,120. The Release Engineer audited the suite. 140 tests had never failed in 18 months. Another 85 tests checked for variations of the same failure mode—redundant coverage. 42 tests validated features that had been removed from the product six months earlier. Total redundant or stale tests: 267—43 percent of the suite. Pruning them reduced execution time to three hours and cost to $310 per run, cutting the monthly bill to $2,480 and saving $55,680 annually. The test suite did not lose meaningful coverage. It lost archaeology.

The tax compounds over time. New tests get added for every new feature, every production incident, every customer complaint. Old tests never get removed because nobody wants to be the engineer who deleted the test that would have caught the next disaster. The result is a test suite that grows linearly with the product's history but not with its current risk surface. You are paying to protect against problems you no longer have while the problems you do have may lack coverage because the signal is buried under noise.

## Test Case Effectiveness

Not all tests contribute equally. Some tests catch regressions frequently—output format validators, policy compliance checks, core capability tests. Others pass on every single run for years. The tests that always pass are not necessarily useless. They may be preventing regressions through deterrence—the model never degrades in that way because the architecture prevents it. Or they may be checking for risks that are real but rare. Or they may be checking for risks that no longer exist because the system changed three versions ago. You cannot know without measuring effectiveness.

Test case effectiveness is the ratio of value provided to cost incurred. Value is how often the test catches a real regression, how severe the regression would be if undetected, and how unique the test's coverage is compared to other tests. Cost is execution time, compute spend, and maintenance burden. A test that takes thirty seconds, costs two dollars, and catches a high-severity regression once per quarter is highly effective. A test that takes thirty seconds, costs two dollars, and has never failed in two years is a candidate for review—not automatic deletion, but review.

A financial services AI team tracked test effectiveness across 440 test cases. They measured three variables: last failure date, severity of last failure, and execution cost. Tests that had not failed in twelve months were flagged for review. Tests that had not failed in eighteen months and checked low-severity issues were flagged for pruning. Tests that had not failed in 24 months were flagged for archival—removed from the default suite but preserved in version control in case the risk resurfaces. The review identified 120 tests for pruning and 35 for archival. Execution time dropped from 5.5 hours to 3.2 hours. Cost dropped 44 percent. No regressions escaped in the following six months because the pruned tests were genuinely low-value.

The counterargument is always the same: "What if we prune a test and that exact failure happens next week?" The answer is: if the failure is real and important, it will show up in production monitoring or user reports, and you will add the test back. The cost of keeping 150 low-value tests running forever is higher than the cost of occasionally re-adding a test you removed. But this only works if you have production monitoring that can detect the regression. Pruning without observability is gambling. Pruning with observability is optimization.

## Identifying Redundant Tests

Redundant tests occur when multiple tests validate the same behavior or failure mode. This happens when different engineers add tests for the same concern without realizing someone else already covered it, when tests are copied and modified but the original is not removed, or when a broad test and several narrow tests all check overlapping conditions. Redundancy is not always obvious. Two tests may look different but functionally validate the same model property.

The most common form of redundancy is coverage overlap. Three tests check that the model does not generate offensive language—one using a profanity word list, one using semantic similarity to known slurs, and one using a toxicity classifier. All three catch the same regression: the model saying something inappropriate. You need one, maybe two if you want belt-and-suspenders coverage. Three is redundant. The second most common form is granularity redundancy. A test checks that the model answers customer service questions correctly. Ten other tests check that the model answers specific sub-types of customer service questions correctly. The ten narrow tests do not add coverage if the broad test already fails when any of them would fail.

A healthcare AI team identified redundancy by analyzing test failure correlation. They ran their 380-test suite against twenty historical model versions that had known regressions. For each regression, they recorded which tests failed. Tests that always failed together were redundant—they detected the same issue. Tests A, B, and C all failed whenever the model generated overly technical medical jargon. Test D failed whenever the model gave unsafe medication advice. Tests A, B, and C were redundant; the team kept the fastest one and archived the other two. Test D was unique. The analysis reduced the suite by 18 percent without losing unique coverage.

The method: correlation analysis across historical failures. If two tests fail together 95 percent of the time, they are likely redundant. If two tests fail together 30 percent of the time, they are likely covering different risks. The correlation threshold determines aggressiveness. A 90 percent correlation threshold prunes conservatively. A 70 percent threshold prunes aggressively. Most teams start at 90 percent to build confidence, then lower it over time. The danger is pruning tests that seem redundant in historical data but would diverge under future regressions. The safeguard is to archive pruned tests, not delete them, so they can be restored if needed.

## Identifying Stale Tests

Stale tests validate behaviors that no longer reflect how the system is used or how it is built. A test checks that the model responds in under 800 milliseconds when the product's latency SLA is now 1,200 milliseconds because you switched to a larger, slower model. A test validates that the model returns product recommendations in a specific JSON structure when the frontend was rewritten six months ago and now uses a different schema. A test checks that the model does not mention competitors by name when your marketing strategy changed and now you do mention competitors to position against them. Stale tests fail for the wrong reasons or pass while validating the wrong thing.

The hallmark of a stale test is that engineers override or skip it repeatedly. If a test fails and the team's response is "oh, that test is outdated, just ignore it," the test is stale. If a test is commented out in CI configuration with a note that says "re-enable once we refactor this," the test is stale. If a test checks a feature that was deprecated eight months ago, the test is stale. Stale tests erode trust in the test suite. Engineers stop investigating failures because they assume most failures are false positives from outdated tests. Real regressions get ignored because they are buried in stale-test noise.

A SaaS company tracked test skip rates. Any test that was manually skipped more than three times in a month was flagged for review. Tests flagged for three consecutive months were marked stale and removed from the suite. The practice identified 47 stale tests in the first quarter. Most validated features that had been removed or significantly changed. A few validated thresholds that were no longer relevant—like a test that checked the model used fewer than 1,500 tokens per response when the team had since switched to a model with 128,000-token context and no longer cared about token count for short responses. Removing stale tests reduced noise, increased trust, and made real failures more visible.

The process for identifying staleness: review tests that fail frequently but are overridden frequently, review tests that reference deprecated features or old product versions, review tests that have not been updated in over a year while the system around them changed, and interview engineers about which tests they trust and which they ignore. Staleness is partly objective—does the test match current system behavior—and partly social—do people still care about what the test validates. A test can be technically correct and still be stale if the risk it checks is no longer a priority.

## Pruning Criteria and Decision Framework

Pruning requires judgment. The criteria most teams use: last failure date, failure severity, execution cost, coverage uniqueness, and alignment with current product behavior. A test that last failed two years ago, checks a low-severity issue, costs three dollars per run, overlaps with two other tests, and validates a feature that no longer exists is an automatic prune. A test that last failed two years ago, checks a catastrophic safety issue, costs three dollars per run, is the only test covering that risk, and validates behavior still in the product is kept—even if it never fails, the risk is too severe to go unchecked.

The decision framework:

**High-value tests** that are always kept: tests that have failed in the past six months, tests that check high-severity risks regardless of failure history, tests that are the sole coverage for a critical behavior, tests that validate regulatory or compliance requirements, and tests that are fast and cheap even if they rarely fail. These are non-negotiable. You do not prune tests that are actively protecting you or that would be expensive to lose.

**Medium-value tests** that are reviewed quarterly: tests that last failed between six and eighteen months ago, tests that check medium-severity risks, tests that have partial coverage overlap with other tests, and tests that are moderately expensive to run. These are kept unless they become clearly redundant or stale. The review is a judgment call—does the team still believe this risk is real and worth checking?

**Low-value tests** that are pruned or archived: tests that have not failed in over 18 months, tests that check low-severity issues, tests that are fully redundant with other tests, tests that validate deprecated or removed features, and tests that are expensive to run relative to the value they provide. Prune means remove from the default suite. Archive means preserve in version control or a separate test suite that runs less frequently, like once per month instead of on every deployment.

A logistics AI team implemented a quarterly pruning process. Every three months, the Release Engineer reviewed tests using the framework above. Tests marked for pruning were moved to an archived suite that ran monthly. If an archived test failed during a monthly run, it was restored to the default suite—the risk was real after all. Over 18 months, 92 tests were archived. Seven were restored after catching regressions in monthly runs. 85 remained archived, confirming they were low-value. The process kept the default suite lean without deleting knowledge.

## The Danger of Aggressive Pruning

The risk is removing a test that catches a rare but catastrophic failure. Your model never hallucinates patient medication—until one day it does, and the test you pruned six months ago would have caught it. The failure rate was zero for two years, so the test looked useless. It was not useless. It was guarding against a tail risk. Aggressive pruning turns test suites into optimized-for-common-cases systems that miss rare disasters.

The safeguard is archival, not deletion. Do not delete tests. Archive them. Store them in version control with clear labels: "Archived 2026-02-01 due to 24 months without failure. Checks for X risk. Restore if risk resurfaces." Run archived tests on a slower cadence—monthly or quarterly instead of per-deployment. If an archived test fails, restore it immediately. If an archived test passes for another year, consider full deletion. But the first step is always archival, not deletion. The cost of storing an archived test is near zero. The cost of losing coverage for a rare catastrophic risk is unbounded.

A second safeguard is production monitoring. You can prune more aggressively if your production observability can detect the regression within hours of deployment. If you prune a test for a rare risk and the risk occurs, production monitoring should alert you before users are harmed. Pruning without monitoring is reckless. Pruning with monitoring is risk management. The formula: archived tests plus production observability equals safe pruning. Either component alone is insufficient.

A third safeguard is stakeholder review. Before pruning any test that checks a regulatory, safety, or high-consequence behavior, get explicit approval from Legal, Compliance, Trust and Safety, or the relevant domain owner. Do not prune a HIPAA compliance test because it has not failed in two years without asking your Legal team if they are comfortable removing it. The test may be preventing violations through deterrence, not detection. Stakeholder review ensures you do not prune tests that serve organizational or legal functions beyond regression detection.

## Test Suite Versioning

Version your test suite the same way you version your model and your code. Every deployment should record which test suite version was used, which tests were included, what the pass rate was, and what thresholds were applied. Test suite versioning allows you to answer the question: "What were we testing for when we deployed version 47 on January 15th?" Without versioning, you cannot reproduce historical test results or understand why a regression was missed.

The simplest form of versioning is a manifest file. Every test suite run generates a manifest that lists every test executed, every test result, every threshold, and every configuration setting. The manifest is stored alongside the deployment artifact. When a regression escapes to production, you pull the manifest and see exactly what was tested. If a test that would have caught the regression was not in the suite, you know the gap. If the test was in the suite but passed, you know the test needs improvement. Without the manifest, you are guessing.

A more advanced form is semantic versioning of the test suite itself. The test suite has a version number: 3.2.1. Major version increments when you add or remove tests for high-severity risks. Minor version increments when you add or remove medium-severity tests or change thresholds significantly. Patch version increments for small adjustments. Every deployment references the test suite version. Rollback decisions can consider both model version and test suite version—if you roll back the model, you may also need to roll back the test suite to match what was tested originally.

A fintech company versioned their test suite using Git tags. Every time the suite changed, they tagged it with a version number and a summary of changes. Every deployment recorded which test suite tag was used. When a regression escaped, they could instantly compare the current test suite to the test suite that was active when the regression was introduced. The practice identified gaps in test coverage and enabled precise rollback—they rolled back both the model and the test suite to a known-good state. Versioning turned their test suite into a first-class deployment artifact instead of an ad hoc collection of checks.

## Scheduled Maintenance and Audit Cadence

Test suite maintenance is not a one-time effort. It is a recurring discipline. The most effective teams audit their test suite quarterly. Every three months, the Release Engineer or Test Owner reviews the suite using the pruning criteria, archives low-value tests, updates thresholds for changed system behavior, removes stale tests, and adds new tests for recent production incidents. The quarterly cadence balances the cost of ongoing maintenance with the risk of letting the suite grow unbounded.

The audit process: generate a test effectiveness report that lists every test, its last failure date, its execution cost, its coverage overlap, and its failure severity. Flag tests that meet pruning criteria. Review flagged tests with the engineering team to confirm they are safe to prune or archive. Update the test suite. Run a validation pass to confirm the pruned suite still achieves target coverage. Deploy the updated suite. Track the impact over the next quarter—did execution time decrease, did cost decrease, did any unexpected regressions escape. Adjust the criteria based on results.

A healthcare AI team implemented quarterly audits in March 2025. In the first audit, they archived 55 tests. In the second audit, they archived 22 more and restored 3 that had been archived in the first round. By the fourth audit, the process stabilized—they archived 8 tests, restored 1, and spent most of the time updating thresholds for recent model changes. The suite went from 510 tests to 390 tests over twelve months. Execution time dropped from 7 hours to 4.5 hours. Cost dropped from $680 per run to $410 per run. No meaningful regressions escaped. The quarterly cadence became part of the release engineering culture.

The alternative is continuous maintenance—engineers prune tests whenever they notice redundancy or staleness. This works for small teams with strong test suite ownership. For larger teams, the quarterly audit provides structure and ensures maintenance actually happens. Without a scheduled cadence, maintenance gets deprioritized until the test suite becomes so bloated that it forces a crisis-driven overhaul.

## Balancing Coverage with Execution Cost

The tension is always between comprehensive coverage and fast, cheap execution. Comprehensive coverage means testing every behavior, every edge case, every integration, every failure mode. Fast execution means running only the tests that provide the most value per dollar and per second. You cannot have both at full scale. The compromise is tiered test suites—a fast core suite that runs on every deployment, a broader extended suite that runs daily or weekly, and a comprehensive full suite that runs monthly or on-demand before major releases.

The core suite contains high-value, high-severity, fast-executing tests. It runs in under 30 minutes and costs under $50. It catches 80 percent of regressions. The extended suite adds medium-value tests, slower tests, and edge case coverage. It runs in under three hours and costs under $200. It catches 95 percent of regressions. The full suite adds rare-risk tests, archived tests, and exploratory checks. It runs in six to twelve hours and costs $500 to $1,000. It catches 99 percent of regressions. Most deployments run the core suite. Weekly releases run the extended suite. Major releases run the full suite.

A customer support AI team structured their suite this way. Core suite: 85 tests, 22 minutes, $35, runs on every deployment candidate. Extended suite: 240 tests, 2.5 hours, $175, runs nightly. Full suite: 410 tests, 6 hours, $620, runs before quarterly releases and after major model upgrades. The structure allowed them to deploy daily with confidence while maintaining comprehensive coverage for high-stakes releases. The cost was predictable—$35 per deployment plus $175 per week plus $620 per quarter. The execution time never blocked a deployment because the core suite was fast enough to fit into the normal release cycle.

The tiering decision is based on test effectiveness and execution cost. The highest-effectiveness tests go in the core suite. Medium-effectiveness tests go in the extended suite. Low-effectiveness and rare-risk tests go in the full suite. As tests migrate between tiers or get archived, the balance shifts. The goal is to keep the core suite lean and fast while ensuring no critical coverage is lost. The worst outcome is a flat suite where every test runs on every deployment and execution time creeps toward eight hours until nobody wants to wait for tests anymore.

## Test Suite Health Metrics

You cannot manage what you do not measure. Track test suite health the same way you track model performance. The metrics that matter: total test count, execution time, execution cost, pass rate, flake rate, tests added per month, tests pruned per month, last maintenance date, and coverage per risk category. Dashboards make the problem visible. Without measurement, test suite bloat is silent until it becomes a crisis.

**Total test count** should grow slower than your product's feature set. If your product has 50 features and your test suite has 600 tests, you are over-testing relative to product scope. If your product has 200 features and your test suite has 150 tests, you are under-testing. The ratio depends on risk—high-risk domains need more tests per feature. Track the trend over time. If test count grows 40 percent per quarter while features grow 10 percent per quarter, you have a pruning problem.

**Execution time and cost** should trend flat or downward as you prune and optimize. If both trend upward every quarter, you are accumulating tests faster than you are removing them. Set a budget—no more than four hours per run, no more than $400 per run—and enforce it. When you hit the budget, you must prune before adding new tests. The budget forces discipline.

**Pass rate** should be above 95 percent on deployment candidates that pass pre-checks. If pass rate is 60 percent, either your deployment process is broken or your test suite is full of flaky or stale tests. **Flake rate**—the percentage of tests that fail intermittently without a real regression—should be under 2 percent. Every flaky test erodes trust. Track which tests flake, fix or prune them.

**Maintenance cadence** tracks how often you review the suite. If the last audit was nine months ago, schedule one immediately. Test suites that go unreviewed for more than six months accumulate bloat that becomes painful to unwind later. A legal AI team set a dashboard alert: if more than 180 days passed since the last test suite audit, an alert fired and the Release Engineering lead got a reminder. The forcing function ensured maintenance happened on schedule.

---

With a test suite that stays lean, effective, and aligned with current product risks, the next challenge is detecting regressions that are not about the model's core outputs—but about the knowledge it retrieves, the data it processes, and the information it depends on to function correctly in production. Chapter 4 examines data, retrieval, and knowledge regression.

