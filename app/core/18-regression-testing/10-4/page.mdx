# 10.4 — User Satisfaction Signal Tracking

How do you know if users are actually satisfied with your AI? Not what your eval suite says. Not what your accuracy metrics show. What do users actually think?

Most teams cannot answer this question. They track technical metrics—latency, accuracy, hallucination rate—and assume that if these improve, users are happier. The assumption is wrong. A model can become more accurate and less satisfying. It can respond faster and feel less helpful. It can reduce errors and increase frustration. User satisfaction is not a function of model performance. It is a function of user perception, expectation, and emotional response. You cannot measure it through evals. You have to ask users—or watch what they do.

User satisfaction is the second critical dimension of production outcome regression, alongside task completion. A model can help users complete tasks while making them miserable. Users finish the interaction but vow never to use the product again. Task completion rate stays steady. Retention collapses. Revenue follows. Satisfaction is the leading indicator of retention, and retention is the leading indicator of long-term business value.

Tracking satisfaction requires collecting signals—both explicit signals where users tell you how they feel and implicit signals where users show you through their behavior. Neither signal type is sufficient alone. Explicit signals are biased toward extreme experiences—users who are very happy or very angry. Implicit signals are noisy and require interpretation. Together, they give you a directional read on whether users are becoming more or less satisfied after a model change.

## Explicit Satisfaction Signals: Ratings, Feedback, NPS

Explicit signals are direct user input: thumbs up, thumbs down, star ratings, written feedback, Net Promoter Score surveys. These signals are valuable because they are unambiguous. A user who clicks thumbs down is telling you they are dissatisfied. You do not have to infer. The challenge is that most users do not provide explicit feedback. You get signal from five to fifteen percent of sessions, depending on how you prompt for it. The users who respond are not representative of all users—they skew toward extreme experiences.

Thumbs up and thumbs down are the simplest explicit signal. After the model responds, you show two buttons. The user clicks one or ignores both. Thumbs down is a stronger signal than thumbs up. A thumbs down usually means something went wrong: the model misunderstood, hallucinated, provided irrelevant information, or violated an expectation. A thumbs up might mean the user was satisfied, or it might mean the user felt obligated to click something. Thumbs up is a weak positive signal. Thumbs down is a strong negative signal.

Tracking thumbs down rate over time reveals satisfaction regression. If thumbs down rate was two percent and it increases to four percent after a model change, users are twice as unhappy. Even if task completion rate holds steady, doubled dissatisfaction will eventually erode retention. You need a threshold: if thumbs down rate increases by more than fifty percent relative to baseline, you investigate. If it increases by more than one hundred percent, you roll back.

Star ratings—one to five stars—provide more granularity than binary thumbs. A three-star rating is different from a one-star rating. Both are dissatisfied, but one-star is fury, three-star is disappointment. Tracking the distribution of star ratings reveals shifts in user sentiment. If the new model increases five-star ratings but also increases one-star ratings, you have a polarizing change—some users love it, some users hate it. If the new model shifts the distribution toward three stars, you have a mediocrity problem—users are less angry but also less delighted.

Mean star rating is a weak metric because it obscures distribution. A mean of 3.8 could mean most users gave four stars, or it could mean half gave five stars and half gave two stars. The distribution matters more than the mean. Track the percentage of one-star and two-star ratings as your dissatisfaction rate. Track the percentage of four-star and five-star ratings as your satisfaction rate. Monitor both. A model change that reduces dissatisfaction rate from ten percent to eight percent but also reduces satisfaction rate from sixty percent to fifty percent is a regression—you are losing your happiest users.

Written feedback—users typing comments after a rating—is the richest signal but the hardest to analyze at scale. Users tell you exactly what went wrong: "The model gave me three options and none of them were relevant." "It took forever to respond." "It kept asking me the same question." "It contradicted itself." Reading user feedback reveals failure modes your eval suite missed. The challenge is volume. If you have ten thousand sessions per day and one percent leave feedback, you have one hundred comments per day. You cannot read them all.

Automated sentiment analysis on written feedback helps. You classify comments as positive, neutral, or negative. You extract common themes: latency complaints, relevance complaints, tone complaints, safety complaints. You track theme frequency over time. If "not relevant" complaints double after a model change, your retrieval or recommendation logic degraded. If "too slow" complaints spike, your latency regressed. Theme tracking turns unstructured feedback into structured signals.

Net Promoter Score asks: "How likely are you to recommend this product to a friend or colleague?" on a scale from zero to ten. Respondents who answer nine or ten are promoters. Seven or eight are passive. Zero through six are detractors. Your NPS is the percentage of promoters minus the percentage of detractors. An NPS of 40 means 50 percent promoters and 10 percent detractors. NPS is a standard business metric that correlates with long-term growth.

NPS is slower to respond to model changes than thumbs down rate, but it reveals retention risk. If NPS drops after a model change, users are less likely to recommend your product. They might still use it—especially if they have no alternative—but they are not advocates. A drop in NPS is a leading indicator of churn when competition arrives.

Explicit signal collection must not interfere with task completion. If you interrupt the user with a feedback prompt before they finish their task, you create abandonment risk. The right time to ask for feedback is after task completion or after session end. For synchronous assistants, you show the feedback prompt after the user's task completes. For asynchronous assistants, you send a follow-up email or in-app notification. The user can ignore it without penalty.

## Implicit Satisfaction Signals: Session Length, Return Rate, Escalation

Implicit signals are behavioral patterns that reveal satisfaction without asking. A user who returns to the product every day is probably satisfied. A user who escalates to a human agent after every AI interaction is probably dissatisfied. A user who spends two minutes per session is having a different experience than a user who spends ten minutes. Implicit signals are always present—every user generates them—but they require interpretation.

Session length is ambiguous. Longer sessions could mean users are engaged and exploring. Or they could mean users are stuck and struggling. You need context. If a user completes their task in three minutes, that is efficient. If a user spends twelve minutes and abandons without completing their task, that is frustration. The signal is not session length alone—it is session length relative to task completion.

Session length regression happens when average time to task completion increases after a model change. Users are spending more time to achieve the same outcome. This suggests friction. The model is asking more questions, providing less relevant options, or requiring more back-and-forth. Even if task completion rate holds steady, increased time to completion degrades user experience. Users have finite patience. A task that used to take two minutes and now takes four minutes feels twice as hard.

Track session length percentiles, not just the mean. If median session length stays constant but the ninetieth percentile doubles, your model is creating rare but extreme friction for a subset of users. These users are stuck in loops, receiving irrelevant responses, or encountering edge cases the model cannot handle. The ninetieth percentile is your failure case signal. If it increases by more than fifty percent, you have a regression.

Return rate is the percentage of users who come back within a defined time window—usually seven days. High return rate suggests satisfaction. Users found value and came back for more. Low return rate suggests the opposite. They tried the product, were unimpressed, and did not return. Return rate is a slower signal than thumbs down rate—it takes a week to measure—but it is less biased. Every user generates a return signal. You are not relying on the minority who leave explicit feedback.

Return rate regression happens when a model change reduces the percentage of users who return. If return rate was 45 percent and it drops to 38 percent, seven percent of your user base is not coming back. This is a retention disaster. Even if task completion rate is stable, you are losing users. The model might be completing tasks but not delighting users. They finish what they came to do and decide the experience is not worth repeating.

Segmenting return rate by user cohort reveals who you are losing. New users might have lower return rates because they are experimenting. Power users might have higher return rates because they depend on the product. If a model change reduces return rate for power users, you are alienating your most valuable segment. If it reduces return rate for new users, you are failing at onboarding. Different problems require different fixes.

Escalation rate is the percentage of sessions where the user escalates to a human agent. High escalation rate means the model is failing to resolve user needs. Users try the AI, it does not help, they escalate. Escalation is expensive—you are paying for both the AI interaction and the human support. It is also a satisfaction killer. Users perceive escalation as failure. They wanted self-service. They got a runaround.

Escalation rate regression happens when a model change increases the percentage of sessions that escalate. If escalation rate was eight percent and it increases to twelve percent, fifty percent more users are giving up on the AI and demanding human help. This suggests the new model is less capable, less trustworthy, or more confusing. Users lose confidence faster. Even if the new model has better accuracy on your eval suite, users do not trust it.

Tracking when in the conversation users escalate reveals what triggers the request. If users escalate after one turn, the model's first response was so bad they immediately gave up. If users escalate after five turns, they tried to work with the model but hit a dead end. Late-stage escalation is worse—it means users invested effort and still failed. Early-stage escalation suggests the model is not engaging users from the start.

Another implicit signal is message tone. Users who are satisfied write short, neutral messages. Users who are frustrated write longer, more emotional messages. They use all caps. They repeat themselves. They use negative language: "This is not working." "Why do not you understand?" "I have already told you this." Sentiment analysis on user messages—not just on feedback, but on every message in the conversation—reveals rising frustration within a session. If frustration language increases after turn three, something about the model's third response is triggering users.

Session abandonment followed by same-session retry is a strong dissatisfaction signal. The user starts a task, abandons it, and immediately starts a new session with the same intent. This means they blamed the conversation, not their goal. They are giving the product a second chance. If the retry also fails, they are gone. Tracking retry rate and retry success rate shows whether the model is inconsistently bad or consistently bad. High retry rate with low retry success rate is the worst case—users keep trying, the model keeps failing.

## Sentiment Analysis on User Messages

Every message a user sends contains emotional information. A user who writes "Thanks, that is exactly what I needed" is satisfied. A user who writes "This is the third time I have asked" is frustrated. A user who writes "Whatever, never mind" has given up. Sentiment analysis extracts this emotional signal and tracks it over time.

Basic sentiment classification labels each user message as positive, neutral, or negative. You run a sentiment model—often a fine-tuned BERT variant or a lightweight classifier—on every user message. You aggregate sentiment scores at the session level: if a session has three positive messages and one negative message, the session sentiment is net positive. You track the percentage of sessions with net negative sentiment. This becomes your frustration rate.

Frustration rate regression happens when a model change increases the percentage of sessions where users express negative sentiment. If frustration rate was six percent and it increases to ten percent, sixty-seven percent more users are expressing unhappiness. Even if they complete their tasks, they are not happy about it. Frustrated users do not return. Frustration rate is a leading indicator of churn.

More sophisticated sentiment analysis tracks emotional progression within a session. A session might start neutral, become negative after the model's second response, and end neutral after the model recovers. Tracking sentiment trajectory reveals where the model loses users and whether it can recover. A model that frequently triggers frustration but also frequently resolves it is better than a model that triggers frustration and never recovers.

You can also track specific frustration phrases. Users express frustration in predictable ways: "I already said," "You are not listening," "This is not what I asked for," "Forget it," "Never mind," "This is useless," "I give up." Detecting these phrases in real time lets you flag high-risk sessions for review or intervention. If a user types a frustration phrase, you might route them to a human agent immediately, before they abandon.

Sentiment analysis on user messages requires a labeled training set. You sample a few thousand user messages and label them as positive, neutral, or negative. You fine-tune a sentiment classifier on this data. The classifier is domain-specific—frustration in customer support conversations looks different from frustration in shopping conversations. A generic sentiment model trained on product reviews will miss domain-specific signals.

Tracking sentiment over time requires session-level aggregation. You calculate the percentage of sessions with net negative sentiment each day. You plot this as a time series. You monitor for upward trends. If sentiment starts degrading before a model change, the degradation is not caused by the model—it might be seasonal, a product bug, or external factors. If sentiment starts degrading immediately after a model change, the model is the cause.

Sentiment regression testing requires a baseline sentiment distribution. Before deploying a new model, measure the percentage of sessions with positive, neutral, and negative sentiment for the current model over two weeks. Set a threshold: if the new model increases negative sentiment by more than two percentage points, you investigate. If it increases by more than five percentage points, you roll back.

## Satisfaction Baseline and Trends

Satisfaction is not static. It varies by time of day, day of week, user cohort, and external factors. A model deployed on Monday might show different satisfaction signals than the same model deployed on Friday. Users are more patient in the morning than late at night. Users are more satisfied when they complete high-value tasks than low-value tasks. You need a satisfaction baseline that accounts for this variance.

Establishing a baseline requires measuring satisfaction signals for the current model over a representative time window—at least two weeks, ideally four. You track thumbs down rate, star rating distribution, NPS, session length, return rate, escalation rate, and sentiment distribution. You segment by time of day, day of week, and user cohort. You calculate mean and variance for each signal. This baseline becomes your comparison point.

Satisfaction trends over time reveal whether the product is improving or degrading independent of model changes. If thumbs down rate has been slowly increasing for three months, you have a product problem, not a model problem. Users are becoming less satisfied because of feature gaps, UX friction, pricing changes, or competitive pressure. Deploying a better model will not fix this. You need product-level intervention.

Conversely, if satisfaction has been stable for months and suddenly drops after a model change, the model is the cause. You have isolated the variable. You can revert the model and confirm that satisfaction recovers. If satisfaction does not recover after rollback, something else changed—a product update, a pricing change, a competitor launch, a PR incident. You need to investigate beyond the model.

Seasonal variance affects satisfaction. Users are more satisfied during certain times of year—holidays, fiscal year-end, back-to-school season—depending on your product. If you deploy a model in December and see satisfaction drop, it might be the model, or it might be that December users have different expectations. You need year-over-year comparisons, not just week-over-week.

User cohort variance affects satisfaction. New users are less satisfied than experienced users because they do not know how to use the product yet. Power users are more satisfied because they have learned the product's strengths and avoid its weaknesses. If you deploy a model that confuses new users, new user satisfaction drops but power user satisfaction stays stable. If you deploy a model that changes behavior power users depend on, power user satisfaction drops but new user satisfaction stays stable. Segmenting by cohort reveals who is affected.

Satisfaction trend monitoring requires automated dashboards. You track satisfaction signals in real time. You plot them as time series. You overlay model deployment events. You set up alerts: if thumbs down rate increases by more than fifty percent for two consecutive hours, you get paged. If NPS drops by more than ten points over a week, you get an email. Alerts let you catch regressions before they compound.

## Satisfaction Regression Detection

Satisfaction regression detection is the process of identifying when a model change has degraded user satisfaction. It requires comparing post-deployment satisfaction signals to baseline and testing whether the difference is statistically significant and operationally meaningful.

Statistical significance matters because satisfaction signals are noisy. Thumbs down rate might be 2.1 percent one day and 2.4 percent the next day due to random variance. You need to distinguish between noise and signal. A fifteen percent increase in thumbs down rate from 2.0 percent to 2.3 percent might be statistically significant over ten thousand sessions, but it might not be operationally meaningful—the product experience has not materially changed.

Operational significance is the threshold where user experience has degraded enough to matter. For thumbs down rate, a fifty percent increase is operationally significant. For NPS, a ten-point drop is significant. For escalation rate, a twenty percent increase is significant. These thresholds are product-specific. You set them based on historical data and business impact. A threshold too tight creates false alarms. A threshold too loose lets real regressions ship.

Detecting satisfaction regression requires a holdout period. You deploy the new model to ten percent of traffic. You collect satisfaction signals for at least 24 hours, ideally 72 hours. You compare these signals to the baseline for the same user cohort, time of day, and day of week. You test for both statistical and operational significance. If both tests pass, you scale to 50 percent. You repeat. If either test fails, you investigate or roll back.

The challenge is delayed signal. Explicit signals like thumbs down appear within minutes of a model response. Implicit signals like return rate take days to materialize. If you wait a week to measure return rate, you might have already scaled a bad model to 100 percent of traffic. You need faster proxies. Session-level sentiment and escalation rate are faster proxies for return rate. If sentiment and escalation degrade, return rate will likely follow.

Satisfaction regression can be segment-specific. Aggregate satisfaction might hold steady while a specific segment collapses. A model change might improve satisfaction for simple tasks but degrade it for complex tasks. It might help mobile users but hurt desktop users. It might work well in English but fail in Spanish. Segmented regression testing requires tracking satisfaction signals for each major segment and setting per-segment thresholds.

Correlation analysis reveals which model behaviors drive satisfaction regression. You log model response characteristics: response length, response time, number of retrieved chunks, number of tool calls, presence of disclaimers, presence of follow-up questions. You correlate these characteristics with satisfaction signals. You find that sessions where the model asked follow-up questions have twice the thumbs down rate. You have identified a candidate cause. You can test by reducing follow-up questions and measuring whether satisfaction improves.

Satisfaction regression recovery requires understanding the root cause. If the regression is due to increased latency, you optimize inference. If it is due to changed tone, you adjust the system prompt. If it is due to less relevant retrieval, you retune your embedding model or rerank logic. If it is due to a capability loss, you restore the capability through fine-tuning or prompt engineering. You deploy the fix to ten percent of traffic, measure satisfaction signals again, and confirm recovery before scaling.

## Correlating Satisfaction with Model Changes

Not all satisfaction changes are caused by model changes. Product updates, pricing changes, competitor actions, external events, and seasonal factors all influence satisfaction. To isolate the model's impact, you need to correlate satisfaction changes with specific model deployment events.

The cleanest way to establish causation is an A/B test. You deploy the new model to 50 percent of users, randomly assigned. The other 50 percent stay on the old model. You measure satisfaction signals for both groups over the same time window. If the new model group has significantly lower satisfaction, the model caused the regression. If both groups have similar satisfaction, the change is due to external factors.

A/B testing requires careful randomization. If you assign the new model to all mobile users and the old model to all desktop users, you cannot isolate the model's effect from the platform's effect. Randomization must be at the user level, stratified by key segments. Each segment should have roughly equal representation in both groups. This controls for confounding variables.

Time-based correlation is weaker but faster. You deploy the new model to all users at a specific timestamp. You track satisfaction signals before and after the deployment. If satisfaction drops immediately after deployment and recovers immediately after rollback, the model caused the regression. The weakness is that you cannot control for confounding factors—something else might have changed at the same time.

Gradual rollout with continuous monitoring is the practical middle ground. You deploy the new model to ten percent of users. You track satisfaction for 24 hours. If satisfaction is stable, you scale to 25 percent. You repeat. At each scale step, you compare the new model cohort's satisfaction to the old model cohort's satisfaction. If the gap widens, you pause and investigate. This approach catches regressions early while providing quasi-experimental comparison.

Feature attribution analysis links specific model behaviors to satisfaction outcomes. You log every model response characteristic: retrieval used, tools called, response length, latency, disclaimers added, follow-up questions asked. You train a regression model to predict satisfaction signals from these features. You find that responses with disclaimers have 1.3 times higher thumbs down rate. You find that responses under one second have 1.4 times higher thumbs up rate. These insights inform model tuning.

Multi-variable analysis isolates the model's contribution. You include model version, time of day, day of week, user cohort, task type, and external events as features. You predict satisfaction using a regression model. The coefficient on model version tells you how much the model contributes to satisfaction variance, independent of other factors. If the coefficient is small, the model is not driving satisfaction changes—other factors dominate.

Satisfaction correlation is not deterministic. Two users can have identical model interactions and report different satisfaction. One user was in a hurry and frustrated by latency. Another user had time and appreciated thoroughness. User context—emotional state, time pressure, prior experience—affects satisfaction independent of model quality. You cannot perfectly predict satisfaction. You can only measure it in aggregate and detect when the aggregate shifts.

## Satisfaction Recovery: How to Respond to Drops

When satisfaction drops after a model deployment, you have three options: rollback, tune, or accept. Rollback means reverting to the previous model. Tune means adjusting the new model to fix the regression. Accept means deciding the regression is acceptable given other improvements. The right choice depends on the magnitude of the drop, the root cause, and the cost of each option.

Immediate rollback is the right choice if satisfaction drops by more than your critical threshold—usually a doubling of dissatisfaction rate or a ten-point NPS drop. This level of regression will cause measurable churn. You cannot afford to leave it in production while you investigate. You roll back first, stabilize satisfaction, then investigate and iterate offline.

Tuning is the right choice if the regression is moderate—a twenty to fifty percent increase in dissatisfaction rate—and you understand the root cause. You adjust the system prompt, reduce latency, retune retrieval, or restore a lost capability. You test the tuned model on a small percentage of traffic. You confirm that satisfaction recovers before scaling. Tuning takes days to weeks, depending on complexity.

Acceptance is the right choice if the regression is small and the new model delivers critical improvements. For example, the new model reduces hallucination rate by eighty percent but increases thumbs down rate by ten percent because it provides more cautious answers. Users are slightly less delighted but much safer. The trade-off is acceptable. You accept the satisfaction regression in exchange for the safety improvement.

Recovery speed matters. Every day a regression stays in production, you lose users. If return rate drops from 45 percent to 38 percent, you are losing seven percent of your user base every week. After four weeks, you have lost 28 percent of potential returning users. Some of them will never come back. Fast detection and fast response minimize the damage.

Communication to users is part of recovery. If users noticed the regression—if support tickets spiked, if social media complaints appeared—you acknowledge it. You tell users you identified an issue, you are working on it, and you expect a fix within a specific timeframe. Transparency builds trust. Silence erodes it. Users tolerate short-term regressions if they believe the team is responsive.

Post-recovery analysis is how you prevent recurrence. After you recover satisfaction, you investigate why your pre-deployment testing missed the regression. Did your eval suite not measure the right dimensions? Did your A/B test not run long enough? Did you not segment by the affected cohort? You update your testing process to catch this failure mode next time. Regression is a learning opportunity.

Satisfaction tracking, like task completion tracking, is not optional. It is the voice of the user, translated into signals you can measure, monitor, and protect. Task completion tells you whether the model is effective. Satisfaction tells you whether the model is tolerable. You need both. And you need both wired into your release gates, your monitoring dashboards, and your rollback triggers—because the fastest model in the world does not matter if users hate using it.

---

*Next: 10.5 — Linking CI to Production Signals*
