# 10.6 — Trace-Level Debugging from CI Failures

The engineer stared at the CI log. Test failed. Model output did not match expected result. No other information. She could see the input. She could see the expected output. She could see the actual output. She could not see why the model produced the wrong answer. Was it a retrieval failure? A reasoning error? A prompt issue? A model behavior change? Without the trace, debugging required guesswork. She spent three hours reproducing the failure locally, running the model with verbose logging, stepping through the pipeline. By the time she found the root cause—a ranking issue in retrieval that surfaced an irrelevant document ahead of the correct one—the entire team's release was delayed.

This scenario repeats every day in AI engineering organizations. Regression tests fail. The failure is real. But the information provided by the test is insufficient to diagnose the root cause. The test reports pass or fail. The engineer needs to know why. Trace-level debugging means instrumenting every regression test to capture not just the final output but the entire execution path: inputs, intermediate steps, retrieved documents, reasoning traces, tool calls, latency breakdowns, token counts, model parameters. When a test fails, the trace tells you exactly where the failure occurred and why.

## Why Pass-Fail Is Not Enough

A pass-fail result is a single bit of information. It tells you whether the output met the threshold. It does not tell you how close the model came. It does not tell you which component of the pipeline failed. It does not tell you whether the failure is consistent or intermittent. It does not tell you whether the failure is a regression from the previous release or a known edge case that occasionally resurfaces. Without this context, debugging CI failures is an archaeological expedition. You dig through code, re-run tests with added logging, reproduce the failure in a local environment, and hope you can isolate the cause.

Trace-level debugging flips this dynamic. The test run itself produces enough data to diagnose the failure. The engineer does not need to reproduce anything. She opens the trace, sees the input, sees the retrieved documents, sees the model's reasoning steps if applicable, sees the final output, and sees exactly where the output diverged from the expected result. Diagnosis that used to take three hours takes ten minutes. The release unblocks. The fix is targeted. The team moves forward.

A legal AI startup adopted trace-level debugging in late 2025 after months of frustration with opaque test failures. Before traces, the median time to diagnose a CI failure was two hours. Engineers would re-run tests locally, add print statements, check retrieval results manually, inspect model outputs. After implementing full trace capture, the median diagnosis time dropped to 15 minutes. The engineer opened the trace dashboard, saw which document was retrieved incorrectly, saw why the ranking was wrong, fixed the retrieval configuration, and re-ran the test. The time savings compounded. The team went from three releases per week to eight.

## What to Capture in a Trace

A complete AI trace includes every input and output at every stage of the pipeline. For a simple prompt-based classification model, this is straightforward: the input text, the prompt, the model response, the token count, the latency, the model version. For a RAG system, the trace includes the user query, the transformed query sent to retrieval, the documents retrieved with their scores, the reranked documents if reranking is used, the final prompt constructed from the query and documents, the model response, the token count, the latency breakdown by retrieval and generation, and the model version.

For a multi-step agent, the trace includes every action the agent took: the initial query, the plan the agent generated, each tool call with inputs and outputs, each reasoning step, the final answer, the total latency, the token count for each generation step, the number of tool calls, and the model version. For a complex pipeline with multiple models, the trace includes outputs from each model, the latency for each model call, the logic that routed the input to specific models, and any fallback or retry logic that executed.

The key principle is completeness. Capture every decision point, every retrieval query, every model call, every intermediate result. Traces are not for human reading during normal operation. They are for human reading during debugging. No one looks at traces when tests pass. Everyone looks at traces when tests fail. The trace must contain enough information to diagnose the failure without additional reproduction or instrumentation.

A B2B customer support AI implemented traces across their entire CI suite in early 2026. Their pipeline had five stages: query understanding, intent classification, retrieval, answer generation, and response formatting. Each stage logged its input, output, model version, latency, and any errors or warnings. When a regression test failed, the trace showed exactly which stage produced the unexpected output. If intent classification failed, the trace showed the query, the classified intent, the expected intent, and the confidence scores for all candidate intents. If retrieval failed, the trace showed the retrieval query, the retrieved documents with scores, and the expected documents. If answer generation failed, the trace showed the final prompt, the model output, and the expected output.

This level of detail made debugging deterministic. The engineer did not need to guess which component failed. The trace told her. She fixed the component, re-ran the test, and moved on.

## Logging Infrastructure for AI Test Runs

Capturing traces requires logging infrastructure that can handle structured data at scale. A single test run might generate megabytes of trace data if the pipeline is complex. A full CI suite with 200 tests generates hundreds of megabytes per run. If you run CI on every pull request and you have 50 pull requests per day, you are generating tens of gigabytes of trace data daily. This data needs to be stored, indexed, queryable, and retained for at least 30 days so engineers can compare traces across releases.

The simplest infrastructure uses a logging library that writes structured JSON to a file during test execution. Each test run gets a unique identifier. Each log entry includes the test identifier, the stage of the pipeline, the input, the output, the timestamp, and any metadata. After the test completes, the JSON file is uploaded to object storage with the test identifier as the key. When a test fails, the CI dashboard links to the trace file. The engineer downloads it, opens it in a JSON viewer, and steps through the execution.

A more sophisticated infrastructure uses a dedicated observability platform. Tests log traces to Datadog, Langfuse, Arize, or Honeycomb. Each test run is a trace with spans for each pipeline stage. The observability platform indexes the traces, provides a UI for searching and filtering, supports diffing traces from different test runs, and integrates with the CI dashboard so engineers can view traces inline with test results. The platform also supports alerting on trace patterns: if a particular retrieval query consistently returns low-relevance documents across multiple test runs, an alert fires and the team investigates before release.

A fintech AI team adopted Langfuse for trace capture in CI in January 2026. Every regression test logged its execution as a trace with nested spans for each pipeline component. When a test failed, the CI dashboard displayed a link to the trace in Langfuse. The engineer clicked the link, saw the full execution tree, expanded the span where the failure occurred, saw the inputs and outputs, and identified the issue. The entire workflow took less than five minutes. The team also used Langfuse to compare traces across releases. If a test started failing after a model upgrade, they could view the trace from the passing release and the trace from the failing release side by side, spot the difference, and determine whether the model behavior change was acceptable or a regression.

## Root Cause Analysis from Traces

The most common failure modes in AI pipelines are retrieval errors, model behavior changes, prompt issues, and data format mismatches. Traces make diagnosing each of these failures straightforward. Retrieval errors show up as missing documents or incorrect ranking in the retrieval span. Model behavior changes show up as different outputs for the same input and prompt in the generation span. Prompt issues show up as unexpected model behavior correlated with recent prompt changes. Data format mismatches show up as parsing errors or missing fields in the input processing span.

A healthcare AI assistant had a regression test that checked whether the model could correctly extract medication names and dosages from clinical notes. The test failed in CI after a model upgrade from GPT-5 to GPT-5.1 in February 2026. The trace showed that the retrieval step was working correctly: the right clinical note was retrieved. The prompt was unchanged. The model output was different. The previous model extracted the medication name and dosage correctly. The new model extracted the medication name but formatted the dosage differently, using milligrams per kilogram instead of total milligrams. The expected output used total milligrams. The trace made this immediately clear. The team realized the new model had different default behavior for dosage formatting. They updated the prompt to specify the desired format explicitly. The test passed. The entire debugging process took 20 minutes.

Another common pattern is intermittent failures. A test passes in most runs but fails occasionally. Without traces, these are nearly impossible to debug. Engineers re-run the test dozens of times trying to reproduce the failure. With traces, every failure is logged. Engineers review the traces from the failed runs, compare them to the traces from the passing runs, and identify the differing factor: a retrieval query that sometimes returns a slightly different document ranking, a model output that occasionally includes extra text, a tool call that sometimes times out.

A legal document AI had an intermittent test failure where a contract risk assessment test passed 95 percent of the time and failed 5 percent of the time. Engineers spent weeks trying to reproduce the failure locally. They finally implemented trace capture. The next time the test failed, they reviewed the trace and discovered that the retrieval step occasionally returned documents in a different order because two documents had identical relevance scores and the tiebreaker was non-deterministic. The trace from a passing run showed document A before document B. The trace from a failing run showed document B before document A. The model's final answer was sensitive to document order. The team fixed the issue by adding a deterministic tiebreaker to the retrieval logic. The test never failed again.

## Comparing Passing and Failing Traces

The most powerful debugging technique is side-by-side trace comparison. You take a trace from a test run that passed and a trace from a test run that failed and you compare them span by span, input by input, output by output. The difference reveals the root cause. This technique works for debugging regressions after model upgrades, prompt changes, or infrastructure changes. It also works for debugging intermittent failures where you have multiple traces and some passed and some failed.

Modern observability platforms support trace diffing natively. You select two trace identifiers and the platform highlights the differences. For AI pipelines, the most common differences appear in retrieval results, model outputs, and latency distributions. If retrieval results differ, the failure is likely a ranking or scoring issue. If model outputs differ, the failure is likely a model behavior change or prompt issue. If latency distributions differ, the failure might be a timeout or performance regression that affects downstream components.

A SaaS customer support AI used trace diffing to debug a regression after upgrading their embedding model from a sentence-transformer to OpenAI's text-embedding-3-large in January 2026. Several regression tests started failing. The team pulled traces from the passing runs before the upgrade and traces from the failing runs after the upgrade. The diff showed that retrieval results changed. The new embedding model ranked documents differently. Some queries that previously retrieved the correct document now retrieved a similar but incorrect document. The team realized the new embedding model had different semantic behavior. They re-evaluated their retrieval thresholds, adjusted the minimum relevance score, and re-ran the tests. Some passed. Some still failed. For those, they added the correct documents to the knowledge base with more explicit keyword overlap so the new embedding model would rank them higher. All tests passed.

Trace diffing also reveals when a test expectation is wrong. Sometimes a test fails because the model behavior improved but the expected output was based on the old, worse behavior. The trace diff shows that the new output is actually higher quality. The team updates the expected output and the test passes. This happens more often than teams expect, especially after major model upgrades.

## Trace Visualization Tools

Traces are structured data. Humans read them more effectively when they are visualized. A raw JSON trace with 200 spans is hard to parse. A visual timeline showing each span as a bar with duration and nesting is easy to parse. The engineer sees the execution flow at a glance: which spans took the longest, which spans failed, which spans were retried, which spans ran in parallel.

Most observability platforms provide built-in trace visualization. Datadog shows traces as waterfall charts. Langfuse shows traces as trees. Honeycomb shows traces as flame graphs. Each visualization emphasizes different aspects. Waterfall charts emphasize latency and parallelism. Trees emphasize nesting and component structure. Flame graphs emphasize which components consume the most time.

For AI-specific traces, custom visualizations add value. A RAG trace visualizer shows the query, the retrieved documents with relevance scores as a bar chart, the final prompt with retrieved documents highlighted, and the model output. An agent trace visualizer shows the agent's plan as a sequence diagram with each tool call as a node and each reasoning step as an annotation. A multi-model trace visualizer shows which models were called, which inputs were routed to which models, and which model produced the final output.

A contract analysis AI built a custom trace visualizer for their CI suite. When a test failed, the engineer opened the visualizer, saw the contract text, the extracted clauses highlighted in different colors by type, the expected clauses in one column and the actual clauses in another column, and the differences highlighted in red. The visualizer also showed the retrieval results if retrieval was involved, the prompt sent to the model, and the model's raw output. This single-screen view eliminated the need to jump between log files, test code, and expected output files. Debugging time dropped by 60 percent.

## Sharing Traces Across Teams

When a test fails and an engineer debugs it, the trace and the diagnosis should be available to the entire team. This is especially important for intermittent failures and regressions that multiple engineers might encounter. The trace becomes part of the institutional knowledge. Future engineers who encounter the same failure can search for previous traces, see how it was diagnosed, and apply the same fix or learn that the failure is a known limitation.

Trace sharing requires a trace repository with search and tagging. Each trace is stored with metadata: the test name, the release candidate, the failure type, the root cause if diagnosed, the engineer who investigated, and the resolution. Engineers can search traces by test name, failure type, or date range. They can tag traces with labels like known issue, model regression, prompt bug, or infrastructure failure. They can annotate traces with comments explaining the diagnosis.

A healthcare AI platform implemented a trace repository using Notion as the frontend and S3 as the storage backend. Every time a CI test failed, the trace was uploaded to S3 and a Notion page was created with a link to the trace, the test name, the failure message, and a comment section. Engineers investigating the failure added their findings to the Notion page. Future engineers searching for the same test name could see all previous failures, their traces, and their resolutions. This institutional memory reduced redundant debugging effort by 40 percent. Engineers stopped solving the same problem twice.

## Trace Retention Policies

Traces are large. A single trace might be several megabytes. A full CI run with 200 tests might generate a gigabyte of trace data. Running CI 50 times per day generates 50 gigabytes per day, 1.5 terabytes per month. Storing all traces forever is expensive. Trace retention policies balance storage cost with debugging value.

The most common policy retains all traces for the past 30 days and deletes older traces unless they are tagged as important. Important traces include traces from production incidents, traces from regressions that required significant debugging effort, and traces from tests that uncovered critical bugs. These traces are retained indefinitely. All other traces are deleted after 30 days. If an engineer needs an older trace, they can re-run the test from the corresponding release candidate if the release candidate is still available.

A more sophisticated policy retains all traces from failed tests for 90 days and all traces from passed tests for 7 days. The reasoning is that failed test traces are more likely to be reviewed during debugging and should be available longer. Passed test traces are rarely reviewed and can be deleted quickly. This policy reduces storage costs by 60 percent compared to retaining all traces for 30 days.

A fintech AI team implemented tiered storage. Recent traces are stored in hot storage with fast access. Traces older than 7 days are moved to warm storage with slower access but lower cost. Traces older than 30 days are moved to cold storage with very slow access but very low cost. Traces older than 90 days are deleted unless tagged as important. This policy reduced storage costs by 80 percent while maintaining fast access to recent traces.

## From Trace Debugging to Incident Conversion

Trace-level debugging does more than speed up CI failure diagnosis. It also reveals patterns in how tests fail. These patterns inform test design, test prioritization, and test coverage. When the same component fails repeatedly across different tests, the team knows that component is fragile and needs attention. When a test fails for a reason that was not anticipated during test design, the team learns something about their system and can add tests for similar edge cases.

The next step in regression testing maturity is converting production incidents into regression tests and using trace data from both CI and production to ensure the new test would have caught the incident before release. This feedback loop—incident to test, trace analysis, release gate adjustment—is what transforms regression testing from a static checklist into a continuously learning system that gets smarter with every failure.

