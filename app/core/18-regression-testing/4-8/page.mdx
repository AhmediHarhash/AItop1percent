# 4.8 — Hallucination Increase from Retrieval Shifts

In September 2025, a healthcare information system updated its retrieval pipeline to improve recall. The change increased the number of retrieved documents from five to eight per query. Retrieval metrics improved immediately — coverage went up by 12 percent, and users reported finding more relevant information. Two weeks later, clinical staff began reporting that the system was providing contradictory answers to the same question. A physician asked about a drug interaction twice in the same day and received two different responses, one of which cited a contraindication that did not exist in any medical database. The team investigated and discovered that hallucination rate had increased from 2 percent to 11 percent. The retrieval change had not caused context overflow. It had introduced conflicting information into the context window, and the model was resolving conflicts by inventing facts.

The causal chain between retrieval quality and hallucination is direct but invisible without measurement. When retrieval provides clear, consistent, relevant context, the model grounds its response in that context. When retrieval provides sparse, contradictory, or irrelevant context, the model fills gaps with plausible-sounding fabrications. A retrieval regression does not just reduce answer quality — it increases hallucination frequency. Testing for hallucination regression means testing whether retrieval changes alter the rate at which the model invents information, not just whether retrieval metrics improve.

## The Causal Chain from Retrieval to Hallucination

Hallucinations are not random. They follow patterns, and one of the strongest patterns is **missing context hallucination** — the model invents information when the retrieval system fails to provide it. A customer support system in early 2025 used a retrieval pipeline that covered 85 percent of product documentation. For the remaining 15 percent — edge case features, deprecated APIs, beta functionality — the retrieval system returned nothing. The model, faced with a user question and no retrieved context, generated answers anyway. It described configuration options that did not exist, cited documentation pages that were never written, and recommended workflows that would fail.

The team discovered the problem when users reported following the model's instructions and encountering errors. A product manager asked how to enable a feature that was still in development. The retrieval system returned no results because the feature was undocumented. The model, instead of saying "I don't have information about that feature," provided a detailed walkthrough involving configuration files and API calls. None of it worked. The feature did not exist yet. The model hallucinated the entire answer because retrieval provided no grounding.

The fix was not improving retrieval coverage — that would take months. The fix was **explicit uncertainty detection**. The team added a check: if retrieval returned fewer than two documents with confidence above 0.7, the model was instructed to say "I don't have enough information to answer that accurately" instead of generating an answer. Hallucination rate for uncovered queries dropped from 42 percent to zero. The model stopped inventing when it had no grounding.

## How Retrieval Regressions Manifest as Hallucinations

A retrieval change that reduces **precision** — the relevance of retrieved documents — increases hallucination because the model sees irrelevant context and invents connections that do not exist. A financial analysis system in mid-2025 updated its retrieval model to improve recall for obscure queries. The change worked for obscure queries but degraded precision for common queries. The retrieval system started returning documents that were topically adjacent but not directly relevant. A query about quarterly earnings for a specific company would retrieve earnings reports from competitors, industry analysis documents, and unrelated financial news.

The model, presented with eight documents, only two of which were relevant, attempted to synthesize an answer from all eight. It would cite revenue figures from a competitor's report as if they belonged to the queried company. It would reference industry trends from a generic analysis as if they were specific facts about the target company. The hallucinations were not obvious fabrications — they were factual statements applied to the wrong entity. Users trusted them because they sounded authoritative and included specific numbers.

The team discovered the problem when a financial analyst cross-checked the model's output against source documents and found misattributed data. The hallucination rate had increased from 3 percent to 18 percent after the retrieval change. The fix was rolling back the retrieval model and re-tuning it to prioritize precision over recall. Retrieval metrics improved, and hallucination rate dropped back to 3 percent. The lesson: retrieval changes that optimize for one metric can degrade another, and hallucination is the downstream consequence.

## Distinguishing Retrieval-Caused from Model-Caused Hallucinations

Not all hallucinations are caused by retrieval. Some are intrinsic to the model — patterns learned during training, biases in the base model, or overfitting during fine-tuning. Distinguishing the two requires **controlled testing** where retrieval is held constant and the model is varied, and then the model is held constant and retrieval is varied.

A legal research system in late 2025 saw hallucination rate increase from 4 percent to 9 percent after a model upgrade. The team needed to determine whether the new model was more prone to hallucination or whether retrieval quality had degraded. They ran the same test suite with the old model and new retrieval, then with the new model and old retrieval. The results were clear: hallucination rate was 4 percent with the old model and old retrieval, 9 percent with the new model and old retrieval, and 5 percent with the old model and new retrieval. The new model was the primary cause. The retrieval change contributed slightly, but the model change dominated.

The fix was not rolling back the model — the new model had better reasoning and speed. The fix was adding **hallucination suppression instructions** to the system prompt, explicitly telling the model to only cite information from retrieved documents and to say "this is not covered in the available documents" when grounding was insufficient. Hallucination rate dropped to 3 percent, lower than the baseline with the old model. The system became more accurate by combining the new model with explicit hallucination-aware instructions.

## Hallucination Rate as a Retrieval Health Metric

If hallucination rate increases after a retrieval change, the retrieval change degraded grounding quality, regardless of what retrieval metrics show. **Hallucination rate is the ground truth metric for retrieval health.** Precision and recall measure retrieval system behavior. Hallucination rate measures model output quality, which is what users experience.

A medical Q&A system in 2025 tracked four retrieval metrics: precision, recall, mean reciprocal rank, and normalized discounted cumulative gain. All four metrics improved after a retrieval pipeline upgrade. The team celebrated the improvement and deployed the change. Three days later, clinicians reported the system was providing incorrect dosing information. The team audited 200 recent queries and found hallucination rate had increased from 2 percent to 7 percent. The retrieval metrics had improved, but grounding quality had degraded.

The root cause was that the new retrieval system returned more documents with higher topical relevance but lower factual precision. A query about a specific drug would retrieve eight documents about that drug, but only three would contain dosing information. The other five discussed mechanisms of action, side effects, and contraindications. The model, seeing eight documents, assumed dosing information was present and synthesized it from adjacent facts. The result was plausible but incorrect dosing guidance.

The fix was adding **document-level grounding checks**. Before generating a response, the model was required to identify which retrieved document contained the answer to the query. If no document contained the answer, the model returned "I don't have specific information about that." If multiple documents contained conflicting information, the model flagged the conflict instead of inventing a synthesis. Hallucination rate dropped to 1 percent. The retrieval metrics stayed high, and grounding quality improved.

## Testing Hallucination Correlation with Retrieval Changes

Hallucination regression tests require **paired evaluation** — running the same test cases with the old retrieval configuration and the new retrieval configuration, then comparing hallucination rates. A standalone test that checks whether the model hallucinates on a fixed set of queries is not a regression test. It is a snapshot. A regression test measures whether hallucination behavior changed.

A customer support system in late 2025 built a hallucination regression suite with 300 queries, each with a known correct answer. The suite was run before and after every retrieval change. The metric tracked was **hallucination delta** — the change in the number of hallucinated responses. If the delta was positive, the change increased hallucinations, and the deploy was blocked. If the delta was zero or negative, the change passed.

The suite caught three retrieval regressions in six months. One change increased the number of retrieved documents from six to ten, which improved recall but introduced contradictory information and increased hallucinations by 14 cases. One change updated the embedding model, which shifted retrieval behavior and increased hallucinations by 8 cases. One change adjusted the relevance threshold from 0.75 to 0.65, which retrieved more documents but reduced precision and increased hallucinations by 21 cases. All three changes were rolled back before reaching production. The hallucination regression gate saved the company from deploying changes that would have degraded user trust.

## The Missing Context Hallucination

When retrieval returns no relevant documents, the model has two options: admit ignorance or hallucinate. Most models default to hallucination unless explicitly instructed otherwise. The **missing context hallucination** is the most preventable form of hallucination because it occurs when retrieval returns a clear signal — no results or low-confidence results — and the system ignores that signal.

A travel booking assistant in mid-2025 used a retrieval system to answer questions about hotel policies. When a user asked about a policy that was not documented, the retrieval system returned nothing. The model generated a response anyway, describing a policy that sounded reasonable but was entirely fabricated. Users followed the fabricated policy, expected refunds that did not exist, and complained when the actual policy was different.

The team added a **retrieval confidence gate**. If retrieval confidence was below 0.6 or if fewer than two documents were retrieved, the model returned a fallback response: "I don't have specific information about that policy. Let me connect you with a support agent who can provide accurate details." Missing context hallucinations dropped to zero. The cost was a small increase in support agent escalations. The benefit was eliminating fabricated policy information that led to user frustration and refund disputes.

## The Conflicting Context Hallucination

When retrieval returns multiple documents that contradict each other, the model attempts to synthesize a coherent answer from conflicting inputs. The result is often a **conflicting context hallucination** — a statement that is internally contradictory or that invents a middle ground between two conflicting facts.

A legal research system in late 2025 retrieved case law documents to answer questions about precedent. A query about whether a specific contract clause was enforceable retrieved five cases. Three cases said the clause was enforceable under specific conditions. Two cases said it was not enforceable under different conditions. The model, attempting to synthesize, generated a response that said the clause was "conditionally enforceable depending on jurisdiction and contract type," citing conditions that did not appear in any of the retrieved cases. The model had invented a synthesis that sounded legally sophisticated but was not grounded in the retrieved documents.

The fix was **conflict detection and surfacing**. The system was modified to detect when retrieved documents provided contradictory answers. When conflict was detected, the model did not synthesize. It surfaced the conflict: "Case law is divided on this issue. Three cases found the clause enforceable under X conditions, while two cases found it unenforceable under Y conditions." The response was longer, but it was accurate. Users could see the conflict and make informed decisions. Conflicting context hallucinations dropped from 12 percent to zero for queries with contradictory retrieval results.

## Hallucination Regression Gates

A hallucination regression gate is a release criterion: **no deploy proceeds if hallucination rate increases by more than a threshold amount.** The threshold is context-dependent. A medical system might use a threshold of zero — any increase blocks the deploy. A customer support system might use 5 percent — small increases are tolerated if other metrics improve significantly.

A financial advisory system in 2025 used a hallucination regression gate with a 3 percent threshold. Every retrieval change was tested against a 500-query eval suite with known correct answers. If hallucination rate increased by more than three percentage points, the deploy was automatically blocked. The gate blocked 18 percent of proposed changes in the first year. Each blocked change was investigated, the root cause was identified, and the change was revised or abandoned. The gate prevented hallucination rate from drifting upward over time.

The gate required investment. The eval suite needed 500 carefully labeled queries, each with a verified correct answer and a hallucination detection script. The scripts used exact-match checks for factual claims, citation verification to ensure cited documents existed, and contradiction detection to catch internally inconsistent responses. Building the suite took three months. Maintaining it required ongoing effort as the product evolved. The return was a system that maintained 2 percent hallucination rate for eighteen months while deploying 47 retrieval and model changes. Without the gate, hallucination rate would have drifted upward with each change, and user trust would have eroded gradually until the system was no longer usable.

---

The next subchapter addresses what happens when you rebuild the retrieval index — whether by re-embedding documents, changing chunk sizes, or updating the corpus — and how to validate that the rebuild did not introduce silent retrieval regressions that only surface in production.
