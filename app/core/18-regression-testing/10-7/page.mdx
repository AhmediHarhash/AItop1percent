# 10.7 â€” Incident-to-Test Conversion Process

Every production incident that escaped testing is a test that should exist. The gap between what your test suite catches and what your users experience is where your regression suite needs to grow. Most teams treat incidents as one-time fires to put out, writing a postmortem, deploying a fix, then moving on. The teams that build reliable systems do something different: they convert every significant incident into a permanent test case that prevents recurrence forever.

The incident-to-test conversion process is not optional paperwork. It is the primary mechanism by which production reality informs your testing strategy. Your test suite represents your assumptions about what can go wrong. Production incidents reveal what actually goes wrong. The delta between these two is your blind spot. Close the loop, and your regression suite becomes a living record of every failure mode you have actually encountered. Leave it open, and you will fix the same bugs repeatedly, each time surprised to discover they returned.

## The Incident-to-Test Pipeline

The conversion process begins the moment an incident is detected. Not after the fix is deployed. Not during the retrospective three days later. The moment your on-call engineer identifies the root cause, a parallel track starts: someone must extract the test case while the incident details are fresh and the failure mode is fully understood.

The pipeline has five stages. First, incident classification determines whether the incident warrants a test case. Not every incident does. A one-time infrastructure blip that will never recur does not need a regression test. A model failure that exposed a gap in your eval coverage absolutely does. The classification decision happens within the first hour of root cause identification.

Second, test case extraction captures the specific conditions that triggered the failure. This is not a vague description. It is the exact input, the exact context, the exact model state that caused the problem. If a model hallucinated a medical dosage for a patient with a rare combination of conditions, the test case includes that exact combination. If a prompt failed for inputs longer than 3200 tokens, the test case includes a 3250-token example. Specificity is everything.

Third, test case generalization identifies the broader pattern. The incident happened with one specific input, but the failure mode likely applies to a class of inputs. A fintech company had an incident where their expense categorization model misclassified a transaction from a Brazilian vendor because the currency symbol was non-standard. The specific test case captured that exact vendor and transaction. The generalized test case covered all non-standard currency symbols from all South American countries. Both tests went into the suite. The specific case ensures the exact incident never recurs. The generalized case prevents variants of the same problem.

Fourth, test automation determines how the test case will run. Some incident-based tests become unit-level evals. Some become integration tests that verify model behavior under specific context conditions. Some become end-to-end production simulation tests that replay the exact user journey that triggered the failure. The automation choice depends on where in the stack the failure occurred and what level of testing would have caught it earliest.

Fifth, test prioritization assigns the new test case to a regression tier. High-severity incidents that caused customer impact or data exposure go into Tier 1 regression, running on every commit. Medium-severity incidents that degraded quality but did not break core functionality go into Tier 2, running nightly. Low-severity incidents that affected edge cases go into Tier 3, running weekly. The incident severity determines test frequency, which determines how quickly you catch a recurrence if someone inadvertently reintroduces the bug.

## Extracting Test Cases from Production Incidents

The quality of your incident-to-test conversion depends entirely on the quality of your incident data capture. If your incident report says "model gave wrong answer," you cannot write a meaningful test. If your incident report says "model returned ICD-10 code J18.9 instead of J18.0 for a patient with confirmed bacterial pneumonia because the input context included the phrase 'possible viral origin' in nursing notes," you can write a test that prevents this exact failure forever.

Incident data capture happens during the debugging process. Your on-call engineer is already collecting this data to identify the root cause. The trick is structuring that data so it converts cleanly into a test case. The incident report template must include fields for reproduction steps, exact inputs, exact outputs, expected outputs, and environmental context. These are not optional nice-to-have fields. They are the raw material for test case generation.

A healthcare AI company standardized their incident reports with a test-ready format. Every incident report included a section called "Reproduction Payload" that captured the exact API request, the exact model state, the exact user context, and the exact timestamp. This was not just for debugging. It was so that within thirty minutes of root cause identification, an engineer could copy that payload directly into a test case, add an assertion for the correct behavior, and commit it to the regression suite. The incident response time decreased because engineers knew exactly what data to capture. The test case quality improved because the reproduction steps were precise. The incident recurrence rate dropped to near zero because every failure became a permanent guard rail.

The extraction process must handle both deterministic and non-deterministic failures. Deterministic failures are straightforward. The model always gives the wrong answer for a specific input. The test case replays that input and asserts the correct output. Non-deterministic failures are harder. The model sometimes gives the wrong answer, depending on sampling parameters, context length, or subtle variations in phrasing. For these, the test case cannot assert a single correct output. Instead, it asserts properties: the output must not contain hallucinated entities, must not contradict the input context, must not include prohibited content. Property-based assertions catch non-deterministic failures that exact-match assertions would miss.

## Incident Classification for Test Generation

Not every incident becomes a test. Incident classification is the filter that determines which failures warrant permanent regression coverage. The classification decision is based on three criteria: recurrence likelihood, impact severity, and test feasibility.

Recurrence likelihood measures whether the failure mode could happen again. A model failure caused by a one-time data corruption event that was immediately fixed has low recurrence likelihood. A model failure caused by an underspecified prompt that is still in production has high recurrence likelihood. If the root cause is still present in the system, a test is mandatory. If the root cause was a transient anomaly that has been eliminated, a test may not add value.

Impact severity measures the consequences of the failure. An incident that exposed patient health information requires a regression test, full stop. An incident that caused the model to use British spelling instead of American spelling for one user might not. Severity is measured in concrete terms: data exposure, financial loss, user safety risk, regulatory violation. If an incident triggered any of these, it becomes a test regardless of recurrence likelihood. The cost of it happening again is too high to risk.

Test feasibility measures whether you can actually write a reliable test for this failure mode. Some incidents are caused by race conditions, network latency spikes, or rare interactions between components that are nearly impossible to reproduce in a test environment. If you cannot write a test that reliably catches the failure, the effort is wasted. Instead, you improve observability so that if the incident recurs, you detect it faster. Not every failure mode is testable. The ones that are testable become tests. The ones that are not testable become monitoring rules.

A legal tech company used a scoring matrix for incident classification. Recurrence likelihood scored 0 to 5. Impact severity scored 0 to 5. Test feasibility scored 0 to 5. Any incident with a combined score of 10 or higher automatically became a regression test. Any incident with a score below 5 was marked "monitor only." The middle range triggered a team discussion to decide. This removed ambiguity from the classification decision. Engineers knew within minutes whether an incident required test generation or just better monitoring.

## Automated Versus Manual Test Creation from Incidents

The conversion from incident to test can be automated for some failure modes and must be manual for others. The distinction matters because it determines how quickly the test suite grows in response to production reality.

Automated test creation works when the incident data is structured and the failure mode is well-defined. If your incident report includes the input payload, the expected output, and the actual output, a script can generate a test case automatically. The test becomes a simple assertion: given this input, the model must return this output. A fintech company built a test generation pipeline that ingested incident reports from their ticketing system, extracted the reproduction payload, generated a pytest file with the appropriate assertions, and opened a pull request with the new test. A human reviewed the test for correctness, but the generation was fully automated. The time from incident to regression test dropped from three days to thirty minutes.

Manual test creation is required when the failure mode is subtle, context-dependent, or requires domain expertise to encode correctly. A medical AI incident might involve a model that gave clinically acceptable but suboptimal advice. The incident report captures what the model said and what it should have said, but determining the right assertion requires medical knowledge. Is the test checking for exact phrasing, or is it checking that the advice includes specific clinical considerations? A human with domain expertise must write the test to ensure it actually prevents recurrence without creating false positives.

The hybrid approach is most common. Automated systems generate a draft test case from the incident data, and a human refines it. The automation handles the boilerplate: setting up the test environment, loading the input data, invoking the model, capturing the output. The human handles the assertion: defining what "correct" means for this specific failure mode. A customer support AI company used this approach. Their automation generated 90 percent of the test code. The human engineer spent five minutes per incident writing the assertion logic and adding comments explaining why this test exists. The test suite grew by 150 cases per quarter without increasing engineering burden significantly.

## Prioritizing Which Incidents Become Tests

If you convert every incident into a test, your regression suite becomes bloated and slow. If you convert too few incidents, your suite misses critical failure modes. Prioritization is the art of choosing which incidents add the most value to your regression coverage.

The prioritization framework has four tiers. Tier 1: incidents that caused customer harm, data exposure, or regulatory risk. These become tests immediately, no discussion. Tier 2: incidents that caused visible quality degradation for multiple users. These become tests within one sprint. Tier 3: incidents that affected edge cases or single users. These become tests if capacity allows, or they go into a backlog. Tier 4: incidents that were false alarms or caused no user impact. These do not become tests.

The framework also considers test coverage overlap. If an incident exposed a failure mode that is already covered by an existing test, you do not need a new test. You need to understand why the existing test did not catch it. Either the existing test is insufficiently comprehensive, or the incident represents a genuinely new variant. A content moderation AI team tracked this carefully. When an incident occurred, they first checked whether any existing test should have caught it. If yes, they enhanced the existing test rather than adding a new one. This kept the test suite lean while improving test quality.

Priority also depends on fix difficulty. An incident caused by a simple prompt typo that is immediately corrected might not need a test. The typo is obvious, the fix is trivial, and the likelihood of someone reintroducing the exact same typo is low. An incident caused by a subtle interaction between prompt structure and context length that took three days to debug absolutely needs a test. The failure mode is non-obvious, and without a test, someone will break it again.

A financial services company used a prioritization rubric: incidents that took more than four hours to debug, affected more than ten users, or involved sensitive data became tests automatically. Incidents that met any two of those criteria were reviewed for test generation. Incidents that met none were logged but not tested. This rule kept the test suite growing at a sustainable rate while ensuring high-impact incidents were always covered.

## Test Case Quality from Incident Data

A test generated from an incident is only as good as the incident data it is based on. Poor incident reports produce weak tests. Detailed incident reports produce tests that prevent recurrence forever.

Test case quality starts with input precision. The test must use the exact input that caused the incident, not a simplified version. A medical AI incident involved a patient intake form where the chief complaint field contained 1800 characters of freeform text with multiple medical terms, abbreviations, and patient context. The initial test case used a 200-character summary because the engineer assumed the specific wording did not matter. The test passed. Two months later, the same failure mode recurred with a different long-form input. The test was rewritten to use the full 1800-character original input, and it finally caught the underlying issue: the model degraded when chief complaint exceeded 1500 characters. Precision matters.

Test case quality also requires output specificity. The test must assert exactly what went wrong and what correct behavior looks like. A vague assertion like "output must be reasonable" catches nothing. A specific assertion like "output must not recommend a medication the patient is allergic to, where allergies are specified in the context" catches the exact failure mode. The best incident-to-test conversions include both negative assertions (what must not happen) and positive assertions (what must happen). The negative assertion prevents the specific incident from recurring. The positive assertion ensures the fix did not introduce a different problem.

Test case quality depends on environmental context. If the incident only occurred for users in a specific region, with a specific account type, during a specific time window, the test must replicate that context. A customer support AI had an incident where the model gave incorrect refund policies to users in Germany because it was using the US policy template. The test case did not just replay the user question. It replicated the user account settings, the region flag, the language preference, and the policy version active at the time of the incident. Without that context, the test would have passed using the US settings, missing the Germany-specific failure.

## Tracking Incident-to-Test Coverage

You need a metric that answers the question: what percentage of production incidents have corresponding regression tests? This metric is your incident-to-test coverage rate, and it tells you whether your test suite is learning from production reality or stagnating.

Incident-to-test coverage is calculated per incident severity tier. For critical incidents (Tier 1), the coverage rate should be 100 percent. Every critical incident becomes a test, no exceptions. For high-severity incidents (Tier 2), the coverage rate should be above 80 percent. Most high-severity incidents become tests, with rare exceptions for truly untestable failure modes. For medium-severity incidents (Tier 3), the coverage rate might be 40 to 60 percent, depending on capacity and test feasibility.

A healthcare AI company tracked incident-to-test coverage in their incident management dashboard. Every incident report had a "Test Status" field with three options: Test Created, Test Not Feasible, Test Pending. The dashboard showed the percentage of incidents in each category by severity tier. When Tier 1 coverage dropped below 100 percent, it triggered an alert to the engineering lead. When Tier 2 coverage dropped below 70 percent, it triggered a team review to understand whether they were falling behind or encountering genuinely untestable failures. The metric kept the team honest about whether they were closing the loop from production to test.

The tracking also measures time-to-test: how long does it take from incident resolution to test case deployment? The target depends on severity. Critical incidents should have a corresponding test deployed within 24 hours. High-severity incidents within one week. Medium-severity incidents within one sprint. If time-to-test is exceeding these targets, it signals a bottleneck in the incident-to-test pipeline. Either engineers lack time for test creation, the test infrastructure is too cumbersome, or the incident reports lack the data needed for test generation.

A fintech company discovered that their time-to-test was averaging 12 days for high-severity incidents. The bottleneck was not engineering time. It was that the incident reports did not include reproduction payloads, so engineers had to reverse-engineer the failure conditions from logs and user reports. They updated their incident report template to require reproduction data, and time-to-test dropped to two days. The fix was not more engineers. It was better incident data capture.

## The Incident Retrospective: Test Gap Analysis

The incident retrospective is where the team asks the critical question: why did our test suite not catch this? Every incident that reaches production represents a gap in your testing. The retrospective is the process of understanding that gap and deciding how to close it.

Test gap analysis starts with test coverage mapping. For the incident in question, what level of the stack should have caught it? Unit-level eval? Integration test? End-to-end regression? If the answer is unit-level eval, why is there no unit test for this input pattern? If the answer is integration test, why is there no test for this component interaction? The gap is not just "we need a test for this incident." The gap is "we need a test category for this class of failures."

A legal tech company ran test gap retrospectives for every Tier 1 and Tier 2 incident. The retrospective had three questions. First, what type of test would have caught this incident? Second, do we have tests of that type, and if not, why not? Third, what test categories are we systematically missing? The retrospective surfaced a pattern: they had strong coverage for document classification errors but almost no coverage for response latency degradation. Latency incidents kept reaching production because there were no latency-specific regression tests. The gap analysis led to a new test category: performance regression tests that asserted response time thresholds for common queries. Latency incidents dropped by 80 percent within two months.

The retrospective also identifies systematic blind spots. If multiple incidents share a common root cause, you have a structural testing gap. A customer support AI had three separate incidents over six months, all involving the model mishandling user inputs that contained special characters. Each incident was fixed individually, but the retrospective revealed the pattern: the test suite had almost no adversarial input tests. They added a test category for input robustness, generating 200 test cases with special characters, Unicode, malformed syntax, and injection attempts. The incident rate for input-handling failures went to zero.

Test gap analysis also measures test escape rate: what percentage of bugs reach production despite passing all tests? A high escape rate indicates your test suite is not predictive of production behavior. A low escape rate indicates your tests are effective. The target escape rate is below 10 percent for high-severity bugs and below 5 percent for critical bugs. If your escape rate is higher, your test suite is not learning fast enough from production incidents.

## From Incident to Immunity

The incident-to-test conversion process is how your regression suite becomes antifragile. Every failure strengthens it. Every incident closes a gap. Over time, the test suite evolves from a snapshot of what you thought could go wrong to a comprehensive record of everything that has actually gone wrong. This is the difference between a test suite that catches regressions and a test suite that prevents them.

A fintech company tracked a metric they called "incident immunity rate": the percentage of past incidents that, if reintroduced into the codebase, would be caught by the current test suite. In their first year of incident-to-test conversion, the immunity rate was 40 percent. Many old incidents were not covered by regression tests because the process had not been systematized. Over two years, the immunity rate climbed to 94 percent. Nearly every incident that had ever occurred was now a permanent test case. The recurrence rate for known issues dropped to less than one per year. The test suite had become immune to its own history.

Your incident-to-test conversion process is the feedback loop between production reality and test coverage. Close the loop tightly, and your regression suite becomes your institutional memory of every failure mode you have encountered. Leave the loop open, and you will fix the same bugs repeatedly, each time with the same surprise that they came back. The choice is simple: incidents can be temporary fires you put out, or they can be permanent lessons your test suite learns. One approach creates chronic instability. The other creates reliability that compounds over time.

Next, we turn to the parallel process: converting bugs into regression tests before they ever reach production.
