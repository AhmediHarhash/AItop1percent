# 3.7 — Test Case Design for Non-Deterministic Systems

How do you test something that gives different answers every time? Ask a language model "Summarize this paragraph" and run it twice — you get two different summaries. Run it a hundred times and you get a hundred variations. Traditional software testing assumes determinism: same input, same output, pass or fail. AI systems break that assumption at their core. The randomness is not a bug. It is fundamental to how these models generate text. And it makes regression testing feel impossible.

It is not impossible. It is just different. You cannot test for exact string matches. You cannot write assertions like "output must equal this specific sentence." But you can test for semantic equivalence, distributional stability, and boundary adherence. You can design test cases that respect the non-determinism while still enforcing quality gates. The teams that master this ship confidently. The teams that ignore it ship regressions disguised as improvements.

## The Determinism Decision

The first choice in every test case is whether to eliminate the randomness or embrace it. Most language models expose a **temperature** parameter that controls output variability. At temperature zero, the model becomes nearly deterministic — it picks the highest-probability token at every step. At higher temperatures, it samples more creatively from the probability distribution. For regression testing, you have two paths.

**Path one: force determinism.** Set temperature to zero, fix the random seed, and write exact-match assertions. This works for cases where creativity is not the point — structured output generation, classification tasks, entity extraction. If your model is supposed to return "approved" or "rejected" based on a policy document, exact matches are fine. If it is supposed to extract three fields from a receipt, exact structure checking is correct. You eliminated the variability because the task does not need it.

**Path two: test the distribution.** Leave temperature at its production value and write assertions that tolerate variability within acceptable bounds. This is necessary when the task requires creativity, nuance, or natural language generation. Summarization, content generation, conversational responses — these need temperature above zero. Your test cases must check that the range of outputs is acceptable, not that any single output is perfect.

The mistake most teams make is defaulting to temperature zero because it feels easier. They test deterministically even though their production system runs stochastically. The tests pass. The model ships. Users see outputs the tests never evaluated. A regression in the creative space goes undetected because the test suite never looked there.

## Multi-Run Sampling for Stochastic Tests

When you test at production temperature, a single run tells you almost nothing. The model might generate a perfect answer on the first try and a terrible one on the second. One sample is not evidence. You need multiple samples to understand the output distribution.

The standard approach: run each test case N times and evaluate the set of outputs. N depends on the cost and the risk. For high-stakes decisions — medical triage, legal reasoning, financial advice — run ten or twenty samples per case. For lower-stakes generation — marketing copy, draft emails — three to five may suffice. The number is not arbitrary. It reflects how much confidence you need that the model behaves acceptably across the distribution, not just occasionally.

In early 2025, a customer support platform tested their AI response generator with single-run assertions. The test suite validated one output per scenario and called it passing. In production, users saw the model occasionally generate hostile, sarcastic responses — outputs that appeared in the distribution but never in the single-sample tests. The team switched to five-run sampling and discovered that 8 percent of outputs in certain edge cases were completely unacceptable. The regression suite now blocks any change where more than one in five outputs fails semantic quality checks.

Multi-run testing costs more — five times the inference cost, five times the latency. But it is the only way to test stochastic systems honestly. If you cannot afford to run five samples per test case in CI, you cannot afford to ship the model.

## Consensus-Based Assertions

Once you have multiple samples, you need a rule for passing. Exact matches are out. Instead, you write **consensus-based assertions**: N of M samples must satisfy the criterion. The shape of the rule depends on the severity of failure.

For safety-critical tasks, require unanimity. All five samples must pass. If the model is generating patient medication instructions, a single dangerous output in five runs is unacceptable. The test fails. For quality-sensitive but not safety-critical tasks, allow some variance. Three of five samples must pass semantic similarity checks. Two can be mediocre. This tolerates the natural variability of creative generation without letting quality collapse.

The assertion might check sentiment, toxicity, adherence to constraints, semantic similarity to a reference, or presence of required elements. You run all five samples through the same evaluator and count how many pass. If the count meets your threshold, the test case passes. If not, it fails.

A financial services company ran regression tests on their earnings call summarization model using three-of-five consensus. Each test case generated five summaries. Each summary was scored for factual accuracy against the ground truth transcript. If at least three of the five scored above 0.90 similarity, the case passed. This rule caught a regression where a prompt change caused the model to occasionally fabricate statements that were not in the transcript. The change passed in two of five runs, failed in three, and the gate blocked the deploy.

Consensus thresholds are subjective but not arbitrary. They reflect your tolerance for variance. Set them too strict and you get flaky tests that fail on acceptable randomness. Set them too loose and you miss real degradation. The right threshold comes from analyzing your production distribution and asking: what percentage of outputs can be suboptimal before users notice and complain?

## Distribution-Based Assertions

Instead of checking individual outputs, check properties of the output distribution. This approach treats the set of outputs as the artifact under test. You are not asking "is this specific output good?" You are asking "is this distribution of outputs acceptable?"

Measure the distribution's center and spread. For numeric outputs — confidence scores, sentiment ratings, toxicity levels — calculate the mean and standard deviation across your N samples. Assert that the mean falls within an acceptable range and the standard deviation stays below a threshold. A regression might not shift every output but it might shift the distribution. The mean sentiment score drops from 0.75 to 0.60. The standard deviation doubles, indicating the model became less stable. These are detectable signals even when individual samples look fine in isolation.

For categorical outputs — labels, classifications, structured choices — measure the frequency distribution. If your model is supposed to classify support tickets into five categories, run each test case ten times and check the distribution of labels. A healthy model might return category A in 60 percent of runs, category B in 30 percent, and edge cases in 10 percent. A regression might flip that distribution — suddenly category C appears 40 percent of the time. The test fails not because any single classification is wrong but because the distribution shifted.

For text generation, measure the semantic diversity of the output set. Embed all N outputs using a sentence transformer model and compute pairwise cosine similarities. If the outputs are too similar, the model lost creativity — it is collapsing to a narrow mode. If they are too dissimilar, the model lost coherence — it is generating unrelated responses. Both are regressions. The test asserts that diversity stays within a healthy range.

## Semantic Similarity as a Pass Criterion

The most common assertion for text generation is semantic similarity to a reference answer. You write or curate a ground truth response for each test case. When the model generates an output, you measure how semantically close it is to the reference. If similarity exceeds a threshold — typically 0.85 to 0.95 cosine similarity using a sentence embedding model — the output passes.

This works because semantic similarity tolerates paraphrase. The model does not need to produce the exact reference text. It can say the same thing in different words, different structure, different tone — as long as the meaning aligns. A reference answer might be "The account was closed due to inactivity." The model might generate "This account is no longer active because it was unused." Cosine similarity between their embeddings might be 0.92. That is a pass.

The threshold depends on how much variation you tolerate. For factual accuracy tasks — summarization, technical documentation, compliance responses — set it high: 0.92 or above. For creative tasks — marketing copy, storytelling, conversational responses — set it lower: 0.80 to 0.85. The looser threshold allows more stylistic freedom while still ensuring the core message is present.

The failure mode is setting the threshold too low. At 0.70 similarity, the model can say something vaguely related but substantively wrong and still pass. At 0.60, the outputs barely resemble the reference. The test suite becomes meaningless. Start conservative — set a high threshold, observe false negatives, and lower it only if you are confident the failures are acceptable variation.

A healthcare documentation startup used semantic similarity assertions to test their clinical note generator. Reference notes were written by physicians. Model outputs were scored against them using a BioBERT embedding model fine-tuned on medical text. The threshold was 0.90. Any change that caused more than 10 percent of test cases to drop below 0.90 similarity triggered a regression block. This caught a prompt rewrite that made notes more concise but omitted critical diagnostic details. Conciseness improved. Semantic alignment dropped. The gate held.

## The Mutation Testing Approach

Traditional software uses mutation testing to find weak test cases: change the code slightly and see if tests catch it. In AI regression testing, mutation testing means changing the input slightly and checking if the output changes appropriately. This tests robustness — small input perturbations should not cause large output shifts.

Design test cases with near-duplicate inputs that differ in small but meaningful ways. If you have a test case for summarizing a news article, create a mutant where you change a single fact — a date, a name, a number. Run both through the model. The outputs should differ only in the corresponding detail. If the mutation causes a completely different summary, the model is fragile. If the mutation is ignored and the outputs are identical, the model is not reading carefully.

Another mutation: paraphrase the input. Take a test case prompt and rewrite it with synonyms, different sentence structure, but the same meaning. The model's output should remain semantically stable. If paraphrasing the question changes the answer's meaning, the model is overfitting to surface phrasing rather than understanding intent. This is a regression in robustness.

A legal tech company used mutation testing on their contract clause extraction model. For each test contract, they created three mutants: one with a date changed, one with a party name changed, one with a financial amount changed. The model was supposed to extract the new values correctly and leave unrelated clauses unchanged. A fine-tuning update passed standard tests but failed mutation tests — changing a date in one clause caused the model to misread an unrelated clause elsewhere in the document. The mutation tests caught what static tests missed.

Mutation coverage does not need to be exhaustive. Five to ten percent of your test cases having mutants is enough to detect brittleness. The goal is not to cover every possible perturbation. It is to check that small, realistic input changes do not cause disproportionate output instability.

## Edge Case Design for AI Systems

Edge cases in AI are not the same as edge cases in traditional software. You are not testing boundary conditions on integers or null pointer handling. You are testing the boundaries of the model's knowledge, the limits of its instruction-following, and the edge of acceptable behavior.

Design test cases for prompts the model should refuse. Requests for illegal advice, harmful instructions, personal data extraction, jailbreak attempts. The model should decline gracefully. If a regression makes it more compliant in general, does it also make it compliant with harmful requests? Test that.

Design test cases for ambiguous inputs. Prompts that could be interpreted multiple ways. The model should either ask for clarification or choose the most reasonable interpretation. If a regression changes how the model resolves ambiguity, you need to know.

Design test cases for knowledge boundaries. Ask the model about events after its training cutoff, niche domains it was not trained on, or intentionally obscure facts. The model should express uncertainty or admit ignorance. If a regression makes it hallucinate confidently, that is worse than the baseline.

Design test cases for multi-turn interactions where context matters. The model should remember what was said three turns ago. It should not contradict itself. It should not repeat the same response. If a regression breaks conversational coherence, single-turn test cases will miss it.

A conversational AI for enterprise support had test cases for polite refusals, handling profanity, and de-escalation. One regression test involved a user asking the same question five times in a row. The expected behavior: the model should recognize repetition and offer to escalate to a human. A model update passed all single-turn tests but failed this one — it answered the same way all five times, never detecting the loop. The test caught a regression in conversational awareness that would have frustrated users immediately.

## Test Case Independence and Order Effects

AI models can exhibit order effects in multi-turn conversations or batch inference. The Nth item in a batch might be influenced by the N minus one items before it. A test case might pass in isolation but fail when run after another test case. This violates a core testing principle: test cases should be independent.

To test for order effects, run your suite in random order and check for flakiness. If a test case passes when run first but fails when run fifth, there is an order dependency. The model might be maintaining hidden state across requests, or the inference server might be caching incorrectly, or the test harness might be leaking context between cases.

For conversational models, test both short and long conversation histories. A regression might only appear after fifteen turns, when the context window is nearly full. If your test cases only cover three-turn conversations, you will miss it.

For batch inference, test both single-item batches and large batches. A model might behave differently when processing 100 items at once versus one at a time. Token-level caching, batching optimizations, or memory constraints might introduce subtle shifts. If your CI tests only single-item inference and production uses batch processing, the environments diverge and regressions slip through.

A batch processing pipeline for document classification tested individual documents in CI but processed documents in batches of 50 in production. A model update introduced a caching bug where the model reused embeddings from the previous document in the batch when processing the current one. Test cases in CI passed because they ran one document at a time. The regression hit production immediately. The team added batch-aware test cases and caught the issue in the next update.

## From Test Design to Statistical Proof

You have designed test cases that respect non-determinism, run multiple samples, assert on distributions, and check for fragility. But even with all this rigor, you still face a question: when you compare two model versions and see a difference, how do you know it is real and not random noise? The answer is statistical significance — the mathematical threshold that separates signal from chance. That is what we address next, in 3.8.
