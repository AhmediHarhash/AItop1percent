# 3.11 â€” Multiple Comparison Corrections

Most teams believe that running more tests increases rigor. They add 50 regression checks to their suite, set each one to trigger at 95 percent confidence, and assume that their release gate is now more reliable. The math says otherwise. With 50 independent tests each using a five percent false positive rate, the probability of at least one false alarm is 92 percent. Their gate is not more rigorous. It is a false positive generator. Nearly every release will trigger a spurious failure, even when the model has not regressed. The team will investigate, find nothing, override the gate, and eventually stop trusting it. The gate becomes ignored, then removed. The root cause was not bad tests. It was bad statistics.

The multiple testing problem is simple. Every statistical test has a chance of producing a false positive, a Type I error where you reject the null hypothesis when it is actually true. For a single test with a five percent significance level, you have a five percent chance of a false alarm. For two independent tests, the probability of at least one false alarm is not five percent but 9.75 percent. For ten tests, it is 40 percent. For 50 tests, it is 92 percent. The more tests you run, the more likely you are to see at least one false positive, even when nothing is wrong. This is called the family-wise error rate, the probability of making at least one Type I error across a family of tests.

If you do not correct for multiple comparisons, your release gate becomes a coin flip. A logistics company ran 80 tests in their regression suite, each checking a different quality dimension or edge case subset. Each test used a five percent threshold. They saw gate failures on 70 percent of releases. Engineering spent half their time investigating false alarms. After six months, they began overriding the gate without investigation, assuming failures were noise. They eventually shipped a real regression that caused a production incident because they had trained themselves to ignore gate signals. The gate had cried wolf too many times. The failure was not in the tests themselves but in the lack of multiple comparison correction.

The principle is straightforward. When you run multiple tests as part of a single decision, you must adjust your thresholds to control the overall false positive rate, not just the per-test rate. There are several correction methods, each with different tradeoffs between conservatism and power. Choosing the wrong method or failing to apply any correction at all turns your release gate into a reliability liability.

## Family-Wise Error Rate and the Cost of False Alarms

The family-wise error rate is the probability that at least one test in your suite produces a false positive. If each test has a false positive rate of alpha, and you run n independent tests, the family-wise error rate is one minus the probability that all tests pass correctly, which is one minus the quantity one minus alpha raised to the power of n. For small alpha and moderate n, this approximates to n times alpha. For 20 tests at five percent each, the family-wise error rate is roughly 64 percent. For 100 tests, it approaches 99 percent. You are virtually guaranteed a false alarm.

This matters because release gates operate on a simple principle: if any test fails, block the release. A suite with a 64 percent family-wise error rate blocks 64 percent of safe releases. Engineering investigates, finds no real issue, and overrides the gate. The override process wastes time. If each investigation takes two hours and you release weekly, that is 1.3 hours per week wasted on false alarms, or 67 hours per year, nearly two engineering weeks lost to noise. If your team releases daily, the cost multiplies by five. The opportunity cost of those hours, the delayed releases, and the eroded trust in tooling far exceed the cost of implementing proper corrections.

False alarms also have a psychological cost. When a release gate triggers, engineering takes it seriously the first five times. They investigate thoroughly. They check logs, re-run tests, compare outputs. By the tenth false alarm, they stop investigating and start assuming the gate is wrong. By the twentieth, they start lobbying to remove the gate entirely. A gate that cries wolf becomes a gate that gets ignored or dismantled. Proper multiple comparison correction keeps the false alarm rate low enough that engineers trust the gate and act on its signals.

A financial services company tracked this explicitly. Before applying corrections, their 60-test suite had a 95 percent family-wise error rate. They saw gate failures on 19 out of 20 releases. Engineering spent an average of 90 minutes per release investigating false alarms. After applying Holm-Bonferroni correction, the family-wise error rate dropped to five percent, and they saw gate failures on one in 20 releases. Investigation time dropped to five minutes per release on average because the one-in-20 signal was usually real. Engineering trust in the gate recovered, and the gate was no longer bypassed.

## Bonferroni Correction: Simple and Conservative

The Bonferroni correction is the simplest and most conservative multiple testing correction. To control the family-wise error rate at level alpha across n tests, set the per-test significance threshold to alpha divided by n. If you want a five percent family-wise error rate across 20 tests, each test uses a 0.25 percent threshold. If a test shows a p-value less than 0.0025, you reject the null hypothesis and flag a regression. Otherwise, you accept the null and allow the release.

The advantage of Bonferroni is simplicity. It requires no assumptions about the correlation structure between tests. It works whether your tests are independent, positively correlated, or negatively correlated. It is easy to implement: divide your target alpha by the number of tests and use the result as the per-test threshold. A healthcare company with 40 tests and a desired five percent family-wise error rate set their per-test threshold to 0.125 percent. Any test with a p-value above that was considered non-significant. The gate only blocked releases when at least one test had a p-value below 0.00125, which happened infrequently and was almost always a real regression.

The disadvantage of Bonferroni is conservatism. By dividing alpha by n, you reduce the per-test false positive rate so aggressively that you also reduce power, the probability of detecting a real regression. For large n, the per-test threshold becomes so strict that only very large regressions are detected. Small or moderate regressions slip through. A media company with 100 tests applied Bonferroni and found that their gate only caught regressions larger than ten percentage points. Smaller regressions, which were still meaningful, went undetected. The gate had high specificity but low sensitivity. They were avoiding false positives but missing true positives.

Bonferroni is most appropriate when n is small, say fewer than 20 tests, or when the cost of a false positive is extremely high, such as in safety-critical systems where a false alarm triggers an expensive rollback or manual review process. For larger test suites or contexts where false negatives are costly, less conservative corrections are preferable.

## Holm-Bonferroni: Slightly More Power

The Holm-Bonferroni correction, also called the Holm step-down procedure, is a refinement of Bonferroni that provides more power while still controlling the family-wise error rate. It works as follows. First, compute the p-value for each of your n tests. Second, sort the p-values from smallest to largest. Third, compare the smallest p-value to alpha divided by n. If it is smaller, reject that test's null hypothesis and move to the next p-value. Fourth, compare the second-smallest p-value to alpha divided by n minus one. If it is smaller, reject and continue. Fifth, repeat, comparing each successive p-value to alpha divided by the number of remaining tests, until you encounter a p-value that fails the test, at which point you stop and accept the null for all remaining tests.

Holm-Bonferroni is uniformly more powerful than Bonferroni, meaning it detects more true regressions without increasing the false positive rate. The reason is that it adjusts the threshold dynamically based on how many tests have already been rejected. After rejecting one test, the threshold for the next test is slightly less strict. After rejecting two, it is less strict still. This sequential adjustment increases power without sacrificing family-wise error rate control.

A logistics company with 50 tests switched from Bonferroni to Holm-Bonferroni and detected three additional real regressions over six months that Bonferroni had missed. The false positive rate stayed the same, five percent per release cycle, but the true positive rate increased by 15 percent. The implementation cost was minimal, just sorting the p-values and applying a sequential threshold. The improvement in gate sensitivity was immediate.

The tradeoff is complexity. Holm-Bonferroni requires you to compute p-values for all tests, sort them, and apply sequential logic. Bonferroni requires only a single division. For automated pipelines, this difference is trivial, and Holm-Bonferroni is nearly always preferable. The only reason to stick with Bonferroni is if your tooling does not support sequential testing or if the regulatory environment requires the simplest possible correction method for auditability.

## Benjamini-Hochberg: Controlling False Discovery Rate

The Benjamini-Hochberg correction takes a different approach. Instead of controlling the family-wise error rate, the probability of any false positives, it controls the false discovery rate, the expected proportion of false positives among all rejected null hypotheses. If you reject ten tests and accept a ten percent false discovery rate, you expect one of those ten rejections to be a false positive. This is less conservative than controlling family-wise error rate, which aims for zero false positives with high confidence.

The procedure is similar to Holm-Bonferroni but uses a different threshold. First, compute and sort the p-values. Second, find the largest p-value that is less than or equal to its rank divided by n, multiplied by the desired false discovery rate q. Third, reject all null hypotheses with p-values less than or equal to that threshold. The result is that you reject more tests than Bonferroni or Holm-Bonferroni would, increasing power, but with a controlled expectation of false discoveries.

A customer support company with 80 tests used Benjamini-Hochberg with a ten percent false discovery rate. They found that it detected 30 percent more regressions than Holm-Bonferroni while keeping the number of false alarms acceptably low. Over a year, they saw roughly one false positive per ten releases, which was tolerable given their release cadence and investigation costs. The increased sensitivity allowed them to catch smaller regressions that would have been missed by stricter corrections. The tradeoff was occasionally investigating a false alarm, but the value of catching real regressions earlier in the process justified the cost.

Benjamini-Hochberg is most appropriate when false negatives are more costly than false positives, and when you can tolerate a small number of false alarms in exchange for higher detection rates. It is widely used in genomics, where testing thousands of hypotheses simultaneously is common, and where missing a true signal is worse than following up on a few false leads. In AI release gates, it is appropriate for exploratory regression suites where you want high sensitivity and are willing to invest time investigating marginal signals. It is less appropriate for strict quality gates where any false positive undermines trust.

## When to Use Each Correction Method

Choosing the right correction depends on three factors: the number of tests, the cost of false positives versus false negatives, and the correlation structure of your tests. Bonferroni is appropriate when you have fewer than 20 tests and the cost of a false positive is very high, such as in safety-critical domains where a false alarm triggers expensive manual review or a production rollback. It is simple, auditable, and guarantees strict control of the family-wise error rate.

Holm-Bonferroni is appropriate when you have 20 to 100 tests and want better power than Bonferroni without giving up strict family-wise error rate control. It is the default choice for most AI release gates. It is nearly as simple as Bonferroni, more powerful, and still conservative enough to keep false positives rare. A team with 50 tests, a five percent false positive budget, and a preference for conservatism should use Holm-Bonferroni.

Benjamini-Hochberg is appropriate when you have many tests, greater than 100, or when the cost of missing a regression exceeds the cost of a false alarm. It is the right choice for high-sensitivity regression suites where you want to catch every possible regression and are willing to investigate a few false leads. It is also useful in early-stage development when you are exploring which metrics matter and want to avoid missing signals. A team with 200 tests, a preference for sensitivity, and a tolerance for occasional false positives should use Benjamini-Hochberg with a ten to 15 percent false discovery rate.

None of these methods work well when tests are highly correlated. If your 50 tests all measure variants of the same underlying quality dimension, such as 50 slightly different phrasings of the same eval question, they are not independent. Applying Bonferroni or Holm-Bonferroni will be overly conservative because the effective number of independent tests is much smaller than 50. In such cases, you should either deduplicate correlated tests or use hierarchical testing strategies that account for correlation structure.

## Hierarchical Testing and Reducing Correction Burden

One way to reduce the correction burden is to organize tests hierarchically and apply corrections at multiple levels. Instead of treating 100 tests as a flat list, group them into categories: accuracy tests, safety tests, latency tests, cost tests, user satisfaction tests. Within each category, apply a correction to control the family-wise error rate. Across categories, apply a second correction. This two-level approach reduces the correction penalty because you are dividing the alpha budget more efficiently.

A healthcare company with 60 tests grouped them into six categories of ten tests each. They used Holm-Bonferroni within each category to control the category-level family-wise error rate at five percent, and then used Bonferroni across the six categories to control the overall family-wise error rate at five percent. This meant each category had a per-test threshold of 0.5 percent, and the cross-category threshold was 0.83 percent. The result was higher power than flat Bonferroni across all 60 tests, because the hierarchical structure reduced the effective number of comparisons. They detected 20 percent more regressions without increasing false positives.

Hierarchical testing also allows you to prioritize critical tests. Suppose you have ten safety-critical tests and 90 exploratory tests. You can set a strict five percent family-wise error rate for the safety tests using Holm-Bonferroni, and a looser 20 percent false discovery rate for the exploratory tests using Benjamini-Hochberg. A release is blocked if any safety test fails. The exploratory tests are used for investigation and trend monitoring but do not hard-block releases. This tiered approach applies strict statistical rigor where it matters most and allows for exploratory signals elsewhere without overwhelming the gate with false positives.

Another hierarchical strategy is sequential testing. Instead of running all 100 tests simultaneously, run the ten most critical tests first. If none fail, proceed with the release without running the remaining 90 tests. If one or more of the critical ten fail, then run the full suite to gather diagnostic information. This reduces the effective number of comparisons for most releases, keeping the correction penalty low while still allowing deep investigation when a signal is detected. A logistics company used this approach and reduced their median test suite runtime from 90 minutes to 20 minutes, running the full suite only when an initial red flag was raised.

## The Correction-Power Tradeoff and Practical Guidance

Every correction method trades off two goals: controlling false positives and maintaining power to detect true regressions. Stricter corrections reduce false positives but also reduce power. Looser corrections increase power but increase false positives. The optimal point depends on the costs of each error type. In domains where a missed regression causes significant user harm, such as healthcare or finance, you should accept a higher false positive rate to maintain high power. In domains where false positives waste expensive engineering time or trigger complex rollback procedures, such as large-scale infrastructure changes, you should use stricter corrections even if it means missing small regressions.

A practical framework is to estimate the cost of each error type in dollars and engineering hours. Suppose a false positive costs two hours of engineering investigation and two hours costs roughly 200 dollars in loaded salary. Suppose a false negative ships a regression that causes 5,000 dollars in user churn or support costs before it is caught and rolled back. The cost ratio is 25 to one in favor of avoiding false negatives. You should use a less conservative correction, such as Benjamini-Hochberg with a ten percent false discovery rate, to maximize power even at the cost of more frequent false positives. Conversely, if a false positive triggers a 24-hour delay in a major product launch and the delay costs 50,000 dollars in lost revenue, and a false negative causes 5,000 dollars in churn, the cost ratio is ten to one in favor of avoiding false positives. You should use a strict correction like Bonferroni.

Most teams do not perform this cost analysis explicitly and default to arbitrary thresholds like five percent per test without correction. This is a mistake. The five percent threshold was designed for single hypothesis tests, not for test suites. Applying it naively to a suite of 50 tests gives you a 92 percent false positive rate, which is absurd. The correct default is to apply Holm-Bonferroni to control family-wise error rate at five percent, which is conservative but defensible. Teams can then tune from there based on empirical false positive and false negative rates observed over time.

Another practical consideration is test suite size. If your suite has grown to 200 tests, you should ask whether all 200 are necessary. Correlated tests add correction burden without adding information. A test that has never caught a regression in two years is dead weight. A test that triggers on noise frequently is worse than no test because it trains engineers to ignore signals. Regularly audit your test suite, remove redundant or noisy tests, and consolidate correlated tests into single metrics. A lean suite of 30 high-signal tests with proper corrections is more effective than a bloated suite of 200 tests with no corrections.

## Grouping Tests and Prioritizing Critical Metrics

Not all tests are equally important. Some measure core functionality that must never regress. Others measure secondary quality dimensions or edge cases that are worth monitoring but not worth blocking releases over. Instead of applying uniform corrections across all tests, you can group tests by importance and apply different correction strategies to each group.

A fintech company divided their 70 tests into three groups. Group A contained ten critical tests covering accuracy on high-stakes financial calculations, compliance with regulatory requirements, and absence of harmful outputs. These used Bonferroni correction with a strict one percent family-wise error rate. Any failure in Group A blocked the release with no override. Group B contained 30 important tests covering general accuracy, latency, and cost. These used Holm-Bonferroni with a five percent family-wise error rate. Failures in Group B triggered investigation and usually blocked the release unless engineering could prove the failure was a false alarm. Group C contained 30 exploratory tests covering tone, edge cases, and new features. These used Benjamini-Hochberg with a 20 percent false discovery rate. Failures in Group C were logged and investigated but did not block releases. This tiered approach gave them strict control where it mattered, reasonable sensitivity in the middle, and exploratory flexibility at the edges.

Grouping also allows you to assign different variance models to different test groups. Critical tests should have low variance, measured with large samples and deterministic scoring. Exploratory tests can tolerate higher variance because they are not used for hard decisions. A media company found that their Group A tests had noise floors below one percent, their Group B tests had noise floors around three percent, and their Group C tests had noise floors above five percent. They set thresholds accordingly: strict thresholds for Group A, moderate for Group B, and loose for Group C. This alignment between test importance, variance, and correction strategy created a coherent quality gate that was both sensitive and specific across the full spectrum of quality dimensions.

Prioritization also helps with communication. When a release is blocked, engineering needs to know whether it failed a critical test or an exploratory one. A single unified gate that treats all tests equally obscures this information. A tiered gate that clearly labels tests by criticality makes it obvious which failures are blockers and which are warnings. This reduces confusion, speeds up investigation, and ensures that engineering effort is allocated to the highest-impact issues.

As your test suite grows and evolves, it accumulates cruft. Tests that were once useful become redundant. Tests that were once reliable become noisy. Edge cases that were once important are no longer exercised by users. Thresholds that were once calibrated drift out of alignment with current system behavior. Without active maintenance, your test suite degrades, and your multiple comparison corrections become either too strict or too loose, undermining gate effectiveness.

