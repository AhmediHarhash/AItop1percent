# 3.6 — Regression Tests: Baseline Comparison

A regression test without a baseline is not a test. It is a measurement with no meaning. The entire purpose of regression testing is to answer one question: did this change make things worse? You cannot answer that question unless you know what "worse" means, and you cannot know what "worse" means unless you have established what "acceptable" looked like before the change. The baseline is not optional. It is the foundation of every decision you will make about whether a model is safe to deploy.

Most teams understand this in theory. In practice, they fail at baseline selection, threshold definition, and regression interpretation. They compare the wrong versions, set arbitrary thresholds, ignore granular regressions in favor of aggregate scores, and treat every detected regression as equally important. The result is a regression testing process that catches nothing meaningful or flags so many false positives that the team stops trusting it. Neither outcome is acceptable.

## The Baseline Is Not "The Old Model"

In early 2025, a legal tech company ran regression tests on a new contract extraction model. They compared it to the previous model and celebrated when aggregate accuracy improved by four percentage points. They shipped the new version. Within two weeks, customer support escalated complaints from enterprise clients: the model was missing force majeure clauses at rates the old model never exhibited. The team ran the regression tests again and confirmed the overall accuracy improvement. The issue was not in the test results. It was in the baseline choice.

The team had compared the new model to a version from eight months earlier — the last major release. But in the intervening months, they had shipped three patch releases that fixed critical bugs in force majeure extraction. Those fixes never made it into the training data for the new model. The regression tests compared the new model to a baseline that represented "shipped state eight months ago," not "best known capability today." The four percentage point aggregate improvement masked a catastrophic regression on a capability that paying customers depended on.

**Baseline selection determines what regressions you can detect.** The baseline must represent the best known performance of the system you are replacing, not the earliest version you have records for. If your production system is running version 2.3.7 with three hotfixes applied, your baseline is version 2.3.7 with those hotfixes — not version 2.0 from six months ago. If you have run experiments that never shipped but demonstrated capabilities you want to preserve, those capabilities belong in your baseline definition, even if they come from multiple model versions.

The most rigorous teams maintain a **golden baseline**: a frozen snapshot of the current production system, run against the same test suite used for regression evaluation, with results stored permanently. When a new model version is proposed, it is compared against the golden baseline — same inputs, same evaluation criteria, same thresholds. The comparison is direct and unambiguous. If the new model performs worse on any dimension that matters, the regression is flagged. If the team decides to ship anyway, the decision is documented, and the golden baseline is updated to reflect the new known-good state.

## Absolute Thresholds Versus Relative Thresholds

A regression can be defined two ways: absolute performance below a minimum acceptable threshold, or relative performance decline compared to baseline. Both matter, but they detect different failure modes.

**Absolute thresholds** define the floor below which a capability is considered broken. If your production model achieves 92 percent precision on entity extraction, and your absolute threshold is 90 percent, then any model that scores below 90 percent precision fails the regression test — even if the baseline only scored 91 percent. Absolute thresholds protect against catastrophic failure. They encode the minimum quality standard the business requires. They are especially important for safety-critical tasks: content moderation, fraud detection, medical coding, compliance classification. On these tasks, falling below the threshold is not a tradeoff decision. It is a blocker.

**Relative thresholds** define acceptable degradation compared to baseline. If your baseline model achieves 92 percent precision, and your relative threshold is two percentage points, then any model scoring below 90 percent precision fails the regression test — even if 90 percent would be acceptable in absolute terms. Relative thresholds protect against silent capability erosion. They force the team to justify why a capability got worse, even if the degraded capability is still "good enough." They prevent the gradual slide that occurs when every release sacrifices a little performance on tasks the team is not actively monitoring.

Most regression frameworks require both. A model must exceed absolute minimums on all critical dimensions and must not regress more than the relative threshold on any dimension compared to baseline. If either condition fails, the regression test fails. The model does not ship until the team either fixes the regression or explicitly accepts it with documented justification.

## Aggregate Scores Hide Granular Regressions

The legal tech company's failure mode is common. Aggregate accuracy improved, so the team assumed the model was better. They did not look at task-level breakdowns. The regression was invisible at the aggregate level and catastrophic at the task level.

**Regression tests must evaluate every capability the system is expected to perform, not just the overall average.** If your model handles ten distinct tasks — entity extraction, sentiment classification, summarization, translation, question answering, code generation, moderation, ranking, reasoning, and formatting — then your regression suite must measure performance on all ten tasks independently. An improvement in summarization does not compensate for a regression in translation. A customer who depends on translation does not care that the average score went up.

In practice, this means maintaining **capability-specific test sets** within your regression suite. Each test set targets one task. Each task has its own baseline, its own absolute thresholds, and its own relative thresholds. The regression report breaks down performance by task before aggregating. If the aggregate score improves but three tasks regressed, the regression test fails. The team must investigate why those three tasks degraded and decide whether the tradeoff is acceptable.

The most mature teams go further: they track regressions at the **example level**. If a test set contains 500 examples, and the new model performs worse than baseline on 50 of those examples, the regression report lists those 50 examples explicitly. The team reviews them. They identify patterns. They discover that the new model struggles with examples involving nested entities, multi-hop reasoning, or ambiguous phrasing — patterns that the aggregate score concealed. Example-level regression analysis turns a vague quality signal into actionable debugging information.

## Handling Intentional Regressions

Not all regressions are bugs. Some are tradeoffs. You accept a two percentage point regression on summarization quality to gain a 40 percent reduction in latency. You accept a slight increase in false negatives on content moderation to reduce false positives that frustrate users. You accept worse performance on rare edge cases to improve the common case. These are legitimate engineering decisions. But they must be made explicitly, not discovered after deployment.

**If a regression is intentional, it must be documented before the model ships.** The documentation includes the capability that regressed, the magnitude of the regression, the benefit gained in exchange, the stakeholders who approved the tradeoff, and the monitoring plan to verify the tradeoff holds in production. Without this documentation, the regression is indistinguishable from an accident. Three months later, when someone investigates why performance declined, they will not know whether it was a bug or a deliberate choice.

Some teams use **regression waivers**: formal exceptions granted by a decision-making authority — usually a product leader, engineering manager, or cross-functional review committee. A waiver allows a model to ship despite failing a regression test. It includes the same documentation as any intentional regression, plus a rationale for why the team believes the tradeoff is worth it. Waivers are recorded in the release notes, linked to the regression test failure, and tracked over time. If a team issues three waivers for the same capability in three consecutive releases, leadership notices. The pattern suggests the team is systematically sacrificing a capability the business may still care about.

## Comparison Methodology: Same Inputs, Comparable Outputs

Running a valid regression comparison requires controlling for everything except the model itself. The test inputs must be identical. The evaluation criteria must be identical. The execution environment must be as similar as possible. If any of these factors changes between baseline and candidate, the comparison is contaminated, and the results are unreliable.

**Same inputs** means the candidate model and the baseline model are evaluated on the exact same test set — same examples, same order, same prompts, same tool schemas if the model uses tools. If the test set is modified between runs, the results are not comparable. If you discover a bug in the test set after running the baseline, you must rerun the baseline with the corrected test set before comparing it to the candidate.

**Comparable outputs** means the evaluation criteria applied to baseline outputs and candidate outputs are identical. If you used a rubric-based LLM judge to score baseline outputs, you use the same rubric, the same judge model, the same prompt to score candidate outputs. If you used human raters to score baseline outputs, you use human raters with the same instructions to score candidate outputs. Changing the evaluation method between runs makes the comparison meaningless. A model that appears to regress may simply be scored by a stricter judge.

**Execution environment** is harder to control but still matters. If the baseline was evaluated with temperature 0.0 and the candidate is evaluated with temperature 0.7, the comparison is invalid. If the baseline was evaluated with a maximum token limit of 1000 and the candidate uses 2000, outputs will differ for reasons unrelated to model quality. If the baseline ran on GPUs with 16-bit precision and the candidate runs on CPUs with 8-bit quantization, inference behavior may diverge. The more variables you control, the more confident you can be that observed differences reflect actual model changes rather than environmental noise.

## False Positives: When a Regression Is Noise

Not every detected regression is real. Some are artifacts of non-deterministic model behavior, evaluation noise, or statistical variance. A model that scores 91.2 percent on one run and 90.8 percent on the next has not necessarily regressed. It may have produced slightly different outputs due to sampling, and those outputs happened to score slightly worse under the evaluation criteria. If you flag this as a regression and block the release, you have introduced a false positive that wastes engineering time.

**Statistical significance testing** is one mitigation. Instead of comparing single point estimates, run the baseline and candidate models multiple times — usually five to ten runs — and compare the distributions. If the candidate's mean score is lower than the baseline's mean score by more than two standard deviations, the regression is likely real. If the difference is within the noise range, it is likely a false positive. This approach requires more compute — you are running every test multiple times — but it reduces the rate at which random variance triggers false alarms.

**Confidence intervals** provide another lens. If the baseline scored 92 percent with a 95 percent confidence interval of 90 to 94 percent, and the candidate scored 91 percent with a confidence interval of 89 to 93 percent, the intervals overlap. The observed difference may not be statistically meaningful. If the candidate scored 87 percent with a confidence interval of 85 to 89 percent, the intervals do not overlap, and the regression is likely real.

**Threshold buffers** offer a simpler heuristic. Instead of flagging any decline, flag declines that exceed a minimum delta — usually one to three percentage points, depending on task criticality. If the baseline scored 92 percent and the candidate scored 91.5 percent, the half-point difference is within the buffer, and the regression test passes. If the candidate scored 89 percent, the three-point drop exceeds the buffer, and the regression test fails. This approach is less rigorous than statistical testing but requires less compute and is easier to explain to non-technical stakeholders.

## The Regression Report: What to Include

When a regression test completes, the output is a regression report. The report is the primary artifact that informs the ship-or-block decision. It must be clear, actionable, and complete.

A well-structured regression report includes the **model identifiers** for both baseline and candidate, the **test set version** used for comparison, the **evaluation date and time**, and the **overall pass or fail status**. It includes **aggregate scores** for each metric tracked — accuracy, precision, recall, F1, latency, cost per query — along with baseline scores and deltas. It includes **task-level breakdowns** showing performance on each capability independently, with per-task pass or fail indicators. It includes **example-level failures** if any examples regressed significantly, listing the example IDs and the magnitude of the regression.

If the regression test failed, the report includes **failure reasons**: which thresholds were violated, which tasks regressed, and by how much. If the regression test passed with warnings — candidate performance declined but remained within acceptable thresholds — the report lists the warnings and the magnitude of the decline. If any regressions were waived, the report includes the waiver justification and the approving authority.

The report is distributed to the team responsible for the model and to any stakeholders who depend on the capabilities being tested. It is stored permanently, linked to the model version, and included in the release notes if the model ships. If the model does not ship, the report documents why, creating a record that future engineers can reference when investigating similar regressions.

## Who Sees the Report, and What Happens Next

The regression report is not just a technical artifact. It is a communication tool that drives cross-functional decisions. Engineering reviews the report to understand what broke. Product reviews it to assess whether the degraded capabilities affect user-facing features. Leadership reviews it to decide whether the tradeoffs are acceptable. If the report shows a critical regression, the model does not ship until the regression is fixed or explicitly accepted. If the report shows minor regressions within acceptable bounds, the model ships with the regressions documented.

The most disciplined teams treat regression reports as **blocking artifacts**. A model cannot proceed to the next stage of the release pipeline — staging, canary, production rollout — until the regression report is reviewed and approved. Approval does not mean "no regressions detected." It means "all detected regressions have been evaluated, and the team has decided they are acceptable or have been mitigated." The approval is recorded, the approver is named, and the decision is auditable.

This process prevents the silent erosion that occurs when regression tests run but no one acts on the results. If regression reports are generated but not reviewed, they are useless. If they are reviewed but not tied to a decision gate, they are advisory at best. A regression testing system that does not block releases when regressions are detected is a measurement system, not a quality gate. Measurement is useful, but quality gates are what prevent bad models from reaching production.

Baseline comparison is the core of regression testing, but it only works if the baselines are valid, the thresholds are meaningful, and the results drive decisions. In the next subchapter, we will examine how to design regression test cases for non-deterministic systems — models that produce different outputs on repeated runs, making traditional test case design strategies unreliable.
