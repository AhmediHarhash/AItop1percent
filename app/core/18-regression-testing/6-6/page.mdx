# 6.6 â€” Threshold Calibration and Adjustment

In August 2025, a fintech company set regression thresholds for their fraud detection model. Accuracy threshold: 91 percent. Precision threshold: 0.88. Recall threshold: 0.85. These thresholds were conservative, set at the 10th percentile of baseline performance during observation. The team documented the thresholds, enforced them in CI, and moved on. Six months later, in February 2026, the thresholds were still 91 percent, 0.88, and 0.85. The model's actual performance over those six months: accuracy between 93 and 95 percent, precision between 0.91 and 0.94, recall between 0.88 and 0.92. The thresholds had become meaningless. They no longer caught regressions because any regression large enough to drop performance below the threshold would be catastrophic, obvious without a gate. The team had built a gate that never closed.

The problem was not the initial thresholds. The initial thresholds were correct when set. The problem was that thresholds were treated as permanent. The team believed that setting thresholds once was sufficient. They did not schedule reviews. They did not adjust thresholds as performance improved. They did not tighten the bar as the model's capability grew. The gate existed, but it provided no protection. When a commit finally did introduce a regression, dropping accuracy from 94 percent to 92 percent, the gate passed. The regression shipped. Users noticed. Trust degraded. The gate's value was destroyed because the threshold had not evolved with the system.

Thresholds are not set-and-forget. They are living parameters that must be calibrated and adjusted as your system matures. This subchapter covers when to adjust thresholds, how to adjust them, who approves adjustments, and how to track changes over time. It explains the difference between tightening thresholds to raise the bar and loosening thresholds to accommodate reality. It describes the governance required to ensure that threshold adjustments are data-driven, not reactive to pressure. It lays out the process that transforms static thresholds into a dynamic quality system that continuously improves.

## Thresholds Are Not Set-and-Forget

The single most important principle of threshold management is that thresholds are not set once and left unchanged. They are reviewed on a schedule, adjusted based on data, and tightened as performance improves. A threshold that is not reviewed becomes stale. It falls behind the system's actual capability. It stops catching meaningful regressions. It creates the illusion of protection without providing it. Stale thresholds are worse than no thresholds because they give the team false confidence in quality gates that no longer function.

The reason thresholds go stale is simple: AI systems improve. You fine-tune the model. You improve the training data. You refactor the prompt. You add retrieval components that improve accuracy. Each improvement raises the system's baseline performance. If accuracy was 89 percent when you set the threshold at 88 percent, but accuracy is now consistently 93 percent, the 88 percent threshold no longer reflects the quality standard you are enforcing. It reflects the quality standard you were enforcing six months ago. It is outdated. It needs to be updated.

Threshold staleness creates two risks. First, it allows regressions to ship. If your threshold is 88 percent but your actual performance is 93 percent, a commit that drops performance to 90 percent will pass the gate. That commit represents a 3 percentage point regression, but the gate does not catch it because 90 percent is still above 88 percent. Users experience degraded performance, but the regression gate stays green. Second, staleness erodes trust in the gate. When a regression ships and a teammate asks why the gate did not catch it, the answer is that the threshold was too low. The natural follow-up question is: why was the threshold too low? The answer is that nobody reviewed it. The team learns that the gate does not reliably protect quality. They stop trusting it. They start viewing it as a checkbox rather than a safeguard.

The solution is scheduled reviews. Every threshold has a review date. Quarterly, semi-annually, or annually, depending on the rate of change in your system. High-velocity teams with frequent model updates review thresholds quarterly. Slower-moving teams with stable models review semi-annually. The review date is documented in the threshold registry. It is owned by the team responsible for the capability. It is not optional. Missing a review is treated the same as missing a production incident retrospective: a process failure that requires explanation and correction.

## Scheduled Threshold Reviews

A threshold review is a structured process that examines recent performance, compares it to the current threshold, and decides whether adjustment is needed. The review is conducted by the threshold owner, typically with input from stakeholders who care about the metric. The review uses data from the observation period since the last review. If the last review was in November 2025 and the current review is in February 2026, the data covers November through February. This data includes every eval run, every metric measurement, every threshold check.

The review begins with a performance summary. What was the median performance over the observation period? What was the 10th percentile? What was the 90th percentile? How did performance trend over time? Was it stable, improving, or degrading? Were there any anomalies, such as a week where performance spiked or dropped due to a specific commit or infrastructure issue? This summary establishes the baseline reality. It shows what the system is actually doing, not what you hoped it would do or what it did six months ago.

Next, the review compares performance to the current threshold. How often did the system exceed the threshold? If the threshold is 88 percent accuracy and the system never dropped below 92 percent, the threshold is too loose. If the threshold is 95 percent accuracy and the system exceeded it only 60 percent of the time, the threshold is too tight. If the threshold is 91 percent accuracy and the system dropped below it twice in three months due to temporary issues that were quickly fixed, the threshold is appropriately calibrated. The comparison reveals whether the threshold is doing work or just sitting there.

The review also examines false positives and false negatives. A false positive is when the gate blocks a release due to a threshold failure, but the failure turns out to be a flaky test, a data pipeline issue, or some other non-regression cause. False positives erode trust. If your team spends an hour debugging a threshold failure only to discover that the eval suite had a transient failure, they lose confidence in the gate. A false negative is when a regression ships because the threshold did not catch it. False negatives erode protection. If users report quality issues that your regression gate missed, the gate is not doing its job. The review tracks both types of errors and adjusts thresholds or tests to minimize them.

Finally, the review produces a recommendation. Tighten the threshold, loosen the threshold, or hold steady. The recommendation is data-driven. It is based on the performance summary, the threshold comparison, and the false positive and false negative analysis. It is documented in the threshold registry. It is approved by the threshold owner and any relevant stakeholders. It is implemented in the CI config. It takes effect immediately for the next commit. This structured process ensures that threshold adjustments are deliberate, transparent, and accountable.

## Data-Driven Threshold Adjustments

Threshold adjustments must be grounded in data, not intuition or pressure. When a teammate argues that a threshold should be lowered because it blocks too many releases, the response is: show me the data. How many releases were blocked? What caused the failures? Were the failures legitimate regressions or false positives? If the failures were legitimate regressions, the threshold is working correctly. If the failures were false positives, the issue is not the threshold but the flakiness of the test or the accuracy of the metric. You fix the test. You do not lower the threshold.

Data-driven adjustments follow a simple rule: thresholds tighten when the system consistently exceeds them, and thresholds loosen only when the current threshold is demonstrably unachievable. If your accuracy threshold is 90 percent and your system has exceeded 92 percent for three consecutive months, you tighten the threshold to 92 percent. The system has proven it can perform at that level. The threshold should reflect that capability. If your accuracy threshold is 90 percent and your system has never exceeded 88 percent despite multiple attempts to improve it, you have three options: invest in improving the system, accept that the capability is not ready to ship, or loosen the threshold to 88 percent and document the trade-off.

Loosening a threshold is not a failure. It is a recognition of reality. If you set an aspirational threshold that the system cannot meet, the threshold becomes an obstacle rather than a safeguard. The team routes around it. They request exceptions. They disable the check. The gate loses its authority. It is better to set a threshold that reflects current capability and enforce it strictly than to set an unachievable threshold and enforce it inconsistently. However, loosening a threshold requires more justification and oversight than tightening one. Tightening raises the bar. Loosening lowers it. Lowering the bar is a regression in your quality standards and must be treated with appropriate gravity.

When you loosen a threshold, you document why. You explain what prevented the system from meeting the original threshold. You outline what would be required to restore the threshold to its previous level. You set a timeline for revisiting the decision. This documentation ensures that loosening a threshold is a temporary accommodation, not a permanent retreat. It creates accountability for improving the system so that the threshold can be tightened again in the future. Without this documentation, loosened thresholds tend to stay loosened indefinitely, and the quality bar drifts downward over time.

## When to Raise Thresholds: The Ratchet Effect

Raising thresholds is how you institutionalize quality improvement. When your system demonstrates consistent performance at a level above the current threshold, you raise the threshold to match. This is called the ratchet effect. Just as a ratchet mechanism prevents backward motion, threshold tightening prevents quality regressions. Once you prove you can deliver 94 percent accuracy, you lock in 94 percent as the new minimum. You do not allow the system to drift back to 91 percent without a deliberate decision and a documented reason.

The ratchet effect is triggered by data. The typical trigger is three months of performance consistently exceeding the current threshold by at least 2 percentage points or 0.02 on a zero-to-one scale. If your accuracy threshold is 90 percent and your system has delivered between 92 and 95 percent accuracy for three consecutive months, you raise the threshold to 92 percent. You do not raise it to 95 percent because that would be the peak, not the floor. You raise it to a level the system reliably sustains. This ensures that the new threshold is achievable under normal conditions, not just under ideal conditions.

Raising thresholds does not require the same level of stakeholder approval as setting initial thresholds. Initial thresholds define the quality standard. Raised thresholds reflect improvement in meeting that standard. As long as the new threshold is still aligned with stakeholder expectations, the threshold owner can approve the raise unilaterally. However, raising thresholds does require communication. You notify the team. You update the documentation. You announce the change in the team channel. This transparency ensures that everyone knows the bar has been raised and that future commits will be held to a higher standard.

The ratchet effect creates a culture of continuous improvement. Teams take pride in raising thresholds. It is a visible, measurable demonstration that the system is getting better. It creates momentum. Once you raise the threshold from 90 to 92 percent, the team starts asking: can we get to 94 percent? Can we get to 95 percent? This mindset shift is powerful. The team is no longer defending a static baseline. They are competing against their own previous performance. Quality becomes a moving target that always trends upward.

## When to Lower Thresholds and the Approval Required

Lowering a threshold is a rare and carefully governed event. It happens in three scenarios. First, when the original threshold was set incorrectly due to bad data or misaligned stakeholder expectations. If you set a threshold at 95 percent based on a flawed baseline measurement, and subsequent observation reveals that the system never actually achieved 95 percent under normal conditions, you lower the threshold to reflect reality. Second, when a system change fundamentally alters capability. If you swap in a new base model that trades some accuracy for lower cost, and the accuracy drop is an intentional trade-off approved by stakeholders, you lower the accuracy threshold to match the new model's capability. Third, when external factors change. If a regulatory requirement is relaxed, a previously strict compliance threshold may be lowered to match the new requirement.

Lowering a threshold requires formal approval from the threshold owner and the stakeholder group that set the original threshold. If Product and Trust and Safety jointly defined the threshold, both must approve any lowering. If Legal mandated the threshold for compliance, Legal must approve. This approval process ensures that threshold lowering is not a unilateral decision by Engineering to make their lives easier. It is a deliberate choice by the organization to accept a lower quality standard, with full awareness of the trade-offs.

The approval process includes a risk assessment. What are the consequences of lowering the threshold? Will users notice degraded quality? Will it increase safety incidents? Will it create compliance risk? Will it damage brand reputation? These questions must be answered with data. If you are lowering an accuracy threshold from 92 percent to 89 percent, you model the impact. How many additional errors will users see? What types of errors? How will that affect user satisfaction, retention, churn? If the answers are not available, you do not lower the threshold until you collect the data to answer them.

Once a threshold is lowered, it is flagged in the threshold registry as a regression. It is reviewed at the next scheduled threshold review, and the team is tasked with restoring the threshold to its previous level within a defined timeline. This timeline is not optional. It is a commitment. If you lower a threshold because of a temporary constraint, you document what needs to happen to restore the threshold and when you expect that to be achievable. This accountability prevents threshold lowering from becoming permanent degradation disguised as temporary accommodation.

## Threshold Change Governance

Threshold changes, whether tightening or loosening, follow a governance process. This process ensures that changes are deliberate, documented, and approved by the appropriate stakeholders. The governance process has four steps. First, proposal. The threshold owner proposes a change based on the data from a scheduled review or an ad hoc analysis. The proposal includes the current threshold, the proposed new threshold, the data supporting the change, and the rationale. Second, review. The proposal is reviewed by the stakeholder group that originally defined the threshold. This group assesses whether the proposed change aligns with their quality expectations and risk tolerance.

Third, approval. If the stakeholder group agrees with the proposal, they approve it. Approval is documented in the threshold registry. The documentation includes the date of approval, the names of approving stakeholders, and any conditions or caveats. For example, a threshold might be approved for tightening with the condition that it will be reviewed again in one month to ensure the new level is sustainable. Fourth, implementation. Once approved, the threshold change is implemented in the CI config. The change takes effect immediately. The team is notified. The updated threshold is enforced on all subsequent commits.

This governance process is lightweight but formal. It does not require weeks of meetings or extensive paperwork. It requires a documented proposal, a review by the right people, and explicit approval. Most threshold changes go through this process in under a week. The process exists not to slow down changes but to ensure that changes are intentional and accountable. It prevents threshold drift. It prevents unilateral decisions. It ensures that everyone who cares about quality has visibility into how the quality bar is evolving.

Threshold change governance also includes an escalation path. If the threshold owner and the stakeholder group disagree on a proposed change, the disagreement is escalated to leadership. If Product wants to lower a threshold to ship a feature faster and Trust and Safety opposes the lowering because it increases safety risk, that conflict is resolved by the VP of Product and the VP of Trust and Safety, or by the CTO if necessary. Escalation is rare, but the path must exist. Without it, threshold disagreements stall in unresolved debates, and the gate either blocks releases indefinitely or is bypassed through informal workarounds.

## A/B Testing Threshold Changes

For high-stakes thresholds, especially those governing user-facing quality or safety, you can A/B test threshold changes before rolling them out fully. An A/B test applies the proposed new threshold to a subset of commits or a subset of traffic, while the rest of the system continues to use the current threshold. You measure the impact. Does the new threshold catch more regressions? Does it create more false positives? Does it affect release velocity? After the test period, you analyze the results and decide whether to roll out the change globally.

A/B testing thresholds is particularly valuable when you are tightening thresholds aggressively. If you want to raise an accuracy threshold from 90 percent to 94 percent, that is a large jump. It might block a significant number of commits. Before enforcing the new threshold on every commit, you test it on 20 percent of commits for two weeks. You track how often the new threshold would have blocked a commit that the old threshold passed. You review those blocked commits to determine whether they were legitimate regressions or acceptable variance. If the new threshold catches meaningful regressions without excessive false positives, you roll it out. If it creates too many false positives, you adjust the threshold or improve the test suite before rolling out.

A/B testing also reveals unintended consequences. Sometimes tightening a threshold in one area degrades performance in another. For example, tightening a latency threshold might push the team to use faster but less accurate models, which degrades accuracy even though accuracy has its own threshold. The A/B test surfaces this trade-off before it becomes a system-wide issue. You can then decide whether the trade-off is acceptable or whether the thresholds need to be rebalanced.

Not every threshold change requires an A/B test. Small adjustments, such as tightening a threshold by 1 or 2 percentage points, can be rolled out directly. Large adjustments, such as introducing a new threshold for a previously ungated metric or tightening by more than 5 percentage points, should be A/B tested first. The cost of an A/B test is low. The cost of rolling out a poorly calibrated threshold that blocks half your releases is high. When in doubt, test.

## The Threshold Changelog: Track All Changes

Every threshold adjustment is logged in a threshold changelog. This changelog is part of the threshold registry. It records the date of each change, the old threshold, the new threshold, the reason for the change, and the stakeholder who approved it. The changelog creates a historical record of how your quality standards have evolved over time. It shows whether thresholds are being tightened, loosened, or held steady. It shows how often thresholds are reviewed and adjusted. It creates accountability for every change.

The changelog is also a diagnostic tool. If quality degrades and you want to understand why, you check the changelog. Did a threshold get loosened recently? Did a key threshold get removed? Did the governance process get bypassed? The changelog answers these questions. It also supports audits. If a regulator or an internal audit team asks how you ensure quality standards are maintained, you show them the changelog. You show them that thresholds are reviewed on schedule, adjusted based on data, and approved by the appropriate stakeholders. The changelog is evidence of a mature, disciplined quality process.

Teams that maintain a detailed changelog build institutional memory. When a new engineer joins and asks why the accuracy threshold is 92 percent, not 90 or 95, you point them to the changelog. They see that the threshold was originally set at 88 percent in June 2025, raised to 90 percent in September 2025 after three months of consistent performance, raised to 92 percent in December 2025 after further improvement, and held at 92 percent through the February 2026 review because performance has been stable. This history explains the current threshold better than any single data point could. It shows the journey, not just the destination.

The changelog also surfaces patterns. If you notice that a particular threshold is adjusted frequently, that signals instability. The metric might be noisy. The test might be flaky. The capability might be undergoing rapid iteration. Whatever the cause, frequent adjustments are a red flag. They suggest that the threshold is not well-calibrated or that the underlying system is not stable. You investigate. You fix the root cause. You stabilize the threshold. The changelog makes these patterns visible.

## Avoiding Threshold Inflation

Threshold inflation is what happens when thresholds are loosened repeatedly under pressure, without corresponding improvements to restore them. A team sets an accuracy threshold at 92 percent. A critical release is blocked by the threshold. Product pushes to lower the threshold to 90 percent to unblock the release. The team agrees, with the understanding that the threshold will be restored to 92 percent after the release. But the restoration never happens. The next release is also blocked at 90 percent. The threshold is lowered to 88 percent. Over the course of a year, the threshold drifts from 92 percent to 84 percent, each step justified as a temporary accommodation. This is threshold inflation. The quality bar has eroded, not through deliberate decision but through accumulated compromises.

Threshold inflation is prevented through governance. Lowering a threshold requires approval. Approval requires a documented plan to restore the threshold. The plan includes a timeline. The timeline is tracked. If the restoration does not happen on schedule, it is escalated. The stakeholders who approved the lowering are accountable for ensuring the restoration occurs. This accountability prevents temporary lowerings from becoming permanent.

Threshold inflation is also prevented by tracking the direction of changes in the changelog. If the changelog shows that a threshold has been lowered three times in six months without any tightening, that is a red flag. It signals that the capability is degrading or that the team is under pressure to ship without maintaining quality standards. Leadership reviews the changelog. They ask why the threshold keeps dropping. They decide whether to invest in improvement, to accept the lower quality standard as the new baseline, or to halt further lowering and focus on stabilization. The changelog makes threshold inflation visible, and visibility enables intervention.

## The Goal Is a Rising Bar

The ultimate goal of threshold calibration and adjustment is a quality bar that rises over time. Your thresholds today should be stricter than your thresholds six months ago. Your thresholds six months from now should be stricter than your thresholds today. This rising bar reflects improving capability. It reflects a culture of continuous improvement. It reflects a team that treats quality as a competitive advantage, not a compliance checkbox.

A rising bar does not mean that every threshold tightens on every review. Some thresholds will hold steady for long periods because performance has plateaued. Some thresholds will loosen temporarily due to system changes or external factors. But the trend, over quarters and years, should be upward. When you plot your thresholds on a timeline, the line should slope upward. When you compare your thresholds today to your thresholds at launch, they should be significantly stricter. That tightening is evidence of maturity. It is evidence that your regression testing system is not just catching failures but driving improvement.

Threshold calibration and adjustment transform static gates into a dynamic quality system. The gates do not just protect against catastrophic regressions. They define the current standard, they incentivize improvement, and they institutionalize gains. The next subchapter introduces a specific type of threshold that has become critical in 2026: cost-per-request thresholds, which ensure that your system remains economically viable as it scales.

