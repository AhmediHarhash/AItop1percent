# 7.7 â€” Model Refusal Rate Regression

In September 2025, a healthcare AI platform deployed a safety update designed to better detect potential HIPAA violations in user queries. The team ran their standard safety test suite. Every test passed. The update shipped to production on a Friday afternoon. By Monday morning, support tickets had tripled. The model was refusing to answer basic clinical questions. "What are common symptoms of Type 2 diabetes?" Refused. "How do I interpret an HbA1c level of 6.8?" Refused. "What medications treat hypertension?" Refused. The safety update had pushed the refusal threshold so far toward caution that the model became nearly unusable for its core purpose. The team had tested whether the model refused harmful queries. They had not tested whether it still accepted legitimate ones. Rolling back the update took six hours. The damage to user trust took six weeks to repair.

Refusal rate is not a single metric. It is a balance between two failure modes: refusing requests you should honor, and honoring requests you should refuse. Get the threshold wrong in either direction and the model fails. Push too far toward safety and users abandon your product because it refuses to help. Push too far toward helpfulness and you enable harm. The regression test gate must detect movement in both directions. This requires baselines that measure not just overall refusal rate, but refusal behavior across specific request categories. The goal is to ensure that when you improve safety in one area, you do not destroy usability everywhere else.

## The Two-Sided Refusal Problem

Over-refusal means the model refuses legitimate, policy-compliant requests. This destroys user experience. Users who receive repeated refusals stop trusting the model and stop using the product. Over-refusal is particularly damaging when it affects core use cases. A customer support bot that refuses to discuss return policies. A legal research assistant that refuses to cite case law because it detects legal language. A medical documentation tool that refuses to transcribe clinical notes. These are not theoretical edge cases. They are real failures that drive users away.

Under-refusal means the model complies with requests that violate policy. This creates safety and compliance risk. A model that generates harmful medical advice. A model that provides instructions for illegal activity. A model that leaks sensitive information in response to a cleverly-worded prompt. Under-refusal failures are less visible than over-refusal failures because users who succeed in bypassing safety controls rarely report it. You discover under-refusal through red-teaming, through incident reports, or through external researchers publishing exploits.

The refusal rate regression gate must test for both. You need a set of requests that the model should accept, and a set of requests that the model should refuse. Before every deployment, you measure how the model handles both sets. If refusal rate for legitimate requests increases above baseline, the model is over-refusing. If refusal rate for policy-violating requests decreases below baseline, the model is under-refusing. Either condition blocks the release.

## Baseline Refusal Rates by Request Category

Aggregate refusal rate obscures category-specific regression. A model might maintain an overall refusal rate of 8 percent across all queries while completely changing which categories it refuses. The new version might refuse twice as many legitimate medical questions while refusing half as many attempts to generate phishing emails. The aggregate number stays stable. The user experience collapses.

Category-specific baselines solve this. You segment your test set by request type. Medical queries. Legal queries. Financial advice. Code generation. Creative writing. Customer support. Personal questions. Potentially harmful requests. For each category, you measure baseline refusal rate. Then for each new model version, you compare refusal rates category by category. This reveals localized regression that aggregate metrics miss.

The legitimate request baseline measures over-refusal risk. You curate a set of requests that are clearly policy-compliant and represent common, valuable use cases. "Explain how photosynthesis works." "Draft a professional email declining a meeting." "Summarize the main points of the Fourth Amendment." "Write a Python function to calculate compound interest." These requests should have near-zero refusal rates. If a new model version starts refusing 15 percent of these, you have over-refusal regression regardless of what happens to the aggregate refusal rate.

The policy-violating request baseline measures under-refusal risk. You curate a set of requests that should always be refused. Requests for harmful content. Requests that attempt prompt injection. Requests that try to extract training data. Requests that ask the model to ignore its instructions. These requests should have near-100 percent refusal rates. If a new model version accepts 10 percent of these, you have under-refusal regression. The model is less safe than the version it is replacing.

## Measuring Refusal Behavior

Refusal detection is not binary. Models refuse in different ways. Explicit refusals: "I cannot provide that information." Deflections: "Let me tell you about something related instead." Partial compliance: answering part of the question while avoiding the sensitive element. Policy explanations: "My usage policy prevents me from generating that type of content." Silence: generating nothing at all. Your refusal detection logic must recognize all of these patterns, not just the explicit refusal template.

You need both automated detection and human review. Automated detection works well for explicit refusals that follow a template. The model generates a string containing "I cannot" or "I am unable" or "My guidelines prevent." You can pattern-match these with high precision. But deflections and partial compliance require human judgment. An automated check might flag the response as compliant because it generated text. A human reviewer recognizes that the model answered a different, safer question instead of the one actually asked.

The measurement protocol runs every refusal-sensitive test case through the model, collects the response, and classifies it. Did the model comply fully? Did it refuse explicitly? Did it deflect or partially comply? Did it generate nothing? You aggregate these classifications by category and compare to baseline. If the explicit refusal rate for legitimate medical queries goes from 2 percent to 18 percent, you have detected over-refusal regression. If the explicit refusal rate for prompt injection attempts goes from 94 percent to 68 percent, you have detected under-refusal regression.

## The Safety-Usability Tradeoff

Every safety improvement creates usability risk. You tune the model to better detect adversarial prompts. Now it also refuses some legitimate queries that happen to contain trigger words. You strengthen refusal instructions in the system prompt. Now the model becomes more conservative and refuses edge cases it used to handle. You apply a post-deployment fine-tune to reduce a specific class of harmful outputs. Now the model over-generalizes and refuses adjacent content that was always acceptable.

The regression gate does not prevent you from making this tradeoff. It makes the tradeoff visible and measurable. Before deploying the safety improvement, you know exactly how much legitimate request handling will degrade. You can make an informed decision about whether the safety gain is worth the usability cost. Sometimes it is. If the safety improvement blocks a serious exploit and the usability regression only affects 3 percent of edge-case queries, you ship it. If the safety improvement provides marginal value and the usability regression affects 20 percent of core use cases, you do not.

The tradeoff becomes explicit when you set thresholds. You might decide that a 5 percent increase in legitimate request refusal rate is acceptable if it accompanies a 15 percent decrease in policy-violating request acceptance rate. You encode this in your release gate logic. The new model version can increase legitimate refusals by up to 5 percentage points and still pass. Beyond that threshold, the release is blocked. This prevents silent drift where safety updates gradually make the model less and less usable over time.

## False Positive Refusals and User Experience

False positive refusals are over-refusal on specific requests. The model refuses a query that is clearly policy-compliant and valuable. These failures compound. One false refusal frustrates a user. Two false refusals in the same session make them doubt the product. Three false refusals make them leave and tell colleagues not to use it. In B2B products, false refusal complaints reach your buyer within hours and threaten renewal.

The most damaging false positives occur in high-frequency workflows. A clinician documenting patient encounters gets refused on every fifth note. A developer using an AI coding assistant gets refused on common function patterns. A customer support agent using an AI drafting tool gets refused on standard return policy language. These are not users testing edge cases. These are users trying to do their core job. When the tool refuses, productivity stops.

Refusal rate regression tests must include high-frequency legitimate use cases. Not just "can the model answer medical questions in general," but "can the model answer the specific types of medical questions that appear 1,000 times per day in our user logs." You sample real production queries that the current model handles successfully, filter for policy compliance, and add them to the legitimate request baseline. If the new model refuses any of these, you have a false positive that will affect real users at scale.

The regression gate threshold should be tighter for high-frequency use cases than for edge cases. A model that refuses an obscure philosophical question 10 percent of the time might be acceptable. A model that refuses a common clinical documentation query 2 percent of the time is not. You can implement this by weighting test cases by production frequency. A high-frequency query that gets refused contributes more to the aggregate regression score than a low-frequency query. This ensures that the model optimizes for the requests that matter most to users.

## False Negative Refusals and Safety Failures

False negative refusals are under-refusal on specific requests. The model complies with a query that violates policy. These failures create direct harm. A model that provides medical advice it should refuse. A model that generates content that violates laws or regulations. A model that leaks information it should protect. Each false negative is a safety incident waiting to be discovered.

The most dangerous false negatives are the ones that bypass multiple safety layers. A query that evades the input filter, generates a policy-violating response, and also evades the output filter. These are rare in well-designed systems, but they are the ones that make headlines when external researchers publish them. "New jailbreak bypasses all safety controls." "Model can be tricked into generating harmful content with simple prompt." The refusal rate regression gate exists to prevent these bypasses from becoming easier over time.

Policy-violating request baselines must include known attack patterns. Prompt injection templates. Roleplay scenarios designed to reframe harmful requests. Multi-turn conversations that start benign and escalate. Obfuscation techniques that hide harmful content in coded language. Your red team generates these as part of ongoing adversarial testing. The best ones become permanent fixtures in the refusal regression suite. If the current model refuses them, every future model must refuse them too. Any version that accepts a previously-refused attack pattern is blocked.

The threshold for false negative regression should be zero. If the current model refuses 100 percent of a certain attack pattern, the new model must also refuse 100 percent. There is no acceptable direction of movement. You do not tolerate a new model that refuses only 95 percent of prompt injection attempts, even if it improves every other metric. Safety capability should never regress. If a new model makes a tradeoff that weakens safety, the release is blocked until the safety gap is closed.

## Tuning Refusal Sensitivity

Refusal sensitivity is controlled by system prompts, fine-tuning, and post-processing filters. Each layer can make the model more or less likely to refuse. System prompt instructions like "decline any request that could be harmful" increase refusal rate. Fine-tuning on datasets that include refusal examples teaches the model to refuse more often. Post-processing filters that detect certain output patterns can block responses even if the model generated them. All three layers contribute to overall refusal behavior, and all three can cause regression.

When refusal rate regresses, you need to identify which layer caused it. Did the system prompt change? Did you apply a fine-tune that shifted the distribution? Did you update a content filter? Refusal rate regression testing should instrument each layer separately. You measure refusal rate with only the system prompt, with system prompt plus fine-tuning, and with the full pipeline including filters. This tells you where the regression originated and where to apply the fix.

Tuning refusal sensitivity is iterative. You adjust the system prompt and measure refusal rate change. You re-tune the model with adjusted refusal examples and measure again. You modify filter thresholds and measure again. Each change shifts the safety-usability tradeoff. The regression gate ensures that the iterative tuning process does not accidentally move you from an acceptable equilibrium to an unacceptable one. You can explore the tradeoff space freely during development. The gate prevents you from deploying a version that crossed a threshold you did not intend to cross.

## Refusal Rate Release Gates

The refusal rate release gate compares current and candidate model refusal behavior across all test categories. The candidate model must meet three conditions to pass. First, refusal rate for legitimate requests must not increase beyond the allowed threshold. Second, refusal rate for policy-violating requests must not decrease beyond the allowed threshold. Third, no individual category can regress beyond its category-specific threshold, even if aggregate refusal rates are stable.

Threshold-setting depends on your risk tolerance and user sensitivity. A product serving regulated industries might set a zero-tolerance threshold for under-refusal regression. No version can make it easier to bypass content policy, period. The same product might allow a 5 percent increase in over-refusal rate if the safety gain is significant. A consumer-facing creative tool might have the opposite thresholds. It might tolerate a small increase in under-refusal risk to avoid frustrating users with false positives. There is no universal right answer. The right answer is the one that aligns with your product goals and regulatory requirements.

The gate should be enforced automatically in CI. Every candidate model runs through the refusal rate test suite. The results are compared to the latest production baseline. If any threshold is exceeded, the pipeline fails and the release is blocked. No human override. No "we will fix it in the next sprint." The model does not ship until it passes. This prevents the gradual drift that happens when teams repeatedly decide that a small regression is acceptable just this once.

## Policy Change vs Behavior Drift

Not all refusal rate changes are regressions. Sometimes you intentionally change policy and expect refusal rates to shift. You decide to allow a new category of content. Refusal rate for that category drops to zero. This is not under-refusal regression. This is policy change. You decide to disallow a category of content you previously allowed. Refusal rate for that category increases to 100 percent. This is not over-refusal regression. This is policy change.

The regression gate must distinguish between policy change and behavior drift. Policy change is documented, deliberate, and accompanied by an updated baseline. Behavior drift is undocumented, unintentional, and represents model instability. The way to separate them is through baseline versioning. When you change policy, you create a new baseline version. The regression gate compares the candidate model to the new baseline, not the old one. This allows the refusal rate to shift in alignment with the policy change, but still blocks unintended drift within the new policy regime.

Baseline versioning requires discipline. Every policy change must trigger a baseline update. You re-run the full refusal rate test suite against the current production model under the new policy interpretation. This becomes the new baseline. All future models are compared to it. If your team changes policy frequently, you will have frequent baseline updates. This is correct. The alternative is to allow policy and baselines to drift apart until the regression gate measures the wrong thing.

The next challenge is detecting when the model stops following the policy altogether, even though the policy has not changed. That is content policy drift, and it requires a different kind of regression test.

