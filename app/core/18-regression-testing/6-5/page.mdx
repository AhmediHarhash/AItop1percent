# 6.5 â€” Setting Initial Thresholds

The hardest thresholds to set are the first ones. You have no historical data. No pattern of performance to anchor to. No baseline measurements to work from. You are asked to draw a line in the sand and declare that performance below this line is unacceptable, and you have nothing but instinct and stakeholder expectations to guide you. Teams freeze at this moment. They delay threshold setting because they want more data. They set thresholds arbitrarily high to avoid false positives, or arbitrarily low to avoid blocking releases. Both approaches fail. The first creates a gate that blocks everything. The second creates a gate that blocks nothing. Neither creates a functioning quality system.

Setting initial thresholds is a specific process with a specific sequence. You do not need six months of data. You do not need perfect certainty. You need a baseline, a measurement period, stakeholder input, and the discipline to start conservative and tighten over time. This subchapter walks through that process step by step, from the moment you have nothing but a model and a test set to the moment you ship your first release with functioning regression gates.

## The Cold Start Problem

When you first implement regression testing, you face the cold start problem. You have built an eval suite. You have defined metrics. You have instrumented your CI pipeline. But you have no data on what constitutes normal performance. You do not know if 92 percent accuracy is good or catastrophic for your use case. You do not know if 400 milliseconds latency is fast or slow. You do not know if 8 percent of outputs containing disallowed patterns is within tolerance or a blocker. Without historical performance data, you cannot set thresholds that reflect reality.

The instinct is to wait. To collect data for weeks or months before setting thresholds. To defer the decision until you have enough information. This instinct is wrong. Waiting to set thresholds means shipping changes without quality gates. It means your regression suite exists but catches nothing. It means you are collecting data but not enforcing standards. By the time you finally set thresholds, your team has become accustomed to shipping without gates, and the introduction of enforcement feels like a new obstacle rather than a necessary safeguard. You lose momentum. You lose stakeholder trust. You lose the window where setting thresholds felt like progress rather than punishment.

The correct approach is to set initial thresholds immediately, based on baseline measurement and stakeholder input, with the explicit understanding that these thresholds are provisional. They will tighten. They will adjust. They will change as you learn what your system can actually achieve. But they exist from day one, and they enforce something rather than nothing. That enforcement creates the data you need to improve the thresholds. It creates the culture of measurement. It creates the expectation that quality is gated, not optional.

## Baseline Measurement Comes First

Before you set a single threshold, you measure the current system. Not the aspirational system. Not the system you want to build. The system you have right now, running in production, serving real users. You run your eval suite against the current production model and capture every metric. Accuracy, latency, cost, safety violations, refusal rate, format compliance. Whatever your regression suite measures, you measure it on the baseline. This is your zero point. This is the performance level you are currently delivering to users.

Baseline measurement serves three purposes. First, it gives you a factual starting point. You cannot improve what you do not measure. If your current accuracy is 89 percent, you know that any threshold below 89 percent is already being violated by production. Setting a threshold at 85 percent would be meaningless because your system is already exceeding it. Setting a threshold at 95 percent might be aspirational but disconnected from current capability. The baseline tells you what is real.

Second, baseline measurement exposes the variability in your system. You do not run the eval suite once. You run it ten times. Twenty times. Fifty times, if the suite is fast enough. You capture the distribution of outcomes. You learn that accuracy ranges from 87 percent to 91 percent across runs. You learn that latency has a long tail with occasional spikes to 800 milliseconds. You learn that cost per request varies by 20 percent depending on prompt complexity. This variability matters. A threshold set at the median will fail half the time. A threshold set at the 95th percentile will almost never fail. You need to know the distribution before you can set a threshold that is both meaningful and achievable.

Third, baseline measurement creates stakeholder alignment. When you present the data to Product, Engineering, and Trust and Safety, everyone sees the same numbers. You are not arguing about what the system should do. You are looking at what the system currently does. That shared reality grounds the conversation. If Product wants a threshold that the baseline never achieves, the conversation becomes about whether to invest in improvement before setting the gate or to set the gate and accept that current performance does not meet it. Either way, the decision is informed by data, not guesses.

## The Observation Period

Baseline measurement is not a one-time snapshot. It is a period of continuous observation during which you collect enough data to understand the system's steady-state performance. For most teams, this observation period lasts between one and four weeks. Shorter than one week and you risk capturing temporary anomalies. Longer than four weeks and you delay threshold setting without gaining meaningful additional insight. The goal is to capture normal performance under normal conditions across a representative set of inputs.

During the observation period, you run your regression suite on every commit, or on a daily schedule if your commit rate is low. You log every metric. You track trends. You watch for drift. If accuracy suddenly drops by 5 percentage points, you investigate whether that reflects a legitimate shift in performance or a temporary issue like a flaky test or a dependency outage. If latency spikes on one day, you check whether that reflects model behavior or infrastructure load. The observation period is not passive data collection. It is active monitoring with the goal of understanding what normal looks like.

By the end of the observation period, you have a distribution for every metric. You know the median, the mean, the 95th percentile, the 99th percentile. You know the best-case and worst-case outcomes. You know which metrics are stable and which are noisy. You know which tests consistently pass and which tests fail intermittently due to flakiness or ambiguous ground truth. This distribution is the foundation for threshold setting. You are no longer guessing. You are working from data.

The observation period also serves as a shakedown for your eval suite. You discover which tests are too strict, which are too lenient, which are poorly defined. You discover which metrics correlate with real user experience and which are vanity metrics that move without impact. You fix these issues before thresholds go live. A threshold based on a flaky test is worse than no threshold at all, because it trains your team to ignore the gate. The observation period gives you time to stabilize the suite before it becomes enforcement.

## Conservative Initial Thresholds That Tighten Over Time

When you set your first thresholds, you set them conservatively. This does not mean setting them so low that they are meaningless. It means setting them at a level that the current system reliably achieves, with enough headroom that normal variability does not cause false failures. A good heuristic is to set initial thresholds at the 10th percentile of observed performance during the baseline period. If accuracy during observation ranged from 87 percent to 91 percent, with a median of 89 percent, you set the threshold at 88 percent. This threshold will catch catastrophic regressions without flagging normal variance.

Conservative thresholds serve two purposes. First, they build confidence. Your team sees the gate working. They see it pass on normal commits and fail on broken commits. They see it catch real problems. This builds trust in the system. If your first threshold is too aggressive and blocks legitimate changes, your team loses trust immediately. They start to view the gate as an obstacle rather than a safeguard. They start to lobby for exceptions. They start to route around the process. Conservative thresholds avoid this death spiral. They let the gate prove its value before it becomes strict.

Second, conservative thresholds create a foundation for ratcheting. Once the initial threshold is in place and the team has demonstrated that they can consistently meet it, you tighten the threshold. You move from the 10th percentile to the 25th percentile. Then to the median. Then to the 75th percentile. Each tightening represents an improvement in baseline capability. Each tightening raises the bar for what you ship. This ratcheting approach creates a culture of continuous improvement. The team is not defending a static line. They are raising the bar every quarter, every cycle, every time they prove they can do better.

Ratcheting requires discipline. You do not tighten thresholds arbitrarily. You tighten them when the data shows that the current threshold is no longer meaningful. If your threshold is set at 88 percent accuracy but your system has not dropped below 90 percent in three months, the threshold is not doing work. It is not catching regressions because regressions would need to drop performance by more than 2 percentage points to trigger the gate. You tighten the threshold to 90 percent. Now the gate has teeth again. Now it catches smaller regressions. This process repeats. The threshold tightens as performance improves. The bar rises as capability rises. The gate stays relevant.

## Threshold Documentation From Day One

Every threshold you set must be documented. Not in a Slack thread. Not in a pull request comment. In a version-controlled, team-accessible threshold registry that records the threshold value, the date it was set, the rationale for the value, the stakeholder who approved it, and the schedule for review. This registry is the source of truth for what gates exist, why they exist, and when they will be revisited. Without this documentation, thresholds become tribal knowledge. They exist in the CI config but nobody remembers why. They block changes but nobody knows who to ask for an exception. They drift out of alignment with team priorities because nobody scheduled a review.

Threshold documentation includes five elements. First, the metric and the threshold value. "Accuracy on customer support golden set must exceed 90 percent." "Latency at p95 must remain below 600 milliseconds." "Cost per request must not exceed 0.08 dollars." These are clear, measurable, and unambiguous. Second, the rationale. Why was this threshold set at this value? "Set at 10th percentile of baseline observation period, February 2026." "Set based on user research indicating 600 milliseconds is the maximum acceptable wait time." "Set based on unit economics requiring cost below 0.10 dollars to maintain margin." The rationale grounds the threshold in data or stakeholder input.

Third, the approval trail. Who decided this threshold? Was it the release engineering team unilaterally? Was it a joint decision between Engineering and Product? Was it mandated by Legal or Trust and Safety for compliance? Knowing who owns the threshold clarifies who can change it. Fourth, the review schedule. Thresholds are not set and forgotten. They are reviewed quarterly, semi-annually, or after major product changes. The documentation specifies when the next review is scheduled and who is responsible for conducting it. Fifth, the change history. Every time a threshold is adjusted, the old value, the new value, the date, and the reason are logged. This history creates accountability. It shows whether thresholds are being tightened over time, loosened under pressure, or held stable.

Teams that document thresholds from day one build institutional knowledge. When a new engineer joins the team, they read the registry and understand the quality standards. When a threshold blocks a release, the team looks up the rationale and decides whether to fix the regression or request a threshold adjustment. When leadership asks why a release was delayed, the team points to the documented threshold and the measured failure. Documentation transforms thresholds from arbitrary checkpoints into defensible quality standards.

## Stakeholder Input on Acceptable Quality Levels

You do not set thresholds in isolation. You set them in collaboration with the stakeholders who define what acceptable quality means for your product. This typically includes Product, Trust and Safety, Legal, and Domain Experts. Each stakeholder brings a different perspective. Product cares about user experience and feature velocity. Trust and Safety cares about preventing harm and maintaining brand trust. Legal cares about compliance and liability. Domain Experts care about correctness and professional standards. These perspectives do not always align. Your job is to translate these perspectives into measurable thresholds that everyone can enforce.

The process begins with a stakeholder workshop. You present the baseline data. You show the distribution of performance across every metric. You ask each stakeholder group: what level of performance is acceptable? What level is unacceptable? Where is the line? Product might say that latency above 800 milliseconds creates user frustration and abandonment, so the threshold should be 700 milliseconds to leave headroom. Trust and Safety might say that any output containing personally identifiable information is a compliance violation, so the threshold is zero detected PII leaks. Legal might say that GDPR requires data handling transparency, so any output that cannot explain its sources is unacceptable. Domain Experts might say that medical advice accuracy below 95 percent exposes the company to malpractice claims.

These inputs are not always consistent. Product wants fast iteration. Trust and Safety wants high safety bars. Legal wants conservative risk mitigation. Your job is not to choose one perspective over the others. Your job is to set thresholds that satisfy all perspectives within the constraints of current capability. If Trust and Safety requires zero PII leaks but the current system leaks PII in 0.5 percent of outputs, you have three options. First, set the threshold at zero and accept that the gate will block releases until the PII detection issue is fixed. Second, invest in improving the model or the PII detection system before setting the threshold. Third, implement a compensating control, such as human review of flagged outputs, and set the threshold at zero for automated release but allow human-reviewed exceptions.

The stakeholder workshop produces a draft threshold set. This draft goes through review. Each stakeholder confirms that the proposed thresholds reflect their priorities. Each stakeholder acknowledges the trade-offs. If a threshold is set lower than a stakeholder prefers because current capability does not support a higher threshold, that stakeholder signs off on a plan to tighten the threshold within a defined timeline. This sign-off creates accountability. It ensures that threshold setting is not a unilateral decision by Engineering but a collective commitment to quality standards.

## The What Would You Ship Exercise

One effective technique for setting initial thresholds is the what would you ship exercise. You present the stakeholder group with ten recent outputs from the model. Five of these outputs are high quality. Five are edge cases, near misses, or clear failures. You ask each stakeholder: would you ship this output to users? For each output, the group discusses. If the consensus is yes, the output represents acceptable quality. If the consensus is no, the output represents unacceptable quality. After reviewing all ten outputs, you have a qualitative sense of where the line falls.

You then measure each of the ten outputs against your eval metrics. The outputs that stakeholders would ship cluster around a certain accuracy level, a certain safety score, a certain format compliance rate. The outputs that stakeholders would not ship fall below that cluster. This clustering reveals the implicit threshold. If every output that stakeholders would ship has an accuracy above 88 percent, your threshold is 88 percent. If every output that stakeholders would not ship has a safety score below 0.75, your threshold is 0.75. The what would you ship exercise translates subjective quality judgment into measurable thresholds.

This exercise works because it grounds the conversation in concrete examples. Stakeholders are not asked to set an abstract threshold for accuracy. They are asked whether they would ship a specific output. That decision is easier and more intuitive. Once you aggregate those decisions across multiple outputs, the threshold emerges naturally. The exercise also surfaces disagreements early. If Product would ship an output but Trust and Safety would not, you have identified a gap in alignment that needs resolution before the threshold is set. Better to discover this gap in a workshop than in a production incident.

The what would you ship exercise is particularly valuable when you are adding regression gates for a new capability or a new domain. You do not have historical data. You do not have baseline performance. But you have outputs, and you have stakeholder judgment. You run the exercise, set the initial threshold based on consensus, and refine the threshold as you collect more data. This approach allows you to gate quality from day one, even when you are working with incomplete information.

## Setting Thresholds for New Capabilities

When you add a new capability to your model, you add new regression tests and new thresholds. These thresholds cannot be based on historical performance because the capability did not exist before. You do not have a baseline. You do not have an observation period. You have a model that can now do something it could not do before, and you need to decide what level of performance is acceptable before you ship it.

The process for setting thresholds for new capabilities mirrors the process for initial thresholds, but compressed. You generate a representative sample of outputs for the new capability. You measure those outputs against your eval metrics. You run the what would you ship exercise with stakeholders. You set a conservative threshold based on the lowest acceptable quality level identified during the exercise. You document the threshold and schedule a review for one month after the capability ships. During that month, you collect real-world performance data. At the one-month review, you adjust the threshold based on observed performance and user feedback.

New capabilities often require new metrics. If you are adding a summarization feature, you need metrics for summary accuracy, completeness, and conciseness. If you are adding a translation feature, you need metrics for translation accuracy and fluency. You cannot set thresholds for these metrics until the metrics are defined and instrumented. This means that the eval work for a new capability includes metric definition, test case creation, threshold setting, and documentation. It is not a small task. Teams that underestimate this work end up shipping new capabilities without regression gates, creating quality risk.

The first threshold for a new capability is always provisional. You are guessing at what acceptable performance looks like. You will be wrong, at least slightly. The one-month review gives you a chance to correct the guess. If the threshold was too strict and blocked legitimate changes, you loosen it. If the threshold was too lenient and missed quality issues that users reported, you tighten it. If the metric itself turned out to be poorly correlated with user satisfaction, you replace the metric and set a new threshold. This iteration is expected. It is not a failure. It is how you learn what quality means for a new capability.

## Threshold Documentation and Ownership

Setting the threshold is not the end of the work. You must document who owns the threshold, who can change it, and what process governs changes. Ownership is typically assigned to the team responsible for the capability. If the customer support team owns the support chat model, they own the thresholds for that model's regression suite. If the legal team mandates compliance thresholds, they own those thresholds. Ownership means responsibility for reviewing threshold performance, proposing adjustments, and approving exceptions.

Threshold ownership also determines escalation paths. If a release is blocked by a threshold failure, who decides whether to fix the regression or grant an exception? If the threshold owner is unavailable, who has override authority? If two thresholds conflict, one from Product and one from Trust and Safety, who arbitrates? These questions must be answered before a threshold blocks a release, not during the release. Teams that leave ownership ambiguous end up with long debates in Slack threads while the release is stalled. Teams that document ownership know exactly who to call.

Clear ownership also enables accountability. If a threshold is set too low and quality degrades, the threshold owner is responsible. If a threshold is set too high and blocks necessary releases, the threshold owner is responsible. This accountability creates an incentive to set thresholds thoughtfully and adjust them based on data. It prevents thresholds from being set arbitrarily by whoever happens to configure the CI pipeline. It ensures that thresholds reflect team priorities and stakeholder input, not engineering convenience.

## Starting With Coverage, Not Perfection

A common mistake when setting initial thresholds is to wait until you can set perfect thresholds for every metric. You want comprehensive coverage. You want tight alignment between thresholds and user experience. You want stakeholder consensus on every value. You want documented rationale for every decision. This perfectionism delays threshold rollout by months. In the meantime, you are shipping changes without gates. You are collecting data but not enforcing standards. You are building the infrastructure but not using it.

The correct approach is to start with coverage, not perfection. Set thresholds for the three most important metrics first. Accuracy, latency, safety. Document those thresholds. Enforce those thresholds. Let the team experience what it feels like to have regression gates. Let the gates catch a few regressions. Let the team see the value. Then add the next three metrics. Then the next three. Within three months, you have comprehensive coverage. Within six months, you have refined the thresholds based on real-world data. This incremental approach builds momentum and trust. It avoids the paralysis that comes from trying to design the perfect system on day one.

Starting with coverage also surfaces process issues early. You discover that your CI pipeline does not have the right hooks for threshold enforcement. You discover that your stakeholder approval process is too slow. You discover that your documentation tooling does not support version-controlled threshold config. You fix these issues while the scope is small, while only three thresholds are affected. By the time you roll out comprehensive coverage, the process is smooth and the tooling is stable. Teams that wait to set thresholds until everything is perfect never discover these issues until the system is complex and the stakes are high.

The journey from no thresholds to comprehensive regression gates is not instant. It takes weeks to measure baselines, weeks to set initial thresholds, weeks to refine those thresholds based on data. But the journey starts on day one. You measure the baseline immediately. You set the first thresholds conservatively. You document and enforce those thresholds from the first commit. This early enforcement creates the culture and the data you need to build a world-class regression system. The next subchapter explores how thresholds evolve over time through calibration and adjustment, ensuring that the quality bar rises as capability improves.

