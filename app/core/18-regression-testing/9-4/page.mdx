# 9.4 — Configuration Drift Detection

Configuration drift is the silent killer of AI system reliability. It is the accumulation of small, undocumented changes to parameters, thresholds, feature flags, environment variables, and runtime settings that slowly push a system away from its tested state. Unlike prompt changes, which teams track because they understand prompts are critical, configuration changes often happen invisibly. An engineer adjusts a timeout. A deployment script updates an environment variable. A feature flag gets toggled during an incident and never toggled back. Each change is small, each seems safe, each is forgotten — and together they create a production environment that no longer matches what your regression tests validate.

Configuration drift manifests as unexplained behavior changes. Your model suddenly returns shorter responses. Your RAG system retrieves fewer documents than expected. Your routing logic sends traffic to the wrong model. Your monitoring alerts fire at odd thresholds. You investigate and find that production is running with different settings than staging, or that two production regions have diverged, or that a configuration value was changed three weeks ago and no one documented it. The system still works, but it does not work the way you tested it. Configuration drift is the gap between the system you validated and the system your users experience.

## The Scope of Configuration Beyond Prompts

When teams think about AI configuration, they think about prompts first. Prompts are visible, version-controlled, and obviously important. But AI systems have dozens of configuration dimensions that are equally critical and less visible. Temperature and top-p control output randomness. Max tokens limits response length. Frequency and presence penalties adjust repetition. Stop sequences terminate generation. Timeout values determine how long you wait for a response before failing over. Retry counts control how you handle transient errors. Each of these parameters affects behavior, and each can drift.

Retrieval configuration is particularly drift-prone. How many documents do you retrieve per query? What similarity threshold do you use? Do you apply a reranking step? What metadata filters are active? These settings are often stored in environment variables or database tables rather than code. They get changed during debugging sessions and never restored. They vary between environments because someone made a one-time adjustment and forgot to propagate it. A RAG system tested with a 0.75 similarity threshold behaves differently when production runs at 0.65, but the drift is invisible unless you explicitly monitor configuration state.

Feature flags are configuration that controls which code paths execute. A feature flag might toggle between two prompt variants, enable or disable a guardrail, switch between model providers, or activate a new routing rule. Feature flags are designed to be temporary but often become permanent. A flag that was enabled during a test stays enabled forever. A flag that should be globally enabled is only enabled in one region. A flag that was disabled during an incident is never re-enabled. Feature flag drift creates systems where behavior depends on complex, undocumented flag states rather than the clean architecture you designed.

Thresholds and cutoffs define decision boundaries. What confidence score is required to bypass human review? What latency triggers a timeout? What error rate triggers an alert? What toxicity score blocks a response? These thresholds are tuned based on production data and business requirements. They change frequently. They are rarely version-controlled. Threshold drift means your system makes different decisions over time even with identical model outputs. A classification threshold set at 0.85 in January might drift to 0.80 by March, letting through borderline cases that you originally filtered. Unless you track threshold values over time, you will not notice the drift until users complain.

## Configuration Drift Causes and Vectors

Manual changes are the primary cause of drift. An on-call engineer adjusts a parameter during an incident to mitigate immediate harm. They increase a timeout to stop alerts, decrease a retrieval count to reduce latency, disable a feature that is causing errors. The incident resolves. The temporary change becomes permanent because no one documents it and no one remembers to revert it. Three months later, the system is running with incident-mode configuration in steady state. The settings that were correct under pressure are not correct under normal operation, but the drift is invisible.

Environment differences introduce drift by design. You deploy the same code to staging and production, but staging uses a lower-cost model to save money. Production uses GPU instances, staging uses CPU. Production has 10-second timeouts, staging has 30-second timeouts because you are less concerned with latency. These differences are intentional, but they mean your tests in staging do not validate production behavior. Configuration parity is impossible — production always has constraints that staging does not — but untracked differences accumulate. Eventually you cannot answer the question "what configuration is actually running in production?" without logging into every environment and checking.

Deployment errors create drift through partial rollouts. You deploy a configuration change to 50 percent of instances and observe. The change improves metrics, so you consider it successful. But you forget to deploy it to the remaining 50 percent. Now half your production traffic runs with the new configuration and half with the old. User experience varies randomly based on which instance they hit. Your regression tests pass because they only hit one instance. Your production metrics show averages across both configurations, masking the inconsistency. Partial rollout drift is especially dangerous in AI systems because the variation is in model behavior, not code behavior, and users notice.

Schema changes in configuration stores cause drift when old and new configurations coexist. You move a configuration parameter from an environment variable to a database. Some instances read the new location, some still read the old. You rename a parameter. Some code uses the old name, some uses the new. You change the expected format of a configuration value from a string to a number. Some parsing code handles both, some does not. During the transition, different parts of your system interpret configuration differently. Regression tests that assume a single configuration source miss the inconsistency.

## Baseline Tracking and Change Detection

Configuration baseline tracking is the practice of recording the known-good configuration state and detecting deviations from it. The baseline is not necessarily the original deployment configuration. It is the configuration that your current test suite validates. Every time you run regression tests, you should record what configuration those tests ran against. If production configuration diverges from the test baseline, you have drift.

The baseline includes every parameter that affects behavior. Model name and version. Temperature, top-p, max tokens, penalties. Retrieval count, similarity threshold, reranking settings. Feature flag states. Timeout values, retry counts, circuit breaker thresholds. Rate limits. Which guardrails are enabled. Which fallback paths are active. This is not metadata stored once in a deployment manifest. This is runtime configuration that must be read from the live system. Many parameters are set in environment variables that vary by instance, or in configuration files that get edited in place, or in remote configuration stores that change independently of code deployments.

Change detection works by comparing current configuration to baseline. You export configuration from production as a structured snapshot. You diff that snapshot against the baseline. Any difference is drift. Some drift is expected — you change configuration deliberately — but expected drift should be documented. If the diff shows a parameter change with no corresponding ticket, pull request, or deployment log, you have unexplained drift. If the diff shows different values across instances or regions that should be identical, you have inconsistency. Both are failures that should block release or trigger investigation.

Automated drift detection runs continuously. You do not wait for a deployment to check configuration. You check it every hour or every day. When drift is detected, you alert. The alert includes what changed, when it changed, and whether the change was documented. If the change is undocumented, the alert escalates. Someone manually modified production configuration outside the normal deployment process. This is a process failure, not just a configuration failure. It suggests that your configuration management system is too hard to use or that your team does not understand the importance of configuration parity with tests.

## Configuration Validation Gates

Configuration validation happens at two points: before deployment and continuously in production. Pre-deployment validation answers the question: Is this configuration safe to deploy? Continuous validation answers the question: Is production still running the configuration we deployed? Both are necessary. Pre-deployment validation prevents bad configuration from reaching users. Continuous validation catches drift that happens after deployment.

Pre-deployment validation starts with schema checks. Does the configuration match the expected format? Are all required parameters present? Are parameter values within valid ranges? Temperature between 0.0 and 2.0. Max tokens positive. Similarity threshold between 0.0 and 1.0. Schema validation catches typos, missing values, and out-of-range settings before they cause runtime errors. It is the cheapest validation because it does not require calling the model. You can validate schema in seconds during CI.

Behavioral validation tests that configuration produces expected behavior. You run a subset of your regression suite against the new configuration in a staging environment. If the regression tests pass, the configuration is likely safe. If they fail, the configuration change broke something. Behavioral validation is slower and more expensive than schema validation, but it catches semantic errors that schema checks miss. A temperature of 1.5 passes schema validation but might produce unacceptably random outputs. Behavioral validation detects this before production.

Consistency checks verify that configuration is uniform across all instances, regions, and environments that should match. If you deploy the same code to three regions, those regions should have the same configuration unless you explicitly choose regional differences. Consistency checks compare configuration snapshots across environments and flag discrepancies. These checks prevent configuration skew where US-East runs different settings than EU-West, leading to regionally inconsistent user experiences.

Continuous production validation runs the same checks after deployment. It compares production configuration to the deployment manifest. It checks that all instances converged to the new configuration. It verifies that no parameters drifted since the last check. If validation fails, you have three options. Roll back to baseline configuration. Document the drift and update the baseline if the drift is intentional. Investigate and remediate if the drift is unexplained. The key is that validation failures are not ignored. Configuration drift is a release-blocking issue, not a warning.

## Environment Configuration Parity

Environment parity is the principle that all environments should run as similar a configuration as possible. Perfect parity is impossible — production has scale, uptime, and cost constraints that staging does not. But unnecessary differences are sources of bugs that only appear in production. The more your environments diverge, the less your staging tests predict production behavior.

Parity violations fall into categories. Model differences: staging uses GPT-5-mini, production uses GPT-5. This saves cost but means staging tests a different model than users experience. Parameter differences: staging uses temperature 0.3, production uses 0.7. Staging is deterministic for test consistency, production is more creative. Infrastructure differences: staging uses a single-region deployment, production is multi-region with failover. Staging has no rate limits, production does. Each difference is a gap between what you test and what you ship.

The solution is not to eliminate all differences — that is impractical — but to track them explicitly. Maintain a configuration diff document that lists every known difference between environments and the rationale for each. When you add a difference, document it. When you remove a difference, document that too. The diff document is your acknowledgment that environments are not identical and your record of what those differences are. If a bug appears in production that did not appear in staging, you consult the diff document to see if any tracked difference explains it.

Synthetic production testing reduces the need for parity by testing in production with real configuration. You send probe traffic through production instances with known inputs and verify outputs. This validates that production configuration matches expectations even if staging configuration does not. Synthetic testing does not replace staging tests — you still need a safe environment to catch regressions before they affect users — but it provides a final check that production matches the configuration your regression tests validated.

## Configuration Change Audit Trails

Every configuration change must be auditable. Who changed what, when, and why. This is not optional. AI systems fail in subtle ways, and configuration drift is a common cause. When you investigate a degradation that started two weeks ago, you need to know what configuration changes happened two weeks ago. Without an audit trail, you are guessing.

Audit trails start with version control for configuration files. Configuration should live in Git, not in environment variables hard-coded into deployment scripts. When configuration changes, it changes via pull request. The pull request includes a description of what changed and why. It includes test results showing that the change passed validation. It gets reviewed by a second engineer. It gets merged only after approval. This process creates a Git history that is your audit trail. You can see every configuration change, who made it, and when.

Runtime configuration changes require additional logging. If configuration lives in a database or a remote configuration service, changes are not captured in Git. You need application-level logging that records every configuration write. The log entry includes the parameter name, the old value, the new value, the timestamp, and the identity of the user or service that made the change. This log is your audit trail for runtime configuration. When a parameter drifts, you grep the log to find when it changed and who changed it.

Automated changes need audit trails too. If a script adjusts configuration based on metrics — scaling up retrieval count under load, adjusting thresholds based on precision-recall curves — those changes must be logged. The log should include not just what changed but why: which metric triggered the change, what decision rule was applied, what threshold was crossed. Automated configuration changes without audit trails are invisible. You see the effect but cannot trace the cause.

## Configuration Rollback Procedures

When drift is detected, you need a path back to baseline. Configuration rollback is simpler than code rollback because configuration does not have dependencies. You can change a parameter without rebuilding or redeploying. But rollback still requires procedure. You need to know what baseline to roll back to. You need to verify that rollback restores expected behavior. You need to ensure rollback completes across all instances, not just some.

Rollback targets are stored configurations that you know work. Every time you deploy a configuration that passes validation, you tag it as a rollback candidate. The tag includes a timestamp, a version identifier, and a pointer to the configuration snapshot. When you need to roll back, you choose the most recent known-good tag and restore it. Rollback targets expire. A configuration from six months ago might not be valid anymore if your model, infrastructure, or business logic has changed. Keep the last five or ten known-good configurations as rollback candidates and archive the rest.

Rollback validation runs the same checks as deployment validation. After restoring baseline configuration, you verify that it produces expected behavior. You run regression tests. You check that instances converged. You monitor metrics to ensure the rollback fixed the issue that triggered it. Rollback is not fire-and-forget. It is a deployment in reverse, and it needs the same validation rigor as a forward deployment.

Partial rollback is sometimes necessary. You detect drift in one parameter but not others. You want to restore that parameter without reverting the entire configuration. Partial rollback requires granular configuration management — the ability to update individual parameters without touching unrelated ones. It also requires testing. Rolling back one parameter in isolation might break an assumption elsewhere in the system. Test partial rollbacks in staging before applying them in production.

Configuration drift detection is your defense against the silent erosion of system reliability. With prompts tested and configuration tracked, the next layer of regression testing addresses tool and function call behavior, where model interactions with external systems introduce another category of failure risk.

