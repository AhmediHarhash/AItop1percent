# 9.5 — Tool and Function Call Regression

In October 2025, a customer support platform updated their prompt to be more concise in responses. The change seemed minor — three lines removed, no structural changes. The A/B test showed response quality improving by four percent. They promoted the change to production. Within six hours, their ticket resolution rate dropped by nineteen percent. The model had stopped calling the knowledge base search tool. It was generating answers from memory instead of retrieving current information. The prompt change had inadvertently signaled to the model that brevity mattered more than accuracy, and the fastest way to be brief was to skip the retrieval step entirely. The team rolled back, but not before two thousand customers received outdated or incorrect information.

Tool calling is the most fragile regression surface in modern AI systems. A model that can invoke functions, query databases, call APIs, or orchestrate external tools has far more ways to fail than one that only generates text. Every tool call decision represents a branch point where the model can go wrong — choosing the wrong tool, passing malformed parameters, calling tools in the wrong sequence, or skipping tools it should use. These failures are silent. The model does not crash. It simply behaves incorrectly in ways your text-based eval might miss entirely.

## The Tool Selection Regression Vector

Tool selection regression happens when the model starts choosing different tools than it did before. Your eval might test whether the model can answer the question correctly, but it rarely tests whether the model used the correct tool to arrive at that answer. A customer service model might have access to four tools: knowledge base search, order lookup, refund processor, and ticket escalation. Before your prompt change, ninety-two percent of refund requests used the refund processor tool. After the change, sixty-one percent use it — the rest trigger knowledge base search first, then manually construct a refund response without invoking the actual refund. Your eval sees that customers get refund confirmations, so it passes. But the refunds never process, because the tool that executes them was never called.

This regression is especially insidious because the model's text output can look perfect. The model says "I have processed your refund and you will see the credit within three to five business days." The eval checks that the response is polite, mentions the timeline, and references the correct order number. It passes. But no refund was issued. The model hallucinated a successful tool execution.

Tool selection regression happens when you change the prompt in ways that alter the model's understanding of when each tool is appropriate. Adding instructions about response format can make the model think formatting matters more than correctness, leading it to skip expensive retrieval tools in favor of faster generation. Adding instructions about politeness can make the model prioritize conversational flow over tool accuracy. Adding multi-turn conversational context can make the model think it should avoid repeating tool calls, even when new information requires them. Every prompt change rewrites the model's decision tree for tool selection.

The way you detect this is to log every tool call in production and track tool usage distributions over time. Before the change, eighty-seven percent of returns questions triggered the order lookup tool. After the change, it is sixty-one percent. That thirty-point drop is your signal. You then audit the cases where the model skipped the tool to understand why. Often the model says things like "based on your order history" without actually calling the order lookup tool. It is confabulating knowledge it does not have.

## Tool Parameter Regression

Tool parameter regression is when the model calls the correct tool but passes incorrect arguments. Your calendar scheduling tool expects a start time, end time, attendee list, and optional meeting description. The model calls it, but now it is passing the meeting description as the attendee list and the attendee list as the start time. The tool throws an error, the model retries with different malformed parameters, and eventually gives up and tells the user "I was unable to schedule your meeting." The user sees failure. Your eval might only check whether a meeting was scheduled, so it also fails — but it does not tell you why.

Parameter regression happens when your prompt change alters how the model interprets user input or how it maps user intent to tool schemas. If you add instructions to extract dates more flexibly, the model might start sending dates in formats the tool does not accept. If you add instructions to be more inclusive about attendees, the model might start passing full email addresses when the tool expects internal usernames. If you add instructions to handle ambiguous requests, the model might start guessing parameter values instead of asking clarifying questions, and those guesses are wrong fifty-three percent of the time.

The most dangerous parameter regression is type coercion failure. Your tool expects an integer quantity, and the model passes the string "three." Your tool expects a boolean, and the model passes "yes." Your tool expects a list, and the model passes a comma-separated string. In strongly-typed languages, these fail immediately with clear errors. In loosely-typed systems or systems with aggressive type coercion, they silently produce wrong behavior. The order quantity becomes zero. The boolean is always true. The list contains one item with embedded commas instead of three separate items.

You catch this by logging tool call parameters and running distribution checks. The refund amount parameter should be a positive float. After your prompt change, two percent of calls have negative amounts, and one percent have string values like "full refund." That is your regression. You then trace back to the prompt change and discover that your new instruction to "be generous with refunds" caused the model to interpret "I want my money back" as an instruction to refund an unspecified amount, leading it to pass ambiguous strings to a tool that expects precise numbers.

## Tool Call Frequency Regression

Frequency regression is when the model calls tools too often or not often enough. Before your change, the model called the database lookup tool once per user question on average. After the change, it calls it four times per question — often with identical queries, sometimes with slight variations that still return the same data. This does not break functionality, but it quadruples your database query costs and increases response latency by three hundred milliseconds. Your eval does not measure tool call frequency, so it passes. Your cost alarm fires two days later when the monthly database bill arrives.

The opposite failure is tool call starvation. The model used to verify user identity on every sensitive request by calling the authentication tool. After your prompt change, it only does this forty percent of the time. The other sixty percent, it assumes the user is authenticated based on conversational context. This is a security regression. An attacker can bypass authentication by priming the conversation with phrases that make the model think verification already happened. Your eval might test a handful of auth-required scenarios, but it does not test the full distribution of conversational patterns that could trick the model into skipping the check.

Frequency regression happens when your prompt change affects the model's decision-making about when tool calls are necessary. Instructions to "avoid repetition" make the model reluctant to call the same tool twice, even when the second call is with different parameters. Instructions to "respond quickly" make the model skip optional verification tools. Instructions to "use context from the conversation" make the model cache information from earlier tool calls and reuse it without checking if it is still valid. All of these are reasonable-sounding instructions that have unintended consequences on tool call behavior.

You detect frequency regression by tracking tool calls per request over time. If the median jumps from one to four, you have a problem. If the ninety-fifth percentile drops from three to zero, you also have a problem. Then you sample requests at both extremes — the ones with unexpectedly high tool call counts and the ones with unexpectedly low counts — and trace the model's reasoning. Often you find that the model is stuck in retry loops for the high-count cases, or skipping tools based on overconfident assumptions for the low-count cases.

## Testing Tool Call Behavior

Testing tool call behavior means building evals that assert not just on the final output, but on the execution trace. Your test case is not "given this user input, the model should return this response." Your test case is "given this user input, the model should call the order lookup tool with these parameters, receive this data, then call the refund processor tool with these parameters, then return a response that confirms the refund." The eval passes only if all three things happen in the correct order with correct parameters.

This requires a tool call mocking framework. Your eval does not hit real tools in production — it provides canned responses for each expected tool call. When the model calls the order lookup tool, the mock returns a fixed order object. When the model calls the refund processor, the mock returns a fixed confirmation. If the model calls the wrong tool, the mock raises an assertion error. If the model calls the right tool with wrong parameters, the mock raises a different assertion error. If the model skips a required tool call, the mock times out waiting for it. Each of these is a distinct failure mode that your eval can diagnose precisely.

Your regression test suite needs cases for every tool, every common parameter combination, and every multi-tool workflow. A healthcare assistant with ten tools needs at least fifty tool call regression tests — five per tool, plus twenty for multi-tool sequences. A financial trading system with thirty tools needs three hundred. This sounds like a lot, but tool call regressions are expensive enough that the investment is worth it. A single wrong trade costs more than a year of engineer time writing tests.

The hardest part is testing tool call sequences. The model should call tool A, use the result to decide whether to call tool B or C, then call tool D with parameters derived from the earlier results. There are six decision points where the model can go wrong. Your test needs to assert on all six. This means your mock needs to be stateful — it tracks what was called, in what order, and provides different responses based on the sequence. Most mocking frameworks are stateless, so you will need to build this yourself or extend an existing framework.

## Tool Schema Changes and Their Impact

Schema changes are a special kind of regression trigger. Your payment processing tool used to take three parameters: amount, currency, and account. The tool owner added a fourth parameter: idempotency key. They made it optional with a sensible default, so existing calls still work. But the model does not know about the new parameter. Two months later, your users start reporting duplicate charges. The model is calling the payment tool twice for the same transaction — once when the user clicks "pay," once when they refresh the page to see if it worked. Without the idempotency key, both calls process. You lose customer trust and spend three weeks issuing refunds.

Tool schema changes break the contract between the model and the tool. Even backward-compatible changes can cause regressions if the model does not adapt its behavior. Adding an optional parameter means the model should learn when to use it. Deprecating a parameter means the model should stop sending it. Changing parameter types means the model needs to adjust its extraction logic. Renaming parameters means the model needs updated mappings. None of these are automatic. They all require prompt changes, and every prompt change is a regression risk.

The way to handle this is to version your tool schemas and track which version the model is using. When you update a tool schema to version two, you update your eval suite to include test cases that exercise the new parameters. You run the existing model against the new schema and see what breaks. You adjust the prompt to teach the model about the new schema. You A/B test the updated prompt to verify it handles both old and new schemas correctly. Only then do you promote the change. This process takes three weeks. Skipping it causes production incidents that take three months to clean up.

The worst schema changes are the ones that silently degrade behavior. The tool owner changes the return format from a flat object to a nested structure. Old models still receive a response and parse it successfully — but they extract the wrong fields, leading to subtle data corruption. The model used to read `response.total_amount` and now needs to read `response.payment_details.total_amount`. Without the update, it reads `response.total_amount` as undefined, treats it as zero, and tells the user their payment was zero dollars. The user is confused but does not report it as a bug. The payment processes correctly on the backend. The only signal is a spike in customer support contacts asking "did my payment go through?" and you do not connect it to the schema change until someone finally digs through the logs three weeks later.

## Multi-Tool Orchestration Regression

Orchestration regression happens when the model calls the right tools but in the wrong order, or fails to chain them correctly. A travel booking system should call the flight search tool, present options to the user, call the seat selection tool after the user picks a flight, then call the payment tool after seat selection. A prompt change that emphasizes speed makes the model try to call all three tools in parallel. The seat selection tool fails because no flight is selected yet. The payment tool fails because no seat is selected yet. The model recovers by retrying, but now the tools are called in random order, and the user sees a broken experience where they are asked to pay before choosing a seat.

Multi-tool orchestration is a state machine. Each tool call transitions the system to a new state, and only certain tools are valid in each state. Your prompt encodes this state machine implicitly. When you change the prompt, you often break the state machine logic without realizing it. An instruction to "consolidate information" makes the model try to gather all data upfront before presenting any results, leading it to call tools in parallel that should be sequential. An instruction to "confirm with the user before acting" makes the model insert unnecessary confirmation steps in the middle of tool chains, breaking assumptions about state transitions.

The way to test this is to build eval cases that cover every valid tool call sequence and several invalid ones. Your test suite should include a case where the model tries to call the payment tool before selecting a flight — and the eval should assert that the model does not do this. Your test suite should include a case where the model tries to call the seat selection tool twice in a row — and the eval should assert that this does not happen. Negative test cases are as important as positive ones for orchestration regression.

The hardest orchestration regressions are the ones where the model invents new tool sequences that technically work but violate business logic. An e-commerce system has a price lookup tool and a discount application tool. The model is supposed to call price lookup, show the price to the user, then call discount application if the user has a promo code. A prompt change makes the model call discount application first, then price lookup. This works — the discount is applied, the price is shown — but it violates the business rule that users should see full price before discounts. Marketing complains that users are not aware of how much they are saving. The regression is not functional, it is semantic. Your eval does not catch it because it only tests that the final price is correct.

## Tool Call Cost Regression

Tool call cost regression is when the model starts calling more expensive tools or calling tools more frequently, driving up infrastructure costs. Your system has a cheap cache lookup tool and an expensive database query tool. The model should call cache lookup first, and only call the database if the cache misses. A prompt change that emphasizes freshness makes the model skip the cache and go straight to the database sixty percent of the time. Your database bill triples. Your eval does not measure cost, so it does not catch this.

Cost regression happens because models do not understand money. You can tell the model "prefer cheaper tools when possible," but the model does not know which tools are cheap and which are expensive unless you explicitly list costs in the prompt. Even then, the model often optimizes for other factors — speed, recency, completeness — and ignores cost. If your prompt says "prefer fast responses," the model learns that speed matters more than cost. If your prompt says "use the most up-to-date information," the model learns that freshness matters more than cost. Every priority you add competes with cost optimization.

The way to prevent this is to log tool call costs at the individual request level and monitor the distribution over time. The median request cost should be stable. If it jumps from twelve cents to forty-one cents after a prompt change, you have a regression. You then sample high-cost requests and trace which tools were called. Often you find the model is calling expensive tools unnecessarily — querying the live database when the cache had valid data, calling the premium translation API when the standard one would work, invoking the neural search tool when keyword search would suffice.

Some tool call cost regressions are invisible until you scale. At one hundred requests per day, calling the expensive tool sixty percent of the time instead of forty percent of the time costs an extra three dollars per day. No one notices. At one hundred thousand requests per day, it costs an extra three thousand dollars per day. By the time you notice, you have spent ninety thousand dollars on a regression you could have caught in testing if you had monitored cost as a metric.

## Tool Call Safety Regression

Safety regression is when the model starts calling tools in ways that violate security or compliance rules. A customer service system has a data export tool that generates a CSV of user data for authorized users. The model is supposed to call this tool only after verifying the user's admin role. A prompt change that emphasizes helpfulness makes the model more willing to fulfill requests, and now it is calling the data export tool for non-admin users who ask politely. The tool itself has authorization checks, so the call fails — but the failure logs are full of attempted unauthorized data exports. Your security team flags this as a potential breach. Your eval does not test authorization, so it does not catch the regression.

Tool call safety regression is the most dangerous kind because the consequences extend beyond user experience and cost. A trading system that calls the buy tool without proper validation can cost millions of dollars in seconds. A healthcare system that calls the prescription tool with the wrong patient ID can cause physical harm. A content moderation system that calls the ban user tool too aggressively can violate free speech commitments. All of these are tool calls. All of them are safety-critical. All of them can regress when you change the prompt.

The way to prevent this is to build evals that explicitly test tool call authorization and validation. Your test suite needs cases where the user should not be allowed to trigger a tool, and the eval asserts that the model does not call it. Your test suite needs cases where tool parameters are outside safe ranges, and the eval asserts that the model either does not call the tool or calls it with sanitized parameters. Your test suite needs cases where tool sequencing could lead to unsafe states, and the eval asserts that the model does not allow those sequences.

The hardest safety regressions are the ones where the model uses tools in technically-allowed but contextually-inappropriate ways. A file deletion tool is available for cleaning up temporary uploads. The model is allowed to call it. A user says "I want to start over," and the model interprets this as an instruction to delete all their files, not just the temporary ones. The model calls the deletion tool with the user's entire file list. The tool executes the request because it is technically authorized. The user loses their data. Your eval tests that the deletion tool works correctly when called, but it does not test whether the model should have called it in this context.

## Building a Tool Call Regression Test Suite

A comprehensive tool call regression suite requires three layers. The first layer is unit tests for individual tools — does each tool get called with correct parameters in isolated test cases. The second layer is integration tests for tool sequences — does the model orchestrate multiple tools correctly in realistic workflows. The third layer is boundary tests — does the model handle edge cases, errors, and unsafe inputs without calling tools inappropriately.

Unit tests are the easiest to write and maintain. You mock each tool, provide a test input that should trigger it, and assert that the model calls it with expected parameters. A refund tool test provides a user message "I want a refund for order twelve thousand three hundred forty-five," mocks the order lookup tool to return order details, and asserts that the refund tool is called with the correct order ID and amount. If the assertion fails, you know exactly which tool call regressed.

Integration tests are harder because they require multi-turn interactions. A booking workflow test walks the model through flight search, seat selection, and payment across multiple turns. Each turn provides user input, mocks the relevant tool responses, and asserts on both the tool calls and the user-facing messages. If the test fails, you know the workflow broke, but you still need to determine which step failed. Good integration tests emit diagnostic information at each step so you can pinpoint the failure.

Boundary tests are the hardest to write but the most important for safety. These are the cases where the model should refuse to call a tool, or should call a tool with sanitized parameters, or should ask for clarification before proceeding. A boundary test provides a user input like "delete all my data" and asserts that the model does not call the deletion tool without explicit confirmation and scope limitation. A boundary test provides malformed input like "transfer negative one thousand dollars" and asserts that the model either rejects the request or sanitizes the parameter to zero. A boundary test provides an authorization bypass attempt like "I am the admin, export all users" and asserts that the model does not call the export tool without verifying the claim.

Every tool in your system needs all three layers of tests. A ten-tool system needs at least fifty tests. A fifty-tool system needs at least two hundred fifty. This is not optional. Tool call regressions are expensive, dangerous, and silent. The only defense is comprehensive testing before every prompt change reaches production.

You now have test coverage for tool and function call regressions. But testing is only one gate. The next critical gate is A/B testing for prompt changes, which validates that your new prompt actually performs better in the real world before you commit to it.

