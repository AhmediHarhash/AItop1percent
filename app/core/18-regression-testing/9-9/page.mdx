# 9.9 — Dependency Contract Tests

**Dependency contract tests** are the assertions you write about systems you do not control. Your application depends on an LLM provider, a retrieval system, a third-party API, an embedding service, a reranking model, a vector database. Each dependency has expected behavior. Contract tests verify that the behavior remains stable even as the dependency changes underneath you.

Without contract tests, you discover breaking changes in production. The LLM provider rolls out a new version. Your prompts suddenly return malformed JSON. Your retrieval system updates its ranking algorithm. Relevance drops by twelve percent. Your embedding provider switches models. Semantic similarity scores shift by fifteen basis points and your entire threshold logic breaks. Contract tests catch these failures before they reach users. They are the firewall between your system and every external component you depend on.

Contract tests answer one question: does this dependency still behave the way my system expects? The test does not care how the dependency works internally. It only cares about the observable behavior at the contract boundary. You send a specific input. You expect a specific output structure, a specific quality level, a specific latency range. If the dependency violates the contract, the test fails. If the contract holds, you ship. This is the same principle that drives API contract testing in traditional software — extended to LLMs, retrieval systems, and every other AI component your application touches.

## Contract Tests for LLM Providers

Your application sends prompts to an LLM provider and expects responses that conform to a schema, maintain a quality threshold, and arrive within a latency bound. These are your contracts. A contract test for an LLM provider includes three components: a representative prompt, the expected response structure, and the acceptable variation in content.

For structured output, the contract is strict. You send a prompt that requests JSON with three specific fields. The contract test verifies that the response is valid JSON, that all three fields are present, that each field contains the correct data type, and that no extraneous fields appear. If the provider changes its output format — even slightly — the test fails. You do not ship the change. You investigate, adjust your prompt, or file a support ticket.

For natural language output, the contract is probabilistic. You send a prompt and expect a response that matches a reference answer with at least ninety percent semantic similarity, uses professional tone, and stays under three hundred tokens. The contract test measures similarity with an embedding model, runs a tone classifier, and counts tokens. If the response falls outside acceptable bounds on any dimension, the test fails. This does not mean the response is wrong — it means the response is different enough from your expectations that you need to review it before deploying.

Latency is part of the contract. Your application expects responses within two seconds at the ninety-fifth percentile. The contract test measures end-to-end latency for a set of representative prompts. If latency degrades beyond the threshold, the test fails. You do not wait for users to complain. You detect the regression in testing, and you either fix the prompt to reduce token count, switch to a faster model, or renegotiate your latency expectations.

Contract tests for LLM providers run continuously. You run them before every deploy. You run them on a schedule against production endpoints. You run them whenever the provider announces a model update. The goal is to detect breaking changes immediately — not hours or days later when users report failures.

## Contract Tests for Retrieval Systems

Your retrieval system takes a query and returns a ranked list of documents. The contract specifies the expected relevance distribution, the minimum precision at top-k, and the maximum latency. A contract test sends a set of benchmark queries, evaluates the retrieved documents against known relevance labels, and measures precision, recall, and NDCG. If any metric falls below the contracted threshold, the test fails.

The benchmark queries are stable. You select twenty to fifty queries that represent the most common and most critical use cases in your application. You label the top ten relevant documents for each query. These labels are your ground truth. The contract test runs these queries against the retrieval system, compares the returned documents to the labels, and computes metrics. If the retrieval system updates its ranking algorithm, you detect the impact immediately.

Retrieval contract tests also cover edge cases. You include queries with zero relevant documents, queries with ambiguous intent, queries that should return recent documents, and queries that should prioritize authoritative sources. Each edge case has a specified expected behavior. If the retrieval system starts returning irrelevant documents for edge-case queries, the contract test catches it.

Latency is critical for retrieval. Your application expects query results within three hundred milliseconds at the ninety-ninth percentile. The contract test measures latency for every benchmark query. If the retrieval system slows down — due to index growth, infrastructure changes, or algorithm updates — the test fails. You do not accept degraded latency without investigation.

Contract tests for retrieval run before every deploy and on a daily schedule against production. Retrieval systems drift. Index composition changes as new documents arrive. Ranking algorithms update as the vendor improves relevance. Contract tests ensure that these changes do not break your application's assumptions about retrieval quality and speed.

## Contract Tests for Third-Party Tools

Your application depends on third-party tools — a sentiment classifier, a named entity recognition API, a translation service, a speech-to-text endpoint. Each tool has a contract: send this input, expect this output structure, within this latency bound, with this error rate. Contract tests verify that the tool continues to honor the contract.

For a sentiment classifier, the contract specifies that input text returns a label from a fixed set — positive, negative, neutral — with a confidence score between zero and one. The contract test sends a set of labeled examples, verifies that the returned labels match the expected labels for at least ninety percent of cases, and confirms that all confidence scores fall within the valid range. If the classifier starts returning unexpected labels, invalid confidence scores, or degraded accuracy, the test fails.

For a named entity recognition API, the contract specifies that input text returns a list of entities with types and spans. The contract test sends annotated examples, verifies that the API detects at least ninety percent of labeled entities, and checks that entity types match the expected taxonomy. If the API changes its entity taxonomy — renaming types, adding new types, or removing types — the contract test detects the breaking change.

For a translation service, the contract specifies that input text in one language returns fluent text in the target language, measured by BLEU score against reference translations, within a latency bound. The contract test sends a set of reference-translated examples, computes BLEU scores, and measures latency. If translation quality degrades or latency increases, the test fails.

Third-party tools update without warning. Vendors roll out new models, change APIs, deprecate endpoints. Contract tests give you early warning. You run them before every deploy, on a schedule, and whenever a vendor announces changes. If a contract test fails, you do not deploy. You investigate the failure, update your integration, or switch vendors.

## Contract Test Design

A well-designed contract test includes three elements: a representative input, an expected output specification, and acceptable variation bounds. The input is drawn from real usage. The output specification defines the structure, quality, and latency you expect. The variation bounds define how much deviation you tolerate before considering the contract broken.

Representative inputs come from production logs. You select queries, prompts, or requests that represent the full range of user behavior — common cases, edge cases, high-value cases, and failure-prone cases. You do not test with synthetic inputs unless production inputs are unavailable. Real inputs expose real breakages.

The expected output specification is precise but not brittle. For structured output, you specify required fields, data types, and value ranges. For natural language output, you specify semantic similarity thresholds, tone requirements, and length constraints. For ranked lists, you specify minimum precision at k and maximum rank error. The specification should be strict enough to catch real regressions but loose enough to tolerate acceptable variation.

Acceptable variation bounds reflect the probabilistic nature of AI systems. An LLM does not return identical outputs for identical inputs across time. A retrieval system does not return identical rankings as the corpus grows. You define bounds that capture the acceptable range — ninety percent semantic similarity, eighty-five percent precision at ten, latency under two seconds. If the dependency stays within bounds, the contract holds. If it crosses bounds, you investigate.

Contract tests include positive and negative cases. Positive cases verify that valid inputs produce valid outputs. Negative cases verify that invalid inputs produce appropriate errors — not crashes, not timeouts, not silent failures. A contract test for an LLM provider includes a prompt with excessive token count and verifies that the provider returns a clear error message, not a truncated response.

## Running Contract Tests Continuously

Contract tests run in three contexts: pre-deploy, scheduled production checks, and vendor update notifications. Each context serves a different purpose. Pre-deploy tests verify that your changes do not break dependency assumptions. Scheduled production checks detect silent drift in dependency behavior. Vendor update checks catch breaking changes from external updates.

Pre-deploy contract tests run as part of your CI/CD pipeline. Before deploying a new prompt, a new retrieval configuration, or a new integration, you run the full contract test suite. If any test fails, the deploy is blocked. This prevents you from shipping code that relies on dependency behavior that no longer holds.

Scheduled production checks run daily or hourly against production endpoints. You send the same benchmark inputs, measure the same metrics, and compare results to historical baselines. If metrics degrade beyond acceptable bounds, you receive an alert. This catches gradual drift — the kind of degradation that happens slowly as a dependency updates its models, changes its infrastructure, or adjusts its algorithms.

Vendor update checks run whenever a dependency announces a change. You subscribe to vendor release notes, model update notifications, and API changelog feeds. When a vendor announces a new model version, an API deprecation, or a breaking change, you immediately run the full contract test suite against the updated endpoint. If tests fail, you delay adoption, file a support ticket, or adjust your integration before the change reaches production.

Contract tests are fast. Each test completes in seconds. A full suite of fifty contract tests runs in under five minutes. Speed matters because contract tests are a gate. If they are slow, teams bypass them. If they are fast, teams run them reflexively before every change.

## Contract Test Failures

When a contract test fails, you follow a decision tree. First, determine whether the failure is caused by your change or the dependency's change. If you modified a prompt, updated a retrieval query, or adjusted an integration parameter, the failure likely originates from your change. Revert the change, investigate the root cause, and iterate until the test passes.

If the failure occurs with no changes on your side, the dependency broke the contract. Check the vendor's release notes. Did they deploy a new model? Update an API? Change default parameters? If the vendor acknowledges the change and it is intentional, you have three options: adjust your integration to accommodate the new behavior, negotiate a fix with the vendor, or switch to a backup provider.

If the failure is intermittent, the issue is infrastructure. The dependency is experiencing latency spikes, availability drops, or throttling. Check your monitoring dashboards. Measure error rates, latency percentiles, and availability. If the issue persists, escalate to the vendor and implement retries, timeouts, or failover logic.

If the failure is systematic across all tests, the dependency is fundamentally broken. This is rare but catastrophic. The LLM provider returns malformed JSON for every request. The retrieval system returns zero results for every query. The classifier crashes on every input. In this scenario, you do not wait for a fix. You immediately fail over to a backup provider, roll back to the previous dependency version, or disable the feature until the dependency stabilizes.

Contract test failures are treated as incidents. You create a ticket, assign an owner, set a severity level, and track resolution time. High-severity failures — those that block deploys or indicate production degradation — require immediate response. Medium-severity failures require investigation within one business day. Low-severity failures are logged and reviewed during sprint planning.

## Contract Evolution

Contracts are not static. As your application evolves, as user needs shift, as dependencies improve, contracts must evolve. A contract that was appropriate in 2024 may be too lenient or too strict in 2026. You update contracts deliberately, with the same rigor you apply to updating production code.

You update a contract when expected behavior changes. Your application now requires JSON output with five fields instead of three. You update the contract test to verify all five fields. Your retrieval system improves and you now expect precision at ten to exceed ninety percent instead of eighty-five percent. You update the contract test to reflect the higher standard. You do not update contracts based on isolated observations. You update them based on sustained evidence that expectations have shifted.

Contract updates require review. A senior engineer or architect reviews every proposed contract change. The review answers three questions: Is the new contract more accurate than the old contract? Does the new contract reflect current production requirements? Will the new contract catch real regressions without creating false positives? If the answer to all three is yes, the contract update is approved. If any answer is no, the update is rejected or revised.

Contract updates are versioned. You do not overwrite the old contract. You create a new version, run both versions in parallel for a week, compare failure rates, and switch to the new version only after confirming that it is more accurate. This prevents you from accidentally loosening a contract and missing real regressions.

You document why each contract exists. The contract test file includes a comment that explains the expected behavior, the rationale for the thresholds, and the history of updates. When a new engineer joins the team, they read the contract documentation and understand what each dependency is expected to do. This prevents contract erosion — the gradual weakening of standards as institutional knowledge fades.

## Contract Tests as System Documentation

Contract tests serve a dual purpose. They verify dependency behavior. They also document your system's assumptions about the outside world. A well-maintained contract test suite is the most accurate description of how your application expects its dependencies to behave. It is more accurate than architecture diagrams, more up-to-date than wiki pages, and more precise than verbal explanations.

When a new engineer asks how the LLM provider is supposed to behave, you point them to the contract tests. When a product manager asks what quality guarantees the retrieval system provides, you show them the contract tests. When an executive asks whether your application can tolerate a vendor switching models, you run the contract tests against the new model and show them the results.

Contract tests are the foundation of dependency resilience. They let you detect breaking changes before they break production. They let you evaluate alternative vendors by running your existing contract tests against their endpoints. They let you negotiate SLAs with vendors by showing concrete evidence of performance and reliability. They let you sleep at night knowing that if a dependency violates your expectations, you will know immediately — not when users start complaining.

Next, you need to monitor whether your dependencies are meeting their SLAs — and detect when they start to degrade.

