# 1.7 — The Traceability Imperative

In October 2025, a healthcare AI team detected a 12-point drop in accuracy on their clinical note summarization system. The regression showed up in production monitoring after three days of degraded output quality. They knew something had broken. They could see the drop in their metrics dashboard. But when they tried to fix it, they hit a wall. The team could not determine which change caused the problem. Was it the prompt revision from the previous week? The model update from three days ago? The retrieval index rebuild from Tuesday? The config change that adjusted context window size? They had changed four components within a seven-day window and kept no version mapping between test results and system state. It took them eleven days to isolate the cause — a prompt change that interacted badly with the new model — and by then, 47,000 clinical notes had been processed with degraded quality. The fix took two hours. The forensics took eleven days.

**Traceability** is the ability to map every test result to the exact versions of every component that produced it. Without traceability, regression detection becomes archaeology. You know something broke. You do not know what. You do not know when. You do not know which combination of changes caused it. And without that knowledge, you cannot fix it — you can only guess, deploy, and hope. This is not acceptable when your system processes medical data, financial transactions, or legal documents. This is not acceptable when your reputation depends on reliability. This is not acceptable at any scale.

Every regression test must carry a complete version fingerprint. If a test fails, you must be able to answer six questions immediately: which prompt version, which model version, which data version, which retrieval index version, which config version, which code version. If you cannot answer all six, you do not have traceability. You have a detective problem.

## The Forensics Tax

The healthcare team spent 264 engineering hours on forensics. They re-ran tests against archived versions of the prompt. They spun up previous model checkpoints. They compared retrieval results across index versions. They examined config diffs. They rebuilt the timeline of changes by correlating Slack messages, pull request timestamps, and deployment logs. They found the cause eventually — but the process was manual, slow, and expensive. Every hour spent on forensics was an hour not spent building features, not spent improving the system, not spent on the next quality improvement.

This pattern repeats across every team that lacks traceability. A regression appears. The team knows the current state is broken. They do not know the last known good state. They do not know which change introduced the problem. So they start walking backward through history, testing hypotheses, eliminating possibilities. It works. It is just catastrophically inefficient. The median time from regression detection to root cause identification, across teams without traceability systems, is eight days. With traceability, it is eight minutes. You see the failure. You check the version fingerprint. You compare it to the last passing fingerprint. You see exactly what changed. You have your suspect.

The forensics tax compounds at scale. A team running 200 regression tests per day will see regressions frequently — that is the point of regression testing. If every regression requires three days of forensics, the team drowns. If every regression is immediately traceable, the team fixes and moves on. The difference between these two states is not talent or effort. It is infrastructure. Traceability is infrastructure. Without it, your regression system detects problems but cannot solve them. Detection without resolution is just expensive anxiety.

## The Six-Dimensional Version Space

An AI system is not a single artifact. It is a composition of six version-controlled components, each of which can change independently. The **prompt version** controls how you ask the model to perform the task. The **model version** controls the underlying capability and behavior. The **data version** controls the examples, ground truth, or fine-tuning corpus. The **retrieval index version** controls what context the model can access. The **config version** controls temperature, token limits, retries, fallback logic. The **code version** controls orchestration, validation, formatting. Each dimension can introduce a regression. Each dimension must be tracked.

Most teams track code version through git. Fewer teams track prompt version. Even fewer track model version explicitly — they assume the model is "whatever is currently deployed" and lose the ability to bisect when behavior changes. Almost no teams track retrieval index version, treating the index as mutable infrastructure rather than a versioned artifact. Config version often lives in environment variables or config files that change without audits. Data version is sometimes tracked, sometimes not, depending on whether the team treats datasets as code or as disposable inputs.

The result is a system where you can reproduce the code but not the behavior. You check out the commit from two weeks ago. You run the tests. The results do not match the results from two weeks ago. Why? Because the model changed. Or the prompt changed. Or the retrieval index changed. Or the config changed. You have one-sixth of the version space under control. That is not enough. Reproducibility requires six-sixths.

The traceability imperative says: every test result must record all six dimensions. When a test runs, you log the git commit hash, the prompt version identifier, the model name and version, the dataset hash, the retrieval index timestamp, the config hash. You store these six values alongside the test result. If the test passes, you have a known-good fingerprint. If the test fails, you have a known-bad fingerprint. You compare the two. You see exactly what changed. You know where to look.

## The Prompt Versioning Problem

Prompts are code. But most teams do not treat them like code. They live in Notion documents, Slack threads, or Python string literals that change without commit messages. A product manager tweaks the phrasing. An engineer adjusts the few-shot examples. A domain expert clarifies an instruction. Each change is small. Each change is untracked. Over three months, the prompt diverges from the version that passed your initial evaluation. A regression appears. You try to debug. You realize you do not have a record of what the prompt looked like when the test last passed. You have the current prompt. You have the memory that it used to work better. You do not have the artifact.

Prompt versioning requires treating prompts as first-class version-controlled artifacts. Store prompts in git. Use semantic versioning or timestamp-based versioning. Tag prompts when they pass evaluation. Reference prompt versions explicitly in test configurations. When you run a regression test, specify which prompt version to use. When you log a test result, record which prompt version was used. If you change the prompt, increment the version, run the regression suite, compare the results against the previous version. If quality drops, you know the prompt change caused it. If quality improves, you have evidence to deploy.

Some teams version prompts by embedding version numbers in filenames: prompt_v1.txt, prompt_v2.txt. Others use git tags: prompt-2026-02-01. Others use hash-based identification: the test logs the SHA-256 hash of the prompt text. The mechanism matters less than the discipline. The rule is: if you cannot tell which prompt was used to produce a test result, you do not have prompt traceability. Regressions caused by prompt changes will be invisible until they reach production.

## The Model Version Cliff

Model providers update models constantly. OpenAI updates GPT-5. Anthropic updates Claude Opus 4.5. Google updates Gemini 3 Pro. Sometimes they announce the update. Sometimes they do not. Sometimes the update is labeled as a new version. Sometimes it is a silent behind-the-scenes tweak to the same version identifier. Your system calls the model API with the same model name you used last month. The behavior has changed. Your regression tests fail. You do not know why. You assume you broke something in your prompt or code. You spend three days debugging. Then you discover the model provider deployed an update. You did nothing wrong. The ground shifted under you.

Traceability requires pinning model versions explicitly and logging them with every test result. Use versioned model identifiers when available: gpt-5-turbo-2026-01-15 instead of gpt-5-turbo. Log the model name and version with every test result. When the provider updates the model, treat it as a new version and re-run your regression suite before adopting it. If you depend on a model that does not offer version pinning, log the date and time of every test run as a proxy for model version — you cannot reproduce the exact model, but you can correlate behavior changes with known provider updates.

Some teams cache model responses for regression testing. They run the test once against the live model, capture the response, and store it. Future regression tests replay the cached response instead of calling the model again. This decouples regression testing from model availability and model version drift. The tradeoff is that your regression suite no longer detects model-level changes — it only detects changes in your prompt, code, and config. That is often acceptable. If your goal is to ensure your changes did not break anything, cached responses provide perfect traceability. If your goal is to detect when the model provider changes behavior, you need live calls with version logging.

## The Retrieval Index Time Bomb

Retrieval-augmented generation systems depend on vector indexes that change over time. You add documents. You remove outdated content. You re-chunk existing documents with a new strategy. You re-embed with a new embedding model. You adjust retrieval parameters. Every change affects which context reaches the model. Every change can introduce a regression. But most teams treat the retrieval index as mutable infrastructure, not as a versioned artifact. The index has no version number. Tests run against whatever index is currently deployed. When a regression appears, you cannot tell whether it was caused by a prompt change, a model change, or an index change.

Retrieval index versioning requires treating indexes as immutable snapshots. When you build or update the index, tag it with a version identifier or timestamp. Store the index as a distinct artifact. Reference specific index versions in test configurations. When you run a regression test, specify which index version to use. When you log a test result, record which index version was used. If you update the index, run the regression suite against both the old version and the new version. Compare the results. If retrieval quality drops, you know the index change caused it.

Some teams version indexes by timestamp: index-2026-02-01. Others use semantic versions tied to the document corpus: index-v3-post-legal-update. Others use hash-based identification: the test logs a hash of the retrieved documents for each query. The last approach provides the finest-grain traceability — if the same query retrieves different documents across two test runs, you know the index changed, even if the version identifier is the same. Hash-based traceability catches silent index drift.

## The Config Drift Hazard

Configuration parameters live in environment variables, YAML files, or database tables. They control temperature, token limits, retry logic, timeout thresholds, fallback behavior. They change frequently. They change without pull requests. An engineer bumps the temperature from 0.7 to 0.9 to experiment with more creative output. A product manager lowers the token limit to reduce cost. An SRE increases the retry count to improve reliability. Each change is reasonable in isolation. None of them trigger a code review. The regression suite runs against the new config. A test fails. You assume the failure is caused by a recent code or prompt change. You spend two days debugging. Then you discover someone changed the temperature three days ago. The failure is not a bug. It is a config change. You did not know to look there because config changes are not tracked like code changes.

Config versioning requires treating configuration as code. Store config files in git. Require pull requests for config changes. Tag config versions when they pass evaluation. Log the config version or a hash of the config contents with every test result. When a regression appears, compare the config version between the failing run and the last passing run. If they differ, you know config drift is a suspect.

Some teams use config hashes: they serialize the entire config as JSON, hash it, and log the hash with every test result. This provides perfect traceability without requiring explicit version numbers. If two test runs produce the same config hash, they used identical config. If the hashes differ, something changed. You compare the two configs and see exactly what. Other teams use semantic versioning for config: they bump the version number every time a config file changes and track the version alongside test results. The mechanism matters less than the principle: if your test results do not record which config was used, you cannot trace regressions caused by config changes.

## The Code Version Anchor

Code version is the one dimension most teams already track. Every test result can reference a git commit hash. If a test fails, you check out that commit and reproduce the failure. If the failure only appears on certain commits, you bisect to find the breaking change. This is standard software engineering discipline. It works. The failure mode is not missing code version tracking — it is assuming code version is the only version that matters.

A test fails. You check the git log. You see three commits since the last passing test. You bisect. You find the breaking commit. You examine the diff. The diff is a one-line change to a comment. The change is obviously harmless. You are confused. You bisect further. Every commit in the range passes when tested in isolation. The failure only appears when running the full suite against the current deployed system. You realize the regression is not in the code. It is in the model or the config or the retrieval index. But you do not have version tracking for those dimensions. You are back to archaeology.

The code version anchor is necessary but not sufficient. You need it. You also need the other five dimensions. Traceability is six-dimensional. One dimension gives you one-sixth of the picture. That is better than zero. It is not enough to fix regressions reliably.

## The Traceability Stack in Practice

A complete traceability stack logs six values with every test result: the git commit hash, the prompt version identifier, the model name and version, the dataset hash, the retrieval index version, the config hash. These six values are stored in your test result database or log system alongside the test name, pass/fail status, metrics, and timestamp. When a test fails, you query for the last passing result for that test, extract its six-dimensional fingerprint, and compare it to the current fingerprint. You see which dimensions changed. You have your suspect list.

If only the prompt version changed, you diff the prompts. If only the model version changed, you investigate the model provider's changelog. If only the config hash changed, you diff the configs. If only the retrieval index version changed, you compare retrieved documents between versions. If only the dataset hash changed, you diff the datasets. If only the code version changed, you bisect the commits. If multiple dimensions changed, you test them one at a time in isolation until you find the culprit.

This workflow requires infrastructure. You need a test harness that captures version metadata. You need a storage system that associates metadata with results. You need tooling that compares fingerprints and diffs artifacts. Most teams build this incrementally. They start with code version tracking. They add prompt versioning when prompt drift causes a painful debug session. They add model version logging after a silent provider update breaks production. They add config hashing after a config change causes a false regression alarm. They add retrieval index versioning when an index rebuild degrades quality. They add dataset hashing when they realize they cannot reproduce last month's evaluation. Eventually, they have all six dimensions. The team that builds this early avoids years of painful forensics.

## When Traceability Becomes Reproducibility

Traceability tells you what changed. **Reproducibility** tells you whether you can recreate the exact same result by using the exact same versions. Traceability is a prerequisite for reproducibility. You cannot reproduce a result if you do not know which versions produced it. But logging versions is not enough. You also need to be able to retrieve and deploy those versions.

If your traceability system logs "prompt version 47" but you cannot access prompt version 47 anymore because it was deleted or overwritten, traceability has failed. If your system logs "retrieval index 2025-12-01" but that index was garbage collected to save storage costs, traceability has failed. If your system logs "model gpt-5-turbo-2026-01-15" but the provider deprecated that version and you can no longer call it, traceability has failed.

Reproducibility requires archiving. Archive old prompts. Archive old indexes. Archive old datasets. Archive old configs. Archive old model responses if you depend on providers who do not offer version pinning. Archiving costs storage. The alternative costs engineering time. A team that spends eleven days on forensics once could have funded five years of archival storage. Storage is cheap. Debugging is expensive. Traceability without archiving is a pointer to nothing. Traceability with archiving is a time machine. When a regression appears, you travel back to the last known good state, compare, and see exactly what broke.

The traceability imperative is not optional. It is the difference between a regression system that detects problems and one that solves them. Detection is the beginning. Resolution is the goal. Traceability is the bridge between the two.

The next question is whether your tests produce the same result every time you run them with the same versions — which requires determinism controls, reproducibility engineering, and an understanding of where randomness enters your system.
