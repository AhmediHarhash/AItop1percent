# 2.6 — Golden Set Versioning and Evolution

In August 2025, a financial services company updated their golden set to reflect new product features. The team deleted outdated examples that tested discontinued functionality and added new examples that tested recently launched capabilities. They overwrote the old golden set file with the new one. Three weeks later, an incident post-mortem tried to determine whether a production failure could have been caught earlier by comparing current model performance to performance from six months prior. The comparison was impossible. The old golden set no longer existed. The team had no way to know whether the model had regressed on scenarios that were important six months ago, or whether those scenarios simply stopped being tested.

The team had treated their golden set as a living document that should always reflect current product state. That instinct was correct. But they implemented it destructively. They deleted history instead of versioning it. The result was a loss of longitudinal comparability. They could no longer measure whether their model was getting better or worse over time on consistent scenarios because the scenarios kept changing. The golden set evolved, but the evolution was undocumented and irreversible.

Golden sets must be versioned. Every change creates a new version. Old versions are archived, not deleted. Version history allows you to compare model performance across time, reconstruct past release decisions, and understand how your product scope evolved. Versioning is not a luxury for teams with mature processes. It is a requirement for any team that wants to operate a regression testing system that compounds value over time instead of constantly resetting to zero.

## Why Golden Sets Must Be Versioned

The primary reason to version golden sets is longitudinal comparability. You want to know whether your system is improving or degrading over time. That question cannot be answered if the test set keeps changing. If you evaluate your January release against Version 1 of the golden set and your July release against Version 4, you cannot directly compare the results. The difference in scores might reflect actual model improvement, or it might reflect the fact that Version 4 contains easier examples, or it might reflect a shift in product scope that made some old examples obsolete.

Versioning allows you to run both versions of the golden set against both releases. You evaluate your July release against both Version 4 (current product scope) and Version 1 (January scope). You discover that on Version 1, the July model scores higher than the January model. That is evidence of improvement on a consistent test set. You also discover that Version 4 includes examples that the January model could not have passed because they test features that did not exist yet. That is evidence of product scope expansion. Both insights are valuable. Neither is possible without versioning.

Versioning also supports incident investigation. When a production failure occurs, you want to know whether your regression tests could have caught it. If the failure involves functionality that used to work, you need to check whether the relevant golden examples were removed from the current version. If they were, you need to restore them or add new examples that cover the failure mode. If they were not removed, you need to understand why the existing examples did not catch the regression. Both investigations require access to historical versions of the golden set.

Versioning enables rollback. If you ship a release that causes unexpected regressions, you need to compare it to previous releases to understand what changed. But if you simultaneously updated the golden set to match the new release, you can no longer run a clean comparison. The test set and the model changed together. You cannot isolate which changes caused the regression. Versioning the golden set separately from model releases preserves your ability to bisect failures.

## When to Version the Golden Set

You create a new version of the golden set whenever the product scope changes, the policy standards change, or the market context changes. These events redefine what your system is responsible for, how it should behave, or what the expected outputs are. When definitions change, the test set must change. But the old version remains available for comparison.

Product scope changes when you launch new features, deprecate old features, or expand to new domains. If you launch a multilingual capability, your golden set must add examples in the new languages. If you deprecate support for a specific workflow, your golden set can remove examples that test that workflow — but those examples are archived in the previous version, not deleted. If you expand from healthcare to legal, your golden set must add legal examples. Each of these changes justifies a new version.

Policy changes when your product guidelines, content standards, or compliance requirements evolve. If your trust and safety team updates the content policy to prohibit a category of output that was previously allowed, your golden set must update the ground truth for affected examples. Responses that were labeled correct under the old policy might now be labeled incorrect. That change justifies a new version. The old version reflects old policy. The new version reflects new policy. Both are preserved.

Market changes when external context shifts in ways that affect expected behavior. If a regulatory change imposes new requirements on your product, your golden set must add examples that test compliance with the new regulation. If a competitor launches a feature that becomes table stakes, your golden set might add examples that test equivalent functionality. If user expectations shift based on broader AI industry trends, your golden set might update the quality bar for acceptable responses. Each of these shifts justifies a new version.

You also version the golden set when you discover that a significant portion of existing examples have incorrect ground truth. If an audit reveals that 15 percent of your labels are wrong, you correct them and create a new version. The old version remains archived as a historical artifact, but the new version becomes the active regression test. Future comparisons use the corrected version. Historical comparisons can still use the old version, but with the caveat that some labels were incorrect.

## Version Control Systems for Golden Sets

Golden sets are code. They belong in version control. Most teams use the same system they use for application code — Git, with the golden set stored in a dedicated repository or a subdirectory of the main application repo. Each version is a commit. Semantic versioning applies: major version increments when the golden set changes in a way that breaks backward compatibility (substantial changes in scope or policy), minor version increments when new examples are added without removing old ones, patch version increments when ground truth labels are corrected or clarified.

The directory structure separates examples, ground truth, and metadata. Examples are stored as structured files — JSON, JSONL, or CSV — with each row representing one golden example: input, expected output, metadata tags. Ground truth is stored separately to allow updates without modifying the examples themselves. Metadata includes version number, creation date, change log, owning team, and policy or product version the golden set corresponds to.

The commit history tells the story of how your product evolved. A diff between Version 2 and Version 3 shows exactly which examples were added, removed, or modified. The commit message explains why: "Added 12 examples for new multilingual support" or "Updated ground truth for 8 examples to reflect revised content policy." The version control log becomes a historical record of product and policy changes, queryable and auditable.

Branching strategies vary. Some teams maintain a single mainline branch for the active golden set and use tags to mark versions. Others use long-lived branches for different product environments — a production branch, a staging branch, and a development branch. The production branch contains the golden set used for release gates. The development branch contains proposed additions that have not yet been validated. When new examples are ready, they are merged into production and the version increments.

## Archival Without Deletion

Never delete old versions. Archive them. An archived version is no longer active — it is not used for release gates or live regression testing — but it remains accessible for historical analysis, incident investigation, and longitudinal studies. Archival is not the same as deletion. Deletion destroys information. Archival preserves it while marking it as historical.

In practice, archival means moving old versions into a dedicated subdirectory or repository, tagging them with version numbers and timestamps, and documenting why they were superseded. A team might have an active golden set at Version 8 and archived versions 1 through 7. Each archived version includes metadata explaining what changed in the next version: "Version 2 added compliance examples for GDPR. Version 3 removed examples for deprecated features. Version 4 corrected ground truth errors identified in Q3 audit."

Archived versions are read-only. They are not updated or modified. If an error is discovered in an archived version, it is noted in the metadata but not corrected. The archived version is a historical snapshot. It reflects the state of the golden set at a specific point in time, including any errors that existed then. Correcting those errors would create revisionist history. The errors are acknowledged, but the archive remains untouched.

Archival enables deep retrospectives. When your product is two years old and your golden set is at Version 14, you can trace the entire evolution. You can see which examples were added when, which were removed, and which had their ground truth updated. You can re-run old model checkpoints against old golden set versions to verify that the regression tests would have caught issues that occurred at the time. You can measure how the difficulty of the golden set has changed as your product matured. None of this is possible if old versions were deleted.

## Managing Scope Drift Between Versions

As your product evolves, some golden examples become obsolete. You launched a feature, tested it for six months, and then deprecated it. The golden examples that tested that feature no longer correspond to live product behavior. Do you keep them or remove them? The answer depends on why they are obsolete.

If the feature was removed because it was rarely used, remove the examples from the active golden set but archive them. They are no longer relevant to current product behavior. But they remain part of your historical record. If you ever re-introduce similar functionality, the archived examples can be restored. If an incident investigation needs to understand how the deprecated feature behaved, the archived examples provide the test cases.

If the feature was removed because it was problematic — it caused compliance issues, user harm, or operational cost — keep the examples in the active golden set but update the expected behavior to refusal or error. The feature may be gone, but the risk is not. Users might still try to trigger the old behavior. Your model must not comply. The golden examples now test that the model correctly refuses requests for deprecated functionality instead of attempting to execute them.

If the examples were based on incorrect assumptions — your team misunderstood user needs or policy requirements when the examples were created — remove them from the active golden set, archive them, and document why they were incorrect. This prevents future teams from reintroducing the same flawed assumptions. The archived version includes a note: "Examples 45 through 52 removed in Version 5 because they tested a feature that was based on a misunderstanding of HIPAA requirements."

Scope drift is normal. Products evolve. But the golden set must evolve in a way that maintains continuity and traceability. Each version builds on the previous one. Additions and removals are documented. The version history is a living record of what your team learned about what matters.

## Golden Set Evolution as a Signal of Product Maturity

The rate of change in your golden set reflects the stability of your product. In early stages, the golden set changes frequently. You are still discovering what matters. You add new examples weekly. You remove examples that turned out to be unimportant. You update ground truth as your understanding of correct behavior sharpens. Frequent versioning is expected. You might increment from Version 1 to Version 10 in six months.

As the product matures, the golden set stabilizes. You still add examples, but at a slower pace. Most new examples come from production failures or edge cases discovered through scale. You rarely remove examples because the core set has been validated over time. You update ground truth occasionally, usually due to policy changes rather than annotation errors. Versioning slows. You might stay at Version 12 for six months before incrementing to Version 13.

This stabilization is a positive signal. It means your product scope is well-defined, your policies are clear, and your understanding of what matters is mature. The golden set no longer thrashes. It evolves incrementally. Each new version represents genuine new knowledge — a new edge case, a new policy requirement, a new product capability — not a correction of past mistakes.

Teams can track this signal explicitly. Measure the churn rate: what percentage of the golden set changes between versions. High churn early is normal. High churn late is a red flag. It suggests either product instability, policy instability, or poor initial understanding of the problem space. Low churn late is expected. It suggests maturity. Tracking this metric over time gives you a quantitative view of how well your team understands the problem you are solving.

## Versioning Discipline as Organizational Memory

Golden set versioning is not just a technical practice. It is an organizational memory system. It captures what your team has learned about what matters. It documents how your product and policies evolved. It preserves the test cases that once caught critical failures even if those test cases are no longer active. It allows new team members to understand the history of decisions and trade-offs that shaped the current product.

Without versioning, this memory is lost. Teams forget why certain examples exist. They remove examples that seem unimportant without realizing those examples were added after costly production incidents. They repeat past mistakes because there is no record of what was tried before. Organizational memory evaporates as team composition changes.

Versioning preserves memory. A new engineer joins the team, examines the golden set, and wonders why a specific example exists. They check the version history. They find the commit that added the example. The commit message references an incident post-mortem. The engineer reads the post-mortem and understands the context. The example is no longer mysterious. It is a scar from a specific failure that the organization paid to learn from. The version history makes that context accessible.

This is why versioning discipline matters as much as versioning tooling. It is not enough to store golden sets in Git. You must also write clear commit messages, document why changes were made, and link versions to the product and policy context that motivated them. The version control system is the infrastructure. The discipline is what makes the infrastructure useful.

Your golden set is not a static artifact. It is a living system that evolves with your product. Versioning ensures that evolution is documented, reversible, and comparable. It transforms a regression test suite into a historical record of what your team has learned about quality. Treat versioning as a core operational discipline, not an afterthought. Your ability to measure progress, investigate incidents, and maintain longitudinal comparability depends on it.

The next challenge is determining when golden examples should be retired or replaced — because even well-maintained golden sets accumulate examples that no longer serve their original purpose.

