# 3.1 â€” The Five Test Types for AI Systems

Regression suites for AI systems require five distinct test types. Not three. Not "whatever we have." Five. Each type catches failures the others miss. Each type measures a different dimension of system behavior. Teams that skip test types ship regressions in those dimensions. Teams that conflate test types get confused results because they are measuring multiple things simultaneously without realizing it.

The five types are **behavioral tests**, **consistency tests**, **safety tests**, **performance tests**, and **regression tests**. Behavioral tests verify that outputs have expected characteristics like format, tone, and structure. Consistency tests verify that similar inputs produce similar outputs. Safety tests verify resistance to adversarial inputs. Performance tests verify that latency, throughput, and cost meet requirements. Regression tests verify that quality has not degraded relative to a baseline. Every production AI system needs all five. Miss one and you are flying blind in that dimension.

## Why Five Types Instead of One

Most teams start with a single test type: correctness. They collect examples, annotate ground truth, and measure whether the model produces the right answer. This catches accuracy regressions. It does not catch behavioral regressions like a model that starts adding unwanted disclaimers. It does not catch consistency regressions like a model that gives different answers to paraphrased questions. It does not catch safety regressions like a model that becomes vulnerable to prompt injection. It does not catch performance regressions like latency increases or cost spikes. Correctness testing is necessary. It is not sufficient.

The reason you need five types is that AI systems have five failure modes that manifest independently. A model can maintain accuracy while losing consistency. A model can maintain accuracy and consistency while becoming vulnerable to adversarial attacks. A model can maintain accuracy, consistency, and safety while becoming three times slower. A model can pass all quality checks while failing to follow formatting requirements. These failures are orthogonal. You cannot detect them all by measuring one dimension.

Consider a fintech company that deployed a new model in early 2025. The model passed all correctness tests. Accuracy on the golden set was 94 percent, up from 91 percent baseline. The team shipped it. Within two days, customer support received dozens of complaints. The model was correct, but it had started inserting compliance disclaimers into every response. Users who asked "What is my account balance?" received responses like "Your account balance is fourteen thousand three hundred dollars. This information is provided for informational purposes only and does not constitute financial advice. Please consult your account statements for official records."

The disclaimers were factually accurate. They were contextually absurd. Behavioral testing would have caught this regression. The team had only tested correctness. They had no tests for tone, formatting, or user experience. They rolled back the model, added behavioral tests to the suite, and discovered that the new model also exhibited a consistency regression on paraphrased queries and a latency regression on complex requests. The accuracy improvement hid three separate regressions in untested dimensions. All five test types are required before deploy. There is no shortcut.

## Test Type 1: Behavioral Tests

Behavioral tests verify that outputs have expected characteristics. These characteristics include format adherence, tone, instruction following, refusal behavior, citation inclusion, tool use correctness, and structural properties. Behavioral tests do not measure whether the answer is correct. They measure whether the answer has the right shape and style.

A customer support system might require that all responses are polite, do not use technical jargon, and end with an offer to escalate if the user is unsatisfied. A legal research system might require that all claims are supported by citations and that speculation is clearly labeled. A coding assistant might require that all code is syntactically valid and that explanations are provided for non-obvious logic. These requirements are behavioral. They apply regardless of whether the underlying answer is correct.

Behavioral tests catch regressions that accuracy metrics miss. A model that starts hallucinating citations passes correctness tests if the underlying answers are correct. Behavioral tests catch the hallucinated citations. A model that starts refusing safe requests passes accuracy tests on the examples it still answers. Behavioral tests catch the refusal rate increase. A model that starts generating malformed JSON passes accuracy tests if the content is correct. Behavioral tests catch the formatting failure.

## Test Type 2: Consistency Tests

Consistency tests verify that similar inputs produce similar outputs. If a user asks "What is the capital of France?" and then asks "Can you tell me France's capital city?" the model should give the same answer. If the model says "Paris" to the first question and "Lyon" to the second, you have a consistency failure. Consistency matters because users will rephrase questions, and they expect stable answers.

Consistency failures are common in generative models. Small changes in phrasing can trigger different reasoning paths, different retrieved documents, or different sampled tokens. Prompt injection attacks exploit consistency failures by finding phrasings that bypass safety guardrails. User trust erodes when models give contradictory answers to equivalent questions. Consistency testing detects these failures by generating paraphrases and measuring output similarity.

Consistency tests also detect temperature-related instability. A model with high temperature might give wildly different answers to the same question on different runs. A model with quantization errors might produce different outputs depending on which inference server handled the request. Consistency testing catches these failure modes by running the same inputs multiple times and measuring variance. High variance signals a problem even if each individual output is correct.

## Test Type 3: Safety Tests

Safety tests verify resistance to adversarial inputs. These include prompt injection attempts, jailbreak attempts, data exfiltration attempts, malicious tool use, unsafe content generation, and bias exploitation. Safety tests assume that users will actively try to break your system. If your test suite only includes benign inputs, you will not discover vulnerabilities until production users exploit them.

A content moderation system might pass all correctness tests while being trivially vulnerable to base64-encoded hate speech. A customer service bot might correctly answer standard questions while being vulnerable to prompt injection that extracts PII from context. A coding assistant might generate correct code while being vulnerable to requests that generate malware. Safety tests catch these vulnerabilities by including adversarial examples in the test suite.

Safety testing requires adversarial thinking. You design inputs specifically intended to cause model failure. You test whether role-playing scenarios bypass content filters. You test whether multi-turn conversations can be used to extract training data. You test whether tool-using agents can be tricked into executing harmful actions. These tests are not intuitive. They require red-teaming expertise. But they are non-negotiable for any production system handling sensitive data or high-stakes decisions.

## Test Type 4: Performance Tests

Performance tests verify that latency, throughput, and cost meet requirements. A model that maintains quality while taking twice as long to respond is a regression. A model that maintains quality while doubling inference cost is a regression. A model that maintains quality but reduces throughput by half is a regression. Performance is not a secondary concern. It is a deployment blocker.

Performance testing requires load simulation. You cannot measure throughput by running examples one at a time. You need to simulate concurrent requests at production scale. You measure P50, P95, and P99 latency under load. You measure cost per request including all infrastructure overhead. You measure throughput degradation as load increases. You measure cold start latency if your deployment uses auto-scaling.

Performance regressions are easy to miss because they do not show up in accuracy metrics. A team ships a model that passes all quality tests. Production latency doubles. Users abandon requests before they complete. The model is accurate on the requests that finish, but the user experience is destroyed. Performance tests catch this before deploy by measuring latency in a staging environment under production-like load.

## Test Type 5: Regression Tests

Regression tests verify that quality has not degraded relative to a baseline. You run the same golden set through both baseline and candidate models. You measure accuracy, behavioral compliance, consistency, safety, and performance for both. You compare the results using statistical tests. If the candidate is significantly worse on any dimension, you block the deploy.

Regression tests are comparative. They do not measure absolute quality. They measure relative quality. A candidate model with 89 percent accuracy is acceptable if the baseline had 87 percent accuracy. A candidate model with 92 percent accuracy is unacceptable if the baseline had 94 percent accuracy. The threshold is not a fixed number. It is a comparison to what you currently have.

Regression tests require statistical rigor because AI systems are non-deterministic. You cannot run each example once through each model and compare outputs. Sampling variance means the observed difference might not reflect a real difference. You need multiple runs, significance tests, and effect size estimates. Without statistical discipline, you will block good deploys due to noise and approve bad deploys due to luck. Regression testing is where statistical rigor matters most.

## Running All Five Types Together

The five test types are not run sequentially. They are run in parallel on every candidate model before deploy. Behavioral tests run on the same golden set as regression tests. Consistency tests run on paraphrased versions of golden set examples. Safety tests run on adversarial examples maintained separately. Performance tests run under load in a staging environment. Regression tests compare all metrics between baseline and candidate.

The result is a test report that shows performance across all five dimensions. If the candidate passes all five test types, you ship it. If it fails any test type, you investigate. If it fails behavioral tests, you diagnose why formatting or tone changed. If it fails consistency tests, you check temperature settings and retrieval variance. If it fails safety tests, you audit for new vulnerabilities. If it fails performance tests, you profile latency and optimize. If it fails regression tests, you compare metrics and decide whether the degradation is acceptable given improvements in other areas.

This multi-dimensional view is essential because improvements on one dimension often cause regressions on another. A model fine-tuned for accuracy might lose consistency. A model optimized for latency might lose accuracy. A model hardened for safety might refuse more benign requests. The five test types surface these trade-offs explicitly. You make informed decisions instead of discovering problems in production.

## The Discipline of Comprehensive Testing

Most teams start with one or two test types and add the rest only after painful production incidents. A model ships with no behavioral testing. Users complain about formatting. The team adds behavioral tests. A model ships with no consistency testing. Users notice contradictory answers. The team adds consistency tests. A model ships with no safety testing. An adversarial user exploits a vulnerability. The team adds safety tests. Each incident teaches the same lesson: you needed all five test types from the beginning.

The teams that build comprehensive test suites from the start avoid these incidents. They catch regressions before users see them. They make deployment decisions with full information about trade-offs. They build trust with stakeholders by demonstrating that quality is verified across all dimensions. The upfront investment in test infrastructure pays for itself the first time it blocks a regression that would have caused a production incident.

All five test types, every deploy, with statistical rigor. That is the standard for professional AI engineering. The next subchapter covers behavioral tests in depth: how to define behavioral requirements, how to measure compliance, and how to catch regressions in format, tone, and structure.
