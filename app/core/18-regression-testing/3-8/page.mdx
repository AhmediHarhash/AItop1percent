# 3.8 — Statistical Significance in Regression Detection

In late 2025, a B2B SaaS company blocked a model deployment because their regression suite showed a 2 percent drop in task accuracy — from 91.4 percent to 89.5 percent across 200 test cases. Engineering spent three days investigating. They re-ran the tests. They audited the training data. They checked for infrastructure drift. They found nothing. A week later, they ran the exact same test suite on the exact same model with a different random seed. Accuracy came back at 91.2 percent. The regression was noise. The block was a false alarm. The company had no statistical rigor in their regression decisions and wasted a full engineering week chasing randomness.

Statistical significance is not optional in AI regression testing. It is the difference between evidence and guessing. When you compare two model versions and see a metric change, you must answer one question with mathematical certainty: is this difference real, or could it have happened by chance? If you cannot answer that question with a p-value and a confidence interval, you are not doing regression testing. You are reading tea leaves.

## Why Eyeballing Differences Does Not Work

Human intuition is terrible at distinguishing signal from noise. A 3 percent accuracy drop feels significant. A 0.5 percent drop feels insignificant. Neither feeling is evidence. The actual significance depends on sample size, variance, and the distribution of the metric. A 0.5 percent drop across 10,000 test cases with low variance is highly significant. A 3 percent drop across 50 test cases with high variance might be pure noise.

The teams that eyeball metrics make two mistakes. First, they block deployments based on differences that are statistically indistinguishable from random variation. This creates a high false alarm rate. Engineering loses trust in the regression suite. They start overriding the gates. Once the gates lose credibility, they lose power. Second, they ship changes with statistically significant regressions because the numbers looked close enough. This creates a high miss rate. Real degradation reaches production. Users notice. Trust erodes.

The correct approach is mechanical: compute the p-value for every metric comparison. If the p-value is below your threshold — typically 0.05 or 0.01 — the difference is statistically significant. If it is above, the difference is noise. You block or ship based on the p-value, not on intuition.

A fintech company implemented this rule in mid-2025. Every model update ran through a regression suite of 1,500 test cases. For each metric — accuracy, precision, recall, toxicity score, latency — they computed a two-sample t-test comparing the new model to the baseline. If any metric showed a statistically significant degradation at p less than 0.01, the deployment was blocked. If metrics showed improvement or no significant change, it shipped. Over six months, this rule blocked four regressions that would have degraded production and cleared eight changes that were initially flagged by engineers eyeballing the numbers but turned out to be noise. The false alarm rate dropped from 40 percent to under 5 percent.

## The P-Value Threshold

The **p-value** is the probability that the observed difference could have occurred by random chance if the two models were actually identical. A p-value of 0.03 means there is a 3 percent chance the difference is noise. A p-value of 0.001 means there is a 0.1 percent chance. Lower is stronger evidence.

Most regression testing uses a threshold of 0.05. If p is less than 0.05, you reject the null hypothesis — you conclude the models are different. If p is greater than 0.05, you fail to reject the null — you treat the difference as noise. Some teams use 0.01 for more conservative decisions. The choice depends on your tolerance for false alarms.

A 0.05 threshold means you accept a 5 percent false positive rate. One in twenty comparisons will show a statistically significant difference even when there is no real regression. If you run 100 metric comparisons in your regression suite, expect five false alarms. This is called the **multiple comparisons problem** and it is dangerous. If you run enough tests, you will always find something that looks significant by chance.

The correction is to adjust your threshold when running many tests. The Bonferroni correction divides your p-value threshold by the number of comparisons. If you run 20 tests and want an overall false positive rate of 0.05, you use a per-test threshold of 0.0025. This is conservative and reduces false alarms at the cost of reduced sensitivity. A less conservative alternative is the Benjamini-Hochberg procedure, which controls the false discovery rate instead of the family-wise error rate.

A content moderation platform ran 40 different metrics in their regression suite — precision and recall for ten different harm categories. Initially they used a 0.05 threshold per metric. They saw frequent false alarms where one random metric out of 40 would flag. After applying Bonferroni correction — threshold of 0.00125 per metric — false alarms dropped by 80 percent and real regressions were still detected.

## Type I and Type II Errors

There are two ways to be wrong in regression testing. **Type I error**: you block a deployment because you think there is a regression when there is not. False alarm. Wasted engineering time. Delayed features. **Type II error**: you ship a deployment because you think there is no regression when there actually is. Missed detection. Production degradation. User complaints.

Your p-value threshold controls the trade-off. A strict threshold like 0.01 reduces Type I errors — fewer false alarms — but increases Type II errors — more missed regressions. A loose threshold like 0.10 reduces Type II errors but increases false alarms. There is no free lunch. You choose which error is more costly.

For high-stakes systems — medical, financial, safety-critical — Type II errors are catastrophic. Missing a real regression can cause harm. Use a loose threshold like 0.05 or 0.10 to maximize sensitivity. Accept more false alarms in exchange for catching every real issue. For lower-stakes systems where engineering velocity matters, Type I errors are costly. A false alarm blocks a feature launch and wastes days. Use a strict threshold like 0.01 to minimize noise.

A healthcare diagnostics AI used a 0.10 threshold. They tolerated frequent false alarms because missing a regression that caused misdiagnosis was unacceptable. A consumer chatbot for entertainment used a 0.01 threshold. They tolerated occasional missed regressions because the consequences were low and engineering time was expensive. Both teams made the right choice for their domain.

The mistake is using the same threshold for everything. Not all metrics have the same cost of error. A regression in toxicity filtering is higher stakes than a regression in response conciseness. You can and should use different thresholds for different metrics within the same suite. Safety metrics get loose thresholds. Quality-of-life metrics get strict ones.

## Confidence Intervals for Metric Comparisons

A p-value tells you whether a difference is statistically significant. A **confidence interval** tells you how big the difference is and how certain you are. Both matter.

A confidence interval gives you a range: "We are 95 percent confident that the new model's accuracy is between 89.2 percent and 91.8 percent." If the baseline model's accuracy is 90.0 percent and that value falls inside the interval, the difference is not significant. If the baseline falls outside the interval, the difference is real.

Confidence intervals also tell you whether a significant difference is meaningful. Suppose your new model's accuracy is statistically significantly higher than the baseline — p equals 0.001. But the confidence interval is 90.05 percent to 90.15 percent, and the baseline is 90.00 percent. The improvement is real but tiny. The difference is detectable because your sample size is huge, but the practical impact is negligible. This is where statistical significance diverges from practical significance.

The rule: check both the p-value and the confidence interval. If the p-value is significant but the confidence interval is narrow and close to the baseline, the change is real but unimportant. If the p-value is not significant but the confidence interval is wide, you lack data — run more test cases and recompute. If the p-value is significant and the confidence interval shows a large shift, you have a real regression that matters.

A search ranking model showed a statistically significant improvement in relevance score: p equals 0.003. The confidence interval was 0.811 to 0.814, compared to a baseline of 0.810. The improvement was 0.002 — two-tenths of one percent. Engineering decided not to deploy the change because the improvement was too small to justify the operational risk of a model swap. The statistical test told them the difference was real. The confidence interval told them it did not matter.

## Effect Size and Practical Significance

**Effect size** measures how big the difference is in standard deviation units. It answers the question: is this difference large enough to care about? A common effect size metric is Cohen's d, calculated as the difference in means divided by the pooled standard deviation. A d of 0.2 is considered small. A d of 0.5 is medium. A d of 0.8 is large.

You can have a statistically significant result with a tiny effect size if your sample is large enough. This happens constantly in large-scale AI testing. You run 10,000 test cases. The new model's accuracy is 90.02 percent versus 90.00 percent for the baseline. The p-value is 0.001. Cohen's d is 0.05. The difference is statistically real but practically meaningless.

The solution is to set minimum effect size thresholds. Only flag a regression if both the p-value is below 0.05 AND the effect size is above a meaningful threshold. For accuracy metrics, a reasonable threshold might be d greater than 0.2 — at least a small effect. For latency, it might be d greater than 0.3 — latency differences need to be noticeable to matter. For toxicity, you might require no minimum effect size — any statistically significant increase in toxicity is a blocker, no matter how small.

This dual gating — p-value plus effect size — prevents both false alarms and meaningless alerts. A change that moves a metric by a statistically significant but trivial amount passes through. A change that moves a metric by a large amount but with noisy data gets flagged for more testing. Only changes that are both statistically real and practically large get blocked.

A recommendation engine used this approach for regression testing. They required p less than 0.05 and Cohen's d greater than 0.25 for any metric to trigger a block. Over twelve model updates, this rule blocked two regressions that mattered and cleared six statistically significant but trivial changes. Without the effect size check, all eight would have been flagged and engineering would have investigated six false alarms.

## Sequential Testing for Early Stopping

Traditional hypothesis testing requires you to decide the sample size upfront. You run N test cases, compute the p-value, and decide. But in practice, you often want to stop early if the result is obvious. If you run 100 test cases and the new model is clearly better with p equals 0.0001, why run 900 more?

**Sequential testing** allows you to check the result after every batch of test cases and stop early if the evidence is conclusive. This is more efficient but requires correction to avoid inflating the false positive rate. The simplest correction is the **alpha spending function**, which allocates your total false positive budget across multiple looks. If you plan to check after 100, 200, 500, and 1,000 test cases, you lower the threshold at each checkpoint so the overall false positive rate stays at 0.05.

A customer support AI used sequential testing to speed up regression validation. They planned to run up to 2,000 test cases but checked after every 200. They used an alpha spending function that required p less than 0.01 for the first check, p less than 0.02 for the second, and p less than 0.05 for later checks. If the new model showed no significant difference or significant improvement by the first or second checkpoint, they stopped testing and shipped. If there was ambiguity, they continued to the full 2,000. This reduced median testing time from four hours to 90 minutes without increasing false positives.

The danger of sequential testing is peeking without correction. If you check the p-value after every 50 test cases and stop whenever it crosses 0.05, you dramatically inflate the false positive rate. Every peek is another chance to see random noise that looks significant. You must use a formal sequential testing framework — alpha spending, group sequential design, or Bayesian alternatives — or you will fool yourself.

## Sample Size Requirements

How many test cases do you need to detect a regression? It depends on the size of the regression you care about and the variance of your metric. The formula is called **power analysis**, but the intuition is simple: larger effect sizes need fewer samples, higher variance needs more samples.

If you want to detect a 5 percent accuracy drop with 80 percent probability — meaning you will catch it four out of five times — and your baseline accuracy has a standard deviation of 2 percent, you need roughly 100 test cases. If the standard deviation is 5 percent, you need closer to 400. If you only care about detecting a 10 percent drop, you can get away with 30 test cases. If you care about detecting a 1 percent drop, you might need 1,000.

The teams that under-invest in test case volume operate in a fog. They see differences but do not know if they are real. They run 50 test cases, see a 3 percent drop, and guess whether to block the deploy. With 50 cases and typical variance, a 3 percent drop is often not statistically distinguishable from noise. They would need 200 cases to know for sure.

The teams that over-invest waste inference cost and time. They run 10,000 test cases when 500 would suffice. They detect differences so small they do not matter. The metric shifts by 0.1 percent and the p-value is 0.0001 and they block the deploy even though the effect size is meaningless.

The right sample size comes from power analysis. Decide the smallest effect you care about detecting. Measure the variance of your metrics on historical data. Plug those numbers into a power calculator and get a sample size. Most teams target 80 percent power — the ability to detect the target effect 80 percent of the time. Higher power requires more samples. Lower power means you miss regressions more often.

A legal document classifier wanted to detect any regression larger than 2 percent F1 score. Their historical variance was 3 percent. Power analysis indicated they needed 350 test cases to achieve 80 percent power at p equals 0.05. They built a suite of 400 cases and validated it by running it on ten historical model pairs where they knew the true effect size. The suite caught all regressions above 2 percent and flagged none below 1 percent. The sample size was calibrated correctly.

## The Danger of Multiple Comparisons Without Correction

Every time you compute a p-value, you take a 5 percent risk of a false positive if you use the standard 0.05 threshold. If you compute 20 p-values, the probability that at least one is a false positive is not 5 percent — it is much higher. The math: one minus 0.95 to the power of 20 equals 64 percent. You are more likely to see a false alarm than not.

This is the **multiple comparisons problem** and it destroys trust in regression suites. Your suite measures accuracy, precision, recall, F1, latency, toxicity, bias, cost, and factual grounding across five different user segments. That is 45 metric comparisons per test run. Without correction, you expect two false alarms per run. Engineering sees constant alerts. None of them are real. They start ignoring the suite.

The solution is **multiple testing correction**. The Bonferroni method divides your threshold by the number of tests. If you run 45 comparisons and want an overall 5 percent false positive rate, you use a per-test threshold of 0.0011. This guarantees that the probability of any false positive across all 45 tests stays at 5 percent. It is conservative — you lose some sensitivity — but it preserves trust.

A less conservative alternative is the **Benjamini-Hochberg procedure**, which controls the false discovery rate. Instead of saying "no more than 5 percent chance of any false positive," it says "no more than 5 percent of positives are false." If your suite flags three regressions, at most one is expected to be noise. This gives you more power to detect real issues while still controlling noise.

A fraud detection AI ran 60 different metrics in their regression suite. Without correction, they saw an average of three flagged metrics per test run. After applying Benjamini-Hochberg correction at a 5 percent false discovery rate, they saw an average of 0.4 flagged metrics per run. Real regressions still triggered alerts. Random noise stopped generating them.

If you run multiple metrics and do not correct for multiple comparisons, your regression suite is a noise generator. If you correct properly, it becomes a signal detector. The difference is a single statistical adjustment that takes five minutes to implement.

## From Statistical Evidence to Test Power

You have determined statistical significance. You have controlled for multiple comparisons. You know your p-values, your confidence intervals, and your effect sizes. But one question remains: if there is a real regression of a certain size, how likely is your test suite to catch it? That probability is called power, and understanding it is the difference between a test suite that detects problems and one that gives you false confidence. That is what we address next, in 3.9.
