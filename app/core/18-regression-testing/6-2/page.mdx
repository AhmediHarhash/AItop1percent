# 6.2 — Threshold-Based Gates: Hard Lines in the Sand

In November 2025, a healthcare platform shipped a new version of their symptom triage assistant. The regression suite showed accuracy at ninety-one point seven percent. The baseline was ninety-two point three percent. The team discussed the drop. It was small. The new prompt was faster. They were behind schedule. The product manager asked if ninety-one point seven was acceptable. The engineer said it was close enough. They shipped. Within two weeks, user complaints spiked. The assistant was routing patients to incorrect urgency levels. Urgent cases were marked routine. Routine cases were escalated. The clinical team reviewed sample outputs and found patterns of misclassification that the regression suite had detected but the team had rationalized as acceptable. They rolled back the prompt, delayed feature work for three weeks, and implemented a hard gate: accuracy below ninety-two percent blocks deployment, no exceptions.

The gate blocked the next four releases. Each time, the engineer investigated, adjusted the prompt, and re-ran the suite. Each time, accuracy recovered above ninety-two percent before the release proceeded. The gate was frustrating. It slowed down velocity. But it prevented another wave of incorrect medical advice from reaching users. The threshold was not arbitrary. It was the minimum quality level that kept patients safe.

## What Is a Threshold-Based Gate

A **threshold-based gate** enforces an absolute performance requirement. The metric must meet or exceed a specific number. Accuracy must be at least ninety-two percent. Latency at the ninety-fifth percentile must be below five hundred milliseconds. Cost per request must not exceed twelve cents. The threshold is fixed. The gate evaluates the metric, compares it to the threshold, and blocks deployment if the metric falls short.

Threshold-based gates are the most common type of quality gate because they are simple to understand, simple to implement, and simple to enforce. The rule is clear. The decision is binary. The metric is either above the threshold or it is not. There is no debate about whether a regression is acceptable. The threshold defines acceptable, and anything below the threshold is not.

## When to Use Hard Thresholds

Hard thresholds work best when the metric represents a critical quality dimension that should never degrade below a certain level. Medical systems use hard thresholds for accuracy because incorrect medical advice can cause patient harm. Financial systems use hard thresholds for fraud detection recall because missing fraud cases costs money and damages trust. Customer support systems use hard thresholds for refusal rate because users interpret refusals as the system being broken.

Hard thresholds are appropriate when the cost of violation is high, immediate, and measurable. If degrading the metric causes user harm, regulatory violation, or significant financial loss, the threshold should be hard. If degrading the metric causes minor inconvenience or aesthetic issues, a softer gate or a relative threshold may be more appropriate.

Hard thresholds also work well when the metric is stable and the baseline is well-understood. If accuracy on your golden set has been ninety-three to ninety-four percent for six months, a threshold of ninety-two percent is reasonable. If accuracy fluctuates between eighty-five and ninety-five percent depending on user traffic patterns, a hard threshold is less useful because it will trigger false positives when the natural variance dips below the threshold.

## The Danger of Arbitrary Thresholds

The biggest risk with threshold-based gates is setting the threshold arbitrarily. A product manager asks, "What should our accuracy threshold be?" An engineer says, "Ninety percent sounds good." The threshold is set. The gate is implemented. The gate fails immediately because current accuracy is eighty-seven percent. The team realizes they set the threshold without checking the baseline. They lower the threshold to eighty-five percent. The gate becomes meaningless because it is below current performance. The team stops trusting the gate.

Arbitrary thresholds create two problems. First, they fail to reflect actual system capabilities. If your system has never achieved ninety-five percent accuracy, setting a ninety-five percent threshold guarantees constant gate failures and constant overrides. The gate becomes an obstacle rather than a safety check. Second, arbitrary thresholds fail to reflect user expectations. If users tolerate eighty percent accuracy without complaint, a ninety percent threshold may be unnecessarily strict. If users complain at eighty-five percent accuracy, an eighty percent threshold is too loose.

Thresholds must be grounded in evidence. The evidence comes from three sources: current baseline performance, user impact analysis, and industry benchmarks. Your current baseline tells you what the system can achieve. User impact analysis tells you what users need. Industry benchmarks tell you what is achievable given the state of the art. A threshold that ignores any of these sources is a guess, and guesses make poor gates.

## Threshold Documentation: Why This Number

Every threshold must be documented with a justification. The documentation answers three questions. First, what is the threshold? Accuracy must be at least ninety-two percent on the golden set. Second, why is the threshold set at this level? Accuracy below ninety-two percent results in a fifteen percent increase in user-reported errors based on historical data from Q2 2025. Third, who approved the threshold and when? Approved by product lead and quality lead on September 15, 2025.

The documentation serves two purposes. First, it makes the threshold legible to future team members. When an engineer joins the team six months later and sees a gate blocking their release, they can read the justification and understand why the threshold exists. Second, it creates accountability. If the threshold is too strict or too loose, the documentation identifies who made the decision and what evidence they used. The team can revisit the threshold with context rather than arguing about it during a blocked deployment.

Threshold documentation lives in the same repository as the gate configuration. It is versioned alongside the code. When the threshold changes, the documentation is updated to explain the change. The documentation becomes the historical record of the team's evolving understanding of acceptable quality.

## Threshold Inheritance: Baseline Plus Buffer

The most reliable way to set an initial threshold is to inherit it from the current baseline and add a small buffer. If current accuracy is ninety-three percent and variance is plus or minus one percent, a reasonable threshold is ninety-two percent. The threshold is one percent below the typical performance, which means the gate will only fail if performance degrades beyond normal variance.

The buffer accounts for noise. Metrics fluctuate. A test suite run at different times of day may show slightly different results due to model API variance, network conditions, or randomness in multi-turn conversations. A threshold set exactly at the current baseline will fail half the time simply due to noise. A threshold set one to two percent below the baseline fails only when real degradation occurs.

The size of the buffer depends on the observed variance. If your metric is stable — variance less than half a percent — a small buffer is sufficient. If your metric is noisy — variance three to five percent — a larger buffer is needed to prevent false positives. The buffer should be large enough to absorb noise but small enough to catch meaningful regressions.

## The Zero-Tolerance Threshold: Some Failures Are Not Acceptable

Some quality dimensions require zero-tolerance thresholds. A healthcare system's assistant must never suggest a treatment plan for a condition the patient does not have. A financial advice bot must never recommend an investment strategy that violates the user's stated risk tolerance. A content moderation system must never allow hate speech or graphic violence to be classified as acceptable.

Zero-tolerance thresholds enforce absolute rules. The threshold is zero failures on a specific category of test cases. If a single test case in the category fails, the gate blocks deployment. There are no exceptions. There are no overrides. The only way forward is to fix the issue and achieve zero failures.

Zero-tolerance thresholds are rare because they are strict and expensive to maintain. They require test suites that cover every edge case in the critical category. They require prompt engineering, tool guardrails, or post-processing logic to ensure the model never produces the forbidden output. They require continuous monitoring because a single production failure invalidates the entire system.

But for certain categories of failure, zero tolerance is the only acceptable standard. If the failure causes physical harm, legal liability, or catastrophic reputational damage, the threshold must be zero. The gate enforces the standard that the team cannot afford to compromise on.

## Testing Threshold Boundaries

Once a threshold is set, it must be tested. The test answers two questions. First, does the gate trigger when it should? If you artificially degrade the metric below the threshold, does the gate block the deployment? Second, does the gate allow valid releases? If the metric is above the threshold, does the gate pass?

Testing threshold boundaries is simple. You modify the test suite or the prompt to intentionally cause a regression. You run the gate. You verify that the gate blocks the deployment and provides a clear failure message. Then you revert the change, run the gate again, and verify that the gate passes. This test validates that the gate is configured correctly and that the CI/CD pipeline respects the gate's decision.

Threshold boundary testing also reveals configuration mistakes. A team sets a threshold of ninety-two percent accuracy, but the gate is misconfigured to check for ninety-two percent or higher on a metric that is reported as a decimal between zero and one. The gate expects zero point nine two but receives ninety-two. The gate fails every time. The team spends two hours debugging before realizing the unit mismatch. Boundary testing catches this immediately.

## Threshold Drift: When Thresholds Quietly Change

Threshold drift occurs when the meaning of the threshold changes over time without the threshold value being updated. A team sets an accuracy threshold of ninety percent in June 2025 based on a golden set of five hundred examples. By December 2025, the golden set has grown to two thousand examples, and the distribution has shifted to include harder cases. The threshold is still ninety percent, but passing ninety percent on the new golden set is more difficult than passing ninety percent on the original set. The threshold has quietly become stricter.

Threshold drift also occurs in the opposite direction. A team sets a latency threshold of five hundred milliseconds at the ninety-fifth percentile. Over time, the team optimizes the prompt and tool architecture. Typical latency drops to three hundred milliseconds. The five hundred millisecond threshold is still in place, but it is now so loose that it catches nothing. The threshold has become meaningless.

Preventing threshold drift requires periodic threshold review. Every quarter, the team revisits each gate. They review the current baseline, the observed variance, and the frequency of gate failures. If the baseline has shifted significantly, the threshold is recalibrated. If the gate has not failed in three months, the threshold may be too loose. If the gate fails on thirty percent of deployments, the threshold may be too strict. The review ensures that thresholds remain aligned with system capabilities and user expectations.

## When Thresholds Should Be Absolute

Threshold-based gates are most effective when the quality dimension has a clear, fixed standard. Accuracy on a well-defined task. Refusal rate on a known set of queries. Compliance with a regulatory requirement. These dimensions do not depend on comparison to a previous version. They depend on meeting an external standard.

Absolute thresholds also work well when the metric is stable and the baseline is mature. If your system has been in production for a year, you have a strong sense of what performance is achievable. You know what users expect. You know what the model can deliver. Setting an absolute threshold is safe because you have enough data to set the threshold intelligently.

But not every quality dimension has a fixed standard. Cost per request depends on pricing, which changes. Latency depends on infrastructure, which evolves. Semantic similarity to baseline depends on the baseline, which updates. For these dimensions, relative thresholds — gates that compare the new version to the current version — are more appropriate.

Relative gates adapt to system evolution. They catch regressions without requiring threshold recalibration every time the baseline shifts. The next subchapter covers how relative gates work, when to use them, and how to combine them with absolute thresholds.
