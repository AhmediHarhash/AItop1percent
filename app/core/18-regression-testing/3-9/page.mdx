# 3.9 â€” Power Analysis and Sample Size

The most common mistake in regression testing is running too few examples to detect the regressions you actually care about. You run 20 test cases, see no failures, declare success, and ship. Three days later, your response quality has dropped 4 percentage points in production. The regression was there. Your test was too small to see it.

This is what power analysis tells you: the probability that your test will catch a real problem when one exists. It answers the question every release gate needs answered: if quality actually dropped by X percent, would my test catch it?

Most teams skip this step. They pick a sample size that feels reasonable, run their tests, and trust the results. Then they wonder why regressions make it to production despite passing all gates. The reason is simple: their tests had 30 percent power. They were flipping a weighted coin and calling it science.

## What Power Analysis Measures

**Statistical power** is the probability of detecting an effect when that effect is real. If your model's accuracy dropped from 92 percent to 88 percent, and you run your regression suite, what are the odds your test flags it as a failure? That probability is your power.

The convention in scientific research is 80 percent power. That means if a real regression exists, you have an 80 percent chance of catching it. Put another way: you accept a 20 percent chance of missing a real problem. This is not perfect. But it is the standard compromise between sensitivity and practicality.

In LLM evaluation, most teams never calculate power. They run whatever sample size fits their budget and timeline, then treat the results as definitive. A test passes with 50 examples, and the team ships. Nobody asks: if quality actually dropped 5 percent, would 50 examples have caught it? Usually the answer is no. The test had 40 percent power. It was more likely to miss the regression than catch it.

This is not acceptable at a release gate. If you cannot prove your test can detect the regressions that matter, you do not have a gate. You have theater.

## The Three Variables: Power, Effect Size, Sample Size

Power analysis involves three variables. You set two, and the third follows.

**Power** is the probability of detection. The standard target is 80 percent. Some teams use 90 percent for critical systems. Anything below 70 percent is not a serious gate.

**Effect size** is the magnitude of the regression you want to catch. If your baseline accuracy is 90 percent, what drop do you care about? A 2 percent drop? A 5 percent drop? A 10 percent drop? The smaller the effect size you care about, the more examples you need.

**Sample size** is how many test cases you run. This is the variable you control directly. Increase sample size, and you increase power for a given effect size. Decrease sample size, and you decrease power.

The relationship is not linear. Doubling your sample size does not double your power. To detect an effect half as large, you need roughly four times as many examples. This is why detecting small regressions is expensive. A test that catches a 10 percent drop with 50 examples might need 800 examples to reliably catch a 2 percent drop.

Most teams discover this too late. They design a gate that must catch 2 percent regressions, run 100 examples, and wonder why production quality drifts. The math never supported the requirement.

## Calculating Sample Size for LLM Evals

For accuracy-based metrics, the sample size formula depends on your baseline rate, your target effect size, and your desired power. A fintech team with 94 percent baseline accuracy wants to catch any drop below 90 percent. That is a 4 percentage point drop. They want 80 percent power.

The calculation shows they need approximately 120 examples per test. Run 120 cases. If accuracy stays at 94 percent, the test passes. If it drops to 90 percent or below, the test has an 80 percent chance of flagging it as a failure. This is a real gate.

Now suppose they want to catch a 2 percentage point drop instead. Same 94 percent baseline, but now they want to detect 92 percent as a regression. The required sample size jumps to roughly 470 examples. To detect a regression half as large, they need four times as many cases.

Most teams cannot afford 470 examples per gate. So they make a choice: accept the larger minimum detectable effect, or invest in more test capacity. There is no free option. If you run 120 examples, you will miss 2 percent regressions. If that is unacceptable, you need more examples.

The critical insight is that this is a design decision, not a discovery. You decide what regressions you must catch. The math tells you what sample size that requires. If you cannot afford that sample size, you revise your requirements or accept the risk.

## The Cost of Underpowered Tests

An underpowered test is worse than no test. It creates false confidence. The team believes they have a quality gate. They see tests passing and trust the release. Then production degrades, users complain, and the team is blindsided.

A healthcare AI company ran a regression suite with 40 examples before every release. Baseline accuracy was 89 percent. They wanted to catch drops below 85 percent. Power analysis showed their test had 52 percent power. Barely better than a coin flip.

Over six months, they shipped eight releases. The regression suite passed every time. Production accuracy dropped from 89 percent to 83 percent. When they finally caught it, they reviewed the test results. Three of those releases had contained real regressions that the 40-example suite missed. The test was too small to see them.

The damage was not just the 6 percentage point drop. It was the six months of false confidence. The team stopped monitoring closely because they trusted the gate. Product and executive leadership believed the system was stable. When the drop was discovered, trust in the entire testing process collapsed.

This is what underpowered tests do. They do not just miss regressions. They create organizational complacency. A gate that catches 50 percent of problems is not a gate. It is a liability.

## The Cost of Overpowered Tests

On the other end, overpowered tests waste resources. If your baseline accuracy is 90 percent, and you want to catch drops below 85 percent, you need roughly 80 examples for 80 percent power. Running 500 examples gives you 98 percent power. You gain 18 percentage points of sensitivity while increasing cost and latency by sixfold.

For most teams, this is not a good trade. The incremental value of moving from 80 percent to 98 percent power is small. You catch a few more edge cases. But you spend six times as long running the test, you burn six times as much compute, and you slow down every release.

The exception is critical systems where even rare regressions are catastrophic. A medical diagnosis model might justify 95 percent power. A financial fraud detection system might justify 90 percent. But for most applications, 80 percent power is the right target. Beyond that, you are paying exponentially more for diminishing returns.

The discipline is knowing when to stop. More examples always help. But at some point, the marginal benefit does not justify the marginal cost. Power analysis tells you where that point is.

## Practical Sample Sizes for LLM Evaluation

In practice, most LLM regression tests run between 50 and 300 examples. This is not because teams calculated optimal sample sizes. It is because 50 examples is fast enough to run in CI, and 300 examples is slow enough that teams resist running more.

For most binary classification tasks with baseline accuracy between 85 and 95 percent, 80 to 150 examples gives 80 percent power to detect a 5 percentage point drop. This is a reasonable default. If your accuracy is 90 percent, and you want to catch drops below 85 percent, aim for 100 to 120 examples.

For retrieval tasks, where you measure recall at K or precision at K, the required sample size depends on the baseline rate and the value of K. A RAG system with 82 percent recall at 5 wants to catch drops below 75 percent. With K equals 5, you need roughly 140 examples for 80 percent power.

For generative tasks scored by LLM judges, the required sample size depends on the judge's variance. If your judge assigns binary pass/fail with 88 percent pass rate, and you want to catch drops below 82 percent, you need approximately 130 examples. But if the judge outputs scores from 1 to 5, and you measure mean score, the variance is higher. You might need 200 examples to detect a half-point drop with 80 percent power.

The key lesson: sample size is not arbitrary. It is a function of your baseline metric, your minimum detectable effect, and your tolerance for missed regressions. Calculate it before you build the test. Otherwise you are guessing.

## Adjusting Power for Different Test Types

Not all gates need the same power. A fast pre-commit gate that runs in 30 seconds might accept 70 percent power and a larger minimum detectable effect. You catch the big regressions, and anything subtle gets caught by the slower, more comprehensive gate downstream.

A final release gate that decides whether code ships to production needs 85 to 90 percent power. This is the last chance to catch a problem before users see it. The cost of a miss is high. The cost of running 200 examples instead of 100 is low compared to the cost of a bad release.

A post-deployment monitor that runs every hour might use 75 percent power and a very small effect size. You are not blocking a release. You are detecting drift. The stakes are lower, and you get multiple chances to catch the problem.

This is the principle: match power to consequence. High-stakes gates get high power. Low-stakes gates accept lower power in exchange for speed. But never run a test without knowing its power. If you do not know the probability of detection, you do not know what the test is actually protecting against.

## The Minimum Detectable Effect

Every test has a minimum detectable effect: the smallest regression it can reliably catch. For a given sample size and desired power, there is a threshold below which regressions are invisible.

If you run 80 examples and want 80 percent power, your minimum detectable effect for a baseline accuracy of 90 percent is approximately 5 percentage points. You can catch a drop from 90 percent to 85 percent. You cannot catch a drop from 90 percent to 88 percent. The test does not have the resolution.

This is not a failure of the test. It is a constraint of statistics. Small effects require large samples. If you cannot afford large samples, you cannot detect small effects. The mistake is pretending otherwise.

A conversational AI team with 87 percent baseline accuracy ran a 60-example regression suite before each release. They believed the test would catch any meaningful regression. Power analysis showed the minimum detectable effect was 6 percentage points. A drop from 87 percent to 81 percent would be caught. A drop to 84 percent would not.

Over four months, production accuracy drifted from 87 percent to 83 percent. Four separate releases contributed to the drift. The regression suite passed every time. Each individual release caused a 1 to 2 percentage point drop, well below the 6 percentage point threshold the test could detect.

When they discovered the drift, they recalculated. To catch 2 percentage point drops with 80 percent power, they needed 340 examples. They increased the sample size, and the next regression, a 2.5 percentage point drop, was caught immediately.

The lesson: know your minimum detectable effect. If it is larger than the regressions you care about, your gate will fail.

## When to Increase Sample Size vs Accept Uncertainty

You cannot eliminate uncertainty. You can only decide how much uncertainty you accept and how much you pay to reduce it.

If your current test has 70 percent power, and you want 85 percent power, you need to increase sample size by roughly 60 percent. If you are running 100 examples, you need 160. The question is: is the reduction in missed regressions worth the increase in cost and latency?

For a weekly release gate, probably yes. You run the test once a week. Adding 60 examples adds a few minutes. The cost is negligible. The reduction in risk is real.

For a pre-commit gate that runs 50 times a day, probably no. Adding 60 percent to every run adds hours to the team's cumulative wait time. The benefit of moving from 70 percent to 85 percent power is not worth the velocity hit.

The decision framework is simple. Calculate the cost of the additional examples in time and compute. Calculate the expected cost of a missed regression: how often does it happen, how bad is it when it does, how much does it cost to fix. If the cost of missing a regression exceeds the cost of running more examples, increase sample size. If not, accept the uncertainty.

Most teams never do this calculation. They pick a sample size based on what feels reasonable, then live with whatever power that produces. This is backwards. Start with the risk you are willing to accept. Then calculate the sample size that delivers it.

The next subchapter covers variance estimation and why understanding the spread in your metric is just as important as understanding the mean.

