# 1.6 — Regression Testing vs Continuous Evaluation

Regression testing and continuous evaluation are not the same discipline. They have different triggers, different purposes, different infrastructure, and different failure modes. Most teams conflate them, assume that one covers the other, and end up with gaps in both. You need both. They protect against different failure modes. Regression testing protects your deploy process. Continuous evaluation protects your production reality.

The fundamental difference is timing and trigger. **Regression testing** is change-triggered and pre-deploy. It runs when you make a change — a model swap, a prompt update, a retrieval adjustment, a system parameter change. It compares the new system against the baseline before you deploy. It answers one question: did this change make the system worse? If yes, you block the deploy. If no, you ship. Regression testing is a gate.

**Continuous evaluation** is time-triggered and post-deploy. It runs continuously in production, sampling live traffic, measuring quality metrics, detecting drift. It does not compare against a pre-deploy baseline. It compares against recent historical performance. It answers a different question: is the system degrading over time for reasons unrelated to code changes? If yes, you investigate and intervene. If no, you keep monitoring. Continuous evaluation is a smoke detector.

The reason you need both is that systems degrade in two different ways. They degrade because you changed something — that is what regression testing catches. They degrade because the world changed — that is what continuous evaluation catches. Data distribution shifts. User behavior evolves. External APIs change responses. Retrieval corpus gets stale. Adversarial users probe for weaknesses. The system you deployed six months ago no longer faces the same inputs it faced at deploy time. Continuous evaluation detects that drift. Regression testing cannot.

## What Regression Testing Protects Against

Regression testing protects against self-inflicted degradation. It catches the problems you create when you change the system. Every model swap is a regression risk. Every prompt edit is a regression risk. Every retrieval adjustment, every safety filter update, every parameter change is a regression risk. Regression testing measures that risk before it reaches users.

The canonical regression testing workflow is a comparison test. You have a baseline system — the current production system. You have a candidate system — the new system you want to deploy. You run both systems against the same eval set. You measure the same metrics. You compare the distributions. If the candidate system performs worse than the baseline on any critical metric by more than your threshold, the regression test fails. You do not deploy.

The strength of regression testing is precision. It isolates the impact of a single change. If your regression test fails, you know exactly why: the change you just made degraded the system. You know which metric regressed. You know by how much. You have a clear decision: ship anyway and accept the regression, adjust the change to eliminate the regression, or abandon the change entirely. The causal link is clear.

The weakness of regression testing is scope. It only measures what you test. If your regression suite tests accuracy but not latency, you will catch accuracy regressions and miss latency regressions. If it tests accuracy and latency but not cost, you will catch cost regressions three weeks later when the bill arrives. If it tests accuracy, latency, and cost but not hallucinations, you will catch hallucination regressions when users complain. Regression testing protects you from the failure modes you thought to measure. It does not protect you from the failure modes you forgot.

The second weakness is representativeness. Regression testing runs against an eval set — typically a few hundred to a few thousand curated queries. That eval set represents the distribution you think matters. It does not represent the long tail of edge cases, adversarial inputs, or emergent user behaviors that only appear in production. A change that passes regression testing on your curated eval set can still degrade production performance on inputs your eval set did not cover.

## What Continuous Evaluation Protects Against

Continuous evaluation protects against environmental degradation. It catches the problems you did not create but that happen anyway. User behavior changes. Data distributions shift. External dependencies change behavior. Adversarial users discover new attack patterns. The retrieval corpus goes stale. The system degrades for reasons that have nothing to do with code changes.

The canonical continuous evaluation workflow is a monitoring loop. You sample production traffic — typically one to ten percent depending on cost and latency tolerance. You run those queries through your eval pipeline. You measure the same quality metrics you measure during regression testing. You aggregate metrics over time windows — hourly, daily, weekly. You detect drift: are current metrics significantly worse than recent historical metrics? If yes, you investigate.

The strength of continuous evaluation is coverage. It measures real production traffic, not a curated eval set. It sees the edge cases your regression tests missed. It sees the adversarial inputs your red team did not anticipate. It sees the distribution shifts that only emerge at scale. If quality is degrading in production, continuous evaluation will detect it even if regression testing would have missed it.

The weakness of continuous evaluation is attribution. When continuous eval detects a quality drop, it does not tell you why. It tells you that accuracy dropped from 89 percent to 83 percent over the past week. It does not tell you whether that drop was caused by a code change, a data shift, a dependency change, or something else. You know you have a problem. You do not know what caused it. Investigation is required.

The second weakness is latency. Continuous evaluation detects problems after they reach production. Users have already experienced the degraded quality. The damage has already started. You can react quickly — within hours or days — but you cannot prevent the problem the way regression testing prevents problems. Continuous evaluation is reactive. Regression testing is preventive.

## How They Work Together

In a mature system, regression testing and continuous evaluation form a defense-in-depth strategy. Regression testing is your first line of defense. It blocks known regressions before deploy. Continuous evaluation is your second line of defense. It detects unknown degradations after deploy. Neither is sufficient alone. Together they provide comprehensive quality protection.

The workflow looks like this. You make a change — a model swap, a prompt update, whatever. Before deploying, you run regression tests. The tests compare the new system against the baseline across all your quality dimensions: accuracy, latency, cost, hallucinations, tone, safety, retrieval quality, tool use, policy compliance, business KPIs. If any critical metric regresses beyond your threshold, the deploy is blocked. If all metrics pass, you deploy to a canary — a small percentage of production traffic.

Once deployed to canary, continuous evaluation monitors the new system in production. It measures the same quality metrics on live traffic. It compares the canary cohort against the control cohort. If continuous eval detects a quality drop that regression testing missed, you roll back immediately. If continuous eval confirms that production performance matches regression test performance, you gradually expand the canary until the new system handles 100 percent of traffic.

At full rollout, continuous evaluation continues monitoring. It detects drift that happens independent of code changes. It catches distribution shifts, adversarial patterns, dependency changes, and corpus staleness. When it detects significant drift, it triggers an investigation. The investigation determines the root cause. If the root cause is a code change that regression testing should have caught, you improve your regression test suite. If the root cause is environmental, you adapt the system to handle the new conditions.

The key insight is that the two systems feed each other. Continuous evaluation finds gaps in regression testing. When continuous eval detects a production regression that regression testing missed, that becomes a new test case for your regression suite. Regression testing validates fixes for drift detected by continuous evaluation. When continuous eval detects drift and you fix it, regression testing ensures the fix does not introduce new regressions. They form a cycle of continuous improvement.

## When to Use Which Approach

The decision of when to use regression testing versus continuous evaluation is not a choice — you use both. But the emphasis and investment differ depending on your deployment frequency and system maturity.

For teams that deploy frequently — daily or multiple times per week — regression testing is critical. High deploy velocity means high regression risk. Every deploy is an opportunity to degrade quality. Without automated regression testing, you either slow down deploys to allow manual review or you ship regressions. Regression testing allows you to maintain high velocity without sacrificing quality. It is the gate that makes fast deploys safe.

For teams that deploy infrequently — monthly or quarterly — continuous evaluation is more critical. Low deploy velocity means environmental drift has more time to accumulate between deploys. The system that worked well at deploy may have degraded significantly by the time the next deploy happens. Continuous evaluation detects that drift and triggers intervention before the next scheduled deploy. It is the alert that makes slow deploys safe.

For teams in regulated industries — healthcare, finance, legal — both are mandatory. Regression testing provides audit trail evidence that you tested quality before deploy. Continuous evaluation provides audit trail evidence that you monitored quality after deploy. Regulators expect both. Compliance frameworks require both. The absence of either creates audit findings and regulatory risk.

For teams with tight margins or strict cost constraints, regression testing must include cost metrics. Continuous evaluation can monitor cost drift, but regression testing is your opportunity to block cost regressions before they affect your budget. If you deploy a change that doubles per-query cost and you do not catch it until the end of the month, you have already spent the money. Regression testing is cost control.

For teams with strict latency requirements — real-time voice, live chat, time-sensitive recommendations — regression testing must include latency metrics at every percentile that matters. Continuous evaluation can detect latency regressions in production, but by then users have already experienced slow responses. Regression testing is your opportunity to block latency regressions before they affect user experience.

## What Happens When You Only Have One

Teams that only have regression testing and no continuous evaluation miss environmental drift. They catch every problem they create. They miss every problem the world creates. Their regression tests pass. Their production quality slowly degrades. They discover the degradation weeks or months later when a business metric declines or a user complains. They investigate and find that the system is producing worse outputs than it did at deploy time — not because they changed anything, but because the input distribution shifted and their system did not adapt.

The e-commerce company from the section introduction provides an example. They had comprehensive regression testing. Every deploy ran through an eval suite that tested accuracy, latency, and cost. Their regression tests caught every self-inflicted problem. But over six months, user search queries evolved. Users started asking more complex, multi-part questions. The system, trained on simpler queries, struggled with the new patterns. Accuracy dropped from 87 percent at deploy time to 78 percent six months later. The regression tests never failed because the team never changed the system. Continuous evaluation would have detected the drift in week three.

Teams that only have continuous evaluation and no regression testing ship avoidable regressions. They deploy changes. The changes degrade quality. Continuous evaluation detects the degradation within days. The team rolls back or deploys a fix. The incident is short-lived — one to three days of user impact. But it was preventable. Regression testing would have caught the problem before deploy. The users would never have experienced the degraded quality.

The financial services company from the Chapter 1 introduction provides an example. They had continuous evaluation. They sampled production traffic and measured accuracy daily. When accuracy dropped, they rolled back within 24 hours. But they had no regression testing. They deployed a prompt change that caused a safety refusal regression — the model became overly cautious and started refusing legitimate financial planning questions. Continuous eval detected the problem. Users experienced one day of over-refusals. Regression testing would have blocked the deploy entirely.

The worst case is teams that have neither. They deploy changes without testing. They do not monitor production quality. They discover regressions when users complain, when business metrics decline, or when a major incident forces investigation. These teams are flying blind. Their system quality is a function of luck, not discipline. The median time to detection for regressions is six to ten weeks. The median revenue impact is in the tens or hundreds of thousands of dollars. These teams do not survive long in competitive markets.

## The Infrastructure Overlap and Divergence

Regression testing and continuous evaluation share infrastructure but diverge in execution. Both require eval datasets, eval metrics, eval execution pipelines, and result storage. Both benefit from LLM-as-judge evals, golden reference sets, and multi-dimensional quality metrics. A team building one can reuse much of the infrastructure for the other.

The divergence is in trigger, comparison baseline, and result storage. Regression testing is triggered by code changes. Continuous evaluation is triggered by time. Regression testing compares against a fixed pre-deploy baseline. Continuous evaluation compares against rolling historical windows. Regression testing stores results as pass/fail gates in your CI/CD system. Continuous evaluation stores results as time-series metrics in your observability platform.

In practice, mature teams build a unified eval platform that supports both workflows. The platform defines eval datasets, eval metrics, and eval execution logic once. It exposes two interfaces: a synchronous interface for regression testing that blocks deploys and an asynchronous interface for continuous evaluation that runs in the background. The same eval logic runs in both contexts. The infrastructure is shared. The workflows are separate.

The shared infrastructure is powerful because improvements in one domain improve the other. When you add a new metric to detect hallucination regressions, that metric becomes available in continuous evaluation automatically. When continuous evaluation detects a new failure mode in production, you add that failure mode to your regression test suite. The two systems evolve together. Your quality coverage expands continuously.

The maturity path is to build regression testing first and continuous evaluation second. Regression testing is easier to build and provides immediate value by preventing self-inflicted regressions. Continuous evaluation requires production instrumentation, sampling infrastructure, and time-series storage — more complex to build but essential for detecting environmental drift. Most teams spend their first year building regression testing and their second year adding continuous evaluation. By year three, they have both, and their quality protection is comprehensive.

The next chapter covers how to build the foundation of regression testing: the test suite itself. The suite is where you define what quality means, what regressions look like, and what thresholds trigger failures. Get the suite right and regression testing becomes a reliable gate. Get it wrong and regression testing becomes theater — green lights that mean nothing.

