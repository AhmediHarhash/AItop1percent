# 4.10 — Document Ingestion Regression Tests

Every document that enters your knowledge base is a potential regression waiting to happen. A single malformed document can break retrieval for hundreds of queries. A document with incorrect metadata can pollute search results for weeks. A document that duplicates existing content can split traffic between two versions, degrading ranking quality. Most teams treat document ingestion as an operational task — upload the file, wait for indexing to finish, assume it worked. That assumption destroys retrieval systems one document at a time.

Document ingestion is not an append-only operation. Every new document changes the retrieval landscape. It competes with existing documents for query matches. It shifts ranking distributions. It introduces new vocabulary, new metadata patterns, new edge cases. If you ingest a thousand documents without validation, you have injected a thousand potential failure points into production. You need ingestion quality gates that block bad documents before they poison the corpus.

## Ingestion Quality Gates

The first gate is format validation. Every document must conform to the schema your ingestion pipeline expects. If your pipeline expects JSON with title, body, and metadata fields, a document missing the title field fails validation. If your pipeline expects markdown-formatted text, a document containing raw HTML or binary data fails validation. Format validation happens before any processing — before embedding generation, before indexing, before storage.

Format validation is not lenient. You do not attempt to repair malformed documents automatically. You do not guess at missing fields. You do not strip out unrecognized content and hope for the best. Every repair introduces risk. A pipeline that silently converts HTML to plain text might strip out critical formatting that affects meaning. A pipeline that auto-generates missing titles might create titles that mislead users. If a document does not meet the format requirements, you reject it at ingestion time and surface the error to whoever submitted it.

The second gate is metadata completeness. Every document must include all required metadata fields: source identifier, document type, creation timestamp, access control tags, owner information. Missing metadata breaks filtering, breaks access control, breaks audit trails. A document without a source identifier cannot be traced back to its origin. A document without access control tags might be served to users who should not see it. A document without a timestamp cannot be ranked by recency.

Metadata completeness gates are strict. If a required field is missing, the document is rejected. If a field contains an invalid value — a timestamp in the wrong format, an access control tag that does not exist in your system — the document is rejected. You do not allow partial metadata. You do not default missing fields to placeholder values. Every document that enters production must be fully specified.

The third gate is content quality validation. You check for minimum content length, language detection, and readability. A document with only three words of content is unlikely to be useful for retrieval. A document in an unsupported language breaks your embedding model. A document with garbled text or encoding errors produces meaningless embeddings. Content quality validation filters out documents that will degrade retrieval even if they are technically well-formed.

Some teams include duplication detection as a quality gate. Before ingesting a document, you check whether a document with identical or near-identical content already exists in the knowledge base. If it does, you reject the new document or merge it with the existing one. Duplicate documents create retrieval problems: users see the same content twice in search results, ranking algorithms do not know which version to prioritize, and storage costs increase without any benefit.

## Testing How New Documents Affect Existing Retrieval

Ingesting a new document changes retrieval behavior for every query that might match that document. You need tests that verify the new document improves retrieval quality or at least does not degrade it. This is not a hypothetical concern. A poorly chosen document can derank high-quality existing documents, introduce noise into search results, or break queries that previously worked well.

You test this by running your golden query set against the index before and after ingesting the new document. For queries where the new document should be relevant, you verify that it appears in the top results. For queries where the new document should not be relevant, you verify that it does not appear at all or appears only at very low ranks. For queries unrelated to the new document, you verify that existing results remain unchanged.

This is a regression test for every ingestion. You are treating the current state of retrieval as the baseline and the ingestion as a change that must not degrade that baseline. If a query previously returned highly relevant documents A, B, and C, and after ingesting document D it now returns D, B, and C — pushing A out of the top three — you need to understand why. Maybe D is more relevant than A, and the ranking improved. Maybe D is less relevant but scored higher due to recency bias, and the ranking regressed.

The test suite includes negative cases: queries that should never match the new document. If you ingest a document about database scaling and it starts appearing in results for queries about frontend frameworks, something is wrong. Either the document was mislabeled, or your embedding model is producing low-quality representations, or your ranking function is broken. The ingestion test catches this before users see it.

For high-traffic systems, you run ingestion tests in staging environments that mirror production. You replicate the production index to staging, ingest the new document in staging, run the full regression suite, and validate that no queries regressed. Only after staging tests pass do you ingest the document in production. This adds latency to ingestion — minutes to hours, depending on index size and test coverage — but it prevents bad documents from reaching users.

## The Corpus Contamination Problem

Corpus contamination is what happens when bad documents accumulate over time and degrade retrieval quality across the entire knowledge base. One bad document is a minor issue. A hundred bad documents create noise. A thousand bad documents make retrieval unusable.

Bad documents come in many forms. Documents with incorrect metadata that appear in searches they do not belong in. Documents with low-quality or outdated content that rank higher than authoritative sources. Documents with formatting errors that make them unreadable. Documents with misleading titles that do not match their content. Each bad document reduces user trust in the system. Each one makes it harder for good documents to rank well.

Corpus contamination is insidious because it happens gradually. You do not notice the first ten bad documents. You do not notice the first hundred. By the time retrieval quality has visibly degraded, the corpus contains thousands of bad documents, and cleaning them up requires months of manual review. Prevention is the only viable strategy.

Prevention means strict ingestion gates and continuous corpus quality monitoring. You block bad documents at ingestion time with format validation, metadata validation, and content quality checks. You monitor corpus quality over time with automated scans that detect documents missing required metadata, documents with suspiciously short content, documents that never appear in search results. You flag these documents for review and either fix them or remove them.

Some teams implement corpus quality scores: every document is assigned a quality score based on metadata completeness, content length, retrieval frequency, user engagement signals. Documents with quality scores below a threshold are quarantined and excluded from retrieval until they are reviewed and improved. This prevents low-quality documents from degrading retrieval while still allowing them to be stored and accessed directly if needed.

## Schema Validation for Document Metadata

Schema validation ensures that every document adheres to the metadata structure your system expects. Schemas define which fields are required, which are optional, what data types are allowed, and what constraints apply to each field. A schema violation is an ingestion failure.

A typical document schema includes fields like document ID, title, body text, source system, creation date, last modified date, author, document type, access control list, tags, and language. Each field has a type: strings, integers, timestamps, arrays. Each field has constraints: title must be non-empty, creation date must be a valid ISO 8601 timestamp, language must be a two-letter ISO 639 code, tags must be selected from a predefined taxonomy.

Schema validation happens at ingestion time, before any expensive processing. You do not generate embeddings for a document that fails schema validation. You do not store it in the vector database. You do not index it. You reject it immediately and return a detailed error message explaining which fields failed validation and why. The error message is specific enough that the ingestion source can fix the problem and retry.

Schemas evolve over time. You add new optional fields. You deprecate old fields. You change data types or constraints. Every schema change requires a migration plan. You cannot simply start ingesting documents with a new schema if millions of existing documents use the old schema. You need backward compatibility: new documents use the new schema, old documents remain valid under the old schema, and queries work correctly across both.

Some teams version their schemas. Each document is tagged with the schema version it conforms to. Queries understand multiple schema versions and normalize results so users see consistent metadata regardless of schema version. When you introduce a new schema version, you ingest new documents with the new version but do not force migration of old documents unless necessary. Over time, the corpus converges to the latest schema version naturally as old documents age out or are updated.

## Duplicate Detection Gates

Duplicate detection prevents the same content from being ingested multiple times under different document IDs. Duplicates waste storage, degrade retrieval quality, and confuse users who see the same result repeated in search output.

The simplest duplicate detection strategy is exact match: compute a hash of the document content and check whether that hash already exists in the corpus. If it does, reject the new document as a duplicate. Exact match catches copy-paste duplicates and re-uploads of the same file. It does not catch near-duplicates: documents with the same content but slightly different formatting, different metadata, or minor edits.

Near-duplicate detection uses content similarity instead of exact matching. You generate an embedding for the new document and compare it to embeddings of existing documents. If cosine similarity exceeds a threshold — typically 0.95 or higher — the documents are considered near-duplicates. You either reject the new document or prompt a human reviewer to decide whether it adds value.

Some near-duplicates are intentional and should not be blocked. A company might have separate documents for the same product in different languages. Those documents are near-duplicates in embedding space but serve different user populations. A knowledge base might have both a summary document and a detailed document on the same topic. Those documents are near-duplicates but serve different use cases. Duplicate detection gates need context: they should flag potential duplicates for review but not auto-reject without understanding intent.

For high-volume ingestion systems, duplicate detection runs asynchronously. You ingest the document first, then check for duplicates in the background. If a duplicate is found, you either remove the newer document or merge it with the older one. Asynchronous detection trades ingestion latency for throughput: you accept the small risk of temporary duplicates in exchange for faster ingestion.

## Incremental Versus Batch Ingestion Testing

Incremental ingestion adds one document at a time. Batch ingestion adds hundreds or thousands of documents in one operation. The testing strategy differs for each.

Incremental ingestion tests focus on per-document validation: does this specific document meet quality gates, does it affect retrieval as expected, does it duplicate existing content. You run the full regression suite for every document because the cost is low — a few seconds of compute per document. Incremental ingestion is common for user-generated content, real-time knowledge base updates, and continuous integration workflows.

Batch ingestion tests focus on corpus-level effects: does this batch introduce duplicates, does it shift ranking distributions, does it degrade overall retrieval quality. Running per-document regression tests for a batch of ten thousand documents is too slow. Instead, you sample: run detailed tests on a representative subset of documents and run lightweight tests on the full batch. You validate that the batch meets aggregate quality thresholds — ninety-eight percent of documents pass format validation, ninety-five percent of documents pass metadata completeness, no more than one percent duplicates detected.

Batch ingestion is common for periodic knowledge base refreshes, bulk data migrations, and integration with external content sources. The risk is higher than incremental ingestion because a systemic issue — a bug in the ingestion script, a schema change that broke compatibility, a data quality problem in the source system — affects every document in the batch. You need staging environment validation: ingest the full batch in staging, run the full regression suite against the updated staging index, and only promote to production after validation passes.

## Staging Environment Validation

Staging environments are mandatory for any production retrieval system that values reliability. Staging is a full replica of production — same index, same documents, same configuration — but isolated from user traffic. You test every change in staging before applying it to production.

For document ingestion, staging validation means ingesting the document in staging first, running retrieval tests, and verifying that no regressions occur. If tests pass, you ingest the same document in production with confidence. If tests fail, you fix the document or reject it without ever affecting production users.

Staging validation catches problems that unit tests miss. Unit tests validate individual components: format validation works, metadata extraction works, embedding generation works. Staging validation tests the entire pipeline end-to-end with realistic data and realistic query patterns. It catches integration bugs, configuration drift between environments, and edge cases that only appear at scale.

The challenge with staging is keeping it synchronized with production. If production has four million documents and staging has three million, tests in staging do not reflect production behavior. If production uses the latest embedding model and staging uses an older version, retrieval results differ between environments. You need automation that keeps staging synchronized: daily snapshots of production data, automated deployment of configuration changes to both environments, monitoring that alerts when staging drifts out of sync.

Staging adds latency to every ingestion. Instead of uploading a document and seeing it appear in production immediately, you upload, wait for staging tests to run, wait for promotion to production. For time-sensitive content — breaking news, incident updates, critical policy changes — this latency is unacceptable. You need a fast path for urgent ingestions that skips staging but includes extra production monitoring. The fast path is an escape hatch, not the default. Most ingestions go through staging.

Next: how to detect and prevent retrieval latency degradation as your index grows and query complexity increases.
