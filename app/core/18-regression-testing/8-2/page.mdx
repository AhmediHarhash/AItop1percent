# 8.2 — Staged Rollout Gates

In October 2025, a customer support platform deployed a fine-tuned model to 100 percent of users at once. The model had passed all regression tests. It had passed pre-deployment validation. The team was confident. Within four hours, support ticket volume tripled. The model had developed a subtle politeness regression — it over-apologized, added unnecessary qualifiers, and made responses 40 percent longer on average. Users found the responses frustrating. The issue never appeared in testing because the test suite focused on correctness, not tone. It never appeared in validation because validation used a dataset that over-represented simple queries where verbosity was acceptable. It only became visible at scale with real users. The team rolled back, fixed the issue, and rebuilt their deployment process around staged rollouts. The next deployment went to one percent of users first. The verbosity issue would have been caught in the first hour.

## Why 100 Percent Deploys Are Dangerous

AI systems fail in ways that unit tests, regression tests, and validation environments do not catch. They fail because real user traffic has distribution shapes your validation data did not capture. They fail because real user behavior triggers edge cases you did not anticipate. They fail because production infrastructure behaves differently under load than it does in validation. They fail because your metrics do not measure the thing that matters most to users. A model that works perfectly in testing can still fail in production. The only way to know if a model works in production is to deploy it to production and measure what happens.

The risk is not that the model will fail catastrophically. The risk is that it will fail subtly. Latency increases by 15 percent. Accuracy drops by three percentage points on a specific query type. User satisfaction decreases in a way your metrics do not capture. Cost per request increases because the model generates longer responses. These failures are not dramatic. They are erosive. They degrade the user experience slowly enough that you might not notice until the damage is done. By the time you detect the issue, thousands or millions of users have experienced it.

Staged rollouts limit the blast radius. Instead of deploying to 100 percent of users and discovering an issue after it affects everyone, you deploy to one percent of users and discover the issue after it affects one percent. The one percent cohort is your canary. If the canary dies, you stop the rollout. If the canary survives, you expand. The cost of a bad deployment is proportional to the number of users affected. Staged rollouts minimize that cost.

## Stage Definitions and Progression

A typical staged rollout has four stages: one percent, ten percent, 50 percent, and 100 percent. The percentages are not universal. Some teams use one percent, five percent, 25 percent, 100 percent. Some teams use one percent, ten percent, 30 percent, 70 percent, 100 percent. The specific percentages matter less than the principle: start small, expand in steps, stop if anything goes wrong. The early stages catch issues that affect a meaningful fraction of users. The later stages confirm that the model works at scale.

The one percent stage is your first real production test. It should run long enough to collect meaningful data but not so long that you delay the rollout unnecessarily. For most systems, 24 hours at one percent is sufficient. This gives you time to measure latency, error rates, user satisfaction, and quality metrics on real traffic. It also gives you time to review logs, investigate anomalies, and consult with stakeholders. If the model behaves identically to baseline at one percent, proceed to ten percent. If anything looks wrong, investigate before expanding.

The ten percent stage is where you catch issues that affect one user in ten or one query in ten. These are issues that might not appear in one percent because the sample size is too small. Ten percent gives you ten times more data and ten times more coverage of edge cases. At ten percent, you can measure per-user metrics with confidence. You can stratify by user segment and see if the model works better for some users than others. You can measure cost at scale and see if the cost projections from validation were accurate. Run ten percent for at least 24 hours, often 48 hours if your traffic is variable.

The 50 percent stage is where you test scalability. Does the model handle half your production traffic without infrastructure strain? Does latency increase under higher load? Do error rates climb? Does the retrieval system keep up? Fifty percent is large enough to surface infrastructure bottlenecks that do not appear at smaller scales. It is also the point where rollback becomes more disruptive. Rolling back from one percent or ten percent is low-risk. Rolling back from 50 percent affects half your users and creates a noticeable experience shift. Confidence at 50 percent is critical. Run this stage for at least 24 hours.

The 100 percent stage is full deployment. If you reached this stage, the model has proven itself at one percent, ten percent, and 50 percent. It has survived real traffic, real users, and real infrastructure load. Full deployment is still not risk-free — new issues can emerge even at 100 percent — but the risk is much lower than it was at one percent. Continue monitoring aggressively for the first 48 hours after reaching 100 percent. Some issues only appear after enough time has passed for distribution drift to accumulate.

## Gate Criteria at Each Stage

Each stage has entry criteria and promotion criteria. Entry criteria are the checks that must pass before you deploy to that stage. Promotion criteria are the checks that must pass before you advance to the next stage. Entry criteria are pre-deployment checks. Promotion criteria are post-deployment checks. Both are necessary. Skipping entry criteria is reckless. Skipping promotion criteria is reckless in a different way.

Entry criteria for the one percent stage are pre-deployment validation results. The model must have passed all regression tests, met all quality thresholds, met all latency thresholds, met all safety checks, and met all cost targets in the validation environment. If validation failed, the model does not proceed to one percent. Entry criteria are binary. Pass or fail. No exceptions.

Promotion criteria for the one percent stage are based on production metrics measured during the stage. The model must match or exceed baseline on all primary quality metrics. Error rates must be at or below baseline. Latency must be at or below baseline at the 95th and 99th percentiles. Cost per request must be at or below target. User satisfaction metrics — if you track them — must be at or above baseline. If any metric regresses significantly, the rollout stops. If metrics are noisy but not clearly worse, you investigate before promoting. If metrics are clearly better, you promote.

Entry criteria for the ten percent stage are successful completion of the one percent stage. If one percent passed all promotion criteria, ten percent proceeds. Promotion criteria for the ten percent stage are the same as one percent but with tighter confidence intervals. You have more data at ten percent, so you can detect smaller regressions. A regression that was borderline at one percent becomes clear at ten percent. You also have more coverage of user segments and query types, so you can stratify metrics and check if the model works equally well for all groups. If one user segment regresses, the rollout stops even if aggregate metrics look fine.

Entry criteria for the 50 percent stage are successful completion of ten percent. Promotion criteria include infrastructure stability in addition to quality metrics. At 50 percent, you are testing whether the system can handle half your traffic without strain. Monitor CPU usage, memory usage, GPU utilization, API rate limits, retrieval latency, and database query times. If infrastructure metrics degrade, the issue might not be the model — it might be your serving infrastructure. Either way, the rollout stops until you fix it.

Entry criteria for 100 percent are successful completion of 50 percent. At this point, you have high confidence the model works. Promotion criteria are the same as 50 percent but sustained for 24 to 48 hours. The model must remain stable at 100 percent for long enough to prove it can handle full production load continuously. If metrics remain stable, the rollout completes. The candidate model becomes the production model. The previous model becomes the fallback.

## Stage Duration and Timing

Stage duration is a trade-off between risk and velocity. Longer stages give you more data and more confidence but slow down deployment. Shorter stages ship faster but increase the risk of missing subtle issues. The right balance depends on your risk tolerance, your deployment frequency, and how fast your metrics converge. High-traffic systems with good telemetry can run short stages — six to twelve hours per stage — because metrics converge quickly. Low-traffic systems or systems with noisy metrics need longer stages — 24 to 48 hours per stage — to collect enough data to distinguish signal from noise.

Traffic volume determines how quickly you can trust your metrics. If you serve one million requests per day, one percent is ten thousand requests. At ten thousand requests, quality metrics converge within hours. Latency percentiles are stable. Error rates are measurable. If you serve ten thousand requests per day, one percent is one hundred requests. At one hundred requests, metrics are noisy. A single slow request skews your latency percentiles. A single error doubles your error rate. For low-traffic systems, extend stage duration or increase the one percent cohort to one thousand requests minimum.

Metric variability determines how long you need to observe. If your quality metric has a standard deviation of one percentage point, you can detect a three-point regression with confidence in a few hours. If your quality metric has a standard deviation of five percentage points, you need more data or more time to detect the same regression. Noisy metrics require longer stages. Stable metrics allow shorter stages. Measure your metric variability during baseline periods and use that to set stage durations.

Some teams use time-based stage durations: 24 hours at one percent, 24 hours at ten percent, 24 hours at 50 percent. Some teams use data-based stage durations: advance when you have collected ten thousand requests, or when confidence intervals on key metrics fall below a threshold. Time-based durations are simpler to implement and explain. Data-based durations are more adaptive and handle traffic variability better. Choose based on your infrastructure and team culture.

## Metrics to Monitor at Each Stage

Every stage monitors the same core metrics but with different thresholds and different urgency. Core metrics include quality metrics, latency, error rates, cost, and operational health. Quality metrics are your primary evaluation metrics: accuracy, precision, recall, F1, semantic similarity, BLEU, ROUGE, whatever you track. These are compared to baseline using statistical tests to determine if the difference is significant. Latency is measured at the 50th, 95th, and 99th percentiles. Error rates include HTTP errors, model errors, timeouts, and retries. Cost is measured as tokens per request or dollars per thousand requests.

Operational health metrics include infrastructure metrics: CPU usage, memory usage, GPU utilization, disk I/O, network throughput. If your system uses retrieval, monitor retrieval latency, vector database load, reranking time, and context assembly time. If your system uses external APIs, monitor API latency and rate limits. Operational health issues often precede quality issues. A spike in retrieval latency might degrade response quality before it shows up in your quality metrics. Catching operational issues early prevents quality issues from occurring.

User-facing metrics matter more than system metrics. User satisfaction scores, task completion rates, session length, repeat usage rates, support ticket volume — these metrics capture what users experience. A model that improves your internal quality metric by two percentage points but frustrates users is worse than a model that holds quality steady but improves user satisfaction. Not every team has real-time access to user-facing metrics. If you do, use them. If you don't, invest in getting them.

Some metrics are stage-specific. At one percent, focus on quality and error rates. The sample size is too small to measure cost or infrastructure impact reliably. At ten percent, add cost and user-facing metrics. You have enough data to see if cost projections hold and if users are responding positively. At 50 percent, add infrastructure metrics. You are now handling enough traffic to stress your infrastructure. At 100 percent, monitor everything. Any issue at 100 percent affects all users.

## Promotion Criteria and Decision Logic

Promotion decisions are based on statistical comparisons between the candidate model and the baseline model. For each metric, you compute a confidence interval or a p-value and decide if the difference is significant. If quality metrics are significantly better, promote. If quality metrics are significantly worse, roll back. If quality metrics are not significantly different, the decision depends on secondary metrics. If latency is better and quality is neutral, promote. If cost is lower and quality is neutral, promote. If all metrics are neutral, promote. Neutral is acceptable. Regression is not.

Significance thresholds should be chosen to balance false positives and false negatives. A loose threshold promotes models that regressed slightly. A tight threshold blocks models that improved slightly. Most teams use p-value thresholds of 0.05 or 0.01 for critical metrics and 0.1 for secondary metrics. Some teams use effect size thresholds: promote only if the candidate model improves quality by at least one percentage point. Effect size thresholds prevent promoting models that are statistically better but practically irrelevant.

Automated promotion is possible but risky. If all metrics pass thresholds automatically, the system can promote the candidate model to the next stage without human review. This works well for teams with mature metrics, mature infrastructure, and high deployment frequency. It works poorly for teams with noisy metrics, immature infrastructure, or low confidence in their thresholds. Most teams use automated checks with manual approval: the system computes metrics and flags anomalies, but a human makes the final promotion decision. The human reviews logs, investigates anomalies, and confirms the metrics are trustworthy before approving.

Manual review is not a bottleneck if you design for it. Provide dashboards that show all key metrics, all comparisons to baseline, all confidence intervals, and all anomalies in one place. Provide links to logs, traces, and example requests. Provide a checklist of promotion criteria and pre-fill it with pass or fail for each item. The reviewer should be able to make a decision in five to ten minutes. If review takes longer, your dashboards are missing information or your criteria are unclear.

## Rollback Criteria and Timing

Rollback criteria are the inverse of promotion criteria. If any critical metric regresses significantly, roll back immediately. If multiple secondary metrics regress, roll back. If operational health degrades, roll back. If user-facing metrics degrade, roll back. Rollback is not a failure. Rollback is the system working as designed. The goal of staged rollouts is to catch issues before they affect everyone. Catching an issue and rolling back is success.

Rollback timing matters. The faster you roll back after detecting an issue, the fewer users are affected. Automated rollback is ideal for critical metrics. If error rates spike above a threshold, roll back automatically. If latency exceeds a threshold, roll back automatically. If quality metrics degrade beyond a significance threshold, roll back automatically. Automated rollback requires confidence in your metrics and your thresholds. If your metrics are noisy or your thresholds are poorly calibrated, automated rollback causes false positives and rollback churn.

Manual rollback is safer but slower. A human reviews the issue, investigates the root cause, and decides if rollback is necessary. This prevents rollback on false alarms but introduces delay. The delay might be minutes or hours depending on your on-call process and team responsiveness. For non-critical systems, manual rollback is acceptable. For critical systems, automate rollback for the most important failure modes and use manual rollback for edge cases.

Post-rollback analysis is critical. Every rollback is a learning opportunity. Why did the issue occur? Why did validation miss it? Why did the earlier stages miss it? What can you add to your test suite, your validation dataset, or your promotion criteria to catch this issue next time? Rollback should trigger a retrospective, not blame. The goal is to improve the process, not to punish the team.

## Stage-Specific Test Suites

Each stage can run additional tests beyond the core regression suite. At one percent, run lightweight tests that validate basic correctness on live traffic. These tests check for crashes, malformed outputs, and obvious quality issues. At ten percent, run deeper tests that check for distribution shifts, edge case coverage, and user segment performance. At 50 percent, run load tests that validate infrastructure stability under higher traffic. At 100 percent, run comprehensive tests that cover all quality dimensions, all user segments, and all operational metrics.

Stage-specific tests should not duplicate pre-deployment validation. They should test things that only become visible in production. User behavior patterns, traffic distribution shifts, infrastructure load, API rate limits, database query patterns — these are production phenomena that validation cannot fully replicate. Stage-specific tests catch issues that emerge only under real conditions.

Test execution should be automated and integrated into the promotion pipeline. When a stage completes, the system runs stage-specific tests, computes metrics, compares to baseline, and flags anomalies. The reviewer sees test results alongside metrics and makes a single decision: promote, investigate, or roll back. Fragmented workflows where tests run separately from metrics slow down deployment and increase the chance of missing issues.

Staged rollouts transform deployment from a binary event into a gradual process with multiple decision points. Each stage is a gate. Each gate requires evidence that the model is safe to proceed. The evidence comes from metrics, tests, and human judgment. Together they form a defense-in-depth strategy that catches issues at multiple points. Pre-deployment validation is the first gate. One percent is the second gate. Ten percent is the third. Fifty percent is the fourth. One hundred percent is the final state. Between each gate, you measure, compare, and decide. The next layer of defense is canary validation, where you monitor the canary cohort in real time and detect issues within minutes instead of hours.
