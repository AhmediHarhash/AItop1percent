# 4.6 — Chunking Strategy Regression

In November 2025, a legal tech company updated their chunking strategy for contract analysis. They had been using 512-token chunks with 50-token overlap, a standard configuration that had worked reliably for two years. But their documents were getting longer — 200-page contracts were common — and retrieval latency was increasing. The team decided to increase chunk size to 1024 tokens and reduce overlap to 25 tokens to cut the total chunk count by 60 percent. The change was mathematically sound. Fewer chunks meant faster retrieval and lower storage costs. They deployed the change on a Friday afternoon. By Monday morning, users were reporting that the system was missing clauses. A query for "force majeure provisions" returned chunks that mentioned force majeure but did not contain the actual clause text. A query for "termination notice requirements" returned chunks that discussed termination but omitted the critical notice period. The system's retrieval accuracy had dropped from 91 percent to 73 percent overnight. The root cause was not the chunk size itself. It was that the new chunking boundaries split key legal clauses across adjacent chunks, and the reduced overlap was not sufficient to preserve context. The team rolled back the change, rebuilt the index, and spent three weeks testing chunk configurations against their eval set before trying again. The lesson: chunking strategy is not a tunable parameter. It is a retrieval architecture decision, and changing it without regression testing is like changing your database schema without running migrations.

Chunk size affects what the retrieval system can return. A 512-token chunk can contain a complete product feature description, a single contract clause, or a full FAQ answer. A 1024-token chunk can contain two features, three clauses, or an entire section of a user guide. Larger chunks provide more context, which improves model synthesis quality when the retrieved information is self-contained. But larger chunks also dilute semantic relevance. If a chunk contains three topics and the user query matches only one, the chunk's embedding represents an average of all three topics. The retrieval system ranks it lower than a smaller chunk focused entirely on the relevant topic. Smaller chunks improve precision — they match queries more tightly — but they sacrifice context. If the answer requires information from adjacent paragraphs, a small chunk might retrieve only part of the answer. Changing chunk size shifts this precision-context tradeoff. If your eval set does not test both precision and context completeness, you will not detect the regression until users complain.

## Overlap Changes and Context Continuity

Chunk overlap exists to preserve context across boundaries. If a document is split into chunks without overlap, and a key concept is mentioned at the end of one chunk and explained at the start of the next, neither chunk alone provides a complete answer. Overlap duplicates the boundary region so that both chunks contain the full context. A 512-token chunk with 50-token overlap includes the last 50 tokens of the previous chunk and the first 50 tokens of the next chunk. This ensures that if a concept spans the boundary, at least one chunk captures the full context.

Reducing overlap saves storage and compute — fewer duplicate tokens, fewer chunks to rank — but it increases the risk of context fragmentation. The legal tech company reduced overlap from 50 tokens to 25 tokens, cutting overlap by half. This worked fine for concepts that fit within a single chunk. But for concepts that spanned multiple sentences — a definition followed by examples, a rule followed by exceptions, a clause followed by subclauses — the 25-token overlap was too narrow. The first chunk ended in the middle of the explanation. The second chunk started in the middle. Neither chunk was self-contained. The retrieval system returned partial information, and the model synthesized answers based on incomplete context. Users noticed immediately because legal contracts are dense. Every word matters. A clause without its exceptions is misleading. A rule without its definitions is ambiguous. The reduced overlap broke retrieval for the most important queries.

Testing overlap changes requires an eval set with queries that explicitly target boundary-spanning concepts. Create test cases where the correct answer is split across two chunks in the current configuration. Verify that the system retrieves both chunks and synthesizes a complete answer. Then change the overlap and run the same test cases. If retrieval degrades — if the system now retrieves only one chunk or retrieves both chunks but ranks them low — the overlap is too narrow. Overlap is a safety margin. You can reduce it, but only if your eval set confirms that context continuity is preserved.

## The Re-Chunk-Everything Problem

Changing chunking strategy requires re-chunking the entire knowledge base and rebuilding the index. This is structurally similar to the re-embedding problem covered in 4-3. The documents are the same. The content is the same. But the retrieval artifacts — the chunks and their embeddings — are completely different. A query that previously retrieved chunks 1, 5, and 12 might now retrieve chunks 3, 9, and 15. The semantic content is the same, but the boundaries have shifted. This is not a problem if the new chunks are strictly better. But if the new chunking strategy introduces regressions, you cannot roll back incrementally. You either accept the new chunking or revert to the old chunking. There is no partial migration.

The re-chunking process is expensive. A knowledge base with 50,000 documents might produce 200,000 chunks at 512 tokens or 100,000 chunks at 1024 tokens. Re-chunking all 50,000 documents, generating new embeddings for all 100,000 chunks, and rebuilding the retrieval index can take hours or days depending on the system. During this time, the knowledge base is either offline or running on the old chunks. Teams that do not plan for this downtime often deploy chunking changes during business hours and discover that the migration takes six hours instead of the expected 30 minutes. Users experience downtime or degraded retrieval. Production incidents are filed. The team scrambles to roll back. The chunking change is abandoned.

The safe approach is to **shadow-chunk in parallel**. Before changing the production chunking strategy, run the new chunking logic on a copy of the knowledge base and compare the outputs. How many chunks does the new strategy produce? How do the new chunk boundaries differ from the old ones? Are there documents where key information now spans multiple chunks? Run retrieval eval on both the old and new chunk sets. If the new chunking performs equally or better on the eval set, it is safe to deploy. If it regresses on critical queries, the chunking strategy needs refinement. Shadow-chunking lets you test the change without risking production retrieval quality.

## Testing Chunking Changes Before Deployment

Chunking regression tests should cover three dimensions: precision, context completeness, and boundary quality. **Precision** measures whether the retrieved chunks are relevant to the query. A query for "refund policy" should retrieve chunks that discuss refunds, not chunks that mention refunds in passing. Precision degrades when chunk size increases, because larger chunks contain more tangential content. **Context completeness** measures whether the retrieved chunks contain all the information needed to answer the query. A query for "how to reset a password" should retrieve chunks that explain the full reset flow, not just the first step. Context completeness degrades when overlap decreases or when key information spans boundaries. **Boundary quality** measures whether chunks are semantically coherent — do they start and end at natural breakpoints, or do they cut sentences in half? Boundary quality degrades when chunking logic is naive, such as splitting purely by token count without respecting paragraph or section boundaries.

An effective chunking regression test suite includes queries that stress each dimension. For precision, include queries with narrow scope: "What is the late payment fee?" The correct answer is a single number, and the retrieved chunk should focus on fees, not general billing policies. For context completeness, include queries that require multi-step reasoning: "What happens if I cancel my subscription mid-month?" The answer might require retrieving billing logic, refund policy, and proration rules. For boundary quality, include queries where the answer spans a natural section break: "What are the eligibility requirements and how do I apply?" The eligibility requirements might be one section, the application process another. A well-chunked document preserves both sections in adjacent chunks with sufficient overlap.

Run these tests on both the old and new chunking configurations. Compare precision, recall, and answer quality. If the new configuration improves precision but degrades context completeness, you are trading one failure mode for another. If the new configuration improves storage efficiency but degrades boundary quality, you are saving cost at the expense of retrieval accuracy. The only chunking change worth deploying is one that improves efficiency without regressing quality on the dimensions that matter for your use case.

## Semantic Coherence of Chunks

A chunk is semantically coherent if it represents a complete thought, a complete concept, or a complete unit of information. A chunk that starts mid-sentence and ends mid-sentence is not coherent. A chunk that contains the first half of a list but not the second half is not coherent. A chunk that includes a heading but none of the content under that heading is not coherent. Naive chunking strategies — such as splitting purely by token count — often produce incoherent chunks because they ignore document structure. A 512-token window does not care whether it starts at a paragraph boundary or in the middle of a bullet list. It cuts wherever the token count lands.

Semantic coherence matters because retrieval systems rank by relevance, and incoherent chunks have weak relevance signals. A chunk that contains a heading and three unrelated sentences from different sections embeds as a mixture of topics. Its embedding is less precise than a chunk that contains a heading and the full section content. When a user queries for the topic in the heading, the incoherent chunk ranks lower because its embedding is diluted. When a user queries for one of the tangential sentences, the incoherent chunk ranks higher than it should because it accidentally includes a match. Incoherent chunks create false positives and false negatives. They degrade both precision and recall.

Better chunking strategies respect document structure. They split at section boundaries, paragraph boundaries, or sentence boundaries. They ensure that headings and their content stay together. They ensure that lists are not split in the middle. They ensure that tables and figures remain attached to their captions. This requires parsing the document structure before chunking, which is more complex than token-counting but produces much higher-quality chunks. Libraries like LangChain, LlamaIndex, and Unstructured provide document parsers that extract structure from PDFs, HTML, Markdown, and Word documents. These parsers identify headings, paragraphs, lists, tables, and other structural elements. The chunking logic can then split at structural boundaries instead of arbitrary token offsets. The resulting chunks are semantically coherent, embed better, and retrieve more reliably.

## Chunk Boundary Regression: Information Split Across Chunks

The most common chunking regression is when a change splits critical information across chunks in a way that the retrieval system cannot recover. A user asks a question. The answer is in the knowledge base. But the answer is now divided between two chunks, and the retrieval system ranks both chunks low because neither one fully matches the query. The model synthesizes from incomplete information. The answer is wrong or partial. The user loses trust.

This happens when chunking logic changes without considering how information is distributed in the source documents. A document that describes a multi-step process might have a heading "Steps to Complete Registration" followed by six numbered steps. If the old chunking strategy kept the heading and all six steps in one chunk, that chunk ranked highly for queries about registration. If the new chunking strategy splits the chunk into two — the heading and steps 1-3 in the first chunk, steps 4-6 in the second chunk — neither chunk is sufficient. A query for "how to complete registration" might retrieve the first chunk, and the model synthesizes an answer from steps 1-3 only. The user tries to follow the instructions and gets stuck at step 4 because they never saw steps 4-6. The information was not missing. It was fragmented.

Detecting chunk boundary regression requires eval cases that test for answer completeness. Create test queries where the gold-standard answer is multi-part: "What are all the requirements for eligibility?" The answer might list five requirements. Verify that the retrieval system returns chunks containing all five requirements, not just the first two. If the new chunking strategy splits the requirements across chunks and retrieval fails to return all of them, the chunking change has introduced a boundary regression. The fix is to adjust the chunking logic to keep related information together — either by increasing chunk size, increasing overlap, or improving structure-aware splitting.

## Metadata Preservation During Chunking Changes

Every chunk carries metadata: the source document ID, the page number, the section heading, the author, the last-modified date, the document tags. This metadata helps the retrieval system filter and rank chunks. A query might prefer chunks from recently updated documents, chunks from specific authors, or chunks from specific sections. If the chunking process loses or corrupts metadata, retrieval behavior changes even if the chunk content is identical.

A common failure mode is when chunk metadata is tied to the old chunking boundaries and not recalculated for the new boundaries. A document with 10 pages might produce 20 chunks at 512 tokens or 10 chunks at 1024 tokens. If the old metadata tagged each chunk with a page range, the new chunks need new page ranges. If the old chunking logic preserved section headings in chunk metadata, the new chunking logic must do the same. If the metadata is not regenerated, the new chunks inherit stale metadata that no longer matches their content. A chunk that now spans pages 3-5 might still carry metadata claiming it is from page 3 only. A user filtering for results from page 5 will not see this chunk, even though it contains relevant information.

The safest approach is to regenerate all metadata from scratch during re-chunking. Parse the source document, extract structural and bibliographic metadata, and attach it to each chunk based on the new boundaries. Do not copy metadata from the old chunks. Do not assume that metadata mappings are stable. Treat re-chunking as a full re-ingestion. This is more work, but it prevents silent metadata corruption that leads to retrieval regressions invisible to standard eval tests. Test metadata preservation explicitly: run queries with metadata filters and verify that the correct chunks are returned after re-chunking.

Chunking changes are architecture changes. They affect every query, every retrieval, every generated answer. They cannot be tested by inspection. They cannot be validated by spot-checks. They require automated regression tests that compare old and new chunking on a comprehensive eval set covering precision, context completeness, boundary quality, and metadata correctness. Teams that treat chunking as a configuration parameter instead of a system dependency learn this lesson in production — usually on a Friday afternoon.

The next risk is context window overflow, where increasing model context limits expose latent retrieval bugs that smaller windows concealed — explored in 4.7, Context Window Overflow Testing.

