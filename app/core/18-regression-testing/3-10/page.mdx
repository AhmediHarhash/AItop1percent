# 3.10 â€” Variance Estimation and Noise Floor Modeling

In October 2025, a healthcare company ran their release gate suite twice on the same model without changing a single parameter. The first run showed 87 percent pass rate. The second run, started ten minutes later, showed 91 percent. Engineering celebrated the improvement and nearly shipped a model upgrade based on the apparent four-point gain. The next morning, a senior engineer ran the same test a third time and got 86 percent. Nothing had changed. The improvement was a mirage. The team had been measuring noise.

Every LLM evaluation has inherent variance. The model itself may sample different tokens on repeated queries, even at temperature zero if the provider does not guarantee deterministic outputs. The eval dataset may contain edge cases where multiple answers are equally valid, causing score fluctuations. When using LLM-as-judge scoring, the judge model introduces its own variance, sometimes rating the exact same output differently on different days. Production metrics like user satisfaction or escalation rate vary naturally with traffic patterns, time of day, and user demographics. None of this is signal. All of it is noise.

The mistake most teams make is treating every difference as meaningful. They see a two percent improvement and assume progress. They see a three percent drop and assume regression. They do not know their noise floor. They do not estimate variance. They cannot distinguish a real change from a measurement artifact. This leads to three failure modes. First, they block releases that are actually fine because natural variance produced a false negative. Second, they ship regressions because natural variance produced a false positive. Third, they lose trust in the release gate entirely because it gives inconsistent results and everyone starts ignoring it.

The principle is simple. If you do not measure variance, you do not know what size of difference matters. If you do not model the noise floor, you cannot set meaningful thresholds. And if your quality gate sensitivity is below your measurement noise, your gate is decorative, not functional.

## Sources of Variance in LLM Evaluation

LLM outputs vary for three fundamental reasons, each contributing to the total variance you observe in your metrics. First is model stochasticity. Even when you set temperature to zero, many provider APIs do not guarantee byte-for-byte identical outputs on repeated queries with the same prompt. GPT-5 running on different backend instances may produce slightly different token probabilities due to numerical precision differences in GPU floating-point operations. Claude Opus 4.5 may use different internal sampling methods depending on load balancing. The result is that the same model, given the same input, can produce different outputs. This variance is usually small for deterministic settings but grows significantly for creative or generative tasks.

Second is eval randomness. Your eval dataset contains cases of varying difficulty. Some queries have clean, unambiguous correct answers. Others sit in gray zones where multiple responses are defensible. A model that scores 90 percent on one random sample of 500 queries might score 88 percent or 92 percent on a different random sample from the same distribution. This is sampling variance, and it decreases with larger eval sets, but never reaches zero. A 500-example eval has higher variance than a 5,000-example eval, which has higher variance than a 50,000-example eval. If your eval set is too small, you are measuring the sample, not the model.

Third is judge inconsistency. When using LLM-as-judge scoring, the judge model itself is stochastic. Ask GPT-5 to rate a response on a 1-to-5 scale twice, and you may get different scores. The judge may interpret ambiguous rubric wording differently on different runs. The judge may have been updated by the provider between your two test runs, changing its behavior slightly. If your scoring depends on an external API, you inherit all of that API's variance. If you use multiple judges for the same task and average their scores, you reduce variance but do not eliminate it. Judge variance is often the largest single contributor to eval noise, especially for subjective dimensions like tone, helpfulness, or clarity.

These three sources combine. A model with inherent stochasticity is evaluated on a finite sample by a stochastic judge. The result is compound variance. You observe a score, but that score is drawn from a distribution. The true underlying model quality is a parameter you are estimating, not directly measuring. If you treat every observed score as exact truth, you are ignoring measurement error. That error propagates into your release decisions.

## The Control-Control Comparison and Noise Floor

The most direct way to estimate your variance is the control-control comparison. Run your entire eval suite on the same model twice without changing anything. Same prompts, same version, same infrastructure. Compare the two runs. The difference you observe is pure measurement noise. This is your noise floor, the minimum detectable difference your testing system can reliably see.

A fintech company in early 2026 ran this experiment on their fraud detection agent. They evaluated 2,000 queries on a Tuesday morning, then re-ran the exact same eval on Tuesday afternoon. The observed accuracy shifted from 89.4 percent to 88.7 percent. The delta was 0.7 percentage points. That became their noise floor estimate. Any model change that produced less than a one-point shift in accuracy was indistinguishable from measurement error. Their release gate threshold was set at three points, well above the noise floor, to ensure that flagged regressions were real.

This approach surfaces uncomfortable truths. A legal services company ran control-control on their contract summarization eval and observed a four percent variance in their summary quality score, which was LLM-judge rated on a 100-point scale. Four points felt large. It meant that any improvement smaller than four points could not be proven. It also meant that their previous release gate, which blocked changes greater than two points, had been triggering on noise. They had been blocking safe releases based on measurement artifacts. After updating their threshold to six points, false positives dropped to near zero, and engineering trust in the gate recovered.

The noise floor is not a single number. It varies by metric. Exact-match accuracy has low variance. LLM-judge helpfulness scores have high variance. Latency has medium variance but with occasional outliers. User satisfaction ratings have high variance because they depend on external user behavior, not just model quality. You must estimate the noise floor separately for each metric you care about. A release gate that uses ten metrics needs ten noise floor estimates. Aggregating them into a single overall score without accounting for per-metric variance hides which signals are reliable and which are noise-dominated.

Once you know your noise floor, you set release thresholds above it. A common heuristic is to set the threshold at three times the observed noise floor standard deviation. If your control-control runs show a standard deviation of 0.5 percent, your threshold is 1.5 percent. This gives you high confidence that differences above the threshold are real. Differences below the threshold are treated as statistically insignificant, and the release proceeds. This is not about being lenient. It is about being correct. Blocking a release because of noise wastes engineering time and destroys trust. Shipping a regression because you could not detect it wastes user trust. Setting thresholds informed by variance estimates avoids both.

## Run-to-Run Variance and Temporal Stability

Even after controlling for model version and dataset, eval scores drift over time due to factors outside your control. Provider infrastructure changes. API latency shifts. Judge models get updated. External dependencies like embedding APIs or retrieval services experience load variation. A model evaluated on Monday and again on Friday may show different performance even though the model itself has not changed.

A customer support company tracked this by running the same eval weekly for three months without changing their model. They used a frozen snapshot of 1,000 support tickets and measured resolution accuracy, response time, and tone consistency. Accuracy was stable, varying only one percent week to week. Response time was less stable, with a five percent coefficient of variation due to API latency changes. Tone consistency, scored by an LLM judge, varied eight percent, driven entirely by judge model updates from the provider. By the end of three months, they had a clear picture of which metrics were stable and which were not. The stable metrics became their primary release gate criteria. The unstable metrics were tracked but not used as hard blockers.

This kind of longitudinal variance tracking reveals which parts of your eval are robust and which are fragile. Fragile metrics are still useful for directional guidance, but they should not gate releases. Robust metrics with low temporal variance become the backbone of your quality gate. A metric that swings five percent week to week without any real change is not a reliable regression detector. A metric that stays within one percent week to week and only moves outside that range when you change the model is a reliable signal. You want your release gates built on reliable signals.

Temporal stability also matters for baseline comparisons. If your production baseline was measured six months ago and your metrics have drifted due to infrastructure changes, your comparison is invalid. A common pattern is to refresh the baseline monthly by re-evaluating the production model on the current test suite with current infrastructure. This ensures that the baseline reflects the same measurement environment as your candidate models. A three percent regression detected against a stale baseline might actually be a one percent regression against a fresh baseline. Fresh baselines keep your gates accurate.

## Judge Variance and Strategies for Reduction

When LLM-as-judge is your primary scoring mechanism, judge variance becomes your primary variance problem. A single judge model asked to score the same output twice may give different scores due to temperature settings, prompt interpretation, or internal stochasticity. A judge prompt that says "rate this response from 1 to 10" without detailed rubrics invites interpretation variance. A judge that compares two responses and picks the better one has lower variance than a judge that assigns absolute scores, because pairwise comparison reduces ambiguity.

One reduction strategy is to use deterministic judge settings. Set the judge temperature to zero. Use the same judge model version across all comparisons. Pin the API version to avoid automatic updates. Store judge responses and reuse them when re-evaluating the same candidate outputs. A deterministic judge does not eliminate variance entirely because even temperature zero is not perfectly deterministic on all APIs, but it reduces it significantly. A media company cut their judge variance from twelve percent to four percent by pinning the judge model version and setting temperature to zero.

A second strategy is to use multiple judges and average their scores. If you have three independent judge models evaluate each output and take the mean score, variance decreases by a factor of the square root of the number of judges. Three judges reduce variance by about 1.7x. Five judges reduce it by about 2.2x. Diminishing returns set in quickly, so five is usually the practical maximum. The judges should be independent, meaning different models or different prompt phrasings, to avoid correlated errors. Three identical judges with the same prompt do not reduce variance because they make correlated mistakes.

A third strategy is to use larger eval sets. Variance decreases with the square root of sample size. Doubling your eval from 1,000 examples to 2,000 reduces variance by about 1.4x. Going from 2,000 to 8,000 reduces it by 2x. This is expensive in compute cost and latency, so it is a tradeoff. For critical release gates, the cost is justified. For exploratory eval during development, smaller sets with higher variance are acceptable because you are not making binary ship-or-block decisions based on them.

A fourth strategy is to use simpler scoring rubrics with less room for interpretation. Instead of "rate the helpfulness of this response on a 1 to 10 scale," use "does this response answer the user's question: yes or no." Binary judgments have lower variance than scaled judgments. Instead of "rate the tone from very negative to very positive," use "does this response contain hostile language: yes or no." Simplifying the judgment task reduces the space in which the judge can vary. The tradeoff is loss of nuance. Binary scores do not capture gradations of quality. For release gates, where the goal is to detect regressions rather than measure fine-grained improvements, binary or three-level scales often suffice.

## When High Variance Means High Risk

High variance is not just a measurement problem. It is also a signal about your system. If your model's outputs vary wildly on repeated queries with the same input, your system is unreliable. If your eval scores swing five percent day to day without any changes, your eval is fragile. If your judge gives inconsistent ratings, your judge is untrustworthy. High variance surfaces instability. The correct response is not to ignore the variance but to investigate its root cause and either stabilize the system or change the measurement.

A legal AI company discovered that their contract clause extraction task had 15 percent run-to-run variance. They initially blamed the judge model. After investigation, they found that the variance was coming from the model itself. The same input clause produced different extracted entities on different runs because the prompt was ambiguous and the model was using high temperature for creativity. The variance was a feature flag, not a bug in measurement. They fixed it by rewriting the prompt for clarity and lowering the temperature to 0.2. Post-fix, variance dropped to three percent. The high variance had been a symptom of a real production risk. Measuring it forced them to fix the underlying instability.

Another pattern is high variance in low-frequency edge cases. A support chatbot showed stable performance on common queries but 20 percent variance on rare edge cases. The edge cases were hard to score because they involved domain-specific jargon, ambiguous user intent, or incomplete context. The high variance signaled that these cases were not well-handled by the model or the eval. The team flagged them for human review and excluded them from automated release gates. The lesson: variance heterogeneity across your dataset tells you which parts of your system are stable and which are not. Aggregating variance into a single global number hides this structure.

High judge variance, specifically, is a red flag that your scoring rubric is underspecified or your judge model is unsuitable for the task. A healthcare company used a general-purpose language model to judge medical accuracy and saw 18 percent variance in scores. They switched to a medically fine-tuned judge model and variance dropped to six percent. The general model lacked the domain knowledge to make consistent judgments. The fine-tuned model had internalized medical reasoning, leading to more stable scores. If your judge variance is high, consider whether your judge is appropriate for the domain. A weak judge is worse than no judge because it creates false confidence.

## Setting Thresholds Above the Noise Floor

Once you have estimated your noise floor, threshold selection becomes a statistical decision rather than a guess. The threshold must be large enough that observed differences exceeding it are unlikely to be due to chance. A standard approach is to set the threshold at the noise floor mean plus two or three standard deviations. If your control-control runs show a mean difference of 0.5 percent with a standard deviation of 0.3 percent, a threshold of 1.4 percent gives you 99 percent confidence that differences above it are real, assuming a normal distribution.

This calculation requires repeated measurements. You cannot estimate variance from a single control-control run. You need at least five, preferably ten, independent runs of the same model to estimate the distribution of measurement noise. A logistics company ran their eval on the same model 15 times over two weeks and computed the standard deviation of each metric. They found that accuracy had a standard deviation of 0.4 percent, latency had a standard deviation of 12 milliseconds, and user satisfaction had a standard deviation of 2.1 percentage points. They set thresholds at three standard deviations above the mean for each metric: 1.2 percent for accuracy, 36 milliseconds for latency, and 6.3 percentage points for satisfaction. Any candidate model that regressed beyond those thresholds was blocked. Anything within those bounds was treated as statistically equivalent to the baseline.

The tradeoff is between sensitivity and false positive rate. A tight threshold catches small regressions but triggers on noise, causing false positives. A loose threshold ignores noise but misses small regressions, causing false negatives. The optimal point depends on the cost of each error type. If a false positive blocks a valuable release and costs two engineering days to investigate, and a false negative ships a regression that causes user churn worth ten thousand dollars, you should tolerate more false positives. If the costs are reversed, you should tolerate more false negatives. Most teams underestimate the cost of false positives because it is paid in engineering time rather than user impact, leading them to set thresholds too tight and trigger on noise.

A refinement is to use confidence intervals instead of point thresholds. After evaluating a candidate model, compute the 95 percent confidence interval for each metric. If the baseline value falls within the confidence interval, treat the candidate as statistically indistinguishable from the baseline and allow the release. If the baseline value falls outside the lower bound of the confidence interval, flag a regression. This approach naturally accounts for sample size and variance. A large, stable eval produces tight confidence intervals and high sensitivity. A small, noisy eval produces wide confidence intervals and low sensitivity, preventing overinterpretation of weak signals.

## Variance Reduction Strategies in Practice

Beyond statistical corrections, operational practices reduce variance. First, use fixed, versioned datasets for release gates. Do not sample a new random subset of production data for each eval. Sampling variance dominates measurement variance in small datasets. A fixed set of 2,000 queries evaluated consistently across all models has lower variance than a randomly sampled set of 2,000 queries drawn fresh each time. The fixed set should be refreshed quarterly to stay representative, but within a quarter, it should be frozen.

Second, run evals at the same time of day. If your eval includes API calls to external services, those services have variable latency depending on time of day and load. A model evaluated at 3am when latency is low may appear faster than the same model evaluated at 3pm when latency is high. Schedule all release gate evals to run during a consistent time window, preferably during off-peak hours when infrastructure load is stable.

Third, use multiple runs and average the results. Instead of evaluating each candidate model once, evaluate it three times and report the mean score. This reduces variance by the square root of three, about 1.7x. The tradeoff is cost and latency. If your eval takes 20 minutes and costs 50 dollars, three runs take an hour and cost 150 dollars. For a release gate making a binary deploy decision, the cost is justified. For exploratory development evals, a single run suffices.

Fourth, separate metrics by variance tier. Create three tiers: low-variance metrics that are stable and reliable, medium-variance metrics that are useful but noisy, and high-variance metrics that are directional but not trustworthy. Use only low-variance metrics for hard release gates. Use medium-variance metrics for alerts and investigation triggers. Use high-variance metrics for trend monitoring but not for binary decisions. A customer service platform classified exact-match accuracy as low-variance, LLM-judge helpfulness as medium-variance, and user satisfaction as high-variance. Only accuracy gated releases. Helpfulness triggered reviews. Satisfaction was tracked but not actionable.

Fifth, continuously monitor and update your variance estimates. Variance is not static. It changes as your infrastructure evolves, your dataset grows, your judge models get updated, and your model distribution shifts. A variance estimate from six months ago may no longer be accurate. Recompute noise floor estimates quarterly by running fresh control-control experiments. Update thresholds based on the new estimates. A media company found that their variance dropped by 40 percent after migrating to a dedicated eval infrastructure with pinned dependencies. They tightened their thresholds accordingly and caught regressions they would have missed with the old, looser thresholds.

The goal is not to eliminate variance, which is impossible, but to understand it, quantify it, and ensure your decision thresholds are robust to it. A release gate that ignores variance is either too sensitive, blocking safe releases, or too loose, missing real regressions. A release gate that models variance and sets thresholds above the noise floor is both sensitive and specific, catching regressions without false alarms.

What happens when you run dozens or hundreds of tests simultaneously, each with its own variance and threshold? The probability of a false positive on any one test is low, but the probability of at least one false positive somewhere in the suite is high. This is the multiple testing problem, and it requires corrections to maintain gate reliability across the full test suite.

