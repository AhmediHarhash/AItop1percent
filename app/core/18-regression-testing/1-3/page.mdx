# 1.3 â€” The Baseline Challenge: Defining Expected Behavior

You cannot detect regression without a baseline. This is not philosophical. It is mechanical. Regression is defined as deviation from a reference point. If you do not have a reference point, you cannot measure deviation. You can measure absolute performance. You can say the new model is 89 percent accurate. But you cannot say whether that is better or worse than before unless you know what before was. Most teams do not have a before. They have production. They have users. They have a model that is running. But they do not have a frozen, documented snapshot of that model's behavior that they can use as a reference point for regression detection. This is the baseline challenge.

## Why "Production Today" Is Not a Baseline

The mistake most teams make is treating production as their baseline. They say "we will compare the new model to production" and assume that is sufficient. It is not. Production is a moving target. The traffic distribution changes daily. User behavior shifts. Seasonal patterns emerge. Infrastructure performance varies. The model itself might be nondeterministic, producing different outputs for the same input depending on sampling variance. If you run your eval suite against production today and again tomorrow, you will see different results even if nothing changed. That variance makes it impossible to distinguish between real regression and random drift.

A baseline must be frozen. It must be a snapshot taken at a specific point in time, with a specific model version, a specific prompt version, specific configuration parameters, and a specific eval set. You run the baseline once, capture the outputs, store them, and never change them. When you test a new model, you run it against the same eval set with the same inputs. You compare the new outputs to the frozen baseline outputs. The comparison is valid because the eval set is identical and the baseline is static. If you see a 2 percent drop in accuracy, you know it is because the new model performs worse, not because the eval set changed or because random variance made the baseline look different today than it did last week.

The teams that treat production as a baseline spend months chasing ghosts. They see a drop in production metrics, assume they shipped a regression, roll back, and the metrics do not recover. Why? Because the drop was not caused by the new model. It was caused by a shift in traffic distribution, a change in user behavior, or a seasonal pattern. Without a frozen baseline, they have no way to isolate the model's contribution to the metric change. They are flying blind.

## What a Validated Baseline Looks Like

A validated baseline has six components. First, a locked model version. You must know exactly which model you are comparing against. Not "the GPT-5 we used in October" but "GPT-5.1 with model ID gpt-5-1106" or "our fine-tuned Llama 4 Scout checkpoint from December 3, 2025, commit hash 7a4f9d2." If you cannot reproduce the exact model, you cannot use it as a baseline. This is why teams that rely on hosted models like GPT-5 or Claude Opus 4.5 sometimes struggle: the provider can update the model under the same name, and suddenly your baseline is different from what you thought it was. If you are using a hosted model, you need to lock to a specific versioned snapshot that the provider guarantees will not change. If the provider does not offer that, you need to save the outputs from the baseline model and treat those outputs as your ground truth.

Second, a locked prompt. The prompt includes the system message, any few-shot examples, the user message template, and the formatting. Change a single word and you change the model's behavior. The baseline prompt must be saved as a text file, versioned in source control, and never modified. If you want to test a new prompt, you compare it to the baseline prompt. You do not update the baseline to match the new prompt. The baseline is the reference. The new prompt is the candidate. If the candidate improves on the baseline, you ship it and create a new baseline for future comparisons. But you do not rewrite history by changing what the baseline was.

Third, locked configuration parameters. Temperature, top-p, max tokens, stop sequences, presence penalty, frequency penalty, and any other sampling or generation parameters must be documented and frozen. A model run at temperature 0.7 behaves differently than the same model run at temperature 0.9. If your baseline used 0.7 and your new model uses 0.9, you are not comparing like to like. The differences you see might be due to the temperature change, not to the model or prompt change. You need to isolate variables. The baseline locks one complete configuration. The new model changes exactly one variable at a time. If you want to test a new model and a new temperature, you run two tests: one with the new model and old temperature, one with the new model and new temperature. This is the only way to understand which change caused which effect.

Fourth, a locked eval set. The eval set is the collection of inputs you run through the model to measure behavior. It must be representative of production traffic, large enough to detect meaningful regressions, and diverse enough to cover edge cases. The eval set must be frozen. You do not add new examples, remove old examples, or modify existing examples. If you change the eval set, you invalidate all previous baseline results because you are no longer measuring the same thing. Adding harder examples makes the baseline look worse. Adding easier examples makes it look better. Neither change reflects real model regression. They reflect eval set drift.

Fifth, captured outputs. The baseline is not just the model and prompt and configuration and eval set. The baseline is the actual outputs the model produced when you ran that configuration on that eval set. You must capture and store those outputs. This is your ground truth for comparison. When you run the new model, you compare its outputs to the stored baseline outputs. You do not re-run the baseline model and compare the two live runs. Re-running introduces variance. The baseline model might produce different outputs the second time due to nondeterminism. Captured outputs eliminate that variance. They give you a stable reference point.

Sixth, documented thresholds. A baseline without thresholds is useless. You need to define what counts as acceptable deviation. Is a 1 percent drop in accuracy a regression? A 2 percent drop? A 0.5 percent drop? Is a 30 millisecond increase in latency acceptable? A 50 millisecond increase? Is a 0.005 dollar increase in cost per request acceptable? You need numbers. You need stakeholder agreement on those numbers. You need to document them in the same place you document the baseline. The thresholds are part of the baseline definition. Without them, you have data but no decision framework.

## How to Capture a Baseline

The process of capturing a baseline is straightforward but requires discipline. You choose a point in time when the system is stable, performing acceptably, and representative of the behavior you want to maintain. This is usually after a successful release, after you have addressed major bugs, and before you start experimenting with new changes. You freeze the code. You lock the model version, prompt, and configuration. You select or generate your eval set. You run the model on every example in the eval set. You capture the outputs, the metadata (latency, token count, cost), and the eval results (accuracy, safety scores, behavioral flags). You store everything in a structured format: a JSONL file, a database table, a cloud storage bucket. You version the baseline with a unique identifier: "baseline-2025-12-15-v1" or "baseline-production-checkpoint-7a4f9d2."

You document what the baseline represents. You write a README or a wiki page that explains: this baseline was captured on December 15, 2025. It uses GPT-5.1 with model ID gpt-5-1106. It uses the customer-service-v3 prompt with system message stored in prompts/customer-service-system.txt. It uses temperature 0.7, top-p 0.9, max tokens 300. It uses the cs-eval-set-v2 eval set with 1,200 examples stored in eval-sets/cs-eval-set-v2.jsonl. It achieved 91.3 percent accuracy, 340 milliseconds median latency, 0.014 dollars per request median cost, 99.7 percent safety pass rate, and 87.2 percent user satisfaction proxy metric. The acceptable regression thresholds are: accuracy must not drop below 89.5 percent, latency must not exceed 400 milliseconds at the 95th percentile, cost must not exceed 0.018 dollars per request, safety pass rate must not drop below 99.0 percent, user satisfaction proxy must not drop below 85.0 percent.

This documentation becomes the contract. When someone wants to ship a new model, they run it against the eval set, compare the results to the baseline, and check whether any metric violates a threshold. If all thresholds pass, the change is safe to ship. If any threshold fails, the change is rejected, or you escalate to stakeholders to approve an exception. The baseline makes the decision process mechanical. You do not argue about whether the new model is good enough. You measure whether it meets the documented thresholds. If it does, it ships. If it does not, it does not.

## The Problem of Baseline Drift

A baseline is only valid as long as it represents the behavior you want to maintain. Over time, the baseline can become stale. Your user base grows. Your product expands to new use cases. Your stakeholders raise the quality bar. The model that was acceptable six months ago is no longer acceptable today. When this happens, you need to establish a new baseline. But you cannot do this casually. If you update the baseline every time you ship a change, you lose the ability to detect regression because you are constantly redefining what acceptable looks like.

The discipline is to update baselines deliberately and infrequently. Most teams update baselines once per quarter or once every six months. You choose a release that represents a meaningful step forward in quality, performance, or capability. You capture a new baseline from that release. You document the change. You communicate to the team that the old baseline is deprecated and the new baseline is now the reference point. You archive the old baseline so you can still refer to it for historical analysis, but you do not use it for regression testing going forward.

Some teams maintain multiple baselines. They have a production baseline that represents the current live system, a candidate baseline that represents the next planned release, and a stretch baseline that represents aspirational performance. When testing a new model, they compare it to all three baselines. The model must not regress relative to the production baseline. If it improves relative to the candidate baseline, that is good. If it approaches the stretch baseline, that is excellent. This multi-baseline approach gives you more context for decision-making. A model that regresses 1 percent relative to production but improves 3 percent relative to the previous candidate might still be worth shipping. A model that improves 2 percent relative to production but is still 8 percent behind the stretch baseline tells you that you are making progress but have not reached your goal yet.

## The Trap of Vague Baselines

The trap most teams fall into is defining baselines vaguely. They say "the baseline is how the model performed last month" but they do not have the outputs. They say "the baseline is the production model" but they do not know which version. They say "the baseline is around 90 percent accurate" but they do not have the exact number, the eval set used to measure it, or the confidence interval. Vague baselines are worse than no baseline because they create the illusion of rigor without the substance. You think you are detecting regression, but you are actually just comparing two poorly measured numbers and guessing whether the difference is meaningful.

Vague baselines lead to endless debates. Engineering says the new model is 92 percent accurate, better than the baseline of 90 percent. Product says the baseline was 93 percent, so this is a regression. Trust and Safety says they remember the baseline being 89 percent, so this is an improvement. Nobody has the data to resolve the disagreement. You waste three days in meetings, eventually ship the model because the deadline has passed, and hope for the best. Two weeks later, users complain that the model is worse. You do not know if they are right because you never established what the baseline actually was.

The solution is to treat baseline creation as a formal process. You do not capture a baseline informally. You do not estimate baseline performance from memory. You do not average together three different measurements from three different weeks and call that the baseline. You run a baseline capture process. You store the results. You version them. You document them. You treat the baseline as an artifact, as important as the model itself. Without the baseline, the model is unshippable because you have no way to verify that the next version is not worse.

## Baseline as Organizational Contract

A validated baseline is more than a technical artifact. It is an organizational contract. It says: this is the behavior we have agreed is acceptable. This is the reference point for all future decisions. If you want to ship a change that makes the model worse in any measured dimension, you need to justify why the regression is acceptable. You need stakeholder buy-in. You need to document the trade-off. You cannot just ship it and hope nobody notices.

The baseline removes ambiguity. Before baselines, teams shipped changes based on intuition. The engineer thought the new model was better, so they shipped it. The product manager thought the new prompt improved user experience, so they approved it. The executive thought the new strategy would save money, so they mandated it. None of these decisions were grounded in measurement. The baseline forces measurement. It forces comparison. It forces accountability. If you ship a regression, it is visible. If you improve on baseline, it is visible. The data replaces intuition.

The best teams treat baseline violations as incidents. If a release candidate regresses on any threshold, that triggers a review. Engineering investigates why the regression occurred. Product evaluates whether the trade-off is acceptable. Trust and Safety evaluates whether the safety regression introduces unacceptable risk. Finance evaluates whether the cost regression fits within budget. The decision to ship despite a regression is made deliberately, with full knowledge of what you are giving up. It is not made by accident because nobody measured the regression in the first place.

The next subchapter covers the five types of regression tests every AI system needs and how to structure them into a regression suite that catches failures before they reach production.
