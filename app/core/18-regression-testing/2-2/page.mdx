# 2.2 — Golden Set Size: The 30 to 100 Rule

Too small and you chase noise. Too large and you burn budget. The sweet spot for most regression golden sets is 30 to 100 carefully designed cases. This range is not arbitrary. It reflects the statistical requirements for detecting meaningful regressions, the practical constraints of high-quality ground truth curation, the cost of running evals frequently, and the operational reality that larger sets decay faster than you can maintain them.

Most teams get this wrong in both directions. Some teams build golden sets with 10 examples because they think regression testing is lightweight. They run their suite, see two failures, and cannot tell whether that represents a 20 percent regression or random variance from a noisy baseline. Other teams build golden sets with 500 examples because they think more is always better. They spend three weeks curating the set, six months letting it go stale, and another two weeks investigating why half the examples no longer reflect production behavior. Both mistakes are expensive. Both are avoidable.

## Why 30 Is the Minimum

Thirty examples is the statistical minimum for detecting regressions with reasonable confidence. Below 30, the sample size is too small to distinguish signal from noise. If your baseline model performs correctly on 27 out of 30 examples, that is 90 percent accuracy. If your new model performs correctly on 25 out of 30 examples, that is 83 percent accuracy. Is that a regression? Maybe. The confidence interval is wide. Run the test again with different random seeds, and you might get 26 out of 30 or 28 out of 30. The observed difference could be real degradation or random variance. You cannot tell.

At 30 examples, you can detect large regressions — 10 to 15 percentage points — with moderate confidence. You cannot detect small regressions. If your baseline accuracy is 90 percent and your new model accuracy is 87 percent, a 30-example set will not reliably catch it. You need more examples to detect smaller changes. But 30 is enough to catch catastrophic failures. If your new model drops from 90 percent to 70 percent accuracy, 30 examples will surface that. The model will fail on 9 examples instead of 3. That difference is visible and actionable.

Thirty examples is also the practical minimum for coverage. If your system handles 5 distinct intent categories and you want at least 5 examples per category, you need 25 examples just for basic coverage. Add 5 examples for edge cases and you are at 30. Go below 30 and you are either skipping intent categories or testing each category with so few examples that random variance dominates. Neither option is acceptable.

The failure mode of sub-30 golden sets is that you cannot trust the results. Your regression suite will flag regressions that are not real because a single unlucky random sample shifted the pass rate. It will miss real regressions because the set is too small to detect them. Engineering will stop trusting the suite. They will override failures because the last three times the suite failed, the failures were false positives. Once trust is lost, the regression suite becomes a checkbox exercise instead of a quality gate. The team will go through the motions, ignore the results, and ship regressions anyway.

## Why 100 Is Often the Ceiling

One hundred examples is the practical ceiling for high-frequency regression testing. Above 100, the cost and latency of running the suite starts to constrain how often you can test. If each example takes 2 seconds to evaluate and you have 200 examples, your regression suite takes 6 to 7 minutes to run. If you are testing 5 model candidates per day, that is 35 minutes of eval time per day. If you include multiple runs for statistical confidence, it is over an hour. The suite becomes slow enough that engineers stop running it on every change. They batch changes, test less frequently, and ship multiple changes together without knowing which one caused a regression when the suite fails.

One hundred examples is also the ceiling for manual ground truth curation. Every example in a golden set needs verified, defensible ground truth. That means expert review. For complex domains — medical, legal, financial — expert review costs 50 to 200 dollars per hour. Annotating one example might take 5 to 15 minutes depending on complexity. Annotating 100 examples at 10 minutes each is 16 hours of expert time. At 100 dollars per hour, that is 1,600 dollars. Annotating 500 examples is 8,000 dollars. Most teams cannot justify spending 8,000 dollars on a single golden set, especially when the set needs to be refreshed every 6 months as the product evolves.

One hundred examples is also near the ceiling for effective maintenance. Golden sets decay. Production behavior shifts. New edge cases emerge. Old examples become irrelevant. You need to review and update your golden set quarterly or it goes stale. Reviewing 100 examples takes a few hours. Reviewing 500 examples takes days. The larger your golden set, the less likely you are to maintain it, and unmaintained golden sets become liabilities. They test for behavior your system no longer exhibits. They miss behavior your system now depends on. They flag regressions that are not regressions because the ground truth is outdated. A 100-example set that is well-maintained is far more valuable than a 500-example set that has not been reviewed in a year.

## The 50 to 70 Sweet Spot

Within the 30 to 100 range, most teams converge on 50 to 70 examples. This size is large enough to detect regressions of 5 to 10 percentage points with good confidence. It is small enough to curate carefully, run frequently, and maintain quarterly. It is large enough to cover 6 to 10 intent categories with meaningful depth per category. It is small enough that engineers can review the full set in an afternoon when investigating a failure. It balances statistical power, operational cost, and maintainability.

At 50 examples, you can detect a regression from 90 percent baseline accuracy to 82 percent new accuracy with 80 percent confidence. At 70 examples, you can detect a regression from 90 percent to 84 percent with 80 percent confidence. These are meaningful regressions. They are the kind of degradation that users notice and stakeholders reject. Smaller regressions — 90 percent to 88 percent — might require 100 to 150 examples to detect reliably. Whether you need to detect regressions that small depends on your domain. For high-stakes applications where 2 percent degradation is unacceptable, you build a larger set. For lower-stakes applications where you can tolerate 5 percent degradation, a smaller set suffices.

The 50 to 70 range also aligns with human cognitive limits. A reviewer can hold 50 examples in working memory well enough to reason about coverage, spot gaps, and detect patterns. At 200 examples, the set is too large to reason about holistically. You lose the intuitive sense of whether the set is representative. You cannot easily answer questions like "do we have enough examples of refusal cases?" or "are we testing multi-step reasoning adequately?" With 50 to 70 examples, you can print the full set, review it on paper, and get a sense of the whole. This makes maintenance, coverage analysis, and stakeholder communication much easier.

## When to Go Smaller

You go smaller when budget or latency constraints dominate. If you are a small team with no eval budget, you might start with 30 examples and grow from there. If you need sub-10-second regression tests because you are testing dozens of model candidates per day, you might cap your golden set at 40 examples. If your domain experts are extremely expensive or scarce, you might limit the set to what you can afford to annotate. Thirty examples is acceptable if those 30 examples are exceptionally well-curated, cover the highest-risk scenarios, and are maintained rigorously. A small, high-quality golden set is better than a large, low-quality one.

The failure mode of going too small is that you miss regressions. Your suite will catch catastrophic failures but miss moderate degradation. You will ship changes that reduce accuracy by 5 to 8 percentage points without detecting them until users complain. Whether that is acceptable depends on your risk tolerance. For experimental products in beta, it might be fine. For production systems serving enterprise customers, it is not.

## When to Go Larger

You go larger when you need to detect small regressions or when you have many distinct scenarios to cover. If your system handles 15 intent categories and you need strong coverage per category, you might need 100 to 150 examples. If your stakeholders require detection of regressions smaller than 5 percentage points, you need 100 to 200 examples to achieve statistical power. If you operate in a highly regulated domain where compliance requires evidence of comprehensive testing, you might build a 200-example golden set and run it weekly instead of daily.

Larger golden sets are also justified when eval cost is negligible compared to regression cost. If running a 200-example regression suite costs 8 dollars and missing a regression costs you 50,000 dollars in incident response and customer appeasement, the 8 dollars is a rounding error. You should test as comprehensively as you can afford. But this logic only applies if you can actually maintain the larger set. A 200-example golden set that is well-maintained is worth the cost. A 200-example golden set that is six months out of date is worse than useless because it creates false confidence.

## How to Prioritize When You Must Stay Small

If you are constrained to 30 to 50 examples, you must prioritize ruthlessly. Every example must justify its inclusion. The prioritization heuristic is: **risk times frequency**. Examples that represent high-risk scenarios get priority even if they are rare. Examples that represent common scenarios get priority even if the risk per instance is moderate. Examples that are both high-risk and common are mandatory.

Start with business-critical scenarios. If losing a specific customer would cost you a million dollars in annual revenue, include examples that represent that customer's primary use cases. If a compliance violation triggers regulatory fines, include examples that test compliance-critical behavior. If a specific failure mode caused a production incident, include examples that would have caught that failure mode. These are non-negotiable. They go into the golden set regardless of how many examples you have.

Next, add representative coverage of your intent distribution. If 40 percent of production traffic is intent A, include enough examples of intent A to detect regressions in that category. If 10 percent of traffic is intent B, include at least 2 to 3 examples of intent B. You do not need proportional representation when the set is small, but you need some coverage of every significant intent. Skipping an intent category means you are blind to regressions in that category.

Finally, add edge cases that expose known model weaknesses. If your model struggles with negation, include negation examples. If your model struggles with multi-hop reasoning, include multi-hop examples. If your model struggles with ambiguous inputs, include ambiguous examples. These examples are your canaries. When the model regresses, it often regresses on edge cases first. If your golden set includes no edge cases, you will not catch regression until it spreads to common cases, by which time the damage is larger.

## The Dynamic Golden Set Trap

Some teams try to solve the size problem by building dynamic golden sets that change based on recent production traffic. They sample the last 100 production queries, annotate them, and use them as the golden set. This approach is tempting because it ensures the set reflects current production distribution. It also destroys the stability property that makes golden sets useful. If the set changes every week, you cannot compare results across weeks. You are measuring two variables — model performance and set composition — and you cannot isolate which one caused the observed change.

Dynamic golden sets are useful for monitoring production drift. They are not useful for regression testing. Regression testing requires a stable baseline. The baseline must be constant so that changes in model behavior are the only variable. If you want to track production drift, build a separate drift monitoring pipeline that samples production continuously. Keep your golden set stable, versioned, and curated. Use it for regression testing. Use production monitoring for drift detection. Do not conflate the two.

The next subchapter covers coverage strategy: how to divide your golden set across intents, edge cases, and risk tiers to ensure you are testing what actually matters.
