# 2.8 — Multi-Domain and Multi-Tenant Golden Sets

In June 2025, an enterprise workflow automation company deployed a prompt optimization that improved average task completion time by 18 percent. Their global golden set passed with 96 percent accuracy, unchanged from the previous release. The optimization shipped to all customers. Within 72 hours, their largest customer — a pharmaceutical company running FDA-regulated approval workflows — filed a breach-of-contract complaint. The optimized prompts used informal language that violated pharma industry standards, shortened outputs that omitted required documentation language, and simplified technical terminology in ways that rendered compliance attestations invalid. The global golden set had tested none of this. It represented average use cases across all customers. It missed the domain-specific requirements that defined correctness for regulated industries.

The company spent 11 days rolling back the change, three weeks rebuilding the customer relationship, and two months implementing per-customer golden sets. The root cause was not the optimization. The optimization worked exactly as designed for most customers. The root cause was the naive assumption that a single global golden set could represent correctness across fundamentally different domains, use cases, and compliance regimes. It cannot.

## The Limits of Global Golden Sets

A global golden set represents the average case across all users, all workflows, and all contexts. For systems serving a homogeneous user base with uniform requirements, a global set might suffice. For enterprise systems serving multiple industries, multiple geographies, or multiple regulatory environments, a global set is structurally insufficient.

The problem is not coverage volume. You can expand a global golden set to thousands of cases and still miss critical domain-specific regressions. The problem is that different domains define correctness differently. A financial services customer expects formal tone, regulatory disclaimers, and precise numerical citations. A consumer brand expects conversational tone, accessible language, and brand voice consistency. A healthcare provider expects HIPAA-compliant phrasing, clinical accuracy, and patient-safety guardrails. An e-commerce platform expects speed, brevity, and transaction-focused outputs. These are not variations on a theme. They are fundamentally different correctness criteria.

A global golden set must either dilute its criteria to the lowest common denominator, losing the specificity each domain requires, or attempt to represent all domains equally, becoming so large and complex that maintaining it is infeasible. Neither approach works. The first creates false negatives — you miss domain-specific regressions because your golden set does not test for them. The second creates operational burden so heavy that teams stop maintaining the golden set altogether, leading to staleness and decay.

**Multi-domain golden sets** acknowledge this reality. They maintain a global core set that covers universal requirements — basic functionality, common use cases, critical safety guardrails — and domain-specific subsets that enforce the correctness criteria unique to each domain. The global set ensures your system does not regress on fundamentals. The domain subsets ensure you do not break specialized use cases that only matter to specific customer segments.

## Structuring Domain-Specific Subsets

A domain subset is not a copy of the global set with a few extra cases. It is a curated collection that represents the unique failure modes, compliance requirements, and quality expectations of a specific domain. Building one requires domain expertise. You cannot infer pharmaceutical compliance requirements from general knowledge. You cannot guess what constitutes acceptable financial advice language by reading blog posts. You need input from domain experts, customer stakeholders, and compliance teams who understand what correctness means in that context.

Start by identifying your domains. Do not over-segment. If you have 500 customers, you do not need 500 domain subsets. Cluster customers by shared correctness criteria. Financial services might be one domain covering banks, investment firms, and payment processors. Healthcare might be another covering hospitals, clinics, and telehealth providers. Legal might cover law firms, corporate legal departments, and compliance consultancies. The goal is not perfect segmentation. The goal is to group customers whose definition of correctness is similar enough that a shared domain subset makes sense.

For each domain, define its unique correctness dimensions. What makes an output correct in this domain that might not matter in others? For financial services, correctness includes regulatory disclaimers, avoidance of forward-looking statements, and precise numerical formatting. For healthcare, correctness includes HIPAA compliance, avoidance of diagnostic language by non-clinicians, and patient safety guardrails. For legal, correctness includes jurisdiction-specific terminology, citation accuracy, and avoidance of unauthorized practice of law. Document these dimensions. They become the scaffolding for your domain subset.

Build the domain subset with domain experts. Do not delegate this to a general AI team. A machine learning engineer cannot write valid pharmaceutical compliance test cases. A product manager cannot write legally sound contract review examples. You need specialists who understand the domain's failure modes, regulatory requirements, and operational risks. Invite them to review actual production cases from that domain. Ask them to identify the outputs that would be unacceptable, the edge cases that cause problems, and the compliance traps that non-experts miss. Use their input to create test cases that reflect real-world domain risk.

Size your domain subsets appropriately. A domain subset does not need to be as large as your global set. It needs to cover the domain-specific dimensions that the global set does not address. For a highly regulated domain like pharmaceuticals, that might mean 150 to 300 domain-specific cases. For a less regulated domain like consumer electronics, it might mean 50 to 100 cases. The right size is the size that gives you confidence that domain-specific regressions will not escape detection. If you are still discovering new domain-specific failure modes in production, your subset is too small.

## Multi-Tenant Golden Sets and Isolation

Multi-tenant systems face an additional complexity. Not only do different customers operate in different domains, they also expect isolation. A change that benefits Customer A must not degrade performance for Customer B. A fine-tuning run that optimizes for one tenant's use cases must not break another tenant's workflows. Multi-tenant golden sets enforce this isolation at the evaluation layer.

**Tenant-specific golden sets** ensure that each major customer has a curated set of test cases representing their unique use cases, edge cases, and correctness criteria. These are not domain subsets. They are per-customer subsets. The distinction matters. A domain subset represents shared requirements across multiple customers in the same industry. A tenant subset represents one customer's specific workflows, data patterns, and success criteria.

You cannot build tenant subsets for every customer. If you serve 10,000 tenants, you cannot maintain 10,000 golden sets. But you can and should maintain tenant subsets for your largest, most critical, or most complex customers. The customers who generate the most revenue, who operate in the highest-risk domains, who have the most customized configurations, or who have the most stringent SLAs. For these customers, a tenant-specific golden set is not overhead. It is risk management.

Building a tenant subset requires collaboration with the customer. Schedule a golden set design session. Bring their domain experts, your trust and safety team, and your AI engineering lead. Walk through their workflows. Ask them to identify the use cases they care most about, the failure modes they fear most, and the edge cases they encounter most often. Use their production data to generate candidate test cases. Have them review and approve the cases. This collaborative approach does two things. First, it ensures the golden set accurately represents the customer's needs. Second, it gives the customer visibility into your quality process, which builds trust.

Maintain tenant subsets as part of your customer relationship, not just your testing infrastructure. When the customer requests a new feature, update their golden set to include test cases for that feature. When they report a production issue, add a test case that would have caught it. When their business changes — new compliance requirements, new workflows, new risk priorities — update their golden set to reflect those changes. A tenant golden set that does not evolve with the customer becomes stale just as quickly as a global set that does not evolve with the product.

## Balancing Global and Local Coverage

The challenge with multi-domain and multi-tenant golden sets is operational complexity. Every additional subset is another artifact to maintain, another evaluation to run, another source of potential regression. If your global set has 800 cases and you add five domain subsets of 150 cases each and three tenant subsets of 100 cases each, you now have 2,300 test cases to manage. That is not inherently unmanageable, but it requires discipline, automation, and clear ownership.

Avoid duplication across sets. If a test case belongs in both the global set and a domain subset, put it in the global set only. Tag it with metadata indicating which domains it represents. This reduces maintenance burden and ensures consistency. If you update the global case, you do not need to update duplicate cases in multiple subsets. The global set should contain all universally applicable cases. Domain and tenant subsets should contain only cases that are unique to that domain or tenant.

Run subsets selectively. You do not need to run every subset on every deployment. Run the global set on every change. Run domain subsets when changes affect domain-specific functionality or when deploying to customers in that domain. Run tenant subsets before major releases, before deploying to that tenant, or when the tenant requests validation. Selective execution reduces evaluation cost and cycle time while maintaining coverage where it matters.

Track coverage independently for each subset. Your global set might achieve 94 percent pass rate, your financial services subset 91 percent, your healthcare subset 97 percent, and your largest tenant subset 89 percent. These differences are signals. A lower pass rate in a domain subset indicates that your model performs less reliably for that domain. A declining pass rate in a tenant subset indicates a potential regression affecting that customer. Do not average these metrics. Report them separately. Each subset represents a different dimension of correctness, and each deserves independent visibility.

Assign ownership for each subset. The global set might be owned by your core AI team. Domain subsets should have co-owners from both the AI team and the domain's primary stakeholders — trust and safety for safety-critical domains, legal for compliance-heavy domains, product for feature-specific domains. Tenant subsets should have a dedicated customer success or account manager as co-owner, ensuring the subset evolves with the customer relationship. Ownership prevents neglect. A subset without an owner will decay.

## When One Global Set Is Enough

Not every system needs multi-domain golden sets. If your product serves a single industry, operates under a single regulatory regime, and defines correctness uniformly across all users, a global set suffices. If your user base is homogeneous — all consumers, all using the same features, all expecting the same quality standards — a global set suffices. If you are early-stage, pre-product-market-fit, or still defining your correctness criteria, start with a global set and add domain subsets only when you have evidence that global coverage is insufficient.

The signal that you need domain subsets is domain-specific production failures. If you deploy a change that passes your global golden set but breaks workflows for customers in a specific industry, you need a domain subset for that industry. If customer complaints cluster around domain-specific issues — compliance violations in one vertical, tone mismatches in another, accuracy failures in a third — you need domain coverage. If your largest customers request validation that your system meets their specific requirements, you need tenant subsets.

Do not build complexity you do not yet need. But when you need it, build it deliberately. A poorly structured multi-domain golden set is worse than a well-maintained global set. The goal is not to have the most subsets or the most test cases. The goal is to have the right test cases that detect the regressions your users will actually experience, whether those users are homogeneous or diverse, whether their needs are universal or specialized, and whether one global definition of correctness serves them all or each requires its own standard.

Your golden set architecture must match your product's complexity, and when your product serves multiple domains with different definitions of correctness, a single global standard becomes a liability rather than a safeguard.
