# 4.9 — Index Rebuild Validation

The team stares at the terminal. The index rebuild just finished — twelve hours of processing, four million vectors recomputed with the new embedding model. The old index still serves production traffic. The new index sits there, ready, waiting. Someone has to make the call: do we cut over? The engineering lead asks the question everyone is thinking: "How do we know it's safe?"

No one has a good answer. They rebuilt the index because the new embedding model promised better retrieval accuracy. But they have no systematic way to validate that the new index produces equivalent or better results than the old one. They have no framework for testing whether the rebuild introduced silent regressions — queries that now return worse results, documents that disappeared, rankings that shifted unexpectedly. They are about to switch production traffic to an index they cannot verify. This is how retrieval systems break in ways that take weeks to detect.

Index rebuilds are among the highest-risk operations in RAG systems. You are replacing the entire foundation of your retrieval pipeline. Every query, every document, every ranking depends on the index working correctly. If the rebuild introduced problems — corrupted embeddings, missing documents, broken metadata mappings — those problems propagate to every user request the moment you cut over. You need validation infrastructure that proves the new index is safe before production ever touches it.

## The Before-and-After Validation Framework

You validate an index rebuild by comparing the new index against the old one using a curated set of test queries. This is not a sample — this is a comprehensive regression suite that covers every query pattern your system handles. You run the same queries against both indexes and compare the results across multiple dimensions: document overlap, ranking stability, metadata consistency, and retrieval quality.

The validation framework starts with your golden query set — the same queries you use for standard retrieval regression tests. You run each query against the old index and capture the full response: the list of retrieved documents, their rankings, their scores, their metadata. Then you run the same queries against the new index and capture the same data. Now you have two sets of results to compare.

Document overlap measures how many of the same documents appear in both result sets. For a purely infrastructure-related rebuild — switching vector databases, upgrading index versions, optimizing storage — you expect near-perfect overlap. If the old index returned documents A, B, C, D, E for a query, the new index should return the same five documents. If document C is missing or document F appears instead, that signals a problem. The rebuild changed something it should not have changed.

Ranking stability measures whether documents appear in the same order. Even if both indexes return the same documents, a ranking shift changes which document users see first. For most queries, you expect rankings to remain stable within a small tolerance. If document B was ranked second in the old index and now ranks fifth in the new index, you need to understand why. Sometimes ranking shifts are intentional — you rebuilt the index specifically to improve ranking quality. But unintentional ranking shifts are regressions.

Metadata consistency verifies that every document carries the same metadata fields and values in the new index. A common rebuild failure mode is metadata loss — the rebuild script fails to preserve custom fields, timestamps, access control tags, or source identifiers. If your retrieval system filters results by metadata or displays metadata to users, losing that data breaks the user experience even if document retrieval still works.

The comparison itself must be automated. You cannot manually inspect results for ten thousand queries. You write validation scripts that compute overlap percentages, ranking correlations, and metadata diffs. You set thresholds: if document overlap drops below ninety-eight percent, the rebuild fails validation. If ranking correlation drops below 0.95, you investigate. If any metadata field is missing from more than one percent of documents, you block the cutover.

## Retrieval Equivalence Testing

Retrieval equivalence testing answers a specific question: does the new index retrieve the same quality of documents as the old index? This goes deeper than document overlap. You want to know whether the new index maintains or improves retrieval accuracy, not just whether it returns similar documents.

You measure equivalence using your standard retrieval quality metrics: precision, recall, and relevance scoring. For each query in your golden set, you already have ground truth — the list of documents that should be retrieved for that query. You compute precision and recall for the old index results and for the new index results. If the old index achieved ninety-two percent recall and the new index achieves eighty-seven percent recall, the rebuild made retrieval worse. That is a regression, not equivalence.

Relevance scoring equivalence compares how well each index ranks the most relevant documents. You use LLM-as-judge or human-labeled relevance scores to evaluate whether the top-ranked document in each result set is actually relevant. If the old index placed a highly relevant document in position one and the new index places it in position four, the rebuild degraded ranking quality even if the same documents were retrieved.

Equivalence testing also includes negative queries — queries that should return zero results. If your old index correctly returned no results for "how to train a unicorn" because your knowledge base contains no unicorn training content, your new index should also return zero results. A rebuild that starts returning irrelevant documents for negative queries has introduced false positives.

Some teams run equivalence tests on a percentage basis: they declare victory if ninety-five percent of queries show equivalent or better results in the new index. This is dangerous. The five percent of queries that regressed may include your most important queries — the ones your users run most often, the ones tied to revenue, the ones that drive engagement. You need to inspect regressions individually, not dismiss them as acceptable noise.

## The Parallel Index Strategy

The safest way to validate an index rebuild is to run the old index and the new index in parallel for a period of time before cutting over. Production traffic continues hitting the old index. A shadow copy of every production query also hits the new index. You log both sets of results and compare them continuously in real time.

Parallel indexing catches problems that static validation tests miss. Static tests use a fixed golden query set. Parallel indexing uses real production traffic — queries you did not anticipate, edge cases your test suite does not cover, query patterns that only appear under load. If the new index produces different results for a production query that users care about, you detect it before the cutover.

You run parallel indexing for days or weeks, depending on query volume and traffic diversity. You need enough time to see seasonal patterns, weekend versus weekday traffic, and queries from all user segments. A fintech RAG system that runs parallel indexing only during weekday business hours misses the weekend queries that might regress. A customer support system that runs parallel indexing only in US time zones misses the queries from European or Asian users.

The parallel index strategy requires infrastructure investment. You are doubling your indexing compute and storage costs during the validation period. But the cost is temporary, and the risk mitigation is enormous. Cutting over to a broken index can cost far more than a few weeks of parallel infrastructure — in lost user trust, degraded experience, and engineering time spent debugging production incidents.

During parallel indexing, you monitor divergence metrics: how often do the two indexes produce different results, and by how much? Small divergence is expected — floating-point precision differences, minor ranking shifts due to index sharding. Large divergence signals a real problem. If ten percent of production queries return completely different documents from the new index, you do not cut over. You investigate the root cause, fix the rebuild process, and start over.

## Cutover Criteria and Rollback Procedures

You need explicit, measurable criteria for when the new index is ready to take over production traffic. This is not a judgment call. It is a checklist of gates that must all pass before the cutover happens.

Document count validation is the first gate. The new index should contain the same number of documents as the old index, plus or minus any documents that were added or removed during the rebuild window. If the old index had four million documents and the new index has 3.8 million, two hundred thousand documents went missing. You do not cut over until you understand where they went and whether they should be recovered.

Retrieval equivalence is the second gate. Ninety-eight percent of queries in your golden set must produce equivalent or better results in the new index compared to the old index. Equivalence is measured by document overlap, ranking correlation, and relevance scores. If equivalence drops below the threshold, you block the cutover.

Latency validation is the third gate. The new index must meet the same latency SLAs as the old index. If the old index served queries at p95 latency of eighty milliseconds and the new index serves at one hundred twenty milliseconds, the rebuild introduced a performance regression. You either optimize the new index or you do not cut over.

Metadata completeness is the fourth gate. Every document in the new index must have all required metadata fields populated. If your system depends on access control tags, document timestamps, or source identifiers, losing that metadata breaks functionality. You validate metadata coverage with automated scripts that check every document in the new index.

Once all gates pass, you cut over in stages. You do not flip production traffic all at once. You route five percent of production queries to the new index and ninety-five percent to the old index. You monitor error rates, latency, and user-facing metrics. If everything looks stable, you increase to twenty percent, then fifty percent, then one hundred percent. Each stage runs for hours or days, depending on traffic volume. If any stage shows problems, you roll back immediately.

Rollback procedures must be tested before you start the cutover. The fastest way to roll back is to route traffic back to the old index. This only works if the old index still exists and is still being updated with new documents. Some teams delete the old index the moment they cut over to the new one. This makes rollback impossible. You keep the old index running in parallel until the new index has been stable in production for at least a week.

## Embedding Consistency Verification

One of the most subtle failure modes in index rebuilds is embedding drift. You rebuild the index using a new embedding model or a new version of the same model. The embeddings themselves change — same document, different vector representation. If the drift is large, retrieval behavior changes unpredictably.

You verify embedding consistency by comparing embeddings for a sample of documents between the old and new indexes. You compute cosine similarity between the old embedding and the new embedding for the same document. If similarity is very high — above 0.98 — the embeddings are effectively identical, and retrieval should behave the same. If similarity drops below 0.90, the embeddings diverged significantly, and retrieval will behave differently.

Embedding drift is not always a problem. If you deliberately upgraded to a better embedding model, some drift is expected and desirable. The new embeddings should produce better retrieval accuracy. But unintentional drift — caused by a bug in the embedding pipeline, incorrect model weights, or a configuration error — is a regression. You detect it by comparing known-good embeddings from the old index to new embeddings and flagging any documents where similarity drops below a threshold.

Some teams store embedding checksums alongside the embeddings themselves. When you generate an embedding for a document, you also compute a hash of the embedding vector and store it as metadata. During a rebuild, you regenerate the embedding and compare the new hash to the old one. If they match, the embedding is identical. If they differ, you investigate. Checksums catch silent corruption — cases where the rebuild produced different embeddings due to a pipeline bug, not an intentional model change.

Embedding consistency verification is especially critical for hybrid search systems that combine vector search with keyword search. If your embeddings drift but your keyword indexes stay the same, the relative weighting between vector and keyword scores shifts. Queries that previously relied on vector similarity now rely more on keyword matching, or vice versa. This changes retrieval behavior in ways that are difficult to predict without systematic testing.

## The Incremental Rebuild Escape Hatch

Some index rebuilds are too large to validate all at once. If you have billions of documents, rebuilding the entire index and validating it in parallel takes weeks and costs tens of thousands of dollars. You need an incremental approach.

Incremental rebuilds partition the index into segments — by date range, by document source, by user cohort — and rebuild one segment at a time. You validate each segment independently before moving to the next. If a segment fails validation, you roll back just that segment without affecting the rest of the index.

The challenge with incremental rebuilds is maintaining query consistency. A query that spans multiple segments might retrieve documents from a mix of old and new index segments. If the segments use different embedding models or different ranking functions, results become inconsistent. A user runs the same query twice and gets different results because the second query hit a newly rebuilt segment.

You solve this with versioned segments. Each segment is tagged with a version number that indicates which embedding model and ranking function it uses. Queries are routed only to segments with the same version. When you rebuild a segment, you increment its version and route queries to it only after validation passes. Over time, all segments converge to the new version, and you retire the old version.

Incremental rebuilds are slower than full rebuilds, but they reduce risk. If a rebuild goes wrong, you lose days of work on one segment, not weeks of work on the entire index. If validation catches a problem, you fix it and re-rebuild the segment without affecting production. The cost of caution is time. The cost of skipping validation is broken retrieval in production for weeks before anyone notices.

Next: how to test document ingestion pipelines to ensure every new document improves retrieval quality instead of polluting it.
