# 7.5 — Harm Taxonomy Coverage Tracking

A harm taxonomy is a structured catalog of every meaningful way your AI system can cause damage. It is not a risk register. It is not a checklist of obvious problems. It is a comprehensive map of failure modes organized by type, severity, and domain, built specifically for your product's context. Most teams approaching safety testing pick categories intuitively — bias, toxicity, privacy, maybe hallucination — and write tests for those. What they rarely ask is whether those categories cover the actual ways their system causes harm in production. Harm taxonomy coverage tracking is the discipline of ensuring that every identified harm category has corresponding tests, that test distribution reflects real severity, and that the taxonomy itself evolves as new failure modes emerge. Without systematic coverage tracking, your safety testing is a spotcheck at best and theater at worst.

## The Standard Taxonomies and Their Gaps

In 2026, several widely-adopted harm taxonomies exist. The NIST AI Risk Management Framework defines categories including discrimination, privacy violations, content harms, manipulation, environmental impact, and system security. The EU AI Act Annex III specifies prohibited practices and high-risk use cases with associated harm types. OpenAI's usage policies and Anthropic's Constitutional AI principles each represent harm taxonomies built for general-purpose models. These frameworks are valuable starting points. They reflect years of research, incident analysis, and cross-industry consensus. But they are not sufficient for product-specific testing.

A healthcare diagnostics system using a general taxonomy might test for demographic bias, data privacy leaks, and hallucinated medical facts — all critical. But the taxonomy likely does not include a category for diagnostic cascade harms, where the model recommends an unnecessary test that leads to a chain of further unnecessary interventions. It probably does not include treatment sequencing errors, where the model suggests medications in an order that creates drug interactions the individual drugs would not cause alone. It might not cover contraindication blindness, where the model ignores patient history that makes a standard recommendation dangerous. These are not edge cases. They are core failure modes for a medical AI, and if your harm taxonomy does not name them explicitly, your test suite will not cover them.

The gap is not that standard taxonomies are wrong. It is that they are general. Your job is to take a standard taxonomy as a foundation and extend it with product-specific harm categories that reflect what actually goes wrong in your domain. A financial advice assistant adds categories for fiduciary duty violations, tax compliance errors, and unsuitable investment recommendations. An HR screening tool adds categories for discriminatory rejection patterns, wage recommendation bias, and accessibility exclusion. A customer service agent adds categories for escalation failures, policy misrepresentation, and tone-based churn. Every product has its own failure surface. The taxonomy must name that surface explicitly.

## Coverage Metrics That Reveal Blind Spots

Coverage tracking starts with a simple question: for each harm category in your taxonomy, how many tests do you have? The answer reveals your blind spots immediately. A team in late 2025 maintained a harm taxonomy with 23 categories for their legal research assistant. When they counted tests per category, the distribution was 87 tests for hallucination, 34 for bias, 19 for privacy leakage, 12 for citation errors, 6 for jurisdiction misapplication, and zero for eight other categories including unauthorized practice of law, statute of limitations miscalculation, and conflict-of-interest detection failures. The model had shipped a year earlier. It had passed all safety reviews. But 35 percent of identified harm categories had no tests at all. Those untested categories were not hypothetical. Each had appeared in production incidents. The team had simply never systematized coverage.

Coverage is not just presence or absence. It is distribution. A harm category with one test is technically covered but practically untested. The threshold depends on category complexity and severity, but a useful baseline is that any high-severity harm category should have at least 10 tests covering different manifestations of that harm type, and any medium-severity category should have at least 5. Low-severity categories can have fewer, but zero is never acceptable if the category is in the taxonomy at all. If a harm type is worth naming, it is worth testing.

Severity-weighted coverage is the metric that balances breadth and depth. Calculate it by multiplying the number of tests in each category by the severity weight of that category, then dividing by the total possible severity-weighted tests if every category had full coverage. A taxonomy with 10 categories, each severity 1 through 3, where category 3 has 10 tests, category 2 has 5 tests, and category 1 has 2 tests, has uneven coverage. If high-severity categories are under-tested relative to low-severity ones, your regression suite is optimized for the wrong risks. Severity-weighted coverage surfaces that imbalance.

You also track coverage growth over time. In a mature system, harm taxonomy coverage should increase quarter over quarter as you discover new failure modes and add corresponding tests. If coverage is static for six months, either your taxonomy is complete — unlikely — or you have stopped learning from production. Coverage stagnation is a signal that your incident response process is not feeding back into your test design.

## The Coverage Gap and How It Compounds

A coverage gap is any harm category with insufficient tests relative to its severity and frequency. Gaps compound in three ways. First, they create false confidence. A regression suite with 300 tests feels comprehensive. But if 280 of those tests cover 3 categories and 20 categories have 1 test each, the suite is a false signal. It tells you that the three well-tested categories have not regressed. It tells you nothing about the other 20. Teams treat green test results as evidence of safety. If the tests do not cover the harm surface, that evidence is misleading.

Second, coverage gaps mean silent regressions. A model update changes behavior in an untested harm category. The regression suite runs green. The change ships. Users encounter the harm. The team investigates and finds that this harm type was in the taxonomy all along, flagged as high-severity, but had zero tests. The regression was preventable. The tooling existed. The gap created the failure.

Third, gaps skew investment. If you measure regression by pass rate across all tests, and most tests are concentrated in a few categories, improvements in those categories dominate your metrics even if higher-severity categories are degrading. A fine-tuning experiment might reduce hallucination by 8 percent, measured across 87 hallucination tests, while increasing privacy leakage by 30 percent, measured across 3 privacy tests. Aggregate pass rate improves. The team ships. Privacy incidents spike. The coverage gap hid the trade-off.

The fix is mandatory minimum coverage. Set a floor for every harm category based on severity: high-severity categories must have at least 15 tests, medium-severity at least 8, low-severity at least 3. Enforce this floor as a release gate. If a new harm category is added to the taxonomy, the release pipeline blocks until minimum coverage is met. This feels heavy-handed. It is supposed to. Harm categories are not suggestions. If a failure mode is important enough to name, it is important enough to block a release until you test for it.

## Balancing Coverage Across Harm Types

Coverage is not just quantity. It is representativeness. A harm category with 15 tests that all target the same narrow failure mechanism is poorly covered despite meeting the numeric threshold. A category with 8 tests that span the breadth of that harm type may be better covered. The balance is between depth — testing a specific failure mode exhaustively — and breadth — testing the full range of failure modes within a category.

For high-severity categories, you need both. Take bias in hiring screening as an example category. Depth means testing gender bias in software engineering roles, age bias in executive roles, and racial bias in customer-facing roles, each with multiple test cases per demographic-role pair. Breadth means also testing for disability bias, accent bias, name-based bias, education pedigree bias, employment gap bias, and geographic bias. A test suite with 40 gender bias tests and zero disability bias tests has depth in one subcategory but no breadth. The hiring model can pass regression on gender while systematically excluding disabled candidates.

The heuristic for balancing depth and breadth is the two-by-two rule. For each harm category, identify the primary subcategories that represent different failure mechanisms. For bias, subcategories might be demographic group, role type, and decision stage. Ensure that every combination of two subcategories has at least one test. Gender bias in technical roles. Age bias in executive roles. Racial bias in customer-facing roles. Name-based bias in initial screening. This creates a sparse but representative coverage grid. You cannot test every combination exhaustively, but you can ensure that no two-dimensional slice of the harm space is completely untested.

You also balance across severity tiers. High-severity categories should have the most tests, but medium and low-severity categories should not be neglected. A rule of thumb: high-severity categories receive 50 percent of total test budget, medium-severity categories receive 35 percent, and low-severity categories receive 15 percent. This reflects that high-severity harms matter most, but also acknowledges that medium and low-severity harms, if frequent, still degrade user trust and product reputation. A system that never causes catastrophic harm but constantly causes minor annoyance is not safe — it is annoying, and users leave.

## Harm Taxonomy Evolution and Coverage Maintenance

Harm taxonomies are not static. New harm categories emerge as you learn from production incidents, as regulations change, and as your product expands into new domains. In early 2025, a tax preparation assistant had a harm taxonomy covering calculation errors, filing deadline mistakes, and audit risk inflation. Then the product expanded to support small business tax. New harm categories appeared: payroll tax misclassification, contractor versus employee misidentification, and sales tax nexus errors. The taxonomy grew from 12 categories to 19. If the team had not updated test coverage in parallel, the regression suite would have covered the original consumer tax use cases while leaving the business tax cases untested.

Taxonomy evolution requires a coverage maintenance process. Every time a new harm category is added, a coverage gap is created. That gap must close before the next release. The process works like this: a production incident or risk assessment identifies a new harm type. The harm type is added to the taxonomy with a severity rating. A coverage target is set based on severity. Test design is assigned to a specific owner with a due date. The release gate enforces that the coverage target is met before the category is considered fully integrated. Until then, the gap is tracked as technical debt and reported in every release readiness review.

You also retire harm categories that are no longer relevant. If your product used to generate medical diagnoses but no longer does, diagnostic harm categories can be archived. Archiving is not deletion. The tests remain in version control and can be reactivated if the product changes again. But active coverage tracking focuses on current harm surface, not historical ones. An ever-growing taxonomy with obsolete categories creates noise and dilutes focus from the categories that matter now.

The taxonomy itself should be reviewed quarterly. Gather incidents from the past quarter, review user feedback and red team findings, check for regulatory updates, and ask: are there failure modes we have seen in production that do not fit cleanly into any existing category? If yes, create a new category. Are there categories we have not seen incidents for in over a year? If yes, consider whether they are truly inactive or whether we are simply not detecting them. A category with zero incidents is either no longer a risk or a measurement gap.

## Test Distribution by Severity and Frequency

Test distribution should reflect both severity and frequency. A high-severity, low-frequency harm needs fewer tests than a high-severity, high-frequency harm, but more tests than a low-severity, low-frequency harm. The trade-off is between likelihood and impact. A harm that happens once a quarter but costs 2 million dollars in liability deserves significant test coverage. A harm that happens once a month but costs 500 dollars in refunds deserves less. A harm that happens a thousand times a day but causes only minor user friction deserves more than its severity alone would suggest, because cumulative impact grows with frequency.

A practical distribution model uses a severity-frequency matrix. Rate each harm category on a severity scale of 1 to 5 and a frequency scale of 1 to 5 based on production data and risk assessment. Multiply the two to get a priority score from 1 to 25. Allocate test budget proportionally to priority scores. A category with severity 5 and frequency 5 gets a score of 25 and receives the most tests. A category with severity 2 and frequency 1 gets a score of 2 and receives the fewest. This is not a perfect formula — some harms are tail risks that deserve attention despite low frequency — but it provides a rational basis for distribution instead of allocating tests based on what is easiest to write or what the team is most familiar with.

You adjust the distribution as production data accumulates. A harm category that was rated low frequency but starts appearing weekly gets re-rated and receives more test budget. A harm category that was rated high severity but has never occurred in production despite six months of monitoring might be downgraded or examined for whether your detection is inadequate. Distribution is not set once. It evolves with your understanding of the product's actual failure surface.

## Mapping Incidents to Taxonomy Categories

Every production incident should map to at least one harm category in your taxonomy. When an incident occurs, the postmortem process includes identifying which harm category it falls under. If the incident maps cleanly to an existing category, you add a new test for that failure mode to prevent recurrence. If the incident does not map to any existing category, that is a signal that your taxonomy is incomplete. You either create a new category or recognize that your existing categories are too coarse-grained and need subdivision.

Incident mapping serves two purposes. First, it validates that your taxonomy reflects reality. If 80 percent of production incidents map to 3 categories, those categories are your actual harm surface, and coverage should reflect that concentration. If incidents are evenly distributed across categories, you have a broad harm surface, and coverage needs to be broad as well. If incidents frequently fail to map to any category, your taxonomy is not useful as a testing framework.

Second, incident mapping drives continuous coverage improvement. Each mapped incident becomes a test case. Over time, your regression suite becomes a living record of everything that has gone wrong, organized by harm type. A mature regression suite is not a collection of hypothetical scenarios. It is a collection of real failures, systematized and automated to ensure they never happen again.

The mapping process itself is simple: in your incident tracker, add a field for harm category. During postmortem, the team identifies the category, writes a test that reproduces the failure, and links the test to the incident. Over the next quarter, coverage reports show which categories are generating incidents. If a category has high incident volume but low test coverage, that is your next investment priority. If a category has high test coverage but incidents keep occurring, either the tests are not representative of real failure modes or the harm is harder to prevent than testing alone can solve, and you need architectural changes.

## Coverage Reporting for Stakeholders

Harm taxonomy coverage is not just an engineering metric. It is a trust signal for stakeholders who need evidence that safety is systematized. Legal wants to know that every regulatory risk category has tests. Product wants to know that every user-facing harm type is covered. Trust and Safety wants to know that every content harm category is tested. Coverage reports make that evidence legible.

A coverage report should include: total number of harm categories, number of categories with zero tests, number of categories below minimum coverage threshold, severity-weighted coverage percentage, and coverage by category with test counts and severity ratings. For each under-covered category, the report lists the gap size and the plan to close it, including owner and target date. The report is generated weekly during active development and included in every release readiness review.

You also report coverage trends. Show coverage growth over the past six months. Show which categories gained tests and which did not. Show which categories had incidents and whether those incidents resulted in new tests. Trends reveal whether your process is working. If coverage grows steadily and incident-driven test additions happen consistently, the system is healthy. If coverage is flat and incidents do not translate into tests, the process is broken.

External reporting is increasingly required for high-risk AI systems under regulations like the EU AI Act. Compliance documentation must demonstrate that safety testing is comprehensive and systematic. A harm taxonomy with measurable coverage provides that evidence. It is not enough to say "we test for bias and toxicity." You must show which types of bias, how many tests per type, which severity ratings, and how you ensure new harm types are detected and added. Taxonomy coverage tracking is the infrastructure that makes that showing possible.

The next challenge is ensuring that compliance itself does not regress — that as regulations evolve and new requirements emerge, your system remains continuously compliant without requiring emergency scrambles before each audit.

