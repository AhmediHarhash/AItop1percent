# 1.4 — Types of Regressions in LLM Systems

LLM systems degrade in ten distinct ways. Each type has its own signature, its own detection method, and its own consequences. If you only test accuracy, you are blind to nine failure modes that will quietly damage your product while your test suite reports green.

Most teams discover this truth the hard way. They build a comprehensive accuracy eval, celebrate when it passes, ship the new model, and watch their support tickets triple. The model is more accurate on the benchmark. It is also three hundred milliseconds slower, costs twice as much per query, and refuses to answer questions it handled perfectly yesterday. The accuracy eval caught nothing because it was not designed to catch anything except accuracy.

The fundamental mistake is treating regression testing as a single discipline when it is actually ten separate disciplines that happen to share infrastructure. Each type of regression requires different baseline data, different comparison methods, and different thresholds. A mature regression suite tests all ten. An immature one tests one or two and hopes for the best.

## Accuracy Regression

This is what everyone tests first and what most teams test only. **Accuracy regression** means the model's correctness on your task metrics declined compared to the baseline. If your previous model achieved 91 percent precision on contract classification and your new model achieves 87 percent, you have an accuracy regression.

Detection is straightforward. You run your golden set through both models and compare task-specific metrics. Precision, recall, F1, exact match, BLEU, ROUGE, custom domain metrics — whatever measures correctness for your use case. The threshold depends on your risk tolerance. Some teams block any decline. Others allow up to two percentage points if other metrics improve. The right threshold is the one that reflects how much accuracy loss your users will tolerate in exchange for other gains.

The nuance is that accuracy is multidimensional. A model can improve on easy cases while regressing on hard ones. It can improve on common queries while regressing on edge cases. It can improve on recent data while regressing on older patterns. Your regression suite must test accuracy across slices, not just in aggregate. If your new model scores 92 percent overall but drops from 78 percent to 64 percent on medical terminology questions, you have a slice-specific accuracy regression that aggregate metrics will hide.

Accuracy regressions happen for predictable reasons. Fine-tuning on narrow data causes the model to forget broader capabilities. Prompt changes that improve one task harm another. Switching to a smaller model sacrifices correctness for speed. Updating retrieval logic changes which context the model sees. Every change carries accuracy risk. The question is whether you measure it before users do.

## Hallucination Increase

**Hallucination increase** means the model generates more false or unsupported claims than the baseline. The model may maintain or even improve accuracy on your primary metrics while simultaneously increasing the rate at which it fabricates information, especially on queries outside your golden set's distribution.

This is the most dangerous regression type because it is invisible to task-specific accuracy evals. A customer support model that accurately categorizes tickets can simultaneously start inventing product features that do not exist. A medical summarization model that correctly extracts symptom lists can simultaneously add diagnoses the doctor never mentioned. The task metric looks fine. The model is lying more often.

Detection requires dedicated hallucination evals that check whether model outputs contain claims unsupported by input context or known facts. For RAG systems, you compare model outputs against retrieved chunks and flag any statement that has no grounding. For general Q&A, you run a set of questions with known correct answers and measure how often the model adds false details. For summarization, you check whether the summary contains information absent from the source. The metric is hallucination rate — what percentage of outputs contain at least one unsupported claim.

Hallucination regressions happen when you change the model's behavior toward creativity or away from conservatism. Lowering temperature reduces hallucinations but also reduces useful elaboration. Raising temperature increases useful elaboration but also increases fabrication. Prompt changes that encourage detailed answers often increase hallucinations. Fine-tuning that rewards longer outputs often increases hallucinations. Switching to a more capable model can reduce hallucinations, but switching to a smaller one usually increases them. Every generation parameter and every model change affects hallucination rate, and most teams never measure it.

## Latency Degradation

**Latency degradation** means the system takes longer to respond than the baseline. This includes model inference time, retrieval time, tool call time, and end-to-end user-facing latency. A model that improves accuracy but takes four hundred milliseconds longer to respond has a latency regression, and depending on your use case, that latency cost may outweigh the accuracy gain.

Latency regressions destroy user experience in ways that accuracy improvements cannot repair. Users tolerate a slightly less accurate answer delivered in three hundred milliseconds. They do not tolerate a slightly more accurate answer delivered in two seconds. For conversational AI, real-time support, and voice interfaces, latency is a primary quality dimension. For batch processing and offline summarization, it matters less. Your regression suite must know which category you are in and test accordingly.

Detection requires measuring end-to-end latency for every query in your regression set. You compare the distribution of latencies, not just the mean. A change that improves median latency but creates a long tail of slow queries is still a regression. The relevant percentiles depend on your SLA. If you promise sub-500ms response time at p95, then p95 latency is your regression threshold. If one in twenty queries suddenly takes nine hundred milliseconds, you have a latency regression even if the mean improved.

Latency regressions happen when you add complexity. Switching to a larger model increases inference time. Adding retrieval steps increases retrieval time. Enabling multi-turn tool use increases interaction time. Adding safety filters increases processing time. Increasing context window increases both prompt processing time and generation time. Every feature you add is a latency risk. If you do not measure it, you will ship latency regressions while celebrating feature completeness.

## Cost Explosion

**Cost explosion** means the per-query cost increased compared to the baseline. This includes model inference costs, embedding costs, retrieval costs, tool call costs, and any other marginal cost that scales with usage. A model that improves accuracy but doubles your cost per query is a cost regression, and if your unit economics cannot absorb it, the improvement is irrelevant.

Cost regressions are particularly insidious because they are invisible at evaluation time and catastrophic at scale. Testing on a golden set of five hundred queries costs thirty dollars whether your per-query cost is three cents or six cents. The difference is irrelevant during testing. It becomes a $40,000 monthly increase when you process 20 million queries. Most teams discover cost regressions in their AWS bill three weeks after deploy.

Detection requires tracking cost metrics for every query in your regression set. You measure input tokens, output tokens, embedding calls, retrieval calls, and tool invocations. You calculate per-query cost based on current provider pricing. You compare the new model's cost distribution against the baseline. If median cost per query increased by more than your threshold — commonly ten to twenty percent — you have a cost regression that must be justified by corresponding quality or latency improvements.

Cost regressions happen when you make the system more capable without constraining resource usage. Switching from a small model to a large one increases inference cost. Increasing max output tokens increases generation cost. Expanding retrieval from top-3 to top-10 increases embedding and LLM context cost. Enabling multi-step tool use increases both inference and API call costs. Adding a reranking step increases compute cost. Every capability improvement is a cost increase until proven otherwise. If you do not measure it before deploy, you will discover it when the CFO asks why AI spend doubled.

## Safety Refusal Drift

**Safety refusal drift** means the model's willingness to refuse unsafe, inappropriate, or policy-violating requests changed compared to the baseline. This regression is bidirectional. The model can become more cautious, refusing requests it previously handled correctly, or it can become less cautious, accepting requests it should refuse. Both are regressions. Both cause user-facing problems.

Over-refusal creates false positives. The model refuses benign medical questions because they mention anatomy. It refuses legal questions because they mention crimes. It refuses customer support questions because they contain frustrated language. Users encounter refusals for legitimate use cases, trust erodes, and your product becomes unusable for the tasks it was designed to handle. If your new model refuses 14 percent of legitimate queries when the baseline refused 3 percent, you have an over-refusal regression.

Under-refusal creates compliance and safety risk. The model answers questions it should escalate to a human. It generates content that violates your content policy. It provides advice in domains where it should defer to experts. If your new model refuses 2 percent of policy-violating queries when the baseline refused 11 percent, you have an under-refusal regression, and depending on your domain, that regression can create legal liability.

Detection requires two eval sets: one of legitimate requests that should succeed and one of policy-violating requests that should fail. You run both through the new model and compare refusal rates against the baseline. The legitimate set measures over-refusal. The policy-violating set measures under-refusal. Both matter. A model that reduces over-refusal while increasing under-refusal has not improved — it has simply traded one failure mode for another.

Safety refusal drift happens whenever you change the model, the system prompt, or the safety layer. Fine-tuning almost always changes refusal behavior, usually increasing compliance with user requests at the cost of safety conservatism. Prompt changes that encourage helpfulness reduce refusals. Switching models changes the underlying safety training. Adjusting safety filter thresholds directly changes refusal rates. Every change is a safety risk. If you do not measure refusal behavior before deploy, you will discover your new safety profile when a user reports a policy violation.

## Retrieval Degradation

**Retrieval degradation** means the relevance or quality of retrieved context declined compared to the baseline. This regression is specific to RAG systems but affects every RAG application. The model receives worse context, so it produces worse outputs, even if the model itself has not changed. Retrieval regressions break RAG systems silently because the model and the eval both work correctly — the problem is that the model never saw the information it needed.

Detection requires measuring retrieval quality independently from generation quality. You take a set of queries with known relevant documents. You run retrieval and measure precision at k, recall at k, or nDCG. You compare the new retrieval system against the baseline. If retrieval precision drops from 87 percent to 79 percent, you have a retrieval regression that will cascade into generation quality problems even if the model improved.

Retrieval regressions happen when you change the retrieval pipeline without re-evaluating retrieval quality. Switching embedding models changes the semantic space and often changes which documents are retrieved. Re-indexing with new chunking logic changes which text is available. Adjusting retrieval parameters like top-k or similarity threshold changes what the model sees. Adding filters or metadata constraints changes retrieval precision. Updating the document corpus changes retrieval recall. Every retrieval change is a potential regression. If you only test generation quality, you will blame the model for a retrieval problem you created.

## Tool Call Amplification

**Tool call amplification** means the system invokes tools more frequently than the baseline. This is not always a regression — sometimes increased tool use reflects better reasoning. But uncontrolled tool call amplification increases latency, cost, and error surface area. A model that calls external APIs three times per query when the baseline called once has a tool call regression unless the additional calls demonstrably improve output quality.

The problem is that tool calls are expensive. Each call adds 200 to 800 milliseconds of latency depending on the API. Each call adds cost. Each call adds a failure point. A model that over-uses tools produces slower, more expensive, less reliable outputs even if the final answer quality is identical. This is particularly dangerous when tool calls involve write operations. A model that creates three calendar events when it should create one has not improved — it has created data corruption risk.

Detection requires logging tool invocation rates for every query in your regression set. You measure how many tool calls the new system makes compared to the baseline. You break this down by tool type — search tools, database tools, API tools, write tools. You compare distributions. If the new system makes 40 percent more tool calls on average, you investigate whether those calls improve output quality or simply reflect a more trigger-happy model.

Tool call amplification happens when you make the model more agentic without constraining tool use. Switching to a model with better tool-use capabilities increases invocation rates. Prompt changes that emphasize tool availability increase usage. Few-shot examples that show multi-tool workflows increase calls. Removing tool-use thresholds or confidence checks increases frivolous invocations. Every change that makes tools more accessible increases tool call rates. If you do not measure and threshold tool use, you will ship a system that calls expensive APIs twice as often as necessary.

## Tone and Format Regression

**Tone and format regression** means the style, structure, or presentation of outputs changed in ways that harm user experience. The model may be equally accurate but write in a different tone. It may provide correct information but structure it differently. It may use different vocabulary, different formality, different levels of detail. These regressions are subjective but user-facing. They matter.

A customer support model that shifts from warm and empathetic to cold and transactional has a tone regression. A legal summarization model that shifts from bullet points to paragraphs has a format regression. A medical explanation model that shifts from patient-friendly language to clinical jargon has a tone regression. The information is correct. The user experience degraded.

Detection is the hardest part because tone and format are not easily quantified. The best approach is to use an LLM-as-judge eval that compares new outputs against baseline outputs on dimensions like tone, formality, structure, and vocabulary. You provide rubrics for each dimension. You run a representative sample of queries through both models and have the judge model score differences. If the judge detects tone shifts in more than 15 percent of comparisons, you have a tone regression that requires investigation.

Tone and format regressions happen whenever you change the model or the prompt. Different models have different default styles. Claude tends toward formal and structured. GPT tends toward conversational and detailed. Llama tends toward concise and direct. Switching models changes tone even if you do not change the prompt. Prompt changes explicitly change tone. Fine-tuning changes tone based on training data style. If you do not explicitly evaluate tone consistency, you will ship tone regressions and discover them when users complain that the product "feels different."

## Policy Compliance Drift

**Policy compliance drift** means the model's adherence to your internal policies, brand guidelines, or regulatory requirements changed compared to the baseline. This is distinct from safety refusals. A model can refuse unsafe requests while simultaneously violating your content policy, your brand voice, your disclosure requirements, or your regulatory obligations.

For healthcare applications, policy compliance includes HIPAA requirements, clinical disclaimer language, and scope-of-practice boundaries. For financial applications, it includes regulatory disclosures, fiduciary duty statements, and investment advice limitations. For customer support, it includes brand voice, escalation protocols, and promise-making boundaries. Every domain has policies. Every policy can be violated. Most teams do not test for policy compliance until after a violation reaches users.

Detection requires a policy compliance eval that checks whether outputs meet your documented requirements. You create test cases for each policy dimension. You run them through the new model. You check whether outputs include required disclaimers, avoid prohibited claims, follow brand guidelines, and respect escalation boundaries. The pass rate must be near 100 percent. A model that complies with 94 percent of policies has a six percent failure rate that will eventually create a compliance incident.

Policy compliance drift happens when you change the model or the prompt without re-evaluating policy adherence. Different models have different training and different default behaviors around compliance. Prompt changes that optimize for helpfulness often reduce compliance. Fine-tuning that prioritizes task performance often weakens policy adherence. If you do not explicitly test policy compliance before every deploy, you will ship policy violations and discover them when Legal asks why your AI made promises you cannot keep.

## Business KPI Regression

**Business KPI regression** means the model's impact on your actual business metrics declined compared to the baseline. This is the only regression type that measures what actually matters — not model quality, not system performance, but user behavior and business outcomes. A model can improve on every technical metric while simultaneously harming conversion, retention, satisfaction, or revenue.

This happens when technical improvements misalign with user needs. A customer support model that becomes more accurate but more verbose increases user frustration. A recommendation model that becomes more precise but more conservative reduces discovery and engagement. A sales assistant model that becomes more compliant but less persuasive reduces conversion. The model is objectively better on your eval. The business is objectively worse.

Detection requires running online A/B tests before full deployment. You deploy the new model to a small percentage of traffic. You measure business KPIs — conversion rate, task completion rate, user satisfaction, revenue per session, churn rate, support ticket volume, whatever metrics define success for your product. You compare the new model's cohort against the baseline cohort. If business KPIs decline, you have a business regression that overrides all technical improvements.

Business KPI regressions happen when you optimize for the wrong proxy. Teams optimize for accuracy because accuracy is easy to measure. Users care about speed, relevance, and trustworthiness. Teams optimize for refusal rates because safety is important. Users care about getting their question answered. Teams optimize for cost because the CFO is watching. Users care about response quality. The only way to detect business KPI regressions is to measure business KPIs. Everything else is a proxy that may or may not align with what matters.

The taxonomy matters because each regression type requires different infrastructure, different thresholds, and different tradeoff decisions. You cannot build a regression suite that catches everything if you do not know what everything is. The next question is what happens when you miss one.

