# 2.3 — Coverage Strategy: Intents, Edge Cases, and Risk

In March 2025, a fintech company shipped a model update that passed every regression test. Their golden set had 60 examples. All 60 passed. The team celebrated, deployed to production, and watched their fraud detection system degrade catastrophically over the next 72 hours. False positives tripled. Legitimate transactions were blocked. Customers called support. The company lost 180,000 dollars in blocked revenue before they rolled back. The root cause was not that their regression suite failed. It was that their golden set had zero examples of the edge case that broke: transactions from new customers with no prior history. The model had learned to flag these as fraud. The golden set never tested for it because nobody thought to include that scenario. Coverage failure.

This is the coverage problem. A golden set can be well-curated, properly sized, and rigorously maintained, and still fail to catch regressions if it does not cover the scenarios that matter. Coverage is not about volume. It is not about testing more examples. It is about testing the right examples. The right examples are the ones that represent core business flows, the edge cases where models break, and the high-risk scenarios where failures cost money, trust, or compliance. If your golden set tests common cases but skips edge cases, you will catch regressions on easy queries while missing regressions on the queries where users actually struggle. If your golden set tests low-risk scenarios but skips high-risk ones, you will miss the failures that matter most.

## The Coverage Matrix

A mature golden set is built around a **coverage matrix** that divides examples across two dimensions: scenario type and risk tier. Scenario type captures what the example tests — core intent, edge case, adversarial case, multi-step flow, refusal case. Risk tier captures the consequences of failure — low risk, medium risk, high risk, critical risk. The matrix tells you how many examples you need in each cell. It ensures that you are not just testing what is easy to test, but what is important to test.

Scenario type is the horizontal axis. Your system handles multiple intents or task types. A customer support bot might handle account inquiries, billing questions, product troubleshooting, refund requests, and escalations. Each intent is a scenario type. Within each intent, you have subcategories: straightforward cases where all information is provided, ambiguous cases where the user query is unclear, edge cases where unusual conditions apply, and adversarial cases where the user is testing system boundaries. Your golden set must cover all of these. If you only test straightforward cases, you are blind to how the model behaves when inputs are messy, incomplete, or adversarial.

Risk tier is the vertical axis. Not all failures cost the same. A model that misclassifies a low-stakes product question costs you nothing. A model that mishandles a high-stakes refund request costs you a customer. A model that leaks PII costs you a compliance violation. A model that provides incorrect medical advice in a health application costs you legal liability. Risk tier categorizes scenarios by the cost of failure. High-risk scenarios get more examples, more scrutiny, and stricter pass thresholds. Low-risk scenarios get fewer examples and more tolerance for occasional failure.

The matrix gives you a two-dimensional map of what you need to test. A minimal viable coverage matrix for a customer support bot might look like this:

- Core intents: 40 examples distributed across 5 intent categories, weighted by production frequency
- Edge cases: 10 examples testing ambiguous queries, missing information, and unusual conditions
- Adversarial cases: 5 examples testing prompt injection, jailbreak attempts, and policy boundary probing
- Refusal cases: 5 examples testing that the model refuses out-of-scope or prohibited requests

Within those 60 examples, you ensure that high-risk scenarios are overweighted. If billing disputes are high-risk because they involve money and customer retention, you include more billing examples than product information examples, even if product information is more common in production. Risk-weighting ensures that failures on high-stakes scenarios get caught before deployment.

## Intent Coverage: Testing What the System Does

Intent coverage ensures that you test every major capability your system claims to support. If your model is designed to handle 8 distinct intents, your golden set must include examples of all 8. The distribution does not need to be uniform. Intents that are more common in production get more examples. Intents that are higher risk get more examples. Intents that are both common and high risk get the most examples. But every intent must be represented. If you skip an intent, you have no visibility into whether the model regresses on that intent.

The allocation heuristic is to start with proportional representation, then adjust for risk. If 50 percent of production queries are intent A, 30 percent are intent B, and 20 percent are intent C, a proportional 60-example golden set would include 30 examples of A, 18 examples of B, and 12 examples of C. But if intent C is high-risk — for example, it involves financial transactions — you might shift the allocation to 25 examples of A, 15 examples of B, and 20 examples of C. The overweighting of intent C ensures that regressions in the high-risk category are detected with higher confidence.

The failure mode of poor intent coverage is silent regression. Your model regresses on intent C, but you only have 3 examples of intent C in your golden set. All 3 pass by luck. Your regression suite reports green. You ship. Production traffic reveals that intent C accuracy dropped from 89 percent to 74 percent. Users encounter failures. Support tickets spike. You roll back. The regression was detectable, but your golden set was not sensitive enough to detect it because intent C was underrepresented.

## Edge Case Coverage: Testing What Breaks

Edge cases are scenarios where the model is most likely to fail. These are queries with missing information, ambiguous phrasing, negation, multi-step reasoning, unusual formatting, or rare vocabulary. Edge cases represent the boundary of the model's capabilities. When a model regresses, it often regresses on edge cases first. If your golden set includes no edge cases, you will not catch regression until it spreads to common cases, by which time the degradation is large enough that users are already complaining.

Edge case coverage is not about testing every possible edge case. That is impossible. It is about testing the edge cases that matter most. The priority is edge cases that: appear frequently enough to affect user experience, expose known model weaknesses, or carry high risk if mishandled. A medical AI that struggles with negation should have edge case examples with negation. A contract analysis AI that struggles with multi-clause conditionals should have edge case examples with multi-clause conditionals. A customer support bot that struggles with frustrated or angry language should have edge case examples with that tone.

The allocation for edge cases is typically 15 to 25 percent of the golden set. If you have 60 examples, 10 to 15 should be edge cases. If you have 100 examples, 15 to 25 should be edge cases. The exact number depends on how much edge case traffic you see in production and how critical edge case performance is to user satisfaction. For systems where most queries are straightforward, 10 percent edge case coverage is sufficient. For systems where 30 percent of production queries are edge cases — because users only escalate to AI when the simple flows fail — you need more.

## Adversarial Coverage: Testing Policy Boundaries

Adversarial cases are queries that deliberately test the limits of what your system should do. These include prompt injection attempts, jailbreak attempts, requests for prohibited content, requests outside the system's scope, and queries designed to elicit unsafe or biased outputs. Most production traffic is not adversarial. But adversarial queries are disproportionately important because they expose safety and compliance gaps. A model that fails on adversarial queries can generate toxic outputs, leak training data, violate content policies, or behave in ways that create legal liability.

Adversarial coverage is not optional for production systems. If your system is user-facing, users will probe it. Some users will do so innocently, asking questions the system should refuse. Some users will do so deliberately, testing whether they can bypass safety controls. If your golden set includes no adversarial examples, you have no visibility into whether your model is handling these cases correctly. You will discover adversarial failures when users report them, by which time the damage is done.

The allocation for adversarial cases is typically 5 to 10 percent of the golden set. If you have 60 examples, 3 to 6 should be adversarial. If you have 100 examples, 5 to 10 should be adversarial. These examples should cover the most common adversarial patterns: requests for harmful content, attempts to override system instructions, attempts to extract training data, attempts to bias the model toward unethical behavior. You do not need to test every possible attack. You need to test the attacks that are most likely and most damaging if they succeed.

## Refusal Coverage: Testing What the System Should Not Do

Refusal cases are queries the model should decline to answer. These include out-of-scope requests, requests that require expertise the model does not have, requests that violate policy, and requests where the model lacks sufficient information to provide a safe answer. Refusal behavior is a feature, not a bug. A model that refuses inappropriate requests is more trustworthy than a model that attempts every request and fails unpredictably.

Refusal coverage ensures that your model maintains appropriate boundaries. If you ship a model update that reduces refusal rates, you need to know whether it reduced over-refusal — where the model refused legitimate requests — or under-refusal — where the model accepted requests it should have declined. Both are important. Over-refusal harms usability. Under-refusal harms safety and compliance. Your golden set must test both.

The allocation for refusal cases is typically 5 to 10 percent of the golden set, split evenly between legitimate requests that should succeed and policy-violating requests that should fail. If you have 60 examples, 3 should be legitimate requests that test for over-refusal, and 3 should be policy-violating requests that test for under-refusal. This balance ensures that you catch regressions in both directions.

## Risk-Weighting: Allocating by Consequence

Risk-weighting adjusts your coverage allocation based on the cost of failure. High-risk scenarios get more examples even if they are rare in production. Low-risk scenarios get fewer examples even if they are common. The logic is simple: catching one high-risk regression is worth missing ten low-risk regressions. You allocate your limited golden set budget toward the failures that matter most.

Risk tiers are defined by consequence. **Critical risk** scenarios are those where failure creates regulatory violations, legal liability, severe reputational damage, or financial loss exceeding 100,000 dollars. These get the highest allocation. If you have 60 examples and 10 percent of your production distribution is critical risk, you might allocate 20 percent of your golden set — 12 examples — to critical risk scenarios. The overweighting ensures that you detect critical regressions with high confidence even if critical scenarios are statistically uncommon.

**High risk** scenarios are those where failure creates customer churn, support escalations, or financial loss between 10,000 and 100,000 dollars. These get above-proportional allocation. If 20 percent of production is high risk, you might allocate 25 to 30 percent of your golden set to high risk scenarios. **Medium risk** scenarios are those where failure creates user frustration or minor financial loss below 10,000 dollars. These get proportional allocation. **Low risk** scenarios are those where failure has negligible consequence. These get below-proportional allocation or are excluded entirely if you are constrained on golden set size.

The failure mode of ignoring risk weighting is that you catch regressions on scenarios that do not matter while missing regressions on scenarios that do. Your golden set is 80 percent low-risk scenarios because those are easy to collect and annotate. You test them thoroughly. You catch every regression in low-risk behavior. Meanwhile, your model regresses on high-risk scenarios, and you have only 2 examples of high-risk behavior in your golden set. Both pass by luck. You ship. The high-risk regression reaches production. It costs you 200,000 dollars. The regression was preventable. Your golden set was not built to prevent it.

## Coverage Gaps and How to Find Them

Coverage gaps are the scenarios your golden set does not test. Every golden set has gaps. The question is whether your gaps are in low-risk scenarios you can tolerate missing, or high-risk scenarios you cannot afford to miss. Finding coverage gaps requires periodic coverage audits where you map your golden set against production behavior, incident history, and stakeholder priorities.

The audit process is straightforward. First, analyze production traffic from the last 90 days. Categorize queries by intent, risk tier, and edge case type. Calculate the distribution. Compare it to your golden set distribution. Identify intents, risk tiers, or edge case types that are underrepresented. Second, review incident history. For every production incident in the last 6 months, check whether your golden set includes an example that would have caught the failure. If not, you have a coverage gap. Third, interview stakeholders. Ask Product, Trust and Safety, and Legal what scenarios they are most worried about. Check whether your golden set tests those scenarios. If not, you have a coverage gap.

When you find a gap, you have three options. First, add examples to fill the gap. This is the right choice if the gap is high risk or high frequency. Second, accept the gap if it is low risk and you are constrained on golden set size. Document the decision so future engineers know the gap is deliberate. Third, build a separate eval set to test the gap. This is the right choice if the gap requires specialized testing that does not fit the golden set's purpose. For example, if you need to test 200 examples of a rare but high-risk edge case, that might justify a dedicated edge case eval set rather than expanding the golden set.

## The Temptation to Over-Cover

Some teams respond to coverage gaps by expanding their golden set indefinitely. They add examples for every possible scenario until the set has 300 examples and takes an hour to run. This is a mistake. A 300-example golden set is too large to maintain. It will decay faster than you can update it. It will include redundant examples that add no signal. It will slow down your release process until engineers start skipping regression tests to meet deadlines. The goal is not comprehensive coverage. The goal is targeted coverage of the scenarios that matter most, executed with enough frequency that regressions are caught before they reach production.

If you find yourself needing more than 100 examples to achieve acceptable coverage, you do not need a larger golden set. You need a multi-tier eval strategy. Keep the golden set small — 50 to 100 examples — focused on the highest-risk, highest-frequency scenarios, and run it on every release candidate. Build a broader eval set — 300 to 1,000 examples — that covers more scenarios with more depth, and run it weekly or before major releases. Build specialized eval sets for specific risks — adversarial robustness, bias, multilingual performance — and run them quarterly or when relevant components change. This multi-tier approach gives you both fast feedback on critical regressions and comprehensive validation when you need it, without forcing every release to wait for a 1,000-example eval to complete.

The next subchapter covers sourcing: where high-quality golden set examples come from and how to collect them without introducing bias or noise.
