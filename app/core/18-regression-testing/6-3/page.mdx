# 6.3 — Relative Gates: Comparison to Baseline

Most teams treat absolute thresholds as the gold standard for release gates. Accuracy above 92 percent. Latency below 500 milliseconds. Hallucination rate under 2 percent. They believe fixed numbers represent objective quality standards that transcend context. They are wrong. For many systems, especially those evolving rapidly or serving diverse use cases, relative gates — gates that compare the new version to the current production version — provide more reliable protection against regression and more realistic standards for continuous improvement.

The fundamental insight behind relative gates is simple: the most important question for most releases is not "is this good enough in absolute terms" but rather "is this better than what we have now, or at minimum, no worse." Your users experience quality relative to their previous interactions. If your system currently achieves 89 percent accuracy and 600-millisecond latency, a new version with 88 percent accuracy and 400-millisecond latency might delight users despite missing your absolute accuracy threshold. Conversely, a version that hits 92 percent accuracy but introduces a new failure mode users never encountered before represents a regression even if all absolute metrics pass.

Relative gates protect against the trap of premature optimization and the tyranny of arbitrary standards. They acknowledge that "good enough" evolves with your system's capabilities and your users' expectations.

## The Baseline Comparison Pattern

A relative gate compares every candidate release against a designated baseline version. The simplest form: the candidate must perform no worse than the baseline on every critical metric. If your production model achieves 87 percent accuracy, 450-millisecond p95 latency, and 3 percent refusal rate, the candidate must achieve at minimum 87 percent accuracy, 450-millisecond latency, and 3 percent refusal rate. Any metric that degrades triggers gate failure.

This approach works particularly well for systems where absolute thresholds are difficult to set. A customer service agent handling returns might need different accuracy standards than one handling billing disputes. A medical documentation assistant might prioritize recall over precision, but the exact balance depends on the specialty and workflow. Setting a universal accuracy threshold requires either extensive negotiation across stakeholders or arbitrary decisions that satisfy no one. A relative gate sidesteps the debate: the new version must be at least as good as the current version, period.

The baseline is typically the current production version, but not always. For teams practicing blue-green deployments, the baseline might be the previous stable release rather than the currently deployed candidate. For teams with multiple production models serving different customer segments, the baseline might be segment-specific. For teams testing major architectural changes, the baseline might be the last version before the change rather than the absolute latest release.

The key requirement: the baseline must be stable, well-understood, and representative of user experience. A baseline that itself represents a temporary regression or a partially deployed experiment creates false comparisons. You need a reference point that reflects the quality users actually receive.

## Relative Thresholds and Tolerance Bands

Pure "no worse than baseline" gates work for critical metrics where any degradation is unacceptable. Safety metrics often fall into this category. If your content moderation system currently catches 96 percent of policy violations, a candidate that catches 94 percent represents a meaningful regression regardless of other improvements. The gate should fail.

For less critical metrics, tolerance bands provide flexibility. A candidate is allowed to underperform the baseline by up to some percentage or absolute amount. If production latency is 400 milliseconds, you might allow candidates up to 420 milliseconds — a 5 percent degradation tolerance. If production accuracy is 89 percent, you might allow candidates down to 87.5 percent — a 1.5 percentage point tolerance.

Tolerance bands acknowledge measurement variance and the reality of engineering tradeoffs. No two eval runs produce identical results. A candidate that scores 88.9 percent where the baseline scored 89.1 percent might simply reflect eval set randomness rather than true quality loss. A small tolerance band filters out noise. Similarly, tolerance bands allow releases that improve one dimension at the expense of minor degradation in another. A candidate that reduces latency from 400 to 300 milliseconds while dropping accuracy from 89 to 88 percent might represent a worthy tradeoff for latency-sensitive applications.

The size of your tolerance bands reflects your risk appetite and measurement confidence. Tight bands — 2 percent latency, half a percentage point accuracy — provide strong regression protection but increase false rejections. Wide bands — 10 percent latency, 2 percentage points accuracy — reduce false rejections but allow more degradation. Teams with high measurement variance need wider bands. Teams serving high-stakes use cases need tighter bands. There is no universal standard. Your bands should reflect the smallest degradation your users would notice and care about.

## The Ratchet Pattern: Baseline Moves Up, Never Down

The most powerful application of relative gates is the ratchet pattern. Every time a candidate passes the gates and deploys to production, it becomes the new baseline. Future candidates must meet or exceed this new standard. Over time, the baseline rises. Your minimum acceptable quality increases with each successful release.

The ratchet pattern creates continuous quality improvement without requiring anyone to manually update threshold configurations. In January, your baseline accuracy is 86 percent. A March release achieves 88 percent and deploys. Now 88 percent is the baseline. A June release must hit 88 percent or higher. If it achieves 90 percent, that becomes the new baseline. By December, your minimum acceptable accuracy might be 93 percent — not because anyone decided to set that threshold, but because your system organically improved over the year and you refused to allow regression.

This pattern works particularly well for teams practicing frequent small releases. Each release makes incremental quality improvements. The ratchet ensures you never lose those gains. Even if a future release focuses on cost reduction or latency optimization and does not improve accuracy, it cannot degrade accuracy below the highest level your system has achieved. The quality floor rises with your capabilities.

The ratchet pattern requires discipline about baseline updates. You only update the baseline after a candidate has proven itself in production. A release that passes gates but causes production issues does not become the new baseline. You roll back and retain the previous baseline until you ship a genuinely superior version. Similarly, you do not update the baseline based on cherry-picked eval runs. The baseline reflects the measured performance of the current production system, not the best score you ever achieved in testing.

Some teams implement time-based baseline decay to handle cases where the production system itself degrades over time due to data drift or infrastructure changes. If production performance drops from 90 to 87 percent over three months due to changing user queries, you might automatically lower the baseline to reflect reality rather than holding candidates to a standard the current system no longer meets. This prevents a situation where you cannot ship any release because production has silently degraded below its own historical baseline. The decay mechanism should be slow, well-monitored, and trigger alerts. A dropping baseline signals a production problem that requires investigation, not just acceptance.

## Relative Gates for New Features Without History

Relative gates assume you have a baseline to compare against. What do you do when launching a genuinely new feature or capability that has no production history? You have three options.

First, you can set temporary absolute thresholds for the new feature while using relative gates for existing features. The new feature must hit minimum quality standards to launch, but those standards are fixed until the feature ships and generates a baseline. After the first production deployment, you switch to relative gates. This approach works well when you have enough confidence to set initial thresholds, even if you know those thresholds will evolve.

Second, you can use a synthetic baseline created from a holdout set or reference model. You evaluate a known-good model or approach on the same eval set you will use for the new feature, treating its performance as the baseline. The candidate must meet or exceed this synthetic baseline to deploy. After deployment, you switch to using actual production performance as the baseline. This approach works when you have a clear reference point — a previous model version, a rule-based system you are replacing, or a human performance benchmark.

Third, you can launch the new feature with observational monitoring rather than gates, collecting data to establish a baseline before enforcing relative gates. The first version ships without comparison, but with instrumentation that measures all key metrics. After several weeks or months in production, you analyze the data, establish baseline performance, and enable relative gates for all future releases. This approach works for low-risk features or internal pilots where you can tolerate initial instability in exchange for real-world data.

The worst option is pretending you have a baseline when you do not. Teams sometimes compare new features to unrelated production metrics or to aspirational targets and call it a relative gate. If your new multilingual support capability is compared to your English-only baseline, you are not making a meaningful comparison. If your new code generation feature is compared to your existing text generation model, the metrics are not comparable. Relative gates require apples-to-apples comparison. When that comparison is impossible, admit it and use a different gating strategy.

## Statistical Significance in Relative Comparisons

A candidate scores 88.2 percent accuracy where the baseline scored 87.9 percent. Is this a meaningful improvement or measurement noise? Without statistical testing, you cannot know. Relative gates that rely on point estimates treat all differences as real, leading to false positives when candidates appear better due to eval set variance and false negatives when genuine improvements fall within the noise.

Proper relative gates incorporate statistical significance testing. You run the candidate and baseline on the same eval set, compute confidence intervals around both measurements, and require that any claimed improvement be statistically significant at a chosen confidence level — typically 95 percent. If the confidence intervals overlap, you treat the two versions as equivalent. If the candidate's confidence interval lies entirely above the baseline's, you have evidence of genuine improvement.

For degradation detection, the logic inverts. If the candidate's confidence interval lies entirely below the baseline's, you have evidence of regression and the gate fails. If the intervals overlap, you treat performance as equivalent and allow the release. This approach protects against blocking releases due to measurement variance while still catching real regressions.

Statistical testing requires sufficient eval set size. Small eval sets produce wide confidence intervals, making it impossible to detect any but the largest differences. If your eval set contains 100 examples and your baseline accuracy is 85 percent, the 95 percent confidence interval spans roughly 78 to 92 percent. A candidate scoring 82 percent would be statistically indistinguishable from the baseline despite appearing 3 percentage points worse. You need several hundred examples per metric to achieve tight enough confidence intervals for meaningful comparison.

Some teams use bootstrap resampling or other resampling techniques to estimate confidence intervals without making distributional assumptions. You resample the eval set with replacement thousands of times, compute the metric for each resample, and use the distribution of resampled metrics to construct confidence intervals. This approach works well when your eval set is moderately sized and your metric is non-standard.

Statistical testing adds complexity to your release pipeline. You need tooling that computes confidence intervals, compares distributions, and reports significance. You need engineers who understand what statistical significance means and does not mean. But for teams serious about relative gates, the investment is worthwhile. The alternative is making release decisions based on differences that might not exist.

## When Relative Gates Outperform Absolute Gates

Relative gates excel in three scenarios. First, when you are iterating rapidly on a new system and absolute quality standards are still evolving. A team building a new contract review assistant might not know what "good" accuracy looks like until they have shipped several versions and gathered user feedback. Absolute gates force premature commitment to thresholds that might be too strict or too lenient. Relative gates allow continuous improvement without requiring anyone to predict final quality requirements.

Second, when your system serves diverse use cases with different quality requirements. A general-purpose coding assistant used for Python web development, embedded C firmware, and SQL analytics faces different expectations in each domain. Setting a universal accuracy threshold either blocks legitimate releases in hard domains or allows regressions in easy domains. Relative gates ensure no domain regresses regardless of absolute performance levels.

Third, when external factors cause measurement drift that makes absolute thresholds unstable. If your eval set evolves over time to reflect changing user queries, a fixed accuracy threshold becomes harder to interpret. A model scoring 90 percent on this month's eval set might be better or worse than a model that scored 92 percent six months ago, depending on how the eval set changed. Relative gates anchor to current conditions. The candidate must perform better than the baseline on today's eval set, not better than some historical score on a different eval set.

Relative gates do not replace absolute gates. They complement them. Most systems use both. Absolute gates enforce minimum acceptable quality for safety, compliance, or user experience. Relative gates prevent regression and drive continuous improvement. A medical coding system might require absolute accuracy above 85 percent for regulatory reasons and relative accuracy no more than 1 percentage point below the current production baseline. Both gates matter. The absolute gate ensures you never ship something dangerously bad. The relative gate ensures you never ship something worse than what users have today.

The teams that ship high-quality AI products most consistently are not the ones with the strictest absolute thresholds. They are the ones with the clearest baseline comparisons and the most discipline about preventing regression.

Next, we examine multi-dimensional gates, where multiple metrics must simultaneously pass for a release to proceed.
