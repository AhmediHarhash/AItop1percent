# 4.2 â€” Query Rewriting and Expansion Techniques

A financial services company deployed an internal knowledge retrieval system in June 2025 to help compliance officers search through regulatory documentation. The system used state-of-the-art embeddings and a carefully chunked corpus of 50,000 regulatory documents. Within two months, usage dropped to near zero. The problem was not the technology. The problem was that compliance officers used domain-specific acronyms and shorthand that appeared nowhere in the regulatory documents themselves. When an officer searched for "KYC violations," the system failed because the documents used the full phrase "Know Your Customer compliance failures." When someone searched for "AML reporting requirements," the system missed documents titled "Anti-Money Laundering Reporting Standards." When users typed "GDPR right to be forgotten," the system could not connect it to documents about "data subject erasure requests under Article 17." By September 2025, the company had wasted eight hundred thousand dollars building a system nobody used. The retrieval technology was perfect. The queries were incompatible with the corpus.

You are dealing with the vocabulary mismatch problem, and it is killing your retrieval quality. Users express information needs using their own mental models, terminology, and linguistic habits. Your documents express information using different terminology, formality levels, and phrasings. Even when the user and the document are talking about exactly the same concept, they might use completely different words. This mismatch is the single biggest cause of retrieval failure in production RAG systems. Vector embeddings help by capturing semantic similarity, but they are not magic. If a user query and a relevant document share no overlapping vocabulary and have no training examples connecting them, the embedding model will fail to recognize their similarity.

Query rewriting transforms user queries into forms that better match your document corpus. You do not change what the user is asking. You change how the question is expressed to maximize retrieval effectiveness. You expand abbreviations, resolve acronyms, add synonyms, correct spelling errors, normalize terminology, and reformulate the question structure. The goal is to bridge the vocabulary gap between user language and document language. A user query like "AML filing deadline" becomes "Anti-Money Laundering reporting deadline, BSA filing deadline, suspicious activity report deadline." You have added the full acronym expansion, the related regulatory term, and the specific report name. Your retrieval surface area has expanded dramatically, increasing the chance of matching relevant documents.

## Synonym Expansion Multiplies Your Retrieval Surface

The simplest form of query rewriting is synonym expansion. You identify key terms in the user query and add their synonyms. A query about "customer complaints" becomes "customer complaints, customer grievances, customer feedback, customer issues, user-reported problems." A query about "product defects" becomes "product defects, manufacturing flaws, quality issues, product failures, defective items." Each synonym gives the retrieval system another chance to match relevant documents that use different terminology.

Synonym expansion is not as simple as looking up words in a thesaurus. You need domain-specific synonyms, not generic English synonyms. In healthcare, "adverse event" and "side effect" and "complication" are synonyms. In finance, "liquidation" and "wind down" and "closure" are synonyms. In software development, "bug" and "defect" and "issue" are synonyms. Your synonym expansion must respect your domain, or you will add noise instead of signal. A thesaurus might tell you that "complaint" is a synonym for "lament" or "grumble," but adding those terms to a customer support query would be useless.

You build domain-specific synonym lists by mining your corpus. You identify terms that appear in similar contexts, co-occur frequently, or are used interchangeably in your documents. You can use statistical methods like word co-occurrence counting or word embedding similarity. You can also manually curate synonym lists by reviewing your most common query terms and identifying alternative phrasings used in your documents. A combination of automated mining and manual curation works best. The automated approach gives you coverage. The manual approach gives you precision.

Once you have synonym lists, you apply them to incoming queries. You identify entities and key terms in the query using named entity recognition or keyword extraction. For each identified term, you look up its synonyms and add them to the query. You then embed the expanded query and retrieve. The expanded query has higher recall than the original query because it matches documents using any of the synonym terms. The risk is that you also increase noise. If your synonyms are too broad or context-inappropriate, you will retrieve irrelevant documents. You manage this risk by using conservative synonym expansion that only adds close synonyms, and by relying on reranking to filter out noise from the expanded results.

## Acronym Resolution Bridges Jargon Gaps

Every domain has acronyms, and every organization has its own acronym vocabulary. Users type "ROI" when documents say "return on investment." Users type "SLA" when documents say "service level agreement." Users type "EOD" when documents say "end of day." If your retrieval system does not expand these acronyms, it will miss relevant documents. Acronym resolution is synonym expansion for abbreviations. You maintain a mapping from acronyms to their full forms, and you expand user queries by adding the full forms alongside the acronyms.

Acronym resolution is complicated by ambiguity. Many acronyms have multiple meanings. "AI" could mean "artificial intelligence" or "action item." "ER" could mean "emergency room" or "entity relationship." "PM" could mean "project manager," "product manager," "post meridiem," or "performance management." You cannot blindly expand every acronym to all possible meanings, or you will flood your query with irrelevant terms. You need context-aware acronym resolution that picks the right expansion based on the query context and user role.

You implement context-aware acronym resolution using a combination of domain dictionaries and disambiguation logic. You maintain a primary acronym dictionary that maps acronyms to their most common expansions in your domain. For ambiguous acronyms, you store multiple possible expansions ranked by frequency or relevance. When you encounter an acronym in a query, you look at the surrounding words to determine which expansion is most appropriate. If a user asks "what is the AI roadmap," and they work in a technology company, you expand "AI" to "artificial intelligence." If they ask "who is responsible for the AI from yesterday's meeting," you expand "AI" to "action item."

You can also use user metadata for disambiguation. If the user is a physician, "ER" expands to "emergency room." If the user is a software engineer, "ER" expands to "entity relationship." If the user is a project manager, "PM" expands to "project management." This role-based disambiguation significantly improves acronym resolution accuracy. You can make it even more sophisticated by using conversational context. If the previous query mentioned "database design," the current query's "ER" likely refers to "entity relationship." If the previous query mentioned "hospital operations," the current query's "ER" likely refers to "emergency room."

When you cannot disambiguate with confidence, you can expand to multiple possible meanings and let retrieval and reranking sort it out. A query "what is the PM process" might expand to "what is the project management process, product management process, performance management process." You retrieve for all three interpretations, and the user's actual intent will be reflected in which retrieved documents are most relevant. This multi-expansion approach increases retrieval cost but improves recall for ambiguous queries. You use it selectively for high-ambiguity acronyms when context is insufficient for confident disambiguation.

## Spelling Correction Saves User Errors

Users make typos. They misspell technical terms. They misremember product names. If your retrieval system cannot handle spelling errors, you will return zero results for misspelled queries, leading to terrible user experience. Spelling correction detects errors in user queries and suggests or automatically applies corrections before retrieval. A query "retreival system" becomes "retrieval system." A query "customr feedback" becomes "customer feedback." A query "machien learning" becomes "machine learning."

Spelling correction for RAG is different from web search spelling correction. In web search, you can rely on massive query logs to identify common misspellings and their corrections. In enterprise RAG, you have limited query volume and domain-specific vocabulary that general-purpose spellcheckers do not recognize. You need to build spelling correction that understands your domain terminology. A medical spellchecker must recognize "myocardial infarction" as correct, not suggest "myocardial infraction." A legal spellchecker must recognize "indemnification" as correct, not suggest "identification."

You build domain-aware spelling correction by combining a general-purpose spellchecking algorithm with a custom dictionary extracted from your corpus. You use algorithms like edit distance or phonetic matching to identify potential corrections for misspelled words. You then filter candidate corrections against your corpus vocabulary to ensure they are relevant to your domain. If a user types "retreival," the edit distance algorithm suggests "retrieval" and "retrial." You check which of these appears in your corpus. If your corpus is about information systems, "retrieval" appears frequently and "retrial" does not, so you choose "retrieval." If your corpus is about legal proceedings, "retrial" appears frequently and you choose that instead.

You also need to decide when to correct automatically and when to suggest corrections. Automatic correction is appropriate when the error is obvious and the correction is confident. If a user types "machien learning" and your corpus contains "machine learning" hundreds of times but never contains "machien," you can safely auto-correct. If a user types "lead generation" in a sales context, you should not correct "lead" to "led," even though "led" is a valid English word. The query is not misspelled. You avoid false positive corrections by checking whether the original query terms appear in your corpus. If they do, assume the query is correct even if the terms would normally be flagged as misspellings.

When you cannot confidently auto-correct, you suggest corrections to the user. "Did you mean retrieval system?" This is better UX than silently correcting and potentially changing the user's intent. It also helps you learn. If users consistently accept a certain correction, you can add it to your auto-correction rules. If users consistently reject a correction, you can add the term to your custom dictionary as a valid word. Over time, your spelling correction improves by learning from user behavior.

## LLM-Based Query Rewriting Goes Beyond Simple Rules

Synonym expansion, acronym resolution, and spelling correction are rule-based techniques. They work well for common cases, but they struggle with complex queries that need structural reformulation. LLM-based query rewriting uses a language model to rewrite the entire query into a form that is better suited for retrieval. You prompt the LLM with the original query and instructions like "Rewrite this query to be more specific and use terminology that would appear in professional documentation." The LLM returns a rewritten query that is clearer, more explicit, and more likely to match your corpus.

LLM-based rewriting handles cases that rules cannot. A user asks "why is the thing not working." An LLM can infer from conversational context or user role what "the thing" refers to and rewrite the query as "why is the payment processing system not working" or "why is the email integration not working." A user asks "how do I do that." An LLM can look at the previous message, see that it mentioned password resets, and rewrite the current query as "how do I reset a user password." A user asks "give me the latest numbers." An LLM can infer from user role and context that "numbers" means "quarterly sales figures" or "application performance metrics" and rewrite accordingly.

The power of LLM rewriting is that it can restructure the query in ways that rule-based systems cannot. A user asks "what changed." An LLM rewrites it as "what changes were made to the product in the most recent release." A user asks "how does ours compare." An LLM rewrites it as "how does our enterprise pricing plan compare to competitor pricing plans." The LLM is using world knowledge, conversational context, and linguistic understanding to transform vague queries into specific retrieval requests. This dramatically improves retrieval quality for ambiguous or underspecified queries.

The cost of LLM rewriting is latency and money. Each query rewrite requires an LLM API call, adding 200 to 800 milliseconds of latency depending on the model and prompt complexity. At scale, this adds up to significant inference costs. You need to decide whether the retrieval quality improvement justifies the cost. For high-value use cases like legal research, medical diagnosis support, or executive decision support, the cost is worth it. For casual knowledge base browsing, it might not be. You can also use tiered rewriting: apply LLM rewriting only to complex or ambiguous queries, and use rule-based rewriting for simple queries.

Another consideration is that LLM rewriting can change user intent if done incorrectly. If the LLM misinterprets the query or hallucinates context that does not exist, it will rewrite the query into something the user did not mean. You mitigate this risk by using conservative rewriting prompts that instruct the LLM to stay close to the original query and avoid adding assumptions. You can also show the rewritten query to the user before retrieval, allowing them to confirm or correct it. "I understood your question as: [rewritten query]. Is that correct?" This adds interaction cost but ensures you do not retrieve based on misinterpreted intent.

## Query Decomposition Breaks Complexity Into Pieces

Complex queries often contain multiple sub-questions that should be answered separately. A query like "how does our pricing compare to the competitor and what are the main benefits of our premium tier" is two questions. A query like "what are the requirements for HIPAA compliance and how do we audit compliance and what are the penalties for violations" is three questions. If you retrieve for the entire compound query, you get documents that partially match each sub-question but do not fully answer any of them. Query decomposition splits these complex queries into individual sub-questions, retrieves for each separately, and merges the results.

Decomposition improves retrieval because it allows focused retrieval for each sub-question. Instead of retrieving documents that vaguely relate to the entire complex query, you retrieve documents that directly answer each component. You then combine these focused result sets to provide comprehensive information for the full query. The user gets complete answers to all parts of their question instead of shallow coverage of the compound query.

You implement query decomposition using an LLM. You prompt the model with the complex query and ask it to break it down into independent sub-questions. For the pricing comparison example, the LLM returns two sub-questions: "How does our pricing compare to the competitor?" and "What are the main benefits of our premium tier?" For the HIPAA example, the LLM returns three sub-questions: "What are the requirements for HIPAA compliance?" "How do we audit HIPAA compliance?" and "What are the penalties for HIPAA violations?" You then retrieve for each sub-question independently, getting separate result sets for each.

The challenge with decomposition is knowing when to apply it. Not every query needs decomposition. Simple queries should be left alone. You use query complexity scoring from the previous subchapter to determine when decomposition is appropriate. If the query has a complexity score above a certain threshold, you decompose it. If it is below the threshold, you retrieve for the original query. You can also detect explicit multi-question patterns using linguistic cues: queries containing "and what" or "and how" or "and why" are often compound queries that benefit from decomposition.

After decomposition and retrieval, you need a strategy for merging results. One approach is to concatenate all result sets and pass the combined context to the LLM for answer generation. The LLM synthesizes information from all retrieved chunks to answer the full query. Another approach is to generate separate answers for each sub-question and then combine them into a structured response. "Regarding pricing comparison: [answer 1]. Regarding premium tier benefits: [answer 2]." The structured approach is better for complex queries where the sub-questions are distinct topics. The synthesized approach is better when the sub-questions are related and the user expects an integrated answer.

## Before and After Retrieval Quality Comparison

You measure the impact of query rewriting by comparing retrieval quality with and without it. Take a sample of 200 real user queries. Split them into a control group and a treatment group. For the control group, retrieve using the raw queries. For the treatment group, apply your full query rewriting pipeline: spelling correction, acronym expansion, synonym expansion, LLM-based rewriting, and decomposition where appropriate. Measure retrieval recall and precision for both groups. Calculate the difference.

In production systems, query rewriting typically improves recall by 20 to 40 percent. This means you retrieve 20 to 40 percent more relevant documents that you would have missed with raw queries. Precision usually stays flat or decreases slightly, by 5 to 10 percent, because expanded queries introduce some noise. The net effect is strongly positive because recall gains outweigh precision losses. Your system finds more of the information users need, even if it also retrieves a few extra irrelevant documents. Reranking handles the precision problem by filtering out the noise from the expanded results.

You also need to measure latency impact. Query rewriting adds processing time. Rule-based techniques like synonym expansion and acronym resolution add 10 to 30 milliseconds. LLM-based rewriting adds 200 to 800 milliseconds. Query decomposition adds even more because you are making multiple retrieval calls. You need to profile your rewriting pipeline to understand where time is spent and optimize the slow components. If LLM rewriting is too slow, you can use a smaller, faster model for simple rewrites and reserve the large model for complex queries. If decomposition is too slow, you can parallelize the sub-query retrievals to reduce overall latency.

Cost measurement is also critical. LLM-based rewriting costs money per query. If you are processing 10,000 queries per day and each LLM rewrite costs 0.02 dollars, you are spending 200 dollars per day on query rewriting. Over a year, that is over 70,000 dollars. You need to decide whether the retrieval quality improvement justifies this cost. For some applications, it absolutely does. For others, you might choose a hybrid approach where you use LLM rewriting only for queries that fail retrieval with rule-based methods. First, try simple rewriting. If retrieval confidence is low, escalate to LLM rewriting. This reduces cost while maintaining quality for the queries that need it most.

## Rewriting Is Not One-Size-Fits-All

Different queries benefit from different rewriting techniques. Short, simple queries often need expansion to add context and synonyms. Long, complex queries often need decomposition to break them into manageable pieces. Ambiguous queries need clarification and specificity. Misspelled queries need correction. Technical queries need acronym resolution. You cannot apply the same rewriting strategy to every query. You need a routing system that analyzes the query and selects the appropriate rewriting techniques.

Your routing system uses query analysis outputs from the previous subchapter. If the query is classified as low complexity and contains no acronyms or spelling errors, you skip rewriting and use the raw query. If the query contains known acronyms, you apply acronym expansion. If the query is flagged as ambiguous, you apply LLM-based rewriting to add specificity. If the query is classified as high complexity, you apply decomposition. Each query flows through the rewriting pipeline, but only the relevant techniques are applied based on the query's characteristics.

This selective rewriting reduces unnecessary processing and cost while maintaining quality. You are not spending LLM inference budget on queries that do not need it. You are not expanding simple queries that are already effective. You are focusing your rewriting effort on the queries that will benefit most. Over time, you can measure which rewriting techniques provide the most value for which query types and optimize your routing accordingly. If acronym expansion improves retrieval for technical queries but hurts retrieval for conversational queries, you apply it only to technical queries.

Query rewriting is not a silver bullet. It does not fix bad corpus chunking, inadequate metadata, or weak embedding models. It is one component of a retrieval pipeline, and it is most effective when combined with good document processing, thoughtful chunking, rich metadata, and strong reranking. If your documents are poorly chunked or your embeddings are low quality, query rewriting will not save you. But if your fundamentals are solid, query rewriting bridges the vocabulary gap between users and documents, dramatically improving retrieval quality. The companies that build the best RAG systems invest in sophisticated query rewriting pipelines that understand their users, their domain, and their corpus. You should do the same.
