# 2.3 â€” Chunk Size Selection: The Tradeoffs That Actually Matter

In January 2025, a fintech startup building a regulatory compliance assistant discovered that their RAG system was retrieving technically correct chunks but producing useless answers. They used 256-token chunks to maximize retrieval precision. When a compliance officer asked, "What are the reporting requirements for suspicious transactions over fifty thousand dollars?" the system retrieved a chunk containing the exact dollar threshold but missing the context about the reporting timeline, the forms required, and the exceptions. The LLM generated an answer that was incomplete and potentially misleading. The compliance team stopped trusting the system. When the startup increased chunk size to 1024 tokens, answers improved dramatically because chunks now included full regulatory clauses with all conditions and exceptions. However, retrieval precision degraded: queries about specific thresholds returned chunks with multiple unrelated rules because chunks were too large and noisy. They eventually settled on 768 tokens with 128-token overlap, but only after three months of trial and error cost them credibility with early users.

You are selecting a chunk size in 2026, and this decision creates a fundamental tradeoff between retrieval precision and context completeness. Small chunks maximize the likelihood that retrieved chunks exactly match the query topic, but they fragment information and force LLMs to work with incomplete context. Large chunks provide complete context but introduce noise and reduce the signal-to-noise ratio of retrieval. There is no universal optimal chunk size. The right size depends on your document structure, query complexity, and downstream task requirements. This chapter walks through the tradeoffs, the empirical patterns that emerge across domains, and the process for finding the sweet spot for your use case.

## Small Chunks: Precise Retrieval But Fragmented Context

Small chunks, typically 128 to 256 tokens, optimize for retrieval precision. When you embed a small chunk, the embedding captures a narrow, focused piece of information. Queries that match this information will rank the chunk highly because the semantic similarity is concentrated. If a chunk contains only a definition of a technical term, and the query asks for that definition, the chunk is a near-perfect match. Retrieval is precise.

The problem is that most real-world queries require more context than a small chunk provides. A user asking, "How do I configure authentication in this API?" does not just need the authentication function signature. They need the function signature, the required parameters, the expected response format, error handling, and usage examples. If these elements are split across multiple small chunks, the retriever must rank all of them highly, and the LLM must synthesize information from multiple fragments. This works only if the retriever returns all necessary chunks and the LLM successfully integrates them.

In practice, retrieval is imperfect. The top-ranked chunk may contain the function signature, but the chunk with error handling may rank fifth, and the chunk with usage examples may rank tenth. If you retrieve only the top three chunks, the LLM receives incomplete information and produces an incomplete or incorrect answer. If you retrieve ten chunks to ensure coverage, you consume more of the LLM's context window, increase latency, and introduce noise from chunks that are not relevant.

Small chunks also struggle with queries that require understanding relationships or conditions. A regulatory rule may state, "Transactions over fifty thousand dollars must be reported within twenty-four hours, except for transactions involving government entities." If this sentence is split into two chunks, one chunk contains the reporting requirement and the other contains the exception. A query asking about reporting requirements may retrieve the first chunk but not the second, leading the LLM to generate an answer that omits the exception. This is worse than no answer because it is subtly wrong.

The fragmentation problem is especially severe for documents with dense information. Scientific papers, legal contracts, and technical specifications pack multiple related facts into single paragraphs. Small chunks split these paragraphs into pieces that lack the connective tissue explaining how facts relate. An LLM receiving these fragments may hallucinate relationships or fail to understand the logical structure.

Small chunks do have legitimate use cases. For keyword-heavy retrieval or simple fact lookup, small chunks work well. If your corpus is a glossary, a product catalog, or a FAQ database where each entry is self-contained, small chunks maximize precision without sacrificing completeness because each chunk already contains a complete unit of information. For question-answering over structured data where queries target atomic facts, small chunks reduce noise and improve ranking.

The teams that succeed with small chunks are those who verify that their use case fits the constraints. They test whether retrieved small chunks contain sufficient context for their LLM to generate correct answers. They measure how often they need to retrieve many chunks to get complete information. They compare retrieval quality with small chunks against larger chunk sizes on representative queries. They do not choose small chunks because they want high precision. They choose small chunks because they validated that high precision without context fragmentation is achievable for their data.

## Large Chunks: Better Context But Noisy Retrieval

Large chunks, typically 1024 to 2048 tokens, optimize for context completeness. When you embed a large chunk, the embedding captures a broader span of information. Queries may match the chunk even if the exact answer is buried within a longer passage. The advantage is that when the chunk is retrieved, the LLM receives ample context: not just the direct answer, but the surrounding explanations, conditions, examples, and qualifications. This improves answer quality because the LLM has the information needed to generate complete, accurate responses.

The problem is that large chunks reduce retrieval precision. A 2048-token chunk may discuss multiple topics, and the embedding averages over all of them. If the query matches one topic within the chunk but the chunk also contains unrelated information, the semantic similarity is diluted. The chunk may rank lower than a small chunk that focuses exclusively on the query topic. When you retrieve large chunks, you often get chunks where the relevant information is only a small fraction of the content, and the rest is noise.

Noise has costs. It consumes the LLM's context window, reducing the number of chunks you can retrieve. It increases latency because the LLM must process more tokens. It introduces a risk of distraction: the LLM may focus on irrelevant parts of the chunk and generate answers based on noise rather than signal. For short context LLMs, large chunks may exceed the context window entirely, forcing you to truncate or retrieve fewer chunks.

Large chunks also create retrieval ambiguity. If a chunk discusses three different regulatory requirements, a query about one requirement may retrieve the chunk, but the LLM must identify which part of the chunk is relevant. This works if the chunk is well-structured and the LLM's attention mechanism focuses correctly, but it is fragile. If the chunk is poorly structured or the LLM's attention is misaligned, the answer may mix information from different parts of the chunk incorrectly.

Despite these challenges, large chunks are often the right choice for complex domains where context is critical. Legal documents, medical literature, and technical specifications frequently have information that only makes sense with substantial surrounding context. A legal clause may reference definitions from earlier sections, exceptions from later sections, and conditions that span multiple paragraphs. Splitting this into small chunks destroys the logical structure. A large chunk preserving the entire clause allows the LLM to reason over the complete rule.

Large chunks also work well when your queries are broad or exploratory. If users ask open-ended questions like "What are the best practices for data security in this framework?" they expect comprehensive answers drawing from multiple aspects of the framework. A large chunk containing a full section on data security provides better raw material for the LLM than several small, narrow chunks that each address one specific aspect.

The production pattern is to use large chunks when answer completeness is more important than retrieval precision, when your LLM has a long context window, and when your documents have dense, interconnected information that loses meaning when fragmented. You accept that retrieval may be less precise and compensate with reranking, metadata filtering, or hybrid retrieval that combines dense and sparse signals to improve ranking.

## The Sweet Spot: Empirical Patterns Across Domains

Industry experience in 2026 has revealed empirical patterns for chunk sizes across domains. These are not universal rules, but they are starting points based on what has worked in production for similar use cases. The pattern that emerges is that chunk size correlates with information density and query complexity.

For FAQ databases, glossaries, and product catalogs where each entry is self-contained and queries target atomic facts, 128 to 256 tokens works well. Each chunk is a complete answer unit. Retrieval is precise. Context is sufficient. For customer support chatbots over knowledge bases with procedural instructions, 512 to 768 tokens is common. Procedures have multiple steps, and each chunk should contain enough steps to be actionable but not so many that unrelated procedures are mixed into the same chunk.

For technical documentation, API references, and user manuals, 768 to 1024 tokens is typical. A chunk should contain a complete function description, parameter list, and usage example, or a complete section explaining a concept with examples. For scientific papers, medical literature, and research articles, 1024 to 1536 tokens is common. Chunks should preserve paragraph or section coherence and include enough context for the LLM to understand experimental methods, results, or arguments.

For legal documents, regulatory filings, and contracts, 1024 to 2048 tokens is frequent. Legal text is dense with conditions, exceptions, and references to other clauses. Small chunks fragment rules and lose critical qualifications. Large chunks preserve the logical structure of clauses. For books, long-form articles, and narrative content, chunk size varies widely based on whether retrieval is for topic discovery or deep reading. For topic discovery, 512 to 768 tokens works. For deep reading where retrieved chunks are presented to users directly, 1024 to 2048 tokens provides better reading experience.

These patterns are descriptive, not prescriptive. They describe what teams have converged on after empirical tuning, but they are not guarantees that the same size will work for your use case. The only way to find the optimal chunk size for your system is to test multiple sizes with your documents and queries.

## Benchmarking Chunk Sizes: The Empirical Approach

Benchmarking chunk sizes is straightforward: chunk your corpus with different sizes, embed the chunks, run representative queries, retrieve results, and evaluate answer quality. The challenge is defining representative queries and measuring answer quality in a scalable way.

Start by collecting a set of test queries that reflect real user information needs. If you have production query logs, sample from them. If you are building a new system, generate queries based on common questions in your domain. Aim for diversity: simple fact lookups, complex multi-part questions, broad exploratory queries, and narrow specific queries. You need at least fifty to one hundred test queries to get reliable signals, more if your use case has high variability.

For each query, define the ground truth answer or the expected behavior. For fact lookups, the ground truth is the correct fact. For complex questions, the ground truth may be a reference answer written by a domain expert. For exploratory queries, the ground truth may be a list of topics or sections that should be covered in the response. Ground truth does not need to be perfect, but it needs to be good enough to distinguish between acceptable and unacceptable answers.

Chunk your corpus with multiple sizes: 256, 512, 768, 1024, and 1536 tokens are reasonable test points. For each chunk size, embed the corpus, run your test queries, retrieve the top five or ten chunks, and generate answers with your LLM using the retrieved chunks as context. Evaluate the answers against ground truth. If you have ground truth answers, use automated metrics like BLEU, ROUGE, or BERTScore. If you have reference answers, use LLM-as-a-judge to compare generated answers against references. If you have topic lists, check whether generated answers cover the expected topics.

Manual evaluation is also valuable. Read a sample of generated answers for each chunk size and assess whether they are complete, accurate, and coherent. Note patterns: do answers from small chunks frequently miss critical context? Do answers from large chunks include irrelevant information or fail to focus on the query topic? This qualitative feedback complements quantitative metrics and helps you understand why certain chunk sizes perform better.

Plot retrieval precision and answer quality as a function of chunk size. Precision typically decreases as chunk size increases because larger chunks are less focused. Answer quality typically increases to a point and then plateaus or decreases as chunks become so large they introduce noise. The optimal chunk size is where answer quality is maximized or where the tradeoff between precision and completeness is balanced according to your priorities.

You should also measure latency and cost as a function of chunk size. Larger chunks mean fewer chunks for the same corpus, which reduces vector database size and retrieval latency. However, larger chunks consume more of the LLM's context window, which may increase LLM latency and cost. If you retrieve five 2048-token chunks, you consume over ten thousand tokens of context, which may exceed the limits of smaller models or significantly increase costs with frontier models.

## Why 512 Tokens Is Not Always Right

The number 512 appears in countless RAG tutorials, example code repositories, and default configurations. It has become a de facto standard, and many teams adopt it without questioning whether it fits their use case. The reason 512 is common is not because it is optimal. It is common because it is a convenient round number that fits well within the context windows of older embedding models and LLMs.

Early embedding models like the original BERT had 512-token context windows. Chunking documents into 512-token pieces ensured chunks fit within the embedding model's input limit. As models evolved and context windows expanded, 512 remained the default due to inertia. Libraries and frameworks set 512 as the default chunk size, and users accepted it without testing alternatives.

For many use cases, 512 tokens is a reasonable compromise. It is large enough to contain a few paragraphs of context, but small enough to maintain some retrieval precision. It works acceptably for general-purpose knowledge bases, documentation, and articles. This is why it persists as a default: it is rarely catastrophically wrong.

However, 512 tokens is frequently suboptimal. For FAQ databases, it is too large and introduces noise. For legal contracts, it is too small and fragments clauses. For scientific papers, it splits abstracts and methods sections awkwardly. For code documentation, it may cut function examples in half. The teams that blindly use 512 tokens are leaving retrieval quality on the table.

The pattern in production is that teams start with 512 tokens because it is the default, encounter retrieval quality issues, and eventually benchmark chunk sizes empirically. By the time they discover that 768 or 1024 tokens works better for their use case, they have already accumulated technical debt: a vector database full of 512-token chunks, application logic that assumes 512-token chunks, and user expectations calibrated to the performance of 512-token chunks. Reprocessing the corpus with a new chunk size is disruptive and expensive.

The lesson is to validate chunk size before committing to it at scale. Benchmark multiple sizes with representative queries during development, before you index your full corpus. Treat chunk size as a hyperparameter to be tuned, not a constant to be copied from tutorials. This upfront investment prevents costly reprocessing later.

## Domain-Specific Optimal Sizes and Adaptive Chunking

Some teams take chunk size optimization further and use domain-specific or document-specific chunk sizes. Instead of a single global chunk size, they configure different sizes for different document types based on empirical performance. Legal documents use 1536 tokens, technical documentation uses 768 tokens, and FAQ entries use 256 tokens. This hybrid approach maximizes retrieval quality across a heterogeneous corpus.

Implementing domain-specific chunk sizes requires metadata-driven ingestion. During document ingestion, classify the document type and store it in metadata. The chunking stage reads the document type and selects the appropriate chunk size from a configuration table. Downstream retrieval logic is unchanged because it operates on chunks agnostic of their size. The complexity is in the ingestion pipeline, where you must maintain multiple chunking configurations and ensure consistency.

Adaptive chunking goes a step further and adjusts chunk size based on document content rather than document type. For example, dense, information-rich sections are chunked smaller to improve precision, while sparse, narrative sections are chunked larger to preserve context. This requires analyzing content during chunking: measuring information density, detecting section types, or using heuristics like sentence count and vocabulary diversity to estimate how focused a section is.

Adaptive chunking is complex and has limited adoption in production as of 2026. It requires custom chunking logic, validation that chunk size variation does not confuse retrieval, and careful tuning to avoid pathological cases where a single document produces wildly varying chunk sizes. The benefit is incremental retrieval quality improvements, which may not justify the engineering cost unless your use case is highly sensitive to chunk size and your corpus is highly heterogeneous.

The pragmatic approach for most teams is to use a small number of discrete chunk sizes, two to four, and assign them based on document type. This captures most of the benefit of domain-specific sizing without the complexity of adaptive algorithms. Reserve adaptive chunking for future optimization after you have exhausted simpler strategies.

## Chunk Size as a Continuous Optimization Target

Chunk size is not a set-and-forget decision. As your corpus evolves, as query patterns change, and as retrieval algorithms improve, the optimal chunk size may shift. New document types may enter your corpus with different structure. Users may start asking more complex questions that require more context. Advances in LLMs may enable longer context windows that make larger chunks practical.

Treat chunk size as a continuous optimization target. Monitor retrieval quality metrics over time. When answer accuracy degrades, investigate whether chunk size is a contributing factor. When you add new document types, benchmark whether the existing chunk size works or whether a new configuration is needed. When you upgrade your LLM or embedding model, revalidate chunk size because the new model may have different optimal input characteristics.

The fintech startup that struggled with 256-token chunks learned this lesson painfully. They chose small chunks because they wanted high precision, but they did not validate that small chunks provided sufficient context for their complex regulatory queries. When they encountered problems, they increased chunk size iteratively without a systematic benchmark, wasting months on trial and error. Had they benchmarked multiple chunk sizes upfront with representative queries, they would have identified the optimal size in weeks and avoided the credibility loss with users.

You are choosing a chunk size in 2026, and you have the tools and knowledge to make this decision empirically. Do not copy 512 tokens from a tutorial. Do not assume that small chunks are always better for precision or that large chunks are always better for context. Test multiple sizes. Measure retrieval quality with real queries. Find the sweet spot for your use case. Treat chunk size as a lever you can pull to improve retrieval quality, and revisit it as your system evolves. Your chunk size decision ripples through every aspect of your RAG pipeline. Make it with data, not assumptions.
