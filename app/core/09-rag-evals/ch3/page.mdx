# Chapter 3 â€” Embeddings and Vector Search

Embeddings are the mechanism by which semantic similarity becomes computable. A query and a document are just text until you encode them into vectors. Once encoded, similarity is geometry: cosine distance, dot product, Euclidean norm. This transformation from language to vectors is what makes retrieval-augmented generation possible, and in 2026 the quality of your embeddings determines the ceiling of your RAG system.

This chapter teaches you how to select, deploy, and optimize embeddings and vector search for production RAG. You will learn what separates good embeddings from bad ones, how dense and sparse embeddings complement each other, how vector databases index and search billions of vectors efficiently, and how to diagnose and fix embedding drift over time. Embeddings are not a black box. They are a tunable component with measurable tradeoffs.

Model selection is the first decision. General-purpose embedding models like OpenAI ada-002 or Cohere embed-v3 work well for broad domains but lack precision on specialized vocabulary. Domain-specific models fine-tuned on medical, legal, or technical corpora outperform general models in those domains but generalize poorly. Multilingual models support cross-language retrieval but sacrifice monolingual performance. You will learn how to evaluate embedding models on your data before committing to one, and how to switch models without reindexing everything.

Dense embeddings represent text as continuous vectors in high-dimensional space, typically 384 to 1536 dimensions. Dense models excel at semantic similarity and paraphrase detection but struggle with rare terms, acronyms, and exact matches. Dense retrieval is fast because vector databases use approximate nearest neighbor search. You will learn when dense embeddings suffice and when they fail.

Sparse embeddings represent text as high-dimensional sparse vectors where most dimensions are zero. Models like SPLADE or BM25 encodings excel at exact term matching and rare word retrieval but miss semantic paraphrases. Sparse retrieval is slower than dense retrieval but more interpretable. In 2026, production systems combine dense and sparse embeddings in hybrid pipelines. You will learn how to fuse results from both without losing precision.

Vector databases are purpose-built systems for indexing and searching embeddings at scale. They differ from relational databases in query semantics, indexing algorithms, and performance characteristics. Popular options in 2026 include Pinecone, Weaviate, Qdrant, Milvus, and Postgres with pgvector. Each supports different indexing algorithms, metadata filtering, and scaling patterns. You will learn how to choose a vector database based on latency, throughput, cost, and operational complexity.

Indexing algorithms determine how vectors are organized for fast retrieval. Flat indexes compute exact similarity by comparing the query to every vector. This is slow but returns exact results. Approximate nearest neighbor indexes like HNSW, IVF, or ScaNN sacrifice exactness for speed by partitioning the space and pruning unlikely candidates. In 2026, HNSW is the default because it offers the best recall-latency tradeoff at scale. You will learn how these algorithms work and how to tune their hyperparameters.

Similarity metrics define how distance is computed between vectors. Cosine similarity measures angle and is invariant to magnitude. Dot product measures both angle and magnitude and is faster to compute. Euclidean distance measures straight-line distance and is sensitive to scale. Most embedding models are trained with cosine similarity, so using dot product requires normalization. You will learn which metric matches your embedding model and how to verify correctness.

Drift occurs when the distribution of queries or documents shifts over time, causing embedding quality to degrade. Drift happens when new vocabulary enters the corpus, when query patterns change, or when document topics evolve. Detecting drift requires monitoring retrieval metrics over time and comparing embedding distributions. Fixing drift requires retraining or fine-tuning embeddings on recent data. You will learn how to monitor for drift and when retraining is worth the cost.

Multi-vector representations assign multiple embeddings to a single document, each representing a different aspect or chunk. Multi-vector retrieval matches the query to all candidate embeddings and aggregates scores. This improves recall when documents are long or multi-topic but increases index size and query cost. ColBERT is a popular multi-vector architecture in 2026. You will learn when multi-vector retrieval justifies the added complexity.

Hybrid search combines vector search with keyword search to balance semantic similarity and exact matching. Hybrid systems run both retrievers in parallel, then merge results using rank fusion or learned scoring. Hybrid search is now standard in enterprise RAG because it handles rare terms and synonyms equally well. You will learn how to tune the weight between vector and keyword results and how to evaluate hybrid performance.

Quantization compresses embeddings from 32-bit floats to 8-bit integers or binary codes, reducing storage and speeding up search. Quantization introduces approximation error but can reduce index size by 75 percent with minimal recall loss. Product quantization and binary quantization are the most common techniques in 2026. You will learn when quantization is safe and how to measure its impact on retrieval quality.

Reranking is a second-pass step that rescores retrieved candidates using a more expensive model than the initial retriever. Rerankers use cross-encoders that jointly encode the query and document, capturing interaction features that embeddings miss. Reranking improves precision at the cost of latency and inference cost. You will learn when to use reranking, how to choose a reranker model, and how to optimize the reranking depth.

Fine-tuning embeddings adapts a pretrained model to your domain by training on query-document pairs from your corpus. Fine-tuning improves precision on domain-specific vocabulary and query patterns but requires labeled data and training infrastructure. In 2026, fine-tuning is standard for high-value verticals like legal, medical, and enterprise search. You will learn how to collect training data, how to fine-tune without overfitting, and how to evaluate improvement.

Scale introduces new challenges as your corpus grows from thousands to billions of vectors. Indexing billions of vectors requires distributed systems, sharding, and replication. Query latency increases with index size unless you partition the space or use hierarchical indexes. Costs scale with both storage and query throughput. You will learn how to architect vector search for billions of vectors without sacrificing latency or breaking budgets.

Learning-to-rank applies machine learning to combine vector similarity with metadata, user feedback, and context signals into a final ranking score. Learning-to-rank models can reweight results based on recency, popularity, or user preferences. They require labeled training data and add inference cost but can significantly improve relevance. You will learn when learning-to-rank justifies the complexity and how to integrate it into your pipeline.

Adversarial embeddings are malicious inputs designed to fool retrieval by optimizing for high similarity to unrelated documents. Adversarial attacks can poison search results or leak sensitive information. Defenses include input validation, anomaly detection, and adversarial training. In 2026, adversarial robustness is a requirement for security-sensitive applications. You will learn how embeddings fail under attack and how to harden your system.

This chapter is technical because embeddings and vector search are the engine of RAG. Understanding them deeply lets you debug retrieval failures, optimize latency and cost, and push the limits of what retrieval can do. In 2026, the best RAG engineers know their embeddings as well as their models.
