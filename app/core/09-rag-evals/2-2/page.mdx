# 2.2 â€” Chunking Strategies: Fixed, Semantic, Recursive, and Agentic

In October 2024, a healthcare analytics company launched a RAG system to help doctors search clinical trial literature. They used fixed 512-token chunks, the default from every tutorial they followed. Within two weeks, physicians reported that search results were incoherent. A query about adverse events for a specific drug returned chunks that started mid-sentence, referenced tables from previous pages without context, and mixed results from different trials in confusing ways. The team realized that 512 tokens split most paper abstracts in half, separated tables from their captions, and cut methods sections into fragments that were meaningless in isolation. They spent six weeks experimenting with chunking strategies, eventually settling on section-aware recursive chunking that respected paper structure. Retrieval quality improved dramatically, but the company had already lost early adopters who never returned after the initial poor experience.

You are choosing a chunking strategy in 2026, and this decision will ripple through every aspect of your RAG system. Chunk size and boundaries determine what your embedding model sees, what your retriever returns, and what context your LLM receives. There is no universal best chunking strategy. The right approach depends on your document structure, query patterns, and downstream use case. This chapter walks through the four main chunking paradigms, their tradeoffs, and the production scenarios where each one wins.

## Fixed-Size Chunking: Simple But Lossy

Fixed-size chunking is the default in most RAG tutorials and libraries. You define a chunk size in tokens or characters, optionally add overlap between chunks, and split documents mechanically. A 10,000-token document becomes twenty 512-token chunks with 50-token overlap. The algorithm is deterministic, fast, and trivial to implement. It also destroys document structure and creates semantically incoherent chunks.

The appeal of fixed-size chunking is simplicity. You do not need to parse document structure, detect section boundaries, or understand content semantics. You just tokenize and split. For small prototypes or proof-of-concept demos, this is often sufficient. Fixed-size chunking also guarantees uniform chunk sizes, which can simplify downstream processing: you know every chunk fits within your embedding model's context window, and you can predict memory and latency with confidence.

Performance characteristics are predictable with fixed-size chunking. Embedding time scales linearly with document count. Storage requirements are easy to calculate. Retrieval latency is consistent across queries. These operational benefits make fixed-size chunking attractive for teams that need predictable infrastructure costs and resource planning.

The problem is that fixed-size chunking ignores meaning boundaries. A paragraph discussing a clinical trial methodology may be split across two chunks, with the first chunk ending mid-sentence and the second chunk starting without context. A table may be separated from its caption, rendering both chunks unintelligible. A code snippet may be split across chunks, breaking syntax and making the code unrunnable. Sections with tight internal coherence like problem statements, experimental setups, or legal definitions are fragmented into pieces that do not make sense in isolation.

When users retrieve these fragments, they receive incomplete information. An LLM trying to answer a question based on a chunk that starts mid-sentence may hallucinate the missing context or refuse to answer because the chunk does not contain enough information. If the next chunk with the continuation is not also retrieved, the answer will be wrong. If the next chunk is retrieved, the LLM receives redundant overlapping text and must deduplicate information, wasting context window budget.

Fixed-size chunking also struggles with documents that have variable density of information. A document with a long introductory section and a short conclusion will be split into many chunks for the introduction and few for the conclusion, regardless of whether the introduction or conclusion contains more valuable information. This creates imbalance in retrieval: the introduction dominates search results even if the conclusion is more relevant to the query.

Information density varies dramatically across document types. Academic papers frontload context in introductions but concentrate novel findings in results sections. Technical manuals have dense procedural steps interspersed with verbose warnings. Legal contracts have boilerplate followed by critical clauses. Fixed-size chunking cannot adapt to these density patterns, treating all content as equally important by default.

Despite these weaknesses, fixed-size chunking is not always wrong. It works reasonably well for corpora with uniform structure, like chat logs, customer reviews, or social media posts where individual messages are short and self-contained. It also works when documents are already pre-segmented into semantically coherent units, and you just need to ensure chunks fit within size limits. If your corpus is homogeneous and your queries are broad, the loss of structure may not hurt retrieval quality enough to justify more complex chunking.

Document types that tolerate fixed-size chunking share common characteristics: short length, flat structure, conversational tone, and topic consistency within each document. Customer support tickets, product reviews, forum posts, and chat messages fit this profile. When considering fixed-size chunking, evaluate whether your documents match these characteristics.

The teams that succeed with fixed-size chunking are those who validate that it works for their specific use case. They benchmark retrieval quality with fixed-size chunks against ground truth queries, measure how often retrieved chunks lack necessary context, and compare against alternative chunking strategies. They do not use fixed-size chunking because it is the default. They use it because they tested it and confirmed it performs acceptably for their data.

## Semantic Chunking: Splitting by Meaning Boundaries

Semantic chunking attempts to create chunks that align with natural meaning boundaries in text. Instead of splitting mechanically by token count, semantic chunking identifies transitions between topics, sections, or ideas and splits there. The goal is to produce chunks where each chunk represents a coherent unit of meaning that can be understood in isolation.

The simplest form of semantic chunking is sentence-boundary splitting. Instead of splitting mid-sentence, you split only at sentence boundaries. This ensures chunks are grammatically complete and reduces the incoherence problem of fixed-size chunking. However, sentence-boundary splitting alone does not solve the problem of splitting related sentences across chunks. A paragraph with five sentences may be split into two chunks, with the first three sentences in one chunk and the last two in another, even though all five sentences discuss the same topic.

Sentence detection requires handling edge cases that naive approaches miss. Abbreviations like "Dr." contain periods that are not sentence boundaries. Decimal numbers like "3.14" should not trigger splits. Quote punctuation interacts complexly with sentence endings. Production sentence-boundary splitting uses libraries like spaCy or NLTK that handle these cases through linguistic models trained on diverse text.

More sophisticated semantic chunking uses section detection. If documents have explicit structure markers like headings, subheadings, or section delimiters, you can split at these boundaries. A scientific paper is split into Introduction, Methods, Results, and Discussion sections. A legal contract is split into numbered clauses. An API documentation page is split into function descriptions. This approach preserves semantic coherence because each section typically discusses a single topic or concept.

Section detection from markup is straightforward for structured formats like Markdown, HTML, or reStructuredText. Markdown uses hash symbols for headers, HTML uses heading tags, reStructuredText uses underline patterns. Parsing these formats reliably requires handling nested structures, distinguishing headings from code blocks or quotes, and normalizing heading levels across documents with inconsistent styles.

When documents lack explicit structure markers, you can use embedding-based semantic segmentation. This technique computes embeddings for sentences or short text windows and identifies boundaries where embedding similarity drops sharply, indicating a topic transition. The intuition is that consecutive sentences about the same topic will have similar embeddings, while sentences at a topic boundary will have dissimilar embeddings. TextTiling and similar algorithms implement this idea.

Embedding-based segmentation requires an additional embedding model and increases computational cost during chunking. You must embed every sentence or window, compute pairwise similarities, identify local minima in similarity, and validate that detected boundaries align with token or character limits. This is much slower than fixed-size chunking, and the quality depends on the embedding model's ability to capture topic shifts, which is imperfect. For some documents, embedding similarity may not correlate well with semantic boundaries: a document that repeats similar phrases or uses consistent terminology throughout may not show clear similarity drops even at topic transitions.

Implementation challenges include choosing the right window size for computing similarities, setting thresholds for what constitutes a significant similarity drop, and handling documents where topic boundaries are gradual rather than sharp. Hyperparameters require tuning on representative documents from your corpus, and optimal values vary across domains and writing styles.

Semantic chunking also introduces variability in chunk sizes. Some sections are short, some are long. If you have a maximum chunk size, you must handle sections that exceed the limit by splitting them further, which reintroduces the problems of fixed-size chunking. If you do not enforce a maximum size, some chunks may exceed your embedding model's context window or your LLM's input limit. Variability also complicates batch processing and resource allocation.

Chunk size distributions under semantic chunking typically follow power law patterns: many small chunks from brief sections, fewer medium chunks from typical sections, and occasional very large chunks from comprehensive sections. This variability requires robust handling in retrieval and generation pipelines, including strategies for truncating or further splitting oversized chunks while preserving their semantic completeness.

Despite these challenges, semantic chunking significantly improves retrieval quality for structured documents. When users query for specific information, they are more likely to receive complete, coherent chunks that contain all necessary context. An LLM answering a question based on a semantically coherent chunk has a much better chance of producing a correct answer than one working with a fragment split mid-paragraph.

The production pattern is to use semantic chunking when your documents have identifiable structure and your use case requires precise answers based on complete context. Legal documents, scientific papers, technical documentation, and long-form articles all benefit from semantic chunking. Conversely, if your documents are unstructured prose without clear sections, or if your queries are broad and require aggregating information from many chunks anyway, the overhead of semantic chunking may not be justified.

## Recursive Chunking: Hierarchical Splits

Recursive chunking is a hierarchical strategy that splits documents into progressively smaller units based on a priority of delimiters. The algorithm starts by attempting to split at high-priority boundaries like section headers. If the resulting chunks are still too large, it recursively splits them at lower-priority boundaries like paragraphs, then sentences, then words, until all chunks fit within the target size.

The advantage of recursive chunking is that it preserves as much structure as possible while enforcing size constraints. A document is first split into chapters. Chapters that exceed the chunk size limit are split into sections. Sections that exceed the limit are split into paragraphs. Paragraphs that exceed the limit are split into sentences. Only in extreme cases where a single sentence exceeds the limit does the algorithm fall back to token-level splitting. This approach creates a hierarchy of chunks with varying granularity, all respecting natural boundaries.

Delimiter priority lists encode document structure knowledge. For technical documentation, priorities might be: section headings, subsection headings, blank lines signaling paragraph breaks, sentence boundaries, then arbitrary token boundaries. For code documentation, priorities include module boundaries, class definitions, function definitions, comment blocks, then statement boundaries. These priorities ensure structural elements are preserved preferentially.

Recursive chunking is particularly effective for documents with deep hierarchical structure like books, manuals, or knowledge bases. A technical manual may have chapters, sections, subsections, procedure steps, and notes. Recursive chunking preserves this hierarchy: a chapter becomes a top-level chunk if it fits, otherwise it is split into sections, and sections are split into subsections as needed. Queries that match high-level topics retrieve chapter-level chunks with broad context, while queries that match specific details retrieve subsection-level chunks with focused information.

Preserving hierarchy metadata during recursive splitting enables sophisticated retrieval strategies. Each chunk tracks which level of the hierarchy it represents, what its parent and siblings are, and whether it was split from a larger unit. This metadata supports hierarchical navigation, context expansion, and query-specific chunk selection based on desired granularity.

The implementation complexity is higher than fixed or simple semantic chunking. You need to define a delimiter priority list, implement recursive splitting logic, track parent-child relationships between chunks, and handle edge cases where a unit at any level exceeds the maximum size. You also need to decide how to handle overlap in hierarchical contexts: should overlapping regions span across hierarchy boundaries, or only within the same level?

Edge case handling includes dealing with documents that lack high-priority delimiters, requiring immediate fallback to lower levels. Some documents have inconsistent heading hierarchies with skipped levels or duplicate numbering. Robust implementations validate hierarchy coherence and repair inconsistencies before chunking, ensuring the resulting chunk tree reflects logical document structure.

Chunk metadata becomes critical in recursive chunking. Each chunk should record its position in the document hierarchy: chapter number, section number, subsection number. This metadata enables hierarchical retrieval strategies where you retrieve a fine-grained chunk but also provide its parent chunks as additional context, giving the LLM both the specific answer and the broader context in which it appears.

Metadata schemas for recursive chunks are richer than for flat chunks. They include hierarchical path components at each level, depth indicators, sibling count and position, and references to ancestor chunks. This metadata supports queries like "find detailed technical specifications within security chapters" by filtering on both hierarchy level and ancestor path.

Recursive chunking also enables progressive retrieval. For a broad query, retrieve high-level chunks. If the user refines the query or requests more detail, retrieve lower-level chunks. This is particularly useful for interactive systems where users iteratively narrow their search. The initial results provide an overview, and subsequent results provide specifics without redundantly repeating the overview.

Interactive retrieval patterns benefit from caching high-level chunks to avoid recomputation during progressive refinement. When a user starts broad and narrows down, the system retrieves parent chunks first, caches them, then retrieves child chunks without re-retrieving parents. This optimization improves response time for multi-turn interactions where users explore a topic hierarchically.

The production teams that use recursive chunking successfully invest in parsing document structure upfront. They build parsers that extract headings, section numbers, and hierarchical relationships from source documents. They store this structure in metadata and use it to guide both chunking and retrieval. They do not treat chunking as a preprocessing step that discards structure. They treat it as a transformation that preserves structure in a different form.

## Agentic Chunking: LLM-Driven Splits

Agentic chunking uses an LLM to decide where and how to split documents. Instead of applying rule-based splitting logic, you pass document text to an LLM with a prompt asking it to identify optimal chunk boundaries based on semantic coherence, topic transitions, and relevance to anticipated queries. The LLM analyzes the document structure and content, then returns a list of chunk boundaries or pre-split chunks.

The promise of agentic chunking is that LLMs can understand document semantics in ways that rule-based algorithms cannot. An LLM can identify when a paragraph is a tangential aside that should be separated from the main argument. It can recognize that two sections discuss the same topic under different headings and should be merged. It can detect that a code example is tightly coupled to its preceding explanation and should remain in the same chunk. These nuances are difficult to capture with delimiter-based or embedding-based splitting.

Semantic understanding enables novel chunking strategies impossible with traditional methods. An LLM can chunk by argumentative structure, keeping claims with their supporting evidence. It can chunk by audience, separating beginner-friendly explanations from advanced technical details. It can chunk by recency, identifying time-sensitive information that should be chunked separately for temporal filtering.

The challenges are cost, latency, and reliability. Sending every document through an LLM for chunking is expensive. For a corpus of ten thousand documents, each requiring several LLM calls to chunk, the cost adds up quickly. Latency is also a problem: chunking becomes a bottleneck if it requires waiting for LLM API responses. Reliability is a concern because LLMs may produce inconsistent or incorrect splits, especially for documents outside their training distribution or in specialized domains.

Cost modeling for agentic chunking requires estimating tokens per document, LLM calls per document, and price per call. A 10,000-token document might require three LLM calls at 4000 tokens each to analyze and propose boundaries, totaling 12,000 input tokens. At typical API pricing of 0.30 dollars per million tokens, this costs roughly 0.0036 dollars per document. For 10,000 documents, total cost is 36 dollars, modest for prototypes but significant at scale or with frequent reindexing.

To make agentic chunking practical, you need to limit its scope. Instead of using it for every document, use it selectively for high-value documents or document types where rule-based chunking performs poorly. For example, use agentic chunking for legal contracts where clause boundaries are critical, but use recursive chunking for routine technical documentation. This hybrid approach balances quality and cost.

Document type classification can route documents to appropriate chunking strategies. Machine learning classifiers or rule-based heuristics identify contracts, research papers, technical manuals, and conversational text, sending each to the chunking strategy that handles it best. This routing layer enables per-document strategy selection based on structure characteristics.

Another pattern is to use agentic chunking to generate training data for a fine-tuned model. Have an LLM chunk a representative sample of documents, review and correct the results, then train a smaller model to predict chunk boundaries based on the LLM-generated labels. The fine-tuned model is faster and cheaper to run at scale, and it captures the chunking logic that the LLM discovered. This amortizes the cost of agentic chunking across the entire corpus.

Fine-tuning approaches include training a sequence labeling model to predict boundary tokens, training a classification model to predict whether each sentence should start a new chunk, or training a ranking model to score potential boundary locations. Each approach has different data requirements and inference characteristics, with sequence labeling generally providing the best balance of accuracy and speed.

Agentic chunking also enables context-aware splitting. You can prompt the LLM with information about the downstream use case: "Split this document into chunks optimized for answering questions about security vulnerabilities" versus "Split this document into chunks optimized for code generation." The LLM can tailor chunk boundaries to the expected query distribution, creating chunks that are more likely to contain the information users will search for.

Use-case-specific prompts might include example queries users will ask, specifying desired chunk characteristics like "self-contained answers to single questions" or "complete code examples with context." The LLM uses these criteria to evaluate potential chunk boundaries, splitting where boundaries create chunks that best match the specification.

The reliability problem is harder to solve. LLMs make mistakes. They may miss section boundaries, split in awkward places, or produce chunks that violate size constraints. You need validation logic to check LLM output: verify that all document text is included in chunks, ensure chunks do not exceed maximum size, and check that boundaries align with sentence or paragraph breaks. If validation fails, fall back to rule-based chunking or retry with a different prompt.

Validation pipelines for agentic chunking include coverage checks ensuring no text is lost or duplicated, size constraint checks flagging oversized chunks, coherence checks using embedding similarity to detect unnatural splits, and completeness checks verifying that references and dependencies within chunks are satisfied. Failed validations trigger fallback or human review queues.

Agentic chunking is not a replacement for rule-based strategies. It is a specialized tool for cases where rule-based strategies fail and the value of better chunking justifies the cost. As LLM inference becomes cheaper and faster, agentic chunking will become more practical for broader use. In 2026, it remains a niche technique used by teams with large budgets and complex documents.

## Comparing Strategies: When Each One Wins

Choosing a chunking strategy is not about picking the "best" method in the abstract. It is about matching the strategy to your document characteristics, query patterns, and operational constraints. Fixed-size chunking wins when documents are homogeneous, structure is minimal, and retrieval precision is not critical. Semantic chunking wins when documents have clear structure and retrieval precision matters. Recursive chunking wins for deeply hierarchical documents where preserving structure improves context. Agentic chunking wins for complex, high-value documents where traditional methods fail and cost is acceptable.

A production RAG system often uses multiple chunking strategies. Legal documents are chunked by clause boundaries. Technical documentation is recursively chunked by section hierarchy. User-generated content like forum posts is chunked with fixed-size limits and sentence boundaries. The chunking strategy becomes a property of the document type, configured in metadata, and applied by the ingestion pipeline based on document classification.

Strategy selection matrices map document characteristics to recommended strategies. Documents with explicit headings favor semantic or recursive chunking. Documents with uniform paragraphs favor fixed-size with overlap. Documents with complex argumentation favor agentic chunking. These matrices encode accumulated wisdom from production deployments and provide starting points for new projects.

The mistake teams make is choosing a strategy based on what is easy to implement or what a library defaults to, rather than what performs best for their use case. The healthcare analytics company that started with fixed 512-token chunks did not fail because fixed-size chunking is inherently bad. They failed because fixed-size chunking was wrong for scientific papers with section structure and specialized terminology. When they switched to recursive chunking that respected paper structure, retrieval quality improved not because recursive chunking is universally superior, but because it matched their document characteristics.

The validation process for chunking strategies is straightforward: take a sample of your corpus, chunk it with different strategies, embed the chunks, run representative queries, retrieve results, and evaluate whether retrieved chunks contain sufficient context for an LLM to answer correctly. Measure precision at different retrieval depths. Count how often the top-ranked chunk lacks critical context. Compare chunk coherence subjectively by reading retrieved results. This empirical evaluation is more valuable than theoretical arguments about which strategy is better.

Evaluation frameworks should include both automated metrics and human judgment. Automated metrics track chunk size distribution, boundary coherence scores from language models, and retrieval performance on labeled test queries. Human evaluation assesses whether chunks make sense in isolation, whether answers can be correctly generated from single chunks, and whether users find retrieved chunks useful.

You also need to consider operational constraints. Semantic and recursive chunking require document structure parsing, which adds complexity to the ingestion pipeline. Agentic chunking requires LLM API access and budget. Fixed-size chunking is trivial to implement and has no external dependencies. If your team lacks the engineering capacity to build and maintain complex chunking logic, starting with fixed-size chunking and iterating based on retrieval quality metrics is a pragmatic choice.

## The Chunking Decision as a Retrieval Quality Lever

Chunking is not a one-time decision. It is a lever you can adjust to improve retrieval quality throughout the lifecycle of your RAG system. When users report poor search results, when answer accuracy degrades, or when you add new document types to your corpus, revisit your chunking strategy. Experiment with different approaches. Measure the impact on retrieval metrics. Iterate.

The healthcare analytics company learned this the hard way. They launched with a chunking strategy copied from tutorials without validating it against their use case. When retrieval quality was poor, they spent weeks debugging embedding models, retrieval algorithms, and prompts before realizing the root cause was chunking. By the time they fixed it, they had lost early users. The lesson is not that they should have known the right chunking strategy from the start. The lesson is that they should have validated their chunking strategy before launching, using empirical evaluation with real queries and real documents.

You are building a RAG system in 2026, and you have the benefit of hindsight from teams that failed before you. Do not assume the default chunking strategy is correct for your use case. Do not assume that because fixed-size chunking works for demos, it will work for production. Test multiple strategies, measure their impact on retrieval quality, and choose based on data rather than convention. Your chunking strategy is the foundation of what your retrieval system sees. Get it right, and the rest of the pipeline has a fighting chance. Get it wrong, and no amount of tuning downstream will compensate for incoherent chunks.
