# 5.11 â€” RAG with Structured Output: Tables, JSON, and Reports

In February 2025, an enterprise analytics company lost a quarter-million-dollar contract when their AI-powered report generation system produced a quarterly financial comparison table with rows and columns misaligned, numbers attributed to the wrong quarters, and percentage calculations that did not match the underlying data. The system used a sophisticated RAG pipeline that retrieved financial data from multiple sources, and a frontier language model that generated narrative reports with embedded tables. The retrieval was accurate. The narrative prose was well-written and insightful. But the table generation was unreliable. The model would occasionally swap column headers, misalign data rows, or insert plausible-looking but incorrect calculations. The client caught these errors during a board presentation. The contract was terminated. The company had optimized for narrative quality but had not implemented robust validation for structured output. That oversight cost them the contract and damaged their reputation in the financial services vertical.

You are building a RAG system right now, and you might be assuming that the output will always be natural language prose. Most RAG tutorials and examples show conversational question-answering systems that generate paragraph-form answers. But many real-world applications require structured output: tables summarizing comparisons, JSON objects with extracted entities and attributes, reports with specific sections and formatting, data exports in standardized formats, or forms pre-filled with information extracted from documents. Generating structured output from retrieved context is fundamentally different from generating prose. It requires schema enforcement, validation, grounding of each field to source documents, and error handling for cases where the structure cannot be populated from the available information. If you are building RAG for any application that outputs structured data, you need to design for structure from the beginning, not bolt it on as an afterthought.

## Why Structured Output Is Different from Free Text

When you generate natural language prose, the language model has enormous flexibility. It can rephrase information, hedge with qualifiers, skip details that are not well-supported, or provide partial information. If the retrieved context says "Q1 revenue was approximately 2.3 million dollars," the model can generate "First quarter revenue was around 2.3 million," preserving the hedging and approximation. The output is forgiving. Small variations in phrasing or structure do not break anything. The user reads the text and interprets it.

When you generate structured output, flexibility disappears. A table has a fixed schema: specific columns, specific data types, specific formatting. A JSON object has a schema with required fields, type constraints, and nesting structure. A report has mandatory sections that must be present. If the schema requires a field called "q1_revenue" with a numeric value, you cannot output "approximately 2.3 million." You must output a number, and that number must be precisely extracted and validated. If the retrieved context is ambiguous, you cannot hedge. You either populate the field with a confident value, leave it empty, or mark it as uncertain using a schema-defined mechanism.

This rigidity creates new failure modes. The model might generate a table where the number of rows does not match the data, creating alignment errors. It might output JSON that is syntactically valid but semantically incorrect, with values in the wrong fields. It might generate a report where section headings are present but content is missing or irrelevant. These errors are often silent in the sense that the output is structurally well-formed but factually wrong. A human reading a prose answer will notice hedging or vagueness. A human consuming a table or JSON might not notice that a value is misaligned or misattributed, especially if it is plausible.

Structured output also requires grounding at a more granular level. In prose, you can cite a document as the source for a paragraph. In structured output, you need to ground each field or cell to a specific source. If a table has a cell showing "Q1 2024 revenue: 2.3M," you need to trace that specific value back to a specific document and ideally a specific location within that document. This granular grounding is essential for validation, debugging, and user trust, but it is harder to implement than document-level citations.

## Table Extraction and Synthesis from Retrieved Documents

Tables are one of the most common structured outputs in RAG systems. Users want to compare data across time periods, across products, across regions, or across any other dimension. The source data is often scattered across multiple documents, each covering a subset of the table. The RAG system needs to retrieve the relevant documents, extract data for each cell, align the data correctly, and synthesize it into a coherent table.

The first challenge is schema determination. Sometimes the table schema is fixed: the user requests a specific comparison with predefined columns and rows. Other times the schema is implicit in the query: the user asks to "compare Q1 through Q4 revenue," and the system must infer that the table should have quarters as columns or rows and revenue as the value. You need to parse the user query to extract the dimensions and metrics being requested. This might require structured query understanding, where you use the language model to extract entities and intent from the query and map them to table dimensions.

Once you have the schema, you retrieve documents that might contain the necessary data. If the user asks for quarterly revenue for 2024, you retrieve documents mentioning quarterly financial results, earnings reports, or financial summaries for 2024. You might retrieve multiple documents, each covering different quarters. The retrieved documents might contain tables themselves, or they might contain the data in prose form: "Revenue for the first quarter of 2024 was 2.3 million dollars."

The next step is extraction. You need to identify which pieces of information in the retrieved documents correspond to which cells in the target table. This is a structured information extraction task. You can prompt the language model with the table schema and the retrieved documents, and ask it to populate the table. "Given these documents, populate a table with columns for Q1, Q2, Q3, Q4 and a row for revenue. Extract the revenue value for each quarter from the documents." The model performs extraction and outputs the table.

The critical step is validation. You cannot trust that the model populated the table correctly. You need to validate each cell. Does the value have the right data type? Is it within a reasonable range? Does the source document actually support this value? You can perform automated validation by checking numeric ranges, comparing totals against sums of components, or verifying that values are monotonic when they should be. You can perform grounding validation by asking the model to cite the specific source for each cell, then programmatically checking that the cited source contains the value.

You also handle missing data explicitly. If the retrieved documents do not contain revenue for Q3, you do not allow the model to hallucinate a plausible value. You either leave the cell empty, mark it as "not available," or indicate that the data was not found in the retrieved documents. This requires instructing the model to distinguish between "data not found" and "data found but uncertain," and to represent these cases clearly in the output.

## Generating JSON Objects with Grounded Fields

JSON is a common structured output format for RAG systems that extract entities, attributes, and relationships from documents. A user might query "Extract all product names, prices, and availability status from the catalog," and the system outputs a JSON array of product objects. Each object has fields for name, price, and availability, and each field value is extracted from the retrieved catalog documents.

Generating JSON from RAG requires strict schema enforcement. You define the JSON schema in advance, specifying required fields, optional fields, data types, and any validation rules. You provide this schema to the language model as part of the prompt: "Given these catalog documents, extract product information and output it as a JSON array conforming to this schema." Many modern language models support constrained decoding or structured output modes where they guarantee syntactically valid JSON conforming to a schema. If your model supports this, use it. It eliminates the risk of malformed JSON.

Grounding is critical for JSON output. Each field value must be traceable to a specific source. You can enforce this by extending the schema to include citation metadata. Instead of just outputting name, price, and availability, you output name, price, availability, and for each field, a source citation with document ID and location. The schema might look like: each field is an object with "value" and "source" sub-fields. This makes grounding explicit and auditable.

You validate JSON output at multiple levels. Syntactic validation checks that the JSON is well-formed and conforms to the schema. Semantic validation checks that the values make sense: prices are positive numbers, availability is one of a defined set of statuses, product names are non-empty strings. Grounding validation checks that each source citation actually exists and contains the claimed value. If any validation fails, you reject the output and either retry with clearer instructions or return an error indicating what failed.

You also handle partial extraction gracefully. If the user asks for ten products but the retrieved documents only contain information about seven, you output seven complete objects rather than ten objects with missing or hallucinated data. You might include metadata in the response indicating that extraction was partial: "extracted 7 of requested 10 products, 3 not found in retrieved documents." This sets clear expectations about completeness.

Another common JSON use case is extracting structured information from unstructured documents. A user uploads a contract and asks the system to extract party names, effective dates, termination clauses, and liability caps. The system retrieves the relevant sections of the contract, extracts the information, and outputs it as a JSON object with fields for each requested element. Each field includes not just the extracted value but also the location in the contract where it was found, enabling the user to verify the extraction.

## Report Generation from Multiple Sources

Reports are structured documents with specific sections, headings, and content requirements. A quarterly business review might require sections on revenue, expenses, profit margins, market trends, and strategic initiatives. Each section draws information from different sources: financial databases, market research reports, internal strategy documents. The RAG system needs to retrieve relevant information for each section, synthesize it into coherent prose or structured data as appropriate, and assemble the complete report with correct formatting.

Report generation requires section-level planning. You start by decomposing the report into sections based on the report template or user requirements. For each section, you generate a query to retrieve relevant information. For the revenue section, you retrieve financial statements and earnings reports. For the market trends section, you retrieve market research and competitor analysis. You might retrieve for all sections in parallel to reduce latency.

Once you have retrieved documents for each section, you generate content for that section. This is where RAG meets structured output. Some sections might require prose: "Write a summary of market trends based on these retrieved market research reports." Other sections might require structured data: "Create a table comparing quarterly revenue across the four quarters based on these financial statements." Some sections might require both: prose summary followed by a supporting table.

You assemble the report by combining the generated sections according to the template. You ensure that section headings are correct, that formatting is consistent, and that the flow is logical. If the report format requires an executive summary, you might generate that last, after all sections are complete, by summarizing the key points from each section. If the report requires conclusions or recommendations, you generate those based on the synthesized information from all sections.

Grounding is essential in reports. Each claim, each data point, each conclusion needs to be traceable to sources. You include citations throughout the report, either as inline references, footnotes, or a bibliography at the end. The citations are not just for user benefit. They are part of your validation process. You can check that every factual claim is grounded in a retrieved document, and you can flag claims that lack grounding for review.

You also validate reports at the structural level. Does the report include all required sections? Are section headings correct? Is the formatting consistent? Are tables properly formatted? Are charts and graphs, if any, correctly rendered? Are citations complete and properly formatted? This validation can be partially automated by checking against the report template schema.

## Schema Enforcement and Validation Mechanisms

Schema enforcement is the process of ensuring that structured output conforms to a predefined schema. This is trivial when the output is simple, like a single number or a yes/no answer. It becomes complex when the output is a multi-level JSON object, a table with many columns and rows, or a report with nested sections.

The most reliable approach is to use constrained decoding if your language model supports it. Constrained decoding forces the model to generate only output that conforms to a schema. You provide a JSON schema or a grammar, and the model's token sampling is constrained to only produce valid output according to that schema. This guarantees syntactic validity. Many open-source models support constrained decoding through libraries like Guidance or Outlines. Some API-based models offer structured output modes with schema enforcement.

If constrained decoding is not available, you rely on post-generation validation. You generate the output, then parse it and validate it against the schema. If validation fails, you have several options. You can retry generation with additional instructions highlighting what was wrong. You can attempt to repair the output programmatically, such as fixing minor JSON syntax errors. You can return an error to the user indicating that structured output could not be generated. You can fall back to unstructured output, providing the information in prose form rather than structured form.

For complex schemas with nested structures, validation becomes more involved. You need to check not just top-level fields but also nested objects, arrays, and relationships between fields. You might have cross-field validation rules: if field A has value X, then field B must have value Y. These rules are difficult to enforce through prompting alone. You implement them as explicit validation logic that runs after generation.

You also validate semantic correctness, not just syntactic conformance. A JSON object might be syntactically valid but semantically wrong. A table might have the right number of columns but values in the wrong columns. A report might have all required sections but with content that does not match the section headings. Semantic validation requires understanding the meaning of the schema and the content, which is harder to automate. You might use heuristics, such as checking that a "revenue" field contains a number that looks like currency, or you might use a separate model to evaluate semantic correctness.

## Validation of Structured RAG Outputs Against Sources

The most critical validation for structured RAG output is grounding validation: ensuring that each element of the structured output is actually supported by the retrieved documents. This is where many systems fail. The model generates a table or JSON object that is syntactically perfect and semantically plausible, but the values are hallucinated or misattributed.

Grounding validation requires explicit citations at the granular level. For a table, each cell should have a citation indicating which document and which location within that document provided the value. For a JSON object, each field should include source metadata. You enforce this by designing your schema to include citation fields, and you instruct the model to populate them. The prompt might say: "For each cell in the table, provide a citation in the format document_id, page_number, line_number."

Once you have citations, you validate them. You retrieve the cited document, navigate to the cited location, and check whether it actually contains the claimed value. This can be partially automated. For example, if a cell claims "Q1 revenue: 2.3M, source: financial_statement_2024.pdf, page 3," you can extract text from page 3 of that PDF and search for mentions of Q1 revenue and values near 2.3 million. If you find a match, the citation is validated. If not, the value is flagged as potentially hallucinated.

For values that involve calculation or synthesis, grounding is more complex. If a table cell shows "year-over-year growth: 15%," and this value is not explicitly stated in any document but is calculated from "2024 revenue: 2.3M" and "2023 revenue: 2.0M," you need to validate both the source values and the calculation. You check that the source values are grounded, then verify that the calculation is correct. This requires understanding the semantics of the field and the expected calculation method.

You also detect hallucinations by cross-checking consistency. If a table shows Q1 revenue as 2.3M in one cell and total annual revenue as 6M in another cell, but the sum of Q1-Q4 revenue is 9M, there is an inconsistency. One or more values are wrong. You flag this for review. Similarly, if a JSON object lists five products but the retrieved documents only mention three products by name, the extra two products might be hallucinated. You check whether all extracted entities appear in the source documents.

## Handling Missing or Incomplete Information in Structured Output

One of the hardest problems in structured RAG is handling cases where the retrieved documents do not contain all the information needed to populate the structured output. If the user requests a table with quarterly revenue for Q1 through Q4, but the retrieved documents only mention Q1, Q2, and Q4, what do you do with Q3?

The wrong answer is to hallucinate a plausible value for Q3. This is what many naive implementations do. The model sees a pattern in Q1, Q2, Q4 values and generates a value for Q3 that fits the pattern. This is extremely dangerous because the hallucinated value will look legitimate in the table, and users might not notice that it is not grounded.

The right answer is to explicitly represent missing data. You leave the Q3 cell empty, or you fill it with a placeholder like "N/A" or "Not found in source documents." If your schema supports it, you use a null value or a special marker indicating missing data. You might also include a note explaining why the data is missing: "Q3 revenue not available in retrieved financial statements."

You can make this more sophisticated by distinguishing between different types of missing data. Data might be missing because the retrieved documents do not cover that time period, in which case you might want to retrieve additional documents. Data might be missing because it was redacted or withheld, in which case you should indicate that. Data might be missing because it does not exist, such as future quarters that have not occurred yet. Each case should be represented differently.

You also handle partial information. If the document says "Q1 revenue was approximately 2.3 million," and your schema requires a precise numeric value, you can either populate the field with 2.3 and add a flag indicating the value is approximate, or you can leave the field empty and provide the approximate value in a note. The right choice depends on your use case and risk tolerance. For financial reporting where precision matters, you might choose to not populate the field if the source is approximate. For general analytics where rough numbers are useful, you might populate it with the approximation and mark it as such.

## Use Cases: Financial Reports, Compliance Tables, Product Comparisons

Structured RAG is essential in domains where information needs to be presented in standardized formats for analysis, compliance, or decision-making. Financial services use it for generating quarterly reports, comparing portfolio performance across time periods, and extracting structured data from unstructured filings. A hedge fund might ask their RAG system to "compare the revenue, profit margin, and market share of our top 10 holdings for Q4 2024," and receive a table with rows for each holding and columns for each metric, with every value grounded in the latest earnings reports.

Compliance and regulatory applications use structured RAG to extract required information from documents and present it in forms or tables required by regulators. A pharmaceutical company might use RAG to extract adverse event data from clinical trial reports and generate structured reports in the format required by the FDA. Each data point must be grounded in source documents to support regulatory audits.

Product comparison and competitive analysis use structured RAG to gather information about multiple products from disparate sources and present it in comparison tables. An enterprise buyer evaluating software vendors might ask a RAG system to "compare pricing, features, and customer reviews for Vendor A, Vendor B, and Vendor C," and receive a table with rows for each evaluation criterion and columns for each vendor. The system retrieves product pages, pricing information, and review sites, extracts the relevant data, and structures it for easy comparison.

Legal contract analysis uses structured JSON extraction to pull key terms from contracts: parties, dates, obligations, termination clauses, liability limits. A legal team reviewing hundreds of vendor contracts might use RAG to extract these terms into a structured database, enabling bulk analysis and identification of outliers or risks. Each extracted term is grounded with a citation to the specific clause in the contract.

## The Cost of Getting Structured Output Wrong

The enterprise analytics company that lost their contract in 2025 learned the hard way that structured output errors are often more damaging than errors in prose. When a prose answer is wrong, users notice the error in context. The answer does not make sense, or it contradicts their knowledge, and they question it. When a structured output is wrong, the error is often silent. A table with misaligned data looks legitimate. A JSON object with values in the wrong fields can be ingested into downstream systems without raising alarms. The error propagates and causes downstream failures.

In the case of the quarterly financial comparison table, the model had swapped Q2 and Q3 revenue values due to ambiguous date references in the source documents. The table looked perfect: all columns present, all rows aligned, all numbers plausible. The error was only caught when a board member noticed that the reported Q2 revenue was inconsistent with a previous report he had seen. The client investigated, found multiple errors in generated tables, and lost confidence in the entire system. The fact that the narrative prose was excellent did not matter. The structured data was unreliable, and that was unacceptable.

After losing the contract, the company rebuilt their structured output pipeline with rigorous validation. They implemented granular grounding for every table cell and JSON field. They added automated consistency checks: totals must equal sums, percentages must be calculated correctly, dates must be in order. They added human-in-the-loop review for high-stakes reports, where generated tables were flagged for human verification before delivery. They made missing data explicit, never allowing the model to fill gaps with plausible-looking hallucinations. When they returned to market six months later, they emphasized their structured output validation as a key differentiator. They provided audit trails showing the source for every data point. They offered validation reports alongside generated tables. They rebuilt trust. They recovered their reputation. The investment in structured output validation saved the company.

You are building a RAG system, and if your output includes tables, JSON, reports, or any structured format, you need to design for structure from the beginning. Define schemas explicitly. Enforce them through constrained decoding or rigorous post-generation validation. Implement granular grounding so that every field, every cell, every value is traceable to a source. Validate semantic correctness, not just syntactic conformance. Handle missing data explicitly. Check consistency across related values. Test extensively with real-world data and edge cases. The stakes are higher for structured output because errors are silent and propagate. Get it right, and you build trust. Get it wrong, and you lose contracts and damage your reputation. Structured RAG is harder than prose RAG, but it is essential for many of the most valuable applications.
