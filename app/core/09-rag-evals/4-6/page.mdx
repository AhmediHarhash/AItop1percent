# 4.6 â€” Retrieval Depth: How Many Chunks to Retrieve

A consulting firm deployed a competitive intelligence system in November 2025 to help analysts research market trends and competitor activities. The system indexed 800,000 articles, reports, and company filings using careful chunking and high-quality embeddings. The architects configured the system to retrieve exactly five chunks per query, a number they chose based on advice from a RAG tutorial they found online. Within three months, analysts were complaining that the system gave incomplete answers. When someone asked "what are the major strategic initiatives announced by our top three competitors in the past year," the system retrieved five chunks total, typically one or two chunks per competitor, missing critical initiatives that were documented in the corpus. When someone asked "what is the pricing structure for enterprise software products in our category," five chunks could not possibly cover the pricing details for dozens of products. By March 2026, the firm calculated that incomplete intelligence was leading to flawed strategy recommendations, and they had lost at least two client engagements worth 3.2 million dollars because their recommendations missed key market developments. The retrieval technology was excellent. The problem was that five chunks is not enough for complex questions, and nobody measured whether five chunks was the right number for their specific use case.

You are confronting the retrieval depth problem, and it is more subtle than it appears. Retrieving too few chunks means missing information. Your LLM cannot answer questions about information that was not retrieved. If the answer requires synthesizing information from ten documents and you only retrieve five, your answer will be incomplete. Retrieving too many chunks means drowning the signal in noise. If you retrieve 50 chunks for a simple question, 45 of them are irrelevant or tangentially related. They waste context window space, they introduce contradictory information, and they increase the chance that the LLM fixates on irrelevant details and generates a poor answer. The optimal number of chunks to retrieve depends on query complexity, topic breadth, answer type, context window size, and corpus characteristics. There is no universal right answer. The five-chunk default you copied from a tutorial is wrong for most real-world queries.

Retrieval depth is the parameter k: how many chunks to retrieve and pass to the LLM for answer generation. In academic papers and toy examples, k is often set to three or five because the test questions are simple and the evaluation corpora are small. In production systems handling complex real-world queries, k needs to be dynamic. Simple factual queries might need three chunks. Complex analytical queries might need 30 chunks. Multi-part questions might need 50 chunks. Questions requiring synthesis across many sources might need 100 chunks. Your system needs to adaptively determine k for each query instead of using a fixed value.

## Why Query Complexity Determines Optimal K

Simple factual queries have small optimal k. When someone asks "what is the return policy," the answer exists in one or two chunks. Retrieving 20 chunks adds no value because 18 of them will not contain information about the return policy. They will contain related but irrelevant information that might confuse the LLM or dilute the clear answer present in the top results. For simple queries, small k preserves precision and reduces noise.

Complex analytical queries have large optimal k. When someone asks "what are the common failure modes of microservices architectures and what mitigation strategies have been successful in production deployments," the answer requires synthesizing information from many sources. Different documents discuss different failure modes: service discovery issues, cascading failures, data consistency problems, observability challenges. Different documents discuss different mitigation strategies: circuit breakers, bulkheads, retry policies, distributed tracing. To provide a comprehensive answer, you need to retrieve chunks covering all these dimensions. Five chunks cannot possibly cover this breadth. You need 20 to 40 chunks to ensure comprehensive coverage.

Multi-part queries also require large k. When someone asks "compare the pricing, features, and customer reviews for our top three competitors," you need chunks about each competitor's pricing, chunks about each competitor's features, and chunks about each competitor's customer reviews. That is at least nine categories of information, and realistically you need multiple chunks per category to get accurate details. You need 30 to 50 chunks minimum. If you retrieve only five chunks, you might get pricing for one competitor, features for another, and reviews for a third, with no complete picture of any single competitor.

You determine query complexity using the techniques from earlier subchapters: counting entities, counting constraints, detecting conjunctions, measuring query length. A complexity score tells you how difficult the query is, and you map complexity scores to retrieval depth. Low complexity maps to k equals 3 to 10. Medium complexity maps to k equals 10 to 25. High complexity maps to k equals 25 to 50. Very high complexity maps to k equals 50 to 100. These ranges are starting points; you tune them based on your corpus characteristics and evaluation results.

## The Diminishing Returns Curve

Retrieval quality does not increase linearly with k. As you retrieve more chunks, each additional chunk is less likely to contain new relevant information and more likely to be marginally related or irrelevant. The first five chunks have high precision because they are the most similar to your query. The next five chunks have lower precision. The next ten have even lower precision. By the time you reach k equals 50, the marginal precision of additional chunks is low. You are retrieving chunks that are barely related to the query.

You measure diminishing returns by plotting recall and precision as a function of k. Take a test set of queries with known relevant documents. For each query, retrieve with k equals 5, 10, 15, 20, 30, 40, 50, 100. Measure recall and precision at each k value. Recall increases with k but at a decreasing rate. Precision decreases with k as you include more marginal results. The optimal k is where recall is sufficiently high and precision is still acceptable. This is usually where the recall curve starts to flatten.

For most corpora and query types, recall at k equals 20 is 1.5 to 2 times higher than recall at k equals 5. Recall at k equals 50 is only 1.1 to 1.3 times higher than recall at k equals 20. The marginal benefit of increasing k from 20 to 50 is much smaller than increasing k from 5 to 20. This diminishing returns curve tells you that very large k values are only justified for extremely complex queries where comprehensive coverage is critical and you are willing to accept the precision hit.

Precision at k equals 5 might be 80 percent, meaning 80 percent of retrieved chunks are relevant. Precision at k equals 20 might be 60 percent. Precision at k equals 50 might be 40 percent. As k increases, the proportion of relevant chunks decreases. Your LLM is dealing with more noise. Whether this tradeoff is acceptable depends on your use case. If you need comprehensive coverage and your LLM is good at filtering noise, large k with lower precision is fine. If you need high-precision answers and your LLM is easily distracted by irrelevant information, small k with higher precision is better.

## How Context Window Size Constrains K

Your LLM's context window imposes a hard limit on k. If your chunks are 500 tokens each and your context window is 8,000 tokens, you can fit at most 16 chunks. If you try to retrieve 30 chunks, you will exceed the context window and either truncate chunks or get an error. You need to choose k such that the retrieved chunks plus your prompt plus the expected answer length fit comfortably within the context window.

Modern LLMs have increasingly large context windows. Models from 2023 often had 4,000 to 8,000 token windows. Models from 2024 and beyond often have 32,000 to 128,000 token windows. Some models support context windows over 1 million tokens. Larger context windows enable larger k values. If your context window is 128,000 tokens and your chunks are 500 tokens, you could theoretically retrieve 200 chunks. In practice, you leave room for the prompt and answer, so realistic maximum k is around 150 to 180.

However, just because you can fit 150 chunks in your context window does not mean you should. LLM performance often degrades with very long contexts. The model struggles to attend to all information equally. Information at the beginning and end of the context receives more attention than information in the middle, a phenomenon called lost-in-the-middle. Retrieval quality matters more than retrieval quantity. Retrieving 30 highly relevant chunks is better than retrieving 150 chunks where only 30 are relevant and the rest are noise.

You also need to consider latency. Processing 150 chunks takes significantly longer than processing 30 chunks. LLM inference time increases with context length. If your latency budget is tight, you need to limit k to keep response times acceptable. You might choose k equals 30 even though your context window could fit k equals 100, simply because k equals 30 provides acceptable latency while k equals 100 does not.

Chunk size also affects the relationship between k and context usage. If your chunks are 200 tokens each, you can fit more chunks than if they are 1,000 tokens each. Smaller chunks allow larger k within a fixed context window, but smaller chunks also mean each chunk contains less context, potentially reducing answer quality. There is a tradeoff between chunk size and retrieval depth. You optimize this by measuring answer quality with different chunk size and k combinations and choosing the configuration that maximizes quality within your latency and cost constraints.

## Dynamic K Selection Strategies

The most effective approach is dynamic k selection where the system determines k for each query based on that query's characteristics. You use query complexity scoring to set a base k value: simple queries start with k equals 5, medium complexity queries start with k equals 15, high complexity queries start with k equals 30. This gives you a complexity-adjusted starting point.

You then apply additional adjustments based on other query characteristics. If the query is multi-intent, you increase k to ensure coverage of all intents. If the query mentions multiple entities, you increase k to ensure coverage of all entities. If the query has temporal constraints like "what changed over the last year," you increase k because you need to retrieve documents from multiple time periods. If the query is comparative, you increase k to ensure balanced retrieval for all entities being compared.

You can also use adaptive k selection based on initial retrieval results. You start with a conservative k value, retrieve, and measure the confidence or diversity of results. If the top results have high confidence and low diversity, you have likely retrieved enough information and you proceed with answer generation. If the top results have low confidence or high diversity, you increase k and retrieve additional chunks. You continue increasing k until confidence is acceptable or until you reach a maximum k threshold.

Another dynamic approach is progressive retrieval. You retrieve with small k, generate a candidate answer, evaluate the answer quality or completeness, and if the answer is insufficient, retrieve additional chunks and regenerate the answer. This iterative approach uses minimal k for queries that can be answered with few chunks, and progressively increases k for queries that need more information. The downside is increased latency from multiple retrieval and generation cycles. The upside is optimal k for each query with no wasted retrieval.

You implement dynamic k selection by building a k-selection module that takes the query and query analysis outputs as inputs and returns an optimal k value. The module uses heuristics, rules, or a small model to map query characteristics to k values. You train or tune this module by measuring answer quality at different k values for your test queries and learning the patterns that predict optimal k. Over time, your k-selection module improves as it learns from more queries and retrieval outcomes.

## Multi-Stage Retrieval With Increasing Depth

An alternative to choosing one k value is to use multi-stage retrieval with different k values at each stage. In the first stage, you retrieve with small k to get the most relevant chunks quickly. You pass these to a fast evaluation step that determines whether you have enough information. If you do, you generate the answer. If you do not, you proceed to a second stage where you retrieve with larger k, adding more chunks to the context. You might have a third stage with even larger k for the most complex queries.

Multi-stage retrieval balances latency and quality. Most queries are answered in the first stage with small k and fast response time. Complex queries that need more information proceed to later stages with larger k, accepting higher latency in exchange for better completeness. You avoid the problem of always using large k and paying the latency cost even for simple queries. You also avoid the problem of always using small k and missing information for complex queries.

The challenge with multi-stage retrieval is determining when to proceed to the next stage. You need a reliable signal that the current stage has insufficient information. One signal is answer confidence: if the LLM generates an answer with low confidence scores, you probably need more information. Another signal is answer completeness: if the LLM generates an answer that explicitly says "I do not have enough information to fully answer this question," you need to retrieve more. A third signal is retrieval confidence: if the similarity scores of retrieved chunks are below a threshold, the retrieval is uncertain and you should try retrieving more chunks with relaxed constraints.

You can also use query decomposition with per-component k selection. You decompose a complex query into sub-queries as discussed in earlier subchapters. For each sub-query, you independently determine optimal k based on the sub-query's complexity. A factual sub-query gets k equals 5. An analytical sub-query gets k equals 20. You retrieve independently for each sub-query with its own k value, then merge all retrieved chunks for answer generation. This gives you fine-grained control over retrieval depth at the sub-query level.

## Measuring the Impact of K on Answer Quality

You measure the relationship between k and answer quality by running experiments with fixed query sets and varying k values. Take 100 test queries with known good answers or known relevant documents. For each query, retrieve and generate answers with k equals 5, 10, 15, 20, 30, 50. Measure answer quality using your preferred metrics: correctness, completeness, relevance, hallucination rate. Plot these metrics as a function of k.

For simple queries, you will see answer quality plateau at low k values. Increasing k beyond 10 does not improve quality and might even decrease it due to noise. For complex queries, you will see answer quality increase with k up to a certain point, then plateau or decrease. The point where quality plateaus is the optimal k for that query type. This optimal k varies across query types and complexity levels, which is why dynamic k selection is necessary.

You also measure the cost-quality tradeoff. Larger k means more retrieval cost, more LLM inference cost, and more latency. You plot answer quality versus cost and identify the k values that provide the best quality per dollar. Sometimes increasing k from 10 to 20 doubles quality while only increasing cost by 50 percent, making it a great tradeoff. Sometimes increasing k from 30 to 50 increases quality by 10 percent while doubling cost, making it a poor tradeoff. You use these measurements to set k thresholds for different query complexity levels.

User satisfaction is another critical metric. Even if answer quality metrics show marginal improvement with larger k, users might perceive significant improvement because the answer covers more aspects of their question or provides more examples. Conversely, users might prefer concise answers from small k even if comprehensive answers from large k score higher on completeness metrics. You measure user satisfaction through feedback mechanisms: thumbs up/down, ratings, follow-up queries. If users consistently rate answers from k equals 15 higher than answers from k equals 30, you have learned something about your users' preferences that metrics alone do not capture.

## When Fixed K Is Acceptable

Dynamic k selection adds complexity to your system. For some use cases, a well-chosen fixed k value is sufficient. If your queries are homogeneous in complexity, if your corpus is uniform in structure, and if you have measured that a specific k value works well for 80 to 90 percent of queries, fixed k might be acceptable. The simplicity benefit of fixed k can outweigh the quality benefit of dynamic k, especially in early-stage systems where you are still establishing baseline performance.

Fixed k is also acceptable when your context window is small and your maximum possible k is limited. If your context window only allows k equals 10 maximum, there is no point in building dynamic k selection that varies between 3 and 10. The range is too narrow to matter. You just choose a fixed k equals 8 or 10 and focus your optimization efforts elsewhere.

However, as your system matures and your use cases diversify, fixed k becomes a limitation. You will encounter queries that need more depth and queries that need less. Your fixed k will be too small for some and too large for others. At that point, investing in dynamic k selection becomes worthwhile. You build it incrementally, starting with simple heuristics like mapping query length to k, then adding complexity scoring, then adding adaptive adjustments based on retrieval confidence.

The companies that build production RAG systems at scale universally use dynamic k selection. They recognize that query diversity requires retrieval depth flexibility. They invest in k-selection logic that considers query complexity, topic breadth, user intent, and retrieval confidence. They measure the impact of k on answer quality and cost. They tune k-selection heuristics based on real query patterns and user feedback. They do not accept a fixed k value from a tutorial. They treat retrieval depth as a critical parameter that requires optimization. You should do the same.
