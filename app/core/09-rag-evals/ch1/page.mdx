# Section 9 — RAG Evaluation Systems

## Chapter 1

### Plain English

RAG evaluation answers one critical question:

**"Did the system say the right thing for the right reason using the right source?"**

Not:
- "Did it sound smart?"
- "Did it answer quickly?"

But:
- Was the answer grounded?
- Were the sources correct?
- Was anything invented?
- Could this be trusted in business?

In 2026, **RAG quality = trust**.

---

### Why RAG Needs Its Own Evaluation System

RAG systems fail in ways pure LLMs do not.

A RAG system can:
- retrieve the wrong documents
- retrieve the right documents but ignore them
- partially ground answers
- hallucinate while citing real sources
- mix facts across documents incorrectly

These failures are subtle, dangerous, and often invisible without proper evals.

---

### What "RAG" Means in 2026

RAG is not just "vector search + LLM".

A production RAG system includes:
- query understanding
- retrieval (vector, keyword, hybrid)
- reranking
- chunk selection
- context assembly
- answer generation
- citation or attribution logic

Each stage can fail independently.

RAG evals must reflect this pipeline.

---

### Core RAG Quality Dimensions

RAG evaluation focuses on **truth alignment**, not fluency.

Key dimensions:

- Retrieval relevance
- Context coverage
- Grounding accuracy
- Faithfulness (no unsupported claims)
- Citation correctness
- Completeness
- Robustness to missing or noisy data

RAG quality is multi-dimensional by nature.

---

### 1) Retrieval Relevance

**Did the system retrieve the right documents?**

Measured by:
- relevance of retrieved chunks
- presence of required sources
- absence of irrelevant or misleading sources

Failure here poisons everything downstream.

You cannot "prompt" your way out of bad retrieval.

---

### 2) Context Coverage

**Did the retrieved context contain all necessary information?**

Common failure:
- retrieving only part of the answer
- missing edge-case details
- truncation due to context limits

Coverage matters more than rank-1 relevance.

---

### 3) Grounding Accuracy (Critical)

**Are all factual claims supported by retrieved context?**

This is the single most important RAG metric.

Failures include:
- hallucinated facts
- mixing facts across sources incorrectly
- extrapolating beyond context

Grounding accuracy is **binary for enterprises**.

---

### 4) Faithfulness

**Did the model stick to the provided context?**

A response can:
- cite documents
- but still add extra information

Faithfulness checks whether:
- every claim is supported
- no external knowledge leaked in

Faithfulness ≠ correctness.
Faithfulness = obedience to context.

---

### 5) Citation Correctness

If citations are used:

- are citations present when required?
- do citations actually support the claim?
- are citations precise (right chunk, not just document)?

Incorrect citations are worse than no citations.

---

### 6) Completeness (RAG-Specific)

**Did the system answer everything the question requires, given the context?**

Failures include:
- ignoring relevant retrieved facts
- summarizing incorrectly
- partial answers due to over-pruning

Completeness is judged **relative to available context**, not external truth.

---

### 7) Robustness to Retrieval Noise

**How does the system behave when retrieval is imperfect?**

Elite RAG systems:
- acknowledge missing info
- say "not found" instead of hallucinating
- degrade gracefully

This is essential for real-world data.

---

### RAG Evaluation Units

You do NOT evaluate RAG per response alone.

You evaluate:
- query
- retrieved documents
- selected context
- generated answer
- citations

Each RAG eval unit is a **full pipeline snapshot**.

---

### Types of RAG Evaluations

#### 1) Retrieval-Only Evals

Evaluate:
- recall@k
- relevance of retrieved chunks
- coverage of known answers

Used to:
- tune embeddings
- tune search parameters
- compare retrievers

---

#### 2) End-to-End RAG Evals

Evaluate:
- final answer quality
- grounding
- completeness
- citation correctness

Most important for product decisions.

---

#### 3) Claim-Level Evals (Advanced)

Break answers into:
- individual claims

Then check each claim against context.

This catches subtle hallucinations.

---

#### 4) Adversarial RAG Evals

Designed to:
- tempt hallucination
- test missing data
- confuse retrieval
- test misleading documents

Critical for enterprise trust.

---

### Automated RAG Evaluation Techniques

Automation can detect:
- missing citations
- unsupported claims (via LLM judges)
- citation mismatch
- retrieval failure patterns

Automation must be:
- calibrated against human evals
- conservative in confidence

RAG automation without calibration is dangerous.

---

### Human-in-the-Loop for RAG

Humans are used to:
- define grounding standards
- validate automated judges
- review edge cases
- audit failures

In 2026, **humans supervise RAG truth**, not generation quality.

---

### RAG Evals in Production

Production monitoring includes:
- hallucination rate
- citation failure rate
- "not found" response rate
- retrieval failure alerts

RAG systems must be monitored continuously.

---

### Enterprise Expectations

Enterprises expect:
- provable grounding
- auditable answers
- predictable behavior
- conservative defaults

A single hallucinated answer can kill adoption.

---

### Founder Perspective

For founders:
- RAG evals protect credibility
- allow faster iteration
- enable enterprise sales
- reduce support burden

RAG failures destroy trust silently.

---

### Common Failure Modes

- evaluating only final text
- ignoring retrieval quality
- trusting citations blindly
- over-optimizing recall at the cost of precision
- averaging grounding scores

These mistakes scale badly.

---
