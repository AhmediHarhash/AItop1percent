# 7.11 â€” RAG Test Fixtures and Synthetic Data Generation

In November 2025, a legal tech startup spent three months building a contract analysis RAG system before they could test it properly. They needed a test corpus of realistic contracts with known correct answers for specific queries. Creating it manually meant hiring lawyers to write contracts, draft queries, and label correct answers. The cost was prohibitive. The timeline was unacceptable. They tried using real client contracts, but legal and privacy constraints made that impossible. They were stuck: they had a system but no way to test it.

The team eventually discovered they could generate synthetic test data using LLMs. They prompted GPT-4 to generate realistic employment contracts, lease agreements, and NDAs. They generated question-answer pairs from these contracts. They seeded the generator with real contract clauses anonymized from public filings. The synthetic corpus was not perfect, but it was realistic enough to catch bugs and measure quality. They created 200 synthetic contracts and 500 query-answer pairs in two weeks instead of three months.

The synthetic test data unlocked their testing workflow. They could test changes quickly without waiting for manual labeling. They could expand test coverage by generating more examples on demand. They could create edge cases that were rare in real data but important to handle. Synthetic test fixtures turned testing from a bottleneck into an accelerator. The lesson was clear: in domains where real test data is expensive, synthetic data is not a compromise. It is a necessity.

## The Testing Data Problem: Real Data Is Expensive and Sensitive

RAG testing requires corpora with known properties: documents with known structure, queries with known answers, known edge cases. Acquiring this data is hard.

Real production data is ideal for realism but has problems. First problem: sensitivity. Production documents might contain PII, trade secrets, confidential information. You cannot use them in test environments without risking leaks. Even anonymizing is risky and expensive.

Second problem: labeling cost. You have documents but no labeled query-answer pairs. Creating labels requires domain experts. A lawyer must review contracts and write questions with correct answers. A doctor must review medical records and label diagnoses. Expert time is expensive.

Third problem: coverage gaps. Real data reflects what happened, not what could happen. You might have 1000 contracts but none that test a specific edge case like conflicting jurisdiction clauses. You cannot test what your data does not cover.

Fourth problem: evolution. Real data goes stale. Regulations change, products change, terminology changes. Test data from six months ago might not represent current reality. Maintaining real test data requires continuous curation.

Synthetic data addresses these problems. It is cheap to generate, contains no sensitive information, covers edge cases on demand, and can be regenerated to match current needs. The trade-off is realism. Synthetic data might miss patterns present in real data. But for testing, synthetic data is often good enough.

## Creating Test Data for RAG: Documents, Queries, and Expected Answers

A RAG test fixture has three components: a document corpus, a set of queries, and expected answers for each query.

Creating synthetic documents starts with defining document types. For a contract analysis system, you need employment contracts, NDAs, lease agreements, vendor agreements. For a customer support system, you need product manuals, FAQs, troubleshooting guides. You list the document types your system handles.

For each document type, you create a template or schema. An employment contract has sections: parties, term, compensation, benefits, termination. You define the structure. You might use a template with placeholders: "This agreement is between EMPLOYER_NAME and EMPLOYEE_NAME for a term of TERM_LENGTH."

You fill placeholders with synthetic values. You use random generation, sampling from lists, or LLM generation. EMPLOYER_NAME might be "Acme Corp" or "Widget Industries." TERM_LENGTH might be "12 months" or "indefinite." You generate variations to create diverse documents.

You use LLMs to generate realistic prose. You prompt an LLM: "Generate a confidentiality clause for an employment contract in the technology industry." The LLM produces plausible legal text. You insert it into your template. The result is a synthetic contract that looks realistic even though no human wrote it.

Creating synthetic queries means anticipating what users will ask. For contract analysis, users might ask "What is the termination notice period?" or "What are the confidentiality obligations?" You list common question types. You generate variations by substituting terms: "What is the notice period?" "What is the termination clause?" "How much notice is required?"

Creating expected answers requires grounding in the synthetic documents. If a contract says "30 days notice," the expected answer for "What is the notice period?" is "30 days." You generate queries and answers together: create a document, extract facts, generate questions about those facts, label the correct answers.

You automate this process. A script generates 100 documents, extracts key facts from each, generates 5 questions per document, labels answers based on extracted facts. You review a sample to verify quality, then generate thousands of test cases.

## Synthetic Question-Answer Pairs from Documents

Generating question-answer pairs from existing documents is a common testing workflow. You have documents, you need queries and answers. You use LLMs to generate them.

The simplest approach is passage-to-question generation. You take a document chunk, feed it to an LLM, and prompt: "Generate a question that can be answered using this text." The LLM produces a question. The passage is the answer. You now have a query-answer pair grounded in your corpus.

You generate multiple questions per passage to increase coverage. You vary the prompt: "Generate a factual question," "Generate a yes/no question," "Generate a question requiring inference." Different question types test different capabilities.

You filter low-quality generated questions. Some generated questions are vague, unanswerable, or trivial. You use heuristics: reject questions shorter than five words, questions without question marks, questions that start with "Can you." You use a classifier to score question quality and keep only high-scoring questions.

You generate answer text, not just answer passages. You prompt the LLM: "Answer this question using only information from the provided text." The LLM generates an answer. You verify the answer is grounded by checking that key terms appear in the source passage. You now have a query and a natural-language expected answer.

You create adversarial question-answer pairs by generating questions that look related but are not answerable from the passage. You prompt: "Generate a question that seems related to this text but cannot be answered using it." This tests whether your system correctly identifies when it lacks information.

You use question-answer generation at scale. You run it on your entire document corpus. You generate 10 questions per document. If you have 1000 documents, you generate 10,000 test cases. You sample and review 100 to verify quality. If quality is acceptable, you use the generated data for testing.

## LLM-Generated Test Cases: Prompts and Quality Control

LLMs are powerful test data generators, but they require careful prompting and quality control.

Effective prompts are specific and constrained. Instead of "Generate a contract," you prompt "Generate an employment contract for a software engineer in California with salary information, benefits, and a non-compete clause." Specificity yields realistic output.

You use few-shot prompting to guide style. You provide examples of real documents or queries. You prompt: "Here are three examples of customer support queries. Generate five more in the same style." The LLM mimics the examples.

You use structured output to enforce schema. You prompt: "Generate a contract in JSON format with fields: parties, term, compensation, clauses." The LLM produces structured data you can parse and validate.

You generate in batches and filter. You generate 100 candidate documents or questions. You apply quality filters: length thresholds, keyword presence, format validation. You keep 70 that pass filters. This is more efficient than carefully crafting each example.

You use multiple models for diversity. You generate half your test data with GPT-4, half with Claude. Different models produce different outputs. Diversity improves test coverage.

Quality control is essential. LLM-generated data has errors. You sample and manually review. You review 10 percent of generated test cases. You measure error rate. If error rate is below 5 percent, you use the data. If higher, you refine prompts and regenerate.

You validate grounding. For question-answer pairs, you verify the answer is actually in the source document. You use automated checks: keyword overlap, semantic similarity. You manually review a sample to catch subtle errors like wrong numbers or inverted meanings.

You version test fixtures. You store generated test data in version control. You document the generation process: which model, which prompt, which seed. If you need to regenerate, you can reproduce the same data. Versioning ensures tests are stable across code changes.

## Realistic Test Corpora: Balancing Realism and Cost

Synthetic data is cheap but less realistic than real data. The challenge is balancing cost and realism.

High-realism synthetic data uses real data as seed. You start with real documents, anonymize them, use them as templates. You replace names, numbers, and sensitive fields with synthetic values. The structure and language are real; only the specifics are synthetic. This is more realistic than generating from scratch but requires access to real data and careful anonymization.

Medium-realism synthetic data uses domain knowledge. You hire a domain expert to write document templates and review generated examples. The expert ensures synthetic data reflects real-world patterns. This is cheaper than labeling real data but more expensive than fully automated generation.

Low-realism synthetic data is fully automated. You generate documents and questions using LLMs without human oversight. This is cheapest but least realistic. It works for catching obvious bugs but might miss domain-specific edge cases.

You choose realism level based on risk. For high-stakes systems like medical or legal RAG, you invest in high-realism data. For low-stakes systems like internal documentation search, low-realism data is sufficient.

You mix realism levels. You have a small high-realism core test set created with expert oversight. You have a large low-realism test set generated automatically. You run both. The core set catches critical failures. The large set provides coverage.

You validate realism by comparing to production. You measure whether synthetic test data has similar properties to production queries: length distribution, vocabulary, question types. If synthetic queries are much shorter or use different vocabulary, they are not realistic. You adjust generation to match production distribution.

## Fixture Management: Versioning Test Data Alongside Code

Test fixtures are code artifacts. You manage them like code: version control, review, and testing.

You store fixtures in your repository. Documents, queries, expected answers live alongside application code. When you check out a branch, you get the test data that matches that code version. This ensures tests are reproducible.

You version fixtures when they change. When you add new document types or new edge cases, you update fixtures and commit the change. The fixture update and code change are part of the same commit or pull request. Reviewers see both.

You review fixture changes like code changes. When someone adds 50 new test queries, you review a sample. Are they realistic? Do they cover new scenarios? Are expected answers correct? Review catches low-quality additions before they pollute the test set.

You test the test data. You write tests that verify fixture properties: all documents are valid format, all queries have answers, all answers reference valid document IDs. Fixture validation catches corruption and errors.

You separate fixtures by stability. You have stable fixtures that rarely change: core test cases that have been validated and trusted for months. You have experimental fixtures: new test cases being evaluated. Stable fixtures are the quality gate for deployment. Experimental fixtures are for development iteration.

You archive obsolete fixtures. When you change document formats or retire features, old test cases become invalid. You move them to an archive rather than deleting them. The archive preserves history and might be useful if you need to understand past behavior.

You sync fixtures across environments. Test fixtures in staging match fixtures in CI match fixtures on developers' laptops. You avoid environment-specific fixtures that cause tests to pass locally but fail in CI.

## Scaling Test Generation: Automating Fixture Creation

Manual test case creation does not scale. You automate generation to create thousands of test cases.

You build a test generation pipeline. The pipeline takes document templates, generates documents, extracts facts, generates questions, generates answers, validates quality, and outputs test fixtures in standardized format. You run the pipeline whenever you need more test data.

You parameterize generation. You configure how many documents to generate, which document types, which question types, which edge cases to include. You run the pipeline with different configurations to create diverse test sets.

You use generation for regression testing. When you find a production bug, you add the failing query to a seed file. The pipeline generates variations of that query and similar edge cases. You create a cluster of test cases around the bug, ensuring it is thoroughly tested.

You generate edge cases on demand. You need to test what happens when all retrieved documents contradict each other. You generate five documents with contradictory information and a query that retrieves all of them. You cannot wait to encounter this in production. You synthesize it.

You scale generation with compute. Generating 10,000 test cases takes time. You parallelize: run 10 generation jobs simultaneously. You use batch APIs that are cheaper than interactive APIs. You trade latency for cost.

You curate generated data. Not all generated cases are worth keeping. You run generation, measure diversity and coverage, keep the most valuable cases, discard redundant or low-quality cases. Curation keeps test sets manageable and high-signal.

## Tradeoffs: When to Use Synthetic Data and When to Use Real Data

Synthetic data is not always the answer. You choose based on your constraints.

Use synthetic data when real data is unavailable, sensitive, or expensive to label. Use it for rapid iteration early in development when you need test coverage quickly. Use it for edge cases that are rare in real data.

Use real data when realism is critical, when you are optimizing for production performance, when you are validating before launch. Use it for final quality checks, for benchmarking against competitors, for audits and compliance.

You start with synthetic, graduate to real. Early development uses synthetic data for fast iteration. As the system matures, you invest in real data for validation. You ship when real-data testing shows quality is acceptable.

You use synthetic data for unit and integration tests, real data for regression and production tests. Synthetic data catches bugs during development. Real data validates production readiness.

You use synthetic data to augment real data. You have 100 real test cases but need 1000. You generate 900 synthetic cases. The real cases anchor quality. The synthetic cases provide coverage.

The legal tech startup's testing strategy evolved. They started with 200 fully synthetic contracts for initial development. This let them iterate quickly and catch obvious bugs. They then acquired 50 real contracts from public court filings, anonymized them, and used them as high-realism templates. They generated 300 more contracts using the real contracts as seeds.

They generated 500 question-answer pairs automatically using LLM passage-to-question generation. They manually reviewed and corrected 100 of them to create a high-quality core test set. They used the remaining 400 as a coverage test set.

They versioned all test fixtures in git. When they added support for a new contract type, they generated 20 synthetic examples and added them to the fixture repository. When they found a production bug, they added the failing case to the test set and generated 10 variations.

Six months later, they had 600 synthetic documents, 200 real-derived documents, and 1200 labeled query-answer pairs. The test corpus covered common cases, edge cases, and known failure modes. Testing went from a three-month blocker to a two-day automated pipeline. Synthetic data generation turned testing from a bottleneck into a competitive advantage.

RAG test fixtures and synthetic data generation are the tools that make comprehensive testing feasible. Real data is ideal but often unavailable or too expensive. Synthetic data is cheap, flexible, and scalable. The teams that master synthetic data generation can test thoroughly without waiting months for labeled datasets. The teams that insist on only real data either wait forever or ship untested systems. The difference is pragmatism about test data sources and discipline about quality control.
