# 7.3 â€” Regression Testing: Catching Silent RAG Degradation

In February 2026, an enterprise search company noticed their NPS score dropping from 72 to 58 over six weeks, but their error logs showed nothing unusual. System uptime was 99.97 percent. Latency was stable. Their monitoring dashboards were green. Yet users were complaining that answers were "less helpful than before" and "missing obvious information." The engineering team initially dismissed this as subjective perception, not a real problem. Two canceled enterprise renewals later, they started digging.

The investigation revealed that answer quality had degraded silently across three separate changes. First, they updated their embedding model to a newer version that had better benchmark scores but worse domain alignment for their legal document corpus. Second, they reindexed documents with a new chunking strategy that improved average chunk coherence but broke context for multi-page tables. Third, they tuned their reranker to reduce latency by 200 milliseconds but reduced top-5 precision by 11 percentage points. Each change looked good in isolation. Together, they destroyed the product.

The company had no regression test suite. They measured each change against benchmarks but never measured whether the change degraded quality on real user queries. They had no baseline of "known good answers" to compare against after changes. They had no automated system to detect quality drops. They learned the hard way that RAG quality degrades silently and invisibly unless you actively test for it. Regression testing is not optional. It is the immune system that prevents slow death by a thousand cuts.

## Why RAG Quality Degrades Without Code Changes

Traditional software regresses when you change code. RAG regresses when you change code, data, models, or configuration. The attack surface for quality degradation is orders of magnitude larger. You update your document corpus by adding new contracts, and suddenly existing queries return outdated documents because the new content dilutes retrieval scores. You switch to a better embedding model, and domain-specific queries break because the new model was not trained on your terminology. You adjust a reranking threshold to improve precision, and recall collapses for edge-case queries.

The first regression vector is model updates. Your embedding provider releases a new version with better MTEB scores. You upgrade. The new embeddings cluster documents differently, changing which documents are semantically close to which queries. Queries that previously returned the right documents now return slightly different documents that seem plausible but miss key information. Users notice. You do not, because you never tested whether the new embeddings maintained quality on your specific queries.

The second regression vector is index updates. You add new documents to your corpus. The vector database rebalances indices, changes internal data structures, updates statistics for approximate nearest neighbor search. Queries that previously returned ten results now return eight because the ANN threshold filtered two results as too distant. Or queries that returned focused results now return broader results because the new documents shifted the semantic space. The corpus grows, quality changes.

The third regression vector is prompt changes. You tweak your generation prompt to handle a new edge case. The tweak fixes that edge case but subtly changes how the model handles common cases. Citations become less precise. Tone shifts. Factual grounding weakens. You changed ten words in a 400-word prompt, and quality regressed on 5 percent of queries. You do not notice until users complain.

The fourth regression vector is configuration drift. Your deployment script changes a reranker temperature from 0.7 to 0.9 to match a new default. Your chunk size config changes from 512 to 500 tokens because of a merge conflict resolution. Your retrieval top-k changes from 20 to 15 because someone thought "it should be faster." Each change is tiny and seems inconsequential. Together they compound into measurable quality loss.

The fifth regression vector is dependency updates. Your vector database client library updates and changes its default similarity metric from cosine to dot product. Your LLM provider updates their model and changes output formatting slightly. Your text processing library updates and changes how it handles unicode. None of these are your code. All of them break your system.

Silent degradation is uniquely dangerous because it evades normal monitoring. Error rates do not spike. Latency does not degrade. The system keeps running. But users slowly lose trust as answers become less accurate, less complete, less helpful. By the time you notice the churn, the damage is done. Regression testing catches these degradations early, when they are single changes you can identify and revert rather than compounded changes that require forensic analysis to untangle.

## Building a Golden Dataset: The Regression Test Corpus

A golden dataset is a collection of queries with known correct answers that represent your production distribution. You use this dataset to measure quality before and after changes. If a change degrades quality on the golden dataset, you reject the change. If quality improves, you ship. The dataset is your quality gate.

Building a golden dataset starts with sampling real production queries. You do not want synthetic queries that look reasonable but miss real user behavior. You want the actual questions users ask, with the actual ambiguity, jargon, typos, and edge cases. You sample 200 to 500 queries covering common patterns, rare but important patterns, and known failure modes.

For each query, you manually label the correct answer or answer properties. This is tedious and expensive but essential. You cannot automate quality measurement without ground truth. You either label "the answer should mention fact X and cite document Y" or you label "the full answer text for this query is Z." The former is more robust to model non-determinism. The latter is more precise.

Some queries do not have a single correct answer. A query like "what are the main risks?" might have five valid answers depending on what "main" means and which document sections you prioritize. For these, you label multiple acceptable answers or define criteria like "must mention at least three of these five risks." You make the labeling as objective as possible.

You version the golden dataset alongside your code. When you add a new feature, you add queries testing that feature to the dataset. When you discover a production bug, you add the failing query to the dataset. The dataset grows over time, accumulating the queries that matter most to your users.

You refresh labels periodically. Documents change, user needs evolve, terminology shifts. A query that had answer A in January might have answer B in June because the underlying document was updated. You re-label queries when the ground truth changes, ensuring your regression tests reflect current correctness.

You split the dataset into a held-out set and an active set. The held-out set never gets used for tuning or optimization. You only run it to validate final changes before production. The active set is used during development for rapid iteration. This prevents overfitting where you tune the system to perform well on your test set but degrade on real production queries.

The golden dataset is expensive to create and maintain, but it pays for itself immediately. The enterprise search company built a 300-query golden dataset after their quality incident. They labeled each query with expected answer properties: required facts, required citations, required tone. They ran the dataset against their production system and established a baseline: 88 percent of queries met all quality criteria.

They then reran the dataset after every change. When they tested a new embedding model, quality dropped to 81 percent on the golden dataset. They rejected the model. When they tested a new chunking strategy, quality improved to 91 percent. They shipped it. When they refactored their prompt, quality held steady at 88 percent. They shipped with confidence. The golden dataset became their North Star metric, the single number that determined whether changes shipped or got rejected.

## Automated Regression Detection: Measuring Quality Drift Over Time

Regression testing is only valuable if it runs automatically. Manual regression testing is slow, inconsistent, and easily skipped under schedule pressure. Automated regression testing runs on every commit, every deployment, and every scheduled interval. It catches degradations immediately, not weeks later when users complain.

The simplest automation is a CI job that runs your golden dataset queries against the system and checks that quality meets a threshold. If the baseline is 88 percent query success rate and the new code produces 84 percent, the CI job fails. The change is blocked. You investigate, fix the regression, or revert the change.

More sophisticated automation tracks quality over time and alerts on trends. You log quality metrics for every production deployment: overall success rate, success rate by query category, latency, cost per query. You plot these metrics and look for degradation trends. If quality drops one percentage point per week for three weeks, you investigate even if no single week triggered an alert.

Trend detection catches slow degradations that per-deployment checks miss. If quality drops from 88 to 87.5 percent, that might be noise. But if it drops from 88 to 87.5 to 87 to 86.5 over four deployments, that is a signal. Something is slowly breaking. Automated trend detection alerts you before the degradation becomes a user-visible crisis.

You set alert thresholds based on impact. A one-point drop in quality might be acceptable for a latency optimization. A five-point drop is never acceptable. A degradation in high-stakes queries like "drug dosage for patient X" is unacceptable even if overall metrics hold steady. You weight queries by importance and set stricter thresholds for critical query categories.

You run regression tests at multiple stages. Pre-commit tests run a small fast subset of the golden dataset to catch obvious breaks before code review. Pre-deployment tests run the full golden dataset to catch subtle degradations before production. Post-deployment tests run the golden dataset against production to verify the deployment succeeded. Scheduled tests run daily or weekly to catch degradations caused by data changes, model drift, or infrastructure issues.

You integrate regression results into your deployment pipeline. If regression tests fail, the deployment is blocked or rolled back automatically. You do not rely on humans to notice test failures and decide whether to ship. You automate the decision: quality regression equals do not ship. This removes pressure to ignore test failures when deadlines loom.

You log detailed diagnostics when tests fail. You store which queries failed, what answers the system produced, what answers were expected. You diff the current output against the baseline output. You provide engineers with enough information to debug the regression without manually reproducing it. Good diagnostics turn a vague "quality dropped" into a specific "query X now returns document Y instead of document Z because the reranker changed."

## Before and After Comparison: Measuring the Impact of Changes

Every change to a RAG system should be evaluated with a before-and-after comparison on the golden dataset. Before the change, you measure quality. After the change, you measure quality. You compare. If quality improved or held steady, you ship. If quality degraded, you investigate or revert.

The comparison is not a single number. You break it down by query category, document type, answer length, citation count, latency bucket. A change might improve quality overall but degrade quality for a specific query category that matters to your most valuable users. The aggregate metric hides the regression. Category-level metrics reveal it.

You track not just success rate but also failure modes. Did the change reduce hallucination but increase "no answer available" responses? Did it improve citation accuracy but increase latency beyond SLA? Did it fix edge cases but break common cases? The failure mode analysis tells you whether a change is a net win or whether it trades one problem for another.

You use statistical significance tests to distinguish real regressions from noise. With a 300-query golden dataset, if quality drops from 88 percent to 87 percent, is that a real degradation or random variation? You run a significance test. If p-value is less than 0.05, you treat it as a real regression. If not, you treat it as noise. This prevents you from rejecting good changes because of statistical fluctuation.

You A/B test changes in production when the before-and-after comparison is inconclusive. You route 10 percent of traffic to the new version and 90 percent to the old version. You measure quality, latency, and user satisfaction for both groups. If the new version performs better, you ramp it up to 100 percent. If it performs worse, you kill it. Production A/B tests are the ultimate regression test because they measure real user impact, not synthetic dataset performance.

You document every comparison. When you test a new chunking strategy, you record: baseline quality 88 percent, new quality 91 percent, improved categories legal-queries and table-extraction, degraded categories short-answers. You store this in your change log. When quality regresses six months later, you can review the change history and see which changes affected which query categories. This makes forensic debugging vastly easier.

You build tooling to make comparisons fast. You do not manually run queries before and after. You have a script that runs the golden dataset, logs results, computes metrics, generates a report, and posts it to Slack or your PR review tool. The whole process takes two minutes and happens automatically. This removes friction and ensures every change gets regression-tested.

## Alert Thresholds for Quality Drops: When to Block Changes

Setting alert thresholds is a judgment call. Too strict, and you block changes that are net positive. Too loose, and you ship changes that degrade quality. You tune thresholds based on your risk tolerance, user expectations, and iteration velocity.

A common starting point is five percent relative degradation. If baseline quality is 88 percent and a change drops it to 83.6 percent, that is a five percent relative drop. You block the change or require a strong justification. If quality drops to 87.1 percent, that is a one percent relative drop. You might accept it if the change brings other benefits like reduced latency or cost.

You set stricter thresholds for critical queries. Queries about safety, compliance, legal, medical, financial information have zero tolerance for regression. If quality on critical queries drops at all, you block the change. Queries about general information or low-stakes topics have looser thresholds. A small regression is acceptable if the change improves the system overall.

You set time-based thresholds for trend detection. If quality drops one percent per week for three consecutive weeks, you investigate even if each individual week is within tolerance. Slow continuous degradation is as dangerous as sudden regression and harder to notice without automated trend monitoring.

You set absolute thresholds as a safety net. No matter what the baseline is, if quality drops below 80 percent on the golden dataset, you do not ship. This prevents degradation from compounding over time. Even if you accept a series of small regressions, the absolute threshold ensures you never degrade past a minimum acceptable level.

You distinguish between different types of errors. Hallucination is worse than "no answer available." Citing the wrong document is worse than missing a citation. You weight errors by severity. A change that reduces hallucination by two percentage points but increases missing citations by three percentage points might be net positive because hallucination is more damaging to user trust.

You allow threshold overrides with documentation. Sometimes you need to ship a change that regresses quality in one dimension to improve it in another. You require engineers to document why the regression is acceptable, what compensating improvements exist, and how you plan to address the regression in future work. The override creates a paper trail that prevents quality from degrading silently.

You review and tune thresholds quarterly. As your system matures, your quality standards should increase. A threshold that was appropriate at 80 percent baseline quality might be too loose at 90 percent. You tighten thresholds as your system improves, maintaining pressure to keep getting better.

## Practical Workflow: Integrating Regression Tests Into Development

Regression testing is only effective if it is easy to run and hard to skip. You integrate it into your development workflow so it happens automatically, not as a manual checklist item that gets forgotten under pressure.

The workflow starts with local development. Before pushing code, the engineer runs a subset of the golden dataset locally. This is 20 to 50 queries that run in under a minute. If these fail, the engineer knows immediately that the change broke something. They fix it before pushing. This catches obvious regressions before code review.

After pushing, CI runs the full golden dataset. This takes 5 to 10 minutes. If it passes, the PR is approved for merge. If it fails, CI posts a comment on the PR with failing queries and diagnostic details. The engineer investigates, fixes the regression, and pushes again. The PR does not merge until regression tests pass.

After merge, staging deployment runs the golden dataset against the deployed staging environment. This catches regressions introduced by environment differences: config files, database versions, model endpoints. If staging tests fail, the deployment is rolled back automatically. No one needs to monitor the deploy; the system handles failures.

After production deployment, you run the golden dataset again against production. This verifies the deployment succeeded and no production-only issues emerged. If production tests fail, you alert the on-call engineer and consider a rollback. Post-deployment tests are the last safety check before users see the change.

You run scheduled regression tests daily. These catch degradations caused by index updates, corpus changes, or external dependencies. If a scheduled test fails, you investigate. You diff current results against yesterday's results. You check recent index updates, model changes, dependency updates. You identify the cause and fix it.

You integrate regression results into your team dashboard. You show current quality, quality trend over the past month, recent regressions and fixes. You make quality visible so everyone on the team sees when it degrades. Visibility creates accountability. When quality is a number on a dashboard everyone checks, regressions get fixed quickly.

You celebrate quality improvements. When a change improves regression test performance, you highlight it in team meetings. When quality trends upward, you recognize the work that made it happen. This creates a culture where quality is valued, not just velocity.

The enterprise search company that suffered silent degradation rebuilt their development workflow around regression testing. They created a 50-query fast regression suite that ran in local development. They required full golden dataset tests to pass before PR merge. They ran post-deployment tests in production and alerted on failures. They reviewed quality trends weekly in team meetings.

Six months later, quality had improved from 88 percent to 93 percent on the golden dataset, and user NPS recovered from 58 to 69. They shipped 40 changes during that period. Regression tests blocked 8 of them for quality issues, which were fixed before shipping. They caught and reverted 3 deployments that introduced regressions in production. They identified and fixed 5 slow degradations detected by trend monitoring.

The regression test suite became the foundation of their quality culture. Every change was measured. Every degradation was caught. Every improvement was validated. They stopped guessing whether changes helped or hurt. They knew, because they tested every change against real queries with known correct answers. That knowledge turned quality from a vague aspiration into a concrete metric they could optimize.

Regression testing is not exciting. It is the boring work that prevents disasters. It is the test suite you run every day that passes every day until the one day it catches a regression that would have destroyed user trust. It is the golden dataset you laboriously labeled that pays for itself the first time it blocks a bad deployment. It is the infrastructure that makes RAG quality predictable, measurable, and improvable. Teams that build it ship reliable systems. Teams that skip it suffer silent degradation until users flee.
