# Chapter 4 â€” Query Understanding and Retrieval

The quality of retrieval begins before the first vector is searched. It begins with understanding the query. A user types a question, and your system must decide what to retrieve, how deeply to search, what filters to apply, and whether the query is even answerable. Query understanding transforms raw input into a structured retrieval plan, and in 2026 it is the difference between precise answers and wasted compute.

This chapter teaches you how to analyze, rewrite, and expand queries to improve retrieval quality. You will learn when to rewrite queries for clarity, when to generate multiple query variants, how to apply filters based on query semantics, and how to adapt retrieval depth dynamically. Query understanding is the front door to your RAG pipeline. Get it right and retrieval becomes easier. Get it wrong and no amount of reranking will save you.

Query analysis is the process of extracting intent, entities, and constraints from the user's input. A query like "latest pricing for enterprise" contains an intent to retrieve pricing information, an entity specifier for enterprise tier, and a temporal constraint for latest. Parsing these elements allows you to filter by product tier and sort by recency before embedding search. Query analysis can be rule-based for simple patterns or model-based for complex language. You will learn how to build query analyzers that extract structured signals without adding latency.

Rewriting transforms the user's query into a form that retrieves better. Queries are often vague, ambiguous, or poorly phrased. Rewriting can expand abbreviations, resolve pronouns, add missing context from conversation history, or simplify complex multi-part questions. For example, "What did the CEO say about it?" might rewrite to "What did the CEO say about revenue in Q3 2025?" using conversation context. You will learn when rewriting helps and when it introduces errors.

HyDE stands for Hypothetical Document Embeddings. Instead of embedding the query directly, you generate a hypothetical answer to the query, then embed and search for that answer. This works because answers are often more similar to documents in embedding space than questions are. HyDE is effective when queries are short or abstract but documents are detailed and concrete. You will learn when HyDE improves retrieval and how to generate hypotheticals without hallucinating.

Multi-query generation creates multiple query variants to improve recall. A single query may miss relevant documents due to phrasing differences. Generating paraphrases, decomposing the query into sub-questions, or creating targeted variants for different document types increases the chance of retrieving all relevant content. Results from all queries are merged and deduplicated. Multi-query generation trades compute cost for recall. You will learn when to use it and how to merge results effectively.

Filters restrict retrieval to a subset of the corpus based on metadata. Filters can be explicit, derived from the query, or inferred from context. For example, a query about "Q4 earnings" can filter by document type equals financial report and date range equals Q4 2025. Filters reduce search space, improve precision, and lower cost by avoiding irrelevant retrieval. You will learn how to extract filter criteria from queries and how to combine filters with vector search.

Depth refers to how many documents or chunks to retrieve. Shallow retrieval fetches the top 5 to 10 results and is fast but may miss relevant content. Deep retrieval fetches 50 to 100 results to maximize recall but increases latency and reranking cost. Adaptive depth adjusts retrieval count based on query complexity or confidence. You will learn how to choose depth based on query type and how to measure the recall-latency tradeoff.

Fusion combines results from multiple retrievers or query variants into a single ranked list. Reciprocal rank fusion and weighted score fusion are common strategies. Fusion improves robustness because different retrievers fail on different queries. You will learn how to fuse results without double-counting duplicates and how to tune fusion weights.

Contextual retrieval uses conversation history or session state to disambiguate queries. In multi-turn conversations, queries often reference prior turns: "What about pricing?" only makes sense if the prior turn discussed products. Contextual retrieval can prepend conversation history to the query, embed the full dialogue, or maintain a session-level index of retrieved documents. You will learn how to design stateful retrieval without leaking context across users.

Hierarchical retrieval searches at multiple levels of granularity. First pass retrieves documents. Second pass retrieves chunks within top documents. This two-stage approach balances document-level relevance with chunk-level precision. Hierarchical retrieval is standard for long-form content like legal documents and research papers. You will learn when hierarchical retrieval outperforms flat chunk search.

Time-aware retrieval ranks recent documents higher than old ones, or filters by date ranges. Temporal relevance is critical for news, policy updates, and versioned documentation. Time-awareness can be encoded as metadata filters, recency weighting in ranking, or temporal embeddings that encode time as part of the vector. You will learn how to model temporal relevance and how to balance it with semantic relevance.

Permission-aware retrieval ensures users only retrieve documents they are authorized to access. In multi-tenant systems, documents are tagged with access control lists, and retrieval filters by user permissions before returning results. Permission checks must happen before reranking to avoid leaking information through result counts or partial matches. You will learn how to implement secure retrieval without sacrificing performance.

Debugging retrieval failures requires visibility into what was retrieved and why. Logging retrieved chunks, similarity scores, and filter matches enables post-hoc analysis. You will learn how to instrument retrieval for debugging, how to identify common failure patterns, and how to use logs to improve query understanding and indexing.

Coverage measures whether retrieval returned all relevant documents for a query. Low coverage means the system missed critical information. Coverage is hard to measure without ground truth labels but can be estimated using redundancy checks or multi-query validation. You will learn how to monitor coverage in production and how to improve it through better chunking, metadata, or query expansion.

Adaptive depth dynamically adjusts retrieval count based on query confidence or complexity. Simple factual queries may need only 5 results. Complex research queries may need 50. Adaptive strategies use query classifiers or retrieval score distributions to set depth. You will learn how to build adaptive retrieval systems that optimize for both latency and recall.

This chapter is about control. Query understanding gives you control over what gets retrieved, how deeply, and with what constraints. In 2026, the best RAG systems do not just search. They plan, filter, expand, and adapt. And that planning happens before the first embedding is computed.
