# 8.8 â€” Incident Response for RAG Failures

In December 2025, a legal document retrieval system experienced a catastrophic failure during a high-stakes trial. Attorneys were using the RAG system to retrieve case law citations in real time during courtroom proceedings. At 10:42 AM, the system began returning hallucinated case citations: cases that did not exist, incorrect dates, and fabricated legal precedents. The legal team, trusting the system, cited three non-existent cases before opposing counsel challenged them. The judge issued a public reprimand for frivolous citations, and the case was jeopardized. The engineering team received an urgent page at 11:15 AM. They scrambled to diagnose the issue: embedding service was operational, vector database was online, LLM API was responding. At 12:30 PM, they discovered the root cause: a recent deployment had accidentally disabled retrieval entirely due to a configuration error. The LLM was generating responses without grounded context, hallucinating citations freely. The team rolled back the deployment, but the damage was done. The law firm terminated their contract, filed a malpractice claim, and the startup faced a 4.5 million dollar settlement. The engineering team had no incident response plan for RAG-specific failures, no runbooks, and no rollback procedures. They learned that RAG systems fail differently from traditional systems, and generic incident response is insufficient.

The failure unfolded in public, in a courtroom, with opposing counsel scrutinizing every word. The attorneys did not suspect the system was hallucinating. They had used it for weeks without issue. It had retrieved valid citations hundreds of times. There was no warning, no error message, no indication that anything was wrong. The system returned confident, authoritative answers. The format was correct: case names, citation formats, dates. Only the content was fabricated. The opposing counsel noticed because they were familiar with the cases and knew they did not exist. If the attorneys had been in a less adversarial setting, the fabrications might have gone undetected for days or weeks.

The engineering team's response was chaotic. They had never planned for a RAG-specific incident. Their runbooks covered database outages, API failures, and latency spikes. They had no runbook for hallucination spikes, retrieval failures, or stale data incidents. They spent 90 minutes diagnosing the issue because they checked generic infrastructure first: Is the database up? Are APIs responding? Is there a network partition? They did not check RAG-specific signals: Are we retrieving context? Are retrieval scores normal? Is the LLM receiving grounded input? By the time they found the configuration error, the attorneys had already cited fabricated cases in court.

You need to understand that RAG failures are unique. Traditional web service incidents involve downtime, latency spikes, or error rates. RAG incidents involve wrong answers, irrelevant retrievals, hallucination spikes, or index corruption. These failures are subtle: the system appears to be working, returning HTTP 200 responses, but the content is wrong. Users may not immediately notice, or they may notice only after making decisions based on incorrect information. By the time the incident is detected, damage is done. You must design incident response specifically for RAG failure modes, not just adapt generic playbooks.

## Classifying RAG-Specific Incident Types

RAG-specific incident types fall into five categories. First, retrieval failures: the vector database returns irrelevant documents, returns zero documents, or fails to return documents at all. Retrieval failures are often silent: the system does not error, it just retrieves poorly. Users receive answers based on weak or missing context, leading to vague, generic, or incorrect responses. Retrieval failures can result from index corruption, misconfigured queries, embedding drift, or vector database outages.

Second, hallucination spikes: the LLM generates confident but incorrect information, often because retrieval provided poor context or no context. Hallucination spikes are the most dangerous failure mode because they produce plausible-sounding but false answers. Users trust the system, act on the information, and suffer consequences. Hallucination spikes can result from retrieval failures, prompt misconfigurations, or LLM API issues.

Third, stale data incidents: the system returns outdated information because document updates were not propagated. Stale data incidents are insidious because the system works correctly from an infrastructure perspective, but the information is obsolete. Users make decisions based on outdated facts, leading to errors, delays, or compliance violations. Stale data incidents result from propagation lag, update detection failures, or reindexing backlogs.

Fourth, index corruption: the vector index is corrupted, returning random or nonsensical results. Index corruption can result from failed updates, storage failures, or bugs in indexing code. Index corruption is often total: all queries return garbage. But partial corruption is more insidious: most queries work, but a small percentage return nonsense, making diagnosis harder.

Fifth, cascading failures: one component fails, and the failure propagates, degrading or disabling the entire pipeline. For example, if the embedding service is throttled, queries cannot be embedded, retrieval cannot proceed, and the system times out or hallucinates. Cascading failures are hard to diagnose because the symptom appears in one component, but the root cause is in another.

Incident classification determines response priority and escalation. Not all incidents are equal. A retrieval failure affecting 5 percent of queries is a P2 incident requiring investigation within hours. A hallucination spike affecting safety-critical information in healthcare or legal domains is a P0 incident requiring immediate response and user notification. A stale data incident in a documentation system is a P3 incident tolerable for days. Classification should be based on user impact, domain risk, and query volume affected. Misclassification leads to under-response or over-response, wasting resources or escalating minor issues unnecessarily.

## Detection: The Hardest Part of RAG Incident Response

Detection is the hardest part of RAG incident response. Unlike traditional services, where error rates spike and monitoring alerts immediately, RAG failures often manifest as gradual quality degradation. Retrieval scores drop slowly, hallucination rates increase subtly, and users complain sporadically. You must instrument your system to detect quality degradation in real time: monitor retrieval similarity scores, empty retrieval rates, LLM refusal rates, and user feedback signals such as thumbs-down or report-incorrect buttons. Automated anomaly detection can flag deviations from baseline: if average retrieval score drops from 0.78 to 0.62 over an hour, trigger an alert.

The challenge is distinguishing signal from noise. Retrieval scores fluctuate naturally based on query difficulty, corpus coverage, and user intent. If you set alert thresholds too low, you fire false alarms daily, leading to alert fatigue. If you set thresholds too high, you miss real incidents. You must calibrate thresholds based on historical data: measure baseline retrieval scores over weeks, compute mean and standard deviation, and alert when scores deviate by more than three standard deviations.

User reports are often the first signal of RAG incidents. Users notice wrong answers, irrelevant retrievals, or nonsensical responses before automated monitoring detects them. You must make it easy for users to report issues: include a "Report Incorrect Answer" button on every response, collect detailed feedback including the query and response, and route reports to an on-call engineer. User reports should trigger investigation, even if automated monitoring shows no anomaly. Sometimes the first user report is an early warning of a widespread issue.

The legal document retrieval system had no user reporting mechanism. Attorneys could not flag incorrect citations in the interface. They had to email support, which was checked once per day. By the time support noticed the emails, the attorneys had already cited fabricated cases in court. A simple "Report Incorrect Citation" button, routed to an on-call engineer, would have triggered investigation within minutes instead of hours.

Monitoring must cover every stage of the RAG pipeline. Monitor embedding latency and error rates: if embedding fails, queries cannot proceed. Monitor retrieval latency, similarity scores, and empty retrieval rates: if retrieval is slow or returns no results, answer quality degrades. Monitor reranking latency and score distributions: if reranking is slow or scores are abnormally low, context quality suffers. Monitor LLM latency, error rates, and refusal rates: if the LLM times out or refuses to answer, users receive no response. Monitor end-to-end latency and success rates: if queries are slow or failing, users abandon the system.

## Runbooks, Rollback, and Communication Templates

Runbooks document step-by-step response procedures for common incident types. A retrieval failure runbook includes: check vector database health, verify embedding service is returning valid vectors, inspect recent deployments for configuration changes, compare current retrieval scores to baseline, roll back recent changes if necessary, and notify users if retrieval is degraded. A hallucination spike runbook includes: verify retrieval is providing context, check if LLM API is returning unusual responses, inspect prompts for recent changes, enable stricter LLM refusal prompts, and roll back prompt or model changes. Runbooks reduce response time and ensure consistent handling across incidents.

Runbooks must be specific, actionable, and tested. A runbook that says "check if retrieval is working" is useless. A runbook that says "query the vector database with a test embedding and verify that similarity scores are above 0.6 and at least 10 results are returned" is actionable. Runbooks should include exact commands to run, expected outputs, and decision trees: if X is true, do Y; otherwise, do Z. Runbooks should be tested in incident drills, where engineers practice following them under time pressure.

Rollback procedures are critical for RAG systems because many failures result from deployments: updated prompts, model changes, configuration edits, or index rebuilds. Your deployment pipeline must support instant rollback: maintain the previous version of prompts, configuration, and indexes, and enable one-command rollback. If a deployment causes a hallucination spike, you roll back within minutes, not hours. Rollback should be automated and tested: practice rolling back in staging, measure rollback time, and ensure rollback does not introduce additional failures.

The legal document retrieval system had no rollback procedure. When the engineering team identified the configuration error, they manually edited the configuration file, restarted the service, and hoped it worked. The manual rollback took 15 minutes. An automated rollback would have taken 2 minutes. Those 13 minutes mattered: during that time, attorneys were still querying the system, receiving hallucinated citations, and potentially citing them in court.

Communication templates ensure clear, timely updates to stakeholders. When an incident occurs, you must notify affected users, internal stakeholders, and executives. A communication template includes: incident summary in one sentence, user impact, current status, estimated time to resolution, and next update time. For example: "We are experiencing an issue where 15 percent of queries return irrelevant results. The issue started at 10:42 AM. We have identified the cause and are rolling back a recent deployment. Expected resolution: 11:30 AM. Next update: 11:00 AM." Communication prevents panic, manages expectations, and builds trust.

Communication during the legal document retrieval incident was nonexistent. The engineering team did not notify the law firm. The attorneys had no idea the system was malfunctioning. If the team had sent a message at 11:15 AM saying "We are experiencing a technical issue with our citation retrieval system. Please do not rely on citations until further notice," the attorneys would have stopped using the system immediately, preventing further fabrications.

## Post-Mortems, Blamelessness, and Severity Levels

Post-incident analysis identifies root cause, contributing factors, and preventive measures. Within 24 to 48 hours of resolving an incident, the team conducts a blameless post-mortem. The post-mortem documents: timeline of events, root cause, why detection was delayed, why mitigation took as long as it did, and what will be done to prevent recurrence. For the legal document retrieval incident, the root cause was a missing feature flag check in the deployment. The contributing factor was lack of automated testing for retrieval-enabled responses. The preventive measures were: add automated tests verifying retrieval is active, add monitoring alerts for zero-retrieval-rate spikes, and implement staged rollouts with canary deployments.

Blameless culture is essential for effective post-mortems. If engineers fear blame or punishment, they hide mistakes, downplay incidents, and avoid transparency. If the culture is blameless, engineers candidly discuss failures, identify systemic issues, and propose improvements. Blameless does not mean accountability-free: engineers are accountable for implementing preventive measures and improving reliability. But the focus is on systems and processes, not individuals.

The legal document retrieval startup did not have a blameless culture. After the incident, the CEO blamed the engineer who deployed the configuration change. The engineer was fired. The remaining engineers became risk-averse, avoiding deployments and fearing innovation. The startup's velocity collapsed. A blameless post-mortem would have identified the systemic issues: lack of automated tests, lack of staged rollouts, lack of monitoring for retrieval failures. The preventive measures would have addressed those issues, improving reliability for everyone.

Incident severity levels define response expectations. P0 incidents are critical: data loss, widespread incorrect answers, or safety-critical failures. P0 incidents require immediate response, executive notification, and public communication. P1 incidents are major: significant query degradation, partial service outage, or high error rates. P1 incidents require response within 30 minutes and resolution within hours. P2 incidents are moderate: minor degradation, low error rates, or isolated issues. P2 incidents require response within hours and resolution within days. P3 incidents are minor: cosmetic issues, rare edge cases, or low-impact bugs. P3 incidents are triaged and fixed during normal work hours.

Severity classification must account for domain risk. A hallucination spike in a recipe recommendation app is a P2 incident: annoying but not dangerous. A hallucination spike in a legal citation system is a P0 incident: it jeopardizes cases and professional reputations. A hallucination spike in a medical diagnosis assistant is a P0 incident: it endangers lives. Severity depends on context, not just technical impact.

## On-Call, Escalation, and Graceful Degradation

On-call rotation ensures 24/7 incident response. Production RAG systems serving global users require around-the-clock monitoring and response. On-call engineers carry pagers, respond to alerts within 15 minutes, and escalate to specialists if needed. On-call shifts are typically one week, with compensation for after-hours work. On-call engineers must have access to runbooks, deployment tools, and rollback procedures. Training for on-call includes simulated incident drills, where engineers practice responding to hypothetical failures.

Escalation paths define who to contact for different incident types. If an on-call engineer cannot resolve an incident within 30 minutes, they escalate to a senior engineer or architect. If the incident is P0, they immediately escalate to the engineering manager and CTO. If the incident affects a high-value customer, they notify the account manager. Clear escalation paths prevent delays and ensure critical incidents receive appropriate attention.

The legal document retrieval system had no on-call rotation. The page went to a Slack channel that was not monitored on weekends. The incident occurred on a Friday morning. The engineering team was in meetings. The page sat unacknowledged for 33 minutes. If the system had a proper on-call rotation with pager duty, the incident would have been acknowledged within minutes.

Monitoring and alerting must cover RAG-specific failure modes. Traditional monitoring covers uptime, latency, and error rates. RAG monitoring must also cover retrieval quality, hallucination signals, and data freshness. Set up alerts for: empty retrieval rate exceeding 10 percent, average retrieval similarity score dropping below 0.65, LLM refusal rate exceeding 5 percent, propagation lag exceeding 30 minutes, and user report rate exceeding baseline. Alerts should fire on sustained degradation, not transient spikes, to avoid alert fatigue.

Incident drills simulate failures and test response procedures. Once per quarter, simulate a RAG incident: inject a configuration error that disables retrieval, corrupt a vector index, or throttle the embedding service. Page the on-call engineer and observe how they respond. Measure detection time, time to identify root cause, time to mitigation, and communication quality. Drills identify gaps in runbooks, monitoring, and rollback procedures. They build muscle memory for engineers, so real incidents feel familiar rather than chaotic.

Drills reveal weaknesses you cannot anticipate. In one drill, the on-call engineer followed the retrieval failure runbook but could not access the vector database because the credentials were stored in a shared document that was offline. The drill exposed a critical gap: credentials were not accessible during an incident. The team moved credentials to a secrets manager accessible to on-call engineers. Without the drill, that gap would have been discovered during a real incident, delaying resolution.

Graceful degradation prevents total failures. If retrieval fails, the RAG system should fall back to a cached response or a generic answer, rather than hallucinating or timing out. If the vector database is down, the system should return an error message to users instead of generating an ungrounded response. If the LLM API is throttled, the system should queue requests and provide estimated wait times. Graceful degradation limits blast radius: instead of a total outage, you have a partial degradation, and users receive lower-quality but safe responses.

The legal document retrieval system had no graceful degradation. When retrieval was disabled, the system did not fall back to an error message. It did not warn users that retrieval was unavailable. It silently generated hallucinated citations, confident and authoritative. A simple fallback: "Retrieval is currently unavailable. Please verify citations manually," would have prevented the entire incident.

Circuit breakers prevent cascading failures. If the vector database is timing out on every request, a circuit breaker detects the pattern and stops sending requests, allowing the database to recover. If the embedding service is returning errors, the circuit breaker halts query processing and returns cached responses. Circuit breakers reduce load on failing components, prevent queues from filling, and accelerate recovery. Circuit breakers should be tuned carefully: trigger too aggressively, and you cut off services that are only temporarily degraded. Trigger too conservatively, and failures cascade.

## Rebuilding with Discipline and Preparation

The legal document retrieval startup rebuilt their incident response after the settlement. They created runbooks for retrieval failures, hallucination spikes, and index corruption. They implemented automated rollback for deployments, tested weekly. They added monitoring alerts for retrieval quality, hallucination signals, and propagation lag. They conducted quarterly incident drills, simulating failures and practicing response. They established a blameless post-mortem culture, documenting every incident and implementing preventive measures. Six months later, a deployment introduced a retrieval latency spike. Monitoring detected it within 90 seconds, paged the on-call engineer, who followed the runbook and rolled back within 4 minutes. Fewer than 200 queries were affected. The CTO asked how they responded so quickly. The engineer replied: "We practiced."

You build incident response discipline now. You classify RAG-specific failure modes and document runbooks for each. You implement automated rollback and test it regularly. You set up monitoring and alerts for retrieval quality, hallucination signals, and data freshness. You create communication templates and escalation paths. You conduct incident drills quarterly. You establish a blameless post-mortem culture. You design graceful degradation and circuit breakers.

You recognize that incidents are inevitable. Production systems fail. The question is not whether you will have an incident, but how quickly you will detect it, how effectively you will respond, and how much you will learn from it. If you prepare now, incidents are learning opportunities and minor disruptions. If you do not prepare, incidents are catastrophes.

RAG incidents are inevitable. What is not inevitable is chaos. You prepare now, before the incident, before the damage, before the trust is lost. That preparation is the difference between a 4-minute rollback and a 4-million-dollar settlement.
