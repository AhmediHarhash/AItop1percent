# 4.8 â€” Contextual Retrieval: Adding Context Before Embedding

In October 2025, a Series B legal tech company watched their RAG system return a clause about termination rights that contradicted the actual policy in their knowledge base. The chunk that came back was technically accurate when isolated, but it referenced "the aforementioned provisions" that existed three paragraphs earlier in the original document. Without that context, the retrieval system served up guidance that was not just incomplete but actively misleading. The customer contract went out with incorrect termination language. The legal exposure was estimated at two million dollars. The root cause was simple: they had chunked their documents into isolated 512-token pieces and embedded each chunk as if it existed in a vacuum. The embedding model had no idea what document the chunk came from, what section it belonged to, or what preceded it. The chunk was orphaned from its meaning.

This is the fundamental problem with naive chunking strategies. You split your documents into manageable pieces so they fit into embedding models and context windows, but in doing so you strip away the document-level and section-level context that gives each chunk its meaning. A sentence like "This overrides the standard policy" is meaningless without knowing which policy, which document, or which version you are reading. When you embed that sentence in isolation, the embedding captures the syntactic pattern but loses the semantic grounding. The vector represents words, not meaning. And when a user query comes in that should match that chunk, the similarity score suffers because the query contains context that the chunk embedding does not.

The problem gets worse when your documents use specialized writing conventions. Legal documents rely heavily on cross-references: "as defined in Section 2.3 above" or "subject to the limitations in Exhibit A." Technical documentation assumes you have read the prerequisites: "after configuring the database connection" or "using the credentials from the previous step." Medical literature builds on earlier definitions: "the aforementioned syndrome" or "as described in the methodology." When you chunk these documents into 256 or 512 token pieces, you sever the connections that hold the meaning together. Each chunk becomes an island, disconnected from the archipelago it belongs to.

Anthropic introduced a technique called contextual retrieval in late 2024, and it addresses this problem directly. The idea is deceptively simple: before you embed each chunk, you prepend it with a short summary of the document-level and section-level context. You use an LLM to generate a one or two sentence description that answers the question, "What document is this chunk from, and what is it about?" Then you concatenate that context summary with the original chunk text, and you embed the combined text. The embedding now captures both the chunk content and its broader context. When a user query comes in, the retrieval system can match not just on the words in the chunk but on the document and section metadata that the context summary provides.

The uplift from this technique is significant. Anthropic reported a 49 percent reduction in retrieval failures on their internal benchmarks when they added contextual retrieval to their chunking pipeline. That number is not universal, it depends heavily on how much your chunks rely on surrounding context, but it gives you a sense of the magnitude. In domains like legal, medical, financial services, and technical documentation, where chunks frequently reference earlier definitions, cross-reference other sections, or use pronouns and shorthand that only make sense in context, the improvement can be even larger. You are effectively giving the embedding model a fighting chance to understand what the chunk is actually saying.

## Generating Context Summaries with an LLM

The implementation is straightforward but requires an additional LLM call at indexing time. For each chunk, you construct a prompt that includes the full document text or at least the surrounding context, and you ask the model to generate a brief summary. The prompt might look like this: "Here is a document. I will give you a chunk from this document. Write a one or two sentence summary that describes what document this chunk is from and what the chunk is about. Do not repeat the chunk text, just provide context." Then you pass in the chunk, the model generates the summary, and you prepend that summary to the chunk before embedding.

The quality of your context summaries depends heavily on prompt design. You want summaries that are concise, informative, and complementary to the chunk content. A good context summary adds information that is not already in the chunk but that helps situate the chunk within the larger document. A bad context summary either repeats what is already in the chunk or provides generic information that does not help with retrieval. For example, if your chunk is "The refund will be processed within 5-7 business days," a good context summary might be "This chunk is from the Customer Returns Policy document, section on refund processing timelines for standard returns." A bad context summary would be "This chunk is about refunds" or "This chunk discusses processing times."

The cost of this operation is non-trivial. If you have ten thousand documents and each document splits into fifty chunks, you are making five hundred thousand LLM calls to generate context summaries. At current API pricing for a small model like Claude Haiku, that might cost you a few hundred dollars in inference costs, plus the latency of making all those calls. This is why contextual retrieval is an indexing-time operation, not a query-time operation. You pay the cost once when you build your vector index, and then you reap the benefits on every query thereafter. But you do need to account for that upfront cost in your pipeline design and budget.

You can optimize costs by batching your LLM calls and using a smaller, faster model for context generation. You do not need frontier-level reasoning to generate document context summaries. A model like Claude Haiku or even a fine-tuned local model can produce adequate summaries at a fraction of the cost. You can also parallelize the generation process across multiple workers to reduce wall-clock time. If you are indexing hundreds of thousands of chunks, you want to spread that work across a cluster so your indexing pipeline completes in hours rather than days.

You also need to think about cache invalidation. If a document changes, you need to regenerate the context summaries for all chunks in that document, re-embed them, and update your vector index. This is more expensive than simply re-embedding the raw chunks, because you are adding an LLM generation step. In a high-churn environment where documents are updated frequently, the cost of maintaining contextual embeddings can add up quickly. You may need to batch updates, run them overnight, or implement incremental indexing strategies to keep costs manageable.

One strategy for high-churn environments is to generate context summaries at multiple levels of granularity. You might generate a document-level summary once per document and reuse it for all chunks in that document. Then you generate section-level summaries once per section and prepend both the document summary and section summary to each chunk. This amortizes the cost of the document-level summary across all chunks, reducing the number of LLM calls you need to make. When a document is updated, you only regenerate the section summaries for the sections that changed, not the entire document.

## The Quality Uplift from Contextual Embeddings

The quality improvement from contextual retrieval shows up in several ways. First, you get better recall on queries that reference document-level concepts. If a user asks, "What is the policy on remote work in the employee handbook?" and the relevant chunk is buried in section 4.2 of a fifty-page handbook, a naive embedding of that chunk might not contain the words "employee handbook" or "remote work policy" if those terms only appear in the document title and section header. But a contextual embedding that includes a summary like "This chunk is from the Employee Handbook, section 4.2 on Remote Work Policy" will match the query much more strongly. The embedding now contains the exact terms the user is searching for, even though they do not appear in the chunk itself.

Second, you reduce false positives from chunks that use generic language. A sentence like "See section three for details" appears in thousands of documents across your corpus. If you embed that sentence in isolation, it will match any query that mentions "section three" or "details," which is not useful. But if you prepend a context summary that says, "This chunk is from the Data Retention Policy, discussing audit requirements," the embedding becomes much more specific. It will only match queries that are actually about data retention and audits, not every query that happens to mention section three.

Third, you improve the quality of cross-references and citations. Many documents in enterprise knowledge bases are highly interconnected. A policy document might reference a procedure document, which references a form, which references a compliance standard. When you chunk these documents naively, the cross-references become orphaned. A chunk that says "as defined in the compliance standard" loses the link to which compliance standard. But a contextual embedding that includes the document title and section context preserves that link. The retrieval system can now follow the chain of references and retrieve the correct supporting documents.

Fourth, you get better handling of pronouns and anaphoric references. Documents are written assuming sequential reading. They use "it," "they," "this," "these" to refer back to entities mentioned earlier. When you chunk a document, these references often land in chunks that do not contain the original entity. A chunk that says "They must submit the form within 30 days" is useless without knowing who "they" refers to. But a context summary that says "This chunk is from the vendor onboarding guide, section on new vendor registration requirements" makes it clear that "they" refers to new vendors.

Fifth, you improve retrieval for queries that contain document structure terms. Users often query using the structure of your documents: "What does the employee handbook say about vacation time?" or "Find the section in the compliance policy about data retention." If your chunks do not contain these structural terms, they will not match these queries. But contextual embeddings that explicitly mention the document name and section heading will match strongly.

## When Contextual Retrieval Is Worth the Cost

Contextual retrieval is not a silver bullet, and it is not always worth the cost. If your documents are mostly short, self-contained pieces like FAQs, blog posts, or standalone articles, the benefit is smaller. Each chunk already contains most of the context it needs, because the document itself is short and focused. Adding a context summary might not change the embedding much. The cost of generating summaries may outweigh the quality gain.

But if your documents are long, structured, and full of cross-references, contextual retrieval can be a game-changer. Legal contracts, technical manuals, regulatory documents, research papers, and enterprise policies all benefit significantly. These documents are written with the assumption that the reader is consuming them linearly, from start to finish, and that they have the context of earlier sections in mind as they read. When you chunk them into isolated pieces, you break that assumption. Contextual retrieval repairs it.

The benefit is especially pronounced in domains where precision matters more than recall. In legal retrieval, getting the wrong clause because it was taken out of context can have serious consequences. In medical retrieval, missing critical context around a dosage or contraindication can harm patients. In financial retrieval, misunderstanding a policy because you did not see the exceptions and qualifications can lead to compliance violations. Contextual retrieval reduces these risks by ensuring that each chunk carries enough context to be interpreted correctly.

You also need to think about the trade-off between context length and chunk length. If you prepend a 50-token context summary to a 512-token chunk, you are increasing the total embedding length by about 10 percent. That means your vector index grows by 10 percent, your embedding costs increase by 10 percent, and your retrieval latency may increase slightly because you are comparing longer vectors. But the quality gain is usually worth it. A 10 percent increase in cost for a 49 percent reduction in retrieval failures is a no-brainer trade-off in most production systems.

In some cases, you might find that the context summary is so valuable that you can actually reduce your chunk size while maintaining or improving retrieval quality. If you were using 512-token chunks to ensure enough context, but you find that 256-token chunks with context summaries perform just as well or better, you have effectively gotten more retrieval precision without sacrificing context. Smaller chunks mean more chunks per document, which means finer-grained retrieval and less noise in each retrieved result.

## Implementing Contextual Retrieval in Your Pipeline

To implement contextual retrieval, you need to modify your chunking and embedding pipeline. Here is the basic workflow. First, you chunk your documents as you normally would, using whatever chunking strategy you have chosen. Second, for each chunk, you construct a prompt that asks an LLM to generate a context summary. You pass in the chunk, the surrounding text, and any document metadata you have. Third, the LLM generates a one or two sentence summary. Fourth, you prepend that summary to the chunk text. Fifth, you embed the combined text using your embedding model. Sixth, you store the embedding in your vector database along with the original chunk text and metadata.

At query time, nothing changes. You embed the user query as normal, perform a similarity search against your vector index, and retrieve the top k chunks. The difference is that the chunks you retrieve are now contextually enriched, so they match the query more accurately. You do not need to modify your retrieval logic at all. The contextual information is baked into the embeddings themselves.

One implementation detail to watch out for: when you return the retrieved chunks to the LLM for generation, you should strip off the context summary and return only the original chunk text. The context summary was useful for embedding and retrieval, but it is redundant and potentially confusing in the final prompt. The LLM does not need to see "This chunk is from the Employee Handbook, section 4.2 on Remote Work Policy" followed by the actual chunk text. It just needs the chunk text. So you need to track where the context summary ends and the original chunk begins, and slice it off before you construct your final prompt.

You can implement this by storing both the contextual embedding and the original chunk text separately in your vector database. Most vector databases let you store arbitrary metadata alongside each vector. You store the contextual text as the vector, but you store the original chunk text as metadata. When you retrieve, you get back the vector and the metadata, and you use the metadata for answer generation while the vector was used for retrieval.

Another implementation consideration is handling very short chunks. If your chunk is only one sentence or a few words, generating a full context summary might make the contextual version much longer than the original. You might end up with a 10-word chunk and a 30-word context summary, which feels backwards. In these cases, you can apply contextual retrieval selectively. Only generate context summaries for chunks above a certain length threshold, say 100 tokens. For very short chunks, embed them as-is, because they are already concise and likely self-contained.

## Variations and Extensions

There are several variations on the basic contextual retrieval technique. One is to generate multiple types of context. Instead of just a document-level summary, you could also generate a section-level summary, a topic-level summary, and a keyword list. You concatenate all of these together before embedding. This gives you richer context but also increases the length of your embeddings and the cost of generation. The marginal benefit diminishes as you add more context, so you need to experiment to find the right balance.

Another variation is to use a smaller, cheaper model for context generation. You do not need a frontier model like Claude Opus to generate a one-sentence summary. A model like Haiku or even a fine-tuned local model can do the job at a fraction of the cost. The summaries do not need to be perfect, they just need to be good enough to improve retrieval. A 90 percent quality summary generated by a cheap model is often better than no summary at all.

A third variation is to generate context at multiple levels of granularity. For example, you could generate a document-level summary once per document, and then reuse that summary for every chunk in the document. Then you generate a section-level summary once per section, and prepend both the document summary and the section summary to each chunk. This reduces the number of LLM calls you need to make, because you are amortizing the document-level summary across all chunks. It is a good way to reduce costs while still getting most of the benefit.

A fourth variation is to use extractive summarization instead of abstractive summarization. Instead of asking an LLM to generate a new summary, you extract key phrases from the document title, section headers, and surrounding paragraphs, and you concatenate them to form a context string. This is much cheaper because you are not making LLM calls, just doing text processing. The quality is lower than abstractive summaries, but it might be good enough for your use case, especially if your documents have clear, descriptive titles and headers.

A fifth variation is to include metadata in the context. If your documents have structured metadata like author, date, category, tags, or department, you can include that metadata in the context summary. For example, "This chunk is from the Q3 2025 Financial Report by the Finance Department, section on revenue projections." This helps retrieval match not just on content but on metadata filters that users might apply in their queries.

## The Long-Term Payoff

The upfront cost of implementing contextual retrieval is real, but the long-term payoff is substantial. You are making a one-time investment at indexing time that improves retrieval quality on every single query for the life of your system. If your RAG system handles ten thousand queries per day, and contextual retrieval reduces your retrieval failure rate by 20 percent, that is two thousand fewer failures per day. Over a year, that is seven hundred thousand fewer failures. If each failure costs you customer trust, support tickets, or incorrect business decisions, the ROI is obvious.

Contextual retrieval is also a forcing function for better document hygiene. When you start generating context summaries, you quickly discover which documents are poorly structured, which sections are ambiguous, and which chunks are genuinely orphaned from their context. This gives you feedback that you can use to improve your source documents. You might realize that certain policies need clearer section headers, or that certain technical manuals need more explicit cross-references. The act of generating context summaries makes the implicit structure of your documents explicit, and that is valuable in its own right.

You also build a valuable asset for future use cases. Once you have contextual summaries for all your chunks, you can use them for purposes beyond retrieval. You can use them to generate document-level summaries by aggregating chunk summaries. You can use them to build topic taxonomies by clustering chunks based on their context. You can use them to identify gaps in your documentation by finding topics that have few or no chunks. The context summaries become a layer of structured metadata that enriches your entire corpus.

In a world where RAG systems are increasingly deployed in high-stakes domains like healthcare, law, and finance, the cost of retrieval failures is only going to grow. A medical RAG system that retrieves the wrong dosage information because it lost context could harm a patient. A legal RAG system that retrieves an outdated clause could expose a company to liability. A financial RAG system that retrieves an incorrect policy could result in a compliance violation. Contextual retrieval is one of the most effective techniques we have for reducing these risks. It is not a magic bullet, but it is a major step forward in making retrieval systems more robust, more reliable, and more aligned with how humans actually write and understand documents.

The lesson from that legal tech company in October 2025 was clear: chunks are not self-contained units of meaning. They are fragments of a larger document, and they only make sense in context. If you embed them in isolation, you lose that context. If you embed them with context, you preserve it. The difference between those two approaches is the difference between a RAG system that works and a RAG system that fails. In production systems where accuracy matters, where decisions are made based on retrieved information, where errors have real costs, contextual retrieval is not optional. It is essential. You cannot afford to treat chunks as isolated fragments. You need to give them the context they need to be understood correctly, and contextual retrieval is how you do that.
