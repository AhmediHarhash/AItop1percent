# 2.10 — Data Quality for RAG: Garbage In, Hallucination Out

In June 2024, a healthcare chatbot gave dangerously incorrect medication dosage advice to hundreds of patients because its RAG system had indexed an outdated clinical guideline from 2019 that contradicted current 2024 protocols. The old guideline recommended dosages that were later found to cause adverse reactions in certain patient populations. When patients asked about the medication, the RAG system confidently retrieved the obsolete guideline and the LLM synthesized responses citing the dangerous old dosages. Three patients were hospitalized before the error was discovered. The subsequent investigation revealed that the document collection contained multiple versions of the same guideline spanning five years, with no clear indication of which was current. The company faced $1.8 million in legal settlements, regulatory fines, and a consent decree requiring comprehensive review of their knowledge base quality controls.

You inherit a foundational truth the moment you build RAG: the quality of your retrieved context directly determines the quality of your generated responses. Garbage in, hallucination out. An LLM given accurate, relevant, current context produces accurate responses. The same LLM given outdated, contradictory, or irrelevant context produces confident nonsense. You cannot fix bad data with clever prompting or powerful models. Quality problems in your knowledge base propagate inexorably through retrieval into generation, emerging as subtle inaccuracies, confident falsehoods, or dangerous errors.

This chapter teaches you how to establish and maintain data quality standards for RAG knowledge bases. You'll learn why document quality matters more than retrieval or generation quality, how outdated or contradictory documents poison your system, how to assess document quality automatically, and how to implement quality gates that prevent bad data from entering your index. By the end, you'll understand that RAG quality begins with data quality, and no amount of engineering sophistication downstream compensates for poor quality documents.

## Why Document Quality Dominates RAG Performance

The machine learning community obsesses over model quality—accuracy percentages, benchmark scores, parameter counts—while treating data quality as an afterthought. This is backwards for RAG systems because retrieval and generation models work exactly as designed. They retrieve what you indexed and generate based on what you retrieved. If what you indexed is wrong, outdated, or contradictory, they faithfully produce wrong, outdated, or contradictory outputs.

A financial services company discovered this brutally in early 2024 when their investment advisory RAG system recommended portfolio allocations based on tax laws from 2018 that had been superseded by 2021 legislation. The RAG system retrieved historical tax planning documents perfectly—the retrieval model worked flawlessly, ranking old documents highly because they used terminology matching user queries. The LLM generated eloquent, well-structured advice based precisely on the retrieved context. Every component performed its function correctly. The system failed because the indexed documents were obsolete.

Document quality issues manifest in three primary failure modes: outdated information causing factually incorrect responses, duplicate or near-duplicate documents causing confused or contradictory responses, and contradictory documents causing hallucinated synthesized responses that reflect no actual source. Each failure mode undermines user trust differently, but all stem from quality problems in the underlying knowledge base.

Outdated documents are insidious because they were once correct. A pharmaceutical company indexed drug interaction databases from 2020 through 2024. When doctors queried about drug combinations, RAG sometimes retrieved 2020 entries instead of current 2024 entries. The old entries weren't wrong when published—they reflected medical knowledge at that time. But medical understanding evolves. Drug interactions discovered in 2022 research weren't mentioned in 2020 documents. Retrieval models have no intrinsic understanding of temporal validity, so they rank old and new documents equally based on semantic similarity alone.

Duplicate documents cause confusion because retrieval returns multiple chunks saying the same thing in slightly different ways. A customer support RAG indexed the same FAQ content from their website, help center, email templates, and training materials. When users asked questions, retrieval returned six chunks with minor variations of the same answer, wasting context window space and confusing the LLM about which version was authoritative. The LLM sometimes blended different wordings, producing responses that matched no actual source document.

Contradictory documents force LLMs into impossible situations. A logistics company's RAG indexed policy documents from multiple departments that gave conflicting guidance on the same topics. The operations manual said international shipments required documentation within 24 hours. The compliance manual said 48 hours. The legal handbook said 12 hours for certain countries. When asked about documentation deadlines, retrieval returned chunks citing all three timeframes. The LLM generated responses attempting to synthesize contradictory requirements, sometimes inventing compromise timeframes like 36 hours that appeared in no source document—a classic hallucination caused by contradictory context.

Document quality problems amplify through retrieval and generation. A single bad document might account for one percent of your index but appear in 20 percent of retrieved results if its content overlaps common query patterns. An insurance company had one obsolete policy document containing terminology that matched frequent customer queries. That single document poisoned 18 percent of RAG responses with outdated policy details, disproportionately impacting system quality relative to its tiny fraction of the overall collection.

The cruel reality is that users trust RAG systems precisely because they cite sources. When your system says "According to our policy document, the deadline is 48 hours" users assume you retrieved accurate, current information. They don't question whether the policy document is obsolete or contradicted by other policies. Citation creates false confidence that amplifies quality failures rather than mitigating them.

## Assessing Document Quality: Properties That Matter for RAG

Building quality controls requires defining what quality means for RAG documents. Quality isn't abstract perfection—it's specific, measurable properties that affect retrieval and generation reliability. Freshness, accuracy, completeness, consistency, clarity, and authority are the dimensions that matter most.

Freshness measures how recently a document was created or updated and whether it reflects current information. Temporal metadata like publication dates, modification timestamps, and effective dates provide explicit freshness signals. For some content, freshness matters critically—tax laws, product specifications, pricing, contact information, regulatory requirements all have clear temporal validity. For other content, freshness matters less—historical analysis, archived decisions, case studies maintain value indefinitely.

A retail company scored documents by age, downweighting anything more than six months old for product information but maintaining full weight for brand history and company background regardless of age. This required classifying documents by content type and applying different freshness policies. Product catalogs received aggressive age-based downweighting. Corporate policies were manually reviewed for currency but not automatically downweighted by age. Tagging documents with content categories enabled appropriate freshness handling.

Accuracy is difficult to assess automatically but critical for domains where errors have consequences. Medical information, legal advice, financial guidance, safety procedures all demand high accuracy. Automated accuracy assessment might use external reference sources for fact-checking, consistency checks across documents, or required review workflows before publication. A healthcare documentation system required physician review and sign-off before clinical guidelines entered the index, creating human-validated accuracy gates.

Completeness measures whether documents contain sufficient information to support useful RAG responses. Fragment documents, partial exports, summaries without details, or documents with extensive redaction fail completeness checks. A legal research company rejected documents under 500 words as likely incomplete, documents with more than 30 percent redacted text as too fragmentary, and documents missing required sections like conclusions or recommendations as structurally incomplete.

Consistency examines whether documents contradict each other or internal facts. Automated consistency checking might compare overlapping content across documents, verify that numbers and dates align across related documents, or flag cases where documents on the same topic provide different answers. A financial services firm built consistency checks that compared product specifications across marketing documents, technical documentation, and legal disclosures, alerting when interest rates, fees, or terms mismatched across sources.

Clarity assesses whether documents are written comprehensibly for their intended audience. Technical jargon without explanation, excessively complex sentence structure, ambiguous pronoun references, or missing context reduce clarity. Automated clarity metrics include readability scores like Flesch-Kincaid, sentence length distributions, and technical term density. A customer support system required documentation to maintain Flesch-Kincaid grade level below 10, rejecting documents with excessive complexity that would confuse LLM context understanding.

Authority identifies document provenance and credibility. Official policy documents carry more authority than draft proposals. Published research papers carry more authority than blog posts. Vendor documentation carries more authority than third-party tutorials. Tracking document sources, publishers, approval status, and review history establishes authority hierarchies. A pharmaceutical company tagged documents with authority levels: regulatory submissions and clinical trial results were highest authority, internal SOPs medium authority, external research papers low authority. Retrieval boosted higher-authority documents when relevance scores were close.

## Automated Quality Checks: Gates Before Indexing

Implementing quality controls means building automated checks that assess documents before they enter your index, rejecting or flagging those failing quality standards. Quality gates operate at ingestion time, examining parsed documents after extraction but before chunking and embedding. This prevents bad data from polluting your index rather than trying to filter it during retrieval.

Age-based quality gates reject or flag documents older than defined thresholds based on content type. Define maximum ages for different document categories: product information might have a one-year maximum, technical specifications six months, pricing 90 days. During ingestion, check document modification dates against thresholds and reject expired content. A logistics company implemented 18-month expiration for operational procedures, rejecting any document that hadn't been reviewed and updated within that window, forcing content owners to explicitly recertify older procedures before reindexing.

Completeness gates check basic document properties indicating likely completeness. Minimum word counts, required sections, minimum paragraph counts, absence of truncation markers like "continued on next page" without continuation. A manufacturing company rejected documents under 200 words, documents missing safety warnings sections, and documents with incomplete tables where row counts didn't match specified totals, preventing fragmentary content from entering the index.

Duplication gates detect when documents are identical or near-identical to already-indexed content. Compute document fingerprints using techniques like MinHash or SimHash and compare incoming documents against existing fingerprints. Reject exact duplicates outright. Flag near-duplicates for human review to determine if both versions should be indexed or only one. An insurance company built fingerprint-based deduplication that rejected documents with more than 90 percent overlap with existing indexed documents, preventing redundant policy copies from cluttering the index.

Contradiction detection identifies documents that conflict with existing indexed content on factual claims. This is technically challenging because it requires understanding document semantics, but heuristics help. Flag documents that discuss the same topics as existing documents but cite different numbers, dates, or categorical answers. A financial services firm built basic contradiction detection that compared numerical values in documents about the same products, flagging cases where interest rates, fees, or terms differed by more than trivial amounts, routing those documents for human review before indexing.

Format validation confirms that documents meet structural expectations for their type. XML documents validate against schemas, JSON against schemas, HTML against expected tags, PDF against readability metrics. A healthcare system validated clinical guideline documents against a schema requiring specific sections—indications, dosing, contraindications, references—rejecting documents missing required sections as likely incomplete or corrupted.

Content validation applies domain-specific rules checking for required elements. Medical documents must mention standard terminology like dosages and side effects. Legal documents must include dates and jurisdictions. Financial documents must include disclaimers. A pharmaceutical company validated drug information documents for required sections, terminology standards, and citation formats, rejecting documents that failed validation as potentially inaccurate or incomplete.

Quality scoring assigns numerical quality metrics to documents based on multiple factors, allowing rejection based on threshold policies or quality-weighted retrieval. Combine freshness, completeness, clarity, and authority into composite scores. Set minimum quality thresholds for indexing—documents scoring below thresholds are rejected or routed for human review. A legal research company computed quality scores from document age, citation count, publisher authority, and completeness metrics, indexing only documents scoring above 70 out of 100 and flagging documents between 50 and 70 for editorial review.

## Handling Quality Failures: Rejection, Quarantine, or Correction

Quality gates generate failures that require handling policies. When documents fail quality checks, you must decide whether to reject them entirely, quarantine them for human review, attempt automated correction, or index them with reduced weight or special flags. Each approach has operational implications for content coverage, manual workload, and system reliability.

Automatic rejection prevents bad data from entering the index but creates gaps in coverage. If 10 percent of documents fail quality checks and you reject them all, your system lacks 10 percent of source content. Users asking about topics covered only in rejected documents receive no answers or only partial answers from related documents. A manufacturing company rejected all documents failing completeness checks, creating knowledge gaps where procedures were documented only in incomplete fragments that users couldn't access.

Quarantine routing sends failed documents to human review queues where content specialists decide whether to accept, correct, or permanently reject them. This maintains quality standards while enabling case-by-case decisions for edge cases that automated checks handle poorly. A healthcare system routed documents failing consistency checks to clinical reviewers who determined whether contradictions reflected legitimate clinical judgment differences or errors requiring resolution. Quarantine systems require operational processes, review tools, and dedicated staff—they trade automation for judgment.

Automated correction attempts to fix quality issues programmatically. Remove duplicate content, update obsolete dates, normalize formatting, extract only relevant sections. Correction works for mechanical issues like formatting problems but fails for semantic issues like factual errors or contradictions. An insurance company automatically corrected formatting inconsistencies and encoding errors in policy documents but routed factual contradictions to human review, recognizing the limits of automated correction.

Degraded indexing accepts failed documents but indexes them with reduced weight, lower priority, or warning flags. Retrieval can access this content but ranks it lower than high-quality documents. Users receive answers from lower-quality sources only when no better options exist. A customer support RAG indexed all content regardless of quality scores but used quality scores as retrieval ranking factors, ensuring high-quality articles appeared before lower-quality ones. This maximized coverage while prioritizing quality.

Version-based handling compares multiple versions of the same document and indexes only the most recent or highest quality version. When ingesting documents, detect versions through filename patterns, metadata, or content similarity. Keep the newest or explicitly mark preferred versions and reject older versions. A pharmaceutical company detected document versions through filename patterns like "protocol_v1.pdf" and "protocol_v2.pdf," indexing only the highest version number and rejecting older versions as obsolete.

Temporal validity windows index documents with explicit effective date ranges, making them retrievable only for queries about those time periods. Historical documents remain accessible for historical questions but don't contaminate current information retrieval. A financial services firm tagged tax guidance documents with tax year metadata, ensuring queries about "current tax treatment" retrieved only current-year documents while queries about specific past years retrieved appropriate historical documents.

Quality feedback loops use retrieval and generation performance to identify quality problems. Track which documents appear in retrieved contexts for queries with poor user satisfaction ratings. Track documents that correlate with hallucination detection flags. Analyze these patterns to identify problematic documents that passed initial quality gates but cause downstream issues. An e-commerce company monitored customer satisfaction ratings for product recommendation responses, investigating documents that appeared in contexts for low-rated responses, discovering outdated product descriptions that initial quality checks missed.

## Maintaining Quality Over Time: Continuous Validation

Data quality isn't a one-time gate at ingestion—it's continuous validation as documents age, source systems change, and domain knowledge evolves. Documents that were high quality at indexing time become outdated. New documents contradict old ones. External references become invalid. Quality maintenance requires periodic revalidation of indexed content against current quality standards.

Scheduled revalidation runs quality checks against already-indexed documents on regular cycles. Daily, weekly, or monthly jobs examine document ages, check external references, detect contradictions, and flag documents failing current quality standards. A healthcare system revalidated clinical guidelines monthly, checking that documents were within their valid date ranges, external citations remained accessible, and content remained consistent with newly indexed guidelines. Documents failing revalidation were flagged for editorial review.

Event-triggered revalidation responds to external signals indicating quality changes. When authoritative sources publish updates, revalidate related documents. When contradictions are detected, revalidate all documents on the same topic. When users report errors, revalidate cited documents. A financial services firm subscribed to regulatory update notifications, automatically revalidating all documents discussing affected regulations when updates were announced, catching obsolete guidance before users encountered it.

Quality metric monitoring tracks aggregate quality scores across your collection over time. Average document age, percentage of documents missing required metadata, duplicate detection rates, contradiction frequency. Degrading metrics indicate quality decay requiring intervention. A logistics company monitored average age of operational procedures, alerting when average age exceeded 12 months, indicating that content updates weren't keeping pace with operational changes.

User feedback integration captures quality signals from user interactions. Thumbs down ratings, correction submissions, retrieval relevance feedback, and explicit error reports all indicate potential quality issues. Aggregate feedback by document to identify consistently problematic content. A customer support system tracked satisfaction ratings for responses, tracing low-rated responses back to source documents, prioritizing editorial review for documents that appeared in multiple low-rated responses.

Comparative quality audits sample documents randomly and evaluate them against detailed quality rubrics through human review. This provides ground truth quality assessment that validates automated quality metrics and catches issues that automated checks miss. A pharmaceutical company conducted quarterly audits reviewing 500 randomly selected documents against comprehensive quality checklists, comparing human quality assessments against automated quality scores to calibrate scoring algorithms.

Deprecation processes manage content lifecycle, explicitly retiring obsolete documents rather than letting them linger. When authoritative sources publish superseding versions, deprecate old versions. When content owners confirm documents are obsolete, mark them as deprecated and remove from active indexes. Maintain deprecated documents in archives for historical reference but exclude them from current retrieval. An insurance company implemented formal deprecation workflows where policy updates automatically deprecated previous policy versions, preventing outdated policies from appearing in current policy queries.

## Production Reality: Quality as Foundational Infrastructure

That healthcare chatbot didn't fail because of sophisticated technical problems—it failed because basic data quality processes didn't exist. Nobody tracked document versions. Nobody enforced deprecation policies. Nobody validated temporal currency. The engineering team built sophisticated retrieval and generation capabilities on top of fundamentally compromised data, and quality problems propagated straight through to patient harm.

You build reliable RAG by treating data quality as foundational infrastructure, not an afterthought. Quality gates prevent bad data from entering. Quality monitoring detects degradation over time. Quality remediation processes respond to identified issues. Quality metrics measure collection health. This operational discipline matters more than retrieval algorithms or generation techniques because quality problems can't be fixed downstream—they must be prevented or caught early.

Start with simple quality checks and expand iteratively based on observed failures. Begin with age-based filtering to catch obviously obsolete content. Add deduplication to prevent redundant content. Implement completeness checks to catch fragmentary documents. Build contradiction detection as you discover conflicting content. A consulting firm started with manual quality review for all documents, automated age-based filtering after discovering obsolete content issues, added deduplication after retrieval returned redundant chunks, and built custom consistency checks after discovering contradictory policy documents in production.

Make quality visible through metrics and dashboards that track collection health. Display average document age, quality score distributions, percentage of documents failing various checks, quarantine queue depth, revalidation coverage. Make quality a team responsibility with regular reviews of quality metrics and incidents caused by quality issues. A pharmaceutical company held monthly quality reviews examining quality metric trends, recent quality incidents, and quality improvement initiatives, treating data quality with the same rigor as application reliability.

The companies that succeed with production RAG obsess over data quality because they understand that retrieval and generation are only as good as what they retrieve. They implement comprehensive quality gates, maintain quality through continuous validation, and respond rapidly to quality issues. The companies that fail build sophisticated systems on garbage data and wonder why their RAG hallucinates confidently. The difference is operational discipline applied to the unglamorous work of curating and maintaining knowledge base quality.

Your RAG system's reputation depends entirely on answer quality. Users don't care about elegant architectures or sophisticated algorithms. They care whether answers are correct, current, and trustworthy. That depends completely on whether your underlying documents are correct, current, and trustworthy. No amount of engineering brilliance compensates for bad data. Quality begins at the source, flows through every component, and manifests in every user interaction. Get quality right and your system becomes indispensable. Get it wrong and users learn to distrust every response, no matter how confident or well-cited.
