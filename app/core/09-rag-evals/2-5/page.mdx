# 2.5 â€” Metadata Extraction and Enrichment for Chunks

In November 2024, a legal discovery platform serving litigation teams discovered that their RAG system was surfacing opposing counsel's privileged documents in search results for their own attorneys. Both sides' documents were indexed in the same vector database, and the retrieval logic had no mechanism to filter by client or case. The system relied entirely on semantic similarity, which worked fine for demo queries but catastrophically failed when production queries matched documents that users were not authorized to access. The breach was discovered during a deposition when an attorney cited a document that should have been invisible to them. The case was thrown out, the client sued the platform for legal malpractice, and the company settled for seven figures. The root cause was not a security vulnerability in the traditional sense. It was the complete absence of metadata for access control, combined with retrieval logic that treated all chunks as equally accessible.

You are extracting metadata in 2026, and this is not optional infrastructure for advanced use cases. It is fundamental to production RAG systems. Metadata is the structured information about a chunk that enables filtering, access control, provenance tracking, and retrieval refinement. Without metadata, your retrieval system is a semantic search engine with no understanding of who should see what, where information came from, or whether it is current. With metadata, you can filter retrieval to relevant sources, enforce access policies, and provide users with the context they need to trust retrieved information. This chapter walks through what metadata to extract, how to extract it, how to design metadata schemas, and how to use metadata to improve retrieval quality.

## What Metadata to Extract: The Essential Fields

The metadata you need depends on your use case, but certain fields are universally valuable. Source metadata tells you where a chunk came from: the original document filename, URL, database record ID, or file path. This is critical for provenance and debugging. When a chunk produces a wrong answer, you need to trace it back to the source document to understand why. When users question the accuracy of retrieved information, you need to cite sources.

Temporal metadata tells you when information was created or last modified: document creation date, modification date, publication date, or ingestion timestamp. This is essential for filtering stale information and prioritizing recent content. A query about current tax regulations should not retrieve chunks from documents published five years ago unless no recent information is available. A query about historical events should not exclude older documents. Temporal metadata enables time-aware retrieval.

Authorship metadata tells you who created the information: author name, organization, department, or role. This is valuable for assessing credibility and filtering by expertise. A query about network security policy should prioritize documents authored by the security team over documents authored by marketing. A query about customer complaints should surface documents from customer support.

Document type metadata categorizes the kind of document: contract, invoice, email, research paper, knowledge base article, or code file. Different document types have different structures and information densities, and retrieval strategies may vary by type. A contract requires careful attention to legal language and conditions. A knowledge base article is written for retrieval and can be trusted more than informal email discussions.

Section or topic metadata describes what part of a document a chunk came from or what subject it addresses: chapter title, section heading, topic label, or category. This enables filtering by topic and provides context about where the chunk fits in the larger document. A chunk from the "Security Considerations" section of a technical specification should be treated differently from a chunk from the "Getting Started" section.

Access control metadata defines who can see the chunk: user IDs, group memberships, security classifications, or permission flags. This is non-negotiable for multi-tenant systems, enterprise applications, or any scenario where not all users should have access to all information. The legal discovery platform failed because they had no access control metadata. Every production RAG system that handles sensitive information must implement this from the start.

Custom domain-specific metadata captures fields unique to your use case: case numbers for legal systems, patient IDs for healthcare, product SKUs for e-commerce, or project codes for engineering documentation. These fields enable domain-specific filtering and retrieval patterns that generic metadata cannot support.

## Automatic vs Manual Metadata Extraction

Metadata can be extracted automatically from document properties and content, or it can be provided manually by users or systems that produce documents. Automatic extraction is scalable but may miss important fields or extract incorrect values. Manual metadata is accurate but does not scale and may be incomplete if users do not populate all fields.

Automatic extraction from document properties is the easiest approach. Most file formats embed metadata in headers or properties: PDFs have metadata dictionaries, Office documents have properties accessible via libraries, emails have headers, and HTML pages have meta tags. Extracting this metadata requires format-specific parsers but is straightforward. The challenge is that embedded metadata is often incomplete, incorrect, or missing entirely.

PDF metadata fields like Title, Author, Subject, and Keywords are frequently unpopulated or contain default values. A PDF created by converting a Word document may have "Microsoft Word" as the Creator and no meaningful Title. Timestamps in PDFs often reflect when the file was copied or converted rather than when the content was created. Relying solely on embedded metadata will leave many chunks without critical fields.

Filename and directory path parsing provides additional metadata when files are organized in structured directories. If documents are stored in a folder hierarchy like "ClientName/Year/DocumentType", you can parse the path to extract client, year, and document type. Filename patterns like "Contract-ClientX-2025-01-15.pdf" can be parsed to extract document type, client, and date. This requires reliable naming conventions, which is common in regulated industries but rare in ad-hoc document collections.

Content-based extraction uses natural language processing or regular expressions to extract metadata from document text. You can detect document type by looking for keywords: documents containing "plaintiff" and "defendant" are likely legal filings, documents containing "abstract" and "methodology" are likely research papers. You can extract dates by searching for date patterns in text. You can extract entities like names, organizations, and locations using named entity recognition.

Content-based extraction is more robust than relying on embedded metadata but is also more error-prone. NER models make mistakes. Date extraction may find dates that are references to historical events rather than document creation dates. Document type heuristics may misclassify edge cases. You need validation logic to check that extracted metadata is plausible and consistent with other fields.

Manual metadata is provided by users or systems at document creation or upload time. When a user uploads a document through a web interface, they can fill in fields like document type, topic, and access permissions. When documents are created by an application, the application can attach metadata like user ID, project code, and creation timestamp. Manual metadata is accurate when users provide it, but users often skip optional fields or enter incorrect values.

The production pattern is to combine automatic and manual metadata. Extract as much as possible automatically, then prompt users to fill in missing critical fields. Validate that manual metadata is consistent with automatic metadata: if a user claims a document was created in 2025 but the file timestamp is 2020, flag the inconsistency. Provide defaults for common fields to reduce user burden: default the author to the uploading user, default the creation date to the current date, default the document type based on file extension.

## Metadata Schemas and Normalization

Metadata must conform to a schema that defines field names, types, allowed values, and validation rules. Without a schema, metadata becomes a collection of inconsistent, freeform key-value pairs that are difficult to query and filter. With a schema, you can enforce consistency, validate values, and build reliable retrieval logic.

A metadata schema defines fields and their types: strings for names and titles, dates for timestamps, enums for document types, lists for tags or categories, and booleans for flags. Field types enable validation: a date field must contain a valid date, an enum field must contain one of the predefined values, a list field must contain a valid array of strings.

Schemas also define required versus optional fields. Required fields must be populated for every chunk: source ID and ingestion timestamp are typically required. Optional fields may be missing for some chunks: author may be unknown for older documents, section heading may not exist for unstructured text. Retrieval logic must handle missing optional fields gracefully, either by treating them as null or by providing default values.

Normalization is critical for fields with freeform or variable input. Dates should be normalized to a standard format like ISO 8601 to enable range queries and sorting. Names should be normalized to handle variations: "John Smith", "J. Smith", and "Smith, John" should all map to a canonical form. Document types should be mapped to a controlled vocabulary: "agreement", "contract", and "legal doc" all map to "contract". Normalization requires lookup tables, fuzzy matching, or LLM-based canonicalization.

The production pattern is to define a core schema with required fields that every chunk must have, and allow custom fields for domain-specific metadata. Store the schema in code or configuration, validate all metadata against the schema during ingestion, and reject or flag chunks with invalid metadata. Update the schema carefully: adding new fields is safe, removing fields or changing field types requires reprocessing the corpus.

## Metadata-Enabled Filtered Retrieval

Metadata transforms retrieval from pure semantic similarity to filtered, contextual search. Instead of retrieving the top-K most similar chunks regardless of source, date, or author, you retrieve the top-K most similar chunks that match metadata filters. This dramatically improves retrieval precision and ensures users only see relevant, authorized information.

Filtered retrieval is implemented by combining vector search with metadata filtering. Most vector databases support metadata filters: when you query for similar chunks, you also specify conditions on metadata fields. For example, "retrieve the top 5 chunks similar to this query where document_type equals contract and creation_date is after 2024-01-01 and access_group contains user_group_X". The database returns only chunks that match both the similarity threshold and the metadata conditions.

Filtering before retrieval is more efficient than filtering after retrieval. If you retrieve 100 chunks based on similarity and then filter to find the 5 that match metadata conditions, you waste computation on the 95 chunks that will be discarded. If you filter during retrieval, the database only computes similarity for chunks that match metadata conditions, reducing latency and cost.

Metadata filtering enables use cases that are impossible with pure semantic search. Access control requires filtering by user permissions: retrieve only chunks where the access_control_list includes the current user. Multi-tenancy requires filtering by tenant ID: retrieve only chunks where the tenant field matches the user's organization. Time-aware retrieval requires filtering by date: retrieve only chunks where the publication date is within the last year. Domain-specific retrieval requires filtering by category or topic: retrieve only chunks where the category is network_security.

The challenge is that metadata filtering reduces the pool of candidate chunks, which may hurt retrieval quality if the filters are too restrictive. If you filter to only chunks from the last month, and the relevant information is in a document from two months ago, you will miss it. The solution is to make filters configurable and allow users to relax or remove filters when retrieval returns no results. Start with strict filters, and progressively relax them if retrieval fails.

## Metadata Enrichment with LLMs

For documents that lack metadata or have incomplete metadata, you can use LLMs to enrich chunks with inferred metadata. Pass the chunk text to an LLM with a prompt asking for document type, topic, key entities, summary, or other fields. The LLM analyzes the content and returns structured metadata in a specified format.

For example, you can prompt an LLM with: "Analyze this text and return a JSON object with the following fields: document_type, primary_topic, mentioned_entities, and a one-sentence summary." The LLM reads the chunk and generates metadata like "document_type: technical_specification, primary_topic: authentication, mentioned_entities: OAuth, JWT, API, summary: Describes OAuth-based authentication for the API using JWT tokens."

LLM-based enrichment is powerful but expensive and slow. Enriching every chunk in a large corpus with LLM calls is prohibitively costly. The production pattern is to use LLM enrichment selectively: for high-value documents, for documents where automatic extraction failed, or for documents in specialized domains where rule-based extraction does not work. Batch LLM enrichment jobs during off-peak hours, cache results, and avoid re-enriching chunks unless the content changes.

You can also use LLM enrichment to validate and correct automatically extracted metadata. After extracting a document type with rule-based heuristics, pass the result to an LLM and ask: "This document was classified as a contract. Does this classification appear correct based on the content?" The LLM can catch misclassifications and suggest corrections. This hybrid approach leverages the speed of rule-based extraction and the accuracy of LLM analysis.

Another use case for LLM enrichment is generating descriptive metadata that is hard to extract with rules: sentiment, formality, complexity level, or target audience. A chunk from a research paper may be marked as high_complexity, target_audience: researchers, formality: academic. A chunk from a FAQ may be marked as low_complexity, target_audience: general_users, formality: casual. This metadata enables retrieval strategies that match chunks to user expertise or query complexity.

## Metadata Quality Control and Validation

Metadata is only valuable if it is accurate. Incorrect metadata is worse than no metadata because it creates misleading filters and breaks retrieval. A document mislabeled with the wrong access group may be shown to unauthorized users. A document mislabeled with the wrong date may be excluded from time-filtered queries. A document mislabeled with the wrong topic may never be retrieved for relevant queries.

Quality control starts with validation during ingestion. Check that required fields are populated, that field values conform to schema types, and that values are plausible. If a creation date is in the future, flag it. If an author field is empty when the document source requires an author, reject or flag it. If a document type is not in the controlled vocabulary, reject or map it to a default type.

You also need consistency checks across fields. If a document was created in 2020 but the modification date is 2018, flag the inconsistency. If a document is labeled as a contract but the filename has no contract-related keywords and the content does not mention clauses or parties, flag it for review. Cross-field validation catches errors that single-field validation misses.

Sampling and manual review are essential for validating metadata quality at scale. Periodically sample chunks, review their metadata, and compare against the source documents. Measure the accuracy of automatically extracted fields: what percentage of document type classifications are correct? What percentage of dates are accurate? What percentage of author names are correct? Use these measurements to tune extraction logic and identify systematic errors.

User feedback is another source of quality signals. When users report incorrect search results or missing information, investigate whether metadata errors contributed to the problem. If users consistently correct or ignore metadata fields in the UI, it signals that those fields are unreliable. Track which metadata fields drive user behavior and prioritize improving the accuracy of those fields.

The production pattern is to treat metadata as data that requires the same rigor as your core application data. Define schemas, validate on ingestion, monitor quality metrics, and iterate on extraction logic when accuracy is insufficient. Do not assume that automatic extraction is good enough. Validate it empirically and fix errors proactively.

## Metadata as the Foundation of Production-Grade Retrieval

The legal discovery platform that exposed privileged documents failed because they treated metadata as optional. They built a RAG system that worked beautifully for semantic search but had no mechanism to enforce access control. When the system went to production, the absence of metadata became a catastrophic vulnerability. The fix was not adding better embeddings or tuning retrieval parameters. The fix was implementing access control metadata, filtering retrieval by user permissions, and auditing that every chunk had correct access metadata before indexing.

Metadata is the difference between a prototype that works for demos and a production system that handles real-world complexity. Prototypes can ignore access control, source provenance, and temporal filtering because demo queries are cherry-picked and users are trusted. Production systems cannot make these assumptions. Users need to see only authorized information, trace retrieved information to sources, filter by date or topic, and trust that results are relevant to their context.

When you build your ingestion pipeline, invest in metadata extraction from the start. Define schemas, extract metadata from as many sources as possible, validate quality, and design retrieval logic that uses metadata for filtering and ranking. Do not defer metadata to a later phase. By the time you need it, you may have already indexed millions of chunks without metadata, and reprocessing to add it will be disruptive and expensive.

Your metadata is the scaffolding that makes retrieval useful. Embeddings capture semantic similarity, but metadata captures context, constraints, and structure. The combination of semantic similarity and metadata filtering is what makes RAG systems work in production. Get metadata right, and your retrieval system is robust, secure, and useful. Get it wrong, and you have a semantic search engine that cannot handle the real-world requirements of access control, provenance, and context-aware retrieval.
