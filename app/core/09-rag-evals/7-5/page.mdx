# 7.5 â€” Edge Case Testing: Empty Retrievals, Contradictions, Stale Data

In December 2025, a legal research platform shipped a RAG-powered case law assistant that worked flawlessly for three months until a partner law firm asked about a niche procedural rule that had no relevant case law in the system. Instead of saying "no information available," the assistant hallucinated a confident answer citing cases that did not exist. The law firm relied on the answer, filed a brief based on it, and faced sanctions for citing nonexistent precedent. The lawsuit settlement cost the startup 1.2 million dollars and their Series B valuation.

The engineering team had tested common queries extensively. They verified the system retrieved relevant cases and cited them correctly. They measured precision, recall, and citation accuracy on a test set of 500 queries. All metrics were excellent. But they never tested what happened when retrieval returned nothing. They never tested what happened when retrieved cases contradicted each other. They never tested what happened when the only relevant case was from a jurisdiction that had since changed its laws.

Edge cases are not rare corner scenarios you can ignore. In production RAG, edge cases are routine. Ten percent of user queries have no relevant documents. Five percent retrieve contradictory information. Three percent retrieve information that is outdated but still in the index. If you do not test edge cases, you ship a system that fails on 15 to 20 percent of real queries. Testing edge cases is not optional. It is testing the failure modes that destroy user trust.

## Empty Retrievals: When the Knowledge Base Has No Answer

The naive RAG implementation assumes retrieval always succeeds. It requests top-k documents, receives k documents, and passes them to the generator. But what happens when no documents are relevant? The vector search still returns k results because you asked for k. Those results have low similarity scores, but they are the closest matches in the index. They are noise, not signal.

If you pass noise to the generator and ask for an answer, the generator will produce an answer. Language models are trained to be helpful. They do not say "I refuse to answer" when given poor context. They blend the irrelevant context with parametric knowledge and generate plausible-sounding text. The result is hallucination: an answer that sounds authoritative but has no grounding in your knowledge base.

Testing empty retrievals means deliberately issuing queries you know have no relevant documents. You query about topics not covered in your corpus. You query using terminology that does not appear in any indexed document. You query about events that occurred after your corpus was last updated. You verify the system handles these gracefully.

The correct behavior is to return "I do not have information about this topic" or "No relevant documents were found." The system must recognize that retrieval failed and refuse to generate an answer. This requires scoring retrieved documents and applying a threshold. If the best match has a similarity score below the threshold, you do not pass it to the generator. You return a no-answer response instead.

Testing threshold tuning is part of edge case testing. You vary the similarity threshold and measure the tradeoff. A high threshold means fewer hallucinations but more no-answer responses, even when marginally relevant documents exist. A low threshold means more answers but higher risk of hallucination on irrelevant context. You test different thresholds on a labeled dataset and choose the one that balances precision and recall according to your risk tolerance.

Some systems use the language model to assess retrieval quality. After retrieving documents, you ask the model: "Do these documents contain information relevant to the query?" If the model says no, you skip generation and return no-answer. Testing this means verifying the model correctly identifies irrelevant retrievals. You create test cases where documents are superficially similar but semantically irrelevant and verify the model rejects them.

Another edge case is partial retrieval. You request k equals 10 documents. The index only has 3 relevant documents. The vector search returns those 3 plus 7 irrelevant ones. Do you use all 10? Do you filter to only the 3 relevant ones? Do you warn the user that information is incomplete? You test this scenario and verify your filtering logic works correctly.

Empty retrieval testing also covers infrastructure failures. What if the vector database is down? What if the API call times out? What if the network connection fails? Naive implementations crash or hang. Robust implementations catch errors, log them, and return a graceful error message to the user. You test these failures by mocking database errors and verifying your error handling works.

## Contradictory Information: When Retrieved Documents Disagree

Real document corpora contain contradictions. Policy documents are updated, but old versions remain indexed. Different authors express different opinions. Regional variations exist. Case law evolves, creating contradictory precedents. A user query might retrieve five documents where three say one thing and two say the opposite.

The naive generator picks one perspective and presents it as fact. Or it blends both perspectives into a confused answer that does not acknowledge the contradiction. Or it alternates between contradictory claims in different sentences, creating an incoherent response. None of these behaviors are acceptable in high-stakes domains.

Testing contradictory information means creating test cases where retrieved documents disagree. You index two versions of a policy document: one saying refunds are allowed within 30 days, another saying 14 days. You query about refund policy. You verify the system acknowledges the contradiction rather than presenting one answer as definitive.

The correct behavior depends on your domain. In some cases, you should present both perspectives: "Document A says 30 days, Document B says 14 days. The most recent document is B." In other cases, you should refuse to answer: "The retrieved documents contain contradictory information. I cannot provide a definitive answer." In yet other cases, you should apply a tiebreaker: prioritize the most recent document, the highest authority source, or the majority opinion.

You test that your system implements the behavior you specified. You create contradictory test fixtures. You verify the output acknowledges contradiction. You verify it applies your tiebreaker rules correctly. You verify it cites all relevant sources, not just the ones that support the chosen answer.

Some contradictions are subtle. Two documents agree on facts but disagree on interpretation. Two documents use different definitions for the same term. Two documents cite different versions of a regulation. Testing requires creating nuanced test cases that capture these subtleties and verifying the system handles them appropriately.

You also test contradiction detection itself. How does the system know documents contradict each other? Some systems use keyword matching: if one document says "allowed" and another says "prohibited," flag contradiction. Others use semantic analysis: embed key claims and measure similarity. You test your detection mechanism by creating contradictory documents with varied phrasing and verifying they are flagged.

False positives are a concern. Two documents might use opposing keywords without actually contradicting. "Refunds are not allowed for digital products" and "Refunds are allowed for physical products" are not contradictory; they cover different cases. You test that your contradiction detection does not flag these as conflicting.

## Stale Data: When Information Is Outdated But Still Indexed

Document corpora age. Policies change. Products are discontinued. Regulations are updated. Team members leave. If you do not actively remove or flag outdated content, your RAG system will confidently cite information that is no longer true.

Testing stale data means creating scenarios where old information is indexed alongside new information. You index a 2024 pricing document saying "Basic plan: 50 dollars per month" and a 2026 pricing document saying "Basic plan: 75 dollars per month." You query about pricing. You verify the system returns the 2026 price, not the 2024 price, and ideally acknowledges that prices changed.

The simplest solution is to use recency as a ranking signal. More recent documents rank higher. You test this by verifying that when two documents are equally relevant, the newer one is retrieved and cited. You create test cases with old and new versions of the same information and verify the system prefers new.

More sophisticated solutions use explicit versioning. Documents are tagged with version numbers or effective dates. Retrieval filters to the latest version. You test this by indexing multiple versions, querying, and verifying only the latest version is retrieved. You test edge cases: what if the latest version does not mention the topic the user asked about, but an older version does? Do you return the old information with a staleness warning, or return no answer?

Another approach is expiration metadata. Documents are tagged with expiration dates. Expired documents are not retrieved. You test this by indexing documents with past expiration dates and verifying they are excluded from results. You test edge cases: what if all documents on a topic are expired? Do you return the most recently expired document with a warning, or no answer?

Some systems use temporal reasoning in queries. A user asks "What was the policy in 2024?" versus "What is the current policy?" The system retrieves different documents based on temporal context. Testing this requires issuing time-specific queries and verifying the right version is retrieved.

Stale data testing also covers the lag between content updates and index updates. You update a document in your CMS. How long until the updated version is indexed and retrievable? You test this by updating a document, waiting, querying, and verifying the answer reflects the update. You measure the lag and verify it meets your SLA.

You test what happens when stale data is the only data. The user asks about a discontinued product. All documentation is from before discontinuation. The system must either acknowledge the product is discontinued, if it has that metadata, or return information with a caveat that it might be outdated. You verify your system handles this appropriately.

## Other Edge Cases: Ambiguity, Multilingual, Format Confusion

Beyond empty retrievals, contradictions, and stale data, production RAG encounters dozens of other edge cases. Ambiguous queries where the user intent is unclear. Multilingual queries where the query language does not match document language. Format confusion where retrieved content is structured data the model cannot interpret.

Ambiguous queries are common. "What is the return policy?" might refer to product returns, employment offer returns, or library book returns, depending on your domain. Testing ambiguity means issuing queries that could mean multiple things and verifying the system either asks for clarification or returns information about all possible interpretations.

Multilingual edge cases occur in global systems. A user asks a question in English. The relevant document is in Spanish. Does retrieval work? Does the generator translate? Testing requires indexing documents in multiple languages and issuing cross-language queries. You verify that either cross-language retrieval works or the system gracefully handles language mismatch.

Format confusion happens when documents contain tables, code, formulas, or other structured content that does not translate well to plain text. The chunker extracts text from a table. The text loses the table structure. The generator tries to answer based on the deformed text and produces nonsense. Testing requires indexing documents with structured content and verifying answers are coherent or the system acknowledges it cannot parse the format.

Another edge case is context overflow. The user asks a broad question. Retrieval returns 50 relevant chunks totaling 20,000 tokens. Your context window is 8,000 tokens. You must truncate. Testing requires issuing broad queries and verifying that truncation preserves the most important information and that the answer acknowledges if information was incomplete.

Chained queries are an edge case in conversational RAG. The user asks "What is the refund policy?" then "How do I apply?" The second query depends on context from the first. Testing requires issuing multi-turn conversations and verifying the system maintains context appropriately. You test that "How do I apply?" retrieves documents about refund application process, not generic application process.

Near-duplicates are another edge case. The same information appears in multiple documents with slight variations. Retrieval returns five chunks that all say essentially the same thing. The context is redundant. The answer repeats itself. Testing requires indexing near-duplicate content and verifying deduplication works or the answer is not redundantly repetitive.

## Building an Edge Case Test Suite: Coverage of Failure Modes

Edge case testing starts with enumerating failure modes. You list all the ways your RAG system could fail that are not captured by happy-path testing. Empty retrieval. Contradictory documents. Stale data. Ambiguous queries. Language mismatch. Format confusion. Context overflow. Near-duplicates. Each failure mode becomes a test category.

For each category, you create specific test cases. For empty retrieval, you write 10 queries on topics definitely not in your corpus. For contradictions, you create 5 pairs of contradictory documents and queries that retrieve both. For stale data, you index old and new versions of 5 documents and write queries for each. You aim for comprehensive coverage of known failure modes.

You automate the tests. Each test issues a query, captures the answer, and asserts expected behavior. For empty retrieval, you assert the answer contains "no information available" or similar. For contradictions, you assert the answer mentions both perspectives or acknowledges uncertainty. For stale data, you assert the answer uses the recent information.

You run edge case tests in CI, alongside unit and integration tests. Edge case test failures are blocking. If a code change breaks edge case handling, you do not ship. This ensures that edge case handling does not regress as the system evolves.

You expand the test suite based on production failures. When a user reports an incorrect answer, you investigate. If it was caused by an edge case you did not test, you add that case to the suite. The suite grows over time, accumulating institutional knowledge about what edge cases matter in your domain.

You measure edge case coverage. What percentage of production queries fall into edge case categories? If 15 percent of queries have no relevant documents but you only test 5 edge cases for that scenario, your coverage is insufficient. You expand the test suite until it represents the actual edge case distribution in production.

You prioritize edge cases by impact. Not all edge cases matter equally. Hallucinating legal precedent has catastrophic impact. Returning a slightly outdated contact email has low impact. You test high-impact edge cases thoroughly. You test low-impact edge cases less rigorously.

The legal research platform rebuilt their system with edge case testing. They created a test suite covering empty retrievals, contradictory case law, overruled precedents, jurisdiction mismatches, and temporal queries. They wrote 150 edge case tests representing scenarios that broke their system in the past or that legal experts identified as high-risk.

The edge case suite caught a bug where the system cited overruled cases without noting they were overruled. It caught a bug where queries about state law retrieved federal law with similar keywords. It caught a bug where the system hallucinated answers when retrieval returned procedural documents instead of substantive law. Each bug would have caused incorrect legal advice. The test suite prevented them from reaching users.

When the company later added a new document type, they expanded the edge case suite with tests specific to that type. When they changed their retrieval logic, the edge case tests verified that edge case handling did not regress. When a user reported an edge case they had not considered, they added it to the suite immediately. The suite became the specification of how the system should handle the messy reality of production queries, not just the clean queries from demos.

Edge case testing is the discipline of assuming things will go wrong and verifying your system handles it gracefully. Empty retrievals will happen. Contradictions will happen. Stale data will happen. The question is whether your system degrades gracefully or catastrophically. Teams that test edge cases ship systems that handle the messy 20 percent of queries robustly. Teams that only test the happy path ship systems that work great in demos and fail in production. The difference is deliberate testing of the scenarios where naive implementations break.
