# 2.4 â€” Overlap and Boundary Handling Between Chunks

In August 2024, an e-learning platform launched a RAG-powered tutoring assistant to help students search course materials. Within the first week, students reported bizarre answer gaps. A question about the derivation of the quadratic formula retrieved a chunk that started with the final step of the derivation, completely missing the setup and intermediate steps. Another query about a historical event retrieved a chunk ending mid-sentence with "and this led to..." but the continuation explaining what happened next was in a different chunk that was not retrieved. The team discovered their chunking logic split documents at strict token boundaries with zero overlap. Critical explanations were severed, context was lost, and students received fragments that made no sense in isolation. When they added 20% overlap between chunks, answer quality improved dramatically because important information now appeared in multiple chunks, increasing the likelihood that at least one retrieved chunk contained complete context.

You are configuring chunk overlap in 2026, and this seemingly minor detail determines whether your retrieval system handles boundary cases gracefully or fails on queries where the answer spans a chunk boundary. Overlap is the redundancy you introduce between adjacent chunks to ensure that no critical information is lost at split points. Too little overlap, and you risk fragmenting important content. Too much overlap, and you waste storage, increase retrieval noise, and consume LLM context with redundant text. This chapter walks through why overlap is necessary, how to choose overlap percentages, the tradeoffs between sentence-boundary and token-boundary splitting, and strategies for maintaining coherence across chunks.

## Why Chunks Need Overlap

The fundamental problem with non-overlapping chunks is that information does not align neatly with arbitrary boundaries. A document is a continuous stream of ideas, arguments, and explanations that flow across paragraphs and sections. When you split this stream into fixed-size chunks, you inevitably cut through the middle of conceptual units. A chunk may end mid-explanation, mid-argument, or mid-example. The next chunk picks up where the previous one left off, but if only one of the two chunks is retrieved, the user receives incomplete information.

Overlap mitigates this problem by duplicating text at chunk boundaries. If chunks have 20% overlap, the last 20% of chunk N appears as the first 20% of chunk N+1. If a conceptual unit spans the boundary between these chunks, it now appears in both, either partially in chunk N or partially in chunk N+1, or fully in one of them depending on where the unit falls. This redundancy increases the probability that a retrieved chunk contains the complete information needed to answer a query.

Consider a technical explanation split into two non-overlapping chunks. Chunk A ends with "The algorithm works by iterating over each element and applying a transformation function." Chunk B starts with "The transformation function is defined as f of x equals x squared plus one." A query asking how the algorithm works may retrieve Chunk A, which mentions a transformation function but does not define it. The answer is incomplete. With 20% overlap, the sentence defining the transformation function appears in both chunks. Retrieving either chunk provides the complete explanation.

Overlap also improves retrieval robustness for queries that match text near chunk boundaries. Without overlap, text at the very end of chunk N has only one chance to be retrieved: if the query matches chunk N. With overlap, text near the end of chunk N also appears at the start of chunk N+1, giving it two chances to be retrieved. If the query embedding is slightly closer to chunk N+1's embedding due to the additional context from the overlapping region, chunk N+1 may be retrieved even if chunk N is not. This redundancy compensates for the imperfect nature of embedding-based retrieval.

The downside of overlap is cost. If chunks have 20% overlap, you store approximately 20% more tokens in your vector database. You embed 20% more tokens during ingestion. When you retrieve multiple overlapping chunks, the LLM receives redundant text, consuming context window budget that could be used for additional unique chunks. For large corpora, these costs add up. The question is whether the retrieval quality improvement justifies the expense.

## Optimal Overlap Percentages

The optimal overlap percentage is a tradeoff between retrieval robustness and storage efficiency. Industry practice in 2026 converges on overlap percentages between 10% and 30%, with 20% being the most common default. This range provides meaningful redundancy without excessive waste.

At the low end, 10% overlap provides minimal redundancy. Only the last tenth of each chunk overlaps with the next chunk. This captures some boundary information but may still miss conceptual units that span more than 10% of a chunk. For 512-token chunks, 10% overlap is roughly 50 tokens, which is typically a few sentences. This is enough to avoid cutting mid-sentence but may not preserve multi-sentence explanations that span boundaries.

At 20% overlap, you duplicate roughly one-fifth of each chunk. For 512-token chunks, this is approximately 100 tokens, or a few paragraphs. This is sufficient to capture most boundary-spanning content: explanations, examples, and argument transitions that flow across chunk boundaries. The storage overhead is manageable, and the retrieval benefit is significant. This is why 20% has become the default in many RAG implementations.

At 30% overlap, you duplicate nearly one-third of each chunk. This provides strong redundancy and maximizes the likelihood that important information appears in multiple chunks. For 512-token chunks, 30% overlap is about 150 tokens. The storage overhead is substantial: for every three chunks, you store the equivalent of one additional chunk in redundant data. The retrieval benefit over 20% overlap is incremental. Most teams find that 30% overlap does not improve retrieval quality enough to justify the cost, except in domains where missing critical information has high consequences.

Beyond 30%, overlap becomes inefficient. At 50% overlap, half of your corpus is duplicated. Storage and embedding costs double. Retrieved chunks contain so much redundant text that LLMs waste context window processing repetitive content. The retrieval benefit plateaus because increasing overlap beyond a threshold does not significantly improve the likelihood of capturing boundary-spanning information.

The empirical approach to selecting overlap is to benchmark retrieval quality with different percentages. Chunk your corpus with 0%, 10%, 20%, and 30% overlap. Run representative queries, retrieve chunks, and evaluate answer quality. Measure how often answers improve with higher overlap and whether the improvement justifies the cost. In most cases, you will find that 20% overlap provides the best balance, but your specific use case may differ.

## Sentence-Boundary vs Token-Boundary Splitting

When splitting documents into chunks, you must decide whether to split at exact token counts or to adjust boundaries to align with sentence breaks. Token-boundary splitting is mechanically simple: tokenize the document and split every N tokens. Sentence-boundary splitting is more complex: tokenize the document, identify sentence breaks, and split at the sentence break nearest to the target token count. The choice affects chunk coherence and overlap behavior.

Token-boundary splitting produces chunks of uniform size but frequently cuts mid-sentence. A chunk may end with "The primary advantage of this approach is that it" and the next chunk starts with "reduces computational complexity significantly." Reading either chunk in isolation is confusing because the sentence is fragmented. An LLM receiving a chunk that ends mid-sentence may generate incomplete answers or hallucinate the rest of the sentence based on context.

Sentence-boundary splitting produces chunks of variable size but ensures every chunk starts and ends with complete sentences. This improves readability and coherence. When a chunk is retrieved, it can be presented to users or passed to an LLM without grammatical awkwardness. The variability in chunk size is usually small: if the target is 512 tokens and you adjust to the nearest sentence boundary, chunks may range from 480 to 540 tokens depending on sentence length. This variability is acceptable for most applications.

Sentence-boundary splitting requires sentence detection, which adds complexity. You need a library like spaCy, NLTK, or a custom sentence tokenizer that handles edge cases: abbreviations like "Dr." that contain periods but are not sentence boundaries, ellipses, and quotations. Sentence detection is language-specific and imperfect, especially for informal text, chat logs, or poorly formatted documents. However, for well-formed documents in major languages, sentence detection is reliable enough for production use.

The interaction between sentence-boundary splitting and overlap is important. When you define 20% overlap, do you mean 20% of tokens or 20% measured at sentence boundaries? Token-based overlap is precise but may create awkward overlap regions that cut mid-sentence. Sentence-based overlap adjusts the overlap to the nearest sentence boundary, ensuring overlap regions are grammatically complete. Sentence-based overlap may result in slightly more or less than 20% actual overlap, but it improves chunk coherence.

The production pattern is to use sentence-boundary splitting whenever possible. The improvement in chunk coherence is worth the small variability in chunk size and the complexity of sentence detection. Token-boundary splitting is reserved for cases where sentence detection is unreliable, such as heavily technical documents with domain-specific abbreviations, chat logs with fragmented grammar, or documents in languages where sentence detection libraries are not available.

## Cross-Chunk References and the Split Mid-Sentence Problem

Even with sentence-boundary splitting and overlap, some information inherently spans multiple chunks. A reference to a figure or table defined earlier in the document, a pronoun whose antecedent is in a previous chunk, or a multi-part argument where the conclusion is in one chunk and the supporting evidence is in another. These cross-chunk references are unavoidable when chunking long documents, and they create retrieval challenges.

When a chunk contains a reference to information not included in the chunk, the LLM may not be able to answer the query correctly. A chunk stating "As shown in Table 3, the performance improvement is significant" is useless if Table 3 is not included. A chunk starting with "These results indicate that the hypothesis is supported" lacks meaning if the results and hypothesis are in previous chunks. The overlap between chunks mitigates this problem for nearby references, but it does not solve it entirely.

One strategy is to enrich chunks with contextual breadcrumbs. When chunking, prepend each chunk with metadata describing its position in the document: the section title, chapter number, or heading hierarchy. For example, a chunk from a technical manual might start with "Chapter 5: Network Configuration / Section 5.2: Firewall Rules" followed by the chunk content. This breadcrumb provides context about where the chunk fits in the document structure, helping the LLM understand the broader context even if the chunk itself is a fragment.

Another strategy is parent-child chunk relationships. Instead of treating all chunks as independent, maintain metadata linking each chunk to its parent section or sibling chunks. When a chunk is retrieved, optionally retrieve its parent or adjacent siblings to provide additional context. This is more complex than simple retrieval but improves answer quality for documents with tight internal coherence. We will explore this further in chapter 2.7 on document hierarchy.

The split mid-sentence problem is a specific case of cross-chunk references where a sentence is inadvertently split across chunks despite using sentence-boundary splitting. This can happen when a sentence is very long and exceeds the chunk size, or when overlap adjustments create edge cases where a sentence appears partially in one chunk and partially in another. Detecting and handling these cases requires validation logic: after chunking, check that no chunk ends with a sentence fragment, and if it does, either extend the chunk to include the full sentence or adjust the previous chunk's boundary.

In practice, a small percentage of chunks will inevitably contain references or fragments that make them less useful in isolation. The goal is not to eliminate this problem entirely but to minimize its frequency through appropriate overlap and boundary handling. Validation metrics like the percentage of chunks that end mid-sentence or contain unresolved references provide signals for tuning chunking logic.

## Strategies for Maintaining Coherence Across Chunk Boundaries

Maintaining coherence across chunk boundaries is an ongoing challenge in production RAG systems. Several strategies help mitigate coherence loss, each with different complexity and applicability.

The first strategy is aggressive overlap. By increasing overlap to 30% or more, you ensure that most boundary-spanning content appears in multiple chunks. This is the simplest strategy but also the most expensive. It works well when storage and embedding costs are not a primary concern and retrieval quality is paramount.

The second strategy is semantic boundary detection. Instead of splitting at fixed token counts, use semantic segmentation to identify natural topic transitions and split there. TextTiling and similar algorithms detect topic shifts by analyzing word co-occurrence patterns or embedding similarity. When you split at topic boundaries, chunks are more likely to be self-contained because each chunk discusses a coherent topic. This reduces cross-chunk references and improves retrieval precision.

The third strategy is hierarchical chunking with context propagation. Split documents into large parent chunks and smaller child chunks. Each child chunk inherits context from its parent: the section heading, the topic, and key definitions. When you retrieve a child chunk, you also retrieve its parent or prepend the parent's summary to the child. This provides the LLM with both the focused answer from the child chunk and the broader context from the parent chunk. Hierarchical chunking is more complex to implement but significantly improves coherence for structured documents.

The fourth strategy is post-retrieval chunk merging. After retrieving chunks, check if adjacent chunks from the same document were retrieved. If chunk N and chunk N+1 are both in the retrieval results, merge them into a single contiguous passage before passing to the LLM. This reconstructs the original document structure dynamically and eliminates redundancy from overlap. The challenge is that merging changes the amount of context passed to the LLM, which may affect prompt design and context window management.

The fifth strategy is LLM-based coherence validation. After chunking, pass each chunk to an LLM and ask whether it is coherent and self-contained. If the LLM reports that the chunk is incomplete or references missing information, adjust the chunk boundaries or flag it for manual review. This is expensive and slow but can catch edge cases that rule-based validation misses. It is practical for small, high-value corpora but not for large-scale ingestion.

The production pattern is to combine multiple strategies. Use sentence-boundary splitting as the baseline. Add 20% overlap to handle typical boundary cases. Use semantic boundary detection when document structure is unclear. Apply hierarchical chunking for structured documents like manuals or research papers. Reserve LLM-based validation for critical documents where coherence is essential. This layered approach balances cost and quality.

## Overlap in Recursive and Semantic Chunking

Overlap behaves differently in recursive and semantic chunking compared to fixed-size chunking. In fixed-size chunking, overlap is a simple percentage of the chunk size: 20% of 512 tokens is about 100 tokens. In recursive and semantic chunking, chunks have variable sizes, and overlap must be adapted accordingly.

For recursive chunking, overlap is typically applied at the lowest level of the hierarchy where token-level splitting occurs. When a section is split into paragraphs, paragraphs may overlap by one or two sentences. When a paragraph is split into smaller chunks due to size constraints, those chunks overlap by a percentage of their size. Higher levels of the hierarchy, such as chapters or sections, usually do not overlap because they are defined by explicit structural boundaries in the document.

For semantic chunking, overlap is applied after detecting topic boundaries. Once you identify where to split, you add overlap by including some content from the previous chunk at the start of the next chunk. The challenge is that semantic boundaries are detected based on topic shifts, and adding overlap may blur the boundary. If you detect a strong topic shift between chunk N and chunk N+1, including overlap from chunk N in chunk N+1 reintroduces the old topic into the new chunk. This can confuse retrieval: chunk N+1 may match queries about both the old topic and the new topic, reducing precision.

One solution is to use asymmetric overlap in semantic chunking. Include more overlap from the end of the previous chunk into the start of the next chunk, but do not include overlap from the start of the next chunk back into the previous chunk. This allows the new chunk to have context from the old topic without polluting the old chunk with the new topic. Another solution is to reduce overlap percentages when semantic boundaries are strong and increase overlap when boundaries are weak or uncertain.

The implementation complexity of overlap in recursive and semantic chunking is higher than in fixed-size chunking. You need logic to calculate overlap dynamically based on chunk sizes and boundaries, validate that overlap does not create incoherent chunks, and handle edge cases like very short chunks where overlap would exceed the chunk size. This is why many teams start with fixed-size chunking and fixed overlap, and only adopt more complex strategies after validating that the additional complexity improves retrieval quality.

## Overlap as a Retrieval Robustness Lever

Overlap is a lever you can adjust to improve retrieval robustness. When users report that retrieved chunks are incomplete or lack necessary context, increasing overlap is a straightforward fix that often improves answer quality. When storage or embedding costs are a concern, reducing overlap can lower expenses with an acceptable degradation in retrieval quality.

The e-learning platform that launched with zero overlap learned this lesson quickly. Students received fragmented chunks that made no sense, and the problem was obvious within days. Adding 20% overlap was a simple configuration change that fixed the majority of issues. The lesson is not that 20% overlap is always right, but that overlap is essential and must be chosen deliberately.

The empirical approach to tuning overlap is similar to tuning chunk size: benchmark retrieval quality with different overlap percentages, measure how often overlap improves retrieved chunk completeness, and balance the quality improvement against storage and cost. Track metrics like the percentage of queries where the top-retrieved chunk lacks necessary context, and observe how this percentage changes with overlap. If 10% overlap reduces incomplete chunks from 30% to 15%, and 20% overlap reduces it further to 5%, the incremental benefit of 20% overlap may be worth the cost.

You are configuring chunk overlap in 2026, and you have the tools and patterns to make this decision empirically. Do not default to zero overlap because it is simpler. Do not blindly use 20% overlap because it is common. Test whether overlap improves retrieval quality for your use case, measure the cost, and choose based on the tradeoff. Overlap is a small detail in the chunking pipeline, but it has an outsized impact on retrieval robustness. Get it right, and your system gracefully handles boundary cases. Get it wrong, and your users receive incomplete, fragmented answers that undermine trust.
