# 7.6 â€” Performance Testing: Latency, Throughput, and Concurrency

In September 2025, a document intelligence startup launched their RAG-powered contract analysis tool to general availability. The first day went smoothly: 200 users, average latency 1.8 seconds, no errors. The second day, a large enterprise customer onboarded 500 employees. Latency spiked to 12 seconds. The third day, three more enterprise customers onboarded. The system became unusable. Queries timed out. The vector database hit connection limits. The LLM API returned rate limit errors. User complaints flooded support. The company had to throttle new signups and spend two weeks rearchitecting for scale.

The engineering team had tested correctness extensively. They verified answers were accurate and well-cited. They tested edge cases and adversarial inputs. But they never tested performance under load. They ran queries one at a time in development. They never measured latency when 100 concurrent users issued queries simultaneously. They never tested what happened when query volume exceeded their database connection pool or LLM rate limits. They assumed performance would scale. That assumption was expensive.

Performance testing is not optional for production RAG. Your system might return perfect answers in 2 seconds when running alone, but if it takes 30 seconds under load or crashes when 50 users query simultaneously, it is not production-ready. Performance testing means measuring latency, throughput, and concurrency limits, identifying bottlenecks, and verifying your system meets SLAs under realistic load.

## Latency Testing: Measuring End-to-End Response Time

Latency is the time from query submission to answer delivery. For RAG, this includes query processing, retrieval, reranking, context assembly, generation, and validation. Each stage contributes latency. The total determines whether your system feels fast or sluggish.

Baseline latency testing measures performance with no load. You issue a single query and measure response time. You repeat for 100 different queries and compute statistics: median, p95, p99, max. This gives you the best-case latency when the system has full resources available.

The median tells you typical performance. If median latency is 1.8 seconds, half your queries complete faster, half slower. This is the number users will associate with your system speed. The p95 tells you how bad it gets for the slowest queries. If p95 is 4.2 seconds, 5 percent of queries take longer than 4.2 seconds. Users who hit the p95 notice. The p99 tells you the outliers. If p99 is 8 seconds, 1 percent of queries are very slow. This matters for high-volume systems where 1 percent is thousands of queries per day.

You test each pipeline stage independently to identify bottlenecks. You measure query processing latency, retrieval latency, reranking latency, generation latency, validation latency. If retrieval takes 200 milliseconds, reranking takes 1200 milliseconds, and generation takes 400 milliseconds, you know reranking is the bottleneck. Optimizing other stages will not significantly improve total latency.

You test latency variation by query type. Short queries might be fast; long queries slow. Queries that retrieve many documents are slower than those that retrieve few. Queries on common topics might hit caches; rare queries always hit the database. You segment queries by type and measure latency for each segment. This reveals whether some query patterns are problematic.

You test latency under different system states. Cold start: the system just started, caches are empty. Warm: the system has been running, caches are populated. Degraded: the database is under load, the LLM API is near rate limits. Each state has different performance characteristics. You test all of them to understand the full latency distribution.

You set SLAs based on user expectations. For interactive use, anything under 2 seconds feels instant. Under 5 seconds feels acceptable. Over 10 seconds feels broken. You measure what percentage of queries meet your SLA and track it over time. If you promise p95 latency under 3 seconds and you are delivering 5 seconds, you have a problem.

## Throughput Testing: Queries Per Second Under Load

Throughput is how many queries your system can handle per unit time. High throughput means you can serve many users concurrently without degradation. Low throughput means adding users quickly overwhelms the system.

Throughput testing starts with ramping load. You start with one query per second and gradually increase: 5 QPS, 10 QPS, 20 QPS, 50 QPS. At each level, you measure latency and error rate. As load increases, latency increases. At some point, latency crosses your SLA threshold or errors start occurring. That is your throughput limit.

The throughput curve shows you how gracefully your system degrades. A good system maintains low latency up to a threshold, then latency increases linearly with load. A bad system has a cliff: latency is fine at 19 QPS, catastrophic at 20 QPS. The cliff indicates a hard resource limit: connection pool exhaustion, rate limit, memory overflow.

You identify the bottleneck by monitoring resource utilization during throughput tests. If CPU usage hits 100 percent before latency degrades, you are CPU-bound. If database connections max out, you are connection-bound. If LLM API rate limits trigger, you are API-bound. If memory usage grows without bound, you have a memory leak.

You test throughput for different query mixes. All queries are not equal. A query that retrieves 10 documents and generates a 500-word answer consumes more resources than a query that retrieves 3 documents and generates a 50-word answer. You create realistic query mixes representing your production distribution and measure throughput for each mix.

You test sustained throughput versus burst throughput. Can you handle 100 QPS for 10 seconds, or 50 QPS for an hour? Burst capacity is different from sustained capacity. Burst tests reveal whether you can handle traffic spikes. Sustained tests reveal whether you can handle steady high load without resource leaks or degradation.

You test throughput scaling. If you add a second server, does throughput double? If you increase your LLM rate limit, does throughput increase proportionally? Linear scaling is ideal but rare. Sublinear scaling is common due to coordination overhead, shared bottlenecks, or diminishing returns. You measure actual scaling to predict how much infrastructure you need for target throughput.

## Concurrency Testing: Handling Simultaneous Queries

Concurrency testing measures how your system handles multiple simultaneous queries. Unlike throughput testing, which gradually ramps load, concurrency testing hits the system with many queries at once.

The simplest concurrency test is issuing N queries simultaneously and measuring how many complete successfully and how long they take. You start with N equals 10, then 50, then 100, then 500. At low concurrency, all queries succeed. At high concurrency, some queries fail: timeouts, connection errors, rate limits, out-of-memory crashes.

Concurrency failures reveal resource contention. If your database connection pool has 20 connections and you issue 100 concurrent queries, 80 queries wait for connections. If your connection timeout is 5 seconds, those 80 queries fail. If your LLM API allows 10 concurrent requests and you issue 50, 40 requests are queued or rejected.

You test concurrency limits for each component. The vector database might handle 200 concurrent queries. The LLM API might handle 10. The context assembly service might handle 50. The system concurrency limit is determined by the weakest component. You identify which component limits concurrency and optimize it.

You test failure modes under concurrency. Do failed queries retry? Do they fail fast or hang? Do failures cascade: does one failed query cause others to fail? Do you have retry storms where failed queries retry simultaneously, creating more load? You test these scenarios and verify your error handling is robust.

You test concurrency fairness. When 100 queries arrive simultaneously, do they all get equal resources, or do some starve while others complete? Do queries that arrive first complete first, or is ordering unpredictable? Fairness matters for user experience. If some users consistently get fast responses while others time out, the unlucky users churn.

You test autoscaling behavior. If your system autoscales based on load, does it scale fast enough to handle concurrency spikes? If 500 queries arrive at once and scaling takes 60 seconds, those queries might fail before new capacity comes online. You test whether autoscaling is fast enough and whether it prevents over-scaling when load drops.

## Vector Database Performance Under Stress

The vector database is often the bottleneck in RAG systems. Testing its performance under stress is critical.

You test query latency versus index size. Does a 1-million-vector index return results as fast as a 10,000-vector index? How does latency scale as the index grows? You create indexes of different sizes and measure retrieval latency for each. This tells you whether your database performance will degrade as your corpus grows.

You test query latency versus result count. Retrieving top-5 results is faster than top-100. But how much faster? You measure latency for different k values and determine the cost of retrieving more results. This informs your decision about how many results to retrieve and whether to retrieve liberally and rerank aggressively or retrieve conservatively.

You test the impact of metadata filtering. Queries with complex filters are slower than unfiltered queries. You measure the cost of filtering by author, date range, category, and combinations of filters. If filtering adds significant latency, you might need to denormalize data or use specialized indices.

You test concurrent query performance. Does the database maintain low latency when handling 50 concurrent queries, or does latency spike? You issue concurrent queries and measure latency distribution. This reveals whether the database can handle production concurrency or whether you need to shard, replicate, or cache.

You test write performance during reads. In production, you are simultaneously querying the index and updating it with new documents. Does writing impact query latency? You run a mixed workload: continuous queries plus periodic writes. You measure query latency during writes and verify it meets your SLA.

You test failure recovery. What happens when a database node crashes? How long until queries start failing? How long until the system recovers? You simulate failures and measure impact. This tells you whether your database setup has adequate redundancy and failover.

## LLM Rate Limits and Latency Under Load

The LLM API is another common bottleneck. Rate limits constrain throughput. Variable latency affects user experience.

You test rate limit behavior. Your API allows 100 requests per minute. What happens when you send 101? Does the 101st request fail immediately, queue, or retry? You test at and above your rate limit and verify your system handles it gracefully. You implement exponential backoff, queueing, or user-facing "high load" messages as appropriate.

You test LLM latency distribution. Generation latency varies based on output length, model load, and randomness. You issue 1000 queries and measure the latency distribution. If median is 600 milliseconds but p95 is 2400 milliseconds, you have high variance. High variance makes it hard to predict user experience.

You test parallel generation. Some systems generate multiple candidate answers and select the best. Does your LLM API allow parallel requests, or do they count against a concurrency limit? You test parallel generation and measure whether it improves quality enough to justify the latency and cost.

You test batching. Some LLM APIs support batching multiple queries in a single request. Does batching reduce latency or cost? You test different batch sizes and measure the tradeoff between latency and throughput. Small batches have lower latency but lower throughput. Large batches have higher throughput but higher latency.

You test degraded API performance. LLM APIs occasionally slow down during high demand. You simulate this by adding artificial delay to API responses and measure how your system behaves. Do timeouts protect you from hanging indefinitely? Does latency stay within SLA when the API is slow?

## Identifying Bottlenecks and Performance Regression Detection

Performance testing is only valuable if you act on the results. You identify bottlenecks and optimize them. You track performance over time and catch regressions.

Bottleneck identification means profiling your system under load. You instrument each pipeline stage with latency tracking. You measure time spent in query processing, retrieval, reranking, generation, validation. The stage with the highest latency is the bottleneck. You optimize it first.

Optimization is iterative. You optimize the bottleneck, re-test, identify the new bottleneck, optimize it. You repeat until latency meets your SLA or until optimization cost exceeds benefit. Common optimizations: caching retrieval results, parallelizing reranking, using faster models, reducing context size, batching requests, adding replicas.

Performance regression detection means running performance tests on every deployment and comparing results to baseline. If median latency increases from 1.8 seconds to 2.2 seconds, you investigate. If p95 latency increases from 4.2 seconds to 6.1 seconds, you block the deployment. You treat performance regressions like correctness regressions: they are bugs.

You track performance metrics in production. You log latency for every query. You compute hourly statistics: median, p95, p99. You plot trends. If latency is creeping upward over weeks, you investigate even if no single deployment caused a large spike. Slow degradation is as dangerous as sudden regression.

You alert on performance violations. If p95 latency exceeds your SLA for an hour, you page the on-call engineer. If error rate spikes above 1 percent, you investigate. Alerts catch performance incidents before users complain.

The document intelligence startup rebuilt their system with performance testing. They established a baseline: median latency 1.5 seconds, p95 latency 3 seconds, throughput 40 QPS, concurrency limit 80 simultaneous queries. They ran performance tests on every deployment. They monitored production latency and alerted on SLA violations.

They discovered their reranker was the bottleneck, consuming 60 percent of total latency. They optimized by switching to a smaller cross-encoder model and parallelizing reranking. Latency dropped to median 1.1 seconds, p95 2.1 seconds. Throughput increased to 70 QPS. They caught a performance regression when a code change added unnecessary database queries, increasing latency by 400 milliseconds. The regression was reverted before reaching production.

They load-tested their system before each major customer onboarding. When a customer with 1000 employees was onboarding, they simulated 1000 concurrent users and verified the system handled it. They discovered they needed to increase their LLM rate limit and add database replicas. They made the changes before onboarding. The launch was smooth.

Performance testing is the discipline of verifying your system works not just correctly but quickly, even under load. It is the difference between a demo that impresses and a product that scales. Teams that performance-test ship systems that stay fast as usage grows. Teams that skip it ship systems that work until they do not, collapsing under load in ways that destroy user experience and company reputation. The difference is testing under realistic conditions before users do it for you.
