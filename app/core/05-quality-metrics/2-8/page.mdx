# 2.8 â€” Latency: Time-to-First-Token, End-to-End, and Perceived Speed

On December 9, 2025, a well-funded AI coding assistant startup called DevFlow launched their flagship product after eighteen months of development. The system was technically impressive: it generated high-quality code suggestions, understood complex codebases, and provided intelligent refactoring recommendations. The founding team, all machine learning PhDs from top research labs, had optimized relentlessly for output quality. Their acceptance rate in testing was seventy-eight percent, far higher than GitHub Copilot's reported sixty percent. They had raised fourteen million dollars in seed and Series A funding based on demos that wowed investors. The product launched to enthusiastic early adopters. Within a week, complaints began appearing on social media. The suggestions were good, but they arrived too late. The median time from keystroke to first suggestion was four point two seconds. By the time the suggestion appeared, most developers had already typed the next line or moved on to a different thought. The cognitive cost of context switching back to evaluate the suggestion exceeded its value. Within three weeks, the weekly active user retention rate was thirty-one percent, down from sixty-four percent in week one. By week six, it was eighteen percent. DevFlow's team had built the highest-quality coding assistant in the market, but they had built it too slow. Users abandoned it for faster alternatives with lower quality. The company pivoted to enterprise sales with longer evaluation cycles, but the consumer market they had targeted was lost. They learned that latency is not an infrastructure detail. It is a quality dimension that determines whether a product is usable at all.

**Latency** is the time between when a user submits a request and when they receive a response. For AI systems, latency breaks down into multiple components: time to first token, the interval before generation begins; incremental token generation latency, the rate at which tokens are produced; end-to-end latency, the total time from request to complete response; and perceived latency, the subjective experience of waiting. Each component affects user experience differently, and each requires different optimization strategies. A system with excellent end-to-end latency but slow time to first token feels unresponsive. A system with fast time to first token but slow token generation feels like it is struggling. A system with good objective latency but poor perceived latency due to lack of feedback still frustrates users. You must optimize all components.

## Why Latency Is a Quality Dimension

Treating latency as purely an infrastructure or performance concern is a category error. Latency directly affects whether users can complete their tasks, whether they trust the system, and whether they perceive outputs as high quality. Consider a customer service chatbot. If response latency is two seconds, the conversation feels natural. If response latency is ten seconds, users disengage, assume the system has frozen, or abandon the conversation entirely. The quality of the response is irrelevant if the user has left before it arrives. Latency gates whether quality can be experienced at all.

The relationship between latency and perceived quality is non-linear and context-dependent. For simple factual queries, users expect sub-second responses. A three-second delay makes the system feel broken even if the answer is perfect. For complex analytical tasks, users tolerate longer latency because they understand the task requires processing time. A twenty-second delay for a detailed financial analysis is acceptable. A twenty-second delay for a restaurant recommendation is not. You must calibrate latency expectations to your application domain and communicate those expectations to users.

Latency also affects trust. Systems that respond instantly feel confident and authoritative. Systems that pause before responding feel uncertain. This is anthropomorphization, but it is real and consequential. Users infer system capability from response speed. A fast system must know the answer. A slow system must be struggling or searching. This inference can work against you even when slow responses are higher quality. Users may prefer fast, adequate responses to slow, excellent responses because the speed signals competence.

The economic impact of latency is measurable and substantial. Amazon found that every one hundred milliseconds of additional latency cost them one percent in sales. Google found that increasing search result latency from four hundred milliseconds to nine hundred milliseconds reduced queries by twenty percent. These are consumer-facing products with massive scale, but the principle applies at any scale. Latency affects engagement, retention, and conversion. It is not a technical detail. It is a product-defining characteristic.

## Time-to-First-Token: The Responsiveness Signal

**Time-to-first-token** is the interval between when the user submits a request and when the first token of the response appears. This is the most psychologically important latency component because it determines whether the system feels responsive. Human perception of responsiveness has hard thresholds. Below one hundred milliseconds, response feels instantaneous. Between one hundred and three hundred milliseconds, response feels fast. Between three hundred milliseconds and one second, response feels acceptable. Above one second, response feels slow. Above three seconds, users begin to wonder if the system is working. Above five seconds, users assume failure and retry or abandon.

These thresholds are not arbitrary. They reflect human cognitive architecture. At sub-one-second latencies, the interaction feels like a continuous dialogue. Above one second, the user's attention begins to wander. Above three seconds, working memory starts to fade, and the user may lose track of what they asked. Above five seconds, the cost of waiting exceeds the expected value of the response for many queries. This means that for most interactive applications, time-to-first-token should target sub-one-second performance for ninety-nine percent of requests.

Time-to-first-token is determined by pre-processing overhead, model load time if the model is not already loaded, prompt encoding, and the first forward pass through the model. For large language models served via API, pre-processing is typically negligible. Model load time is amortized across requests through persistent serving. Prompt encoding scales linearly with prompt length, so long prompts increase time-to-first-token. The first forward pass is the dominant factor. For a model with one hundred billion parameters, a single forward pass on a high-end GPU takes fifty to two hundred milliseconds depending on implementation and batch size. This sets a floor on time-to-first-token for self-hosted models.

Optimizing time-to-first-token requires minimizing prompt length, using speculative decoding or other techniques that reduce first-pass latency, and ensuring that infrastructure has sufficient capacity that requests are not queued. Prompt length is under your control. Do not include unnecessary context in prompts. Use prompt compression techniques if your application requires long prompts. Speculative decoding predicts likely first tokens using a small, fast model, then validates and refines with the full model, reducing latency by thirty to fifty percent in favorable cases. Infrastructure capacity planning is straightforward: provision enough serving capacity that queueing delay is negligible at peak load.

One common failure mode is optimizing average time-to-first-token without considering the tail. The median request might have six hundred milliseconds time-to-first-token, but the ninety-ninth percentile might be eight seconds due to occasional cold starts, resource contention, or network delays. Users who experience the tail perceive your system as slow even though most requests are fast. Monitor and optimize tail latency, not just medians.

## Incremental Token Generation and Streaming

Once the first token is generated, subsequent tokens are produced incrementally. For autoregressive models, each token requires a forward pass through the model, but these passes are faster than the first pass because they reuse cached activations. Typical token generation rates for large language models in 2026 range from ten to fifty tokens per second depending on model size, hardware, and batch size. For a response of one hundred tokens, this translates to two to ten seconds of generation time after time-to-first-token.

Users perceive incremental generation differently depending on whether tokens are streamed or buffered. **Streaming** means displaying each token as it is generated, so the user sees the response appear word by word. **Buffering** means waiting until the entire response is complete before displaying it. Streaming dramatically improves perceived latency because users begin consuming the response immediately after time-to-first-token rather than waiting for end-to-end completion. A five-second streamed response feels faster than a three-second buffered response because the user is productively engaged for most of the five seconds, whereas they are waiting unproductively for all of the three seconds.

Streaming is now standard practice for consumer-facing AI applications. ChatGPT, Claude, Gemini, and most coding assistants stream responses. Users have come to expect this behavior. If your application buffers responses, it will feel slower than competitors even if your objective latency is lower. Implement streaming unless you have a strong reason not to. The implementation is straightforward: as each token is generated, send it to the client over a persistent connection like WebSockets or server-sent events. The client appends each token to the display in real time.

The one downside of streaming is that it precludes certain types of post-processing. If you need to filter or modify the response before displaying it, you cannot do so token by token. You must buffer the complete response, process it, then display it. This trades improved perceived latency for the ability to enforce quality controls. For safety-critical applications, this trade is often correct. For most applications, streaming is preferable.

## End-to-End Latency and Task Completion

**End-to-end latency** is the total time from request submission to response completion. For a response with time-to-first-token of one second, generation rate of twenty tokens per second, and response length of one hundred tokens, end-to-end latency is six seconds. This is the latency that appears in system logs and performance dashboards. It is also the latency that determines whether users can complete tasks efficiently.

End-to-end latency matters most for non-interactive tasks. If a user submits a document for summarization and walks away, time-to-first-token is irrelevant. End-to-end latency determines when they can return to see the result. If they expect a two-minute summary task to complete in one minute but it takes five minutes, they are frustrated. If they expect it to take ten minutes and it completes in eight, they are satisfied. Expectation setting is as important as raw performance.

For interactive tasks, end-to-end latency matters when it determines whether a workflow is feasible. A developer using a coding assistant can tolerate five seconds per suggestion if suggestions are occasional. If the workflow requires ten suggestions in sequence to complete a task, the total latency is fifty seconds, which may be unacceptable. The system is individually responsive but collectively too slow. This is a product design problem, not an infrastructure problem. You must design workflows that limit sequential dependencies on slow operations.

Optimizing end-to-end latency requires optimizing the entire pipeline: prompt construction, model inference, post-processing, and response delivery. In most systems, model inference is the dominant cost, but do not assume this without measurement. Profile your pipeline. Identify bottlenecks. A common surprise is that post-processing, such as parsing model outputs, calling external APIs, or formatting responses, consumes more time than expected. Optimize every stage that contributes materially to end-to-end latency.

## Perceived Latency and User Experience Design

**Perceived latency** is the subjective experience of waiting. It is influenced by objective latency but also by progress indicators, feedback, animation, and user expectations. A five-second operation with a progress bar feels faster than a three-second operation with no feedback. A ten-second operation that the user expects to take ten seconds feels faster than a five-second operation they expected to take one second. Perceived latency is as important as objective latency because it directly determines user satisfaction.

The most effective technique for reducing perceived latency is streaming, already discussed. The second most effective technique is providing immediate feedback that the request was received and processing has started. When the user clicks submit, immediately display a loading indicator. When generation begins, immediately display the first token. Never leave the user staring at a static screen wondering if their request was registered. Feedback transforms waiting from anxious uncertainty into productive anticipation.

Progress indicators help when total response time is predictable. If generating a report takes thirty seconds, display a progress bar that fills over thirty seconds. This sets expectations and reassures the user that the system is working. The challenge is that language model generation time is not deterministic. Response length varies by query, and generation rate varies by server load. You can estimate completion time based on historical data, but the estimate will be wrong for individual requests. An inaccurate progress bar is worse than no progress bar because it violates user expectations. Use progress indicators only when you can estimate completion time with reasonable accuracy.

Animation and transitions also affect perceived latency. A response that fades in smoothly feels faster than a response that appears abruptly. A loading spinner that rotates smoothly feels faster than one that stutters. These effects are small, but they accumulate. Invest in polished UI interactions. They are part of your quality surface.

## Latency Budgets and Allocation

A **latency budget** is the total acceptable latency for a user workflow, decomposed into allocations for each pipeline stage. For example, you might set a latency budget of three seconds for a question-answering task, allocated as follows: five hundred milliseconds for prompt construction and preprocessing, two thousand milliseconds for model inference, three hundred milliseconds for post-processing, and two hundred milliseconds for response delivery and rendering. Each component owner is responsible for meeting their allocation. This prevents latency from creeping up due to incremental additions across the pipeline.

Latency budgets are most effective when they are based on user research. Measure the maximum latency users tolerate before abandoning tasks. Measure the latency at which satisfaction begins to decline. Use these thresholds to set budgets. If users abandon after five seconds, your budget must be well below five seconds to account for tail latency. If satisfaction declines after two seconds, target one point five seconds for typical cases.

Allocating the budget across pipeline stages requires understanding where time is currently spent and where optimization is feasible. If ninety percent of latency is model inference and inference time is determined by model size and hardware, you have limited room to optimize without changing the model or hardware. If fifty percent of latency is post-processing, you can optimize post-processing and achieve substantial gains without touching the model. Profile first, then allocate budget based on what is feasible.

Enforce latency budgets through automated monitoring. Every request should log latency for each pipeline stage. Compute percentiles and track them over time. Alert when any stage exceeds its budget at the ninety-fifth or ninety-ninth percentile. Treat latency budget violations as bugs, not as inevitable drift. This discipline prevents latency from degrading unnoticed over time.

## Latency Trade-offs with Other Quality Dimensions

Latency is almost always in tension with other quality dimensions. Higher-quality models are larger and slower. More thorough post-processing improves accuracy but adds latency. Retrieval-augmented generation improves factuality but requires database queries that add latency. Safety filtering adds latency. Every quality improvement must be evaluated against its latency cost.

The correct approach is to treat latency as a constraint, not a variable to be minimized. Set a latency budget based on user requirements, then optimize other quality dimensions within that constraint. Do not build the highest-quality system you can and hope latency is acceptable. Build the highest-quality system that meets your latency budget. This inverts the optimization problem in a way that aligns with user needs.

In practice, this means making explicit trade-offs. If adding a retrieval step improves accuracy by five percentage points but adds one second to latency, and your latency budget has only two hundred milliseconds of slack, you cannot add the retrieval step without removing something else or increasing your budget. Articulate this trade-off. Decide based on which quality dimension matters more to your users. Document the decision so that future team members understand why the system is designed the way it is.

One effective pattern is to offer multiple quality tiers with different latency characteristics. A fast mode uses a smaller, faster model and minimal processing for users who value speed. A thorough mode uses a larger, slower model and extensive post-processing for users who value quality. This allows users to choose the trade-off that fits their current task. Many coding assistants implement this pattern: quick suggestions for routine completions, detailed suggestions for complex functions.

## Model Selection and Infrastructure for Latency

Latency is determined largely by model size and inference infrastructure. Larger models produce higher-quality outputs but require more computation per token. GPT-5 with hundreds of billions of parameters generates higher-quality responses than GPT-5-mini with tens of billions of parameters, but it also takes longer. For latency-sensitive applications, smaller models are often the right choice even if quality is somewhat lower.

The landscape of model options in 2026 is rich. OpenAI offers GPT-5 and GPT-5.1 for high quality with moderate latency, GPT-5-mini for lower quality with low latency. Anthropic offers Claude Opus 4.5 for maximum quality, Claude Sonnet 4.5 for balanced quality and speed, Claude Haiku for speed. Google offers Gemini 2 Pro and Gemini 2 Flash. Meta's Llama 4 family includes models from eight billion to four hundred billion parameters. Each model represents a different point in the quality-latency trade-off space. You must benchmark multiple models for your application and select the one that meets your quality requirements within your latency budget.

Infrastructure choices also matter. Serving models on high-end GPUs like NVIDIA H100s provides lower latency than serving on older GPUs like A100s. Using larger batch sizes amortizes fixed overhead but increases latency for individual requests due to queueing. Using model parallelism reduces latency for very large models by distributing computation across GPUs. Using quantization reduces model size and latency with minimal quality loss. These are infrastructure decisions, but they have direct product implications. Budget for the infrastructure required to meet your latency targets.

## Monitoring and Debugging Latency in Production

Latency must be monitored continuously in production. Instrument every request to log latency for each pipeline stage. Compute and track the following metrics: median, ninety-fifth percentile, ninety-ninth percentile, and maximum latency over rolling time windows. Alert on regressions. If the ninety-fifth percentile latency increases from two seconds to three seconds, investigate immediately. Latency regressions indicate either a code change that introduced inefficiency, a model change that increased inference time, or an infrastructure issue like resource contention.

Debugging latency regressions requires detailed tracing. Use distributed tracing tools like OpenTelemetry to track requests across services and identify where time is being spent. Common causes of latency regressions include unoptimized code in newly added features, increased prompt lengths due to feature additions, model updates that increase inference time, database query slowdowns due to schema changes, and infrastructure issues like CPU throttling or network congestion. Each cause requires a different fix. Tracing identifies the cause quickly.

User-reported latency often differs from server-logged latency. Network latency, client-side rendering time, and time spent in browser JavaScript all contribute to perceived latency but do not appear in server logs. Implement client-side latency monitoring to capture the full user experience. Measure time from user action to when the first token appears on screen and time to when the complete response is rendered. Compare client-side and server-side metrics to identify whether latency issues are server-side or client-side.

## The Psychology of Speed

Understanding the psychology of speed is essential for designing systems that feel fast even when objective latency is non-trivial. Users perceive active waiting as shorter than passive waiting. If the system displays a progress indicator or streams tokens, users feel engaged and estimate elapsed time as lower than actual. Conversely, if the system is silent, users feel abandoned and estimate elapsed time as higher than actual. This is why streaming is so effective: it converts passive waiting into active consumption.

Users also perceive speed relative to alternatives. If your system responds in five seconds and competitors respond in three seconds, your system feels slow even if five seconds is objectively fast. Competitive context matters. Benchmark against alternatives and ensure you are not significantly slower. If you are slower, communicate why: "Our detailed analysis takes a few extra seconds but provides more accurate results." Framing manages expectations.

Speed also affects error tolerance. Users tolerate lower quality from fast systems than from slow systems. A fast system that occasionally makes mistakes is forgiven because the user can quickly retry. A slow system that makes mistakes is intolerable because retrying is costly. This creates a strategic choice: optimize for speed and accept lower quality, or optimize for quality and invest heavily in eliminating errors. The right choice depends on your domain. In entertainment applications, speed matters more. In medical applications, quality matters more. Most applications fall somewhere in between and require balancing both.

## Latency as a Competitive Moat

In mature markets where multiple products offer similar functionality, latency becomes a primary differentiator. Google won search in part because their results were not just better but also faster. Amazon won e-commerce in part because their site was faster than competitors. In AI products, where quality differences between top models are narrowing, latency will increasingly determine market leaders. A product that is ten percent better in accuracy but fifty percent slower will lose to a product that is good enough and fast.

This has strategic implications. If you are entering a competitive market, latency should be a core design constraint from day one. Do not build the most capable system and optimize latency later. Build a fast system and optimize quality within that constraint. This is counterintuitive for research-oriented teams who are trained to maximize model performance, but it is correct for product-oriented teams who must maximize user satisfaction.

Latency also compounds over sessions. A product that saves users two seconds per interaction saves twenty seconds per ten interactions, two hundred seconds per hundred interactions. Over weeks and months, this accumulates to hours of saved time. Users perceive this aggregate savings and develop loyalty to the faster product. Latency is not just about individual interactions. It is about the cumulative cost of using your product over time.

Having explored how latency affects the immediacy and accessibility of quality, we now turn to another dimension that operates across contexts and users: calibration, which measures whether the system's confidence aligns with its actual accuracy.
