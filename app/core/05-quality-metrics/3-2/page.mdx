# 3.2 â€” Designing Metrics That Drive Decisions

On July 22, 2025, the product team at a healthcare technology company with 134 employees presented their quarterly metrics review to the board. Their AI-powered symptom checker showed impressive numbers across seventeen tracked metrics: 94.3% user satisfaction, 2.1 second average response time, 87% conversation completion rate, 4.2 average interactions per session, 12% week-over-week user growth, and positive sentiment scores across all major feature categories. The board asked a single question: based on these metrics, what specifically would you do differently next quarter? The product team struggled to answer. High satisfaction suggested they should continue current approaches, but so would medium satisfaction. Fast response times indicated technical health, but they had no slower baseline for comparison. Growth was positive, but they did not know which features or behaviors drove it. After forty minutes of discussion, the team admitted that none of their seventeen metrics had actually influenced a single product decision in the previous six months.

Three weeks later, a physician using the system noticed that it consistently underestimated cardiovascular risk in women presenting with atypical chest pain symptoms, a known diagnostic bias in medical training data. The system had been exhibiting this bias for five months, during which every tracked metric remained stable and positive. None of the seventeen metrics detected the problem because none were designed to answer decision-relevant questions like "Does our system exhibit gender-based diagnostic disparities?" or "Are we missing serious conditions at different rates across demographic groups?" The metrics told the team they were fast, popular, and well-received, but not whether they were safe, equitable, or clinically sound. The company spent $340,000 on additional medical review, retrained their model with rebalanced data, and implemented new metrics specifically designed to detect demographic disparities in risk assessment. The new metrics were fewer in number but directly actionable: when gender-based assessment gaps exceeded five percentage points, the team knew to audit training data and prompt engineering for bias.

## The Vanity Metric Trap

Vanity metrics are measurements that make you feel good but do not inform action. They trend upward when things go well and downward when things go poorly, creating an illusion of insight while providing no guidance about what to do differently. Total user count, cumulative interactions, aggregate satisfaction scores, and overall accuracy percentages often function as vanity metrics because they summarize complex systems into single numbers that obscure actionable details. When your overall accuracy is 87%, you do not know whether to improve your retrieval logic, refine your prompts, add training data, or change nothing because 87% might be excellent for your use case.

The distinguishing feature of a vanity metric is that its movement does not trigger a specific decision procedure. If your metric goes up, you continue current practices. If it goes down, you... also continue current practices but with more concern? Or you investigate broadly without clear hypotheses? The metric creates awareness that something changed but provides no leverage for responding to that change. This differs fundamentally from actionable metrics where movement triggers predetermined responses: if demographic risk assessment gaps exceed five percentage points, you audit for bias. If fabrication detection flags more than 2% of outputs, you restrict model temperature and add citations. If retrieval precision drops below 0.85, you rebuild your embedding index.

Many teams accumulate vanity metrics because measurement is easier than decision-making. Adding another metric to your dashboard requires only instrumentation and visualization work, while committing to take specific actions when metrics move requires organizational alignment, resource allocation, and accountability. Tracking user growth is straightforward; deciding what you would do differently if growth slowed requires hard choices about feature priority, marketing spend, and product direction. Vanity metrics let teams appear data-driven without confronting the decision-making that data-driven actually requires.

## The Decision-Forcing Test

Every metric you track should pass a simple test: if this number changes by ten percent in either direction, what specifically would we do differently? If you cannot answer this question with concrete actions, the metric is decorative rather than functional. Your response should be specific enough that someone outside your team could execute it based on your answer alone. "We would investigate" is not specific enough. "We would increase retrieval chunk size from 512 to 1024 tokens and re-evaluate retrieval precision" is specific. "We would improve the model" is not specific. "We would add 500 adversarial examples to our training set targeting the failure mode that triggered this metric" is specific.

Applying this test to existing metrics often reveals that most of your dashboard fails it. You track response latency, and if it changes, you would "investigate performance," which is not a decision but a commitment to eventual decision-making after more analysis. You track user satisfaction, and if it drops, you would "review recent changes," which again defers the actual decision. The test exposes that these metrics create monitoring theater rather than decision support. They prove you are watching the system without proving you know what to do about what you observe.

The test also reveals when metrics are measuring the right things but lack the context needed to drive decisions. Average response latency might be actionable if you have established thresholds: below 2 seconds is acceptable, above 3 seconds requires infrastructure scaling, above 5 seconds triggers user communication about degraded service. Without these thresholds and associated actions, the metric floats in isolation, waiting for someone to decide what its values mean. Decision-forcing metric design means establishing these thresholds and actions during metric creation, not after problems emerge.

## Counterfactual Thinking in Metric Design

Designing actionable metrics requires counterfactual thinking about what you would do in worlds where your system behaves differently. If your medical symptom checker exhibited the gender-based cardiovascular risk gap you eventually discovered, what specific action would that trigger? You might commit to immediate training data audit, targeted addition of female-presenting cardiovascular cases, prompt engineering to emphasize atypical presentations, or temporary human review of all cardiovascular assessments. Each of these responses implies a different metric design. A training data audit response requires metrics that track performance by demographic group and condition type. A prompt engineering response requires metrics that compare performance across different prompt variations. A human review response requires metrics that trigger review based on condition severity and assessment confidence.

This counterfactual approach forces you to design metrics backward from decisions rather than forward from available data. You start by listing the decisions you might need to make about your system: whether to retrain, whether to adjust prompting, whether to add retrieval sources, whether to restrict certain use cases, whether to increase human oversight, whether to modify user interfaces. For each potential decision, you ask what evidence would trigger that decision and how you would measure that evidence. This creates a direct line from measurement to action that prevents metrics from floating free of organizational response capacity.

The process often reveals that you lack clear decision procedures even when you have reasonable metrics. You measure retrieval precision, but you have never decided what precision threshold would trigger rebuilding your embedding index versus adding more source documents versus changing your chunking strategy. The metric exists but connects to no decision tree. Building the decision tree in advance transforms the metric from interesting information to actionable signal. When retrieval precision drops to 0.82, you know exactly what to do because you documented the decision procedure when you designed the metric.

## Pairing Metrics With Runbooks

Actionable metrics require runbooks that document exactly what to do when metrics indicate problems. A runbook for the cardiovascular gender gap metric might specify: when the gap exceeds five percentage points, create a Jira ticket assigned to the ML lead, audit the training set for gender balance in cardiovascular cases, generate a report of all false negatives in the affected category, and schedule a review meeting within three business days. The runbook transforms the metric from a measurement into an organizational trigger that initiates a predetermined response sequence.

Writing runbooks during metric design rather than after incidents reveals which metrics you actually have capacity to act on. If you cannot write a plausible runbook specifying who would respond, what they would investigate, and what changes they might make, you probably should not track the metric. The inability to write a runbook indicates either that the metric measures something you cannot control, or that you have not thought through what different metric values should mean for your system. Either way, adding the metric to your dashboard just creates noise.

The runbook discipline also prevents metric proliferation by forcing prioritization based on response capacity rather than measurement ease. Your team might have engineering capacity to respond to three triggered metrics per week. If you track forty metrics, most of them will necessarily be response-orphaned: measurements that could indicate problems but which no one has time to address. Better to track the twelve metrics that correspond to your twelve highest-priority failure modes, write detailed runbooks for each, and ensure that metric triggers actually result in action. The untracked failure modes remain risks, but at least you are honest about which risks you have chosen to monitor and which you have chosen to accept.

## Metrics That Compare Options

Many of the most actionable metrics are comparative rather than absolute, measuring differences between system variants rather than properties of a single system. A metric showing that version A of your prompt produces 23% fewer fabricated citations than version B directly answers the question "which prompt should we deploy?" Comparative metrics eliminate the interpretation challenge that plagues absolute metrics: is 87% accuracy good? Good compared to what? Good enough for deployment? Good enough to stop iterating? Comparative metrics sidestep these questions by focusing on the decision you actually face: should we switch from our current approach to this alternative?

Running comparative evaluations requires infrastructure for maintaining multiple system variants and routing evaluation traffic across them. You need the ability to simultaneously evaluate your current production system, experimental variations, and potentially external baselines. This infrastructure investment pays returns by making every evaluation directly decision-relevant. Instead of measuring that your current system achieves 87% accuracy and leaving teams to debate whether that is good enough, you measure that your current system achieves 87% while the proposed modification achieves 91% on your failure-targeted test set. The decision becomes clear: deploy the modification.

Comparative metrics also enable progressive refinement through A/B testing in production. When you deploy system changes, you keep the previous version running in shadow mode, measuring whether the new version actually improves the metrics you care about under real usage conditions. If your new retrieval approach was supposed to reduce fabrication by 23% based on offline evaluation, but production measurement shows only 8% reduction, you have immediate feedback that something about production differs from your test conditions. This tight feedback loop between decisions and outcomes makes your metrics genuinely useful for steering system evolution.

## Leading Versus Lagging Indicators

Actionable metric design distinguishes between leading indicators that predict future problems and lagging indicators that confirm problems already occurred. Response time degradation might be a leading indicator for user churn: users experiencing slow responses today are more likely to abandon the system next week. Fabrication rates detected in automated scanning are leading indicators for user trust erosion and potential liability. These leading indicators enable preemptive action before consequences materialize. Lagging indicators like completed user churn or filed complaints confirm that problems happened but arrive too late to prevent harm.

Prioritizing leading indicators in your metric design requires understanding the causal chains linking system behaviors to ultimate outcomes. For the medical symptom checker, the ultimate outcome you care about is patient safety and appropriate care seeking. Lagging indicators of problems include patient complaints about incorrect advice or physicians reporting missed diagnoses. Leading indicators might include detecting that the system fails to recommend emergency care for high-risk symptom combinations, that it exhibits demographic performance gaps, or that it contradicts established clinical guidelines. These leading indicators let you intervene before patients receive harmful advice rather than discovering problems through adverse outcomes.

Building leading indicators often requires more sophisticated measurement than lagging indicators because you must model the relationship between current system behavior and future outcomes. Lagging indicators are straightforward: did something bad happen? Leading indicators require predicting: will something bad happen if current patterns continue? This prediction might be based on historical correlations, expert judgment about risk factors, or theoretical models of how systems degrade. The investment in leading indicators is justified by their decision-making value: they create time to act before failures occur rather than merely documenting failures after they happen.

## Metric Granularity and Aggregation Levels

Actionable metrics must be measured at the right granularity to support actual decisions. System-wide averages often obscure the patterns that drive action. Knowing that your overall accuracy is 87% does not tell you whether to focus improvement effort on medical queries versus financial questions, on complex multi-step reasoning versus simple fact lookup, or on handling ambiguous inputs versus clear requests. Measuring accuracy separately for each of these segments reveals where improvement efforts should concentrate, transforming a vague sense that "we could do better" into specific targets for engineering work.

The appropriate granularity depends on your decision-making structure. If you have separate teams working on different capability areas, you need metrics at the team-responsibility level: retrieval team metrics, generation team metrics, safety team metrics. If you make decisions about which use cases to prioritize, you need metrics by use case: customer service performance, technical support performance, sales assistance performance. If you adjust system behavior based on user expertise, you need metrics by user segment: novice user experience, expert user experience, occasional user experience. Each of these granularities enables different decisions, and your metric design should reflect the decisions you actually make.

Over-aggregation creates decision paralysis by mixing good and bad performance into mediocre averages. Your system might excel at simple factual questions while failing badly at complex reasoning tasks, but if you measure only overall accuracy, both capabilities average to "acceptable." No decision follows from "acceptable overall performance." Measuring separately reveals that simple factual accuracy is 96% while complex reasoning accuracy is 72%, immediately suggesting that complex reasoning is where improvement effort should concentrate. The disaggregated metrics drive action where aggregated metrics inspire complacency.

## Metrics That Reveal Trade-offs

The most sophisticated actionable metrics expose trade-offs between competing objectives, making visible the choices you must make about system priorities. A metric showing that increasing retrieval chunk size improves factual grounding by 12% but slows response time by 340 milliseconds reveals a speed versus accuracy trade-off that requires a decision about which matters more for your use case. Making this trade-off explicit through measurement is far more valuable than optimizing either dimension in isolation, because it surfaces the actual constraint governing your system design.

Trade-off metrics require measuring multiple dimensions simultaneously and analyzing their correlations and interactions. You cannot discover the retrieval chunk size trade-off by measuring only accuracy or only latency; you need both measurements across multiple chunk size settings to reveal the relationship. Building this multi-dimensional measurement capacity takes more infrastructure than tracking simple univariate metrics, but the decision-making value justifies the investment. Teams equipped with trade-off metrics make informed choices about system configuration, while teams with only univariate metrics optimize locally without understanding global impacts.

Some trade-offs involve tension between internal efficiency and external quality. Reducing model temperature might decrease fabrication rates by 15% while also reducing response diversity and making outputs feel more formulaic. This creates a trade-off between factual safety and user engagement. Measuring both dimensions lets you make informed choices about where on the safety-engagement spectrum your use case should operate. A medical application might prioritize safety and accept formulaic responses, while a creative writing assistant might accept more factual looseness in exchange for surprising outputs. The trade-off metric enables different applications to make different choices based on their specific requirements.

## Removing Metrics That Do Not Drive Decisions

Regular metric audits should remove measurements that have not influenced decisions in the past quarter. If you tracked user session length for three months and never once changed your system based on that measurement, session length is consuming dashboard space and cognitive attention without providing value. Removing it clarifies focus and prevents the false sense of comprehensive monitoring that comes from tracking many metrics without acting on most of them.

The removal process often encounters resistance because metrics feel like insurance: you might not need them now, but what if you need them later? This insurance logic is valid only if you have capacity to respond when the metric indicates a problem. If you lack that response capacity, the metric is not insurance but theater, creating an appearance of monitoring without the substance of action readiness. Better to acknowledge honestly which risks you have capacity to monitor actively and which you must accept or address through other means.

Metric removal also reveals dependencies and assumptions about system behavior. When you propose removing a metric, stakeholders who object often articulate for the first time what decision they think the metric enables. "We need to track session length because it might indicate engagement problems" becomes a hypothesis you can test: have engagement problems ever manifested as session length changes? If not, what would engagement problems actually look like in your data? This discussion often leads to designing a better metric that directly measures engagement quality rather than a proxy like session length, improving your measurement infrastructure by forcing clarity about what you actually need to know.

## Building Decision Literacy Alongside Metrics

Actionable metrics require decision-making capability to match measurement capability. Your team must be equipped and empowered to respond when metrics indicate problems, or the metrics become sources of anxiety rather than tools for improvement. Decision literacy means understanding what actions are available, what evidence justifies each action, and how to execute changes safely. Without this literacy, even well-designed metrics create stress: you know something is wrong because the metric says so, but you do not know what to do about it.

Building decision literacy requires documenting the action space for each metric: what can we actually change in response to this measurement? For a fabrication detection metric, the action space might include adjusting model temperature, modifying prompts to encourage citations, adding retrieval sources, increasing chunk overlap, or restricting use cases where fabrication risk is high. Documenting these options in advance means that metric triggers start decision-making processes rather than investigation spirals. The team knows the menu of responses and can quickly evaluate which is most appropriate for the specific metric movement they observed.

This decision literacy also includes understanding intervention costs and risks. Retraining your model might improve accuracy but costs three days of engineering time and introduces risk of regression on other capabilities. Adjusting prompts is faster but might have subtle effects on tone and formatting that require evaluation. Restricting use cases improves safety but reduces user value. Making these trade-offs explicit in your runbooks means that metric-driven decisions account for both benefits and costs rather than reflexively pursuing metric improvement regardless of price.

## From Measurement to Action

The transition from vanity metrics to actionable metrics requires organizational changes beyond measurement infrastructure. You need decision-making processes that consume metric signals and produce concrete actions. You need accountability for responding when metrics indicate problems. You need resource allocation that prioritizes metric-driven improvements over feature development when quality issues emerge. Without these organizational supports, even perfectly designed actionable metrics become decorative because no one has mandate or capacity to act on them.

The healthcare company's experience illustrates this transition. Their initial seventeen metrics were individually reasonable measurements of system properties, but collectively they created measurement theater rather than decision support. The shift to fewer, more focused metrics based on specific clinical risks required not just new instrumentation but new organizational processes: monthly reviews of demographic disparity metrics with mandatory action items when thresholds were exceeded, dedicated engineering time for addressing metric-identified issues, and clear escalation paths when metrics indicated patient safety concerns. The metrics became actionable not just through better design but through organizational commitment to acting on what they measured.

Your metric design methodology must bridge measurement and action by ensuring every metric you track has a clear owner, a documented decision procedure, and organizational capacity to respond. Metrics without these supports accumulate as dashboard clutter, creating the appearance of rigor while leaving actual decision-making to intuition and reaction. The question is not whether you are measuring enough, but whether what you measure actually changes what you do, and that question requires examining not just your metrics but the decision-making infrastructure that surrounds them.

Understanding what makes metrics actionable prepares you for the challenge of establishing the human baselines that give automated metrics meaning and set the ceiling for what you can expect machines to measure reliably.
