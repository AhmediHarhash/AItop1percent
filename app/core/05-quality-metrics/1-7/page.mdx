# 1.7 â€” Why Averages Hide Catastrophic Failures

In early 2025, a multinational e-commerce company deployed a product recommendation system powered by GPT-4o with retrieval augmented generation across their catalog of 14 million products in 23 countries. The system analyzed customer browsing history, purchase patterns, and natural language queries to suggest products. The product team ran extensive offline evaluation before launch, measuring recommendation accuracy against held-out test data. The system achieved 94.3 percent accuracy overall, substantially exceeding their 90 percent threshold for deployment. Based on this strong performance, they rolled out the system to 180 million active users worldwide.

Within two weeks, customer complaints spiked in three countries: Nigeria, Indonesia, and Egypt. Users reported receiving nonsensical product recommendations that ignored their queries and browsing history. The team investigated and discovered a shocking pattern. While overall accuracy was indeed 94.3 percent, accuracy varied dramatically by country and language. For users in the United States, United Kingdom, and Germany, accuracy reached 97.8 percent. For users in Nigeria, Indonesia, and Egypt, accuracy dropped to 41 percent, 38 percent, and 44 percent respectively. The 94.3 percent average was mathematically correct but operationally meaningless: it combined excellent performance on 85 percent of users with catastrophic failure for 15 percent of users, then reported a number that suggested universal success.

The team had violated a fundamental principle of quality measurement: they had averaged across non-homogeneous populations without first disaggregating to understand variance. The 94.3 percent figure was not wrong, but it was deceptive. It answered the question "what is the average accuracy across all users" when the question that actually mattered was "what is the worst accuracy experienced by any substantial user segment." The average hid the catastrophe in the tail. This is not a statistical anomaly or an edge case. This is the central challenge of AI quality measurement in systems that serve diverse populations.

## The Mathematics of Deceptive Averages

When you compute an average across populations with heterogeneous performance, the average is dominated by the largest populations and the highest-performing segments. For the e-commerce recommendation system, users in high-income English-speaking countries represented 85 percent of the user base and achieved 97.8 percent accuracy. Users in lower-income countries with less training data representation represented 15 percent of the user base and achieved 40 percent accuracy. The weighted average is 0.85 times 97.8 percent plus 0.15 times 40 percent, which equals 83.1 plus 6.0, totaling 89.1 percent.

The actual reported number was higher at 94.3 percent because the team's test set was not representative of production usage. The test set oversampled high-resource languages where labeled data was abundant and undersampled low-resource languages where labeled data was scarce. This sampling bias inflated the average accuracy by overweighting the populations where the system performed well and underweighting the populations where it failed. The combination of averaging across heterogeneous populations and biased sampling produced a metric that bore little relationship to real-world quality.

Even with a perfectly representative test set, the average would still hide the critical information. Knowing that average accuracy is 89 percent tells you nothing about the range of experiences your users have. It does not tell you that 15 percent of users receive effectively random recommendations. It does not tell you which user segments fail or why. It does not tell you whether the failures are evenly distributed or concentrated in vulnerable populations. The average collapses a complex distribution into a single number that erases the very information you need to ensure equitable and reliable service.

## Why Worst-Case Performance Defines Acceptable Risk

For many AI systems, the worst-case performance across user segments defines acceptable risk rather than the average performance. A hiring screening tool with 93 percent accuracy on average but 68 percent accuracy for candidates from historically underrepresented groups creates discriminatory outcomes that violate employment law and ethical norms. A medical diagnosis system with 96 percent accuracy on common conditions but 71 percent accuracy on rare diseases fails the patients who need help most. A content moderation system with 98 percent accuracy on spam but 79 percent accuracy on hate speech fails to protect vulnerable users from harm.

In each case, the average accuracy might meet your deployment threshold, but the system is nevertheless unacceptable because it fails catastrophically for subpopulations that matter. The EU AI Act recognizes this principle by requiring high-risk AI systems to measure and report performance across demographic segments and to avoid discriminatory outcomes. Article 10 specifically mandates that training data be relevant, representative, and free from bias, acknowledging that systems optimized for average performance often fail on underrepresented groups. The regulation codifies what should already be obvious: you cannot average away your obligation to serve all users.

The obligation to measure worst-case performance applies even when the worst-performing segments are small. A fraud detection system that achieves 97 percent precision overall but only 83 percent precision on transactions from users in rural areas harms a minority of users, but those users bear the full cost of the system's failure. They experience higher false positive rates, more frozen accounts, more declined legitimate transactions. The fact that they are a minority does not reduce the harm they experience. It merely makes the harm easier to hide in aggregated metrics.

## The Mechanics of Disaggregation

**Disaggregation** means measuring quality separately for each meaningful subpopulation before computing any aggregates. For user-facing systems, you disaggregate by demographics like age, gender, location, language, and socioeconomic indicators. For task-specific systems, you disaggregate by input characteristics like query complexity, document length, domain, and edge case categories. For safety-critical systems, you disaggregate by risk level, consequence severity, and failure mode.

The e-commerce recommendation system should have disaggregated accuracy by country, language, product category, user tenure, and price tier before computing any averages. This would have immediately revealed the catastrophic performance in Nigeria, Indonesia, and Egypt. It would have shown that performance degraded for product categories with sparse training data. It would have revealed that new users received worse recommendations than established users because the system had less behavioral history to work with. Each of these insights was hidden in the 94.3 percent average.

Disaggregation requires defining the segments that matter for your domain. For healthcare systems, you might disaggregate by age, gender, race, primary language, insurance status, and medical condition. For financial systems, you might disaggregate by income level, credit history, geographic region, and transaction type. For content recommendation systems, you might disaggregate by user demographics, content category, language, and cultural region. The right segments are those where you expect performance to vary and where variation creates harm or inequity.

## Measuring Tail Performance with Percentiles

Beyond disaggregation by categorical segments, you must also measure tail performance within segments using percentile metrics. **Tail performance** refers to the experience of your worst-served users, typically measured as p95, p99, or p999 percentile latency, accuracy, or error rate. For latency, p99 latency tells you the experience of your slowest 1 percent of requests. For accuracy, measuring accuracy on your bottom 5 percent of users by any relevant dimension tells you whether your system has catastrophic failure modes that averages hide.

Suppose your customer service chatbot achieves 88 percent resolution rate on average, meaning 88 percent of users report their issue was resolved. This sounds acceptable until you measure tail performance and discover that p95 conversation length is 47 messages compared to a median of 8 messages. The users in the tail are having dramatically worse experiences, requiring six times as many conversational turns to reach resolution or abandoning in frustration. The average resolution rate hides the fact that 5 percent of users receive effectively broken service.

Tail performance often reveals technical problems that affect small percentages of users but signal serious quality issues. If p99 latency is ten times higher than p50 latency, you likely have infrastructure bottlenecks, database query inefficiencies, or retry loops that affect a minority of requests but indicate systemic fragility. If accuracy on your bottom 10 percent of users by confidence score is dramatically lower than accuracy on high-confidence predictions, you likely have model calibration problems or unhandled edge cases. Tail metrics surface the problems that averages smooth over.

## The Obligation to Report Disaggregated Metrics

Once you have disaggregated your quality metrics and measured tail performance, you have an ethical and often legal obligation to report those disaggregated metrics rather than hiding them behind averages. The EU AI Act requires documentation of performance across subgroups for high-risk systems. The U.S. Equal Employment Opportunity Commission requires adverse impact analysis for hiring tools, which amounts to disaggregating performance by protected categories. Even outside regulated domains, users and customers deserve to know whether a system serves all populations equitably or fails subgroups while maintaining acceptable averages.

Reporting disaggregated metrics requires acknowledging uncomfortable truths. You must admit that your system performs far worse for some users than others. You must quantify the magnitude of performance gaps across demographic groups. You must document which populations experience the worst service. This transparency creates accountability but also creates pressure to fix the disparities rather than hiding them. The discomfort of disclosure is precisely the point: if you are unwilling to report disaggregated performance, you probably should not deploy the system.

The practice of reporting only aggregated metrics is often a deliberate choice to obscure poor performance on minority populations. When a hiring tool vendor reports 91 percent accuracy without breaking down performance by race or gender, they are choosing to hide information that stakeholders need to assess fairness. When a medical device manufacturer reports 94 percent sensitivity without disaggregating by patient demographics, they are choosing to obscure disparities that affect health outcomes. The choice to aggregate without disaggregating is rarely innocent. It is usually a strategy to obtain approval while avoiding scrutiny of subgroup performance.

## Building Disaggregation into Your Workflow

Disaggregation cannot be an afterthought. It must be built into your evaluation workflow from the beginning. This means maintaining metadata about your evaluation data that lets you slice performance by relevant dimensions. For user-facing systems, every evaluation example needs demographic tags, language indicators, and task difficulty labels. For domain-specific systems, every example needs tags for edge case categories, input complexity, and risk level. Without this metadata, you cannot disaggregate even if you want to.

The e-commerce company rebuilt their evaluation pipeline with mandatory disaggregation. Every recommendation example in their test set was tagged with user country, language, product category, price tier, and user tenure. Their evaluation reports showed accuracy for each segment separately before showing any average. They established minimum performance thresholds for every segment: no segment could fall below 85 percent accuracy regardless of overall average performance. They instrumented their production system to track disaggregated metrics in real-time, alerting when any segment's performance degraded below threshold.

This approach revealed problems that aggregated metrics would have hidden. When they tested GPT-4.5 as a potential upgrade from GPT-4o, overall accuracy improved from 94.3 percent to 95.7 percent, suggesting the upgrade was beneficial. But disaggregated metrics showed that accuracy for Nigerian users actually declined from 41 percent to 38 percent while accuracy for U.S. users improved from 97.8 percent to 98.9 percent. The upgrade made the disparity worse even as it improved the average. Without disaggregation, they would have deployed a system that harmed the users who were already worst-served.

## The Statistical Power Problem

Disaggregation introduces a statistical challenge: as you divide your data into smaller segments, you reduce the sample size available for measuring performance in each segment, which increases measurement uncertainty. If your test set contains 10,000 examples and you disaggregate by 20 countries, you have an average of 500 examples per country. Some countries might have far fewer examples, making accuracy estimates unreliable. You might observe apparent performance differences that are actually statistical noise.

The solution is not to avoid disaggregation but to ensure adequate sample sizes for every segment you care about. This often requires oversampling minority populations to achieve statistical power in tail segments. The e-commerce company's test set contained 200,000 examples total, with intentional oversampling to ensure at least 5,000 examples for every country even when that country represented less than 1 percent of production traffic. This oversampling ensured they could reliably measure performance in every segment they cared about.

When you cannot obtain large enough samples for some segments, you must acknowledge measurement uncertainty and use confidence intervals rather than point estimates. Reporting that accuracy for Egyptian users is 44 percent with a 95 percent confidence interval from 39 percent to 49 percent is more honest than reporting 44 percent as if it were precise. The uncertainty does not eliminate the obligation to measure and report disaggregated performance. It merely requires you to quantify and communicate the limitations of your measurements.

## The Interaction Between Disaggregation and Optimization

Disaggregation also affects how you optimize systems. When you optimize for average performance, you create incentives to improve performance on large, high-performing segments because they dominate the average. When you optimize for worst-segment performance, you create incentives to improve performance on small, low-performing segments because they define the minimum. These optimization objectives often conflict: interventions that improve average performance often hurt worst-case performance by further optimizing for majority populations.

The e-commerce team found this conflict when tuning their retrieval system. Increasing the number of retrieved documents from ten to twenty improved average recommendation accuracy by 1.4 percentage points but increased latency, and the latency increase was much larger for users in regions with poor internet connectivity. For U.S. users, p95 latency increased from 890 milliseconds to 1,100 milliseconds, an acceptable change. For Nigerian users, p95 latency increased from 3,200 milliseconds to 7,800 milliseconds, making the system nearly unusable. Optimizing for average accuracy degraded tail latency for the users who already had the worst experience.

Addressing this conflict requires multi-objective optimization that balances average performance, worst-case performance, and fairness across segments. You might set minimum performance thresholds for every segment while optimizing average performance, rejecting any change that improves the average while violating minimum thresholds. You might weight segments equally rather than by size, removing the incentive to over-optimize for large segments. You might optimize for p95 performance rather than average performance, focusing improvement efforts on tail experiences rather than typical experiences.

## The Tyranny of the Majority

The pattern of averaging across heterogeneous populations creates a tyranny of the majority where systems are optimized for large, well-represented populations at the expense of small, underrepresented populations. Training data is more abundant for wealthy countries, so models perform better for wealthy countries. Evaluation benchmarks overrepresent high-resource languages, so models optimize for those languages. Product teams focus on segments with the most users, so features and performance improvements benefit majority populations.

This tyranny is not inevitable. It results from choices about what to measure and what to optimize. When you measure and report only averages, you make it easy to ignore minority populations. When you optimize for averages, you create incentives to sacrifice minority performance for majority gains. When you set deployment thresholds based on overall performance without minimum segment-level requirements, you permit systems that fail catastrophically for minorities while serving majorities well.

Breaking the tyranny requires institutional commitment to disaggregation and equity. It requires measuring performance for every population you serve and refusing to deploy systems that fail any population, even if overall averages are acceptable. It requires investing in data collection and model improvement for underrepresented populations even when that investment does not maximize average performance. It requires building evaluation infrastructure that makes disaggregation automatic rather than optional. Most fundamentally, it requires acknowledging that average performance is a nearly meaningless metric when population heterogeneity is large.

## The Path Forward

The e-commerce company's experience illustrates a general principle: in systems that serve diverse populations, disaggregated metrics are not optional. They are the only honest way to measure quality. Averages are deceptive when variance is large, hiding catastrophic failures in minority populations while suggesting universal success. Your obligation is to disaggregate before you aggregate, to measure and report performance for every meaningful subpopulation, to set minimum performance thresholds for every segment, and to refuse to deploy systems that meet average thresholds while failing tail segments.

This approach requires more work than computing simple averages. It requires metadata tagging, oversampling minority populations, statistical power analysis, and multi-objective optimization. It requires acknowledging and fixing performance disparities rather than hiding them in aggregates. But this work is not optional if you take seriously the obligation to build systems that serve all users rather than just the majority. The discomfort of disaggregation is a feature, not a bug. It forces you to confront the quality problems that averaging lets you ignore.

The techniques of disaggregation and tail performance measurement form the foundation for understanding whether your AI system truly works for everyone it claims to serve. But knowing that performance varies across populations raises a deeper question: how do you design evaluation strategies that catch these disparities before deployment rather than after user harm occurs, and how do you build continuous monitoring systems that detect emerging disparities in production before they accumulate into crises. Those questions bring us to the challenge of designing evaluation protocols that match the complexity and stakes of production AI systems.
