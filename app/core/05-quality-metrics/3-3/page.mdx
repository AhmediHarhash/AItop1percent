# 3.3 â€” Human Judgment Baselines and Inter-Annotator Agreement

On September 8, 2025, an e-commerce company with 203 employees launched an automated product review summarization system after six months of development. The engineering team had optimized the system to achieve 89% agreement with a reference dataset of human-written summaries, measuring similarity using semantic embedding distance and ROUGE scores. Three weeks after launch, customer support began receiving complaints that review summaries were highlighting minor product flaws while burying critical defects, making low-quality products appear better than they were. When the team investigated, they discovered that their reference dataset had been created by a single contractor who wrote summaries emphasizing positive aspects to avoid potential legal issues with negative characterizations. The 89% agreement with this reference set meant the system had successfully learned to replicate one person's legally conservative summarization style rather than producing summaries that actual customers would find helpful.

The team had never measured how different humans would summarize the same reviews, so they had no baseline for what agreement was achievable or what variation was acceptable. They belatedly commissioned an inter-annotator agreement study where five customer service representatives independently summarized fifty product reviews. The representatives agreed with each other only 71% of the time on which product attributes to emphasize, 63% on severity characterizations, and 82% on overall sentiment. These inter-annotator agreement numbers revealed that the task of review summarization involved significant subjective judgment even among trained evaluators, and that their automated system's 89% agreement with a single annotator was meaningless without understanding the baseline human agreement rate. More critically, the study revealed that the original contractor's legally conservative style was an outlier; the five customer service representatives consistently highlighted safety and functionality issues more prominently, reflecting what customers actually needed to make purchase decisions. The system had been optimized against the wrong target.

## The Human Ceiling Principle

Before you build automated metrics for any AI system, you must establish what humans can reliably measure, because human measurement reliability sets the ceiling for automated measurement. If expert humans agree on a judgment only 75% of the time, no automated metric will reliably exceed 75% agreement with a true ideal unless it is measuring something different than humans measure. This principle applies across all quality dimensions: factual accuracy, helpfulness, toxicity, bias, fluency, relevance. Each dimension has a human agreement ceiling determined by how subjective or objective the judgment is, how clear the evaluation criteria are, and how much expertise the judgment requires.

Understanding these ceilings prevents two common mistakes in automated metric design. First, it prevents you from setting unrealistic targets for automated metrics. If your human annotators agree only 80% of the time on whether AI-generated medical advice is appropriate, expecting your automated metric to achieve 95% accuracy implies that your automation is measuring something different than your humans measured, usually something easier but less relevant. Second, it prevents you from treating human judgments as ground truth when they are actually noisy estimates. If different doctors disagree 30% of the time on whether a symptom assessment warrants emergency care, treating any single doctor's judgment as the correct answer introduces error into your evaluation dataset.

The human ceiling varies dramatically across tasks and domains. Factual accuracy judgments in domains with clear right answers like arithmetic or simple fact checking might achieve 95-98% human agreement. Helpfulness judgments for open-ended questions might achieve only 65-75% agreement. Toxicity and bias judgments often fall between these extremes, with agreement rates depending heavily on how explicitly the evaluation criteria are defined. Measuring these ceilings for your specific task with your specific evaluators is not optional background research but a foundational requirement for valid metric design.

## Designing Inter-Annotator Agreement Studies

A proper inter-annotator agreement study requires multiple annotators independently evaluating the same examples using the same rubric, allowing you to measure how often they agree and to analyze patterns in their disagreements. The study design must address several critical choices about sample size, annotator selection, rubric specificity, and agreement metrics. Each choice affects both the cost of the study and the validity of conclusions you can draw about human baseline performance.

Sample size depends on the complexity of your task and the granularity of judgments you need to measure. For coarse judgments like overall quality ratings on a five-point scale, fifty to one hundred examples evaluated by three to five annotators often suffices to establish baseline agreement rates. For finer-grained judgments like identifying specific error types or rating multiple dimensions independently, you need larger samples to ensure each judgment category appears frequently enough to measure reliably. The e-commerce team's fifty reviews would have been adequate for measuring overall sentiment agreement but was too small for measuring agreement on specific product attribute mentions.

Annotator selection determines whose judgments define your baseline and therefore whose perspective your automated metrics will be compared against. Using domain experts as annotators means your baseline reflects expert judgment, which might differ systematically from end-user judgment. Using end users means your baseline reflects actual user needs but might include more noise from inexperienced evaluators. Using a mix lets you measure expert-user agreement gaps, revealing whether the task requires expertise or whether untrained users can evaluate quality reliably. The e-commerce team's decision to use customer service representatives was appropriate because these representatives regularly helped customers interpret reviews, making them proxies for informed user judgment.

## Rubric Design and Evaluation Protocols

The evaluation rubric given to annotators determines how much of the judgment task is explicit versus implicit, directly affecting agreement rates. A vague rubric like "rate the helpfulness of this response on a scale of one to five" leaves annotators to infer what helpfulness means, guaranteeing wide variation in judgments. A detailed rubric that operationalizes helpfulness as "provides actionable information relevant to the question, uses clear language, includes appropriate caveats, and avoids unsupported claims" gives annotators concrete criteria, typically increasing agreement at the cost of measuring only what the rubric explicitly defines.

The trade-off between rubric specificity and judgment comprehensiveness is fundamental to annotation study design. Highly specific rubrics increase agreement but might miss important quality dimensions that are hard to operationalize. Vague rubrics capture holistic human judgment but produce noisy measurements. Most effective rubrics strike a middle ground by explicitly defining key terms and providing canonical examples while leaving room for evaluator judgment on cases that fall between clear categories. A toxicity evaluation rubric might explicitly define severe toxicity as "content that threatens, dehumanizes, or advocates violence against specific individuals or groups" while leaving moderate toxicity for evaluator interpretation based on context.

Evaluation protocols should include annotator training, calibration exercises, and ongoing quality checks. Training ensures all annotators understand the rubric the same way and have seen canonical examples of each quality level. Calibration involves having all annotators evaluate a small shared set and discussing disagreements to align interpretations before beginning the main study. Quality checks during annotation catch annotator drift, where individuals' judgment criteria slowly shift over time, and annotator fatigue, where quality deteriorates as the task becomes repetitive. The e-commerce study would have benefited from calibration discussions to align the five representatives on how to weight safety issues versus cosmetic flaws in summaries.

## Measuring Agreement: Kappa and Beyond

Raw agreement percentages are necessary but insufficient for characterizing inter-annotator reliability because they do not account for chance agreement. If you ask annotators to classify responses as helpful or unhelpful and your dataset contains 90% helpful responses, annotators who randomly guess "helpful" every time will achieve 90% raw agreement with each other purely by chance. Cohen's kappa and its variants correct for this chance agreement, providing a more rigorous measure of whether annotators agree more than random guessing would predict.

Kappa values range from negative one to positive one, where values below zero indicate agreement worse than chance, zero indicates agreement at chance level, 0.4 to 0.6 indicates moderate agreement, 0.6 to 0.8 indicates substantial agreement, and above 0.8 indicates nearly perfect agreement. These thresholds provide rough guidance, but interpretation depends on task difficulty and consequence severity. Kappa of 0.6 might be acceptable for subjective helpfulness ratings but concerning for medical diagnosis judgments where you need high reliability. The e-commerce team's 71% raw agreement on attribute emphasis might correspond to kappa around 0.55 to 0.65 depending on attribute frequency distribution, indicating moderate but not strong agreement.

Beyond simple kappa, you should analyze disagreement patterns to understand what makes evaluation difficult. Do annotators disagree randomly across all examples, or do disagreements cluster on specific cases? Clustering suggests that certain examples are genuinely ambiguous or that your rubric needs refinement for edge cases. Do certain annotator pairs agree more than others? Systematic annotator differences might indicate that your evaluators have different expertise levels or different interpretations of the rubric. Are disagreements typically small (adjacent ratings on a scale) or large (opposite ends of the scale)? Large disagreements suggest fundamental task ambiguity while small disagreements might be acceptable noise.

## Using Human Baselines to Calibrate Automated Metrics

Once you have measured human agreement rates, you use these baselines to calibrate what you should expect from automated metrics. If expert humans agree on factual accuracy judgments 92% of the time with kappa of 0.85, an automated fact-checking metric that achieves 88% agreement with expert judgments is performing near the human ceiling and probably cannot improve much without fundamentally different approaches. If humans agree only 68% of the time on helpfulness ratings with kappa of 0.48, an automated metric achieving 72% agreement might actually be performing suspiciously well, suggesting it is measuring something more objective than the holistic helpfulness judgment humans are making.

This calibration prevents you from over-optimizing automated metrics in ways that divorce them from human judgment. If your inter-annotator agreement study showed that humans disagree frequently on nuanced cases but agree on clear-cut cases, your automated metric should exhibit similar patterns. A metric that confidently assigns scores to the nuanced cases where humans disagree is probably not capturing the genuine ambiguity that humans perceive. Better to design metrics that express uncertainty on ambiguous cases, mirroring human judgment distributions rather than falsely resolving inherent ambiguity.

The calibration also guides where to invest in better human evaluation versus where to accept disagreement as inherent to the task. If low inter-annotator agreement stems from vague rubrics or insufficient training, you can improve measurement by refining evaluation protocols. If low agreement persists despite clear rubrics and trained evaluators, the task itself involves irreducible subjective judgment, and you should design your automated metrics and your system monitoring to account for this subjectivity rather than pursuing illusory objectivity.

## Building Gold Standard Datasets Through Consensus

When human agreement is imperfect, creating ground truth evaluation datasets requires deciding how to resolve disagreements. Common approaches include majority voting, where the most frequent judgment becomes the gold label; expert arbitration, where a senior evaluator reviews disagreements and makes final calls; and consensus discussion, where annotators discuss disagreements until reaching agreement. Each approach has different costs, biases, and appropriateness depending on your task.

Majority voting is efficient and works well when most disagreements are random noise rather than systematic differences in valid perspectives. If three annotators rate helpfulness as 4, 4, and 3 on a five-point scale, majority vote produces 4 with reasonable confidence that this represents consensus. But majority voting can suppress minority perspectives that might be valuable, especially on subjective judgments where multiple valid interpretations exist. If two annotators think a response is politically biased while one thinks it is neutral, majority vote labels it biased, potentially enshrining the majority's political sensitivities as objective truth.

Expert arbitration acknowledges that some judgments require more expertise than others and that not all disagreements represent equally valid perspectives. For medical accuracy evaluation, having a senior physician review disagreements between junior evaluators makes sense because medical accuracy is objective even when evaluation is difficult. For helpfulness or style judgments, expert arbitration is more problematic because it privileges one person's subjective preferences over others'. The e-commerce team might use senior customer service representatives to arbitrate factual disagreements about product attributes while accepting that summary emphasis choices involve legitimate variation in judgment.

Consensus discussion is expensive but produces the highest quality labels by forcing annotators to articulate their reasoning and to consider alternative perspectives. When annotators disagree on whether a medical response is appropriate, discussing the disagreement often reveals that they were weighing different risk factors or interpreting the question differently. This discussion produces not just a consensus label but shared understanding of what makes the judgment difficult, which often suggests rubric refinements that prevent similar disagreements on future examples. The cost is that consensus discussion does not scale beyond small critical datasets.

## Iterative Rubric Refinement Based on Disagreements

Analyzing disagreement patterns should feed back into rubric refinement, creating an iterative process where evaluation criteria become increasingly precise and aligned with the quality dimensions you actually care about. If annotators frequently disagree on whether responses are "concise," the disagreement suggests that concision means different things to different evaluators. Some might focus on word count, others on information density, others on directness of addressing the question. Refining the rubric to explicitly define concision as "addresses the question in fewer than 150 words without omitting critical information" reduces this source of disagreement, though it also changes what you are measuring from holistic concision to a specific word-count-bounded completeness.

This refinement process makes trade-offs between measurement reliability and measurement validity visible. You can always increase inter-annotator agreement by making rubrics more mechanical and objective, but mechanical objectivity sometimes misses the subtle qualities that matter for real system performance. Measuring response length is perfectly reliable but captures only part of what makes communication concise. Measuring holistic concision captures more of what matters but with lower reliability. Your refinement process must balance these competing pressures based on whether you need reliable measurement of narrow dimensions or noisy measurement of broad qualities.

The refinement process also reveals when you are conflating multiple distinct quality dimensions into single judgments. If annotators frequently disagree on overall response quality, breaking this judgment into separate ratings for accuracy, helpfulness, concision, and tone often increases agreement on each component while revealing that overall quality is not a single dimension but a composite. This decomposition makes your evaluation more actionable by showing which specific dimensions need improvement rather than providing vague signals that "quality is low."

## Accounting for Subjectivity in System Design

When inter-annotator agreement reveals that human judgment on your task is inherently subjective, your metric design and system operation should embrace this subjectivity rather than pretend it does not exist. Instead of trying to find the single correct answer to subjective questions, you should measure whether your system's answers fall within the range of reasonable human judgment. For the e-commerce review summarization task, where humans agreed only 71% on attribute emphasis, success might be defined as producing summaries that at least three of five evaluators consider acceptable rather than matching a single gold standard summary.

This acceptance-range approach changes how you evaluate system outputs and how you present results to users. Evaluation becomes less about matching a ground truth label and more about ensuring outputs would receive acceptable ratings from most human evaluators. User-facing interfaces might acknowledge subjectivity by offering multiple summary perspectives or by labeling summaries as emphasizing particular aspects rather than claiming objective comprehensiveness. These design choices align system behavior with the underlying reality that review summarization involves judgment calls about what matters most.

Some teams resist acknowledging subjectivity because it complicates the engineering story about system performance. Claiming 89% accuracy against gold labels sounds more definitive than explaining that your summaries fall within acceptable human judgment ranges 76% of the time. But false definitiveness creates brittleness. When the e-commerce system launched, customers immediately noticed that summaries felt off because the system was optimized against an outlier annotator's style. A system designed around the documented range of reasonable human judgment would have been more robust to individual variation and more aligned with actual customer needs.

## Specialized Considerations for Domain Expertise

Tasks requiring domain expertise introduce additional complexity into human baseline measurement because expert agreement might differ from non-expert agreement, and the appropriate baseline depends on who will use your system. Medical diagnosis AI should be evaluated against expert physician agreement, not general public agreement, because physicians have the expertise to judge medical appropriateness. But conversational AI for customer service might be better evaluated against customer agreement than expert linguist agreement, because customer perception determines system success regardless of what linguistic experts think.

When expertise matters, your inter-annotator agreement study should measure both expert agreement and expert versus non-expert agreement. Do experts agree more than non-experts, suggesting that training improves reliability? Do experts agree with non-experts, suggesting that the judgment is intuitive even without training? Or do experts systematically judge differently than non-experts, suggesting that expertise changes evaluation criteria? These patterns determine whether your automated metrics should target expert judgment, non-expert judgment, or some combination.

Expert disagreements often cluster on edge cases that require specialized knowledge or that exist in genuinely ambiguous spaces between established categories. Two cardiologists might disagree on whether subtle ECG changes warrant immediate intervention or watchful waiting, not because one is wrong but because medical judgment involves weighing probabilities under uncertainty. Your metric design for such tasks must account for this irreducible uncertainty, potentially by having your automated metrics flag these edge cases for human review rather than attempting to force definitive classifications where even experts reasonably disagree.

## Dynamic Baselines for Evolving Tasks

Human judgment baselines are not static, especially for tasks where norms and standards evolve over time. Content moderation provides a clear example: what human evaluators considered acceptable language in 2020 might differ from judgments in 2026 as social norms shift and platform policies change. Inter-annotator agreement measured two years ago might not reflect current judgment patterns, meaning your automated metrics calibrated against old baselines could be measuring against outdated human standards.

Maintaining current human baselines requires periodic re-measurement, typically annually or when significant changes occur in your domain, your user population, or your evaluation criteria. These updates capture drift in human judgment and ensure your automated metrics remain aligned with current human perspectives. The updates also provide opportunities to refine rubrics based on accumulated experience about which quality dimensions matter most and which judgment disagreements reflect genuine task ambiguity versus resolvable evaluation problems.

For rapidly evolving tasks, you might maintain rolling human evaluation alongside automated metrics, continuously collecting human judgments on subsamples of production outputs. This parallel evaluation tracks whether automated metrics remain aligned with human judgment over time and provides early warning when automated scores begin diverging from human ratings. The divergence might indicate that your automated metric is becoming stale, that human judgment is shifting, or that your system behavior is changing in ways that confound your metrics. Without ongoing human baselines, you would not detect these alignment failures until system problems became visible through user complaints or incidents.

## From Human Measurement to Machine Measurement

Establishing human baselines is not an academic exercise but a practical foundation for building automated metrics that actually measure what you care about. The e-commerce team's failure to conduct inter-annotator agreement studies before building their automated summarization metrics led them to optimize against an unrepresentative baseline, producing a system that satisfied their metrics while frustrating users. The subsequent inter-annotator agreement study revealed both that review summarization involved significant subjective judgment and that their original reference annotator was an outlier, providing the insights needed to rebuild their evaluation approach around the actual range of acceptable human judgment.

Your metric design process should begin with human measurement, proceed to automated metric development only after establishing what humans can reliably measure, and regularly validate that automated metrics remain aligned with human judgment as both systems and standards evolve. This human-centered foundation ensures that your metrics measure qualities that matter to actual users rather than artifacts that are easy to compute, and that you understand the ceiling constraints that limit how accurate any automated measurement can become.

With human baselines established, you face the question of whether to use AI systems themselves as evaluators, a technique that offers scalability but introduces meta-level evaluation challenges that require careful design to avoid systematic failures.
