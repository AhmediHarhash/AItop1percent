# 4.9 â€” Summarization and Content Generation Metrics

On July 22, 2025, a legal technology company launched a contract summarization tool for mid-sized law firms. The system used GPT-4o to generate one-page summaries of commercial contracts, highlighting key terms, obligations, and risk factors. During development, the team evaluated their system using ROUGE scores, a standard summarization metric that measures n-gram overlap between generated summaries and reference summaries. Their model achieved ROUGE-1 scores of 0.58, ROUGE-2 scores of 0.41, and ROUGE-L scores of 0.53 on a test set of 2,000 annotated contracts. These numbers exceeded published benchmarks for legal document summarization. The team celebrated and deployed to fourteen law firms representing 340 attorneys.

Within six weeks, the firms began reporting problems. The summaries were fluent and well-structured, but attorneys noticed critical omissions and subtle mischaracterizations. A summary of a licensing agreement stated that the licensee could "use the software for internal purposes" but omitted a crucial clause limiting use to a single geographic region, a restriction that determined whether the contract met the client's needs. Another summary described a termination clause as "standard 30-day notice" when the actual contract required 30 business days and excluded certain holiday periods, a difference that shifted the effective notice period from six weeks to ten weeks. The attorneys began manually reviewing full contracts rather than trusting the summaries, eliminating the efficiency gains the tool promised. By September, eleven of the fourteen firms had canceled their subscriptions. The post-mortem revealed that ROUGE scores measured surface-level textual overlap but missed the dimensions that mattered: whether summaries preserved legally significant details, whether they characterized clauses accurately, and whether they highlighted the information attorneys needed for decision-making. The company had optimized for the wrong metric entirely. They spent the next eight months rebuilding their evaluation framework around human attorney judgments, burning through 1.2 million dollars in runway and losing their early market momentum.

## The Collapse of ROUGE as a Quality Signal

ROUGE scores dominated summarization evaluation from their introduction in 2004 through the early 2020s. The metric compares n-grams in the generated summary to n-grams in one or more reference summaries. ROUGE-1 measures unigram overlap, ROUGE-2 measures bigram overlap, and ROUGE-L measures longest common subsequence. A ROUGE-1 score of 0.58 means that 58 percent of the unigrams in the reference summary appear in the generated summary, or vice versa depending on whether you compute recall, precision, or F1. For extractive summarization systems that select sentences from the source document, ROUGE provided reasonable signal: higher overlap with human-written summaries generally indicated better summaries.

Large language models broke the assumptions underlying ROUGE. Modern LLMs generate abstractive summaries that paraphrase and restructure information rather than extracting sentences verbatim. A GPT-4o summary might convey the same information as a reference summary using completely different words and sentence structures, yielding low ROUGE scores despite high quality. Conversely, a summary can achieve high ROUGE scores by copying many phrases from the reference while missing key information or including subtle errors. The legal tech company's experience demonstrated this failure mode: their summaries scored well on ROUGE because they used legal terminology and phrasing similar to the reference summaries, but they omitted details and nuances that ROUGE could not measure.

**Faithfulness** measures whether the summary's claims are supported by the source document. A faithful summary makes no statements that contradict or extend beyond what the source material states. The legal licensing summary that omitted the geographic restriction was unfaithful by omission: it stated something true but incomplete in a way that misled readers. Measuring faithfulness requires comparing each claim in the summary back to the source document to verify support. Early faithfulness evaluation used simple word overlap methods, which failed for the same reasons ROUGE failed. Current approaches in 2026 use LLMs as evaluators: you prompt GPT-4o or Claude 4 to read the source document and the summary, then identify any claims in the summary that lack support in the source. This LLM-as-judge approach achieves 0.79 to 0.87 correlation with human faithfulness ratings depending on prompt design and domain.

**Informativeness** measures whether the summary captures the most important information from the source. A summary might be perfectly faithful but miss critical details. The challenge is defining importance: what makes one fact more important than another? In legal contracts, importance relates to risk, obligations, and decision-relevance. In news articles, importance relates to newsworthiness and reader interest. In scientific papers, importance relates to novel findings and methodological contributions. Generic importance measures based on word frequency or sentence position fail across these domains. You need task-specific importance definitions, typically operationalized through human judgments. Evaluators read the source and the summary, then rate how much of the important information the summary captured on a 1-to-5 scale.

**Conciseness** measures whether the summary achieves informativeness without unnecessary length. A summary that copies the entire source document would be perfectly faithful and maximally informative but useless as a summary. You measure conciseness through compression ratio: source length divided by summary length. Legal contract summaries typically achieve 20:1 to 40:1 compression, reducing a 30-page contract to a one-page summary. But compression ratio alone misses quality: a one-sentence summary of a 30-page contract has extreme compression but low informativeness. You need to measure informativeness and conciseness together, often as an efficiency ratio: informativeness score divided by summary length.

## Summarization Quality in 2026

The legal tech company rebuilt their evaluation around three human judgment tasks. First, attorneys read summaries without seeing the source contract and rated whether the summary gave them enough information to make an initial assessment of contract fit. Second, attorneys read both the summary and the full contract, then identified any claims in the summary that were misleading, wrong, or missing critical context. Third, attorneys compared summaries from different systems for the same contract and selected which summary they would prefer to use in practice. These human evaluations became the gold standard against which the team measured all system changes.

Human evaluation costs create a measurement budget tradeoff. Each attorney evaluation cost the company between twelve and thirty-five dollars depending on contract length and evaluation task complexity. Evaluating 100 contracts with three attorneys per contract for reliability cost 3,600 to 10,500 dollars. The team could not afford to run full human evaluation for every model iteration during development. They needed cheaper proxy metrics that correlated with human judgments. This is the central tension in summarization evaluation: human judgment is the gold standard but too expensive for continuous measurement, while automatic metrics are cheap but measure the wrong things.

**LLM-as-judge evaluation** emerged as the practical middle ground. You prompt a capable LLM like GPT-4o, Claude 4, or Gemini 2 to evaluate summaries using detailed rubrics that mirror your human evaluation criteria. For faithfulness, you provide the source contract, the summary, and a prompt asking the model to identify any unsupported claims. For informativeness, you ask the model to rate how comprehensively the summary captures key contract terms. For preference judgments, you show the model two summaries and ask which an attorney would find more useful. The legal tech team found that GPT-4o judgments correlated 0.74 with attorney faithfulness ratings and 0.68 with attorney informativeness ratings. These correlations were high enough to use LLM evaluation during development, then validate with attorney evaluation before major releases.

LLM-as-judge evaluation has failure modes you must understand. The evaluator LLM can hallucinate, claiming a summary contains information it does not or failing to recognize valid paraphrases. Evaluator LLMs show biases favoring summaries that match their own generation style, which creates circularity if you use GPT-4o to both generate and evaluate summaries. Evaluator LLMs struggle with subtle domain knowledge: an attorney might recognize that describing a clause as "standard" is misleading in context while GPT-4o rates the description as accurate. You mitigate these issues through prompt engineering, using multiple evaluator models, and regularly validating LLM judgments against human judgments to detect drift.

Reference-free evaluation methods assess summary quality without comparing to human-written reference summaries. Traditional ROUGE requires reference summaries, which are expensive to collect. Reference-free approaches include question-answering evaluation where you generate questions about the source document and check whether the summary provides correct answers, and natural language inference evaluation where you check whether the source document entails claims made in the summary. These methods showed promise in research but have seen limited production adoption as of 2026 because LLM-as-judge approaches proved more flexible and reliable.

## Content Generation Beyond Summarization

The legal tech company's summarization challenges represent a broader category: content generation tasks where the system must produce original text meeting quality criteria beyond correctness. Marketing copy generation, product description writing, email drafting, report creation, and creative writing all face similar evaluation difficulties. You cannot measure quality with word overlap because there is no single correct output. You cannot measure with accuracy because there is no factual ground truth for many creative decisions. You must measure on dimensions like relevance, appropriateness, style, and user preference.

**Relevance** measures whether generated content addresses the intended purpose and audience. A system generating product descriptions for an e-commerce site must produce descriptions that help customers decide whether to purchase the product. Relevance evaluation requires understanding the use context. You provide evaluators with the input (product specifications, images, existing data), the generated description, and context about the target audience and site presentation. Evaluators rate whether the description emphasizes attributes the target audience cares about, answers likely questions, and supports purchase decisions. Relevance is task-specific: a product description relevant for technical enthusiasts differs from one relevant for casual consumers.

**Originality** measures whether content is novel rather than copied from sources. Large language models trained on web text can inadvertently reproduce copyrighted material or training data verbatim. The EU AI Act and evolving copyright law in 2026 create legal risks around generated content that copies protected works. You measure originality through plagiarism detection: comparing generated content against the model's training data, against web search results, and against other generated content from the same system. Detecting memorization in LLM outputs remains challenging because models paraphrase and recombine training data in ways that avoid exact match detection. Some teams use n-gram overlap at multiple n-gram lengths, flagging any generated content with over 30 percent overlap with any single source.

**Style adherence** measures whether generated content matches required stylistic constraints. A system generating customer service emails must match the company's brand voice: formal or casual, verbose or concise, technical or accessible. Style encompasses word choice, sentence structure, tone, and formatting. Measuring style requires defining style specifications explicitly enough for evaluation. Some teams create style rubrics with 5 to 12 specific dimensions like formality level, sentence complexity, use of jargon, and emotional tone, then train human evaluators to rate generated content on each dimension. Other teams collect example content representing the target style and measure how similar new generated content is to the examples using embedding similarity or classifier-based approaches.

Brand safety and appropriateness create additional constraints for commercial content generation. You must ensure generated content contains no offensive language, does not make claims that violate advertising regulations, and does not include inappropriate content for the intended audience. Measuring safety uses classifier-based approaches similar to conversational safety: you run generated content through toxicity detectors, claim verification systems, and content policy classifiers. Unlike conversational AI where safety violations appear in user input that you must handle, content generation safety violations originate in your system's output, making them entirely your responsibility to prevent.

## Human Preference as the Gold Standard

The legal tech company's pivot to human evaluation reflected a broader industry shift in 2024-2026: recognizing that for many content generation tasks, human preference is the only meaningful quality metric. If attorneys prefer summary A over summary B, summary A is better by definition for that use case. This realization led to widespread adoption of preference-based evaluation methodologies.

**Pairwise preference evaluation** presents evaluators with two outputs for the same input and asks which they prefer. You generate summaries of the same contract using two different models or prompts, show both summaries to an attorney, and ask which they would rather use. The attorney does not rate summaries on absolute scales or check against references, just expresses preference. This task is faster and more reliable than absolute rating tasks because humans are better at comparative judgments than absolute judgments. Pairwise evaluation produces win rates: system A beats system B on 64 percent of examples, ties on 12 percent, loses on 24 percent. You can compute confidence intervals and statistical significance for these win rates.

Collecting preference judgments at scale requires interface design that minimizes evaluator burden. The legal tech company built a web application showing the contract and two summaries side by side, with a single click to select preference and optional comment fields for explaining the choice. Attorneys could complete one comparison in 45 to 90 seconds. The company collected three independent judgments per comparison to measure inter-annotator agreement. When all three attorneys agreed, they used the unanimous judgment. When two agreed and one disagreed, they used the majority judgment. When all three disagreed, they marked the comparison as ambiguous and excluded it from metrics.

**Bradley-Terry modeling** extends pairwise comparisons into system rankings. If you have five different summarization systems and collect pairwise preferences between all pairs, you have 10 pairwise comparisons. Bradley-Terry models fit a skill rating to each system such that the probability of system A beating system B in a comparison equals a logistic function of the difference in their ratings. This produces a ranked ordering of systems with uncertainty estimates. The legal tech company used Bradley-Terry models to rank six different GPT-4o prompting strategies, three fine-tuned variants, and two Claude 4 configurations, identifying which approaches generated summaries attorneys preferred.

Preference evaluation faces challenges around consistency and bias. Different evaluators have different preferences, creating variance in judgments. Individual evaluators sometimes contradict themselves, preferring A over B in one comparison and B over A in another comparison with the same examples. You measure consistency through agreement metrics: what percentage of pairwise judgments agree between evaluators, and how often does an individual evaluator agree with their own past judgments on repeated examples? Acceptable inter-annotator agreement for preference tasks typically ranges from 0.6 to 0.8 Cohen's kappa depending on task difficulty. Agreement below 0.5 indicates the task is too subjective or poorly defined.

Evaluator demographics and expertise affect preferences in systematic ways. The legal tech company initially collected preferences from paralegals and legal support staff because attorney time was expensive. They discovered that paralegal preferences diverged from attorney preferences: paralegals favored longer, more comprehensive summaries that included background context, while attorneys preferred shorter summaries focusing only on decision-critical terms. When they weighted evaluations by attorney preferences, system rankings changed significantly. You must ensure evaluator demographics match your end user demographics or explicitly model and account for demographic preference differences.

## Automated Metrics for Development Velocity

Human evaluation creates a feedback loop measured in days or weeks: you modify your system, generate outputs on a test set, collect human judgments, analyze results, and iterate. This cycle is too slow for rapid development. You need automated metrics that provide signal within minutes to hours so you can iterate quickly during development, then validate promising candidates with human evaluation before deployment.

**Embedding similarity** measures how semantically similar generated content is to reference content using neural embedding models. You encode the generated summary and a reference summary into dense vectors using models like OpenAI's text-embedding-3-large or sentence-transformers, then compute cosine similarity between the vectors. High similarity suggests the generated summary captures similar semantic content to the reference. Embedding similarity correlates more strongly with human judgments than ROUGE does, with typical correlations of 0.45 to 0.62 depending on domain and embedding model. That correlation is too weak to replace human evaluation but strong enough to guide development iteration.

**BERTScore** computes token-level similarity between generated and reference text using contextual embeddings from BERT or similar models. Unlike ROUGE which requires exact word matches, BERTScore recognizes that "doctor" and "physician" are semantically similar. The metric computes embeddings for every token in both the generated and reference text, finds the maximum similarity match for each token, and aggregates into precision, recall, and F1 scores. BERTScore achieves 0.52 to 0.68 correlation with human judgments on summarization tasks, outperforming ROUGE but still falling short of LLM-as-judge approaches.

**Perplexity and likelihood scores** measure how probable the generated content is under a language model. Lower perplexity indicates more fluent, natural-sounding text. But perplexity correlates only weakly with content quality because fluent text can be irrelevant, unfaithful, or wrong. The legal tech company found essentially zero correlation between summary perplexity and attorney quality ratings. Perplexity remains useful for detecting degenerate outputs like repetitive text or incoherent word salad, but provides little signal for distinguishing good summaries from great ones.

The practical approach combines automated metrics for filtering and ranking with human evaluation for validation. You generate 20 candidate summaries using different prompts, model parameters, or retrieval strategies. You compute embedding similarity, BERTScore, and LLM-as-judge scores for all candidates, discarding the bottom 12 based on automated metrics. You collect human preference judgments on the top 8 candidates to select the final approach. This hybrid workflow uses expensive human evaluation efficiently by reducing the candidate set before human review.

## Task-Specific Quality Dimensions

Different content generation tasks emphasize different quality dimensions. Email drafting prioritizes appropriateness and tone more than informativeness. Creative fiction writing prioritizes originality and engagement over factual accuracy. Technical documentation prioritizes clarity and accuracy over stylistic elegance. You must define task-specific quality criteria and measure them explicitly.

The legal tech company initially treated all summarization as a single task. They discovered attorneys had different quality expectations for different contract types. Summaries of non-disclosure agreements needed to emphasize confidentiality scope, obligations, and term duration. Summaries of service agreements needed to highlight deliverables, payment terms, and liability caps. Summaries of employment contracts needed to focus on compensation, benefits, termination conditions, and non-compete clauses. Generic summarization metrics could not capture these task-specific requirements.

They implemented contract-type-specific evaluation rubrics. For NDA summaries, attorneys rated whether the summary clearly identified what information was protected, who could access it, how long confidentiality lasted, and what exceptions applied. For service agreements, attorneys rated whether the summary identified scope of work, delivery timeline, payment schedule, and limitation of liability. This task-specific evaluation surfaced quality differences that generic metrics missed: a system might generate excellent NDA summaries with 4.2 out of 5 average ratings while producing mediocre service agreement summaries with 2.8 out of 5 ratings.

Evaluation rubrics require documentation and evaluator training. The legal tech company created a 23-page evaluation guide defining each contract type, listing the information categories that summaries must cover, providing examples of good and bad summaries, and explaining the rating scale. They trained new attorney evaluators through a calibration process: evaluators rated 30 practice summaries, discussed their ratings with experienced evaluators, and repeated until their ratings achieved 0.65 or higher agreement with experienced evaluators. This investment in evaluator training improved consistency and made the evaluation scores meaningful across different evaluation batches.

## Production Measurement and Feedback Loops

The legal tech company's development evaluation framework measured quality on a fixed test set of 800 contracts. That test set did not represent the full diversity of contracts their customers processed in production. Three months after launch, they noticed a pattern in customer complaints: summaries of contracts from certain industries (telecommunications, real estate, healthcare) had higher error rates than summaries of contracts from other industries. Their test set had undersampled these industries.

Production measurement requires capturing quality signals from real usage. Explicit user feedback through thumbs-up/thumbs-down buttons or rating prompts provides the clearest signal but suffers from low response rates. The legal tech company added a feedback widget to their summary interface asking "Was this summary helpful?" with Yes/No buttons. Only 7 percent of summaries received explicit feedback, but that 7 percent revealed clear patterns: summaries of telecommunications contracts had 23 percent negative feedback rate versus 8 percent negative feedback rate for summaries overall.

Implicit behavioral signals supplement sparse explicit feedback. If an attorney reads a summary then immediately opens the full contract, that suggests the summary was insufficient. If an attorney exports the summary to include in a client memo, that suggests the summary was useful. The legal tech company tracked summary export rate, full contract open rate after viewing summary, and time spent reading summary. They found summaries with longer reading times and higher export rates correlated with higher quality ratings in their test set evaluation.

You can deploy automated quality checks in production to flag potentially problematic outputs. The legal tech company ran every production summary through their LLM-as-judge faithfulness check, flagging summaries that GPT-4o identified as potentially unfaithful for human review. They sampled 0.5 percent of production summaries for detailed attorney review each week, stratifying the sample to cover different contract types, industries, and length ranges. This ongoing evaluation revealed quality drift before customer complaints accumulated.

Feedback loops must drive improvement. Collecting production quality signals has no value unless you use them to refine your system. The legal tech company built a data flywheel: production summaries that received negative feedback or failed automated quality checks went into a queue for attorney review. Attorneys corrected the summaries and explained what was wrong. These corrected examples became fine-tuning data for model improvement. Every month they fine-tuned their GPT-4o deployment on the accumulated correction data. This closed-loop process reduced negative feedback rate from 12 percent at launch to 4 percent after six months.

## Conclusion

Summarization and content generation evaluation requires abandoning traditional metrics like ROUGE that measure surface-level text overlap in favor of human-judgment-based approaches that assess faithfulness, informativeness, relevance, and preference. The legal tech company's failure demonstrated the cost of optimizing for convenient but misleading metrics. In 2026, LLM-as-judge evaluation provides a practical middle ground between expensive human evaluation and worthless automatic metrics, achieving sufficient correlation with human judgments to guide development while requiring periodic human validation to prevent drift. Task-specific quality dimensions matter more than generic metrics, requiring customized evaluation rubrics that capture what makes content useful for specific purposes and audiences. Production measurement closes the loop, revealing quality issues in deployment conditions and generating data to drive continuous improvement. With summarization and content generation metrics established, the final category of task-specific metrics addresses multimodal systems that must understand and reason across images and text, where evaluation faces the compound challenges of both visual and textual quality assessment.
