# 7.10 — Cross-Locale Metric Stability: When Metrics Drift Differently by Language and Region

A travel booking platform powered by Claude Opus 4.5 launched their conversational assistant in English in February 2025, achieving 94 percent accuracy on hotel search queries and 4.6 out of 5 user satisfaction ratings. Encouraged by the success, they expanded to Spanish, French, German, Japanese, and Brazilian Portuguese in May 2025, using the same model, the same prompts translated by professional translators, and the same metrics. By August, the product team noticed something disturbing. User satisfaction in English remained at 4.6. Spanish was at 4.5, close enough to attribute to normal variance. But German was at 3.9, Japanese at 3.4, and Brazilian Portuguese at 3.2. The accuracy metric told a similar story: 94 percent in English, 87 percent in German, 78 percent in Japanese, 71 percent in Portuguese. The team had assumed that language was just a surface feature, that quality would translate uniformly, and that a single set of metrics and thresholds would work globally. They were expensively wrong.

The investigation took six weeks and revealed layers of complexity the team had not anticipated. The model's training data was heavily English-skewed, giving it weaker performance in other languages. The prompts were professionally translated but culturally awkward, using formal phrasing where casual language was expected or vice versa. The evaluation set used to calculate accuracy was based on English-centric travel patterns, missing regionally specific hotel preferences and booking behaviors. Japanese users expected different information density and politeness levels than the model provided. Brazilian Portuguese users frequently mixed languages in queries, which the model handled poorly. And the user satisfaction metric was influenced by cultural differences in how people use rating scales, with Japanese users rating systematically lower than Western users for the same experience. The single accuracy number and single satisfaction number obscured all of this variation, giving the illusion of understanding while hiding a quality crisis in every non-English market.

## Why Quality Varies by Locale and Why Your Metrics Must Capture It

**Locale-aware metrics** recognize that AI quality is not a universal property but a context-dependent one, varying across languages, regions, and cultural contexts. A model that performs brilliantly in English for American users might struggle with British English, fail in Indian English, and collapse in Nigerian English, despite all three being the same language. The differences stem from vocabulary, idioms, cultural references, domain knowledge, and user expectations. Your metrics must surface these differences rather than averaging them away.

Language is the most obvious source of variation. Large language models in 2026 are trained predominantly on English text, with decreasing representation of other languages roughly following internet content distribution. English, Mandarin, Spanish, and French have relatively rich training data. Languages like Vietnamese, Swahili, or Icelandic have much less. Low-resource languages suffer from higher error rates, more hallucinations, worse instruction following, and weaker domain knowledge. A model that achieves 95 percent accuracy on English medical queries might achieve 60 percent on Thai medical queries, not because the task is harder but because the model has seen far less Thai medical text during training.

Even within a single language, regional variation matters. Spanish spoken in Mexico differs significantly from Spanish in Spain or Argentina in vocabulary, grammar, and cultural context. A model trained primarily on European Spanish will make errors with Mexican Spanish, confusing regionalisms, misunderstanding cultural references, and generating text that sounds foreign to Mexican users. The same applies to English across the US, UK, Australia, and India, to French across France and West Africa, and to Portuguese between Brazil and Portugal. Treating Spanish or English as monolithic categories guarantees measurement blind spots.

Cultural context shapes user expectations in ways that transcend language. Japanese users expect high formality and indirectness in customer service interactions. American users expect friendliness and efficiency. German users expect precision and thoroughness. A model that generates responses optimized for American expectations will feel wrong to Japanese users, even if the information is accurate and the language is fluent. User satisfaction metrics will diverge, not because the model is technically worse but because it is culturally misaligned. Your metrics need to distinguish between technical quality and cultural fit.

Domain knowledge distribution is rarely uniform across locales. A model might have excellent knowledge of American hotels, airports, and travel regulations because that information is well-represented in training data, but weak knowledge of Indonesian hotels, Brazilian domestic flight rules, or European train systems. When you measure accuracy on hotel search queries, you are implicitly measuring how well the model's training data covered the hotels your users care about. If your user base is global but your training data is American-skewed, accuracy will be high for American queries and low for others, and you need metrics that surface this imbalance.

Regulatory requirements vary by region, creating different quality standards. The EU AI Act imposes requirements that do not apply in the US or Asia. Data protection rules differ between jurisdictions. Content moderation standards for acceptable speech vary culturally and legally. A model that meets compliance requirements in one region might violate rules in another. If you measure compliance with a single global metric, you risk being compliant on average while violating specific regional requirements, which is not how regulators see the world.

## Designing Locale-Aware Metrics

The first step is segmenting your metrics by locale rather than reporting a single global number. Every metric you track overall should also be calculated separately for each language, region, or user segment you serve. This does not mean abandoning global metrics but adding dimensions. You still want to know your overall accuracy or satisfaction, but you also need to know how those metrics break down. When overall accuracy is 90 percent, you need to see that it is 95 percent in English, 88 percent in Spanish, 82 percent in German, and 74 percent in Japanese. The segmented view makes problems visible.

Segmentation requires careful instrumentation. Your logging must capture locale information for every request: the user's language, their geographic region, their cultural context if detectable. This information must flow through your entire metrics pipeline so you can slice every metric by locale. The travel booking platform discovered that their original instrumentation only captured language, not region, making it impossible to distinguish between Spanish queries from Spain versus Mexico or English queries from the US versus India. They had to add region detection and backfill historical data, a painful process that delayed their response to the quality crisis.

Once you have segmented metrics, you need locale-specific thresholds. A single accuracy threshold of 90 percent applied globally means you might be at 95 percent in English and 75 percent in Thai and still pass the threshold on average. This is unacceptable if you have Thai users who deserve a quality product. Locale-specific thresholds recognize that different locales might have different baseline quality levels and different rates of improvement. You might set 95 percent as the target for English, 90 percent for high-resource languages like Spanish and French, and 85 percent for lower-resource languages, with explicit plans to close the gap over time.

Setting different thresholds by locale is uncomfortable because it explicitly acknowledges that you are delivering different quality to different users. This feels like accepting inequality. The alternative is worse: pretending all users receive the same quality when they do not, or refusing to launch in lower-resource languages because they cannot immediately meet English-level quality. The honest approach is to set appropriate thresholds per locale, communicate them clearly, and invest in improving lower-performing locales rather than hiding the disparities behind global averages.

## Universal Metrics Versus Locale-Specific Metrics

Some quality dimensions are universal: response time, availability, crash rates, security. Users in every locale expect fast, reliable, secure systems. These metrics should have universal thresholds with no locale-based variation. If your system is fast for English users and slow for Japanese users, that is not a cultural difference, that is a quality problem. Universal metrics provide a baseline of consistency across your entire user base.

Other quality dimensions are inherently locale-specific and require different measurement approaches in different contexts. Tone and formality are culturally determined. What counts as polite in Japanese is different from what counts as polite in American English. You cannot measure tone against a single universal standard; you need locale-specific evaluation sets with locale-appropriate expectations. The same applies to humor, which rarely translates well, and cultural references, which are meaningful only in specific contexts.

Accuracy sits in between. The concept of accuracy is universal: does the model provide correct information. But what counts as correct can be culturally determined. If a user asks for hotel recommendations and the model suggests properties that are technically accurate but culturally inappropriate—a hotel without halal food options in a Muslim-majority country, or a very formal hotel when the culture expects casual hospitality—is that accurate or not. Your accuracy metric needs to be grounded in locale-specific evaluation data that reflects what correctness means in each context.

One effective pattern is maintaining a core set of universal metrics supplemented by locale-specific metrics. Every locale gets measured on response time, availability, basic accuracy, and safety. Then each locale has additional metrics tailored to its specific quality needs: formality scores for Japanese, code-switching handling for multilingual regions, regional knowledge depth for geographically specific queries. The universal metrics ensure a baseline quality floor. The locale-specific metrics capture the nuances that determine whether the product actually works well for local users.

Another pattern is using universal metrics for high-level monitoring and locale-specific metrics for deep investigation. Your executive dashboard shows global accuracy and satisfaction, segmented by major locales. When a locale shows a problem, you drill down into locale-specific metrics that help diagnose the root cause. This balances the need for simple, comprehensible high-level metrics with the need for detailed, context-aware debugging tools.

## Evaluation Data and Ground Truth Across Locales

The travel booking platform's accuracy metric was calculated using an evaluation set of 10,000 hotel search queries collected from English-speaking users in the United States. When they expanded to other locales, they translated the queries and expected answers into other languages and measured accuracy against the translated set. This approach guaranteed failure. The translated queries did not reflect how actual German, Japanese, or Brazilian users searched for hotels. The expected answers reflected American travel preferences, not local ones. The evaluation set was measuring how well the model served American users speaking foreign languages, not how well it served actual foreign users.

You need locale-specific evaluation data collected from real users in each locale, reflecting their actual query patterns, their actual expectations, and their actual definition of quality. This is expensive. You cannot just translate one evaluation set. You need separate data collection efforts in each market, with local annotators, local quality standards, and local domain expertise. For a product serving ten languages across twenty countries, this means building and maintaining twenty evaluation sets, each with thousands of examples, each requiring ongoing updates as the product evolves.

The cost is unavoidable if you want accurate measurement. You can reduce it through careful prioritization: larger markets get more comprehensive evaluation sets, smaller markets get smaller sets focused on the most critical quality dimensions. You can share some evaluation data across similar locales: Spanish in Mexico and Spain are different, but they are more similar to each other than to Japanese, so you might start with a shared Spanish set and add locale-specific augmentations. But the core principle remains: you need ground truth data that reflects the locale you are measuring.

Annotation quality varies across locales due to differences in annotator availability, training, and cultural context. English annotation is relatively easy because there is a large pool of experienced annotators and well-established best practices. Annotating in Vietnamese or Swahili is harder because the pool is smaller, annotators may lack ML familiarity, and best practices are less developed. This creates measurement noise: your English metrics might be based on high-quality annotations while your Vietnamese metrics are based on lower-quality annotations, making direct comparison misleading.

To manage this, invest in annotator training and quality control proportional to the locale's importance. For major languages, build local annotation teams with deep expertise. For smaller languages, consider a hybrid approach where local annotators handle language-specific tasks while centralized teams handle universal quality aspects. Use inter-annotator agreement metrics segmented by locale to detect when annotation quality is degrading. And be honest about measurement confidence: if your evaluation set for a low-resource language is small and your annotator agreement is low, your metrics have wide error bars, and you should communicate that uncertainty rather than reporting point estimates with false precision.

## When Metrics Drift Differently Across Locales

Metric drift, already challenging in a single locale, becomes exponentially more complex when different locales drift at different rates and in different directions. The travel platform discovered this in October 2025 when they updated their model from Claude Opus 4.5 to Claude 4. English accuracy improved from 94 percent to 96 percent. German improved from 87 percent to 91 percent. But Japanese dropped from 78 percent to 74 percent, and Portuguese dropped from 71 percent to 67 percent. The new model was better overall and better on high-resource languages, but worse on the lowest-resource languages. Aggregating these changes into a single metric would show improvement, hiding the fact that they had just made the product worse for their most vulnerable user segments.

This pattern is common with model updates. New models are trained on larger and more diverse datasets, but the distribution of that data still favors high-resource languages. The model gets better on average, and that average improvement comes primarily from high-resource languages, while low-resource languages see smaller gains or even regressions. If you only track global metrics, you see improvement and ship. If you track segmented metrics, you see the disparate impact and can make a more informed decision: accept the tradeoff, do not ship to low-resource languages yet, or invest in targeted improvements before launch.

Prompt changes also have locale-specific effects. A prompt modification that improves English performance might degrade performance in other languages because the prompt's phrasing or structure translates poorly. The travel platform tried adding few-shot examples to their prompts to improve accuracy. The examples were written in English and translated. English accuracy improved by 2 percentage points. German and Spanish improved by 1 point. Japanese and Portuguese did not improve at all, and in some cases got worse, because the translated examples did not capture the right patterns for those languages. The team learned to develop and test prompt changes separately per locale, rather than assuming a change that works in English will work everywhere.

User behavior drift can also vary by locale. As users in different markets adopt your product at different rates and use it for different purposes, their query distributions change differently. English users might shift toward more complex travel planning queries over time, while Japanese users shift toward simpler booking tasks. If your metrics are calculated on a fixed evaluation set, they will not capture this drift. If you use live traffic for evaluation, you need to segment the analysis by locale to see how each market is evolving independently.

## Balancing Consistency and Localization in Measurement

There is a fundamental tension between wanting consistent metrics across all locales for simple comparison and wanting localized metrics that capture real quality in each context. Push too far toward consistency and your metrics become meaningless in many locales, measuring the wrong things or applying the wrong standards. Push too far toward localization and you cannot compare performance across locales, making it impossible to allocate resources or set priorities.

The best approach is hierarchical measurement. At the top level, define a small number of truly universal metrics that apply everywhere with the same calculation and the same thresholds: availability, response time, critical safety violations. These metrics are your quality floor. Every locale must meet them, no exceptions. They are simple, comparable, and actionable. Below that, define a second tier of metrics that are calculated consistently across locales but with locale-specific thresholds: accuracy, user satisfaction, task completion. These metrics use the same formula everywhere, but what counts as acceptable varies by context. At the bottom, define locale-specific metrics that only make sense in particular contexts: formality in Japanese, code-switching in multilingual markets, regional knowledge depth where geography matters.

This hierarchy gives you both comparability and context. You can tell executives that overall accuracy is 88 percent and availability is 99.9 percent across all locales, providing a simple summary. You can show them that accuracy ranges from 95 percent in English to 70 percent in low-resource languages, highlighting where investment is needed. And you can drill into locale-specific metrics when debugging particular quality issues in particular markets. Each layer serves a different purpose and a different audience.

Documentation is critical for making this work. Your metric playbook, already important for a single locale, becomes essential for multi-locale operations. Each metric needs clear documentation of whether it is universal or locale-specific, how it is calculated in each locale, what thresholds apply where, and why. This prevents confusion where someone looks at a metric, assumes it means the same thing everywhere, and makes bad decisions based on false comparisons. The documentation should also explain how to interpret differences between locales: is a gap a problem to fix, a temporary state during rollout, or an acceptable reflection of different baseline capabilities.

## The Resource Allocation Problem

Multi-locale measurement surfaces a difficult question: how do you allocate quality improvement resources when different locales have different quality levels and different rates of improvement. Do you invest in bringing low-performing locales up to the baseline, or do you invest in making high-performing locales even better. Do you prioritize large markets where more users are affected, or small markets where quality gaps are most severe. Your metrics should inform this decision but cannot make it for you.

One framework is setting minimum acceptable thresholds per locale based on market maturity and expectations, then investing to bring all locales above their thresholds before pursuing stretch goals. A newly launched locale might have a lower threshold than a mature one, but every locale has a floor below which quality is unacceptable. This ensures you do not abandon low-resource languages entirely while still allowing differentiated investment based on context. The travel platform eventually adopted a policy that every locale must reach 85 percent accuracy before they would invest in pushing any locale beyond 95 percent. This forced continuous improvement in low-performing markets rather than letting them stagnate.

Another approach is measuring and optimizing quality velocity: how fast is each locale improving, and where can marginal investment have the biggest impact. A locale at 70 percent accuracy that is improving 2 percentage points per quarter might deserve more investment than a locale at 90 percent that is improving 0.5 points per quarter, because the lower-performing locale has momentum and room to grow. Velocity metrics help you identify where effort is paying off and where you are hitting diminishing returns.

User impact should also weigh into allocation decisions. A 5 percentage point quality gap in a language with ten million users affects more people than a 10 point gap in a language with 100,000 users. But those 100,000 users might have fewer alternatives, making your product more critical to them. Metrics should quantify both the absolute impact in terms of affected users and the relative impact in terms of alternatives and importance. This gives you a richer picture for making allocation tradeoffs.

## Closing Metric Drift, Maintenance, and Quality Measurement

The travel booking platform spent nine months addressing their cross-locale quality crisis. They built separate evaluation sets for each language, hired local annotators, set locale-specific thresholds, and invested heavily in improving their lowest-performing markets. By June 2026, they had brought all locales above 85 percent accuracy, with high-resource languages at 94 to 96 percent and low-resource languages at 85 to 88 percent. User satisfaction showed similar improvement and convergence. The effort cost approximately 1.8 million dollars in engineering time, annotation, and compute resources. The result was a product that actually worked well for users in every market they served, backed by metrics that honestly reflected quality in each context.

The lessons from metric drift, maintenance, and evolution are hard-won but clear. Metrics are not static. They drift as user behavior evolves, as models change, as products grow in complexity, and as you expand across languages and regions. You must monitor drift actively, distinguish signal from noise, and update your metrics deliberately. You must maintain a living playbook that documents what you measure and why, keeping it synchronized with product evolution. You must validate the instrumentation that feeds your metrics, ensuring the data itself is trustworthy. And you must design locale-aware metrics that surface quality variation rather than hiding it behind global averages.

Quality measurement is not a one-time setup but an ongoing discipline. It requires tooling, process, ownership, and cultural commitment. It requires accepting that metrics will sometimes deliver uncomfortable truths about disparate quality levels or declining performance. It requires balancing the desire for simple, universal metrics against the reality that quality is contextual and multidimensional. And it requires recognizing that the metrics themselves are products that need design, testing, maintenance, and evolution, just like the AI systems they measure. When you get measurement right, you gain clear sight into what quality means, whether you are delivering it, and how to improve it. When you get measurement wrong, you navigate blind, making decisions based on numbers that do not reflect reality, and the consequences compound over time until they become impossible to ignore.

