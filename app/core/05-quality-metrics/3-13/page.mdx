# 3-13 â€” Preference and Pairwise Win-Rate Evaluation

In February 2025, a startup building an AI writing assistant faced a decision that would determine their next funding round. They had two competing model architectures. Model A scored seventy-eight out of one hundred on their absolute quality rubric. Model B scored seventy-six out of one hundred. Based on absolute scores, they should ship Model A. But when they ran a small preference study where users compared outputs from both models side-by-side without knowing which was which, users preferred Model B outputs sixty-three percent of the time. The absolute scores said Model A was better. User preferences said Model B was better. They had to choose.

The team chose Model A because their evaluation infrastructure was built around absolute scoring and changing direction would delay their launch by three weeks. They shipped in March. By May, user engagement was below projections. Customer feedback mentioned that the writing "felt stiff" and "lacked personality" compared to competitor tools. In June, they finally ran comprehensive preference testing against competitors and discovered that their model lost to both major competitors in head-to-head comparison despite having competitive absolute quality scores. They spent the summer rebuilding around Model B architecture, which had tested better in preferences all along. The three-week delay they avoided in March became a four-month setback because they optimized for the wrong metric. Absolute quality scores measured something, but not the thing that determined whether users would choose their product.

## The Preference Evaluation Paradigm

**Preference evaluation** asks which of two outputs is better rather than asking how good a single output is. You show human raters or automated judges two model outputs for the same input, and you ask them to choose which one they prefer or to indicate no preference if they are equivalent. The fundamental unit is the pairwise comparison, not the absolute score. This paradigm shift seems minor but it changes evaluation methodology, changes what you can measure, and often changes which model you decide to ship.

The power of preference evaluation comes from how humans naturally think about quality. When you ask someone "how good is this essay on a scale of one to ten," you are asking them to map their intuitive quality sense onto an arbitrary numeric scale. Different people use the scale differently. Some people never give tens. Some people rarely give below five. The absolute scores are noisy and hard to calibrate. But when you ask "which of these two essays is better," most people can answer confidently. Relative comparison is more natural than absolute measurement for subjective quality.

Preference evaluation is particularly valuable when quality is multidimensional and the dimensions trade off against each other. Model A might be more factually accurate but less engaging. Model B might be more creative but occasionally inappropriate. Absolute scoring forces you to weight these dimensions explicitly. Preference evaluation lets raters make holistic judgments that implicitly balance the trade-offs according to what actually matters for user satisfaction. The weights emerge from preferences rather than being specified upfront by evaluation designers who might guess wrong.

The writing assistant startup's problem was that their absolute rubric weighted correctness, clarity, and conciseness equally. Users actually cared much more about personality and engagement, dimensions the rubric barely measured. Model A optimized for the stated rubric. Model B had personality that made users happy. Preference evaluation would have captured this without needing to specify the correct weights upfront. The preferences would have revealed what users actually valued.

## Pairwise Comparison Methodology

The basic **pairwise comparison** methodology presents two outputs generated by different models or model variants for the same input. Raters select which output is better according to specified criteria, or they indicate that the outputs are roughly equivalent. The comparison should be blind, meaning raters do not know which model generated which output, to prevent bias. The order of presentation should be randomized to prevent position bias where raters systematically prefer the first or second option.

Criteria for comparison must be specified clearly but they can be high-level. "Which response would be more helpful to the user?" is a valid criterion. "Which summary better captures the key information?" is valid. "Which code solution would you rather use?" is valid. The criteria should align with actual user value rather than with technical metrics. The whole point of preference evaluation is capturing holistic user-centric quality that absolute rubrics struggle to measure.

Sample size requirements for pairwise comparison are different from absolute scoring. With absolute scores, you need enough examples to cover the distribution of inputs. With pairwise comparisons, you need enough comparisons to distinguish models statistically. If the true preference rate is sixty-five percent for Model A versus Model B, you need at least one hundred comparisons to detect that with reasonable confidence. If the true preference rate is fifty-two percent, you need thousands of comparisons. Tighter races require more data.

Position bias and order effects are real and must be controlled. Humans have weak tendencies to prefer the first option or the second option shown depending on context. Automated LLM judges have stronger position biases, sometimes preferring first or second options at rates as high as fifty-five to forty-five instead of fifty-fifty. Control this by presenting each pair twice with reversed order and aggregating results, or by randomizing order and checking that first and second position win rates are balanced across your dataset. If they are not balanced, your comparisons are biased.

## Win-Rate as a Quality Metric

**Win-rate** is the percentage of pairwise comparisons where Model A beats Model B. If you run two hundred comparisons and Model A wins one hundred thirty, Model B wins sixty, and ten are ties, Model A's win-rate is sixty-five percent if you exclude ties or sixty-one percent if you count ties as half a win for each. Win-rate is intuitive, easy to communicate, and directly measures what you care about: how often users prefer your model over the alternative.

Win-rate has attractive statistical properties. It is bounded between zero and one hundred percent. It is directly interpretable: a sixty percent win-rate means your model is preferred sixty percent of the time. It aggregates cleanly across different inputs and raters. It lets you compare more than two models by running pairwise comparisons for each pair and building a win-rate matrix. The matrix shows which models beat which other models and by what margin.

A challenge with win-rate is that it requires a comparison baseline. Win-rate is always relative to something. You cannot measure "the win-rate of Model A" in isolation; you can only measure "the win-rate of Model A versus Model B." This means you need to decide what to compare against. Common choices include previous model version, leading competitor model, human-written reference outputs, or a fixed baseline model that you measure all development iterations against. The choice affects how you interpret win-rates and how you make decisions based on them.

Another challenge is that win-rate is a single number that aggregates across all inputs. A model might have sixty percent overall win-rate but perform very differently on different input types. It might win seventy-five percent on short inputs and forty-five percent on long inputs. Or win eighty percent on technical content and forty percent on creative content. Slicing win-rate by input properties reveals these patterns and prevents shipping models that have good average win-rate but terrible performance on important subsets. Report both overall win-rate and win-rate breakdowns by key dimensions.

## Preference Evaluation vs Absolute Scoring

The choice between preference evaluation and absolute scoring depends on what you are trying to measure and how you will use the results. **Preference evaluation** excels when quality is subjective, when you need to compare specific alternatives, when trade-offs between quality dimensions matter, or when you want to predict user choice behavior. **Absolute scoring** excels when you need to track quality over time on a consistent scale, when you need to measure improvement on specific criteria, when you need to debug why quality is low, or when you need interpretable subscores for different dimensions.

Many teams need both. Use absolute scoring during development to identify specific weaknesses and guide improvements. Use preference evaluation before launch to validate that your model beats the baseline it is replacing. Use absolute scoring in production monitoring to detect quality degradation. Use preference evaluation periodically to ensure you still beat competitors. The methods complement each other. Absolute scoring provides detailed diagnostic insight. Preference evaluation provides validation that the model delivers user value.

A medical AI company in 2025 used absolute scoring rubrics to evaluate clinical note completeness, accuracy, and readability during development. The rubrics helped them identify that their model was missing medication history in twelve percent of notes, which they fixed. But before deploying to hospitals, they ran preference evaluations where physicians compared their model's notes to notes written by medical scribes. Only sixty-two percent of physicians preferred the AI notes, below their seventy-five percent threshold for deployment. The absolute scores had looked good but the preference test revealed the AI notes felt "mechanical" and "missing clinical judgment." They used the preference feedback to improve tone and returned to preference testing, eventually reaching seventy-eight percent physician preference before deploying.

The key insight is that absolute scores measure whether you meet your own standards while preferences measure whether you meet user standards. Your standards might be wrong. Your rubric might weight dimensions incorrectly. Your threshold for "good enough" might be too low or too high. Preferences ground your evaluation in actual user value. If users prefer competitor outputs to yours, your absolute scores are measuring the wrong things or using the wrong thresholds. Fix the scores, do not ignore the preferences.

## ELO-Style Ranking Systems

When you need to compare more than two models, **ELO-style ranking systems** extend pairwise comparison to multi-model scenarios. ELO is a rating system originally developed for chess that assigns each player a numerical rating. When two players compete, the winner takes rating points from the loser. The number of points transferred depends on the rating difference: upsets transfer more points than expected wins. Over many games, ratings converge to reflect true skill levels.

ELO applied to AI models works similarly. Each model starts with a base rating, often 1500. You run pairwise comparisons between randomly selected model pairs. When Model A beats Model B, Model A gains rating points and Model B loses the same number of points. The transfer amount depends on current ratings and a K-factor that controls how quickly ratings adjust. After hundreds or thousands of comparisons, ratings stabilize and you have a ranking of all models. Higher-rated models beat lower-rated models more than half the time.

The **Chatbot Arena** leaderboard run by LMSYS uses exactly this approach. They collect pairwise preferences from users who chat with two anonymous models and choose which response they prefer. These preferences feed into an ELO system that ranks models. As of early 2026, GPT-4.5 and Claude Opus 4.5 occupy top positions with ratings around 1350, while smaller open models have ratings around 1100 to 1200. The rating differences correspond to expected win-rates: a model rated 100 points higher wins approximately sixty-four percent of comparisons.

ELO systems work well for continuous evaluation where new models are added frequently. You do not need to rerun all comparisons when adding a new model; just run comparisons between the new model and existing models until the rating stabilizes. The system naturally handles different model strengths: a model might be rated 1250 because it beats weaker models consistently and loses to stronger models consistently. The rating encodes expected performance against the field, not just pairwise performance against one baseline.

The limitation is that ELO provides a single scalar rating, collapsing potentially complex model performance into one number. A model might be excellent at creative writing but poor at factual question answering, while another model has the opposite profile. They might have similar ELO ratings because they each beat half the field, but they are better for different use cases. Address this by running separate ELO systems for different task categories, or by slicing ELO ratings by input type. A model might have 1300 rating on summarization and 1150 on question answering, giving you more actionable information.

## Handling the Subjectivity Problem

Quality subjectivity is both the motivation for preference evaluation and a challenge in implementing it. Different raters have different preferences. What one user considers helpful another considers verbose. What one user finds creative another finds inappropriate. **Subjectivity** means that there is no single ground truth preference, only a distribution of preferences across raters. Preference evaluation must account for this or it produces misleading results.

One approach is **majority preference**, where you collect preferences from multiple raters for each comparison and use the majority judgment. If three out of five raters prefer Model A for a given input, you count that as a win for Model A. Majority preference is simple but it discards information: a three-to-two split is treated the same as a five-to-zero split even though the five-to-zero case shows much stronger consensus. It also requires multiple raters per comparison, which is expensive.

A more sophisticated approach is **probabilistic preference modeling** where you model the probability that a random rater from your target population will prefer Model A over Model B. Instead of asking "which model is better," you ask "what fraction of users will prefer Model A." If you collect five preferences and three favor Model A, your estimate is sixty percent with uncertainty based on sample size. You can use Bayesian methods to compute confidence intervals: sixty percent with a ninety-five percent confidence interval of thirty to eighty-five percent, indicating high uncertainty from small sample.

An alternative is **preference segmentation** where you cluster raters by their preference patterns and analyze preferences separately for each segment. Segment one might be users who value accuracy over creativity, segment two might reverse that priority. Model A wins with segment one but loses with segment two. If your product targets segment one, Model A is the right choice even if it has fifty percent overall win-rate. Segmentation requires collecting demographic or behavioral data about raters and having enough raters per segment to make stable estimates.

The writing assistant startup discovered through post-mortem analysis that professional writers preferred Model A, which was precise and formal, while casual users preferred Model B, which was engaging and conversational. Their early testing had oversampled professional writers because those were easier to recruit. The preference signal was real but unrepresentative of their target market, which was casual users. Better sampling would have revealed that Model B was the right choice for their market segment.

## LLM Judges for Preference Evaluation

Using LLM judges for pairwise preference evaluation is increasingly common and mostly works well. You give the judge both outputs and ask which is better. LLM judges handle pairwise comparison more reliably than absolute scoring because relative comparison is simpler than absolute measurement. Judges do not need to calibrate to a scoring scale; they just need to decide which of two concrete options is better. This reduces one source of judge variance.

However, LLM judges have position bias in pairwise comparisons. Several studies in 2025 showed that GPT-4 and Claude Opus models exhibit position bias, preferring outputs in the first position or second position at rates different from fifty-fifty when outputs are actually equivalent. The bias is usually small, fifty-two to fifty-three percent instead of fifty percent, but it is real. With hundreds of comparisons, it materially affects win-rates. A model with a true fifty-five percent win-rate might show fifty-eight percent if position bias favors it or fifty-two percent if position bias opposes it.

Mitigate position bias by **swapping positions**. For each pairwise comparison, present the outputs in both orders and collect judgments for both. If the judge prefers A in position one versus B in position two, and also prefers A in position two versus B in position one, you have strong evidence that A is genuinely better. If the judge prefers A in position one versus B in position two but prefers B in position one versus A in position two, the preferences are contradictory and likely driven by position bias. Treat this as a tie or as invalid data.

LLM judge preferences should be validated against human preferences regularly. Collect both human and LLM preferences on a sample of comparisons, then measure agreement. Agreement above seventy percent correlation is generally acceptable for screening and development purposes. Agreement below sixty percent suggests the LLM judge has biases or failure modes that diverge from human preferences. A developer tools company in 2026 found that GPT-4o agreed with their developer users on code quality preferences eighty-one percent of the time, high enough to use for rapid iteration. Claude Opus 4.5 agreed seventy-six percent. They used GPT-4o for development testing and validated major decisions with human preferences.

## Collecting High-Quality Human Preferences

Human preference collection seems simple but doing it well requires careful design. Poor preference collection produces noisy data that does not predict user behavior. **High-quality preference collection** requires clear instructions, representative raters, sufficient context, and quality control. Each component matters.

Instructions must specify what "better" means in your context. "Choose which response is better" is ambiguous. Better for what? Better how? Instructions should ground "better" in user value: "Choose which response would be more helpful if you were the user asking this question," or "Choose which summary would save you more time while giving you the information you need." The framing affects which option raters choose. Test different framings and pick the one that best predicts actual user behavior in your product.

Rater representativeness is critical. Raters should match your target user population in relevant dimensions. If your product targets non-experts, do not collect preferences exclusively from experts. If your product targets a specific language or culture, include raters from that context. If your users have domain knowledge, your raters need similar knowledge to make meaningful judgments. The writing assistant startup's error was using unrepresentative raters. Representative sampling is more important than large sample size. Better to have one hundred preferences from representative raters than one thousand from unrepresentative ones.

Context matters for comparisons. Raters need enough information about the task, the user intent, and the desired outcome to make informed preferences. If you show them two summaries without showing the source document, they cannot judge whether the summaries are accurate. If you show them code solutions without specifying requirements, they cannot judge whether the code meets needs. Provide full context but avoid overloading raters with information they will not read. A legal AI company found that providing two sentences of case background significantly improved preference quality compared to providing no background or ten sentences of background.

Quality control separates signal from noise. Include **attention checks** where the correct answer is obvious to ensure raters are engaged. Include **duplicate comparisons** presented twice to check rater consistency. Flag raters who fail attention checks or show low consistency. Collect multiple preferences per comparison and flag comparisons with high disagreement for review. Measure inter-rater reliability and investigate when it is low. Low reliability might mean the comparison is genuinely difficult, or it might mean your instructions are unclear or your raters are not qualified.

## Bootstrap Confidence Intervals for Win-Rates

Win-rates are estimates subject to sampling uncertainty. If you run one hundred comparisons and observe sixty wins, the true win-rate might be anywhere from fifty to seventy percent with reasonable probability. **Confidence intervals** quantify this uncertainty and prevent overconfident decisions based on noisy estimates. The standard method is bootstrapping, which works well for win-rates and requires no distributional assumptions.

Bootstrap confidence intervals work by resampling your observed comparisons with replacement to create many simulated datasets, computing win-rate on each simulated dataset, and using the distribution of simulated win-rates to estimate confidence intervals. If you have one hundred comparisons, you randomly draw one hundred comparisons from your data with replacement, compute win-rate, and repeat this one thousand times. The 2.5th and 97.5th percentiles of the simulated win-rates form a ninety-five percent confidence interval.

A model with sixty wins out of one hundred comparisons has an observed win-rate of sixty percent, but the ninety-five percent confidence interval is approximately fifty to seventy percent. If your threshold for deploying the model is sixty-five percent win-rate, you cannot conclude the model meets the threshold because the interval includes values below sixty-five percent. You need more comparisons to narrow the interval. With four hundred comparisons and two hundred forty wins, the win-rate is still sixty percent but the confidence interval tightens to fifty-five to sixty-five percent. You still cannot conclude the model meets sixty-five percent threshold, but you are closer.

Confidence intervals inform how many comparisons you need. Work backwards from your decision criteria. If you need to distinguish between fifty-five and sixty percent win-rate with high confidence, you need enough comparisons that the confidence interval for a fifty-five percent true win-rate does not overlap the confidence interval for a sixty percent true win-rate. This requires several hundred comparisons. If you need to distinguish between fifty and seventy percent, far fewer comparisons suffice. Match sample size to the precision your decisions require.

Report confidence intervals with win-rates. "Model A has a sixty percent win-rate" is incomplete. "Model A has a sixty percent win-rate with a ninety-five percent confidence interval of fifty-five to sixty-five percent based on four hundred comparisons" is complete. The interval tells you how confident to be in the estimate. Wide intervals mean you need more data. Narrow intervals mean you have sufficient data to make confident decisions. Intervals also prevent false precision: a win-rate of 57.3 percent based on fifty comparisons is reporting false precision when the confidence interval is forty-seven to sixty-seven percent.

## Preference Evaluation in Production Monitoring

Preference evaluation is not just for pre-launch comparison. It works for production monitoring by continuously collecting preferences between your production model and alternatives. This provides ongoing validation that your model remains competitive and alerts you if competitors or new model versions surpass your quality. **Production preference monitoring** catches problems that absolute metrics might miss.

Implement preference monitoring through sampling. Select a random subset of production requests, perhaps one percent, and generate outputs using both your production model and one or more comparison models. Present the outputs to users or to automated judges for preference evaluation. Track win-rate over time. If win-rate against your previous model version degrades, your quality may have regressed. If win-rate against competitors degrades, competitors may have improved and you need to respond.

Users can be preference raters in some contexts. After a user receives a response from your model, you can show them an alternative response from another model and ask which they prefer. This has benefits: users are the actual target raters, feedback is immediate and contextual, and data collection integrates into product usage. It also has costs: users might find the request intrusive, showing alternative responses might reveal that better alternatives exist, and users might not engage seriously with preference questions. Test this approach carefully and use it only when user experience costs are acceptable.

An enterprise search company in 2026 implemented production preference monitoring by generating search results from both their current model and their previous model for five percent of queries. They showed users results from their current model but collected results from the previous model in the background. Once per week, they sampled two hundred of these query pairs and had internal raters judge which result set was better. The current model maintained an eighty-two percent win-rate over the previous model for the first two months after launch, but the win-rate dropped to seventy-six percent in month three. Investigation revealed that their current model performed worse on a new class of technical queries that had increased in frequency. They retrained with additional technical data and the win-rate recovered to eighty-five percent.

## When Preferences Disagree with Metrics

Sometimes preference evaluation contradicts your other metrics. Your accuracy metrics say Model A is better, but users prefer Model B. Your latency data says Model A is faster, but users prefer Model B. Your absolute quality rubric says Model A scores higher, but head-to-head preference testing favors Model B. What do you do? Trust the preferences. Users decide what succeeds. If users prefer B, ship B, then investigate why your other metrics did not predict user preference.

The investigation teaches you what users actually value. Perhaps accuracy matters less than you thought and engagement matters more. Perhaps latency differences below a certain threshold are imperceptible and other factors dominate. Perhaps your quality rubric missed important dimensions. Use the preference feedback to improve your metrics so they align better with user value. The goal is not achieving perfect correlation between metrics and preferences, which is impossible. The goal is having metrics that usually point you in the same direction as preferences, with preferences as the tiebreaker when they conflict.

A customer service AI team discovered that users preferred responses that were slightly less accurate but acknowledged uncertainty appropriately. Their accuracy metric penalized "I'm not sure" responses, so Model A never expressed uncertainty and was sometimes confidently wrong. Model B expressed uncertainty when appropriate and was preferred by users sixty-eight percent of the time despite lower technical accuracy. The team updated their rubric to reward appropriate uncertainty expression. Their absolute metrics then aligned better with user preferences, and they could use faster absolute scoring for development while reserving preference testing for validation.

This pattern repeats across products. Users prefer outputs that feel natural over outputs that are technically optimal. Users prefer outputs that acknowledge limitations over outputs that overpromise. Users prefer outputs that match their expectations even if those expectations are not objectively ideal. Preferences reveal these truths that your engineered metrics miss. Listen to preferences and update your metrics accordingly. Do not dismiss preferences as "subjective" when they contradict your "objective" metrics. The subjective preferences are what users act on.

## The Combinatorial Challenge of Multi-Model Preference Testing

Testing preferences becomes combinatorially expensive with many models. Comparing two models requires presenting each pair of outputs. Comparing four models requires six pairwise comparisons for full coverage: A vs B, A vs C, A vs D, B vs C, B vs D, C vs D. Comparing ten models requires forty-five comparisons. If you need two hundred samples per comparison for statistical power, testing ten models requires nine thousand total comparisons. This is impractical.

Solutions include **round-robin sampling** where each comparison gets a subset of examples rather than all examples, **Swiss-system tournament** where models are paired based on current estimated strength similar to tournament seeding, or **ELO systems** that efficiently aggregate many comparisons without requiring all pairwise coverage. The right approach depends on your goals. If you need a definitive ranking, ELO systems work well. If you need to validate that your model beats a specific baseline, you only need one pairwise comparison and can skip the rest.

Another approach is **hierarchical comparison** where you first compare models in groups, identify the best from each group, then compare the group winners. This reduces the number of comparisons at the cost of possibly missing the true best model if it loses in an early group comparison due to noise. Hierarchical comparison works better when you have prior information about model quality and can seed groups so favorites are unlikely to be eliminated early. It is a practical compromise between comprehensive testing and resource constraints.

For continuous evaluation, **incremental comparison** adds new models to an existing ELO or ranking system without rerunning all comparisons. When you develop a new model version, compare it against a few reference models at different rating levels. If it beats the 1200-rated reference and loses to the 1300-rated reference, you estimate its rating at 1250 and slot it into the ranking. This estimate has uncertainty but it is sufficient for deciding whether the new model is an improvement. If you decide to ship it, you collect more comparisons over time to refine the rating estimate.

With metric design methodology now complete, including custom metrics, leakage prevention, judge reliability, and preference evaluation, Chapter 4 applies these principles to specific task types, beginning with classification and structured output tasks where evaluation challenges differ substantially from open-ended generation.
