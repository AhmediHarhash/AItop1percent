# 2.2 — Groundedness: Attribution, Citation, and Source Fidelity

In August 2025, a legal technology startup providing AI-powered contract analysis went from 120 enterprise customers to 73 in six weeks. The product analyzed commercial agreements and generated risk summaries for corporate legal teams. The system used a retrieval-augmented generation architecture, pulling relevant contract clauses and then generating natural language explanations of legal risks. Every output included citations to specific sections of the contract, displayed as footnote-style references. The system looked authoritative and rigorous. Then a Fortune 500 client's legal team conducted a detailed audit and found that 22% of the citations pointed to the wrong section of the contract. The claims were often correct—there really was an indemnification obligation or a termination clause—but the cited section did not support the claim. Sometimes the citation was off by a few paragraphs. Sometimes it pointed to an entirely different part of the agreement. The AI was confabulating sources. Word spread quickly in the corporate legal community. Within weeks, the startup was facing contract terminations and a class action lawsuit from customers who claimed they had relied on the system's analysis to make business decisions. The company had tracked correctness of the risk assessments and user satisfaction scores. They had never systematically verified that citations matched claims.

This is the groundedness crisis. Your AI system can generate correct, useful, well-written outputs and still fail catastrophically if those outputs are not properly grounded in verifiable sources. **Groundedness** means that every factual claim in your output can be traced back to a specific source, and that source actually supports the claim. It is not enough to provide citations. The citations must be accurate. It is not enough for the output to be correct. The correctness must be demonstrable through source material.

## Why Groundedness Is Different From Correctness

Teams routinely confuse groundedness with correctness, treating them as synonyms. They are not. An output can be correct but not grounded. An output can be grounded but not correct. These are independent dimensions that fail in different ways and require different evaluation strategies.

A **correct but ungrounded** output states accurate facts but does not cite sources or cites sources that do not actually contain the claimed information. Imagine a medical AI that generates a summary stating "Patient has history of hypertension." This might be factually correct—the patient does have hypertension—but if the claim is not explicitly stated in the provided medical record and the system cites the record as the source, the output is ungrounded. The AI is using general medical knowledge or patterns from training data rather than the specific input documents. The statement is true but not verifiable from the sources presented to the user.

A **grounded but incorrect** output cites real sources accurately but draws wrong conclusions from those sources. A financial analysis system analyzes a quarterly earnings report and states "Revenue declined 15% year-over-year" with a citation to page 3 of the earnings report. Page 3 does discuss revenue, and the citation is accurate, but the actual figure on that page shows a 15% increase, not decline. The system has correctly identified the relevant source but misread the data. The output is grounded in the sense that it points to a real, relevant source, but it is not correct.

The distinction matters because the failure modes and remediation strategies are different. Correctness failures often stem from model capabilities, prompt design, or data quality. Groundedness failures stem from the architecture of how you retrieve information, how you link generated text to source material, and how you verify attribution. A system can become more correct without becoming more grounded, and vice versa.

## The Three Levels of Groundedness

Groundedness exists on a spectrum from completely ungrounded to precisely grounded at the claim level. Most systems operate somewhere in the middle, providing vague or partial grounding that seems rigorous but collapses under scrutiny.

**Document-level groundedness** means the output cites a source document without specifying which part of that document supports which claim. A research AI generates a summary of a scientific paper and includes a citation to the paper at the end. This tells the user where the information came from in general but does not help them verify specific claims. If the summary includes ten factual statements, the user would need to read the entire paper to check each one. Document-level grounding provides accountability—you can trace back to a source—but not verifiability. It is better than nothing but insufficient for high-stakes applications.

**Passage-level groundedness** means the output cites specific sections, paragraphs, or page numbers. A legal AI analyzing a contract states "The agreement includes a 90-day termination notice requirement (Section 8.3)" and the citation points to the exact section. The user can jump directly to that section to verify the claim. This is the standard for most RAG systems and represents a significant improvement over document-level citation. However, passage-level grounding still has limitations. If the cited passage is long or complex, the user might struggle to identify which sentence actually supports the claim. If a passage contains multiple pieces of information, the citation might be technically accurate but not precise enough to validate the specific claim.

**Claim-level groundedness** means every individual factual statement in the output is linked to a specific source that directly supports that exact claim. If an output makes five factual statements, there are five citations, each pointing to the precise sentence or data point in the source material. This is the gold standard for applications where verification is critical—regulatory compliance, medical decision support, legal analysis, financial reporting. It is also the most difficult to achieve because it requires decomposing generated text into atomic claims and tracking the provenance of each claim through the generation process.

The legal AI that collapsed went from document-level to passage-level groundedness in their product evolution, adding section-specific citations to make the system appear more rigorous. But they never validated that the cited sections actually contained the claimed information. They assumed that if the retrieval system returned a relevant passage and the generation model produced a coherent summary with a citation, the citation must be accurate. This assumption was wrong in more than one-fifth of cases.

## How RAG Systems Make Groundedness Critical

Retrieval-augmented generation architectures promise to solve the hallucination problem by grounding model outputs in retrieved documents. Instead of relying on model knowledge encoded during training, RAG systems retrieve relevant information from a knowledge base and condition the generation on that information. The theory is that outputs will be factually grounded because they are derived from real documents rather than model imagination.

This theory is only as good as your implementation. RAG systems can fail at groundedness in multiple ways that are invisible if you only evaluate the quality of the generated text.

**Retrieval failures** occur when the retrieval system does not find the right documents or passages. If a user asks a question and the retrieval step returns marginally relevant documents that do not actually answer the question, the generation model might synthesize an answer using its parametric knowledge rather than the retrieved content. The output will include citations to the retrieved documents, creating the appearance of groundedness, but those documents do not support the answer. You have ungrounded generation dressed up with irrelevant citations.

**Attribution failures** occur when the generation model produces correct information but attributes it to the wrong source. This happens when the model retrieves multiple documents, synthesizes information across them, and then cites only one document or cites the wrong document for a particular claim. For instance, a system retrieves three research papers on a medical topic, generates a summary combining insights from all three, but only cites the first paper. The output is partially grounded but the attribution is incomplete. A user trying to verify a specific claim might not find it in the cited source.

**Hallucination despite retrieval** occurs when the generation model ignores the retrieved content and generates information from its training data, then cites the retrieved documents anyway. This is more common than teams expect. If the retrieved passages are low quality, ambiguous, or do not directly answer the question, the model might default to generating a more fluent answer based on what it knows, while still following the instruction to include citations. You end up with model knowledge presented as if it were sourced from retrieved documents.

Detecting these failures requires evaluating the retrieval step and the generation step independently. You cannot just read the final output and judge whether it seems grounded. You need to check whether the retrieval system returned the right sources, whether the generation model used those sources, and whether the citations accurately reflect which source supports which claim.

## Measuring Groundedness Rigorously

Most teams measure groundedness by asking human raters "Does this output seem well-sourced?" or "Are the citations accurate?" These vibe checks are not sufficient. Rigorous groundedness evaluation requires decomposing outputs into claims, identifying the cited source for each claim, and verifying that the source supports the claim.

Start by **claim extraction**. You break the generated output into individual factual statements. A single sentence might contain multiple claims. "The company reported revenue of $500 million in Q3, up 20% from Q2" contains three claims: Q3 revenue was $500 million, Q2 revenue was lower, and the increase was 20%. Each claim needs independent verification.

Next, **source identification**. For each claim, identify which source the output cites as supporting that claim. In a well-designed system, this is explicit—each claim has an inline citation or footnote. In poorly designed systems, there is a single citation at the end of a paragraph or section, and you have to infer which claims are supposed to be supported by which sources.

Then, **verification**. For each claim-source pair, check whether the source actually supports the claim. This is not a semantic similarity check. You are not asking whether the source discusses related topics. You are asking whether a human reading the source would conclude that the claim is supported by the information in the source. This requires human judgment, domain expertise, and careful reading.

You calculate a groundedness score as the percentage of claims that are accurately grounded. A system that makes 100 claims per output with 92 accurately grounded is 92% grounded. This is different from a correctness score. A claim can be correct but not grounded if the cited source does not support it. A claim can be grounded but incorrect if the source is wrong or if the claim misrepresents what the source says.

Some teams automate parts of this process using LLM-based evaluators. You can prompt a strong model like GPT-5.1 or Claude 4 Opus to extract claims from generated text, identify citations, retrieve the cited source passages, and judge whether each passage supports each claim. This works reasonably well for catching egregious failures—claims with no citation, citations to non-existent sources, claims that contradict their cited sources. But automated evaluation struggles with nuance. If a source says "up to 20% of patients experience side effects" and the generated claim says "20% of patients experience side effects," is that grounded? The claim is technically not what the source says, but it might be a reasonable interpretation. Human judgment is still necessary for borderline cases.

## The Grounding-Fluency Trade-Off

There is an inherent tension between groundedness and fluency. Highly grounded outputs that stay extremely close to source material often sound stilted and repetitive. Fluent, natural-sounding outputs often involve synthesis and paraphrasing that moves away from exact source wording, which introduces opportunities for attribution errors.

Consider a system that generates investment research summaries from earnings call transcripts. A highly grounded approach would generate text that closely paraphrases the transcript with frequent citations: "The CEO stated that revenue growth was driven by strong performance in the enterprise segment (transcript page 3). Operating margins improved due to cost reductions in sales and marketing (transcript page 5). The company expects continued momentum in the second half of the year (transcript page 7)." This is verifiable but reads like a collection of extracted quotes.

A more fluent approach would synthesize across the transcript: "The company demonstrated strong performance in Q2, with enterprise revenue growth and improved operating margins positioning them well for the remainder of the year." This is more readable but harder to ground. Which part of the transcript supports "positioning them well for the remainder of the year"? It is an inference drawn from multiple statements rather than a direct quote. You could cite the whole transcript, but that is document-level grounding, not claim-level.

Your product requirements determine where you land on this trade-off. For regulatory compliance, legal analysis, medical decision support, and financial reporting, you prioritize groundedness even at the cost of fluency. Users need to verify claims, and verification requires precise attribution. For consumer applications, content generation, and creative tasks, you prioritize fluency because users are less likely to verify sources and more likely to value readability.

You can mitigate the trade-off through interface design. Instead of forcing the generated text to include inline citations that disrupt reading flow, you can provide citations in a sidebar or tooltip that appears when users hover over a claim. You can generate fluent text and then separately display the source passages that support each claim. You can offer two modes: a reading mode that hides citations and a verification mode that shows them. But you cannot eliminate the trade-off entirely. Groundedness requires linking claims to sources, and those links add complexity.

## Citation Accuracy as a Reliability Signal

When you deploy an AI system with citations, users interpret those citations as a reliability signal. The presence of sources suggests that the information is verifiable and trustworthy. If the citations are inaccurate, you have not just failed at groundedness—you have actively misled users into trusting incorrect information.

This is why the legal AI startup faced a lawsuit. Their customers relied on the cited contract sections to make business decisions. When the citations turned out to be wrong, customers argued they had been deceived. A system without citations would have been less useful but also less dangerous because users would have known they needed to verify claims themselves. By providing citations, the system made an implicit promise of verifiability that it did not fulfill.

The EU AI Act, which came into full enforcement in 2026, classifies certain AI systems as high-risk based on their application domain and impact. Systems used for legal analysis, medical diagnosis, credit scoring, and employment decisions face strict transparency and accuracy requirements. For these systems, providing citations is not optional—it is a regulatory requirement. But providing inaccurate citations may be worse than providing no citations at all, because it creates a false sense of rigor.

You need to establish a quality threshold for citation accuracy before you deploy citations in production. If your groundedness evaluation shows that fewer than 95% of citations are accurate, you should not present citations to users. Instead, you should invest in improving your attribution system until you can meet a reliability standard. It is better to launch without citations and add them later than to launch with unreliable citations that erode trust.

## Improving Groundedness in RAG Systems

If your groundedness metrics are below target, you have several intervention points. The most common failure is at retrieval—the system does not have access to the right information, so it cannot ground its outputs properly. You improve retrieval by refining your chunking strategy, improving your embedding model, expanding your knowledge base, or using hybrid search that combines semantic and keyword matching.

The second intervention point is prompt design. You instruct the model explicitly to only use information from retrieved passages and to cite sources for every claim. You can provide examples of properly grounded outputs in few-shot prompts. You can instruct the model to quote directly from sources rather than paraphrasing when precision is critical. Some teams use constitutional AI techniques to train models that are more naturally inclined to stick to source material.

The third intervention point is post-processing. After the model generates an output with citations, you verify the citations programmatically. You extract each cited passage, extract each claim, and use a verification model to check whether the passage supports the claim. If the verification model flags a claim as ungrounded, you either remove the claim, remove the citation, or regenerate that part of the output. This adds latency and cost, but it significantly reduces the rate of citation errors that reach users.

Some teams use a two-step generation approach: first, generate quotes or extracted information from sources with citations, then synthesize those quotes into fluent text while preserving the citations. This separation makes it easier to maintain accurate attribution because the synthesis step operates on already-grounded material.

## When Groundedness Conflicts With Usefulness

Strict groundedness can sometimes make outputs less useful. If your knowledge base does not contain the information needed to answer a question fully, a grounded system will give an incomplete answer or admit it does not know. An ungrounded system will use model knowledge to provide a more complete answer. Which is better?

The answer depends on your users and your domain. In medical, legal, and financial applications, users need to know when information is not in the authoritative sources. An admission of uncertainty is more valuable than an unverifiable claim. In consumer applications, users often prefer a helpful answer even if it is not fully grounded, as long as the system is not presenting model knowledge as if it were sourced from documents.

One approach is to separate grounded and ungrounded information in the output. The system generates a response that includes information from retrieved documents, clearly cited, and also includes relevant background information from model knowledge, clearly labeled as general knowledge rather than sourced from the specific documents. This transparency allows users to distinguish between what is verifiable in the provided sources and what is supplementary.

Another approach is to adjust groundedness requirements based on claim criticality. High-stakes claims—numbers, dates, legal obligations, medical recommendations—must be grounded in sources. Lower-stakes claims—background context, definitions, general explanations—can draw on model knowledge. You classify claims by risk level and apply different grounding standards accordingly.

## Groundedness as a Competitive Advantage

As AI systems proliferate, users are becoming more sophisticated about evaluating quality. Early adopters might have been impressed by fluent text generation. Current users want to know where information comes from and whether they can verify it. Groundedness is evolving from a nice-to-have feature to a competitive requirement.

Products that provide accurate, claim-level citations differentiate themselves from products that provide vague or unreliable sources. In enterprise sales, procurement teams now routinely ask vendors to demonstrate how their AI systems ground outputs in source material. They conduct citation accuracy audits as part of vendor evaluation. If your system cannot pass that audit, you lose deals.

Groundedness also affects user trust and retention. Users who discover that citations are inaccurate stop trusting the entire system, even for claims that are correctly cited. One bad experience with a hallucinated source undermines confidence in all future outputs. Conversely, users who verify a few citations and find them accurate develop trust that generalizes to unverified claims. Citation accuracy has a multiplicative effect on perceived reliability.

The next quality dimension you must address is completeness, which governs whether your outputs answer the full question or only part of it. An output can be perfectly correct and impeccably grounded but still fail if it omits critical information that the user needed to make a decision.
