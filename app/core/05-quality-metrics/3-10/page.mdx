# 3-10 â€” Custom Metric Design Patterns and Anti-Patterns

In March 2024, a healthcare AI startup with fourteen engineers spent six weeks building what their product lead called "the most comprehensive quality dashboard in medical summarization." They tracked forty-seven different metrics across their clinical note generation system. Token accuracy, BLEU score, entity extraction precision, readability indices, sentiment alignment, temporal consistency, negation handling, medication name accuracy, diagnosis code matching, and thirty-eight more. Every metric had a Grafana panel. Every release triggered a cascade of numbers. The dashboard was beautiful. The product was failing.

Their Series A investors pulled term sheet negotiations in May after a pilot hospital reported that twenty-three percent of generated summaries contained medication dosage errors that human reviewers had missed because they were focused on the metrics that looked good. The startup had measured everything except the one thing that mattered: whether the summary would cause a doctor to make a different clinical decision than they would have made reading the original notes. The forty-seven metrics created an illusion of rigor while obscuring the fundamental quality question. When you design custom metrics, you face a choice between measurement theater and genuine insight. Most teams choose theater without realizing it.

## The Custom Metric Necessity

Off-the-shelf metrics fail when your product does something that existing benchmarks were not built to measure. You cannot evaluate a legal contract analysis tool with ROUGE scores because contract quality is not about word overlap with reference documents. You cannot evaluate a customer service chatbot with perplexity because low perplexity responses can still make customers angry. You cannot evaluate a code generation system with pass-at-k alone because passing tests does not mean the code is maintainable, secure, or aligned with team conventions. The moment your product's value proposition diverges from what standard benchmarks measure, you enter custom metric territory.

The problem is that designing custom metrics is hard and most teams approach it incorrectly. They start with the question "what can we measure" instead of "what determines whether this product succeeds or fails." This leads to **metric proliferation**, where teams add measurement after measurement hoping that quantity will compensate for lack of clarity. It does not. A hundred irrelevant metrics provide less insight than one metric that captures actual user value. The healthcare startup measured forty-seven things because they had not done the harder work of figuring out what actually mattered.

Custom metrics are necessary when you operate in domains with specialized quality requirements, when your product combines multiple capabilities that need coordinated evaluation, when user value comes from properties that standard metrics ignore, or when you need to measure improvement on dimensions that matter to your specific market. If you are building medical AI, you need custom metrics for clinical accuracy. If you are building legal AI, you need custom metrics for liability risk. If you are building financial AI, you need custom metrics for regulatory compliance. The question is not whether you need custom metrics but whether you will design them rigorously or haphazardly.

## Design Pattern: Decomposition Metrics

**Decomposition metrics** break a complex quality concept into measurable components that can be evaluated separately and then combined. This pattern works when the overall quality you care about is too abstract to measure directly but can be understood as a combination of specific properties. A legal contract analysis system might decompose "contract quality" into completeness, consistency, risk identification, precedent alignment, and clause clarity. Each component gets its own measurement approach, and the combination provides insight into overall quality.

The healthcare startup needed a decomposition approach. Instead of measuring clinical summary quality directly, they could have decomposed it into clinical decision support value, factual consistency with source notes, temporal ordering accuracy, and critical information prominence. Each component requires different measurement techniques. Clinical decision support value requires expert review or physician surveys. Factual consistency can be measured with semantic entailment models. Temporal ordering can be checked algorithmically. Critical information prominence can be measured by checking whether important findings appear in summary headers. The decomposition forces you to be explicit about what aspects of quality matter and how to measure each one.

The danger with decomposition is creating components that do not actually compose back into the quality property you care about. If you decompose customer service quality into response time, sentiment positivity, and entity extraction accuracy, you have missed whether the response actually solved the customer's problem. The components must be necessary conditions for the quality property, and their combination must be sufficient. This requires domain expertise and validation against real outcomes. You cannot decompose by intuition alone.

Decomposition metrics work best when combined with **weighting schemes** that reflect the relative importance of components. A five-component decomposition where all components are weighted equally is probably wrong. In the legal contract example, risk identification might be three times as important as clause clarity. In the medical example, factual consistency might be ten times as important as readability. The weights should come from stakeholder input, outcome data, or failure analysis, not from democratic averaging. If you cannot justify the weights, you have not understood the domain well enough to build custom metrics.

## Design Pattern: Proxy Metrics

**Proxy metrics** measure something adjacent to the property you actually care about because the real property is too expensive or too slow to measure directly. User satisfaction is what you care about, but surveying every user is impractical, so you measure task completion rate as a proxy. Deployment readiness is what you care about, but running full integration tests on every commit is too slow, so you measure unit test coverage as a proxy. Proxies trade perfect measurement for practical measurement.

The key to effective proxy metrics is understanding the relationship between the proxy and the real outcome. A good proxy has high correlation with the outcome, measures something that actually causes the outcome rather than just correlating by accident, and remains stable as your product evolves. Task completion rate is a good proxy for satisfaction if task completion actually causes satisfaction and if the relationship holds across different user segments and product versions. If users complete tasks but remain frustrated by the experience, the proxy breaks.

In 2025, a fintech company building an AI financial advisor used conversation length as a proxy for user engagement, assuming longer conversations meant users were getting value. They optimized their model to extend conversations through follow-up questions and elaboration. Average conversation length increased by forty percent over three months. User retention decreased by eighteen percent. The proxy had broken because longer conversations were not causing engagement; they were causing frustration as users had to work harder to get simple answers. The model had learned to optimize the proxy while destroying the outcome.

Proxy metric failure follows predictable patterns. The proxy becomes the target, and the system learns to game the proxy without improving the underlying outcome. The relationship between proxy and outcome changes as the product evolves, but teams continue using the stale proxy. The proxy works in some contexts but not others, and teams apply it uniformly. Or the proxy never had a causal relationship with the outcome and only appeared to work because of confounding factors. Validating proxies requires checking correlation, testing causation through experiments, monitoring the relationship over time, and being willing to abandon proxies when they break.

## Design Pattern: Compound Metrics

**Compound metrics** combine multiple measurements through formulas or models to produce a single score that represents overall quality. F1 score combines precision and recall through harmonic mean. Customer lifetime value combines purchase frequency, average order value, and retention rate through multiplication. Compound metrics let you track multiple dimensions while maintaining a single quality indicator. They work when you need to balance trade-offs between competing objectives or when quality truly is multidimensional.

The formula matters enormously. Arithmetic mean treats all components equally. Harmonic mean punishes low scores on any component. Geometric mean provides multiplicative combination. Weighted sums let you specify importance. Minimum across components enforces that all must be good. Maximum across components rewards strength in any area. The choice encodes assumptions about how the components relate to quality. If you use arithmetic mean when you should use minimum, you will ship products that are terrible on critical dimensions but average overall.

A customer support AI company in 2024 built a compound metric called Response Quality Score that combined answer accuracy at forty percent weight, response time at thirty percent, and politeness at thirty percent. The formula was a weighted sum normalized to a zero to one hundred scale. The model scored seventy-five on average. They shipped to a major client. Within two weeks, the client escalated that fifteen percent of responses contained factually incorrect information that damaged customer trust. The compound metric had hidden the accuracy problem because fast and polite incorrect answers still scored well. A minimum-based compound metric would have surfaced the issue immediately.

Compound metrics fail when the formula does not match the actual logic of how components determine quality, when weights are set arbitrarily instead of being derived from outcome data, when the aggregation hides critical failures, or when the components themselves are poorly chosen. Building effective compound metrics requires domain expertise to choose components, outcome analysis to determine weights, and validation against edge cases to ensure the formula behaves correctly when components are unbalanced. The healthcare startup's forty-seven metrics were not failing because they were compound; they were failing because the components did not matter and the weights were nonsense.

## Anti-Pattern: Metric Soup

**Metric soup** is what happens when teams add metrics without removing them, measure everything that seems interesting, and create dashboards with dozens or hundreds of numbers that no one actually uses to make decisions. The healthcare startup had metric soup. So do most teams building custom metrics. The soup feels safe because more measurement seems more rigorous, but it actually obscures insight and wastes engineering time maintaining measurement infrastructure that provides no value.

Metric soup emerges from several organizational failures. Teams conflate comprehensiveness with rigor, believing that measuring more things makes them more scientific. Product managers add metrics to demonstrate thoroughness without thinking about decision-making. Engineers build measurement infrastructure because it is technically interesting regardless of whether anyone uses it. Executives ask for metrics without specifying what decisions the metrics should inform, so teams measure everything defensively. The soup thickens over time as metrics accumulate but are never deprecated.

The cost of metric soup is higher than most teams realize. Engineering time spent instrumenting, computing, storing, and displaying irrelevant metrics. Cognitive load for everyone who looks at dashboards and tries to make sense of the numbers. Decision paralysis when metrics conflict and no one knows which ones matter. False confidence from seeing lots of green numbers when critical quality problems are unmeasured. Opportunity cost of not building the few metrics that would actually drive better decisions. The healthcare startup spent more time maintaining their forty-seven metrics than they spent on the human review process that would have caught the medication errors.

Fixing metric soup requires discipline. Audit existing metrics and ask for each one: what decision does this inform, who uses it to make that decision, and what would we do differently based on different values. If you cannot answer all three questions, remove the metric. Enforce a metric budget where adding new metrics requires removing old ones. Separate diagnostic metrics that you check occasionally from core metrics that drive decisions. Build metrics in response to specific decisions you need to make, not preemptively. The goal is not zero metrics; it is the right metrics.

## Anti-Pattern: Metric Fog

**Metric fog** occurs when metrics have unclear definitions, ambiguous measurement procedures, or uncertain relationships to product quality. You track "response relevance" but three different people on your team have three different understandings of what relevance means. You measure "hallucination rate" but the measurement procedure depends on the judgment calls of individual raters. You monitor "user engagement" but nobody has specified whether engagement is good or bad for your particular product. The numbers are there but they do not mean anything precise.

Metric fog is more insidious than metric soup because the metrics look legitimate but do not support actual decision-making. A product manager cannot say "we need relevance above eighty-five percent before shipping" if relevance does not have a precise definition that measurement consistently implements. An engineer cannot debug why relevance dropped from seventy to sixty if the measurement procedure is ambiguous. An executive cannot compare quality across product areas if each area defines metrics differently. Fog creates the appearance of measurement without the substance.

The most common source of metric fog is skipping the operationalization step. Teams agree that they need to measure "factual accuracy" but never specify what counts as a fact, what sources are considered authoritative, how to handle claims that are partially accurate, or what measurement procedure will compute the score. They hand-wave "we'll use human raters" without creating rubrics, training procedures, or inter-rater reliability checks. They build automated scorers without validating that the scorers measure what they are supposed to measure. The metric name exists but the metric does not.

Eliminating metric fog requires written specifications for every metric. The specification must define what the metric measures in precise language, describe the measurement procedure in enough detail that two people could independently implement it and get the same results, provide examples of edge cases and how they should be scored, state assumptions and limitations, and link the metric to product decisions. If you cannot write this specification, you do not have a metric yet. This seems like bureaucratic overhead but it is actually thinking overhead. The writing forces the clarity that metrics require.

## Anti-Pattern: Metric Theater

**Metric theater** is measurement that looks rigorous but does not connect to product decisions or real outcomes. You run extensive benchmark evaluations before every release and publish the results in internal dashboards, but you ship regardless of what the benchmarks say. You track quality metrics that show your product is excellent, but customer churn is increasing. You measure properties that are easy to measure rather than properties that matter. Theater metrics make stakeholders feel good without improving product quality.

The healthcare startup was doing metric theater. Their forty-seven metrics made the engineering team feel scientific and made executives feel that quality was under control, but the metrics did not connect to clinical outcomes. When medication errors appeared in production, none of their forty-seven metrics had predicted or detected the problem. The measurement was elaborate but decorative. Theater metrics are worse than no metrics because they create false confidence while consuming resources.

Metric theater often involves measuring proxy properties that correlate with quality in benchmark settings but not in production. You measure BLEU score because it is easy to compute and because academic papers use it, not because BLEU actually predicts whether your users will be satisfied. You track model latency because infrastructure teams care about it, not because your users notice the difference between two hundred and three hundred milliseconds. You monitor token accuracy because token-level metrics are standard, even though your users care about document-level coherence. The metrics are real but they measure the wrong things.

Detecting metric theater requires asking uncomfortable questions. When this metric is red, do we delay the release or do we ship anyway and blame the metric? When we improved this metric by twenty percent last quarter, did any user outcomes actually improve? If we stopped measuring this tomorrow, would anyone notice or would any decisions change? If you are measuring it but not using it, you are doing theater. The fix is not better metrics; it is connecting metrics to decisions and being willing to act on what the metrics tell you.

## Validation: Does Your Custom Metric Actually Work

Building a custom metric is hypothesis creation. You hypothesize that this measurement captures something important about product quality. Like any hypothesis, it requires testing. Most teams skip this step. They design a metric, implement it, start tracking it, and assume it works because the numbers look reasonable. Then they discover months later that the metric was measuring something other than what they thought or that optimizing the metric made the product worse.

**Concurrent validation** checks whether your metric correlates with ground truth quality measurements. You collect metric scores and ground truth labels for the same examples, then measure correlation. If your custom "clinical decision support value" metric is supposed to predict whether doctors find summaries useful, you need to collect both metric scores and doctor usefulness ratings, then check if high metric scores correspond to high usefulness ratings. Low correlation means your metric is not measuring what you think it measures. This requires having ground truth, which is often expensive, but you only need it for validation, not for ongoing measurement.

**Predictive validation** checks whether your metric predicts future outcomes. If your metric is supposed to measure quality, higher metric scores should lead to better user retention, lower support tickets, higher task success rates, or whatever outcome your product is trying to drive. Collect metric data for different users or time periods, then check if metric values predict outcome differences. A support quality metric that does not predict satisfaction or retention is not measuring quality. This validation takes time because you need to wait for outcomes, but it is essential for knowing if the metric matters.

**Intervention validation** checks whether deliberately changing the metric value changes the outcome you care about. Run an experiment where you modify your product to improve the metric score without changing anything else, then measure impact on outcomes. If improving your "response relevance" metric by twenty percent does not improve user satisfaction or task completion, the metric might be measuring something irrelevant. This is the strongest validation because it tests causation rather than just correlation, but it requires the ability to manipulate the metric independently.

**Failure case validation** checks whether your metric correctly identifies known bad examples. Collect examples where your product failed or where users complained, then check if your metric scored them as low quality. If your metric gives high scores to examples that users hated, the metric is broken. Also check the reverse: collect examples where your product succeeded and users were delighted, and verify that your metric scores them highly. A metric that cannot distinguish success from failure is useless regardless of how sophisticated its methodology seems.

## The Metric Design Review Process

Custom metrics should go through a formal design review before implementation, not because bureaucracy is good but because most custom metrics are bad and peer review catches problems early. The review should cover metric purpose, operationalization, validation plan, and maintenance cost. The purpose question forces clarity about what decision this metric informs and why existing metrics are insufficient. The operationalization question ensures the metric has a precise definition and measurement procedure. The validation plan specifies how you will know if the metric works. The maintenance cost estimate prevents teams from building metrics they cannot afford to compute.

The review should include stakeholders from multiple functions. Product managers verify that the metric connects to user value and product strategy. Engineers verify that the metric is computable with available data and reasonable cost. Domain experts verify that the metric makes sense for the specific domain. Data scientists verify that the measurement methodology is sound. Users or user researchers verify that the metric aligns with actual user needs. A metric designed by a single function in isolation will reflect that function's blind spots.

The review should require written documentation. The act of writing forces precision that conversation does not. The document should include metric definition, motivation for why this metric is needed, measurement procedure, validation plan, maintenance cost estimate, decisions this metric will inform, success criteria for the metric itself, and known limitations. If the team cannot produce this document, they are not ready to implement the metric. The document also serves as a contract. If the metric passes review and gets implemented, teams commit to actually using it for the specified decisions.

Not all custom metrics need this level of process. Diagnostic metrics that you check occasionally or experimental metrics that you are testing can follow a lighter process. But core metrics that will drive product decisions, appear in executive dashboards, or determine release criteria should go through rigorous review. The cost of implementing a bad metric that drives bad decisions for months is much higher than the cost of spending a few hours on review before implementation.

## When Standard Metrics Become Custom Metrics

Sometimes what looks like a standard metric becomes a custom metric when you adapt it to your domain. Accuracy seems standard, but defining what counts as correct in your specific application requires custom judgment. Precision and recall are standard formulas, but deciding how to define true positives and false positives for your use case is custom work. Latency is a standard infrastructure metric, but determining acceptable latency thresholds for your user experience is custom analysis.

A code generation company in 2025 used "test pass rate" as their core quality metric because it seemed standard and objective. Tests either pass or fail. But as they expanded to different programming languages and frameworks, they discovered that pass rate meant different things in different contexts. In strongly-typed languages, passing tests indicated high correctness because the type system caught many errors. In dynamic languages, passing tests meant less because type-related bugs were not tested. In systems with extensive test suites, pass rate was meaningful. In systems with sparse tests, it was not. What seemed like a standard metric became a custom metric once they accounted for these contextual factors.

The adaptation process requires domain expertise and empirical validation. You start with the standard metric definition, then modify it based on your domain requirements, validate that the modification improves predictive value for your outcomes, document the modification clearly so others understand you are not using the standard version, and monitor whether the modification remains valid as your product evolves. Undocumented adaptation is how teams end up with fog. Everyone thinks they are measuring accuracy but they are actually measuring five different things all called accuracy.

## Building a Custom Metric Library

Organizations that build multiple AI products or multiple versions of the same product should maintain a **custom metric library** that documents proven metrics, their definitions, measurement procedures, validation results, and lessons learned. This prevents reinventing metrics for every project and ensures consistency across teams. The library should include both metrics currently in use and deprecated metrics with explanations of why they were retired. Failed metrics are valuable learning artifacts.

The library entry for each metric should include everything needed to implement and use it. Precise definition of what is being measured. Step-by-step measurement procedure. Code or pseudocode for computation. Validation evidence showing the metric works. Known limitations and failure modes. Decisions the metric should and should not inform. Cost estimates for computation. Ownership and maintenance responsibility. Without this documentation, the library becomes a graveyard of metric names that nobody understands.

The library should be versioned because metrics evolve. When you discover a bug in a metric's implementation or a better way to compute it, you create a new version rather than silently changing the existing one. This maintains reproducibility and lets you understand how changes in metric values relate to changes in metric definitions versus changes in actual quality. A metric that changed definition three times and shows improvement might not represent actual improvement at all.

The library should not become a dumping ground. Apply the same discipline to library entries that you apply to individual metrics. Remove metrics that are no longer used. Mark metrics as deprecated when better alternatives exist. Require new submissions to justify why existing metrics are insufficient. The goal is a curated collection of proven metrics, not an archive of every measurement anyone ever tried.

## The Iterative Nature of Custom Metric Design

Your first custom metric will probably be wrong. This is normal. Metric design is iterative. You build version one based on your best understanding of what matters, you deploy it and see how it behaves, you discover ways it breaks or misleads, you revise it, and you repeat. The teams that succeed with custom metrics are not the teams that design perfect metrics on the first try; they are the teams that iterate quickly and learn from failures.

The iteration cycle should be deliberate. Set a review date when you implement a new metric. After one month or one quarter, evaluate whether the metric is actually informing decisions, whether it correlates with outcomes, whether it has surprised you in good or bad ways, and whether you would design it differently knowing what you know now. Treat this as a formal review, not an ad hoc check. Many bad metrics persist because nobody ever stops to evaluate whether they are working.

Be willing to kill metrics that are not working. This is harder than it sounds because metrics become institutional fixtures and people resist change. But a metric that looked good in theory and fails in practice should be retired, not continued out of inertia. Document why you are killing it so future teams do not make the same mistake. Replace it with something better, not with nothing. The goal is learning, not measurement continuity.

The best metric designers maintain a practice of **metric retrospectives** after major product decisions or releases. What did the metrics tell us before the decision? What actually happened after the decision? If the metrics predicted wrong, why? If they predicted correctly, what gave them predictive power? These retrospectives build institutional knowledge about what kinds of metrics work in what contexts. Without them, teams repeat the same metric design mistakes every cycle.

With patterns understood and anti-patterns identified, the next critical consideration is ensuring your metrics are not contaminated by leakage between training and evaluation data, which Chapter 3-11 addresses through systematic contamination detection methodology.
