# 5.4 â€” Interleaving Experiments and Online Evaluation

The search relevance team thought they had run a definitive experiment in February 2026. They tested their new neural ranking model against the existing lexical model using a standard A/B test with 10,000 users per group over three weeks. The results showed no statistically significant difference in click-through rate: 24.3% for the new model versus 24.1% for control, with a p-value of 0.78. The team concluded that the expensive new model provided no benefit and decided to stick with the existing system. Two months later, a researcher suggested trying an interleaving experiment as a sanity check. They showed users results from both models mixed together in the same result list and tracked which results users clicked. The interleaving experiment, running for just five days with 2,000 users, showed a clear preference for the new model: users clicked new model results 58% of the time versus 42% for control model results, with a p-value below 0.001. The new model was substantially better, but the A/B test had been too insensitive to detect it. The team had nearly made a catastrophic decision based on an underpowered experiment. They shipped the new model in May, and over the following quarter, engagement metrics improved by 7%, translating to approximately 1.2 million dollars in annual revenue impact.

This case demonstrates why interleaving experiments have become the gold standard for evaluating ranking and retrieval systems. Interleaving provides much higher statistical sensitivity than A/B testing by creating within-user comparisons rather than between-user comparisons. When you show a user results from both models simultaneously and measure which results they prefer, you control for user heterogeneity, context variability, and temporal factors that add noise to between-user comparisons. The sensitivity gain is not marginal. Interleaving experiments typically require five to ten times fewer users than A/B tests to achieve the same statistical power. For products with limited traffic or small effect sizes, this difference determines whether you can measure quality improvements at all.

## The Core Interleaving Mechanism

Interleaving works by combining results from two systems into a single ranked list that is shown to the user, then attributing user interactions back to the system that provided each result. The attribution reveals which system's results users prefer. The simplest interleaving method, called **team draft interleaving**, works like a sports team draft. The two systems take turns selecting results to add to the interleaved list. System A picks its top result, then system B picks its top result, then system A picks its next-highest result that has not yet been selected, and so on. Results that both systems rank highly appear near the top of the interleaved list. Results that only one system ranks highly appear lower in the interleaved list. When the user clicks a result, you attribute that click to whichever system originally provided that result.

The key insight is that user clicks reveal relative preference between systems within the same context. If both systems rank document X highly but system A ranks document Y highly while system B ranks document Z highly, and the user clicks Y but not Z, that is evidence that system A is doing something right that system B is not. By accumulating many such comparisons across users and queries, you build a statistical case for which system is better. The comparisons happen within users rather than between users, which eliminates between-user variance from the measurement.

The sensitivity advantage comes from reducing variance. In an A/B test, user-to-user variance is enormous. Some users are highly engaged clickers and some barely click at all. Some users have high-intent queries and some are just browsing. Some users arrive in the morning when they are focused and some arrive in the evening when they are tired. All this variance inflates the confidence intervals around your treatment effect estimate. In an interleaving experiment, you compare treatment to control within the same user, same query, same time, same context. The between-user variance does not affect the comparison because each user serves as their own control. The remaining variance comes from whether system A happened to have better results than system B for this particular query, which is much smaller than the variance across users.

## Team Draft Interleaving Implementation

Implementing team draft interleaving requires careful handling of several edge cases and design decisions. You need to decide how to handle ties, where both systems rank the same document at the same position. You need to decide what to do when one system runs out of unique documents before the other. You need to decide how many positions to interleave and what to do with the remaining positions if the interleaved list is shorter than the displayed result list. Each decision affects the fairness and sensitivity of the experiment.

The standard approach for ties is to give credit to both systems if a tied document is clicked. If system A ranks document D at position 3 and system B also ranks document D at position 3, and D appears in the interleaved list and gets clicked, both systems receive credit for that click. This maintains fairness because both systems contributed equally to surfacing that document. An alternative approach is to assign credit randomly with 50-50 probability, but this adds noise without improving fairness.

When one system runs out of unique documents, you can either stop the interleaving and fill remaining positions with documents from the system that still has documents, or you can continue the draft by allowing the remaining system to pick documents unopposed. The first approach is cleaner but may not fill the entire result list. The second approach fills the list but gives an advantage to the system with more unique documents. The right choice depends on whether diversity of sources matters for your use case.

A medical information retrieval system implemented team draft interleaving in mid-2025 to compare their new evidence-based ranking system against their existing popularity-based ranking. They had to make a crucial implementation decision about list length. Their interface showed ten results by default but users could expand to see up to thirty results. Should they interleave all thirty potential results, or just the first ten? Interleaving all thirty meant more data per query because clicks on positions 11-30 would count, but it also meant diluting the signal because clicks on lower positions are less frequent and potentially less meaningful. They ran simulations using historical click data and found that interleaving twenty positions gave the best tradeoff between signal and noise. Positions beyond twenty contributed so few clicks that including them added more variance than information.

## Balanced Interleaving and Team Draft Variants

Team draft interleaving has a subtle bias: if system A's top result is more attractive than system B's top result, system A's second result will appear higher in the interleaved list than system B's second result, giving system A's second result more exposure. This compounds advantages for systems that have strong top results. For most applications this bias is actually desirable because you care most about top results, but in some cases you want to evaluate overall ranking quality independent of top-result quality.

**Balanced interleaving** addresses this by ensuring both systems contribute equally to each prefix of the interleaved list. At each position, the algorithm chooses whether to pick from system A or system B in a way that keeps the cumulative contribution from both systems as equal as possible. This requires more complex bookkeeping but produces fairer comparisons when you care about ranking quality throughout the list rather than just at the top.

A video recommendation system tested balanced interleaving versus team draft in late 2025. They found that team draft consistently favored systems that had highly clickable thumbnail images for their top recommendation, even if the overall ranking quality was worse. The first result dominated click behavior so much that subsequent results barely mattered. Balanced interleaving spread the contributions more evenly and gave a clearer signal about overall ranking quality. They adopted balanced interleaving for algorithm development but continued using team draft for final validation experiments because they ultimately cared most about optimizing the top result.

## Probabilistic Interleaving Methods

**Probabilistic interleaving** methods sample documents from both systems with probabilities proportional to their relevance scores, rather than deterministically interleaving by rank order. This provides sensitivity to the magnitude of relevance score differences, not just rank order differences. If system A ranks document D with score 0.9 and system B ranks it with score 0.6, probabilistic interleaving is more likely to include that document in the list when sampling from system A than when sampling from system B. This captures information that team draft interleaving discards.

The tradeoff is complexity and interpretability. Probabilistic interleaving requires calibrated relevance scores, not just rankings. Many systems produce scores that are not calibrated across different queries or document types. Probabilistic interleaving also produces lists that vary randomly between users seeing the same query, which can complicate debugging and user experience consistency. For systems that have well-calibrated scores and where the score information is valuable, probabilistic interleaving can outperform team draft. For most applications, team draft is simpler and sufficient.

## Interleaving Beyond Ranking: Generative AI Applications

Interleaving was developed for ranking and retrieval systems, but the core principle of within-user comparison generalizes to other AI applications. For any system where you can show users outputs from two models in the same session and measure preference, you can apply interleaving logic. Generative AI applications are increasingly using variants of interleaving to measure quality differences that are too subtle to detect with A/B testing.

A legal document drafting AI company wanted to test two versions of their contract generation system in early 2026. One version used GPT-4.5 and the other used a fine-tuned Claude 3.7 model. Standard A/B testing would assign each user to one model and measure downstream outcomes like contract acceptance rate or time to finalization. The company expected small effect sizes and had limited user volume, making A/B testing impractical. Instead, they implemented a side-by-side comparison interface. When a user requested a contract draft, the system generated two drafts in parallel, one from each model, and presented them side-by-side with labels "Version A" and "Version B." Users could choose which version to use as their starting point, and the system tracked which version was chosen. After 500 contracts, the data showed that users chose the Claude-based version 64% of the time, providing clear evidence of preference. The company ran a follow-up A/B test that confirmed the preference translated to downstream outcomes, then deployed the Claude-based system.

This side-by-side approach is a direct generalization of interleaving. Instead of interleaving documents in a ranked list, you are interleaving two complete outputs in a comparison interface. The sensitivity advantage comes from the same mechanism: within-user comparison eliminates between-user variance. The challenge is that explicitly showing two outputs requires UI real estate and user attention. For short outputs like search results, this is feasible. For long outputs like multi-page documents, you need to design the comparison interface carefully to avoid overwhelming users.

## Implicit vs Explicit Preference Signals

Interleaving can measure either implicit preferences (which results users click without being told they are comparing systems) or explicit preferences (asking users to directly compare outputs and indicate preference). Implicit preferences have the advantage of not affecting user behavior. Users interact naturally with the interleaved results without knowing they are in an experiment. Explicit preferences have the advantage of providing stronger signal per interaction because users are directly indicating preference rather than having preference inferred from clicks.

The tradeoff is between signal strength and naturalness. Implicit preferences reflect actual user behavior in production but provide weaker signal because a click or no-click is a noisy indicator of preference. Explicit preferences provide clean signal but may not reflect actual production behavior because asking users to compare and judge changes their interaction mode. They become evaluators rather than natural users.

Most production interleaving systems use implicit preferences because maintaining natural behavior is more important than maximizing signal per interaction. However, explicit preference collection is valuable in research settings or for qualitative validation. If your implicit interleaving experiment shows that users prefer system A, you can validate this by running a small explicit preference study where you ask a subset of users to directly compare outputs and explain why they prefer one over the other. The qualitative feedback helps diagnose what system A is doing better.

## Online Evaluation as Continuous Quality Monitoring

**Online evaluation** extends interleaving and A/B testing into continuous quality monitoring rather than one-time experiments. Instead of running an experiment for a fixed period to decide whether to ship a change, you continuously measure quality on live traffic and detect degradations or improvements in real-time. Online evaluation is essential for AI systems because model quality can drift due to data distribution shift, model degradation, infrastructure changes, or adversarial inputs. You need to know immediately when quality drops, not after users churn.

The simplest online evaluation approach is to track key metrics continuously and set up alerts that fire when metrics move outside expected ranges. Click-through rate, task completion rate, error rate, latency, and user satisfaction scores should be monitored continuously with anomaly detection that accounts for normal variance and seasonality. When a metric drops significantly, you investigate whether the drop is due to a quality issue or some other factor. This is reactive monitoring: you detect problems after they affect users.

A more sophisticated approach is to continuously run shadow experiments where you evaluate new models or algorithm variants on live traffic without serving their outputs to users. You send each live request to both the production system and the shadow system, collect both outputs, and evaluate them offline or with a separate evaluation service. This lets you validate that a new system is better before switching traffic to it, and it lets you detect when the production system starts underperforming relative to alternatives.

A content moderation platform ran continuous shadow evaluation throughout 2025. They served production traffic with their deployed model but also sent every moderation decision request to two shadow models: an updated version of the deployed model retrained on recent data, and an experimental model with a different architecture. They evaluated all three models' predictions against delayed ground truth labels from human reviewers. This setup let them detect when the production model's accuracy started degrading relative to the updated model, which happened in March when the deployed model was six months old and content patterns had shifted. They also detected that the experimental model, while more accurate on average, had much higher false positive rates on a specific content category that was important for user experience. Without shadow evaluation, they might have shipped the experimental model based on offline metrics and caused a user experience problem.

## Designing Online Evaluation Without Degrading UX

The main risk of online evaluation is degrading user experience by showing inferior outputs. In traditional A/B testing, you accept this risk for a limited time period and limited fraction of users. In continuous online evaluation, you need mechanisms that prevent sustained exposure to inferior systems. The gold standard is interleaving or side-by-side comparison where users see multiple options and choose, so they are never forced to accept inferior output. When this is not feasible, you need guardrails.

One guardrail is **confidence-based routing**, where you evaluate a new system only on requests where you have high confidence it will perform well. You build a classifier that predicts whether the new system is likely to outperform the production system for a given request based on request features. You route only high-confidence requests to the new system and collect outcomes. As the new system proves itself on high-confidence requests, you gradually expand to medium-confidence and eventually low-confidence requests. This de-risks deployment by limiting initial exposure to cases where the new system is expected to shine.

Another guardrail is **circuit breakers** that automatically revert traffic to the production system if key metrics drop below thresholds. If you start routing 10% of traffic to a new system and error rate spikes or user satisfaction plummets, the circuit breaker immediately shifts that traffic back to production. This requires real-time metric monitoring and automated traffic shifting, which is complex infrastructure, but it prevents sustained damage from deployments that go wrong.

A voice assistant company deployed a new natural language understanding model in mid-2025 with confidence-based routing and circuit breakers. They built a confidence model that predicted whether the new NLU model would correctly interpret user intent based on query features like length, ambiguity, and whether it contained rare words. They initially routed only queries with confidence scores above 0.9 to the new model, representing 20% of traffic. They monitored error rate and user satisfaction in real-time with circuit breakers set at 1.5x production error rate and 5% absolute drop in satisfaction. The new model performed well on high-confidence queries, so they gradually lowered the confidence threshold over two weeks until all traffic was on the new model. At one point during the ramp, they hit a batch of queries with an unexpected pattern that triggered high error rates, and the circuit breaker automatically reverted traffic to the old model within 30 seconds. The team investigated, fixed the issue, and resumed the ramp. Without these guardrails, the error spike would have affected users for much longer.

## Sample Size Requirements for Interleaving

While interleaving is much more sensitive than A/B testing, it still requires adequate sample size. The sample size depends on the magnitude of the quality difference you are trying to detect, the variance in user preferences, and the number of comparisons per user. For ranking systems, a rough guideline is that interleaving requires 5-10x fewer users than A/B testing for the same power, but this varies widely.

A practical approach is to run power analysis based on pilot data or simulations. If you have historical click data, you can simulate what the interleaving experiment would have observed by taking two versions of your system (or the same system at different points in time), generating interleaved rankings for historical queries, and simulating which results would have been clicked based on historical click-through rates. This simulation gives you an empirical distribution of win rates and lets you calculate how many queries you need to detect a given effect size with desired confidence.

An e-commerce search team ran simulations in early 2026 before launching their first interleaving experiment. They took ranking snapshots from their current production model and from a model from six months prior that they knew was worse. They simulated team draft interleaving on 10,000 historical queries and found that the current model won 57% of clicks versus 43% for the old model. The confidence interval on this win rate with 10,000 queries was tight enough to clearly distinguish the systems. They concluded that 10,000 queries, which represented about three days of traffic, would be sufficient to detect differences of 5 percentage points or larger in win rate. For smaller differences, they would need longer experiments or more traffic.

## Combining Offline, Online, and Interleaving Evaluation

The most robust evaluation strategy combines offline evaluation on test sets, online A/B testing or interleaving experiments, and continuous online evaluation monitoring. Each method provides different information and covers different failure modes. Offline evaluation is fast and cheap but may not reflect real user behavior. Online experiments reflect real behavior but are slow and expensive. Continuous monitoring detects regressions but does not tell you whether new variants are better. You need all three.

A mature evaluation pipeline looks like this: First, you develop a new model or algorithm and evaluate it offline on held-out test sets and benchmarks. If offline metrics show improvement, you proceed to shadow evaluation where the new system runs on live traffic but does not serve results to users. If shadow evaluation shows improvement without revealing concerning failure modes, you proceed to a small-scale interleaving or A/B experiment. If the experiment shows positive results, you gradually ramp the new system to more traffic while monitoring with continuous online evaluation. If continuous monitoring detects degradation, you investigate and potentially roll back.

Each stage is a gate that filters out changes that are not ready. Most changes fail at the offline evaluation stage because they do not improve on test sets. Some changes pass offline evaluation but fail shadow evaluation because they have higher latency or cost than expected. Some changes pass shadow evaluation but fail experiments because offline metrics did not correlate with user behavior. Some changes pass experiments but reveal problems during ramp due to edge cases that did not appear in experiments. This multi-stage pipeline is expensive to build and maintain, but it dramatically reduces the risk of shipping quality regressions to users.

A fraud detection company built this pipeline in 2025 after several incidents where models that looked good offline performed poorly in production. They instituted a rule that no model could be deployed without passing all four stages: offline eval showing at least 2% relative improvement in precision at fixed recall, shadow eval showing no increase in latency or error rate, A/B test showing at least 1% improvement in fraud detection rate with no increase in false positives, and one week of successful ramp with continuous monitoring. This discipline slowed down deployments but eliminated production incidents. Over six months, they evaluated twenty-three model changes. Fourteen failed offline eval, three failed shadow eval, two failed A/B testing, and one was rolled back during ramp due to an edge case. Only three of the original twenty-three made it to full production. This aggressive filtering ensured that every deployed change was a genuine improvement.

## When Interleaving Fails or Misleads

Interleaving is not a silver bullet. It works well for ranking and retrieval systems where user preferences can be inferred from interactions, but it fails or misleads in several situations. If user clicks do not correlate with actual value, interleaving optimizes for clickbait rather than quality. If the two systems produce results that are differently formatted or visually presented, users may prefer one system for superficial reasons unrelated to relevance. If one system is much faster than the other, speed differences may confound preference signals in ways that are hard to isolate.

The clickbait problem is particularly insidious. If system A surfaces sensational but low-quality content and system B surfaces accurate but boring content, users may click system A's results more often even though system B's results are objectively better. Interleaving will conclude that system A is better because it measures revealed preference through clicks, not objective quality. You need to guard against this by tracking downstream metrics like time on page, bounce rate, or explicit feedback that indicate whether clicked results were actually valuable.

A news recommendation system faced this in late 2025. They tested a new model that was trained to maximize click-through rate against their existing model that was trained to balance clicks with content quality signals. The interleaving experiment showed clear preference for the new model: users clicked its recommendations 61% of the time versus 39% for the existing model. The team was ready to ship, but a product manager insisted on checking downstream metrics. They found that articles recommended by the new model had 30% higher bounce rate and 25% shorter read time, indicating that users were clicking but not engaging with the content. The new model had learned to optimize for clickbait headlines. Interleaving correctly measured that users clicked more often, but clicking more often was not the right goal. The team redesigned the reward function to include engagement metrics and retrained the model.

The complementarity between interleaving experiments and broader quality frameworks becomes clear when you recognize that no single evaluation method tells the complete story, and that building durable AI products requires constantly checking whether your optimizations are aligned with genuine user value rather than proxy metrics that can be gamed.
