# 2.1 — Correctness: Factual Accuracy and Logical Validity

On March 14, 2025, a Series B healthcare technology company lost a $3.2 million contract renewal with a major hospital network. The company's AI medical documentation assistant had been deployed across 47 clinical sites, helping physicians generate discharge summaries and referral letters. The system achieved a 94% user satisfaction score and cut documentation time by 60%. By every metric the product team tracked, the system was performing beautifully. Then a routine audit by the hospital's compliance team discovered that 11% of generated documents contained factually incorrect medication dosages. The dosages were logically valid—they fell within standard ranges and followed proper formatting—but they were wrong. In three cases, the AI had transcribed "50mg" when the physician had dictated "15mg." The errors never caused patient harm because pharmacists caught them during verification, but the hospital network terminated the contract immediately. The startup's head of product had been tracking response time, completion rate, physician satisfaction, and BLEU scores against reference summaries. Nobody had been systematically checking whether the numbers in the outputs were actually correct.

This is the correctness problem. You think you are measuring it, but you are probably measuring something adjacent to it. Correctness is not user satisfaction. It is not fluency. It is not similarity to human-written examples. **Correctness** is whether the output is factually accurate and logically valid for the specific input provided. It is the most fundamental quality dimension, the one that seems obvious until you try to operationalize it, at which point you discover that your mental model of correctness was far too simple.

## The Two Components of Correctness

Correctness has two independent components that you must evaluate separately: factual accuracy and logical validity. A statement can be factually accurate but logically invalid. A statement can be logically valid but factually inaccurate. Most failures of correctness involve one component failing while the other succeeds, which is why they slip past evaluation systems that conflate the two.

**Factual accuracy** means the claims in the output correspond to reality or to the information in the input. If you ask an AI system what the capital of France is and it responds "Paris," that is factually accurate. If it responds "Lyon," that is factually inaccurate, even though Lyon is a real French city and the response is grammatically correct and logically coherent. Factual accuracy is about the correspondence between symbols and the world they represent.

**Logical validity** means the reasoning in the output follows from the premises and does not contain internal contradictions. If a system analyzes a contract and concludes "This agreement expires on June 30, 2026, therefore it will still be active on July 15, 2026," the logical validity is compromised even if June 30, 2026 is the factually accurate expiration date. The conclusion does not follow from the premise. Logical validity is about the coherence of the reasoning chain, independent of whether the underlying facts are correct.

Consider a legal AI system analyzing a commercial lease. The system extracts the monthly rent as $12,000 from the contract text—factually accurate. It then calculates the annual rent as $144,000—logically valid multiplication. But the contract includes an escalation clause that increases rent by 3% each year, and the system fails to account for this when projecting total costs over a five-year term. The individual components are correct, but the composite answer is wrong because the logical model of how to combine those facts is incomplete. You need to evaluate both the fact extraction and the reasoning process to understand where correctness broke down.

## How Correctness Varies by Task Type

The operational definition of correctness changes dramatically based on task type. A generation task, an extraction task, and a classification task all require different correctness evaluation approaches, and teams routinely fail by applying the wrong evaluation method to their task.

For **extraction tasks**, correctness is relatively straightforward to define. You have source material and you are pulling specific information from it. Either the extracted value matches the source or it does not. When you extract a customer name from an email, there is a ground truth answer embedded in that email. Your evaluation framework can compare the extracted value character-by-character against the true value. The challenge is handling cases where the true value is ambiguous—an email signature might include "John Smith, PhD" and you need to decide whether "John Smith" or "John Smith, PhD" is the correct name extraction. But at least the space of possible correct answers is finite and bounded by the source material.

For **classification tasks**, correctness means assigning the input to the right category from a predefined set. When you classify customer support tickets into "billing," "technical," or "account," there is typically one correct category, though some inputs legitimately belong to multiple categories. The correctness evaluation is whether the predicted category matches the true category. The complication emerges when your category definitions are fuzzy or when new types of inputs arise that do not fit cleanly into your original taxonomy. A ticket about "I was billed for a technical issue that should have been covered under warranty" spans multiple categories, and whether the system classifies it as "billing" or "technical" might both be defensible. You need explicit rules for handling edge cases, and those rules become part of your correctness definition.

For **generation tasks**, correctness becomes much harder to pin down because there is no single ground truth answer. When you generate a product description, a code function, or a medical summary, there are many possible outputs that would be correct. This is where teams make their biggest mistakes: they substitute proxy metrics for actual correctness evaluation. They measure BLEU scores or ROUGE scores, which quantify similarity to reference examples, and they treat high similarity as evidence of correctness. But a generated output can be very similar to a reference example and still be factually wrong or logically invalid. Conversely, an output can be dissimilar to all reference examples and still be completely correct.

The medical documentation system that lost the hospital contract was evaluating generation quality by comparing AI-generated summaries to physician-written summaries using semantic similarity metrics. A summary that said "Patient prescribed lisinopril 50mg daily" scored nearly identically to one that said "Patient prescribed lisinopril 15mg daily" because the sentence structure, medical terminology, and semantic embedding were almost identical. The single-digit difference that mattered—the dosage—was lost in aggregate similarity scoring. To measure correctness in generation tasks, you need to decompose the output into verifiable claims and check each claim independently.

## Partial Correctness and Catastrophic Errors

Most AI outputs are not binary correct or incorrect. They are partially correct, containing a mixture of accurate and inaccurate elements. How you handle partial correctness determines whether your evaluation system actually protects you from failures or just gives you a false sense of security.

A customer service AI generates a response to a question about return policy. The response correctly states the 30-day return window, correctly identifies that items must be unused, correctly notes that original packaging is required, but incorrectly states that shipping fees are refundable when company policy is that shipping fees are non-refundable. The output is 75% correct by claim count. Do you score this as 0.75 correctness? Do you score it as completely incorrect because any wrong claim makes the whole response wrong? Do you weight the claims by importance?

The answer depends on the consequences of the errors. If the incorrect claim about shipping fees leads customers to expect refunds they will not receive, you have created a customer satisfaction problem and a potential liability issue. The three correct claims do not offset the damage from the one incorrect claim. This is the problem of **catastrophic errors**—inaccuracies that, despite being a small fraction of the output, cause disproportionate harm.

Financial applications are particularly vulnerable to catastrophic errors in numerical correctness. An investment research AI analyzes a company's quarterly earnings and generates a summary. It correctly describes the revenue, operating expenses, strategic initiatives, and market context. But it miscalculates the earnings-per-share by confusing diluted and basic share counts. An analyst relying on this output might make incorrect investment decisions. The EPS figure is one small element in a multi-paragraph summary, but it is the element that directly drives investment decisions. You cannot treat all errors as equal.

Your correctness evaluation needs error severity tiers. Not every factual inaccuracy matters equally. A misspelled company name in a generated report is an error, but it does not compromise decision-making. An inverted percentage in a financial forecast is catastrophic. You need to classify claims in your outputs by their decision criticality and weight correctness scores accordingly. A system that is 98% correct on low-criticality claims and 85% correct on high-criticality claims is more dangerous than a system that is 90% correct uniformly across all claims, because users will learn not to trust the second system but might trust the first.

## The Illusion of Measurement

The medical documentation company thought they were measuring correctness because they had an evaluation pipeline. They sampled 200 AI-generated summaries per week, had clinicians rate them on a 5-point scale for accuracy, and tracked the average rating over time. The average rating was 4.3 out of 5. The team interpreted this as "86% accurate." What they were actually measuring was whether clinicians, in a brief review, perceived the summaries as generally accurate. They were not checking every medication name, every dosage, every date, every diagnostic code. They were skimming for plausibility.

Human raters asked to evaluate correctness tend to check whether the output seems right, not whether it is right. If the output is fluent, uses appropriate terminology, and does not contain obvious nonsense, raters will score it as correct even if it contains subtle factual errors. This is especially true for domains where the rater is not an expert. You cannot ask a non-lawyer to evaluate the correctness of legal analysis or a non-physician to evaluate the correctness of medical reasoning. Even domain experts conducting rapid evaluations will miss errors that would be caught by careful verification.

To measure correctness rigorously, you need claim-level verification against ground truth. You decompose the output into individual factual claims and logical inferences. For each claim, you identify the source of truth—either the input material, an external knowledge base, or the logical rules that govern the domain. You verify each claim against that source. This is expensive and slow, which is why teams avoid it. But there is no shortcut. Aggregate similarity metrics, human ratings of overall quality, and vibes-based evaluation do not measure correctness.

Some teams try to use AI to evaluate AI correctness, having a second model judge whether the first model's output is accurate. This can work for checking logical validity—you can prompt a model to identify contradictions or reasoning errors in generated text. But it does not work for checking factual accuracy unless the evaluating model has access to ground truth. If you ask GPT-5 to evaluate whether a Claude-generated medical summary is factually correct, GPT-5 can only check whether the summary is consistent with its training data, not whether it is consistent with the specific patient record that was the input. LLM-based evaluation of correctness is useful as a filter to catch obvious errors, but it cannot replace ground truth verification.

## Correctness Across Model Generations

The correctness profile of frontier models has improved dramatically from GPT-3 to GPT-4 to GPT-5.1 and Claude Opus 4.5 to Claude 4, but the nature of errors has changed rather than disappeared. Early models made frequent factual errors that were easy to spot—hallucinating non-existent citations, making arithmetically impossible claims, confusing basic facts. Current models make fewer errors but more subtle ones. They are more likely to misrepresent nuance, to conflate correlation with causation, to state a claim with unwarranted confidence. These errors are harder to detect because the output seems authoritative and well-reasoned.

This creates a dangerous dynamic where users trust model outputs more than they should. When GPT-3.5 told you that Abraham Lincoln sent an email, you knew something was wrong. When GPT-5.1 generates a legal analysis that subtly misinterprets a clause in a contract, you might not notice unless you read the contract carefully yourself. The systems have crossed into the zone where their errors are sophisticated enough to fool non-experts and sometimes even experts who are not reading carefully.

Model correctness also varies by domain in ways that are not intuitive. A model might be highly accurate on medical question answering but struggle with historical facts. It might excel at extracting structured data from invoices but fail at extracting the same types of data from handwritten forms. You cannot assume that correctness in one domain transfers to another. Your evaluation needs to cover the specific types of inputs and outputs your application handles, not generic benchmarks.

Furthermore, model behavior changes between versions in unpredictable ways. An upgrade from Claude Opus 4.5 to Claude 4 Opus might improve overall benchmark performance while introducing new failure modes on your specific task. Teams discover this when they deploy a new model version and see new types of errors appear. You need regression testing for correctness—a suite of test cases with verified ground truth answers that you run against every model version before deployment.

## Building Correctness Into Your Development Loop

Measuring correctness is necessary but not sufficient. You need to build correctness improvement into your development workflow. This means creating a feedback loop where errors drive changes to prompts, retrieval logic, guardrails, or model selection.

When you identify a correctness error in production or during evaluation, you trace it back to root cause. Was the model given the right information to answer correctly? If not, the problem is in your retrieval or data pipeline. Was the information present but the model ignored it or misinterpreted it? If so, the problem is in your prompt or instruction design. Did the model make a logical error even with correct information and clear instructions? If so, you might need a different model, a chain-of-thought approach, or external validation.

Each error becomes a test case. You add the input and the correct output to your evaluation set. You modify your system to fix the error. You run the full evaluation set to verify the fix did not break other cases. Over time, you accumulate a comprehensive test suite that covers the error modes you have encountered. This is regression testing for AI systems.

Some teams maintain a continuously growing evaluation set where every production error, every QA finding, and every user complaint that traces to correctness becomes a new test case. The evaluation set becomes a living artifact that represents the actual distribution of challenging inputs your system encounters. This is far more valuable than a static benchmark created before deployment.

## When Correctness Is Not Enough

Correctness is necessary but not sufficient for a functional AI system. You can build a system that achieves 99% factual accuracy and logical validity and still fail if the outputs are not grounded in verifiable sources, if they omit critical information, or if they do not help users complete their tasks. Correctness is the foundation, but you need additional quality dimensions stacked on top of it.

The healthcare documentation system that lost the contract had a correctness problem, but even if they had solved it, they would still need to ensure their outputs were grounded in the medical record, complete enough to satisfy regulatory requirements, and useful enough that physicians would actually rely on them. Correctness alone does not guarantee a successful product.

Understanding how correctness interacts with other quality dimensions is essential. An output can be correct but ungrounded—it states accurate facts but does not cite sources, leaving the user unable to verify the claims. An output can be correct but incomplete—it accurately answers part of the question but omits essential context. An output can be correct but useless—it provides factually accurate information that does not help the user accomplish their goal. Each of these is a distinct failure mode requiring distinct solutions.

The next dimension you must master is groundedness, which addresses the question of whether every claim in your output can be traced back to a specific source. This becomes particularly critical in retrieval-augmented generation systems, where the promise is that outputs will be anchored in retrieved documents rather than model knowledge alone.
