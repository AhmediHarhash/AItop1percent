# 5.12 â€” Unit Economics Instrumentation

In September 2025, a Series C document analysis startup celebrated crossing $10 million in annual recurring revenue and announced plans to scale to $50 million within eighteen months. The company had strong growth metrics: forty percent month-over-month user growth, ninety percent gross retention, and an average contract value of $24,000 per year. The board approved a $15 million Series D funding round to fuel expansion. Three months into the growth phase, the CFO asked engineering to analyze infrastructure costs and discovered that the company was losing $1.40 on every dollar of revenue. Their largest customers were processing document volumes that cost $42,000 per year to serve at a contract price of $24,000. The AI models they used for extraction and classification cost $0.18 per document page while their average revenue per page was $0.09. They had been growing revenue while growing losses even faster, and their path to $50 million revenue would require $90 million in cumulative losses before achieving the scale economies they hoped would eventually make the unit economics work.

The failure was launching and scaling a business without instrumenting the fundamental unit economics: cost per document, revenue per document, margin per customer, and which segments or use cases were profitable versus subsidized. The company knew their aggregate revenue and aggregate infrastructure costs, but they had no visibility into how those costs varied by customer size, document type, or processing complexity. They discovered too late that their ten largest customers generated forty percent of revenue but consumed seventy-five percent of infrastructure costs, while their long tail of small customers were profitable but too small to compensate for the losses on enterprise accounts. If they had instrumented unit economics from the beginning, they would have discovered this problem when they had 100 customers instead of when they had 2,000, and they could have adjusted pricing, optimized serving costs, or changed their target customer profile before burning through their runway.

## Cost-to-Serve Instrumentation

**Cost-to-serve** is the total expense of processing a single user request, including model inference costs, infrastructure overhead, data storage, and amortized engineering costs. For AI systems, inference cost typically dominates: a GPT-4 API call might cost $0.03 to $0.15 depending on input and output length, while the infrastructure to serve the request costs $0.001 and the amortized engineering cost is $0.002 per request at scale. Accurate cost-to-serve calculation requires instrumenting your system to log every cost-incurring event and attribute it to the specific user request that caused it.

The first component of cost-to-serve is **model inference cost**, which varies by model size, input token count, output token count, and API pricing or self-hosting costs. For API-based models, you should log the exact token counts and multiply by the per-token price to get the exact cost of each call. For self-hosted models, you should measure GPU utilization and amortize the infrastructure cost across all requests served during each time period. A request that requires 500 input tokens and 200 output tokens on GPT-5 at January 2026 pricing of $2.50 per million input tokens and $10.00 per million output tokens costs $0.00125 for input and $0.00200 for output, totaling $0.00325 in inference cost.

The second component is **data storage and retrieval cost** for systems that use retrieval-augmented generation or store conversation history. A RAG system that queries a vector database and retrieves fifteen document chunks might incur $0.0008 in database costs per query based on the pricing of the vector database provider and the size of the embeddings. A chatbot that maintains conversation context across ten turns might store 5,000 tokens of history that cost $0.001 per month in storage costs amortized across all conversations. These costs are smaller than inference costs but can add up at scale, and they vary significantly by use case: a user who asks complex questions requiring large context retrievals costs more than a user who asks simple questions.

The third component is **infrastructure overhead** for serving traffic, load balancing, logging, monitoring, and running the supporting services that enable your AI system. These costs are typically amortized across all requests based on the fraction of infrastructure capacity each request consumes. If your infrastructure costs $20,000 per month to run and you serve 10 million requests per month, the average infrastructure overhead is $0.002 per request. For requests that are more expensive to process because they require more CPU time, database queries, or API calls to external services, you might attribute a higher overhead cost by measuring actual resource consumption and charging proportionally.

The fourth component is **amortized engineering cost** for the team that builds and maintains the system. If your engineering team costs $2 million per year and you serve 100 million requests per year, the amortized engineering cost is $0.02 per request. This cost is often omitted from unit economics analysis because it is a fixed cost rather than a variable cost, but it is real and must be covered by revenue. Early-stage companies with low request volumes have high amortized engineering costs per request, making the full cost-to-serve much higher than just the inference cost. As you scale, amortized engineering cost per request declines, improving unit economics even if inference cost per request stays constant.

## Revenue Attribution and Margin Analysis

**Revenue per interaction** is the amount of money you earn from a single user request, either directly through usage-based pricing or indirectly through subscription pricing divided by usage volume. Accurate margin analysis requires attributing revenue to individual interactions so you can compare it against cost-to-serve and determine which interactions are profitable. For usage-based pricing like $0.10 per API call, revenue attribution is straightforward: each call generates $0.10 in revenue. For subscription pricing like $50 per month, revenue attribution requires estimating how many interactions each user consumes and dividing subscription revenue by interaction count.

A company charging $100 per month for unlimited use of a document analysis tool needs to instrument how many documents each subscriber processes to calculate revenue per document. If the average subscriber processes 500 documents per month, the revenue per document is $0.20. If cost-to-serve is $0.15 per document, the margin is $0.05 per document or twenty-five percent. This margin analysis tells you whether your business model is viable at current pricing and costs. If cost-to-serve were $0.25 per document, you would be losing $0.05 per document and would need to either raise prices, reduce costs, or change the business model to survive.

Margin analysis becomes more complex when revenue and costs vary significantly across users or use cases. A subscriber who processes 5,000 documents per month at a $100 monthly subscription generates $0.02 in revenue per document, while a subscriber who processes fifty documents generates $2.00 per document. If cost-to-serve is $0.15 per document, the high-volume user is unprofitable with a negative $0.13 margin while the low-volume user is highly profitable with a $1.85 margin. Without instrumentation that reveals this distribution, you might think your average margin of $0.05 per document is acceptable when in fact your business is a cross-subsidy where low-volume users are subsidizing high-volume users.

You should calculate margin separately for cohorts defined by **customer tier**, **use case type**, **document complexity**, or other dimensions that drive cost or revenue variation. A cohort analysis might show that enterprise customers have negative thirty percent margins because they negotiate volume discounts while consuming high inference costs on complex documents, while small business customers have seventy-five percent margins because they pay full price and process simple documents. This analysis reveals that growth in enterprise revenue actually makes your economics worse, while growth in small business revenue makes your economics better. Without cohort-level margin visibility, you would optimize for the wrong customer segment and accelerate your path to insolvency.

## Worst-Slice Cost Analysis

**Worst-slice cost analysis** identifies the specific users, use cases, or tasks that have the highest cost-to-serve or the worst margins, so you can decide whether to optimize them, restrict them, or price them differently. In most AI systems, cost-to-serve follows a power law distribution where the top ten percent of expensive requests consume fifty to eighty percent of total costs. Understanding what makes those requests expensive lets you improve unit economics dramatically by addressing a small number of high-impact cases.

A document analysis system might discover that five percent of documents are scanned PDFs that require OCR preprocessing before analysis, and those documents cost $2.40 to process compared to $0.15 for native digital documents. The scanned PDFs represent twenty percent of documents processed but sixty percent of total costs. The worst-slice cost analysis reveals that you have three options: invest in optimizing OCR to bring the cost down to $0.50 per scanned document, add a surcharge of $1.00 for scanned documents to make them profitable at current costs, or restrict scanned document processing to premium tiers and refuse to process them on cheaper plans. Each option has different implications for user experience and revenue, but you cannot make an informed choice without first identifying that scanned documents are the cost driver.

You should instrument your system to automatically tag each request with attributes that might drive cost variation: input length, output length, task complexity, user tier, time of day, model version used. These tags let you slice your cost data along different dimensions and identify worst-slice cases. A query that generates a 4,000-token output costs fifteen times more than a query that generates a 250-token output at typical API pricing, so output length is a critical tag. A query that requires three model calls because the first two failed validation costs three times more than a query that succeeds on the first call, so retry count is a critical tag. A query from a free-tier user generates zero revenue while a query from an enterprise user might generate $0.50 in attributed revenue, so user tier is a critical tag.

The worst-slice analysis should also identify **unprofitable users** who consistently generate high costs relative to the revenue they provide. A user who is on a $20 per month plan but generates $80 per month in inference costs is destroying $60 per month in value and will never be profitable unless they reduce usage or upgrade to a higher plan. You should monitor these users, reach out to understand their use case, and either migrate them to appropriate pricing or offboard them if they are unwilling to pay enough to cover their costs. It feels counterintuitive to turn away active users, but retaining users who destroy value is worse than losing them because every additional dollar they spend costs you more than a dollar to serve.

## Instrumentation Architecture

Unit economics instrumentation requires logging every cost-incurring event with sufficient context to attribute it to a user, request, or session and to aggregate costs across dimensions of interest. The logging should happen as close as possible to where costs are incurred: model inference libraries should log token counts and costs, database clients should log query costs, and infrastructure monitoring should log compute costs. These logs should flow to a data warehouse where they can be joined with user metadata, revenue data, and request attributes to enable margin analysis.

A typical instrumentation architecture has three layers: **event collection** where services emit structured logs for each cost event, **attribution logic** where events are joined with user and request context to enable slicing, and **aggregation and reporting** where costs are summed by cohort and compared against revenue to calculate margins. The event collection layer might emit logs like "user_id: 12345, request_id: abc-789, model: gpt-4o, input_tokens: 450, output_tokens: 280, cost: 0.00393, timestamp: 2026-01-15T14:32:11Z." The attribution layer joins this log with user metadata showing that user 12345 is on the $50/month professional plan, and request metadata showing that request abc-789 was a document summarization task. The aggregation layer sums costs for all document summarization requests by professional plan users and divides by the number of such requests to get average cost, then divides subscription revenue by request volume to get average revenue, and subtracts cost from revenue to get margin.

The data warehouse schema should include tables for **cost events**, **user metadata**, **request metadata**, **revenue events**, and **margin calculations**. Cost events table stores one row per inference call, database query, or infrastructure cost allocation with fields for timestamp, user ID, request ID, cost amount, and cost type. User metadata table stores one row per user with fields for user ID, plan tier, monthly subscription price, signup date, and any other attributes relevant for cohort analysis. Revenue events table stores one row per payment with fields for user ID, timestamp, amount, and payment method. Margin calculations table stores aggregated results showing cost, revenue, and margin by cohort and time period. This schema enables both real-time monitoring of current margins and historical analysis of how margins have evolved as your product, pricing, or cost structure changed.

## Cost Dashboards and Alerting

Unit economics data should be visible through dashboards that show current margins, trending margins, cohort-specific margins, and worst-slice costs. The dashboard should answer questions like: What is our current overall margin? Which customer tier has the best margin? What percentage of requests are unprofitable? Which task types have the highest cost-to-serve? How have margins trended over the past three months? These questions help product and finance teams understand whether the business model is working and where to focus optimization efforts.

The dashboard should highlight **margin trends** because margins that are declining over time indicate that costs are growing faster than revenue, a sign that unit economics are deteriorating as you scale. A company that started with sixty percent margins and is now at forty percent margins is on a trajectory toward zero and must either increase prices, reduce costs, or change the business model before margins turn negative. The trend is more important than the absolute value: a company with thirty percent margins that are stable or improving is in better shape than a company with fifty percent margins that are declining five points per quarter.

You should configure **cost alerts** that fire when unit economics breach thresholds that indicate problems. An alert might fire when overall margin drops below thirty percent, when margin for any customer tier goes negative, when the proportion of unprofitable requests exceeds twenty percent, or when any individual user's cost-to-serve exceeds five times their revenue contribution. These alerts ensure that unit economics problems are detected and addressed quickly rather than accumulating silently until they threaten the business. The alert should include diagnostic information showing which cohorts or users are driving the problem so the team can investigate and respond without first having to dig through logs.

Cost dashboards should also show **projected margins at scale** by modeling how costs and revenue will change as you grow. If your current margin is forty percent at $1 million in ARR but your largest cost components have economies of scale, your projected margin at $10 million in ARR might be sixty percent because amortized engineering costs and infrastructure overhead decline per request. Conversely, if your current margin is forty percent but your inference costs grow linearly with volume while your revenue per user is capped by pricing, your projected margin at $10 million ARR might be twenty percent because inference costs grow faster than revenue. These projections help leadership understand whether current margins are sufficient given the trajectory toward scale economies or diseconomies.

## Optimization Priorities from Unit Economics

Unit economics analysis should directly drive your optimization roadmap by revealing which cost reductions or revenue increases would have the largest impact on margin. A structured approach to optimization prioritization calculates the margin impact of each potential optimization, estimates the engineering effort required, and ranks optimizations by impact per unit of effort. The highest-priority optimizations are those that materially improve margins with reasonable engineering investment.

A company with thirty percent overall margin might analyze five potential optimizations: switching from GPT-4 to GPT-5 to reduce inference cost by thirty percent, implementing response caching to reduce duplicate inference costs by fifteen percent, adding a premium tier priced at double the current rate, restricting the highest-cost features to premium tiers only, and optimizing the most expensive five percent of requests to reduce their cost by fifty percent. Each optimization has different margin impact and engineering cost. Switching models might improve margin from thirty to forty-five percent with two weeks of testing and validation work, a fifteen-point margin improvement for ten engineering days or 1.5 points per day. Implementing caching might improve margin from thirty to thirty-five percent with three weeks of engineering work, a five-point improvement for fifteen days or 0.33 points per day. Restricting features might improve margin from thirty to forty percent with minimal engineering work but risk alienating users, a ten-point improvement with unknown user impact that requires careful experimentation before committing.

The optimization with the highest margin impact per engineering day is switching models at 1.5 points per day, making it the clear top priority. The optimization with the highest total margin impact is restricting features at ten points, but it has user experience risk that makes it higher-uncertainty. The optimization with the lowest return is caching at 0.33 points per day, making it a lower priority unless caching also provides latency benefits that improve user experience independently of margin impact. This prioritization framework ensures that optimization efforts are allocated to the highest-return opportunities rather than being driven by whatever the team finds interesting or whatever someone read in a blog post.

## Unit Economics and Pricing Strategy

Unit economics analysis should inform your pricing strategy by revealing what prices are necessary to achieve target margins given your cost structure. If your cost-to-serve is $0.50 per request and you want fifty percent margins, your revenue per request must be $1.00, which translates to pricing of $1.00 per request for usage-based pricing or a subscription price divided by expected usage that yields $1.00 per request. If market research shows that customers will only pay $0.80 per request, you have three options: accept thirty-seven percent margins, reduce cost-to-serve to $0.40 to achieve fifty percent margins at $0.80 pricing, or change the product to target users who value the output enough to pay $1.00 or more.

You should experiment with **usage-based pricing** for products where cost-to-serve varies significantly by user, because usage-based pricing aligns revenue with costs better than subscription pricing. A document analysis product where cost per document ranges from $0.10 to $2.00 depending on complexity should charge per document rather than offering unlimited processing for a fixed monthly fee, because the unlimited model will attract high-cost users who destroy margins while deterring low-cost users who would be profitable. Usage-based pricing captures more value from high-volume users while remaining accessible to low-volume users, and it ensures that users who generate high costs pay proportionally for the resources they consume.

For subscription pricing, you should use unit economics to set **usage caps** or **feature restrictions** by tier that ensure users at each tier are profitable. If cost-to-serve averages $0.20 per request and your $50/month tier targets fifty percent margins, users at that tier should consume no more than 500 requests per month on average. You should enforce soft limits at 500 requests with prompts to upgrade, and hard limits at 750 requests where the service becomes unavailable until the next billing period. These limits prevent users from consuming resources that cost more than their subscription price and ensure that unit economics remain positive across all users.

Premium tiers should be priced to capture value from users whose willingness to pay exceeds the base tier while offering differentiated features that justify the higher price. If unit economics analysis shows that ten percent of users consume fifty percent of costs by using advanced features like large document processing or real-time analysis, those features should be restricted to a premium tier priced three to five times the base tier. The premium tier price should reflect the incremental cost-to-serve plus a margin target, ensuring that premium users are profitable even though they consume more resources. A user who processes large documents at $1.20 cost per document should be on a tier priced to generate $2.40 revenue per document, or $120/month for 100 documents instead of $50/month for 100 small documents.

## Margin Improvement Experiments

Unit economics instrumentation enables you to run controlled experiments that measure the margin impact of product, pricing, or cost changes. An A/B test that shows a feature change increases engagement by fifteen percent is interesting, but an A/B test that shows the feature change increases engagement by fifteen percent and reduces cost-to-serve by twenty percent due to shorter interactions is actionable because you know the margin impact. Similarly, an A/B test showing that a price increase from $50 to $70 per month reduces conversion by ten percent is informative, but adding cost data shows that the remaining ninety percent of users have forty-five percent higher margins, making the price increase profitable despite lower conversion.

You should run pricing experiments that test different price points, feature bundling strategies, or usage caps to find the combination that maximizes margin without excessively harming conversion or retention. A pricing experiment might test $40, $50, and $60 per month for the professional tier while measuring conversion rate, usage patterns, cost-to-serve, revenue per user, and margin per user. The experiment might reveal that $60 pricing reduces conversion by twenty percent but the remaining eighty percent of users have margins that are fifty percent higher, resulting in twenty percent higher total margin dollars despite lower user count. This result suggests that $60 pricing is superior to $50 pricing from a margin perspective, even though it serves fewer users.

You should also run cost optimization experiments that test different model versions, prompting strategies, or caching configurations to find the combination that minimizes cost while maintaining acceptable quality. A model experiment might test GPT-4, GPT-5, and Claude Opus 4.5 on a representative sample of production traffic while measuring quality metrics, cost-to-serve, and latency. The experiment might show that Claude Opus 4.5 achieves equivalent quality to GPT-4 at forty percent lower cost and twenty percent lower latency, suggesting that switching models would improve margins by fifteen percentage points with no quality degradation. This result makes the model switch a high-confidence decision that can be rolled out to all users.

With business metrics, ROI frameworks, release gates, SLOs, and unit economics covered, Chapter 5 has provided the complete picture of how to measure, monitor, and optimize the business value your AI system delivers. Chapter 6 addresses the specialized metrics required by safety, compliance, and regulated industries where quality failures carry regulatory, legal, or existential risk beyond the business metrics covered here.
