# 3-11 â€” Leakage and Contamination Checks

In August 2025, a legal AI company preparing for their Series B raised prepared a comprehensive benchmark presentation showing their contract analysis model outperformed GPT-4o and Claude Opus 4.5 by twelve percentage points on their proprietary evaluation set. The benchmark included three hundred contract clauses with expert annotations for risk assessment. Their model achieved ninety-one percent accuracy. Competitors managed seventy-eight to seventy-nine percent. The presentation convinced two major law firms to begin pilot programs. In October, an independent evaluation by one of the law firms using their own contract set showed the company's model at seventy-three percent accuracy, worse than Claude Opus 4.5 at seventy-seven percent. The pilot programs ended. The Series B fell apart.

The investigation revealed what had happened. The company's training data pipeline had accidentally included their evaluation set because both used contracts from the same public dataset and the deduplication logic had failed. The model had seen sixty percent of the evaluation examples during training. It was not generalizing; it was memorizing. The benchmarks showed model capacity to recall training data, not model capacity to analyze novel contracts. The twelve-point advantage was an artifact of **data leakage**, and it cost the company their funding round and market credibility.

## The Contamination Problem

**Data contamination** occurs when information from your evaluation set appears in your training data, making your metrics unreliable because the model is being tested on material it has already seen. The contamination can be exact duplicates, near-duplicates with minor variations, paraphrases that preserve semantic content, or shared source material that provides context clues. Any overlap means your evaluation is measuring memorization plus generalization instead of pure generalization, and you cannot tell which fraction is which.

Contamination has always existed in machine learning but it becomes acute with large language models for several reasons. Training datasets contain billions of documents scraped from diverse sources, making manual inspection impossible. Evaluation benchmarks are published online and then get scraped into future training sets. Models have massive capacity to memorize training examples, especially with repeated exposure. Pretraining data is often proprietary or undocumented, so you cannot verify what went into the base models you are fine-tuning. The scale and opacity create conditions where contamination is likely and detection is hard.

The consequences of contamination are severe. You believe your model is better than it actually is, leading to premature deployment. You optimize toward improvements that come from leakage rather than real capability gains. You make architectural or training decisions based on false signals. You publish benchmarks that other researchers cannot reproduce because they do not have the same contamination. You waste resources pursuing dead ends because your metrics are lying to you. The legal AI company lost millions in valuation and years of partnership development because they trusted contaminated metrics.

Contamination is not always accidental. In late 2024, a research lab was found to have deliberately included benchmark questions in their training data to boost published performance numbers. The practice, discovered through forensic analysis by independent researchers, destroyed the lab's reputation and led to retractions of multiple papers. Most contamination is accidental, but the incentive structure around benchmark performance creates pressure to look the other way when you suspect contamination might be helping your numbers. Rigorous contamination checking is an ethical discipline as much as a technical one.

## Forms of Leakage: Exact Duplicates

**Exact duplicate leakage** is the simplest form. An evaluation example appears verbatim in the training data. This can happen through careless data splits where evaluation examples are not properly held out, through multiple datasets that share underlying sources, through benchmark publication followed by dataset scraping, or through version control mistakes where updated evaluation sets accidentally get merged into training sets. Exact duplicates are the easiest to detect but also the most damaging because the model can memorize the exact answer.

Detection requires comparing every evaluation example against every training example, which is computationally expensive at scale. For text data, you can use hash-based approaches where you compute cryptographic hashes of evaluation examples and check if those hashes appear in hashes of training examples. This finds exact matches efficiently. You can also use substring matching where you check if any substring of a certain length from an evaluation example appears in training data. A common threshold is checking for any shared substring of at least twenty-five tokens, which catches exact duplicates plus minor variations.

The challenge is dealing with structured data and multimodal inputs. For code, you need to handle whitespace normalization, variable renaming, and comment differences. For images, you need perceptual hashing rather than byte-level hashing. For structured records, you need to handle field reordering and formatting variations. A legal contract might appear in both training and evaluation data with different formatting, different paragraph numbers, and different header styles but identical content. Your deduplication logic must account for these variations.

When you find exact duplicates, the only safe response is removing them from evaluation data. You cannot remove them from training data retroactively if the model is already trained. Some teams try to keep the examples but mark them as contaminated and report metrics separately, but this creates confusion about which numbers to trust. The cleaner approach is maintaining contamination-free evaluation sets even if that means discarding examples you worked hard to collect. A smaller clean evaluation set provides more reliable signal than a larger contaminated one.

## Forms of Leakage: Semantic Overlap

**Semantic overlap leakage** occurs when training and evaluation examples are not identical but are similar enough that the model can leverage training knowledge to answer evaluation questions without genuine generalization. The evaluation question asks about the capital of France and the training data contained a document listing European capitals. The evaluation example involves analyzing a rental agreement and the training data contained rental agreement templates with similar clauses. The examples are not duplicates but they share semantic content that advantages the model.

Semantic overlap is harder to detect than exact duplicates because it requires understanding meaning rather than just matching strings. One approach is using embedding similarity. You embed all evaluation examples and all training examples using a sentence or document encoder, then search for high-cosine-similarity pairs. Pairs with similarity above a threshold, often 0.85 to 0.95 depending on domain, are flagged as potential leakage. This catches paraphrases and semantically equivalent content even when the text differs.

The difficulty is setting appropriate thresholds. Too low and you flag examples that are merely in the same domain but do not provide real advantages. Too high and you miss subtle overlap that still constitutes leakage. The right threshold depends on model capacity, domain specificity, and task type. For factual question answering, high similarity between questions is significant because specific facts matter. For reasoning tasks, moderate similarity might not matter if the reasoning path differs. For generation tasks, similarity in style or structure might matter more than similarity in content.

A fintech company in 2025 discovered semantic leakage when their financial summarization model performed suspiciously well on earnings reports from a specific industry vertical. Investigation revealed their training data included investor presentations that summarized the same earnings information. The model was not learning to summarize from raw financial data; it was learning to retrieve memorized summaries from similar contexts. Their ninety percent accuracy on that vertical dropped to sixty-seven percent when they used earnings reports from companies not mentioned anywhere in training data. The semantic overlap had inflated their metrics by twenty-three percentage points.

## Forms of Leakage: Prompt Leakage

**Prompt leakage** happens when your evaluation prompts are too similar to prompts in training data, letting the model pattern-match prompt structure rather than understanding task requirements. This is particularly relevant when fine-tuning models on instruction datasets or when using models pretrained on instruction-following data. If your evaluation prompt is "Summarize the following document" and the training data contained thousands of examples with that exact prompt structure followed by summaries, the model can succeed by recognizing the pattern rather than by understanding summarization.

The problem intensifies when using public models trained on internet data that includes published benchmarks. If you evaluate GPT-4o or Claude Opus 4.5 on MMLU and those models were trained on data that included MMLU questions and answers published in papers or blog posts, the evaluation is contaminated. Model providers are aware of this and typically try to deduplicate benchmarks from training data, but the deduplication is imperfect. When Anthropic and OpenAI report benchmark results, they note which benchmarks might have contamination risk. You should assume some level of prompt leakage when using public benchmarks with public models.

For custom evaluations, you can reduce prompt leakage by diversifying prompt templates. Instead of using the same prompt structure for all evaluation examples, randomize phrasing, instruction order, and formatting. Use different ways to ask for the same task. Include examples where the prompt structure is deliberately different from common instruction patterns. This does not eliminate leakage but it reduces the model's ability to succeed through pure pattern matching. You want the model to understand the task from task semantics, not from recognizing a memorized prompt template.

Detecting prompt leakage is difficult because you often do not know what prompts appeared in training data. One signal is suspiciously high performance on templated evaluations coupled with sharp performance drops when prompt templates are varied. If your model gets ninety percent accuracy with prompt template A and seventy percent accuracy with semantically equivalent template B, that suggests template A appeared in training data. Another signal is comparing performance on published benchmarks versus structurally similar but unpublished evaluations. Higher performance on published benchmarks suggests possible contamination from benchmark publication.

## Forms of Leakage: Judge Contamination

**Judge contamination** is particularly insidious and rarely checked. When you use an LLM as a judge to evaluate another model's outputs, you face the risk that the judge model was trained on data related to the evaluation task or on previous evaluation outputs. If you use GPT-4o to judge the quality of legal summaries and GPT-4o's training data included legal summary evaluations or rubrics, the judge might be pattern-matching against memorized evaluation patterns rather than genuinely assessing quality.

This becomes especially problematic when using model outputs as training data in subsequent iterations. A common pattern is using a strong model to generate preference pairs or quality scores for fine-tuning a weaker model. If you later use a model from the same family to judge the fine-tuned model, the judge and the evaluated model share lineage and training data. The judge might favor outputs that resemble its own training distribution rather than outputs that are objectively better. This creates **incestuous evaluation** where models trained on synthetic data are evaluated by judges trained on similar synthetic data.

In early 2026, a team building a specialized Claude-based application noticed that when they used Claude Opus 4.5 as a judge, their Claude-fine-tuned model consistently outperformed a GPT-4.5-based competitor. When they used GPT-4.5 as a judge, the competitor outperformed their model. When they used human evaluation, the two models were approximately equal. The judges were biased toward models from their own family, likely because training data overlap made the judge models favor outputs that matched patterns they had seen during training. The team resolved this by using an ensemble of judges from different model families and weighting by agreement with human preferences.

Mitigating judge contamination requires using judges trained on different data distributions than your evaluated models, validating judge reliability against human evaluations regularly, using multiple judges from different model families, and being cautious about using model outputs as training data for future models that will be judged by related models. The fundamental problem is that LLM judges are not objective scorers; they are pattern matchers that favor patterns similar to their training data. If those patterns overlap with your evaluated model's patterns for reasons other than quality, the evaluation is contaminated.

## Contamination Detection Methodology

Building a contamination detection pipeline requires multiple complementary approaches because no single method catches all forms of leakage. The pipeline should run automatically before every evaluation and should be maintained as a first-class engineering system, not an afterthought. The investment pays for itself the first time it catches contamination before you make decisions based on contaminated metrics.

Start with **exact match detection** using hash-based comparison. Hash every evaluation example and check hashes against training data hashes. Use multiple hash functions to reduce collision risk. For text, consider both full example hashes and n-gram hashes to catch partial overlap. For code, normalize before hashing to handle formatting differences. For structured data, serialize to a canonical format before hashing. Build an index of training data hashes for efficient lookup. Flag any exact matches and investigate whether they are genuine leakage or expected overlap in common vocabulary.

Add **substring overlap detection** for near-duplicates. Use suffix arrays or other efficient substring matching algorithms to find shared substrings above a length threshold. Twenty-five tokens is a reasonable starting point for text. Longer thresholds reduce false positives but miss smaller overlaps. Shorter thresholds catch more overlap but generate more false positives requiring manual review. Tune the threshold based on your domain and model capacity. Large models can leverage shorter shared substrings than small models.

Implement **embedding-based similarity search** for semantic overlap. Embed evaluation examples and training examples using a high-quality encoder. Use approximate nearest neighbor search to find training examples with high cosine similarity to each evaluation example. FAISS or similar libraries make this tractable at scale. Flag pairs with similarity above 0.90 for automatic removal and pairs between 0.85 and 0.90 for manual review. The manual review is necessary because high similarity sometimes reflects shared domain vocabulary rather than contaminating overlap.

Include **date-based filtering** when possible. If your evaluation examples were created or published after your training data cutoff, you can be confident they were not in training data. This requires trusted timestamps and applies only to data you control. It does not help when using models trained by others or when your training data includes recent scrapes that might include recently published benchmarks. But for data you generate internally after training, date filtering provides strong contamination guarantees with minimal computational cost.

## Building Leak-Proof Evaluation Pipelines

A **leak-proof evaluation pipeline** treats contamination prevention as a system property rather than a post-hoc check. The pipeline ensures that evaluation data and training data never mix through architectural separation, process discipline, and automated validation. This requires upfront investment but eliminates entire classes of leakage bugs that are difficult to debug after the fact.

Start with **physical separation** of evaluation and training data. Store them in different directories, different databases, or different storage systems. Use access controls so that training pipelines cannot read evaluation directories and vice versa. Tag evaluation data clearly at creation time and maintain that tag through all processing steps. Make it difficult to accidentally include evaluation data in training through system design rather than relying on human vigilance. One company uses completely separate cloud projects for training and evaluation data with no cross-project service account access, making accidental mixing architecturally impossible.

Implement **version control and provenance tracking** for all evaluation data. Every evaluation example should have metadata recording when it was created, who created it, what source it came from, and what processing was applied. When you run an evaluation, log the exact versions of all evaluation examples used. This lets you retroactively check if specific examples were contaminated and exclude them from historical metrics without rerunning evaluations. It also lets you audit when contamination was introduced if you discover it later.

Use **adversarial generation** for evaluation data when possible. Instead of collecting real-world examples that might exist in scraped training data, generate synthetic evaluation examples specifically designed to not appear in any training set. For reasoning tasks, create novel problems with unique details. For knowledge tasks, combine facts in ways that would not appear naturally in documents. For generation tasks, specify unusual constraints or combinations. Adversarial generation is more expensive than collecting existing data but provides stronger contamination guarantees.

Establish **training data freezes** before evaluation. Once you decide to evaluate a model, freeze the training data manifest and document exactly what data was included. Do not allow training data additions or modifications after the freeze until evaluation is complete. This prevents subtle contamination where evaluation data gets added to training data between when you decided on evaluation examples and when you actually ran the evaluation. The freeze should be enforced programmatically, not just by policy.

## Contamination Reporting and Transparency

When you publish metrics internally or externally, you should report contamination checking procedures and results. This builds trust and lets consumers of metrics assess reliability. A benchmark score without contamination reporting is increasingly considered incomplete in 2026, similar to how reporting test accuracy without a test set description was considered incomplete in earlier eras of machine learning. The standard is rising.

The contamination report should include what detection methods were used, what thresholds were applied, how many evaluation examples were flagged for each method, how flagged examples were handled, and what residual contamination risk remains. Be specific. "We checked for contamination" is not useful. "We used exact hash matching and found zero duplicates, embedding similarity at 0.90 threshold and found and removed twelve examples, and manual review of examples flagged at 0.85 to 0.90 threshold and found no additional leakage" is useful.

Report **contamination rate** as a metric in its own right. If you checked one thousand evaluation examples and found thirty contaminated ones that you removed, your contamination rate was three percent. High contamination rates suggest problems with data collection or splits. Contamination rates that increase over time suggest that your evaluation set is leaking into new training data, which happens when benchmarks are published and then scraped. Tracking contamination rate over time helps you understand whether your evaluation set is staying clean.

When contamination is discovered after metrics are published, issue corrections. This is uncomfortable but necessary. The legal AI company that lost their funding round had they discovered the contamination themselves and issued corrected metrics, they might have lost credibility temporarily but retained some trust. By letting their customer discover it, they lost all credibility. Organizations that proactively find and report contamination build reputations for rigor. Organizations that hide contamination get discovered eventually and suffer larger reputational damage.

## Temporal Contamination in Production

**Temporal contamination** occurs in production systems when your model is evaluated on data from similar time periods or contexts as training data, making the evaluation unrepresentative of actual performance on future unseen data. This is subtler than train-test leakage but equally important. A financial forecasting model trained on 2023-2024 data and evaluated on mid-2024 data might look great but fail on late 2025 data because market conditions shifted. The evaluation was technically clean but temporally contaminated.

Preventing temporal contamination requires **forward-looking evaluation** where you always evaluate on data from time periods after your training data. If your training data includes all documents through March 2026, your evaluation data should be from April 2026 or later. This ensures you are measuring the model's ability to generalize to truly unseen future contexts, not just unseen examples from the same time period and context as training data. Many production failures come from models that passed temporally contaminated evaluations but could not handle distribution shift in actual deployment.

For production monitoring, establish **rolling evaluation sets** that are refreshed regularly with new data that definitely postdates all training data. A customer service model might maintain monthly evaluation sets with conversations from the most recent month, which by definition cannot have been in training data if the model was trained earlier. These rolling sets measure whether model quality degrades over time as language patterns and user behavior evolve. Degradation signals the need for retraining with more recent data.

Temporal contamination is particularly acute in domains with rapid change. A news summarization model trained on 2024 news might perform poorly on 2026 news because new topics, entities, and events dominate. A code generation model trained before a major framework release might perform poorly on code using new framework features. A medical AI trained before new treatment guidelines were published might give outdated advice. Static evaluation sets do not catch these temporal degradations. You need evaluation strategies that account for distribution shift over time.

## Contamination in Third-Party Models

When you use third-party models like GPT-4.5, Claude Opus 4.5, or Gemini 2, you face contamination risks you cannot fully control because you do not know what was in training data. Model providers deduplicate known benchmarks but cannot deduplicate your custom evaluation sets that they have never seen. If your evaluation set comes from public sources that might have been scraped into training data, contamination is possible. This is not a criticism of providers; it is an inherent challenge of working with models trained on internet-scale data.

The mitigation is using **private evaluation sets** constructed from proprietary data that definitely was not publicly available during the model's training period. A company evaluating models for customer support can use their own historical support tickets from internal systems. A company evaluating medical models can use clinical notes from their health system that are not publicly accessible. Private data provides strong contamination guarantees but requires that you have relevant private data available.

For public benchmarks, be skeptical of published results. The 2025-2026 period has seen increasing awareness that published benchmark performance may reflect contamination more than capability. Independent researchers have found evidence of contamination across multiple major benchmarks. When model providers report benchmark scores, read the contamination discussion carefully. Anthropic and OpenAI now routinely report which benchmarks they suspect may have partial contamination. Use those reports to calibrate how much weight to put on different benchmarks.

Consider **benchmark freshness** when selecting evaluations. Benchmarks published before a model's training cutoff are high contamination risk. Benchmarks published after the cutoff are lower risk but not zero risk if the benchmark source material existed before publication. Benchmarks created specifically after model release by independent parties have the lowest contamination risk. The GSM8K benchmark has been extensively contaminated through model training; newer reasoning benchmarks published in 2026 are cleaner. Refresh your benchmark choices regularly to stay ahead of contamination.

## The Contamination Arms Race

As awareness of contamination grows, we are entering an **arms race** between contamination and detection. Model trainers have incentives to maximize benchmark performance, which can be achieved through capability improvement or through contamination. Detection methods improve, making contamination harder to hide. Training methods adapt to evade detection, such as paraphrasing benchmark questions during training. Detection methods adapt again. This dynamic creates ongoing pressure to maintain rigorous contamination checking.

One emerging technique is **adversarial contamination testing** where evaluators deliberately create honeypot examples that look like they could be in training data but definitely are not, then check if the model performs suspiciously well on them. If a model trained in early 2026 performs better on synthetic examples from a style that matches late 2025 public data versus synthetic examples from a different style, that suggests the training data included that late 2025 public data even if the training cutoff claims otherwise.

Another technique is **canary examples** embedded in published benchmarks. Before publishing a benchmark, create a small set of distinctive examples with unusual properties. Monitor model performance on these canaries in future model releases. If performance on canaries increases while performance on other examples does not, that suggests the model was trained on the published benchmark. This technique helps researchers detect when their benchmarks have been scraped into training sets, even if model providers do not disclose training data sources.

The arms race creates pressure for **evaluation data protection** similar to how test sets in academic machine learning are protected. Some organizations now treat evaluation data as confidential and do not publish it even internally beyond the immediate evaluation team. Others use secure evaluation services where models are evaluated by third parties who maintain evaluation set confidentiality. The inconvenience is justified by the need to maintain evaluation integrity in an environment where contamination can happen through many channels.

## Organizational Discipline Around Contamination

Preventing contamination requires organizational discipline, not just technical controls. Teams need training on contamination risks, clear policies about data handling, regular audits of data pipelines, and accountability when contamination occurs. A contamination incident should trigger process review and corrective action, not just fixing the specific instance. Most contamination is preventable through better processes.

Establish a **designated evaluation data steward** responsible for evaluation data integrity. This person maintains evaluation sets, performs contamination checks, approves new evaluation examples, and audits that evaluation data is not leaking into training data. Centralizing responsibility ensures that contamination checking is not forgotten or skipped under schedule pressure. The steward should have authority to block training runs or model releases if contamination is suspected.

Require **contamination sign-off** before publishing metrics or making model release decisions. Before any metric goes into a presentation, dashboard, or publication, someone must certify that contamination checking was performed and that the metric is clean. This slows down metric publication slightly but dramatically reduces the risk of publishing contaminated metrics that later need correction. The sign-off requirement forces teams to think about contamination proactively rather than reactively.

Include contamination scenarios in **red team exercises** and security reviews. Imagine an adversary trying to contaminate your evaluation data to make a bad model look good. What attack vectors would they use? Could they inject examples into your data collection pipeline? Could they manipulate your deduplication logic to allow contaminated examples through? Could they socially engineer access to evaluation data and leak it into training data? Testing these scenarios reveals weaknesses in your contamination prevention systems.

With leakage detection established, ensuring your automated evaluation judges remain reliable over time requires systematic meta-evaluation practices, which Chapter 3-12 explores through comprehensive reliability discipline frameworks.
