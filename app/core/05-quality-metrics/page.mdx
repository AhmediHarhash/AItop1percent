# Section 5 — Quality Dimensions & Metrics

## Chapter 1

### Plain English

Quality dimensions answer:

**"In what ways can this AI system be good or bad?"**

Metrics answer:

**"How do we measure those ways without lying to ourselves?"**

Most AI teams fail here because they:
- collapse quality into one score
- chase vanity metrics
- optimize what's easy to measure instead of what matters

In 2026, elite teams treat metrics as **decision instruments**, not numbers for dashboards.

---

### Why This Section Exists

Without clear quality dimensions:
- metrics are meaningless
- teams optimize the wrong behavior
- regressions hide behind averages
- leadership loses trust in reports
- safety issues get diluted

This section exists to:
- define *what* we measure
- define *why* we measure it
- define *how* metrics connect to shipping decisions

Metrics are powerful. Misused metrics are dangerous.

---

### Quality Is Multi-Dimensional (Non-Negotiable)

There is no single "quality" score.

AI systems are evaluated across **multiple dimensions**, each reflecting a different failure mode.

Typical 2026 dimensions include:

- Correctness
- Grounding
- Completeness
- Usefulness
- Safety
- Robustness
- Latency
- Cost efficiency
- Consistency
- Tone / appropriateness (when relevant)

Not every task uses every dimension equally.

Your eval strategy must define:
- which dimensions apply
- how much they matter
- which ones block shipping

---

### Core Quality Dimensions (Explained)

#### 1) Correctness

**Does the response satisfy the task requirements?**

Correctness depends on:
- task type
- risk tier
- allowed uncertainty

For Tier 0 tasks:
- correctness must be exact

For Tier 2–3 tasks:
- partial correctness may be acceptable

Correctness without context is meaningless.

---

#### 2) Grounding (Truthfulness to Sources)

Applies when:
- RAG is used
- documents are referenced
- facts must come from specific sources

Grounding measures:
- whether claims are supported
- whether citations are accurate
- whether the model invents facts

Grounded but incomplete > confident hallucination.

---

#### 3) Completeness

**Did the response address all required parts of the request?**

Failures here look like:
- answering only half the question
- skipping constraints
- ignoring follow-ups

Completeness is often overlooked and causes subtle dissatisfaction.

---

#### 4) Usefulness

**Would a real user consider this response helpful?**

This dimension captures:
- clarity
- relevance
- actionability

Something can be correct but useless.

Usefulness is often measured via human judgment, not automation.

---

#### 5) Safety

**Did the system avoid forbidden behavior?**

Includes:
- policy violations
- unsafe advice
- data leakage
- harmful instructions

Safety is usually:
- binary
- non-negotiable
- release-blocking

You do not average safety.

---

#### 6) Robustness

**Does the system behave well under stress or variation?**

Includes:
- ambiguous inputs
- malformed requests
- adversarial phrasing
- long contexts

Robustness protects against real-world chaos.

---

#### 7) Consistency

**Does the system behave predictably across similar inputs?**

Inconsistency:
- erodes trust
- breaks automation
- confuses users

Consistency is critical for enterprise adoption.

---

#### 8) Latency

**How fast does the system respond?**

Especially critical for:
- voice
- real-time agents
- interactive UX

Latency is a quality dimension, not just infra.

A perfect answer that arrives too late is a bad answer.

---

#### 9) Cost Efficiency

**How much does quality cost?**

Includes:
- token usage
- model selection
- tool calls
- infrastructure

Cost efficiency becomes critical at scale.

Elite teams optimize **quality per dollar**, not just quality.

---

### Metrics: Turning Dimensions into Signals

Metrics are **proxies**, not truth.

A good metric:
- correlates with real quality
- is stable over time
- detects regressions
- informs decisions

A bad metric:
- is easy to game
- hides failures
- looks good on slides

---

### Metric Types (2026 Standard)

#### 1) Binary Metrics (Gates)

Examples:
- Safety violation: yes/no
- Grounding failure: yes/no
- Tool execution success: yes/no

Used for:
- release blocking
- Tier 0 / Tier 1 tasks

Binary metrics are powerful and unforgiving.

---

#### 2) Scalar Metrics (Scores)

Examples:
- Helpfulness score (1–5)
- Clarity score (1–5)
- Completeness score (1–5)

Rules:
- define anchors
- avoid vague midpoints
- never use alone for gating

---

#### 3) Distribution Metrics

Instead of averages, look at:
- percentiles
- tail behavior
- worst-case slices

Example:
- "95% of responses above threshold"
- "0 critical failures in golden set"

This avoids hiding disasters behind averages.

---

#### 4) Comparative Metrics

Compare:
- version A vs version B
- model X vs model Y
- prompt v1 vs v2

Humans are better at comparison than absolute scoring.

Used heavily in iteration loops.

---

### Metric Design Rules (Critical)

#### Rule 1: Metrics Must Map to Ground Truth
If a metric doesn't reflect Section 02, it is noise.

---

#### Rule 2: No Metric Without a Decision
Every metric must answer:
- "What do we do if this changes?"

If the answer is "nothing," delete the metric.

---

#### Rule 3: Separate Signal from Noise
Do not mix:
- safety with usefulness
- latency with correctness
- cost with grounding

Each dimension deserves clarity.

---

#### Rule 4: Avoid Metric Overload
More metrics ≠ better understanding.

Elite teams track:
- a small set of core metrics
- a larger set of diagnostic metrics (used when needed)

---

### Aggregation (How Metrics Roll Up)

You rarely use a single number.

Common patterns:
- dimension-level thresholds
- weighted rollups (with safety as hard gate)
- slice-based views (by task, tenant, risk)

Never let a good average override a bad failure.

---

### Metrics for Different Task Types

#### Chat
- usefulness
- correctness
- safety
- tone

#### RAG
- grounding accuracy
- hallucination rate
- completeness

#### Tool Calling
- correct tool selection rate
- argument validity rate
- execution success rate

#### Agents
- task completion rate
- failure recovery rate
- unintended action rate

#### Voice
- latency
- interruption handling
- misunderstanding recovery

Metrics must follow task reality.

---

### Enterprise Perspective

Enterprises expect:
- explainable metrics
- auditability
- stable definitions
- consistency across releases

They care less about raw scores and more about:
- regressions
- risk exposure
- predictability

---

### Founder Perspective

Founders need metrics that:
- guide iteration
- protect trust
- inform shipping decisions
- balance quality vs cost

Bad metrics waste time and money.

---

### Common Failure Modes

- optimizing one metric at the expense of others
- averaging away safety failures
- using benchmarks instead of product metrics
- tracking metrics no one acts on
- changing metric definitions silently
- confusing correlation with causation

These failures scale fast.

---

### Interview-Grade Talking Points

You should be able to explain:

- why quality is multi-dimensional
- why averages are dangerous
- how metrics connect to release gates
- how cost is a quality dimension
- how metrics differ by task type

This is staff-level thinking.

---

### Completion Checklist

You are done with this section when you can:

- list core quality dimensions for any AI system
- design metrics per task type
- explain why safety is a hard gate
- explain why averages lie
- explain how metrics drive decisions

Do not move on until this feels obvious.

---

### How This Connects Forward

Now that quality is measurable, the next question is:

**Who decides what's good, and how do humans stay in the loop?**

That is Section 06 — Human Evaluation Systems.
