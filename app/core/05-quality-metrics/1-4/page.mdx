# 1.4 â€” How Quality Perception Differs Across Stakeholders

In early 2026, a B2B SaaS company offering AI-powered data analysis tools reached a breaking point. Their system analyzed customer datasets and generated business insights with executive summaries. The engineering team was proud of the technical achievement: 91% accuracy on benchmark tests, 2.4 second average latency, robust handling of edge cases, and comprehensive safety filters. The product team was frustrated because user engagement was declining despite the strong technical metrics. The legal team was nervous because several enterprise customers had raised concerns about audit trails and explainability. The finance team was alarmed because cost per query had increased 40% over three months, making the unit economics unsustainable. The sales team was struggling to close deals because prospects kept asking questions about accuracy that engineering said were already solved. When the executive team convened a quality review to understand why a technically excellent product was failing in market, they discovered that each stakeholder group had a completely different definition of quality, measured it differently, and prioritized different dimensions. The engineering team optimized for correctness and robustness. The product team cared about usefulness and user satisfaction. The legal team needed transparency and audit compliance. The finance team watched cost. The sales team needed simple accuracy claims that prospects could understand. Nobody was wrong. But nobody was aligned, and the product was failing because it optimized for some stakeholder needs while ignoring others.

The failure was not a technical problem. It was an alignment problem disguised as a quality problem. The company had invested heavily in quality measurement, tracking nine dimensions across fifteen metrics. But they had never explicitly mapped which stakeholders cared about which dimensions or built processes for resolving conflicts when different stakeholder needs pulled in different directions. Engineering built what they thought was a high-quality system based on their definition of quality. Product launched it believing quality was good based on engineering's metrics. Customers evaluated it using their own definition of quality, which did not match either engineering's or product's definitions. Legal evaluated it using compliance frameworks that no one had mapped to the technical metrics. Finance evaluated it using cost metrics that were divorced from the quality discussion. Each group talked about quality as if it were a single thing that everyone understood the same way. In reality, quality meant something different to each stakeholder, and those differences were never reconciled.

## Engineering's Definition of Quality

Engineering teams typically define quality in terms of measurable technical properties: accuracy, precision, recall, latency, throughput, error rates, and robustness to edge cases. These dimensions are prioritized because they are quantifiable, reproducible, and directly influenced by engineering decisions about model architecture, training data, and inference optimization. When an engineering team says their system has "high quality," they usually mean that it performs well on their evaluation benchmarks, handles a wide range of inputs without crashing, responds quickly, and degrades gracefully under load. This definition is not wrong, but it is narrow. It focuses on properties that can be evaluated in a test environment before users ever see the system.

The data analysis company's engineering team measured quality using a held-out test set of 5,000 labeled examples covering common business analysis scenarios. They computed accuracy by comparing generated insights to human-written reference answers using semantic similarity. They measured latency using synthetic load tests. They evaluated robustness by testing with malformed inputs, unusual data distributions, and adversarial examples. By all these measures, the system was high quality. But these measures did not capture whether the insights were useful to business users who lacked the technical background to interpret statistical nuances. They did not capture whether the explanations were sufficient for legal teams to audit decisions. They did not capture whether the cost structure made sense for the pricing model. Engineering built a technically excellent system that solved the wrong problem because their definition of quality did not encompass the problems that mattered to other stakeholders.

This is not a criticism of engineering-centric quality definitions. Engineering teams are right to prioritize technical correctness and reliability. A system that is not technically sound cannot be saved by good product packaging. But technical excellence is necessary, not sufficient. The challenge is that engineering teams often treat their definition of quality as complete rather than as one perspective among several. When product or customers raise concerns that are not captured in engineering metrics, engineering teams sometimes dismiss those concerns as subjective or unmeasurable rather than recognizing them as different but equally valid dimensions of quality. The solution is not for engineering to abandon technical metrics. The solution is to recognize that technical metrics measure some aspects of quality but not all aspects, and to build explicit bridges between engineering metrics and the concerns of other stakeholders.

Engineering's focus on reproducibility and quantification creates a bias toward metrics that can be automated and computed cheaply. This leads to over-reliance on metrics that are easy to measure rather than metrics that matter most to users. It is easier to compute semantic similarity between generated text and reference text than to measure whether users found the insight actionable. It is easier to count error rates than to measure whether errors occurred in high-stakes situations or low-stakes situations. The engineering instinct to prefer hard numbers over soft judgments is valuable for maintaining rigor, but it becomes dangerous when it leads teams to optimize for measurability rather than impact.

## Product's Definition of Quality

Product teams typically define quality in terms of user satisfaction and business outcomes: whether users find the system useful, whether it helps them accomplish their goals, whether they continue to use it over time, and whether they recommend it to others. These dimensions are prioritized because product teams are accountable for user engagement, retention, and ultimately revenue. When a product manager says a system is "high quality," they usually mean that users are happy with it, usage metrics are trending upward, and customer feedback is positive. This definition complements engineering's definition but often conflicts with it. A system can be technically excellent but not useful. A system can be useful but not technically robust.

The data analysis company's product team evaluated quality primarily through user feedback and behavioral metrics. They tracked how often users accepted or rejected generated insights, how long they spent reviewing reports, and whether they returned to use the system again. They conducted user interviews to understand what users found valuable. By these measures, quality was mediocre. Users appreciated the speed and found some insights valuable, but they struggled to trust the outputs because they could not verify the reasoning. They wanted more context about data sources and assumptions. They wanted confidence intervals on predictions. They wanted plain-language explanations instead of statistical jargon. These needs did not show up in engineering's accuracy metrics because accuracy measured whether outputs were correct, not whether users could understand and act on them.

Product's definition of quality is often more holistic than engineering's but also more subjective and harder to measure precisely. User satisfaction depends on many factors beyond the AI system itself: UI design, onboarding, documentation, customer support. Disentangling whether low satisfaction reflects poor AI quality or poor product packaging is challenging. This measurement difficulty sometimes leads engineering teams to dismiss product's quality concerns as too vague to act on. The solution is not for product to adopt engineering's metrics wholesale. The solution is to build explicit mappings between user needs and technical dimensions. If users say they cannot trust outputs, that maps to groundedness and transparency. If users say insights are not useful, that maps to completeness and usefulness. If users abandon the system mid-task, that might map to latency or coherence. Making these mappings explicit allows engineering and product to have productive conversations about quality improvements instead of talking past each other.

Product teams also face pressure to ship quickly and iterate based on user feedback, which creates tension with engineering's preference for comprehensive pre-launch testing. Product wants to launch with minimum viable quality and improve based on real usage data. Engineering wants to launch only when the system meets high technical standards across all measured dimensions. Both approaches have merit. Launching early generates real user feedback that cannot be obtained from synthetic benchmarks. Launching late reduces the risk of public failures that damage brand reputation. The conflict is not resolvable through better metrics. It is resolvable through explicit negotiation about risk tolerance and quality gates that both teams agree on before development begins.

## Legal and Compliance's Definition of Quality

Legal and compliance teams define quality primarily in terms of risk mitigation: whether the system complies with regulations, whether outputs can be audited, whether decisions can be explained to regulators, and whether the system avoids generating content that creates liability. These dimensions are prioritized because legal teams are accountable for regulatory compliance and risk management. When a legal team says a system has "acceptable quality," they mean it meets regulatory requirements, provides adequate documentation for audits, and does not expose the company to legal risk. This definition is often orthogonal to both engineering's technical metrics and product's user satisfaction metrics.

The data analysis company's legal team became concerned about quality when several enterprise customers asked questions that engineering's metrics could not answer. Customers wanted to know how the system decided which insights to highlight and which to omit. They needed audit trails showing what data was used for each analysis. They asked whether the system could produce systematically biased results for certain demographic groups, which mattered because their analyses informed compensation decisions subject to employment law. They needed documentation of the AI system's capabilities and limitations for their own regulatory filings under the EU AI Act, which classifies certain business decision tools as high-risk systems requiring transparency and human oversight. Engineering's 91% accuracy metric told legal nothing about whether these requirements were met.

Legal's quality concerns are often treated as external constraints rather than core quality dimensions, which creates tension. Engineering sees legal requirements as bureaucratic overhead that slows down development. Legal sees engineering's resistance to documentation and explainability as increasing risk. The reality is that for any AI system deployed in a regulated domain or enterprise context, legal and compliance requirements are first-order quality dimensions. A system that cannot demonstrate compliance is not high quality, regardless of technical performance. The solution is to include transparency, explainability, and audit capability as explicit quality dimensions from the start, with metrics and thresholds defined in collaboration with legal. This might mean logging all data sources used for each output, providing confidence scores with calibrated meaning, or implementing review processes for high-stakes decisions. These requirements impose technical costs, but they are not optional.

Legal teams also operate on longer time horizons than product or engineering teams. A compliance failure might not surface until a regulatory audit eighteen months after deployment. Legal's definition of quality includes forward-looking risk assessment that asks not just whether the system works today but whether it will withstand scrutiny from regulators who may interpret requirements differently than the company does. This temporal dimension creates friction when legal requests extensive documentation and testing that product sees as delaying launch and engineering sees as over-engineering. The conflict is real because the consequences of compliance failure are catastrophic in ways that technical failures rarely are. A buggy feature can be patched. A compliance violation can result in fines, sanctions, or business shutdown.

## Finance's Definition of Quality

Finance teams define quality in terms of unit economics: whether the system can operate sustainably at target margins, whether cost per user is below lifetime value, and whether the cost structure scales favorably as usage grows. When a finance team says a system has "viable quality," they mean it can operate profitably or within budget constraints. This dimension is often invisible to engineering until it becomes a crisis. Engineers optimize for accuracy and speed, sometimes without realizing that their optimizations are making the system economically unviable.

The data analysis company's finance team watched with growing alarm as the engineering team improved accuracy from 89% to 91% by switching from GPT-5 to Claude Opus 4.5 and adding a multi-step reasoning pipeline. The accuracy improvement was real. The cost increase was catastrophic. Cost per query went from $0.18 to $0.31. At the current usage levels of 2.3 million queries per month, that was an additional $300,000 per month in inference costs. The product's pricing model assumed cost per query under $0.20. At $0.31 per query, the unit economics were underwater. When finance raised this issue in a quality review, engineering responded that accuracy was more important than cost and suggested raising prices. Product responded that the market would not bear higher prices and suggested reducing features to cut costs. The conversation devolved into arguing about priorities rather than collaboratively solving the cost-quality tradeoff.

Finance's quality concerns are often dismissed as penny-pinching or as constraints that will be solved by future scale or optimization. Sometimes this is true. Early-stage products can run at negative unit economics while proving value. But dismissing cost as a quality dimension is dangerous because cost constraints are hard limits. You can launch a product with mediocre accuracy and improve it over time. You cannot launch a product that loses money on every transaction and hope to make it up in volume. Cost needs to be treated as a first-order quality dimension with explicit metrics and thresholds. Engineering should know the cost budget per query and should optimize within that constraint. If achieving required accuracy within cost constraints is impossible, that is a signal to reconsider the product strategy, not to ignore the cost dimension.

Finance teams also care about predictability and variance in costs, not just average costs. A system with average cost of $0.20 per query but high variance where 10% of queries cost over $1.00 creates budgeting problems and might indicate that certain usage patterns are economically unsustainable. Finance wants to understand the cost distribution across query types, user segments, and time periods. They want to know whether costs are trending upward as the model handles more complex queries or whether costs remain stable. These concerns map to technical decisions about caching strategies, model selection for different query types, and timeout policies that prevent runaway costs on pathological inputs. But engineering teams often do not instrument cost metrics with the granularity finance needs, creating a gap between finance's questions and engineering's data.

## Sales and Customer Success's Definition of Quality

Sales and customer success teams define quality in terms of how well the system addresses customer questions and objections: whether customers believe the accuracy claims, whether they understand what the system does and what it does not do, whether it solves their specific pain points, and whether they can explain it to their own stakeholders. When sales teams say a system has "sellable quality," they mean they can confidently answer prospect questions and close deals. This definition is shaped by direct customer conversations and reflects how customers evaluate systems before purchasing.

The data analysis company's sales team struggled to sell the product despite strong engineering metrics because prospects asked questions that engineering's quality metrics did not address. Prospects asked: "What types of data does this work on?" Engineering's test set covered common scenarios but did not document which data types were supported. Prospects asked: "How accurate is this for my industry?" Engineering measured overall accuracy but did not break it down by industry vertical. Prospects asked: "Can you guarantee 95% accuracy?" Engineering knew the system achieved 91% on their test set but could not provide guarantees for novel data. Prospects asked: "What happens when it makes a mistake?" Engineering focused on preventing mistakes but had not documented failure modes or built mechanisms for users to report errors. Each of these questions was reasonable from a customer perspective and impossible to answer from engineering's quality metrics.

Sales and customer success's quality needs often surface late in product development because these teams are not involved in early design discussions. By the time sales talks to customers and discovers that the product does not address their concerns, engineering has already built the system and product has already defined the roadmap. Fixing the gaps requires rework. The solution is to involve sales and customer success in quality definition from the start. Sales can articulate what claims customers need to hear and what evidence they need to believe those claims. Customer success can identify what post-sale support issues arise from quality gaps. These inputs should shape quality dimensions, metrics, and thresholds just as much as engineering's technical capabilities and product's user research.

Customer success teams also care about edge cases and failure recovery in ways that differ from engineering's focus. Engineering measures how often the system fails. Customer success cares about what happens after failure: can users recover gracefully, does the system provide helpful error messages, can customer success agents diagnose problems quickly when users report issues. These concerns translate to technical requirements around error logging, user-facing explanations of failures, and tooling for support teams to debug production issues. But these requirements are often deprioritized because they do not improve accuracy or performance metrics that engineering optimizes for.

## Users' Definition of Quality

End users define quality in terms of whether the system helps them do their job better, faster, or with less effort. Users care about accuracy, but only insofar as it affects their outcomes. They care about speed, but only if the system is also useful. They care about comprehensive outputs, but only if they can process the information quickly. User quality perceptions are shaped by their workflow context, their domain expertise, and their alternatives. A system that feels high-quality to a novice might feel inadequate to an expert. A system that works well in a high-stakes decision context might be too cautious for exploratory analysis. User quality perception is also relative. A system that seems slow compared to a human expert might seem fast compared to the user's previous manual process.

The data analysis company eventually conducted in-depth user research to understand why engagement was declining despite strong technical metrics. They discovered that users evaluated quality based on criteria that appeared nowhere in engineering's metrics. Users cared whether insights were novel, meaning the system told them something they did not already know. Engineering's accuracy metric rewarded reproducing known insights, not surfacing new ones. Users cared whether insights were actionable, meaning they could take a business decision based on the output. Engineering's metric did not distinguish between accurate observations and actionable recommendations. Users cared whether they could verify insights without spending hours digging through raw data. Engineering had focused on generating correct insights but not on providing verification paths. Each of these gaps represented a different dimension of quality that mattered to users but was invisible to engineering's evaluation.

User quality perception is the ultimate arbiter of product success. If users do not perceive the system as high quality, they will not use it, regardless of technical metrics. But user perception is also the most difficult to measure during development because it requires putting the system in front of real users in realistic contexts. Most engineering quality processes rely on synthetic benchmarks and controlled evaluations that cannot capture the full complexity of how users experience quality. The solution is to include user feedback and behavioral signals as first-class quality metrics, not just post-launch monitoring but as part of the development cycle. This means closed betas, user studies, and qualitative feedback collection long before launch. It means accepting that user perception might contradict technical metrics and being willing to prioritize user needs even when engineering believes the system is already high quality.

Different user segments also have different quality expectations that cannot be satisfied with a single system configuration. Power users want comprehensive outputs with all available detail and are willing to spend time reviewing information. Casual users want concise summaries and fast responses. Domain experts want technical accuracy and precise terminology. Non-experts want plain-language explanations and contextual definitions. A system optimized for power users will frustrate casual users and vice versa. This creates a design challenge that cannot be solved through better measurement alone. It requires segmentation strategies, adaptive interfaces, or explicit user controls that let different users get different experiences from the same underlying system.

## Mapping Stakeholder Concerns to Quality Dimensions

The practical challenge of managing stakeholder quality perception is that you cannot optimize for every stakeholder's definition simultaneously. Some stakeholder needs will conflict. Engineering wants to use the most accurate model even if it costs more. Finance wants to minimize cost even if it reduces accuracy. Product wants to ship quickly even if legal needs more time for compliance review. These conflicts are not resolvable through measurement. They are resolvable through explicit negotiation and prioritization based on product strategy and risk tolerance.

The first step is mapping each stakeholder's quality concerns to specific dimensions in your quality framework. Engineering's focus on accuracy maps to the correctness dimension. Product's focus on user satisfaction maps to usefulness, completeness, and tone. Legal's concerns map to safety, transparency, and groundedness. Finance's concerns map to cost and latency. Sales's concerns map to robustness and usefulness in specific customer contexts. Making these mappings explicit clarifies where stakeholder needs align and where they conflict. When everyone agrees that safety is critical, you can set high thresholds and prioritize safety work. When engineering prioritizes correctness but users prioritize usefulness, you have identified a tradeoff that requires product judgment.

The second step is establishing stakeholder-specific views of quality dashboards and metrics. Engineering needs detailed technical metrics across all dimensions to guide optimization. Product needs high-level metrics focused on user-facing dimensions with trends over time. Legal needs compliance metrics with clear pass-fail thresholds. Finance needs cost metrics with projections. Sales needs customer-facing metrics that can be shared with prospects. Each stakeholder group needs to see quality information in the form that is actionable for their decisions. This does not mean creating separate metrics for each group. It means organizing and presenting the same underlying measurements in ways that address each group's questions.

The third step is creating forums for regular stakeholder alignment discussions. Monthly quality review meetings that bring together representatives from each stakeholder group provide a venue for surfacing conflicts early and negotiating tradeoffs collaboratively. These meetings are not status updates. They are decision-making sessions where stakeholders present their quality concerns, engineering explains technical constraints and opportunities, and the group makes explicit choices about which dimensions to prioritize. Without these forums, stakeholder conflicts remain hidden until they explode into crises that force rushed decisions under pressure.

## Building Alignment Through Shared Metrics

While different stakeholders have different quality priorities, alignment is possible when metrics are designed to bridge stakeholder concerns. A well-designed quality metric should be meaningful to multiple stakeholders even if they care about it for different reasons. Latency is meaningful to engineering as a performance optimization target, to product as a user experience factor, to finance as a driver of infrastructure cost, and to sales as a competitive differentiator. Groundedness is meaningful to engineering as a technical capability, to product as a trust factor, to legal as a compliance requirement, and to users as a verification mechanism. When metrics span stakeholder concerns, they create natural alignment points where everyone is pulling in the same direction.

The data analysis company rebuilt their quality framework around shared metrics that spanned stakeholder concerns. They defined usefulness as a metric that combined user ratings, usage behavioral signals, and business outcome tracking. This metric was meaningful to product as a user satisfaction indicator, to sales as evidence of value, to engineering as a validation that technical improvements translated to user benefit, and to finance as a leading indicator of retention. They defined transparency as a metric that measured whether outputs included source attribution, confidence calibration, and reasoning traces. This metric was meaningful to legal for compliance, to users for verification, to sales for addressing trust concerns, and to engineering as a technical challenge. By designing metrics that multiple stakeholders cared about, they created natural collaboration points rather than conflicting priorities.

The shared metrics approach requires compromise. Engineering might prefer purely technical accuracy metrics that are easier to optimize. Product might prefer subjective user satisfaction metrics that capture holistic experience. Legal might prefer binary compliance metrics that give clear pass-fail signals. Shared metrics are necessarily more complex and harder to optimize than single-stakeholder metrics. But the organizational benefit of alignment outweighs the measurement complexity. When everyone monitors the same core metrics and understands how those metrics relate to their concerns, quality discussions become collaborative problem-solving rather than territorial disputes about whose definition of quality matters more.

Shared metrics also require shared definitions and measurement methodologies. When engineering and product both track usefulness but engineering measures it through automated semantic similarity and product measures it through user surveys, they are measuring different things despite using the same term. This definitional drift creates confusion and conflict. The solution is to establish canonical metric definitions that specify exactly how each metric is computed, what data sources feed into it, and what the thresholds mean. These definitions should be documented, versioned, and maintained as shared artifacts that all stakeholders reference. When metric definitions change, all stakeholders should be notified and given the opportunity to understand how the change affects their interpretation of quality.

## When Stakeholder Misalignment Kills Products

The most common failure mode for AI products is not technical inadequacy. It is stakeholder misalignment on quality definition that persists until after launch, at which point it is too late to fix. Engineering builds what they think is a high-quality system. Product launches it believing engineering's assessment. Customers evaluate it using criteria that neither engineering nor product anticipated. Legal discovers compliance gaps after enterprise customers raise concerns. Finance realizes the unit economics do not work after usage scales. Each failure is preventable through earlier alignment, but alignment is uncomfortable because it forces explicit negotiation of tradeoffs that everyone would prefer to avoid.

The data analysis company survived their quality crisis by conducting a comprehensive stakeholder quality alignment workshop. They brought together representatives from engineering, product, legal, finance, sales, and customer success. Each group presented what quality meant to them and what metrics they tracked. The workshop facilitator mapped these definitions to the multi-dimensional quality framework and identified conflicts. Engineering's push for higher accuracy using more expensive models conflicted with finance's cost constraints. Product's desire for comprehensive outputs conflicted with user preferences for concise summaries. Legal's need for detailed explanations conflicted with latency requirements. Rather than pretending these conflicts did not exist, the team made explicit tradeoff decisions. They decided that for high-stakes analyses above a certain dollar threshold, transparency and groundedness were more important than cost and latency. For low-stakes exploratory analyses, speed and cost mattered more than exhaustive completeness. By making these tradeoffs explicit and mapping them to user segments and use cases, they created a coherent quality strategy that all stakeholders understood and supported.

The workshop also revealed assumptions that different groups held about what users needed. Engineering assumed users wanted the most accurate possible answers. User research showed users wanted good enough answers that they could verify quickly. Sales assumed prospects needed accuracy guarantees. Prospect interviews showed they needed confidence ranges and failure mode documentation. Legal assumed the EU AI Act required full technical explainability. Closer reading showed it required appropriate transparency relative to risk, which was less stringent. By surfacing and testing these assumptions, the team aligned on a more realistic and achievable quality target that satisfied all stakeholder concerns without over-engineering.

The workshop methodology itself matters. Generic cross-functional meetings where each team presents status updates produce little alignment. Structured workshops where a facilitator forces explicit mapping of stakeholder concerns to quality dimensions, documents conflicts, and facilitates decision-making produce durable alignment. The facilitator should be someone with sufficient technical and product knowledge to understand all stakeholder perspectives but without direct responsibility for any single function's goals. This neutrality allows them to push for hard conversations that team members might avoid to preserve relationships or avoid conflict.

## Creating Durable Stakeholder Alignment

Stakeholder alignment on quality is not a one-time event. It requires ongoing communication and recalibration as the product evolves, the market changes, and the team learns what actually matters. The quality dimensions that are critical at launch might be less important at scale. The tradeoffs that make sense for early adopters might not make sense for mainstream customers. The metrics that align stakeholders during development might not be the metrics that matter in production. Durable alignment requires processes that keep stakeholders synchronized as understanding evolves.

The data analysis company institutionalized stakeholder alignment through monthly quality councils that brought together representatives from each functional area to review quality metrics, discuss threshold violations, and negotiate upcoming quality investments. These meetings were not status updates. They were working sessions where tradeoffs were debated and resolved. When engineering wanted to improve accuracy at the cost of latency, product and finance weighed in on whether users and unit economics could absorb the tradeoff. When legal needed new transparency features, engineering and product discussed implementation cost and roadmap impact. When user research revealed new quality gaps, the full stakeholder group decided how to prioritize addressing them. This ongoing negotiation kept everyone aligned and prevented the silent divergence of quality definitions that had previously caused stakeholder conflicts to explode into crises.

The quality council structure also created accountability for alignment. Each stakeholder group designated a quality council representative who was responsible for understanding their function's quality needs and representing them in council discussions. These representatives became the interface between the council and their functional teams, translating council decisions back into actionable work and surfacing new concerns from their teams. This structure prevented the common failure mode where alignment happens at senior levels but never propagates to the teams doing implementation work.

Documentation of council decisions proved crucial for maintaining alignment over time. The team maintained a quality decision log that recorded every significant tradeoff decision, the stakeholders involved, the rationale, and the agreed-upon metrics for measuring success. When new team members joined or when decisions were questioned months later, this log provided the context for why certain quality choices were made. It also prevented re-litigation of settled questions and created institutional memory that survived team turnover.

Building stakeholder alignment on quality perception is essential work that most teams skip in favor of diving into measurement and optimization. The result is technically excellent systems that fail to satisfy the needs of multiple stakeholder groups because those needs were never explicitly mapped, prioritized, and balanced. The next step in building robust quality systems is understanding how to move from measurement to evaluation, and from evaluation to continuous improvement, which requires understanding the metric hierarchy that transforms raw measurements into actionable signals.
