# 3.8 â€” Statistical Significance and Sample Size for AI Metrics

In November 2025, a healthcare documentation startup rolled out a new Claude Opus 4.5 prompt for their clinical note generation system, replacing their previous GPT-5 configuration. The evaluation team ran their standard test: fifty de-identified patient encounters drawn from their validation set. The new prompt achieved eighty-four percent accuracy on correctly extracting diagnoses and treatment plans, compared to seventy-nine percent for the old prompt. A five-point improvement looked strong. The team celebrated the win, deployed to production serving two hundred hospitals, and moved on to the next project. Three weeks later, their automated monitoring flagged a problem: production accuracy was running at seventy-six percent, worse than the baseline they had replaced. The team was confused. They re-ran their fifty-case evaluation and got eighty-one percent this time, still better than baseline. They ran it again and got seventy-eight percent. Then eighty-three percent. Then eighty percent. The scores were jumping around by five points between runs, making it impossible to know whether the new prompt was actually better or whether they had gotten lucky on the first evaluation.

The root cause was insufficient sample size combined with high measurement variance. With only fifty test cases and a non-deterministic model, random variation could easily produce five-point swings in accuracy. The initial eighty-four percent result was within the noise of their measurement process, not a signal of real improvement. The team had confused a lucky roll of the dice with a genuine quality gain. They had violated a fundamental principle of empirical measurement: before you can detect a difference between systems, you need enough samples to distinguish signal from noise.

## The Noise Problem in AI Evaluation

AI systems are non-deterministic. The same model with the same prompt on the same input can produce different outputs due to sampling temperature, subtle differences in API state, or stochastic elements in retrieval or tool use. This non-determinism means that measuring any quality metric produces a distribution of values rather than a single fixed value. Running your evaluation once gives you one sample from that distribution. Running it ten times gives you ten samples. The question is whether the sample you observed is representative of the true underlying distribution or whether it is an outlier driven by chance.

**Statistical significance** is the discipline of distinguishing signal from noise. It asks: if there is no real difference between two systems, what is the probability that random variation would produce a difference as large as what I observed? If that probability is very low, say less than five percent, you can be reasonably confident that the difference is real. If that probability is high, say thirty percent, the difference might just be noise and you should not trust it. Statistical significance does not tell you whether the difference matters, only whether it is likely to be real rather than illusory.

The challenge is that statistical significance depends critically on sample size. With a small sample, only very large differences are statistically significant because noise dominates. With a large sample, even tiny differences become statistically significant because noise averages out. This creates a trap: if you use too few samples, you will miss real differences or falsely believe in fake differences. If you use too many samples, you will detect differences that are real but too small to matter. The art is choosing a sample size that reliably detects differences you care about while not wasting resources on detecting irrelevant tiny differences.

The healthcare documentation team re-ran their evaluation with five hundred cases instead of fifty and measured accuracy ten times to estimate variance. The new prompt showed eighty-one point three percent accuracy with a standard deviation of one point two percent across runs, while the old prompt showed seventy-nine point seven percent with a standard deviation of one point one percent. The difference was now clearly not statistically significant: the confidence intervals overlapped substantially. What looked like a five-point improvement in a fifty-case evaluation was actually a one-point difference that could easily be explained by chance. The production regression they observed was similarly within noise. Neither system was meaningfully better. The team had wasted three weeks deploying and debugging a change that did not matter.

Understanding statistical significance requires understanding variance, confidence intervals, and power analysis. These are not optional advanced topics. They are foundational to any empirical measurement on non-deterministic systems.

## Variance and Confidence Intervals

Every metric you measure has uncertainty. When you report that your model achieves eighty-five percent accuracy, what you really mean is that in your particular evaluation sample, eighty-five percent of cases were correct. If you measured a different sample, you might get eighty-three percent or eighty-seven percent. The **variance** of your metric captures how much the measured value would jump around if you repeated the measurement multiple times. High variance means your metric is noisy and unreliable. Low variance means your metric is stable and repeatable.

Variance depends on two factors: the inherent variability in your system's outputs and the size of your sample. A highly non-deterministic model with temperature set to one point zero will produce more variance than a nearly deterministic model with temperature near zero. A small sample will produce more variance than a large sample because individual outliers have more impact. You can reduce variance by either making your system more deterministic or by increasing your sample size. Since making systems deterministic often reduces quality, increasing sample size is usually the better approach.

A **confidence interval** quantifies the range of values you would expect if you repeated your measurement many times. A ninety-five percent confidence interval means that if you ran your evaluation one hundred times, ninety-five of those runs would produce a metric value within the interval. Confidence intervals make uncertainty explicit. Instead of claiming your model achieves eighty-five percent accuracy, you report eighty-five percent accuracy with a ninety-five percent confidence interval of eighty-two to eighty-eight percent. This tells your audience that the true accuracy is very likely somewhere between eighty-two and eighty-eight, but you cannot pin it down more precisely with your current sample size.

Computing confidence intervals for AI metrics is straightforward when you have binary outcomes like correct versus incorrect. You can use standard binomial proportion confidence intervals, such as the Wilson score interval or the Clopper-Pearson exact interval. For a sample of one hundred cases with eighty-five correct, the ninety-five percent confidence interval is approximately seventy-seven to ninety-one percent. For a sample of one thousand cases with eight hundred fifty correct, the interval tightens to eighty-three to eighty-seven percent. The interval width shrinks as the square root of sample size, so quadrupling your sample halves your uncertainty.

An educational content generation team measured their explanation quality score, which ranged from zero to ten based on expert ratings. With one hundred examples, they observed a mean score of seven point four with a standard deviation of one point eight. The ninety-five percent confidence interval for the mean is approximately seven point zero to seven point eight. This four-point-eight range felt too wide for confident decision-making. They increased their evaluation set to four hundred examples and recomputed the confidence interval: seven point three to seven point five. This narrower interval gave them confidence that their true quality was close to seven point four, not somewhere between seven and eight.

Confidence intervals should be reported alongside every metric used for decision-making. A metric without a confidence interval is an invitation to over-interpret noise as signal. Showing confidence intervals forces honesty about measurement uncertainty and prevents teams from chasing phantom improvements that fall within the noise band.

## Sample Size Calculations

How many evaluation cases do you need to measure a metric reliably? This depends on three factors: the variability of your metric, the size of the difference you want to detect, and the confidence level you require. **Sample size calculations** formalize this relationship, telling you how many samples are sufficient to achieve your measurement goals.

The most common scenario is comparing two systems: your baseline and a candidate improvement. You want to know whether the candidate is better, and you want to avoid false positives where you conclude it is better when it is not and false negatives where you conclude it is not better when it actually is. Sample size calculations tell you how many cases you need to control both error rates.

For binary metrics like accuracy, a standard formula uses the expected proportions for each system, the desired significance level, and the desired power. Significance level, typically set at five percent, controls false positives: how often you will incorrectly conclude there is a difference when there is not. Power, typically set at eighty percent, controls false negatives: how often you will correctly detect a difference when there is one. A common rule of thumb is that to detect a five-percentage-point difference in accuracy with eighty percent power and five percent significance, you need roughly four hundred cases per system. To detect a three-point difference, you need roughly one thousand cases. To detect a one-point difference, you need several thousand cases.

These numbers shock many teams. Fifty-case evaluation sets, which are common in practice, can only reliably detect differences of ten percentage points or more. Anything smaller is lost in the noise. If your baseline is at eighty percent accuracy and your candidate is at eighty-five percent, a fifty-case evaluation will fail to detect the improvement about half the time. You will either falsely conclude there is no improvement and stick with the worse system, or you will get lucky and observe a difference but not know whether it is real.

A conversational AI team was comparing two retrieval strategies for their Gemini 2-based assistant. They wanted to detect a three-percentage-point improvement in answer relevance. Sample size calculations indicated they needed roughly one thousand queries to achieve eighty percent power. Their existing evaluation set had two hundred queries. They invested two weeks in expanding the set to one thousand two hundred queries by sampling from production logs and having annotators label them. This upfront investment paid off immediately: when they ran the comparison, the new retrieval strategy showed a four-point improvement with a tight confidence interval that clearly excluded zero. They deployed with confidence, knowing the improvement was real. Without the larger sample, they would have observed a noisy signal and likely made the wrong decision.

Sample size calculations are approximations, not exact requirements. The formulas assume certain statistical properties that may not hold perfectly for your data. But they provide valuable guidance about whether your evaluation set is in the right ballpark. If your sample size is an order of magnitude too small, the calculation will tell you. If it is in the right range, you can trust your measurements.

Many teams resist expanding evaluation sets because annotation is expensive and time-consuming. This is a false economy. Measuring with insufficient data produces unreliable results that lead to bad decisions. The cost of deploying the wrong model or failing to deploy a better model is almost always greater than the cost of collecting more evaluation data. If you cannot afford to measure properly, you cannot afford to make the decision.

## Non-Determinism and Repeated Measurements

Non-deterministic models add a layer of complexity to statistical significance: the same model on the same inputs produces different outputs across runs. This means you cannot measure the model's performance with a single evaluation run. You need to run the evaluation multiple times and aggregate the results to get a stable estimate.

The question is how many times to run the evaluation. Running once gives you one sample of the performance distribution. Running ten times gives you a much better estimate of the mean and variance. Running one hundred times is even better but may be impractical. The tradeoff is between measurement precision and computational cost. For high-stakes decisions, running the evaluation at least three times and averaging the results is a reasonable minimum. For critical decisions, ten runs is safer.

When you run multiple evaluations, you should report both the mean and the variance across runs. A model that achieves eighty-five percent accuracy on average but varies between seventy-eight and ninety-two percent across runs is very different from a model that achieves eighty-five percent with a tight range of eighty-three to eighty-seven percent. The second model is more predictable and likely more reliable in production. Variance across runs is itself a quality metric: lower variance means more consistent behavior.

A financial fraud detection team using GPT-5.1 with a temperature of zero point seven for generating investigation reports measured their relevance metric across ten evaluation runs. The mean score was seventy-nine percent, but individual runs ranged from seventy-three to eighty-four percent, a troubling eleven-point spread. They reduced temperature to zero point three and re-ran the evaluation. The mean stayed at seventy-nine percent, but the range tightened to seventy-six to eighty-one percent. The lower temperature did not improve average quality but substantially reduced variance, making the system more predictable. They deployed the lower-temperature configuration because reliability mattered as much as average performance.

Repeated measurements also allow you to compute confidence intervals that account for non-determinism. Instead of assuming a fixed underlying performance and computing uncertainty from sampling, you directly observe the distribution of performance across runs. This empirical approach is more accurate when non-determinism is substantial. You simply run your evaluation multiple times, measure the metric each time, compute the mean and standard deviation, and construct a confidence interval from those statistics.

The cost of repeated measurements is computation time. If each evaluation run takes two hours, ten runs take twenty hours. This is manageable for overnight runs but may be prohibitive if you want to iterate quickly. One solution is to use repeated measurements selectively: run single evaluations during development for rapid feedback, but run ten evaluations for launch decisions. Another solution is to reduce non-determinism by lowering temperature or using more deterministic prompting, making single evaluations more reliable.

## The Multiple Comparisons Problem

When you measure many metrics or compare many system variants, you increase the chance of observing a statistically significant result purely by luck. This is the **multiple comparisons problem**. If you test twenty metrics at a five percent significance level, you expect one false positive even if nothing is truly different. If you compare ten system variants pairwise, that is forty-five comparisons, and you expect two or three false positives. These false positives look like real findings but are actually noise.

The standard correction for multiple comparisons is the Bonferroni correction: divide your significance threshold by the number of comparisons. If you are testing ten metrics and want an overall five percent false positive rate, use a zero point five percent threshold for each individual test. This makes each test much more conservative, reducing false positives but increasing false negatives. You will miss some real differences that you would have caught without correction.

An alternative is the Benjamini-Hochberg procedure, which controls the false discovery rate: the proportion of significant results that are false positives. This is less conservative than Bonferroni and often more appropriate for exploratory analysis where you want to surface promising directions even if some are false leads. The key is to apply some correction rather than ignoring the problem, which guarantees that you will be misled by spurious findings.

A recommendation system team evaluated fifteen different prompt variations for their Claude 4-based content summarizer, measuring accuracy, relevance, and engagement for each. That is forty-five metrics across fifteen variants. Without correction, they found seven metrics that showed statistically significant improvement over the baseline at the five percent level. With Bonferroni correction, only two remained significant. Those two represented real improvements worth pursuing. The other five were noise that would have wasted engineering effort if pursued.

The multiple comparisons problem is particularly severe in automated hyperparameter tuning or prompt optimization, where you may evaluate hundreds or thousands of configurations. In those cases, you need a held-out test set: use one set of data to select the best configuration, then validate it on a separate set to confirm the improvement is real. The held-out test protects against overfitting to noise in the tuning set.

Multiple comparisons also arise when you monitor many metrics in production. If you check one hundred metrics every day and alert when any of them exceeds a significance threshold, you will generate false alarms regularly. The solution is to adjust alerting thresholds to account for the number of metrics being monitored, or to require that anomalies persist across multiple days before triggering escalation.

## Power Analysis and Detecting Small Differences

**Statistical power** is the probability that you will detect a real difference when one exists. High power means you are unlikely to miss true improvements. Low power means you will often fail to detect real differences, leading to false negatives. Power depends on sample size, effect size, and significance level. Larger samples increase power. Larger effect sizes increase power. More lenient significance levels increase power but also increase false positives.

Power analysis is the process of choosing sample size to achieve a target power, typically eighty percent. This ensures that if there is a meaningful difference, you will probably detect it. Skipping power analysis and using whatever sample size is convenient leads to underpowered studies that waste resources by collecting data that is insufficient to answer the question.

The relationship between effect size and required sample size is dramatic. Detecting a ten-percentage-point difference in accuracy might require one hundred samples, while detecting a one-point difference requires ten thousand samples. This means you must decide in advance what size of improvement matters. If only large improvements are worth deploying because of implementation costs, you can use a smaller sample. If even small improvements are valuable because you operate at massive scale, you need large samples.

An advertising optimization team wanted to test whether a new GPT-5.1 prompt for generating ad copy improved click-through rate. Their baseline CTR was two point five percent. They considered a zero point one percentage-point improvement, from two point five to two point six percent, to be valuable given their volume of ten million impressions per day. Power analysis indicated they needed a sample of roughly five hundred thousand impressions per variant to detect this difference with eighty percent power. They ran an A/B test for one week, accumulating sufficient data, and detected a zero point one two point improvement that was clearly significant. The small effect size was economically meaningful because of their scale, but measuring it required a large sample.

Power analysis also helps you decide when not to run an experiment. If the sample size required to detect a meaningful difference is larger than you can practically collect, the experiment is not worth running. You will either fail to detect real improvements or waste resources collecting data that still leaves you uncertain. In those cases, you may need to either accept lower power and risk missing improvements, or focus on larger changes that are easier to detect.

## Bayesian Approaches for Small Samples

Frequentist statistical significance, based on p-values and confidence intervals, requires relatively large samples to be informative. When your sample is small, confidence intervals are wide and few comparisons reach significance. **Bayesian approaches** offer an alternative that can be more informative with limited data by incorporating prior beliefs and updating them based on evidence.

In Bayesian inference, you start with a prior distribution representing your beliefs about a metric before collecting data. You then update this prior based on observed data to produce a posterior distribution representing your updated beliefs. The posterior gives you a probability distribution over possible values, allowing statements like "there is a seventy-five percent probability that the new model is better than the baseline" rather than the more awkward frequentist statement "we reject the null hypothesis that there is no difference at the five percent significance level."

Bayesian approaches are particularly useful when you have informative priors from previous experiments or domain knowledge. If you know from past experience that prompt improvements typically yield two to five percentage point gains in accuracy, you can encode this as a prior. When you observe a three-point improvement in a new experiment, the posterior will be more concentrated around three points because it aligns with your prior. If you observe a fifteen-point improvement, the posterior will be more skeptical, requiring stronger evidence to believe such a large effect.

A content moderation team with limited evaluation data, only one hundred fifty examples, used Bayesian inference to compare two safety classifiers. Their prior, based on previous experiments, suggested that accuracy differences between well-tuned classifiers were typically between zero and five percentage points. They observed a three-point difference in their sample. The Bayesian posterior indicated an eighty percent probability that the new classifier was better, even though the frequentist p-value was only marginally significant at zero point one two. The Bayesian approach gave them more confidence to deploy because it incorporated their prior knowledge that three-point improvements were realistic.

Bayesian methods also handle sequential testing more naturally than frequentist methods. You can continuously monitor an A/B test and stop when the posterior probability exceeds a threshold, without the multiple testing penalties that frequentist methods incur. This allows more efficient experimentation when you are collecting data over time.

The challenge with Bayesian approaches is choosing appropriate priors. Uninformative priors, which encode minimal assumptions, produce results similar to frequentist methods and do not help much with small samples. Informative priors require domain expertise and can introduce bias if the prior is wrong. In practice, Bayesian methods are most useful when you have a history of similar experiments to inform your priors and when your samples are too small for frequentist methods to be decisive.

## Sample Size for Slice-Level Metrics

When you measure metrics at the population level, you can pool all your data and achieve reasonable sample sizes. When you measure metrics at the slice level, for specific user segments or query types, your effective sample size per slice is much smaller. If you have one thousand evaluation cases and ten slices, you have only one hundred cases per slice. This smaller sample makes it harder to detect differences within slices even when those differences are large and important.

Slice-level measurement requires either larger overall samples or acceptance of higher uncertainty within slices. If you want to measure accuracy for ten different query types with the same precision you achieve at the population level, you need ten times as many total cases. This multiplicative effect makes comprehensive slice-level measurement expensive.

One approach is to prioritize slices: measure most metrics at the population level and measure slice-level performance only for the most critical dimensions and most important slices. A language model serving global users might measure overall accuracy on a large sample but measure per-language accuracy only for the top five languages by volume. This focuses measurement effort where it matters most.

Another approach is to use hierarchical or shrinkage estimators that borrow strength across slices. If you observe ninety percent accuracy in English and seventy-five percent accuracy in Spanish, but the Spanish sample is small, a hierarchical model can partially shrink the Spanish estimate toward the overall mean, reducing the influence of noise. These methods require more sophisticated statistical modeling but can improve slice-level estimates when data is limited.

A legal document analysis system measured contract clause extraction accuracy across twenty contract types. Their overall evaluation set had two thousand contracts, so roughly one hundred per type. With such small per-type samples, their confidence intervals were very wide, making it hard to identify which contract types needed improvement. They collected an expanded evaluation set with five thousand contracts, raising the average per-type sample to two hundred fifty. This increased sample size tightened confidence intervals enough to clearly identify that employment contracts and licensing agreements had significantly lower accuracy than other types, directing their optimization efforts.

Slice-level sample size requirements often surprise teams. You cannot measure ten slices reliably with the same sample that measures one population reliably. Either invest in larger samples or accept that slice-level metrics will be noisier and less actionable than population metrics.

## Practical Heuristics for Sample Size

While formal sample size calculations provide precise guidance, they require statistical expertise and can be intimidating. Practical heuristics offer rough guidance that works for most cases. These are not substitutes for proper calculations when stakes are high, but they prevent egregious errors.

A useful heuristic for binary metrics like accuracy is the rule of ten: you need at least ten positive examples and ten negative examples in your sample to estimate a proportion reliably. If you are measuring a ninety percent accurate system, you need at least one hundred cases to expect ten errors. Below this threshold, your estimate will be very noisy.

Another heuristic is the minimum detectable difference rule: if you want to detect a difference of X percentage points, you need roughly four hundred divided by X squared cases per system. To detect a five-point difference, you need four hundred divided by twenty-five, roughly sixteen cases per system. To detect a two-point difference, you need four hundred divided by four, one hundred cases. This gives ballpark estimates that are often within a factor of two of formal calculations.

A third heuristic is the confidence interval width rule: if you want your ninety-five percent confidence interval to be no wider than plus or minus Y percentage points, you need roughly one divided by Y squared cases. To achieve plus or minus three percentage points, you need roughly one thousand divided by nine, about one hundred cases. To achieve plus or minus one point, you need roughly ten thousand cases.

These heuristics are simplifications that ignore variance and effect size subtleties, but they are directionally correct and easy to remember. They help you quickly assess whether your evaluation set is in the right range or an order of magnitude too small. When the heuristics suggest you need a much larger sample, it is time to do a formal calculation or accept that your measurements will be noisy.

## The Cost-Precision Tradeoff

Larger samples produce more precise measurements, but collecting and labeling data is expensive. The question is how much precision you need. For some decisions, rough estimates are sufficient. For others, precise measurements are critical. The **cost-precision tradeoff** asks you to balance measurement quality against resource constraints.

Early in development, when you are exploring many directions and making rapid decisions, low precision may be acceptable. A fifty-case evaluation that tells you a new approach is roughly similar to baseline or dramatically better is enough to guide iteration. As you approach launch, precision becomes more important because the cost of deploying a worse system is higher. At launch, you want high precision to confidently claim improvement.

One strategy is staged evaluation: start with small samples for rapid iteration, expand to medium samples for candidate selection, and use large samples for final validation. A conversational AI team might use fifty cases for daily prompt experiments, three hundred cases for weekly model comparisons, and two thousand cases for launch decisions. This tiered approach balances speed and rigor.

Another strategy is adaptive sampling: start with a small sample and expand only if results are close. If your candidate model is clearly better or clearly worse than baseline in the first one hundred cases, you do not need more data. If the results are ambiguous, expand to five hundred cases. This saves annotation effort when signals are strong while ensuring precision when signals are weak.

Cost-precision tradeoffs are context-dependent. A safety metric on a medical device requires much higher precision than a user preference metric on a social media feature because the consequences of error are asymmetric. The key is to make the tradeoff explicit rather than defaulting to whatever sample size is convenient.

## Bridging to Aggregation

You now understand that AI metrics are noisy, that distinguishing signal from noise requires adequate sample size, and that reporting confidence intervals makes uncertainty explicit. You have tools to calculate how much data you need and to avoid drawing confident conclusions from insufficient evidence. But even with large samples and narrow confidence intervals, population-level metrics can hide critical failures in specific segments. The next challenge is designing aggregation strategies that reveal slice-level and cohort-level patterns rather than averaging them into invisibility, ensuring that your metrics expose problems instead of concealing them.

