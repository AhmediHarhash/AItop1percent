# 4.11 â€” Reasoning and Chain-of-Thought Task Metrics

On September 17, 2025, a financial analysis platform deployed a new reasoning model for investment research. The system used OpenAI's o1-preview model to analyze companies and generate investment recommendations. Initial testing showed impressive results: the model correctly identified undervalued stocks in 78 percent of historical cases where human analysts had made successful calls, significantly outperforming the previous GPT-4-based system at 61 percent. The platform rolled out to 340 professional investors managing approximately $8.7 billion in assets. Within six weeks, three major clients threatened to terminate contracts. The problem was not final recommendation quality. The model's buy and sell calls were directionally correct. The problem was reasoning transparency. Investors could not trust recommendations when the reasoning chain contained logical leaps, contradictions, and unsupported assertions that human analysts would never make. The platform pulled the o1-based system in November and spent two months building evaluation frameworks that measured reasoning quality, not just answer correctness.

This failure reveals the central challenge of evaluating reasoning systems: the answer is not enough. When your AI system thinks through multi-step problems, users need to trust not just what it concludes but how it concludes. Measuring reasoning quality requires decomposing the thinking process into steps, evaluating logical validity of each step, assessing whether the model considered relevant factors, and tracking how errors propagate through reasoning chains. These are fundamentally different metrics than accuracy on final outputs.

## Step-Level Accuracy: Every Link in the Reasoning Chain

**Step-level accuracy** measures what percentage of individual reasoning steps are logically valid, factually correct, and relevant to the problem. This is distinct from answer-level accuracy, which only checks if the final output is correct. A model can reach the right answer through flawed reasoning, or produce wrong answers despite mostly valid reasoning.

A mathematical problem-solving system in late 2025 used Claude 3.5 Sonnet with extended thinking to solve competition-level math problems. On a test set of 500 problems from the American Mathematics Competitions, the system achieved 73 percent final answer accuracy. The team assumed this meant the reasoning was solid. When they manually reviewed reasoning chains, they found a different story.

Many correct answers came through reasoning chains with errors. A geometry problem might require calculating an angle, then using that angle to find a side length, then using the side length to find an area. The model would miscalculate the angle, producing 47 degrees instead of the correct 53 degrees. Then it would make a compensating error in the next step, using the wrong trigonometric identity in a way that accidentally canceled out the angle error. The final area was correct, but the reasoning was wrong twice.

Other problems had correct reasoning for most steps but failed in the final calculation. A combinatorics problem might require five reasoning steps: identifying the problem type, setting up the counting framework, handling overcounting, applying inclusion-exclusion principle, and computing the final number. The model would execute the first four steps perfectly, then make an arithmetic error in the last step, producing a wrong answer despite sound reasoning.

Measuring step-level accuracy required decomposing each problem solution into discrete reasoning steps and evaluating each step independently. The team built annotation guidelines defining what constituted a step: identifying problem structure, stating relevant theorems, setting up equations, performing calculations, and drawing intermediate conclusions. Human experts annotated 200 problem solutions with step-level correctness labels.

The analysis revealed that step-level accuracy was 89 percent while final answer accuracy was 73 percent. The model was reasoning correctly most of the time but small errors were accumulating and corrupting final answers. This insight guided improvement efforts: they focused on error checking and validation of intermediate steps rather than trying to make the model smarter overall.

## Reasoning Completeness: Did the Model Consider Everything That Matters

**Reasoning completeness** measures whether the model's reasoning process addressed all factors relevant to the problem. A model can produce correct reasoning for the factors it considers while missing critical considerations entirely.

A medical diagnosis AI in early 2026 used GPT-4o with chain-of-thought prompting to generate diagnostic assessments from patient symptoms and history. The system achieved 84 percent diagnostic accuracy on a test set of 600 cases. But physicians reviewing the reasoning chains noted concerning patterns. The model frequently ignored medication interactions, discounted family history of certain conditions, and overlooked subtle symptom combinations that human doctors would flag.

In one case, a patient presented with fatigue, weight loss, and increased thirst. The model correctly reasoned through the possibility of diabetes, checked relevant lab values, and recommended diabetes testing. But the reasoning chain never mentioned the patient's history of hyperthyroidism or the medication they were taking for it, both of which could cause identical symptoms and should have been considered in the differential diagnosis. The final recommendation happened to be correct, but the reasoning was incomplete.

Measuring completeness requires defining what factors should be considered for each problem type. The medical diagnosis team built completeness checklists for common diagnostic scenarios: patient symptoms that must be addressed, medical history elements that must be checked, differential diagnoses that should be considered, and risk factors that should be evaluated. Each reasoning chain was scored on what percentage of checklist items were addressed.

The completeness metric revealed that the model addressed an average of 71 percent of relevant factors. It was highly consistent in addressing primary symptoms and obvious risk factors but frequently missed secondary considerations, drug interactions, and less-common differential diagnoses. This pattern was invisible when measuring only diagnostic accuracy because in most cases the missing factors did not change the correct diagnosis. But in edge cases, incomplete reasoning led to missed diagnoses.

The team improved completeness by adding structured prompting that explicitly asked the model to consider specific factor categories before reaching conclusions. They added verification steps checking whether the reasoning addressed items from the completeness checklist. These interventions increased completeness to 87 percent and improved diagnostic accuracy in edge cases from 68 percent to 79 percent.

## Error Propagation: How One Wrong Step Corrupts Everything Downstream

**Error propagation** measures how badly downstream reasoning is affected when an upstream step contains an error. In some reasoning chains, early errors have minimal impact. In others, one wrong step makes everything after it invalid.

A legal reasoning system in mid-2025 analyzed contracts to identify potential risks and compliance issues. The system used a multi-step reasoning process: extract key terms, identify relevant regulations, check each term against regulatory requirements, assess risk level, and recommend modifications. When the team measured step-level accuracy, they found 91 percent of steps were correct. But only 78 percent of final risk assessments were correct, a larger gap than expected from random errors.

The problem was error propagation. When the extraction step misidentified a key term, the entire downstream analysis became invalid. The model would reason correctly about regulatory compliance for the wrong term, producing logically valid but factually irrelevant analysis. When the regulation identification step missed a relevant law, all subsequent compliance checking ignored that regulatory dimension, producing incomplete risk assessment.

Measuring error propagation requires injecting errors at different positions in reasoning chains and measuring impact on downstream steps. The legal reasoning team built a test framework that took correct reasoning chains and artificially corrupted specific steps, then evaluated how many downstream steps remained valid.

They discovered that extraction errors corrupted an average of 4.2 downstream steps, making them irrelevant or incorrect. Regulation identification errors corrupted 2.8 downstream steps. Compliance checking errors typically corrupted only the final risk assessment, about 1.1 steps on average. Early-stage errors had disproportionate impact compared to late-stage errors.

This analysis revealed that improving extraction accuracy would have much higher ROI than improving compliance checking accuracy. A 5 percent improvement in extraction would prevent errors from propagating into approximately 0.21 downstream steps per case, while a 5 percent improvement in compliance checking would only improve the final step. The team redirected engineering resources to focus on early-stage reasoning quality.

## Faithfulness: Does the Visible Reasoning Reflect Actual Computation

**Faithfulness metrics** measure whether the chain-of-thought reasoning the model presents actually reflects its internal computation process. This is critical for reasoning models because users trust reasoning chains as explanations of how the model reached its conclusions. If the visible reasoning is a post-hoc rationalization rather than the actual path to the answer, that trust is misplaced.

A scientific research assistant in late 2025 used Claude 3.5 Sonnet with extended thinking to answer complex questions about published papers. The system would display reasoning chains showing how it connected information from multiple papers to answer questions. Users trusted these reasoning chains as accurate representations of the model's thought process.

A research team studying the assistant's behavior ran experiments to test faithfulness. They gave the model questions where the reasoning chain suggested specific papers were critical to the answer. Then they reran the same questions with those supposedly critical papers removed from the context. If the reasoning chain was faithful, removing papers the model claimed were essential should degrade answer quality. In 34 percent of cases, removing supposedly critical papers had no impact on answer quality. The model had cited papers in its reasoning chain that were not actually influencing its conclusions.

They also tested counterfactual faithfulness by inserting incorrect information into papers the reasoning chain cited. If the model was actually reasoning through the cited papers, inserting errors should cause wrong answers. In 28 percent of cases, inserting factual errors into cited papers did not change the model's answers. The model was generating reasoning chains that looked like they depended on specific papers when the actual computation was relying on something else, likely knowledge from training.

Measuring faithfulness is difficult because you cannot directly observe internal model computation. You can only run experiments that test whether behavior matches what the reasoning chain implies. The research assistant team developed three faithfulness tests: information removal, where supposedly critical inputs are removed and impact is measured; counterfactual editing, where inputs are corrupted and sensitivity is measured; and reasoning chain manipulation, where the model is asked to reason differently and changes in conclusions are measured.

These tests revealed that faithfulness varied by question type. When questions required synthesizing information across papers, reasoning chains were highly faithful with 89 percent of cited papers actually influencing conclusions. When questions could be answered from the model's training knowledge, reasoning chains were less faithful at only 64 percent, with the model generating plausible-looking citations that were not actually used in reaching conclusions.

## Comparing Reasoning Model Families: o1 Versus Extended Thinking Versus Gemini Thinking

Different reasoning model families use different architectures and training approaches. OpenAI's o1 and o3 use reinforcement learning to optimize reasoning chains. Claude's extended thinking uses longer context and more inference-time computation. Gemini's thinking mode uses specialized prompting. These architectural differences create different reasoning characteristics that require different evaluation approaches.

A research evaluation platform in early 2026 compared reasoning quality across model families on a benchmark of 400 complex reasoning tasks spanning math, science, logic, and analysis. They measured not just accuracy but also reasoning chain length, step-level correctness, completeness, error propagation, and faithfulness.

OpenAI's o1 produced longer reasoning chains with an average of 18 steps per problem versus 12 for Claude extended thinking and 10 for Gemini thinking. But o1's step-level accuracy was 93 percent versus 89 percent for Claude and 85 percent for Gemini. The longer chains were actually more reliable per step. Claude's shorter chains achieved similar final accuracy through higher completeness, addressing relevant factors more comprehensively in fewer steps. Gemini's chains were shortest and least accurate but completed fastest, averaging 4.2 seconds versus 11.7 for o1 and 8.3 for Claude.

Error propagation patterns differed across families. In o1, early-stage errors propagated through an average of 5.8 downstream steps because of the longer chains. In Claude, early errors propagated through 3.4 steps. In Gemini, propagation was only 2.1 steps due to shorter chains. This meant o1 had the highest variance in quality: when it reasoned correctly from the start, it reached very strong conclusions, but early errors caused spectacular failures.

Faithfulness was highest in o1 at 87 percent, likely because the RL training optimized for reasoning chains that actually produced correct answers. Claude showed 81 percent faithfulness. Gemini was lowest at 72 percent, suggesting its thinking mode reasoning chains were more post-hoc rationalization than genuine computation paths.

These comparative metrics revealed that no single model family dominated across all reasoning dimensions. o1 offered the most reliable step-by-step reasoning but was slowest and most sensitive to early errors. Claude balanced reasoning quality with speed and robustness. Gemini was fastest but least reliable. Choosing which model to use required understanding which reasoning characteristics mattered most for specific use cases.

## Reasoning Efficiency: Quality Per Token of Reasoning

Reasoning models consume more tokens than direct-answer models because they generate intermediate thinking steps. **Reasoning efficiency** measures the quality you get per token of reasoning, helping you understand whether the additional cost is justified.

A coding assistant in mid-2025 compared GPT-4o in direct mode versus o1 in reasoning mode for code generation tasks. Direct GPT-4o generated answers averaging 320 tokens per response with 79 percent correctness. O1 generated reasoning chains averaging 1,840 tokens before the final code, with total output of approximately 2,180 tokens and 88 percent correctness. The 9 percentage point quality improvement cost 6.8 times as many tokens.

Was this efficient? The team calculated quality per thousand tokens. Direct GPT-4o achieved 246 correct solutions per thousand tokens (79 percent divided by 0.32). O1 achieved 40 correct solutions per thousand tokens (88 percent divided by 2.18). Direct GPT-4o was approximately 6 times more token-efficient despite lower raw accuracy.

But this analysis missed cost-of-error. When GPT-4o generated wrong code, developers spent time debugging and fixing it. When o1 generated wrong code, developers could often spot errors in the reasoning chain before running the code, saving debugging time. The team measured developer time per task: 8.2 minutes with GPT-4o including debugging time versus 6.7 minutes with o1 including reasoning review time. From a human-time perspective, o1 was more efficient even though it was token-inefficient.

Reasoning efficiency metrics should account for the full cost model: token cost for generation, human time cost for review and debugging, and task success value. Different use cases have different cost structures. For high-volume automated tasks where human review is minimal, token efficiency dominates. For tasks where human time is expensive and errors are costly, reasoning quality per token matters less than end-to-end human efficiency.

## Self-Consistency in Reasoning: Does the Model Reach the Same Conclusion Multiple Times

**Self-consistency** measures whether a model produces the same reasoning and conclusions when asked the same question multiple times. High self-consistency suggests the model has a stable internal understanding. Low self-consistency suggests the model is guessing or highly sensitive to sampling randomness.

A scientific reasoning system in early 2026 used Gemini 2.0 with thinking mode to answer research questions. The team noticed that running the same question multiple times sometimes produced different answers. They ran a systematic test: 200 questions, each posed 5 times with identical inputs, measuring how often the model reached the same final answer and how similar the reasoning chains were.

For 73 percent of questions, the model reached the same answer all 5 times, showing high self-consistency. For 19 percent, it reached the same answer 4 out of 5 times. For 8 percent, answers varied across multiple attempts. The questions with low self-consistency were not randomly distributed. They clustered in specific categories: questions requiring subtle distinctions, questions where evidence was ambiguous, and questions at the edge of the model's knowledge.

The team then measured reasoning chain similarity using a semantic similarity metric. Even when final answers were consistent, reasoning chains varied substantially. On questions where the model answered identically 5 out of 5 times, reasoning chains had an average semantic similarity of 0.68 out of 1.0. The model was reaching the same conclusion through different reasoning paths.

This raised a question: is variation in reasoning chains a problem if answers are consistent? The team investigated cases where reasoning varied but answers matched. In some cases, the variation represented genuine flexibility, with the model reasoning through different valid approaches. In other cases, variation revealed shallow understanding, with the model reaching correct answers through different flawed reasoning that happened to land on the same conclusion.

Measuring self-consistency requires multiple samples per test case, which multiplies evaluation cost. The team used a tiered approach: first test each question once to measure raw accuracy, then sample 5 times for questions where accuracy was marginal or particularly important. This concentrated measurement resources on cases where self-consistency information was most valuable.

## Reasoning Depth: Simple Logic Versus Deep Analysis

Some reasoning tasks require only a few logical steps. Others demand deep analysis connecting many pieces of information. **Reasoning depth** measures how many inferential steps the model successfully chains together before quality degrades.

A business strategy AI in mid-2025 generated strategic recommendations for companies based on market data, competitive analysis, and company information. Simple recommendations might require 3 to 4 reasoning steps: identify market trend, assess company position, note competitive gap, recommend action. Complex recommendations might require 10 to 15 steps: analyze multiple market trends, assess interactions between trends, evaluate company strengths across dimensions, model competitive responses, consider implementation constraints, sequence actions over time.

The team measured reasoning quality as a function of required depth. They built a test set with problems requiring different numbers of inferential steps, from 2-step simple analyses to 15-step complex strategic reasoning. They measured at what depth reasoning quality degraded.

The model maintained 91 percent quality on problems requiring 2 to 4 steps. Quality held at 87 percent for 5 to 7 steps. It dropped to 78 percent for 8 to 10 steps and fell to 63 percent for 11 to 15 steps. The degradation curve revealed that the model could reliably handle moderate reasoning depth but struggled with deep multi-step analysis.

Interestingly, adding explicit step-by-step prompting did not improve deep reasoning quality. The model would generate more reasoning tokens and break analysis into smaller steps, but quality still degraded at similar total inferential depth. The bottleneck was not lack of intermediate steps but ability to maintain coherence across many inferential leaps.

This depth limitation guided product scoping. The team designed features to work within the 8-step reasoning depth where quality remained acceptable. For problems requiring deeper analysis, they broke tasks into multiple rounds where each round handled 6 to 8 steps and a human reviewed intermediate outputs before proceeding. This hybrid approach maintained quality for complex problems while acknowledging the model's depth limitations.

## Measuring Reasoning Under Uncertainty: How Models Handle Ambiguity

Real-world reasoning often involves incomplete information, ambiguous evidence, and uncertain conclusions. **Uncertainty handling metrics** measure how well models reason when definitive answers are not available.

A diagnostic reasoning system in late 2025 analyzed business problems and recommended solutions. Many problems involved ambiguous evidence: customer complaints might indicate product quality issues or support training gaps or market positioning problems. The model needed to reason through multiple hypotheses, assess which had stronger support, and acknowledge remaining uncertainty.

The team built a test set of 300 ambiguous problems where evidence supported multiple interpretations. Each problem was annotated with what hypotheses should be considered, what evidence supported each, and appropriate confidence levels for conclusions. They evaluated whether the model's reasoning appropriately handled uncertainty.

They found the model had a strong bias toward certainty. In 67 percent of ambiguous cases, the model presented conclusions with high confidence despite significant evidence for alternative explanations. The reasoning chains would mention alternative hypotheses but then dismiss them with insufficient justification, converging too quickly on a single interpretation.

The team developed uncertainty handling metrics measuring whether models acknowledged ambiguity, considered multiple hypotheses, weighed evidence appropriately, and expressed confidence calibrated to actual evidence strength. These metrics revealed that reasoning quality in the presence of uncertainty was substantially lower than reasoning quality for clear-cut problems, 71 percent versus 86 percent, even though raw accuracy was similar.

Improving uncertainty handling required both prompting changes to encourage hypothesis exploration and evaluation changes to reward appropriate caution rather than penalizing hedging. The team found that explicitly asking models to consider alternative explanations and rate confidence in each improved uncertainty handling from 71 percent to 79 percent.

## Reasoning Robustness: Sensitivity to Problem Phrasing and Context

Small changes in how a problem is phrased should not dramatically change reasoning quality if the model genuinely understands the underlying logic. **Reasoning robustness** measures how sensitive reasoning is to problem formulation.

A mathematical reasoning system in early 2026 solved algebra and calculus problems. The team noticed that performance varied depending on how problems were stated. A question about projectile motion might achieve 84 percent accuracy when stated in standard textbook form, 79 percent when stated as a word problem with irrelevant details, and 72 percent when stated with variable names different from convention.

They built a robustness test set with problems stated in multiple equivalent forms: standard form, verbose form with extra context, minimal form with sparse wording, and unconventional form with unusual notation or framing. Each version of the problem had identical mathematical content but different surface features.

Reasoning robustness was measured as the standard deviation in accuracy across problem variants. A perfectly robust model would show zero standard deviation, solving all variants with equal reliability. The actual model showed average standard deviation of 11 percentage points, meaning typical problems had accuracy spread of plus or minus 11 percent depending on phrasing.

The highest sensitivity was to notation changes. Using unconventional variable names decreased accuracy by an average of 14 percentage points, suggesting the model relied heavily on pattern matching to standard forms rather than genuine mathematical understanding. Verbose wording with irrelevant details decreased accuracy by 8 percentage points, indicating distraction from excess context. Minimal sparse wording decreased accuracy by only 4 percentage points.

These robustness metrics revealed brittleness in reasoning that was invisible when testing only on standard problem formulations. The team used robustness testing to identify areas where the model was pattern-matching rather than reasoning, then improved training and prompting to increase genuine understanding versus surface-level pattern recognition.

## The Metrics You Need to Trust Reasoning Systems

Evaluating reasoning systems requires metrics that go beyond answer accuracy. You need step-level accuracy measuring logical validity of each reasoning step. You need completeness metrics measuring whether all relevant factors are considered. You need error propagation analysis understanding how mistakes compound through reasoning chains. You need faithfulness metrics testing whether visible reasoning reflects actual computation. You need comparative metrics understanding different reasoning architectures. You need efficiency metrics measuring quality per token of reasoning. You need self-consistency metrics checking stability of reasoning. You need depth metrics measuring how many inferential steps maintain quality. You need uncertainty handling metrics evaluating reasoning under ambiguity. You need robustness metrics testing sensitivity to problem formulation.

Build these metrics into your evaluation framework before deploying reasoning systems. Instrument your systems to capture intermediate reasoning steps, not just final outputs. Create test sets specifically designed to stress reasoning capabilities across these dimensions. Measure reasoning quality separately from answer accuracy, because users who need to trust your system's conclusions need to trust its thinking process.

The next challenge is evaluating AI systems that have been fine-tuned for specific tasks, where you must measure not just specialized performance but also what general capabilities were sacrificed to achieve that specialization.

