# 5.11 â€” Quality SLOs and Error Budgets

In June 2024, a B2B legal research AI company maintained a Service Level Objective of 99.9 percent uptime for their platform, the industry standard for enterprise SaaS. They monitored infrastructure health obsessively, maintained redundant systems, and paged on-call engineers at 3am whenever API response times exceeded two seconds. Over eighteen months, they achieved 99.95 percent uptime, exceeding their SLO and earning praise from their infrastructure team. During that same period, the quality of their case law citations degraded from ninety-four percent accuracy to eighty-seven percent as legal precedents evolved and their training data aged. No one noticed until a law firm using the platform lost a motion because an AI-cited case had been overturned three months earlier. The law firm sued for malpractice, seeking $1.2 million in damages. The legal research company settled for $480,000 and lost the customer relationship worth approximately $180,000 in annual recurring revenue. In their post-mortem, the CTO wrote: "We measured the wrong thing. Our platform was always available, but it was reliably delivering wrong answers."

The failure was treating uptime as the only measure of service quality while ignoring the accuracy and freshness of the AI outputs users actually cared about. Traditional SLOs focus on availability, latency, and error rates because those metrics are straightforward to measure and relevant for deterministic software where correct behavior is guaranteed by code. For AI systems, uptime is necessary but not sufficient: a system that is available one hundred percent of the time but hallucinates fifteen percent of the time is useless. You need **quality SLOs** that measure the dimensions of AI behavior that determine whether the system is actually delivering value, and you need error budgets that force explicit tradeoffs between shipping new features and maintaining quality.

## Defining Quality SLOs for AI Systems

A **Service Level Objective** is a target value or range for a service metric that represents the minimum acceptable quality your users should experience. Traditional SLOs are typically framed as "99.9 percent of requests will complete successfully in under 200ms" or "the service will be available 99.95 percent of the time." Quality SLOs for AI systems extend this framework to behavioral metrics: "95 percent of chatbot responses will be rated relevant by users," "hallucination rate will be below 2 percent," "worst-slice accuracy across demographic groups will exceed 90 percent." The SLO is not an aspirational goal but a threshold that defines the boundary between acceptable and unacceptable service quality.

Quality SLOs must be **measurable**, meaning you have automated systems that continuously collect the metrics and calculate whether you are meeting the objective. A SLO like "responses will be helpful" is not measurable unless you define helpfulness operationally and instrument your system to compute it. A SLO like "92 percent of responses will receive a helpfulness score above 3 out of 5 from our LLM-as-judge rater" is measurable because you can automatically evaluate every response and calculate the percentage meeting the threshold. If you cannot measure it in production, it cannot be a SLO.

Quality SLOs must be **achievable**, meaning your system can reliably meet the target under normal operating conditions with reasonable engineering investment. A SLO like "zero hallucinations" is not achievable with current language models and would be violated constantly, making it useless as a threshold for decision-making. A SLO like "hallucination rate below 1 percent" might be achievable with a combination of retrieval-augmented generation, citation requirements, and verification steps. The SLO should represent the quality level you commit to maintaining, not the quality level you wish you could achieve.

Quality SLOs must be **meaningful**, meaning that violating them indicates a real problem that affects users or business outcomes. A SLO like "average response length above 100 characters" is measurable and achievable but not meaningful because response length does not correlate with user value unless you have evidence that very short responses are unhelpful. A SLO like "90 percent of customer support queries will be resolved without human escalation" is meaningful because it directly measures the value the AI delivers by reducing support costs. Every SLO you define should have a clear answer to the question "why does this threshold matter to users or the business?"

## The Error Budget Framework

An **error budget** is the inverse of your SLO, representing how much quality degradation you can tolerate before the service is considered to be in violation. If your SLO is ninety-five percent accuracy, your error budget is five percent. The error budget is the amount of room you have for failures, experiments, and rapid iteration before you need to stop feature work and focus on quality improvements. The error budget framework creates a feedback loop where quality problems automatically trigger shifts in team priorities, preventing the gradual erosion of quality that occurs when teams are always focused on shipping features and never on maintaining fundamentals.

The error budget is consumed by any quality degradation below your SLO target. If your hallucination rate SLO is one percent and you observe a 1.3 percent hallucination rate for a week, you have consumed thirty percent of your error budget for that week. If you burn through your entire error budget for a month, the framework dictates that you halt feature development and dedicate resources to quality improvements until the metrics return to within SLO or until you explicitly decide to raise the error budget by relaxing your SLO. This policy ensures that quality problems do not accumulate indefinitely and that teams are forced to balance feature velocity with quality maintenance.

The error budget also enables **controlled risk-taking** by making explicit how much quality degradation you can accept in exchange for shipping faster or experimenting with higher-risk changes. If your error budget for the quarter is ninety-five percent consumed, you cannot afford any risky releases that might degrade quality further. If your error budget is only twenty percent consumed, you have room to experiment with a new model version or prompt technique that might improve quality but also might make it worse. The error budget quantifies how much safety margin you have before triggering a quality crisis, letting you calibrate your risk appetite based on your current quality trajectory.

Error budgets should be calculated over **multiple time windows** to distinguish between transient spikes and sustained degradation. A one-hour violation of your hallucination SLO might be a temporary glitch that burns one percent of your daily error budget but does not indicate a systemic problem. A violation that persists for eight hours burns thirty percent of your daily budget and is more concerning. A violation that persists for three days consumes your entire weekly budget and should trigger an incident response. A violation that persists for two weeks consumes half your monthly budget and might trigger a feature freeze until quality is restored. Multi-window error budgets prevent you from overreacting to noise while ensuring you respond to sustained quality problems.

## Designing AI-Specific SLOs

AI systems require SLOs that cover multiple quality dimensions because AI failures are diverse and cannot be captured by a single metric. A comprehensive quality SLO framework for a chatbot might include **accuracy SLOs** like "93 percent of factual questions will be answered correctly," **safety SLOs** like "99.8 percent of responses will contain no harmful content," **relevance SLOs** like "90 percent of responses will be rated relevant by users," **hallucination SLOs** like "1.2 percent or fewer responses will contain unsupported factual claims," and **equity SLOs** like "worst-slice accuracy across demographic groups will be within 5 percentage points of best-slice accuracy." Each SLO covers a different failure mode and has an associated error budget that can be burned independently.

Latency and cost should also have SLOs because they affect user experience and business viability even when quality is high. A **latency SLO** might specify "95 percent of queries will receive a response in under 2 seconds" and a **cost SLO** might specify "average cost per query will remain below $0.15." These SLOs ensure that quality improvements do not come at the expense of unacceptable performance degradation or cost increases. A change that improves accuracy by two percentage points but increases p95 latency from 1.8 seconds to 3.2 seconds violates the latency SLO and should be rejected even though it improves quality, because the latency degradation will harm user experience more than the accuracy improvement helps.

You should have both **aggregate SLOs** that measure overall system quality and **slice SLOs** that measure quality for specific subpopulations, use cases, or input types. An aggregate accuracy SLO of ninety-three percent might be met while accuracy for medical queries is only eighty-five percent, indicating a systematic weakness that the aggregate obscures. A slice SLO like "accuracy on medical queries will exceed 90 percent" ensures that no use case falls too far below the aggregate standard. Slice SLOs prevent you from optimizing average quality at the expense of worst-case quality, which is where users experience the most harm.

The number of SLOs should be large enough to cover all critical quality dimensions but small enough that teams can internalize them and make decisions using them. A reasonable number is between five and fifteen SLOs depending on the complexity of your system. Too few SLOs and you miss important quality dimensions. Too many SLOs and they become a bureaucratic checklist that no one pays attention to. The SLOs should be reviewed quarterly and adjusted as your product evolves, your user base grows, and your understanding of what quality means improves.

## SLO-Driven Prioritization

The primary value of SLOs and error budgets is that they create a **forcing function for prioritization** between shipping features and fixing quality. Without SLOs, the decision to work on quality is subjective and often loses to the tangible appeal of shipping a new feature that will be visible to users and stakeholders. With SLOs, the decision is objective: if you are burning error budget faster than your replenishment rate, you must work on quality regardless of what features are waiting to ship.

A typical SLO-driven prioritization policy specifies that when error budget burn rate exceeds one hundred percent of the replenishment rate, the team shifts from normal feature work to a **quality sprint** where at least fifty percent of engineering capacity is dedicated to improving the metrics that are out of SLO. If burn rate exceeds two hundred percent, indicating that quality is degrading rapidly, the team shifts to a **feature freeze** where one hundred percent of engineering capacity is dedicated to quality until metrics return to within SLO. These thresholds can be adjusted based on your risk tolerance, but the policy should be defined in advance and followed mechanically when error budget conditions are met.

This prioritization framework protects quality from being continuously deprioritized in favor of features. Product managers cannot argue that "we just need to ship this one more feature and then we will work on quality" when the error budget is exhausted, because the policy does not allow for one more feature until quality is restored. Engineering leaders cannot be pressured to sacrifice quality for deadlines because the policy makes quality a prerequisite for feature work, not an optional activity that happens if time permits. The SLO framework transforms quality from a subjective preference into an objective constraint that shapes roadmap decisions.

SLO-driven prioritization also aligns engineering and product incentives. Product teams often resist quality work because it is not visible to users and does not generate immediate business value, while engineering teams want to work on quality because it makes the system easier to maintain and reduces stress from production incidents. SLOs create a shared framework where both teams agree that quality is required to stay within SLO, and feature work is allowed when quality is within SLO. This alignment eliminates the recurring conflict over whether to ship features or fix quality by making the decision algorithmic based on measurable criteria.

## Communicating SLOs to Stakeholders

SLOs are most effective when they are visible to the entire organization and used as a communication tool to align expectations between engineering, product, sales, and executive teams. A public SLO dashboard that shows current metric values, SLO thresholds, error budget remaining, and historical trends makes quality status transparent and provides a shared reference point for conversations about whether the system is meeting standards. When sales asks if they can demo a new feature to a customer, engineering can point to the dashboard showing that hallucination rate is currently 1.4 percent against a 1.0 percent SLO and error budget is ninety percent consumed, indicating that now is not the time to take risks on demos.

The SLO dashboard should highlight **error budget burn rate** in addition to current metric values because burn rate is a leading indicator of problems while current metrics are lagging indicators. If your hallucination rate is currently 0.9 percent, within your 1.0 percent SLO, but burn rate over the past three days is two hundred percent, that indicates quality is deteriorating rapidly and you will be out of SLO within a few days unless something changes. Burn rate helps teams take proactive action before violating SLOs rather than reactive action after the violation has occurred.

You should also communicate SLOs to customers, at least in aggregate form, to set expectations about the quality they should expect and to build trust that you are monitoring and maintaining quality. A customer-facing SLO might state "Our AI maintains 95 percent accuracy on factual questions as measured by monthly audits, and we publish our performance against this standard on our status page." This transparency signals that you take quality seriously and gives customers a basis for holding you accountable if quality degrades. Some customers, particularly enterprise buyers, will require SLOs as part of their contracts, and having pre-existing SLOs with historical performance data makes those negotiations easier.

Internal communication about SLOs should include not just whether metrics are in compliance but also **why SLOs are set at their current levels**. Engineers joining the team should understand that the ninety-three percent accuracy SLO is set at that level because historical data shows that accuracy below ninety-three percent correlates with elevated churn, not because ninety-three is a round number someone picked arbitrarily. Product managers should understand that the 1.0 percent hallucination SLO is set at that level because hallucinations above that rate create legal and reputational risk that exceeds the business value of shipping features faster. When everyone understands the rationale behind SLOs, they are more likely to respect them and less likely to request exceptions.

## Adjusting SLOs Over Time

SLOs should not be static but should evolve as your product matures, your user base grows, and your risk tolerance changes. An early-stage product with a small user base might set a hallucination SLO of 2.5 percent because the error budget is large relative to the scale of harm that failures cause. As the product scales to millions of users, a 2.5 percent hallucination rate might cause thousands of failures per day, creating unacceptable support costs and reputation risk. The SLO should be progressively tightened to 1.5 percent, then 1.0 percent, then 0.5 percent as the scale of impact justifies the increasing engineering investment required to achieve higher quality.

SLOs should be reviewed and potentially adjusted **quarterly** based on three inputs: **historical performance data** showing what quality levels you have actually achieved, **user feedback** showing what quality levels users expect and tolerate, and **business context** showing what quality levels the business can afford to maintain. If your accuracy SLO is ninety-three percent but you have consistently achieved ninety-six percent for six months with little effort, the SLO should be raised to ninety-five percent to reflect the quality level you have proven you can sustain. If your hallucination SLO is 1.0 percent but user complaints spike whenever hallucination rate exceeds 0.8 percent, the SLO should be tightened to 0.8 percent to align with user expectations.

SLO adjustments should be treated with the same rigor as other technical decisions, requiring written proposals that document the current SLO, the proposed new SLO, the rationale for the change, the estimated engineering cost of maintaining the new SLO, and the expected benefit in terms of user experience or business outcomes. A proposal to tighten the accuracy SLO from ninety-three to ninety-five percent should quantify how much engineering time will be required to achieve and maintain the higher accuracy, how much the tighter SLO will reduce churn or increase willingness to pay, and whether the return on investment is positive. This discipline prevents SLOs from being set arbitrarily or adjusted based on wishful thinking rather than realistic assessment of capabilities and trade-offs.

You should resist pressure to relax SLOs to make them easier to meet unless the current SLO is demonstrably too strict based on user feedback and business outcomes. Relaxing SLOs is often proposed when teams are struggling to maintain quality and want to redefine the problem away rather than solve it. If your hallucination SLO is 1.0 percent and you are consistently at 1.3 percent, the solution is to improve hallucination prevention, not to raise the SLO to 1.5 percent and pretend the problem is solved. The only legitimate reasons to relax a SLO are that user feedback indicates the current level is better than users expect, that the business context has changed to make the current level unaffordable, or that the SLO was initially set too aggressively based on incomplete understanding of what is achievable.

## Error Budgets for Experimentation

Error budgets enable controlled experimentation by quantifying how much quality risk you can accept in exchange for learning. A team with ninety percent of their error budget remaining can afford to run experiments that have a ten to twenty percent chance of degrading quality, because even if the experiment fails they will still be within SLO. A team with only five percent of their error budget remaining cannot afford any experiments that risk quality degradation and should focus on safe, incremental improvements until the error budget replenishes.

You should allocate error budget explicitly to planned experiments in advance, similar to how you allocate financial budgets to projects. A quarterly planning process might allocate twenty percent of the error budget to testing a new model version, fifteen percent to experimenting with prompt techniques, ten percent to a canary deployment of a risky feature, and hold the remaining fifty-five percent in reserve for unplanned issues. This allocation ensures that experimentation is bounded and that teams do not consume error budget reactively through unplanned quality degradations that leave no room for intentional risk-taking.

When an experiment consumes error budget, that consumption should be tracked and attributed to the experiment so you can calculate return on investment. An experiment that consumed five percent of your quarterly error budget and resulted in a three percentage point improvement in accuracy has a positive return because the long-term quality improvement more than compensates for the temporary quality degradation during the experiment. An experiment that consumed five percent of error budget and resulted in no improvement has a negative return and indicates that the experimental approach was not valuable. Tracking error budget consumption by experiment helps you learn which types of experiments are worth their risk and which consistently waste error budget without yielding improvements.

## Integrating SLOs with Incident Response

SLO violations should automatically trigger incident response processes that mobilize the team to diagnose and fix the problem. A violation is not just a number on a dashboard but a signal that users are experiencing degraded quality and that the system is not meeting its commitments. The incident response process should follow the same structure as reliability incidents: declare an incident, assign an incident commander, assemble a response team, diagnose the root cause, implement a fix, validate the fix restored quality, and conduct a post-mortem to prevent recurrence.

The severity level of a quality incident should be determined by how far out of SLO the metric is and how quickly error budget is being consumed. A hallucination rate of 1.1 percent against a 1.0 percent SLO is a **P2 incident** that requires response within a few hours but not immediate paging. A hallucination rate of 2.0 percent that has been sustained for six hours is a **P1 incident** that requires immediate response, paging on-call engineers, and potentially rolling back recent changes. The severity thresholds should be defined in advance as part of your SLO documentation so that incident response is triggered automatically based on metrics rather than requiring human judgment about whether a problem is severe enough to warrant response.

Post-mortems for quality incidents should focus on why the SLO violation occurred, why existing safeguards failed to prevent it, and what additional safeguards or process changes would prevent recurrence. If a hallucination rate spike was caused by a prompt change that was not adequately evaluated before deployment, the post-mortem action item might be to add hallucination testing to the release gate. If a quality degradation was caused by training data drift that was not detected until production, the action item might be to implement automated data quality monitoring that alerts before drift causes quality problems. The goal is to continuously improve your quality infrastructure and processes so that each incident leads to systemic improvements that reduce the probability of similar incidents in the future.

With quality SLOs and error budgets providing a framework for maintaining quality in production, the final business metrics challenge is understanding the unit economics of your AI system: the cost to serve each task, the margin on each interaction, and which users or use cases are profitable versus subsidized.
