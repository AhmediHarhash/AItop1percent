# 3.1 â€” Starting From Failure Modes, Not Features

On March 14, 2025, a financial services startup with forty-seven employees discovered that their AI-powered loan assessment assistant had been confidently approving microloans based on fabricated credit histories for eleven consecutive days. The model had processed 2,847 applications during this period, generating $4.2 million in questionable approvals before a human loan officer noticed that an applicant's stated employment history included a company that had ceased operations two years prior. The engineering team had implemented a comprehensive metrics dashboard tracking response latency, user satisfaction scores, approval rates by demographic category, and completion percentages. Every metric showed green. The system was fast, users rated it highly, approval rates stayed within expected demographic distributions, and 98.7% of sessions completed successfully. Not a single metric detected that the model was inventing facts to fill gaps in incomplete credit files.

The post-incident analysis revealed that the team had designed their metrics around the features they had built rather than the failures they needed to prevent. They tracked whether the system completed loan assessments quickly because they had optimized for speed. They measured user satisfaction because they had invested in interface design. They monitored demographic approval rates because they had implemented fairness constraints. But they had never explicitly listed what could go wrong with an AI system making consequential financial decisions, and therefore never designed metrics to detect those specific failure modes. The fabrication problem had been theoretically understood during development, mentioned in passing during architecture reviews, but never translated into a measurable property with thresholds and monitoring. The gap between theoretical risk awareness and operational measurement cost them $4.2 million in loan writedowns and eight months of regulatory scrutiny.

## The Backward Metrics Problem

You design metrics backward when you start with what you built and ask how to measure it. This approach feels natural because your features are concrete and present while potential failures are abstract and future. You implemented a retrieval-augmented generation pipeline, so you measure retrieval precision and generation fluency. You built a multi-turn dialogue system, so you track conversation length and turn-taking smoothness. You added a fact-checking component, so you count how often it triggers. Each metric corresponds to an engineering investment, creating a comforting sense that you are measuring what matters because you are measuring what you built. But this backward approach systematically misses the gap between what your system does and what it should never do.

The forward approach inverts this logic by starting with an exhaustive list of failure modes before writing any measurement code. You begin by asking what could go wrong in the specific deployment context where your system will operate. Not generic AI risks, but concrete failures tied to your domain, your users, and your consequences. For a medical triage assistant, the critical failure mode might be dismissing urgent symptoms as minor concerns. For a legal research tool, it might be citing overturned precedents as current law. For a customer service agent, it might be making unauthorized commitments that create contractual obligations. Each of these failures requires different measurement approaches, and none of them naturally emerge from measuring the features you happened to build.

This inversion creates immediate clarity about what truly matters. When you list that your financial assistant must never fabricate credit history details, you immediately recognize that response latency and user satisfaction scores, while useful, do not directly address this critical constraint. You need a metric that specifically detects invented facts, which might involve checking that every claim about employment history, payment records, or account balances traces to an actual data source. This metric would likely slow down your system and complicate your architecture, which explains why feature-first metric design tends to avoid it. Measuring fabrication is harder than measuring speed, and when you start with speed, fabrication never makes it onto the dashboard.

## Mining Failure Modes From Section 2

Your problem framing work from Section 2 already contains most of the failure modes you need to measure, but teams rarely complete this connection. The process of defining success criteria, identifying edge cases, and mapping stakeholder concerns surfaces dozens of specific ways your system could fail. A team building a content moderation assistant might have identified during problem framing that their system needs to handle cultural context, distinguish satire from genuine hate speech, and avoid over-censoring political discourse. Each of these concerns represents a failure mode: misunderstanding cultural references, flagging satire incorrectly, or silencing legitimate political expression. Yet most teams finish problem framing, start building, and design metrics around model accuracy and processing throughput without explicitly connecting measurement back to the failures they documented.

The connection requires a translation step where you convert each concern from problem framing into a measurable property and a detection method. If you identified that missing cultural context could cause your content moderator to flag innocuous references as violations, you need a metric that measures false positive rates specifically on culturally ambiguous content. This might require building a test set of known-safe content that carries cultural markers, or implementing a human review loop that tracks whether moderators overturn the AI's decisions more frequently on certain content types. The metric becomes concrete only when you specify exactly what you would measure, how you would collect the data, and what threshold would trigger concern.

This translation often reveals that you identified more failure modes during problem framing than you can feasibly measure in production. A team might list thirty distinct ways their system could fail but have resources to actively monitor only eight metrics with proper alerting and response procedures. This constraint forces prioritization based on failure severity and likelihood rather than measurement convenience. The fabrication risk in the loan assessment system was high severity and moderate likelihood, clearly more important than response latency or interface satisfaction, but it was deprioritized because it was harder to measure. Your metric design methodology must include explicit rules for prioritizing which failure modes deserve active measurement even when measurement is expensive.

## Failure Mode Catalogs as Living Documents

Maintaining a failure mode catalog as a living document changes how your team thinks about metrics over time. The catalog starts as a structured list of everything that could go wrong, organized by consequence severity, likelihood, and measurability. For each failure mode, you document the stakeholder harm if it occurs, the expected frequency based on similar systems, whether you currently have a metric that detects it, and if not, what would be required to measure it. This catalog becomes the source of truth for metric design decisions, reviewed quarterly as your system evolves and new failure modes emerge from production incidents.

The catalog makes implicit trade-offs explicit by forcing you to write down which failure modes you are choosing not to measure and why. Perhaps you identified that your legal research assistant could cite dissenting opinions as majority holdings, but measuring this requires building a comprehensive legal opinion parser that would take six months. The catalog documents this decision: "Dissent misattribution is not currently measured due to parsing complexity. Estimated impact: medium severity, low frequency. Revisit if we observe user complaints about citation quality." This documentation prevents the failure mode from being forgotten and creates a clear trigger for revisiting the measurement decision later.

Over time, the catalog accumulates evidence from production that refines your understanding of which failure modes actually matter. You might have worried during initial design that your customer service agent would struggle with regional accent variation, allocated measurement resources to track accent-related failures, and discovered after six months that accent issues account for less than one percent of customer complaints while unexpected requests for account changes you lack authorization to perform represent eighteen percent of escalations. The catalog tracks this learning, allowing you to shift measurement resources from theoretical concerns to empirically validated problems.

## Building Failure-First Test Sets

Translating failure modes into metrics requires test sets specifically designed to trigger each failure condition rather than general-purpose evaluation sets that sample broadly from your task distribution. If your failure mode is fabricating employment history in loan assessments, you need test cases where employment data is partially missing, ambiguous, or inconsistent across sources. A general loan assessment test set might include some such cases by coincidence, but a failure-targeted set intentionally oversamples exactly the conditions that could trigger fabrication. This oversampling makes the failure mode detectable in evaluation even when it occurs rarely in production.

Building these test sets demands domain expertise and creativity about how to construct inputs that stress-test each specific failure mode. For a medical triage system where the critical failure is dismissing urgent symptoms, you need cases where serious conditions present with mild initial symptoms, where patients use lay terminology that obscures clinical significance, and where common benign explanations could mask dangerous underlying causes. You cannot generate these cases by randomly sampling patient interactions or by prompting a language model to create medical scenarios. You need medical professionals to craft examples that specifically probe the boundary between routine concerns and dangerous misses.

The investment in failure-targeted test sets pays compound returns because these sets remain valuable as your system evolves. When you modify your retrieval logic, add new models, or adjust prompting strategies, you can immediately verify whether the change affects any of your critical failure modes by rerunning these targeted evaluations. Teams that rely only on general evaluation sets often discover that their aggregate metrics stay stable even as specific failure modes get worse, because the failures are too rare in the broad distribution to meaningfully move overall numbers. Failure-targeted sets with intentional oversampling detect these regressions immediately.

## Connecting Metrics to System Components

Each failure mode typically maps to specific components in your architecture, and effective metrics reflect this component structure rather than only measuring end-to-end outputs. If your retrieval-augmented system can fail by either retrieving irrelevant documents or by hallucinating facts even when given relevant documents, these are distinct failure modes requiring separate metrics. Measuring only the final output quality conflates these problems, making it impossible to diagnose where interventions are needed. When your end-to-end accuracy drops, you need to know whether to improve retrieval, generation, or the integration between them.

Component-level metrics create faster debugging cycles by pointing directly at the source of failures. The loan assessment system that fabricated credit histories would have benefited from separately measuring whether the retrieval stage successfully found employment records, whether the generation stage only made claims supported by retrieved records, and whether the final answer included appropriate uncertainty markers when data was missing. These three metrics correspond to three architectural components and three distinct failure modes. Instead, the team measured only whether loan officers accepted the system's recommendations, an end-to-end metric that was insensitive to fabrication because the fabricated details often sounded plausible enough to pass casual review.

This component-level approach requires more instrumentation effort but dramatically reduces the time between detecting a problem and understanding its cause. When your fabrication detection metric triggers, you immediately know the issue is in the generation stage's tendency to fill gaps rather than admit uncertainty. You do not need to investigate whether retrieval is failing, whether the user interface is confusing, or whether training data quality has degraded. The metric's specificity to a failure mode and a component makes the diagnostic path clear.

## Adversarial Thinking in Metric Design

Designing metrics from failure modes naturally encourages adversarial thinking about how your system could be wrong in subtle ways that traditional accuracy metrics miss. Adversarial thinking means actively trying to construct scenarios where your system fails despite appearing to succeed by standard measures. For a summarization system, adversarial thinking might surface that summaries could be accurate on factual details but systematically omit information that contradicts a particular viewpoint, creating bias through selection rather than fabrication. Standard accuracy metrics that check whether stated facts are correct would miss this failure mode entirely.

This adversarial perspective becomes even more important as models become more capable and failures become more subtle. Early language models failed obviously with grammatical errors, nonsense outputs, and irrelevant responses. Modern models fail by being confidently wrong, by providing sophisticated-sounding answers that are subtly misleading, or by optimizing for engagement metrics in ways that diverge from user interests. These sophisticated failures require sophisticated detection, and you will not design the necessary metrics unless you actively imagine how a very capable system could still cause harm.

Building adversarial thinking into your process means regularly convening red team exercises where participants deliberately try to break your system in ways that would not trigger existing metrics. For the financial assistant, a red team might probe whether it can be manipulated into approving loans by subtly suggesting that missing data should be interpreted favorably, or whether it exhibits different fabrication rates across demographic groups in ways that create fair lending violations. Each successful red team attack becomes a new failure mode to measure, expanding your catalog and your metric coverage.

## Metrics That Detect Slow Degradation

Many critical failures in AI systems emerge gradually rather than suddenly, and your metrics must be designed to detect slow degradation before it causes visible harm. A customer service agent might slowly drift toward more verbose responses as you tune it for engagement, eventually doubling average interaction time and overwhelming your support team. An information retrieval system might gradually lose precision as your document corpus grows and your similarity thresholds become less appropriate, leading to slow increases in irrelevant results that users learn to tolerate until they defect to competitors. These gradual shifts do not trigger threshold-based alerts designed for sudden failures.

Detecting slow degradation requires metrics that track trends over time rather than absolute values at single points. You need statistical process control approaches that model normal variation in your metrics and alert when sustained directional changes exceed what random fluctuation would predict. If your customer service response length normally varies between 120 and 180 words with a mean around 150, a single session at 200 words is not concerning. But if your rolling seven-day average climbs from 150 to 155 to 161 to 168 over a month, something systematic is changing even though no individual session triggered an alert.

These trend-based metrics require more sophisticated analysis infrastructure than simple threshold alerts, but they catch a critical class of failures that threshold-based systems miss. Many of the most expensive AI failures in production have been gradual degradation stories where individual instances looked fine but aggregate behavior slowly shifted in costly directions. The loan assessment system might have been fabricating credit details at gradually increasing rates for weeks before the eleven-day detection window, but without trend analysis on fabrication metrics, the slow increase never triggered concern.

## From Features to Failures in Practice

Transitioning from feature-based to failure-based metric design in an existing system requires an audit of your current metrics followed by explicit gap analysis. List every metric you currently track, then for each one, ask whether it directly measures a failure mode that would cause user harm or business risk. Response latency measures user experience quality, but which specific failure mode does it prevent? High latency might cause user frustration, making this a real failure mode worth measuring. User satisfaction scores correlate with many things, but which specific failure does a satisfaction drop indicate? If satisfaction could drop because of hallucinations, bias, or unhelpful responses, the metric is too coarse to guide intervention.

The gap analysis reveals which of your documented failure modes lack any corresponding metrics. In the loan assessment case, fabrication of credit details was a known concern but had no direct measurement. Gap analysis makes this omission visible and quantifiable: you are monitoring six feature-related metrics and zero fabrication-detection metrics despite fabrication being your highest-severity failure mode. This creates the business case for investing in the harder measurement work, reframing it from a nice-to-have enhancement to a critical gap in your quality assurance.

Filling the gaps requires prioritization because you cannot immediately build metrics for every unmeasured failure mode. Priority should be determined by a combination of failure severity, likelihood, and measurement feasibility. Very severe, very likely failures deserve measurement investment even when measurement is expensive. Very severe but unlikely failures might be addressed through other risk controls rather than continuous monitoring. Low-severity failures, regardless of likelihood, can often be monitored through periodic sampling rather than real-time metrics. This prioritization framework prevents the gap analysis from becoming overwhelming while ensuring your most critical failure modes get measurement coverage.

## Failure Modes as Metric Requirements

Treating each documented failure mode as a requirement for metric design creates accountability and completeness in your quality assurance approach. In the same way that functional requirements drive feature development, failure mode requirements should drive metric development. Your metric design is incomplete until every high-priority failure mode has at least one corresponding metric, just as your product is incomplete until every high-priority feature is implemented. This framing elevates metric design from an afterthought to a first-class engineering concern with clear deliverables and success criteria.

This requirements-driven approach also clarifies scope boundaries and handoffs between teams. If your legal research assistant has a documented failure mode around citing overturned precedents, someone must own the metric that detects this failure. Ownership includes defining the exact measurement methodology, implementing the instrumentation, setting thresholds, configuring alerts, and documenting the response procedure when the metric indicates a problem. Without this ownership structure, failure modes remain documented but unmeasured because no one has clear responsibility for translating concern into measurement.

The transition from feature-first to failure-first metric design represents a fundamental shift in how you think about quality assurance for AI systems. Features are what your system does; failure modes are what it must never do. Starting from failure modes ensures that your measurement infrastructure actually protects against the harms that matter rather than just quantifying the capabilities you built. This shift requires more upfront work to catalog failures, design targeted test sets, and implement sophisticated detection logic, but the alternative is systems that perform well on the metrics you track while quietly failing in the ways you did not measure. The financial assistant's eleven-day fabrication spree demonstrated this alternative's cost, and your metric design methodology should learn from that failure before your system produces its own version of the same story.

Your next challenge is ensuring that the metrics you design from failure modes actually drive decisions rather than accumulating as dashboard decoration, which requires understanding what makes a metric actionable versus merely informative.
