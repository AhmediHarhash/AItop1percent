# 4.15 â€” Long-Context and Large-Document Task Metrics

On November 8, 2025, a legal technology company launched a contract analysis system using GPT-4 Turbo with 128K token context to analyze complex multi-party agreements. During development testing on individual contracts averaging 15,000 tokens, the system achieved 91 percent accuracy in identifying key terms, obligations, and risks. The company marketed the system's ability to handle extremely long documents without chunking or summarization, a major advantage over previous tools. They rolled out to 23 law firms analyzing contracts ranging from straightforward vendor agreements to complex merger documents spanning hundreds of pages. Within five weeks, attorneys reported erratic quality. The system performed well on short contracts but produced unreliable results on documents longer than 60,000 tokens. Key obligations mentioned in the middle of long contracts were frequently missed. Cross-references between sections separated by many pages were not properly connected. Risk assessments for long documents often failed to consider important provisions that appeared later in the document. The company pulled the system in mid-December and spent two months diagnosing the problem. The root cause was position-dependent quality degradation: the model's ability to retrieve and reason about information varied dramatically depending on where that information appeared in the long context. Their evaluation framework had tested average performance but missed positional effects.

This failure pattern is emerging across long-context applications in 2026 as context windows expand to 200K tokens and beyond. Models can technically ingest enormous documents, but quality is not uniform across context positions. Information at the beginning and end is processed more reliably than information in the middle. Cross-document reasoning is weaker than single-document reasoning. Quality degrades as document length increases, even within the supported context window. Measuring long-context quality requires position-aware, length-sensitive metrics that capture these complex patterns.

## Retrieval Accuracy Across Context Positions: Beginning, Middle, End

The most fundamental long-context metric measures whether the model can accurately retrieve information from different positions in the context. Research has consistently shown that language models exhibit a **lost in the middle** effect, where information in the middle of long contexts is retrieved less accurately than information at the beginning or end.

A research synthesis AI in early 2025 used Claude Opus 4.5 with 200K context to answer questions based on multiple research papers. The team measured retrieval accuracy by inserting target information at different positions in the context and asking questions that required retrieving that information. They divided the context into five regions: first 20 percent (beginning), 20 to 40 percent (early middle), 40 to 60 percent (middle), 60 to 80 percent (late middle), and 80 to 100 percent (end).

Retrieval accuracy was 94 percent for information in the beginning region. It dropped to 89 percent in early middle, 81 percent in middle, 86 percent in late middle, and recovered to 92 percent at the end. The characteristic U-shaped curve showed that middle positions were substantially less reliable than the beginning and end.

They tested whether this pattern held across different document lengths. For documents using 40K tokens out of the 200K window, the accuracy spread was smaller: 93 percent beginning, 90 percent middle, 92 percent end. For documents using 120K tokens, the spread widened: 94 percent beginning, 78 percent middle, 91 percent end. For documents using 180K tokens, approaching the context limit, the pattern was most severe: 93 percent beginning, 71 percent middle, 90 percent end.

The middle degradation was not just a function of absolute position but of proportional position within the used context. When relevant information appeared halfway through a long document, retrieval quality dropped significantly. This meant that as users provided longer documents, quality became increasingly uneven.

They built position-aware evaluation sets that tested retrieval from each context region separately. This revealed failures invisible in average accuracy metrics. A model might have 88 percent overall retrieval accuracy, but this could hide 94 percent at beginning, 82 percent in middle, and 93 percent at end. Users providing documents with critical information in middle sections would experience worse quality than average metrics suggested.

## Cross-Document Reasoning: Connecting Information Scattered Across Multiple Documents

Many long-context use cases involve reasoning across multiple documents, not just retrieving facts from a single long document. **Cross-document reasoning metrics** measure the model's ability to connect information spread across different documents in the context.

A financial analysis AI in mid-2025 analyzed companies by processing multiple documents: annual reports, earnings call transcripts, SEC filings, news articles, and analyst reports. Questions required synthesizing information across documents: "How does the revenue growth in the earnings call compare to the guidance in the annual report, and how do analysts explain any discrepancy?"

The team measured cross-document reasoning accuracy by constructing questions that explicitly required information from multiple documents. Single-document questions that could be answered from any one document achieved 89 percent accuracy. Two-document questions requiring information from exactly two documents achieved 82 percent. Three-document questions dropped to 76 percent. Four-document questions fell to 68 percent. As the number of documents requiring integration increased, accuracy degraded substantially.

They also measured distance effects. When two relevant documents were adjacent in the context (one after the other), cross-document accuracy was 84 percent. When relevant documents were separated by one irrelevant document, accuracy dropped to 79 percent. When separated by three or more irrelevant documents, accuracy fell to 72 percent. The model struggled to maintain connections across context when relevant information was separated by intervening content.

Document order effects were also significant. When Document A came before Document B in the context, and the question required comparing A to B, accuracy was 81 percent. When the question required comparing B to A (asking about the earlier document in relation to the later one), accuracy dropped to 76 percent. The model had slight bias toward using later context to interpret earlier context rather than the reverse.

These cross-document reasoning challenges meant that naively stuffing all available documents into context did not produce reliable multi-document reasoning. The legal technology company from the opening story had put entire contracts into context expecting the model to reason across sections reliably, but cross-section reasoning degraded with document length and section separation.

## Context Efficiency: Quality Per Token of Context Used

Long-context models consume substantially more compute and cost per request than short-context processing. **Context efficiency metrics** measure how much quality you gain per token of context used, helping you optimize the trade-off between context length and cost.

A document question-answering system in late 2025 compared two approaches: putting full documents in context versus chunking documents and retrieving only relevant chunks. Full document approach used an average of 47,000 tokens per query. Retrieval approach used an average of 8,000 tokens per query. Full document achieved 87 percent answer accuracy. Retrieval achieved 81 percent accuracy.

The quality difference was 6 percentage points in favor of full context. But the cost difference was substantial. At GPT-4 Turbo pricing, full context queries cost approximately $0.47 each. Retrieval queries cost $0.08 each. Cost per correct answer was $0.54 for full context versus $0.10 for retrieval.

They calculated quality per dollar: full context achieved 185 correct answers per dollar. Retrieval achieved 1,013 correct answers per dollar. Despite lower raw accuracy, retrieval was approximately 5.5 times more cost-efficient.

But this analysis missed error cost. Wrong answers had downstream consequences. In a customer support application, wrong answers required human escalation, costing approximately $4 in human time. The full context approach had 13 percent error rate, costing $0.52 per query in escalation costs. Retrieval had 19 percent error rate, costing $0.76 in escalation. When accounting for error costs, full context total cost was $0.99 per query versus $0.84 for retrieval, a smaller gap.

Context efficiency depends heavily on use case. For applications where errors are cheap and volume is high, retrieval approaches are more efficient despite lower accuracy. For applications where errors are expensive or quality thresholds are critical, full context may be more efficient despite higher per-query costs.

They also measured quality as a function of context length to find the optimal point. Using 10K context achieved 76 percent accuracy. 20K achieved 80 percent. 40K achieved 84 percent. 60K achieved 86 percent. 80K achieved 87 percent. 100K achieved 87 percent. Quality plateaued around 80K tokens, meaning additional context beyond that point added cost without improving accuracy. The optimal strategy used approximately 60-80K context, achieving near-maximum quality without the cost of very long context.

## Degradation Curves: How Quality Changes as Document Length Increases

Model quality on long-context tasks is not constant as document length increases. **Degradation curves** measure how accuracy, precision, recall, or other quality metrics change as input length grows.

A summarization system in early 2026 needed to handle documents from 1,000 to 100,000 tokens. They measured summary quality as a function of input length. For documents under 5,000 tokens, summary quality scored 8.6 out of 10 on human evaluation. For 5,000 to 15,000 tokens, quality was 8.3. For 15,000 to 30,000 tokens, quality dropped to 7.9. For 30,000 to 60,000 tokens, quality was 7.3. For documents over 60,000 tokens, quality fell to 6.7.

The degradation was approximately 0.3 quality points per 15,000 additional tokens. This was not a hard cutoff at the context limit but a gradual decline as documents grew longer. The model was not running out of context window; it was experiencing increased difficulty maintaining coherent understanding across very long inputs.

They analyzed what aspects of quality degraded most with length. Coverage (what percentage of important information was included in summaries) degraded from 91 percent for short documents to 78 percent for long documents. Coherence (how well summary points connected logically) degraded from 8.9 to 7.4. Factual accuracy (absence of hallucinations) degraded from 96 percent to 88 percent. Conciseness (avoiding redundancy) actually improved slightly with longer documents, from 7.8 to 8.1, likely because very long documents had proportionally shorter summaries.

Degradation curves revealed quality boundaries. If they needed summary quality above 8.0, they could reliably handle documents up to approximately 15,000 tokens but not beyond. If they needed quality above 7.5, they could handle up to 40,000 tokens. Understanding these boundaries guided product scoping and user communication about expected quality for different document lengths.

They also measured variability in degradation. For some document types, like technical reports with clear structure, degradation was mild: quality only dropped from 8.6 to 7.8 as length increased from 5K to 80K tokens. For other types, like transcripts of unstructured conversations, degradation was severe: quality dropped from 8.3 to 6.1 over the same length range. Document structure and coherence affected how well models handled long contexts.

## Comparing Long-Context to Chunked and RAG Approaches

Long-context processing is not the only way to handle long documents. Chunking documents and using retrieval-augmented generation (RAG) are alternative approaches with different trade-offs. **Approach comparison metrics** help you decide when to use each strategy.

A legal research platform in mid-2025 evaluated three approaches for answering questions about case law spanning thousands of pages: full-context using Claude Opus 4.5 with 200K context, semantic chunking with GPT-4 Turbo retrieving relevant sections, and hybrid approach using retrieval to identify relevant sections then full-context processing of those sections.

They measured accuracy, latency, cost, and interpretability. Full-context achieved 84 percent accuracy with average latency of 12 seconds and cost of $1.20 per query. Retrieval-based achieved 79 percent accuracy with 4 second latency and $0.25 cost. Hybrid achieved 87 percent accuracy with 8 second latency and $0.65 cost.

The hybrid approach was best on accuracy but not on cost or latency. The full-context approach was second-best on accuracy but slowest and most expensive. The retrieval approach was fastest and cheapest but least accurate.

They analyzed which approach worked best for different query types. For factual lookup questions requiring finding specific information in documents, retrieval achieved 88 percent accuracy versus 86 percent for full-context, and was far faster and cheaper. Retrieval was optimal for lookup queries.

For synthesis questions requiring connecting information across many sections, full-context achieved 81 percent versus 72 percent for retrieval, and hybrid achieved 87 percent. Full-context and hybrid were superior for synthesis, with hybrid offering the best accuracy.

For questions where relevant information was densely concentrated in a few sections, retrieval achieved 91 percent accuracy versus 84 percent for full-context. Retrieval was optimal when information was concentrated. For questions where relevant information was scattered throughout documents, full-context achieved 78 percent versus 68 percent for retrieval. Full-context was better for scattered information.

This query-type analysis revealed that no single approach was universally optimal. The best strategy was query-adaptive: use retrieval for lookup and concentrated-information queries, use full-context or hybrid for synthesis and scattered-information queries. They built a router that classified queries and selected the appropriate processing approach, achieving 86 percent accuracy at $0.48 average cost, better than any single approach.

## Instruction Following Fidelity Across Long Contexts

As context length increases, models sometimes lose track of instructions or constraints specified in prompts. **Instruction following fidelity** measures whether the model continues to follow specified requirements as context grows longer.

A content moderation system in late 2025 used long-context models to analyze conversations spanning hundreds of messages. The prompt specified detailed moderation policies: flag harassment but not heated but civil disagreement, flag disinformation but not opinions, flag explicit content but not medical or educational discussions. These nuanced guidelines required careful interpretation.

The team measured instruction following by checking whether the model applied specified policies correctly as conversation length increased. For conversations under 20 messages (approximately 5,000 tokens), policy adherence was 92 percent. For 20 to 50 messages (5,000 to 15,000 tokens), adherence was 89 percent. For 50 to 100 messages (15,000 to 30,000 tokens), adherence dropped to 84 percent. For over 100 messages (30,000+ tokens), adherence fell to 78 percent.

The model was not completely forgetting instructions but was applying them less consistently as context grew. It would sometimes miss nuanced distinctions specified in the prompt, falling back to simpler heuristics. Heated disagreements would be flagged as harassment despite instructions to distinguish these. Medical discussions of explicit topics would be flagged despite educational exceptions.

They tested whether instruction position mattered. When moderation policies were specified at the beginning of the prompt, adherence was 84 percent for long conversations. When repeated at the end of the prompt after all context, adherence improved to 88 percent. When included at both beginning and end, adherence reached 91 percent. Instruction repetition improved consistency.

They also tested instruction complexity. Simple binary rules ("flag explicit content") maintained 91 percent adherence across long contexts. Nuanced multi-condition rules ("flag explicit content unless it appears in medical or educational context, and even then flag if it seems gratuitous rather than necessary for the discussion") dropped to 79 percent adherence in long contexts. Complex instructions degraded more than simple ones.

## Attention Dilution: Model Capacity Spread Across Long Context

Language models have limited attention capacity. When that capacity must cover a very long context, attention to any specific part is diluted. **Attention dilution metrics** measure how this affects quality.

A research assistant in early 2026 analyzed multiple academic papers simultaneously. The team measured how adding more papers to the context affected the model's understanding of each individual paper. They provided questions about specific papers and measured answer accuracy as a function of total context length.

With one paper in context (approximately 8,000 tokens), questions about that paper achieved 89 percent accuracy. With three papers (24,000 tokens), accuracy for questions about any specific paper dropped to 86 percent. With five papers (40,000 tokens), accuracy fell to 82 percent. With ten papers (80,000 tokens), accuracy dropped to 76 percent. The model's ability to understand any individual paper degraded as more papers competed for attention.

This attention dilution was not just about retrieval. Even when the model successfully retrieved relevant passages from a specific paper, its interpretation and reasoning about those passages degraded when many other papers were also in context. The limited model capacity was being spread across more content, reducing depth of processing for each piece of content.

They measured how relevance affected attention dilution. When all papers in context were relevant to the question, dilution was less severe: accuracy dropped from 89 percent for one paper to 81 percent for ten papers. When only one paper was relevant and nine were irrelevant distractors, dilution was worse: accuracy dropped to 73 percent for ten papers. Irrelevant context was more distracting than relevant context.

This finding had implications for context management strategies. Naively including all available documents was counterproductive when many were irrelevant. Selective inclusion of only relevant documents improved quality even if it meant using shorter context. Quality was not just about total information available but about signal-to-noise ratio.

## Latency and Cost Scaling: The Practical Limits of Long Context

Long-context processing has significant latency and cost implications that affect practical usability. **Performance scaling metrics** measure how latency and cost increase with context length.

A document analysis company in mid-2025 measured latency for GPT-4 Turbo queries as a function of context length. With 5,000 token context, queries completed in an average of 2.1 seconds. With 20,000 tokens, latency increased to 4.8 seconds. With 50,000 tokens, latency reached 11.2 seconds. With 100,000 tokens, latency hit 23.7 seconds. With 150,000 tokens, latency was 38.4 seconds.

Latency scaled super-linearly with context length. Doubling context length more than doubled latency. At very long contexts approaching 150K tokens, latency became a user experience problem. Users were unwilling to wait 40 seconds for responses, even if quality was high.

Cost also scaled linearly with context length but had different practical implications. A 5,000 token query cost approximately $0.05. A 100,000 token query cost approximately $1.00. For applications serving millions of queries per month, cost scaled to hundreds of thousands or millions of dollars. Long-context processing was expensive at scale.

They measured the cost-quality-latency trade-off space. Short context (under 10K tokens) offered low cost ($0.05 to $0.10) and fast latency (2 to 4 seconds) but limited quality (78 to 81 percent accuracy). Medium context (10K to 40K tokens) offered moderate cost ($0.10 to $0.40) and latency (4 to 10 seconds) with good quality (81 to 85 percent accuracy). Long context (40K to 100K tokens) offered high cost ($0.40 to $1.00) and slow latency (10 to 25 seconds) with best quality (85 to 87 percent accuracy).

The optimal strategy depended on requirements. For cost-sensitive applications with moderate quality requirements, medium context was optimal. For latency-sensitive applications, short context was necessary despite lower quality. For quality-critical applications where cost and latency were acceptable, long context was justified. Understanding these trade-offs required measuring all three dimensions simultaneously.

## Context Window Boundaries: What Happens Near the Limit

Models behave differently when context approaches the maximum supported window. **Boundary effect metrics** measure quality degradation near context limits.

A contract analysis system in late 2025 used Claude Opus 4.5 with 200K context. Most contracts fit comfortably within this limit, but some approached or exceeded it. They measured quality as a function of how close inputs were to the 200K boundary.

For documents using less than 50 percent of context window (under 100K tokens), quality was stable at 87 percent accuracy. For documents using 50 to 75 percent (100K to 150K tokens), quality remained 86 percent. For documents using 75 to 90 percent (150K to 180K tokens), quality dropped to 82 percent. For documents using 90 to 100 percent (180K to 200K tokens), quality fell to 76 percent. Performance degraded sharply near the context boundary.

When documents exceeded the context window and were truncated, quality collapsed. Documents truncated by 5 percent achieved only 68 percent accuracy. Documents truncated by 10 percent fell to 59 percent. Truncation caused dramatic quality loss, much worse than slight degradation near the boundary.

They also measured error patterns near the boundary. When documents were near the context limit, the model made more omission errors (missing information) than commission errors (incorrect information). It was struggling to maintain complete understanding rather than hallucinating. Error rate for omissions increased from 8 percent for short documents to 19 percent for documents near the boundary. Commission error rate only increased from 5 percent to 7 percent.

Understanding boundary effects guided deployment decisions. They set a soft limit at 85 percent of the context window (170K tokens out of 200K). Documents approaching this limit triggered warnings to users about potential quality degradation. Documents exceeding this limit were recommended for alternative processing strategies like summarization before analysis or splitting into multiple analyses.

## Multi-Pass Processing: Quality Versus Cost Trade-offs for Iterative Analysis

One strategy for handling long documents is multi-pass processing: analyze the document in multiple passes, each focusing on different aspects. **Multi-pass metrics** compare quality and cost of iterative approaches versus single-pass full-context processing.

A financial document analysis system in early 2026 compared two approaches. Single-pass: put the full document in context and answer all questions in one request. Multi-pass: first pass extracts key information and creates a summary, second pass answers questions using both full document and summary.

Single-pass achieved 84 percent accuracy with average cost of $1.10 per document and latency of 18 seconds. Multi-pass achieved 88 percent accuracy with average cost of $1.45 per document and latency of 26 seconds. Multi-pass was more accurate but more expensive and slower.

They analyzed which questions benefited most from multi-pass processing. Simple factual questions showed minimal benefit: 87 percent accuracy for single-pass versus 88 percent for multi-pass. Complex analytical questions showed substantial benefit: 79 percent accuracy for single-pass versus 90 percent for multi-pass. The multi-pass approach was most valuable for questions requiring synthesis and analysis rather than factual retrieval.

They tested a hybrid strategy: use single-pass for simple questions and multi-pass for complex questions. A classifier determined question complexity and routed to the appropriate processing approach. The hybrid achieved 87 percent accuracy at $0.95 average cost and 15 second latency, better than single-pass on all dimensions and better than multi-pass on cost and latency while close on accuracy.

Multi-pass processing also enabled specialized prompting in each pass. The first pass used extraction-optimized prompts focusing on finding key information. The second pass used reasoning-optimized prompts focusing on analysis and synthesis. Specialized prompting for each pass improved quality compared to generic prompts in single-pass.

## The Metrics You Need for Long-Context Quality

Evaluating long-context AI systems requires metrics that capture position-dependent effects, cross-document reasoning, cost-quality trade-offs, degradation patterns, and comparison to alternative approaches. You need retrieval accuracy across context positions measuring whether the model can find information at the beginning, middle, and end. You need cross-document reasoning metrics measuring connections across multiple documents. You need context efficiency metrics measuring quality per token of context used. You need degradation curves showing how quality changes with document length. You need comparisons to chunking and RAG approaches understanding when each strategy works best. You need instruction following fidelity measuring whether the model maintains adherence to requirements in long contexts. You need attention dilution metrics measuring how capacity spreads across content. You need latency and cost scaling metrics understanding practical limits. You need boundary effect metrics measuring quality near context limits. You need multi-pass processing comparisons evaluating iterative approaches.

Build these metrics into your evaluation framework before deploying long-context systems. Do not assume that models supporting 200K context will perform uniformly across that entire window. Test position effects systematically. Measure quality as a function of document length. Compare long-context to alternative approaches on cost, latency, and quality. Understand where long context provides value and where it introduces unnecessary cost. Design context management strategies that optimize the trade-off between information completeness and processing efficiency.

Long-context capability is powerful but not magic. Quality, cost, and latency all scale with context length in complex ways. Only systematic measurement reveals whether long-context processing is the right approach for your specific use case, or whether chunking, retrieval, or hybrid strategies would serve you better.

