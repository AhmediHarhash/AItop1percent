# 2.3 — Completeness: Coverage Without Bloat

On November 6, 2025, a financial services firm using an AI-powered investment research assistant discovered that their analysts had made a series of suboptimal portfolio decisions over the previous quarter. The AI system analyzed company earnings reports, SEC filings, and market data, then generated concise investment summaries for each stock under consideration. The summaries were accurate, well-sourced, and readable—averaging 250 words per company. Analysts loved them because they could review dozens of companies quickly. The problem emerged during a routine portfolio review. A senior partner noticed that the firm had significantly increased holdings in a telecommunications company just before it announced a major regulatory penalty. The AI summary had covered revenue trends, subscriber growth, and competitive positioning. It had not mentioned that the company was under investigation by the Federal Communications Commission, a fact disclosed in a footnote on page 47 of the quarterly 10-Q filing. The information was there. The AI system had access to it. But the system optimized for conciseness and judged the regulatory investigation to be less important than the financial metrics. Six analysts read that summary and made buy recommendations. The investigation resulted in a $340 million fine and a 23% stock price decline. The firm's AI vendor had tracked correctness, groundedness, and user satisfaction. Nobody had systematically evaluated whether the summaries included all material information.

This is the completeness problem. Your AI system can be correct, grounded, and well-written while still failing because it omits essential information. **Completeness** means the output addresses the full scope of the question or task, not just the parts that are easy to answer or that fit neatly into a desired output length. It is the dimension that governs coverage and the dimension where optimization for user experience most frequently conflicts with quality requirements.

## What Completeness Actually Means

Completeness is not about generating long outputs. It is about ensuring that the output includes all information necessary for the user to accomplish their goal. A 100-word summary can be complete if it covers every relevant aspect of the topic. A 2,000-word report can be incomplete if it omits a critical detail.

**Complete outputs** answer the whole question, address all relevant dimensions of the problem, and include qualifications, edge cases, and exceptions where they matter. If a user asks "What are the payment terms in this contract," a complete answer includes the base payment amount, the payment schedule, late payment penalties, currency, and any contingencies that affect payment. An incomplete answer might provide the payment amount and schedule but omit the penalties, leaving the user with an inaccurate understanding of the contractual obligations.

**Incomplete outputs** answer part of the question while ignoring or glossing over other parts. This happens for several reasons. The AI system might not have access to all relevant information, so it answers based on what it has. The system might be optimizing for conciseness and trimming content it judges less important. The system might be following a template or structure that does not accommodate certain types of information. Or the system might lack the reasoning capability to identify all relevant aspects of the question.

The distinction between completeness and correctness is crucial. An output can be 100% correct on every claim it makes while being dangerously incomplete because it omits claims that should have been made. The financial research system generated factually accurate summaries. Everything it said about revenue, subscribers, and market position was correct and grounded in filings. But it failed on completeness by not including the regulatory investigation. Correctness is about the accuracy of what you say. Completeness is about the coverage of what you choose to say.

## The Tension Between Completeness and Conciseness

Users want concise outputs. Product teams optimize for conciseness because it improves user satisfaction scores, reduces reading time, and makes interfaces cleaner. But conciseness and completeness are often in direct conflict, and teams routinely sacrifice completeness to achieve conciseness without realizing the trade-off they are making.

You instruct your AI system to generate summaries of customer support conversations for handoff between agents. You set a target length of 150 words. The system learns to prioritize the most recent messages and the primary issue, omitting earlier context, secondary issues, and customer sentiment. An agent reading the summary sees "Customer unable to log in, password reset attempted, issue unresolved" and continues troubleshooting the login problem. What the summary omits is that the customer is on a deadline to access tax documents before an IRS filing deadline, has already spent 90 minutes on the phone with support, and threatened to cancel their subscription. The summary is concise and technically accurate, but it is incomplete in a way that leads the agent to mishandle the interaction.

The financial research system set a 250-word target for investment summaries. This forced the generation model to prioritize. It prioritized quantitative financial metrics because those were easy to extract and summarize concisely. It deprioritized regulatory and legal information because that information was more complex, less structured, and harder to summarize without nuance. The completeness failure was a direct result of the conciseness constraint.

You need explicit principles for what to include and what to omit when length constraints force trade-offs. Some information is mandatory—it must be included regardless of length targets. Some information is prioritized—it should be included if space allows. Some information is optional—it can be omitted if necessary. You cannot delegate these decisions entirely to the model. You need to define the rules.

For the financial research case, the correct principle would have been: any material risk disclosed in regulatory filings is mandatory content, even if it requires exceeding the length target. The 250-word guideline should have been a target for typical cases, not a hard constraint that overrides completeness requirements. The system needed explicit instructions that regulatory, legal, and reputational risks take precedence over length optimization.

## How to Measure Completeness Without Rewarding Verbosity

Measuring completeness is harder than measuring correctness or groundedness because there is no clear ground truth for what constitutes a complete answer. Correctness can be checked against facts. Groundedness can be checked against sources. Completeness requires judgment about what should have been included but was not.

The most rigorous approach is **rubric-based evaluation**. You define the dimensions or components that a complete answer must address for each type of query. For investment research summaries, the rubric might include: financial performance, strategic initiatives, competitive positioning, regulatory environment, and material risks. For customer support summaries, the rubric might include: primary issue, troubleshooting steps taken, customer sentiment, account history, and urgency. You evaluate each output against the rubric, checking whether each required dimension is covered.

You score completeness as the percentage of required dimensions addressed. If the rubric has eight dimensions and an output addresses six of them, the completeness score is 75%. This approach works well when you have a structured task with consistent information requirements. It breaks down for open-ended tasks where the set of relevant dimensions varies by input.

An alternative approach is **information coverage analysis**. You identify all information in the input that is potentially relevant to answering the question. For a summarization task, this means extracting key facts, figures, entities, and events from the source material. For a question-answering task, this means identifying all parts of the input that relate to the question. You then check what percentage of that information appears in the output. This gives you a coverage metric: if the source material contains 20 relevant facts and the output includes 16 of them, you have 80% coverage.

The challenge with coverage metrics is distinguishing between important and unimportant information. Not all omissions are equally problematic. Omitting a minor detail might be fine. Omitting a material risk is catastrophic. You need to weight information by importance, which requires domain knowledge and judgment. Some teams use a two-tier system: critical information that must be included and supporting information that should be included if space allows. Completeness is then measured separately for each tier.

A third approach is **comparative evaluation**. You generate multiple outputs for the same input using different systems or configurations. You have human raters compare them and identify which output is most complete. This does not give you an absolute completeness score, but it tells you which system or approach produces more complete outputs. Over time, you build intuition about what completeness looks like for your domain and can translate that into more precise evaluation criteria.

You can also use LLM-based evaluation for completeness. You provide the input, the output, and a prompt asking whether the output fully addresses the input. For question-answering tasks, you can ask "Does this answer address all parts of the question?" For summarization tasks, you can ask "Does this summary capture all key information from the source?" GPT-5.1 and Claude 4 Opus are reasonably good at these judgments, though they tend to penalize verbosity and reward conciseness, which can lead them to score incomplete-but-concise outputs higher than complete-but-detailed outputs. You need to calibrate LLM evaluators with examples that demonstrate the difference between appropriate conciseness and problematic incompleteness.

## Completeness Across Different Task Types

The operational definition of completeness varies dramatically by task type. What it means for a summary to be complete is different from what it means for an answer to be complete, which is different from what it means for a generated artifact to be complete.

For **summarization tasks**, completeness means capturing all salient information from the source. A complete summary of a research paper includes the research question, methodology, key findings, and implications. An incomplete summary might include findings but omit methodology, leaving readers unable to evaluate the validity of the research. The challenge is that different users have different definitions of salient. A researcher reading a paper wants methodological details. A business executive wants implications and applications. Completeness is relative to user needs.

For **question-answering tasks**, completeness means addressing every aspect of the question. If a user asks "What are the side effects and contraindications of this medication," a complete answer includes both side effects and contraindications. An answer that provides a thorough discussion of side effects but does not mention contraindications is incomplete, even if the side effects section is detailed and accurate. Multi-part questions are particularly prone to completeness failures because systems sometimes focus on the first or most prominent part of the question and neglect subsequent parts.

For **extraction tasks**, completeness means pulling all relevant information from the source. If you are extracting entities from a document, a complete extraction includes all entities of the specified type. If you are extracting key-value pairs from an invoice, a complete extraction includes all fields. Incomplete extractions might get the most prominent items but miss items that are formatted differently, appear in unexpected locations, or are less common.

For **generation tasks** like code writing or content creation, completeness means that the generated artifact includes all components necessary to function or serve its purpose. A complete code function includes input validation, core logic, error handling, and return values. An incomplete function might have correct core logic but lack error handling, making it unsuitable for production use.

For **planning tasks**, completeness means the plan addresses all necessary steps, resources, and contingencies. An incomplete project plan might include major milestones but omit risk mitigation strategies, resource allocation, or dependencies between tasks.

You need task-specific completeness criteria. A single generic completeness metric applied across all task types will miss the nuances of what completeness means in each context.

## The Different Failure Modes of Incompleteness

Incompleteness manifests in several distinct patterns, each with different causes and solutions.

**Scope incompleteness** occurs when the system answers a narrower question than was asked. A user asks "How do I troubleshoot network connectivity issues on Windows," and the system provides detailed steps for Wi-Fi problems but does not address Ethernet, VPN, or DNS issues. The answer is high quality within its scope, but the scope is too narrow.

**Depth incompleteness** occurs when the system provides surface-level information without the detail needed for decision-making. A user asks about the tax implications of a business decision, and the system responds "There may be tax consequences; consult a tax professional." This is true but not useful. A complete answer would explain what types of tax consequences might arise, which factors affect them, and what information a tax professional would need.

**Context incompleteness** occurs when the system provides an answer that is correct in general but fails to account for specific context. A user asks "Should I invest in index funds," and the system provides a generic answer about the benefits of index fund investing without considering the user's financial situation, investment timeline, risk tolerance, or tax circumstances. The answer is correct but incomplete because it ignores contextual factors that affect whether the recommendation applies.

**Edge case incompleteness** occurs when the system addresses the typical case but omits exceptions, special cases, or conditions where the general answer does not apply. A system explains how a refund policy works but does not mention that digital products are non-refundable, or that refunds are processed differently for international orders. Users who fall into edge cases end up with incorrect expectations.

**Temporal incompleteness** occurs when the system provides information that is accurate at one point in time but omits information about how things change over time. A contract analysis system summarizes pricing terms but does not mention that pricing changes after the first year, or that a promotional rate expires. The user gets an incomplete picture of the long-term obligations.

Each failure mode requires different interventions. Scope incompleteness often stems from query understanding problems—the system does not recognize all parts of the question. Depth incompleteness stems from output length constraints or model limitations. Context incompleteness stems from lack of access to user context or failure to use available context. Edge case incompleteness stems from training data that over-represents typical cases. Temporal incompleteness stems from failure to reason about time-dependent information.

## Incompleteness Versus Incorrectness

When users encounter an incomplete output, they often perceive it as incorrect. But the failure mode is different, and conflating the two leads to misdiagnosis and ineffective fixes.

An incorrect output makes false claims. An incomplete output makes only true claims but omits important true claims. If you measure only correctness, incomplete outputs score perfectly because every statement they make is accurate. You fail to detect that they are missing critical information.

Consider a medical AI that generates a patient discharge summary. The summary includes diagnosis, treatment provided, and follow-up appointments. It does not include medication allergies because the allergy information was in a separate part of the medical record that the system did not retrieve. The summary is 100% correct—everything it says is accurate. It is also dangerously incomplete. A physician reading the summary might prescribe a medication the patient is allergic to.

The fix for incorrectness is improving model accuracy, refining prompts, or enhancing training data. The fix for incompleteness is often improving data retrieval, adjusting output structure to ensure all required sections are included, or modifying instructions to emphasize coverage over conciseness.

If you measure only correctness and your system is performing well, you might be blind to completeness problems. You need separate metrics. Your evaluation pipeline should track both the accuracy of included information and the coverage of required information.

## Building Completeness Into System Design

The most effective way to improve completeness is to build completeness requirements into your system architecture, not to rely on prompts or post-hoc evaluation.

**Structured output formats** help ensure completeness by creating explicit slots for different types of information. If your system generates outputs in a structured format with required fields, the system cannot omit those fields. A contract analysis system might use a schema with mandatory sections for parties, obligations, terms, termination conditions, and risks. If any section is missing, the output is invalid. This prevents accidental omissions.

**Checklists and rubrics** embedded in prompts guide the model to consider all relevant dimensions. You include a checklist in the prompt: "When analyzing an investment opportunity, address: financial performance, competitive position, management quality, regulatory environment, and material risks." This does not guarantee the model will address all items, but it significantly reduces omission rates compared to generic instructions.

**Multi-step generation** breaks complex tasks into subtasks, each responsible for one aspect of completeness. For financial research summaries, you might have separate generation steps for financial analysis, competitive analysis, and risk analysis, then combine the results. This ensures that each dimension gets explicit attention rather than competing for limited output space in a single generation pass.

**Retrieval completeness** is foundational. If your retrieval system does not pull all relevant information from your knowledge base, your generation system cannot include it. You need recall-focused retrieval that prioritizes finding all relevant passages, even at the cost of precision. You can filter out less relevant information during generation, but you cannot generate information you never retrieved.

**Dynamic length allocation** allows output length to vary based on the complexity and information density of the input. Instead of a fixed 250-word summary, you use a variable-length summary that expands when there is more information that meets your inclusion criteria and contracts when there is less. This prevents the scenario where you have material information that does not fit within length constraints.

## When Completeness Must Override Other Dimensions

There are domains where completeness is the paramount quality dimension, and you must be willing to sacrifice other dimensions to achieve it. Medical decision support, legal analysis, safety-critical systems, and regulatory compliance are all areas where an incomplete output can cause more harm than a lengthy or less fluent output.

If you are building a system that generates safety instructions for industrial equipment, completeness is non-negotiable. The instructions must include all safety warnings, all protective equipment requirements, all emergency procedures. An instruction set that omits a critical warning because it exceeds length targets is unacceptable. In this context, you optimize for completeness and tolerate longer, more repetitive outputs.

If you are building a system that analyzes legal contracts, you cannot omit material terms or obligations to improve readability. Every indemnification clause, every liability limitation, every termination condition must be surfaced. Users can skim over details they consider less important, but the system cannot make that decision for them.

The financial services firm learned this the hard way. They prioritized user experience and conciseness, and their analysts paid the price through portfolio losses. After the incident, they restructured their AI system to prioritize completeness. Summary lengths increased by 40%. Analysts initially complained about longer outputs. Then they discovered several other cases where the old summaries had missed material information, and they understood why the change was necessary.

## Completeness and the Next Dimension

A complete output is not necessarily a useful output. You can provide all relevant information in a way that is too dense, too technical, or too poorly organized for the user to act on it. Completeness is about what information is included. The next dimension, usefulness, is about whether that information is presented in a way that enables the user to accomplish their goal.

