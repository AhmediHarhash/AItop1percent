# 4.7 — Code Generation and Developer Tool Metrics

On November 18, 2025, a fintech company building internal development tools discovered that their AI code generation system had introduced critical security vulnerabilities into production systems across seven different services. The company had deployed an AI coding assistant six months earlier to accelerate their development cycle, integrating GPT-5.1 and Claude 4 models into their IDE and CI/CD pipeline. The system generated code snippets, completed functions, and even wrote entire modules based on natural language specifications from developers. Their metrics showed impressive results: eighty-four percent of generated code compiled successfully, seventy-three percent passed unit tests on first generation, and developers reported saving an average of four hours per week. The thirty-two-person engineering team celebrated these productivity gains and expanded the system's usage. But a routine security audit revealed that AI-generated authentication code in three services had logic flaws that would allow unauthorized access under specific conditions, AI-generated data processing code in two services leaked personally identifiable information through excessive logging, and AI-generated SQL query builders in two services were vulnerable to injection attacks. None of these vulnerabilities triggered test failures because the tests themselves did not cover the security scenarios. The company took five services offline for emergency patching, disclosed the vulnerability to affected customers, and faced regulatory scrutiny from financial services authorities. The incident cost them approximately two point three million dollars in emergency remediation, seven point six million in lost revenue during the outage, and immeasurable reputational damage.

The root cause was measuring code generation quality purely by functional correctness—does the code compile and pass tests—without considering code quality dimensions like security, performance, maintainability, and adherence to best practices. The team had optimized their AI system to generate code that worked in the narrow sense of producing correct outputs for tested inputs, but they had not evaluated whether the code was production-quality in the broader sense of being safe, efficient, and maintainable. Their metrics gave them false confidence that AI-generated code was equivalent to human-written code when in fact it was systematically worse on dimensions they were not measuring. This failure revealed that code generation requires fundamentally different evaluation approaches than other AI tasks because the artifact being produced—executable code that will run in production systems handling real user data and money—carries risks that text generation or classification do not.

## Functional Correctness as Foundation

The most basic requirement for generated code is **functional correctness**: does the code actually do what it is supposed to do? This seems straightforward but is surprisingly complex to measure. Unlike text generation where you can use similarity metrics to compare output to reference text, or classification where you can count correct predictions, code correctness requires executing the code and verifying its behavior across a range of inputs.

The standard approach is **test pass rate**: what percentage of generated code passes a suite of test cases that specify the intended behavior? You provide the AI system with a problem specification, it generates code, you run the code against test cases, and you count how many tests pass. A test pass rate of one hundred percent means the code produces correct outputs for all tested inputs. A rate of zero percent means it fails all tests. Intermediate rates indicate partial correctness.

The fintech company measured test pass rate as their primary metric during development. For each code generation task, they maintained a test suite with an average of twelve test cases covering normal inputs, edge cases, and error conditions. Their models achieved seventy-three percent average test pass rate, meaning that on average the generated code passed about nine of twelve tests on the first attempt. This seemed acceptable given that developers could debug and fix the failed cases, and the time saved still resulted in net productivity gains.

But test pass rate has fundamental limitations. First, it depends entirely on test quality. If your tests do not cover security scenarios, performance requirements, or edge cases, code can pass all tests while having serious flaws. Second, it treats all tests as equally important. Passing eleven of twelve tests sounds good, but if the one failed test covers data validation that prevents injection attacks, the code is dangerously broken despite ninety-two percent test pass rate. Third, it does not measure partial credit. Code that produces almost-correct outputs fails just as hard as code that produces complete nonsense, even though the former is much closer to being usable.

## Compilation Success and Syntax Validity

Before code can be tested, it must be syntactically valid and compilable. **Compilation success rate** measures what percentage of generated code can be compiled or parsed without syntax errors. This is a lower bar than functional correctness but still meaningful, especially for languages with complex syntax like C++ or Rust where even experienced developers frequently write code that does not compile on first attempt.

The fintech company tracked compilation success separately from test pass rate. Their models achieved eighty-four percent compilation success across Python, JavaScript, and Java code generation tasks. The sixteen percent of generations that failed to compile had various issues: syntax errors, undefined variable references, type mismatches in statically typed languages, incorrect import statements, and malformed function signatures. These compilation failures were often easy for developers to fix—adding a missing parenthesis or import statement—but they represented cases where the generated code required human intervention before it could even be tested.

Compilation success varies dramatically by language and task complexity. For Python, a dynamically typed language with forgiving syntax, models achieved ninety-one percent compilation success. For TypeScript, with its static type system, success dropped to seventy-six percent because the model struggled to generate code that satisfied type constraints. For languages like Rust with strict compile-time guarantees, success dropped further to sixty-two percent. This variation revealed that models found it harder to satisfy formal constraints like type systems than to generate syntactically plausible code.

The gap between compilation success and test pass rate is also informative. In the fintech case, eighty-four percent of code compiled but only seventy-three percent passed tests, implying that eleven percentage points of generated code compiled but did not work correctly. This is the dangerous middle ground: code that looks plausible, runs without crashing, but produces wrong results. Some of the security vulnerabilities fell into this category—code that compiled, ran, and passed basic functional tests but had logic errors that created security holes.

## Runtime Errors and Robustness

Even code that compiles and passes tests can fail in production due to **runtime errors**: exceptions, crashes, infinite loops, resource exhaustion, and other failures that occur during execution. Measuring runtime error rate requires running generated code in realistic conditions and observing whether it fails.

The fintech company initially did not measure runtime errors separately because they assumed that code passing tests would not have runtime issues. This assumption was wrong. They discovered that AI-generated code had higher runtime error rates than human-written code, particularly for error handling and edge cases. The model would generate the happy path correctly—code that worked when inputs were valid and conditions were normal—but would fail to handle errors gracefully.

Common patterns included generated code that did not check for null or undefined values before dereferencing them, did not validate user inputs before processing them, did not handle network failures in API calls, and did not manage resources properly leading to memory leaks or unclosed file handles. These issues did not show up in unit tests that focused on correct behavior under normal conditions, but they caused production failures when inputs were malformed or systems experienced transient errors.

They addressed this by expanding their test suites to include **negative test cases** that verify error handling: tests with invalid inputs that should trigger validation errors, tests with resource constraints that should trigger graceful degradation, tests with simulated infrastructure failures that should trigger retry or fallback logic. When they measured test pass rate including negative tests, performance dropped from seventy-three percent to fifty-nine percent. The gap revealed that models were much better at generating correct logic for normal cases than at generating robust error handling for abnormal cases.

## Code Quality Beyond Correctness

Functional correctness is necessary but not sufficient for production code. Code must also meet quality standards around readability, maintainability, efficiency, and security. **Code quality metrics** attempt to measure these dimensions, though they are inherently harder to quantify than correctness.

Common code quality metrics include **cyclomatic complexity** (how many independent paths exist through the code, with lower being simpler and more maintainable), **code duplication** (how much code is repeated, with less being better), **naming quality** (whether variables and functions have clear, descriptive names), **comment density** (whether complex logic is explained with comments), and **adherence to style guides** (whether code follows established formatting and naming conventions).

The fintech company initially did not measure code quality, assuming that working code was good enough and developers would refactor as needed. This assumption was wrong. They found that AI-generated code was systematically worse on quality dimensions than human-written code. Generated functions had higher cyclomatic complexity, averaging eight point four versus five point two for human code. Generated code had more duplication, repeating similar logic across different functions rather than abstracting into shared utilities. Generated code had worse naming, using generic names like "temp" and "data" rather than descriptive names that conveyed intent. Generated code had fewer comments, particularly for complex logic that would be hard for future maintainers to understand.

These quality deficits had real costs. Code reviews of AI-generated code took longer because reviewers had to work harder to understand what the code was doing. Maintenance tasks took longer because the code was harder to modify safely. Bugs were more common in later modifications because the code's implicit assumptions were not documented. When they calculated the total cost of working with AI-generated code including initial generation, review, debugging, and maintenance, the productivity gains shrank significantly. Code that saved four hours during initial generation might cost two hours in extra review time and six hours in extra maintenance over the next quarter.

## Security and Safety Metrics

The security vulnerabilities that triggered the fintech incident were the most serious manifestation of inadequate code quality evaluation. **Security metrics** for generated code should include both automated analysis and human review.

Automated security analysis uses **static analysis tools** that scan code for known vulnerability patterns: SQL injection vulnerabilities, cross-site scripting vulnerabilities, insecure deserialization, hardcoded credentials, weak cryptography, and other issues cataloged in resources like the OWASP Top Ten. These tools can process generated code immediately and flag potential security issues before the code reaches human review.

The fintech company added static security analysis to their CI/CD pipeline after the incident, scanning all AI-generated code with tools like Semgrep, CodeQL, and Snyk. This revealed that approximately eighteen percent of AI-generated code triggered security warnings, compared to six percent of human-written code. The most common issues were SQL injection vulnerabilities in database query construction, path traversal vulnerabilities in file operations, and insufficient input validation in API endpoints. Many of these vulnerabilities existed in code that passed functional tests because the tests did not attempt malicious inputs.

Not all security warnings indicate real vulnerabilities—static analysis tools have false positive rates—but they indicate code that requires careful review. The team implemented a policy that all AI-generated code triggering security warnings must receive manual security review from a senior engineer before merging. This reduced the volume of vulnerable code reaching production but did not eliminate it entirely because static analysis cannot catch all vulnerability types, particularly logic errors like the authentication bypass that required understanding the broader system context.

## Performance and Efficiency

Beyond security, AI-generated code often has **performance issues** that affect system responsiveness, resource consumption, and operating costs. Models tend to generate straightforward, inefficient implementations rather than optimized solutions.

The fintech company discovered this when they noticed increased database query times after deploying AI-generated data access code. The generated code worked correctly but used inefficient query patterns: selecting all columns when only a few were needed, loading entire result sets into memory instead of streaming, executing queries in loops instead of using batch operations, and missing indexes on frequently queried fields. These inefficiencies did not cause functional failures but increased database load, slowed response times, and drove up infrastructure costs.

Measuring performance requires **benchmarking**: running generated code with realistic data volumes and measuring execution time, memory consumption, database queries, network calls, and other resource usage. Comparing these metrics to baseline implementations or human-written code reveals performance gaps.

When they benchmarked AI-generated code against human-written equivalents, they found that generated code was on average two point four times slower and used three point one times more memory. The variance was high: some generated code was nearly as efficient as human code, while some was an order of magnitude worse. The performance gaps were largest for tasks involving data processing, algorithmic complexity, or I/O optimization—areas where careful design makes large performance differences.

They addressed this by adding performance requirements to their code generation prompts and evaluation metrics. Prompts specified acceptable performance bounds like "query should complete in under 100 milliseconds for tables with up to one million rows". Generated code was benchmarked automatically, and code exceeding performance budgets was flagged for optimization. This improved average performance but did not close the gap entirely—models still struggled to generate algorithmically optimal solutions for complex problems.

## Developer Productivity and Adoption Metrics

Beyond code quality, the ultimate measure of a code generation system's value is whether it actually improves **developer productivity**. This requires measuring human factors: how much time developers save, how willing they are to use the system, and how the system affects overall development velocity.

The fintech company tracked several productivity metrics. **Time to completion** measured how long developers took to complete tasks with and without AI assistance. **Acceptance rate** measured how often developers accepted AI suggestions versus writing code manually. **Retention rate** measured how much AI-generated code remained unchanged over time versus being rewritten. **Developer satisfaction** was measured through surveys asking whether the AI system helped or hindered their work.

Initial results were promising. Developers using the AI system completed tasks an average of thirty-four percent faster than control groups not using the system. Acceptance rate was sixty-one percent—developers accepted about six of every ten AI suggestions. These numbers justified the investment in the AI system and drove expansion of its usage. But these metrics did not account for downstream costs.

After the security incident, they refined their productivity measurement to include the full lifecycle: initial generation, code review, debugging, testing, and maintenance. When they included time spent fixing bugs in AI-generated code, investigating security issues, and refactoring poorly structured generated code, the productivity gains shrank dramatically. Factoring in all downstream costs, developers using the AI system were only nine percent faster than those not using it. For senior developers writing complex, security-sensitive code, the AI system actually reduced productivity because the time spent reviewing and fixing generated code exceeded the time saved during initial generation.

This led to a more nuanced deployment strategy. They configured the AI system to be more aggressive for simple, low-risk tasks like test code generation, documentation, and boilerplate, where high acceptance rates and low review costs made the productivity gains clear. For complex, high-risk tasks like authentication, payment processing, and data handling, they configured it to provide suggestions rather than full generation, giving developers starting points but requiring more human judgment and review.

## Comparative Evaluation Against Baselines

A key question for code generation metrics is: compared to what? Measuring that generated code passes seventy-three percent of tests is not informative without knowing whether human developers pass eighty percent or sixty percent. **Comparative evaluation** requires establishing baselines.

The fintech company established several baselines. They measured how human developers performed on the same code generation tasks, tracking compilation success, test pass rate, code quality metrics, and time to completion. They also compared different AI models against each other, testing GPT-5.1, Claude 4, Llama 4, and specialized code models like Codex on identical tasks.

Human baseline comparison revealed that experienced developers had higher test pass rates (ninety-one percent) and better code quality (lower complexity, better naming, more comments) but took much longer (three point six times as long on average). This confirmed that the AI system was trading quality for speed. Junior developers had test pass rates (sixty-eight percent) similar to AI but much better code quality because they followed established patterns and asked for help when uncertain, while the AI confidently generated plausible-but-wrong code.

Cross-model comparison showed significant variation. Claude 4 had the highest test pass rate (seventy-nine percent) and best code quality scores. GPT-5.1 was fastest but had lower test pass rate (seventy-one percent) and more security issues. Specialized code models had high compilation success (eighty-nine percent) but lower test pass rate (sixty-seven percent), suggesting they were good at generating syntactically valid code but weaker at understanding requirements. This informed model selection: they used Claude 4 for complex, high-stakes code and GPT-5.1 for simple, repetitive code where speed mattered more than perfection.

## Test Coverage and Specification Completeness

Code generation quality is bounded by **test coverage** and **specification completeness**. A model can only reliably generate correct code if the tests or specifications fully describe the intended behavior. Gaps in specification lead to ambiguity that models resolve incorrectly.

The fintech company discovered that many code generation failures traced to incomplete specifications rather than model limitations. A developer might ask for "a function to process user data" without specifying error handling requirements, performance constraints, security requirements, or edge case behavior. The model would generate something that worked for normal inputs but failed for edge cases or violated security requirements.

They addressed this by implementing **specification templates** that prompted developers to include key requirements: expected inputs and outputs, error handling requirements, performance constraints, security requirements, and test cases. When specifications were complete, test pass rates improved from seventy-three percent to eighty-seven percent. This suggested that roughly fourteen percentage points of failures were due to ambiguous requirements rather than model inability.

They also measured **test coverage** using standard code coverage tools that tracked which lines and branches of generated code were exercised by tests. Higher coverage meant more thorough testing, making test pass rate a more meaningful indicator of correctness. They found that tests for AI-generated code had lower coverage (sixty-eight percent) than tests for human-written code (eighty-four percent), suggesting that either the tests were less thorough or the generated code was more complex than expected.

## Iterative Refinement and Fix Rate

Code generation is often iterative: the model generates code, tests run, failures are identified, and the model generates a revised version. **Fix rate** measures how effectively models improve code in response to feedback.

The fintech company implemented an iterative generation pipeline where test failures were fed back to the model with instructions to fix the errors. They measured first-pass test pass rate (seventy-three percent) and second-pass test pass rate after one round of iterative fixing (eighty-four percent). This eleven percentage point improvement showed that models could effectively debug their own code when given clear error messages.

However, iterative refinement had diminishing returns. A third pass improved test pass rate to eighty-seven percent (three percentage points), and a fourth pass to eighty-eight percent (one percentage point). Beyond three iterations, additional attempts rarely fixed remaining failures, suggesting the model was stuck in local optima or the failures required understanding the model did not have.

They also tracked **fix quality**: did the iterative fixes introduce new errors while fixing old ones? Approximately twenty-three percent of fixes that resolved one test failure introduced regressions that broke previously passing tests. This regression rate was much higher than human debugging (eight percent regression rate), suggesting that models were pattern-matching on error messages rather than understanding the underlying issues.

## Long-Term Maintenance and Technical Debt

The fintech incident revealed that code quality issues compound over time as systems evolve. **Technical debt** from AI-generated code—shortcuts, poor structure, missing documentation, and other quality deficits—creates ongoing maintenance costs that erode initial productivity gains.

They quantified technical debt by tracking **code churn**: how often code was modified after initial generation. AI-generated code had three point two times higher churn than human-written code, indicating it required more frequent modification. They also tracked **bug density**: bugs per thousand lines of code. AI-generated code had two point one times higher bug density, with bugs discovered over months rather than caught during initial testing.

The cumulative effect was that AI-generated code consumed disproportionate maintenance resources. Although it represented thirty-two percent of the codebase, it accounted for fifty-nine percent of bug fixes, forty-seven percent of code review time, and sixty-one percent of security incidents. These downstream costs offset much of the initial productivity gain.

This led them to reconsider their evaluation framework. Instead of measuring only immediate metrics like test pass rate and time to completion, they implemented **lifetime value tracking** that estimated the total cost of code ownership including generation, review, debugging, maintenance, and eventual replacement. Under this framework, some AI-generated code had negative lifetime value—it cost more to maintain than it saved during generation.

## Human-AI Collaboration Patterns

The most successful code generation deployments treat AI as a **collaboration partner** rather than a replacement for developers. Measuring collaboration effectiveness requires different metrics than measuring fully autonomous code generation.

The fintech company experimented with several collaboration modes. In **suggestion mode**, the AI provided code suggestions that developers could accept, reject, or modify. In **completion mode**, the AI completed partial code that developers started. In **generation mode**, the AI generated full functions from specifications. They measured acceptance rates, modification rates, and productivity gains for each mode.

Suggestion mode had the highest acceptance rate (seventy-nine percent) but smallest per-suggestion impact because suggestions were small code snippets. Completion mode had moderate acceptance (sixty-four percent) and moderate impact. Generation mode had lowest acceptance (forty-one percent) but highest impact when code was accepted. However, generation mode also had the highest downstream costs when code was accepted but later needed significant revision.

They found that the optimal collaboration pattern varied by developer experience and task complexity. Senior developers preferred suggestion and completion modes that left them in control, while junior developers preferred generation mode that provided more structure. For high-risk code, all developers preferred suggestion mode. For low-risk boilerplate, all preferred generation mode.

## Measuring the Right Things

The deeper lesson from code generation metrics is that measuring only immediate functional correctness gives false confidence about production readiness. Code that compiles and passes tests might still be insecure, inefficient, unmaintainable, or accumulate technical debt that erodes long-term productivity. Comprehensive evaluation requires measuring code quality, security, performance, developer productivity including downstream costs, and long-term maintenance burden.

This understanding of code generation metrics completes our survey of task-specific evaluation approaches and bridges naturally to the broader question of how to design evaluation strategies that align AI system quality with actual business value, which we address in the next chapter.

