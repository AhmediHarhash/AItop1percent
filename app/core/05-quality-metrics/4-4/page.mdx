# 4.4 â€” Tool-Calling and Function-Execution Metrics

On January 18th, 2026, a financial services startup discovered that their GPT-5-powered portfolio management agent was executing trades with incorrect security identifiers in nine percent of transactions. The company had raised fifty-three million dollars to automate wealth management for high-net-worth individuals, deployed their agent to manage four hundred and twenty-seven portfolios totaling one point three billion dollars in assets, and tested tool-calling functionality through unit tests that verified the agent could successfully invoke each available function. Their testing showed that the agent selected the correct trading tool ninety-six percent of the time and formatted parameters according to API schemas with ninety-eight percent accuracy. What they never measured was semantic parameter correctness: whether the security identifiers, quantities, and order types made sense given the user's intent and portfolio context. The problem surfaced when a client's annual tax review revealed that the agent had purchased shares of a biotechnology company with ticker symbol "TECH" when the client had requested technology sector exposure, which the agent should have interpreted as a sector allocation rather than a literal ticker symbol. The subsequent audit found hundreds of similar mistakes where syntactically correct tool calls produced semantically wrong outcomes. Parameter values passed schema validation, API calls executed successfully, but the trades did not accomplish what users intended. The engineering team had optimized for technical function execution while completely ignoring the semantic correctness that determines whether tool calls achieve their intended purposes.

The failure exposed the critical distinction between tool calls that work and tool calls that do what you meant. Every AI agent system depends on translating natural language intentions into precise function calls with correctly specified parameters, then interpreting function responses appropriately to guide subsequent actions. The financial services startup proved that syntactic correctness provides almost no guarantee of semantic correctness. Their agent demonstrated fluent API manipulation while systematically misunderstanding user intentions in ways that standard testing never caught. You cannot evaluate tool-calling quality by checking whether function signatures match or whether API calls return success codes. You need metrics that capture whether the agent selected the right tool among alternatives, whether parameter values make sense semantically, whether the agent handles edge cases and errors appropriately, and whether chained tool calls compose correctly to achieve complex goals.

## Tool Selection Accuracy and Appropriateness

The first challenge in tool use is choosing which tool to invoke from the available set. Modern agents often have access to dozens or hundreds of functions spanning different capabilities, APIs, and abstraction levels. Selecting the wrong tool either fails immediately or succeeds in accomplishing something other than what the task requires. The financial services agent had access to forty-seven trading and portfolio management functions. It selected the correct function ninety-six percent of the time on test cases, but this metric hid important patterns about when and why selection failures occurred.

**Function selection accuracy** measures what fraction of tool invocation decisions choose the function that experts would select for the given task context. You evaluate this through test cases where human annotators identify the correct tool from the available set, then measure whether the agent's selection matches. This metric provides a headline number for tool selection quality but tells you nothing about the nature of selection errors or about ambiguous cases where multiple tools could potentially work. The financial services agent achieved ninety-six percent selection accuracy on their test suite, which comprised mostly straightforward cases with clear correct tools. Production usage introduced many more ambiguous scenarios where selection difficulty was much higher.

**Selection accuracy stratified by ambiguity** reveals how tool choice quality degrades when multiple tools could potentially address a need. You categorize test cases into unambiguous cases with one clear correct tool, moderately ambiguous cases with two to three plausible tools, and highly ambiguous cases where many tools might work with different tradeoffs. Measuring accuracy within each stratum shows whether your agent handles only obvious cases or makes intelligent choices even when the decision is subtle. The financial services agent showed ninety-eight percent accuracy on unambiguous cases, seventy-nine percent on moderately ambiguous cases, and fifty-three percent on highly ambiguous cases, revealing that selection quality collapsed precisely when judgment mattered most.

**Tool confusion matrix analysis** identifies systematic selection errors by examining which incorrect tools the agent chooses when it fails. If your agent consistently confuses tool A and tool B, you have either a capability problem where the agent cannot distinguish their purposes or a naming problem where similar names create confusion. You build a confusion matrix showing actual versus predicted tool selections, then analyze off-diagonal patterns. The financial services agent confused several pairs of similarly named functions, like "place market order" versus "place limit order" and "get portfolio value" versus "get account balance," suggesting that function naming and description clarity contributed significantly to selection errors.

**Context sensitivity in tool selection** measures whether the agent appropriately adjusts tool choices based on conversation history, user preferences, and current system state. The same user utterance might require different tools depending on context. "Buy one hundred shares" requires different trading functions depending on whether the portfolio already has a position, whether the market is open, and what the user's risk preferences are. You evaluate context sensitivity by creating test cases where appropriate tool choice depends on context, then measuring whether the agent selects correctly. The financial services agent showed weak context sensitivity, often selecting default tools without adequately considering portfolio state or user preferences that should have influenced the choice.

**Tool capability boundary recognition** measures whether the agent correctly identifies when no available tool can accomplish the requested task. Attempting to force unsuitable tools to handle out-of-scope requests leads to errors or inappropriate workarounds. Better agents recognize capability boundaries and communicate clearly when tasks exceed available tools. You test this by requesting actions that fall outside your tool set, measuring whether the agent appropriately declines or escalates rather than misusing available tools. The financial services agent sometimes attempted creative but inappropriate combinations of available tools when the correct response would have been acknowledging that the requested action was not supported.

## Parameter Correctness and Semantic Validity

Selecting the correct tool only solves half the problem. The agent must then specify parameters that make the tool call accomplish its intended purpose. Parameter correctness operates at multiple levels: syntactic correctness where values match expected types and formats, schema correctness where parameters satisfy validation rules, and semantic correctness where values make sense given task context and user intent. The financial services agent passed syntactic and schema validation almost perfectly but failed semantic correctness catastrophically.

**Syntactic parameter correctness** measures whether parameter values match expected types and formats. String parameters should be strings, numbers should be numeric, dates should follow date formats, enumerations should use allowed values. This is the easiest form of correctness to achieve and measure because it requires only checking values against type definitions and format specifications. Modern function calling systems typically enforce syntactic correctness automatically through type systems or schema validation. The financial services agent achieved ninety-eight percent syntactic correctness, with the remaining two percent representing cases where the agent produced malformed values that API validation rejected.

**Schema constraint satisfaction** measures whether parameters satisfy validation rules beyond basic types. Numeric parameters might require positive values or specific ranges. String parameters might require minimum lengths or pattern matching. Object parameters might require specific required fields. You measure constraint satisfaction by applying schema validation to agent-generated parameters, tracking violation frequency and types. The financial services agent satisfied schema constraints at ninety-six percent, slightly lower than syntactic correctness because constraints sometimes involved subtle rules that the agent missed.

**Semantic parameter correctness** measures whether parameter values make sense given task context and user intent, regardless of whether they technically satisfy schemas. This is the hardest and most important form of correctness to measure. A security identifier that passes format validation might reference a completely inappropriate security. A quantity that satisfies numeric range constraints might be ten times larger than intended. A date that matches date format might be ten years in the past. You measure semantic correctness through expert annotation of whether parameter values reflect appropriate interpretation of user intent. The financial services agent achieved only seventy-one percent semantic correctness despite near-perfect syntactic and schema correctness, revealing a massive gap between technical validity and meaningful accuracy.

**Parameter value plausibility** measures whether parameters fall within expected distributions even when no hard constraints exist. If historical trading quantities average between one hundred and five thousand shares but the agent suddenly generates a trade for five hundred thousand shares, you have a plausibility problem even if the quantity technically passes validation. You measure plausibility by comparing generated parameters against historical distributions, flagging outliers for review. The financial services agent generated implausible parameter values in fourteen percent of tool calls, including extremely large quantities, unusual order types for routine transactions, and dates far outside normal ranges.

**Cross-parameter consistency** measures whether parameter values make sense in combination even if each individually validates. An order to buy shares combined with a sell order type creates inconsistency. A limit price far above current market price combined with a buy order suggests the agent misunderstands the interaction between parameters. You evaluate consistency through rules that express parameter relationships and through human judgment of whether parameter combinations achieve coherent semantics. The financial services agent showed cross-parameter inconsistencies in eight percent of multi-parameter tool calls, most frequently involving mismatches between order types, quantities, and price specifications.

## Entity Resolution and Reference Accuracy

Many tool parameters reference entities: users, objects, records, or resources that exist in external systems. The agent must correctly resolve natural language references to specific entity identifiers. This resolution problem creates a major source of semantic parameter errors because entity identifiers often have non-obvious formats and similar entities require careful disambiguation. The financial services agent's TECH ticker symbol mistake exemplified entity resolution failure where the agent mapped language to the wrong entity identifier despite syntactic correctness.

**Entity identifier correctness** measures whether the agent uses the right identifier for referenced entities. When users mention "my technology holdings" or "the Apple stock," the agent must resolve these references to correct security identifiers. You evaluate correctness by comparing agent-selected identifiers against ground truth annotations, measuring what fraction of entity references resolve correctly. The financial services agent achieved eighty-three percent entity identifier correctness, with failures typically involving ambiguous references, partial entity names, or misunderstanding the entity type being referenced.

**Entity disambiguation quality** measures how the agent handles references that could match multiple entities. If users mention "Apple," do they mean Apple Inc or Apple Hospitality REIT? If they mention "tech sector," do they mean a specific sector ETF or a generic request for technology exposure? You test disambiguation by creating intentionally ambiguous references, measuring whether the agent asks for clarification, makes reasonable default choices, or confidently selects wrong entities. The financial services agent showed poor disambiguation, frequently making confident but incorrect entity selections rather than requesting clarification when references were ambiguous.

**Entity reference consistency** measures whether the agent maintains consistent entity resolution across multiple references within a task or conversation. If a user discusses "the biotech position" in one turn and "that position" in the next turn, both references should resolve to the same security. You evaluate consistency by tracking entity mentions across conversation context, measuring whether resolution stays stable. The financial services agent sometimes resolved the same entity differently across turns, creating confusion where instructions about one security were applied to another.

**Entity existence verification** measures whether the agent validates that referenced entities actually exist before invoking tools with them. Using a nonexistent security identifier in a trade attempt might fail gracefully or might create errors that corrupt workflow state. Better agents verify entity existence when references are uncertain. You measure verification behavior by introducing references to nonexistent entities and tracking whether agents detect the problem before or after attempted tool use. The financial services agent rarely verified entity existence proactively, often attempting tool calls with invalid identifiers and handling failures reactively rather than preventing them.

## Error Handling and Recovery in Tool Execution

Tool calls fail for many reasons: invalid parameters, service unavailability, authorization problems, state conflicts, or transient errors. How agents handle these failures determines whether temporary problems become permanent task failures. The financial services agent demonstrated weak error handling, treating most failures as terminal rather than attempting recovery strategies that could have salvaged many failed tasks.

**Error detection rate** measures whether the agent recognizes that tool calls failed. This might seem obvious but many agents have difficulty distinguishing between success, partial success, and failure, particularly when tools return complex responses or when failures manifest as unexpected success responses rather than explicit errors. You measure detection by deliberately triggering tool failures and tracking whether the agent's subsequent behavior indicates awareness of the failure. The financial services agent correctly detected explicit API errors but missed cases where trades executed at unexpectedly poor prices or where state changes indicated problems despite success responses.

**Error classification accuracy** measures whether the agent correctly identifies error types when failures occur. Transient network errors require retry. Invalid parameter errors require correction. Authorization failures require escalation. State conflict errors might require replanning. You evaluate classification by analyzing agent responses to different error types, measuring whether recovery strategies match error characteristics. The financial services agent poorly classified errors, often retrying errors that required parameter correction and giving up on errors that simple retry would have resolved.

**Retry strategy appropriateness** measures whether the agent retries failed tool calls when appropriate and avoids retrying when inappropriate. Transient errors often resolve with simple retry. Permanent errors will fail repeatedly. Intelligent agents distinguish these cases and apply exponential backoff or limit retry attempts. You measure retry behavior by triggering different error types, tracking retry attempts, and evaluating whether strategies match error characteristics. The financial services agent sometimes retried indefinitely on permanent errors and sometimes gave up immediately on transient errors, showing no systematic retry logic.

**Error recovery success rate** measures what fraction of tool call failures the agent successfully recovers from without task failure. High recovery rates indicate robust error handling that prevents temporary problems from cascading. Low recovery rates indicate fragile workflows where single failures often doom entire tasks. You measure recovery by tracking tool call failures and downstream task outcomes, calculating how often failed calls are followed by eventual task success through alternative approaches or retry. The financial services agent recovered successfully from only thirty-one percent of tool call failures, with most failures leading to task abandonment or escalation.

**Graceful degradation capability** measures whether the agent can accomplish partial task goals when tool failures prevent complete success. If one tool in a workflow fails, can the agent complete other steps and communicate what was accomplished versus what remains blocked? You evaluate graceful degradation by blocking specific tools and measuring whether agents achieve partial success or treat all failures as total. The financial services agent showed minimal graceful degradation, typically abandoning entire tasks when any step failed rather than completing achievable portions.

## Tool Response Interpretation and Context Integration

Successful tool calls return responses that the agent must interpret correctly to guide subsequent actions. Response handling quality determines whether agents incorporate tool results appropriately into their reasoning and later tool calls. The financial services agent sometimes misinterpreted ambiguous responses or failed to extract key information from complex return values, leading to downstream errors that originated in response handling rather than initial tool selection or parameter specification.

**Response parsing accuracy** measures whether the agent correctly extracts information from tool return values. APIs return structured data that the agent must parse according to response schemas. Parsing failures might miss important fields, misinterpret data types, or assume structure that does not always exist. You measure parsing accuracy by analyzing whether agent behavior reflects accurate understanding of response contents. The financial services agent showed parsing errors in six percent of tool responses, particularly when responses included optional fields or variable structure that the agent did not handle robustly.

**Response semantic interpretation** measures whether the agent understands the meaning of response values beyond mechanical parsing. A status code might be "success" but a price execution might be far from requested price, indicating a problem despite technical success. A query might return empty results that the agent should interpret as information absence rather than error. You evaluate semantic interpretation through analysis of how agent reasoning and subsequent actions reflect response meaning. The financial services agent sometimes treated poor execution quality as complete success because the trade technically executed, missing signals that results were unsatisfactory.

**Response context integration** measures whether the agent incorporates tool results appropriately into its understanding of task state and conversation context. Tool responses update the agent's knowledge of system state and should influence subsequent tool selection and parameter choices. You evaluate integration by tracking whether later agent actions reflect information from earlier tool responses. The financial services agent showed weak context integration, sometimes repeating tool calls with identical parameters despite responses indicating those parameters were problematic, suggesting the agent failed to learn from tool feedback.

**Handling unexpected response formats** measures how agents deal with responses that differ from expected schemas. APIs sometimes return error formats instead of success formats, include additional optional fields, or use slightly different structure than documented. Robust agents handle format variations gracefully. Brittle agents fail when responses deviate from expectations. You measure robustness by introducing schema variations and tracking whether agents continue functioning. The financial services agent handled only precisely expected response formats, failing when APIs returned valid but slightly unexpected structures.

**Multi-response synthesis** measures whether agents correctly combine information from multiple tool calls. Complex tasks often require calling several tools and synthesizing their responses into coherent understanding. An agent might query portfolio holdings, check current prices, and retrieve user preferences before making trading decisions. Each response contributes information that must be integrated correctly. You evaluate synthesis by analyzing whether decisions reflect appropriate combination of multiple response contexts. The financial services agent sometimes failed to synthesize responses appropriately, making decisions based only on the most recent tool result while ignoring relevant earlier information.

## Chained Tool Calls and Workflow Composition

Many tasks require sequences of tool calls where each call depends on previous results. Workflow composition quality determines whether agents can chain tools effectively to accomplish complex goals. The financial services agent needed to perform multi-step workflows like rebalancing portfolios, which required querying holdings, calculating required trades, checking available funds, and executing multiple transactions in dependency order. Composition failures caused the agent to execute steps in wrong order, use stale information, or skip dependency checks.

**Dependency ordering correctness** measures whether agents sequence tool calls appropriately when dependencies exist between steps. You cannot execute a trade before checking available funds. You cannot rebalance without first querying current allocation. You evaluate ordering by analyzing whether agents attempt tool calls before their dependencies are satisfied. The financial services agent showed ordering errors in eleven percent of multi-step workflows, attempting actions before gathering necessary information or skipping validation steps that should precede execution.

**Information flow correctness** measures whether data flows properly through tool call chains. Parameters for later tools should use response values from earlier tools when appropriate. An agent that calls a quote tool, receives a price, then calls a trade tool with a different price shows information flow failure. You evaluate flow by checking whether parameter values in later tool calls appropriately reference earlier response values. The financial services agent showed information flow errors in seventeen percent of chained workflows, using stale or incorrect values instead of propagating responses correctly.

**Transaction boundary handling** measures whether agents appropriately group tool calls into transactional units that should succeed or fail together. Some operations should be atomic: if you cannot complete all steps, you should not complete any. Other operations allow partial success. You evaluate transaction handling by analyzing whether agents implement appropriate atomicity guarantees and whether they clean up partial state when workflows fail mid-execution. The financial services agent lacked systematic transaction handling, sometimes leaving portfolios in inconsistent states when multi-step operations failed partway through.

**Conditional execution logic** measures whether agents correctly implement branching in workflows where later tool calls depend on earlier results. If a balance check fails, skip the trade execution. If a security is not found, search by alternative identifier. You evaluate conditional logic by testing workflows with varying conditions, measuring whether agents execute appropriate code paths. The financial services agent showed weak conditional logic, often executing tool calls unconditionally when response values should have triggered different paths.

**Workflow efficiency** measures whether agents accomplish tasks with minimal tool calls. Unnecessary calls increase latency, cost, and error surface area. You measure efficiency by comparing agent tool call counts against optimal paths defined by experts. The financial services agent averaged one point six times as many tool calls as optimal paths, with excess primarily from redundant queries and failure to cache information that remained valid across multiple uses.

## Testing Infrastructure and Failure Mode Discovery

Measuring tool-calling quality requires infrastructure that goes beyond traditional model evaluation. You need ability to mock or simulate tools, inject failures, track parameter values, analyze tool call sequences, and attribute failures to specific decisions. The financial services startup initially tested tool calling through unit tests that verified each tool could be invoked successfully, missing all the semantic correctness issues and workflow composition problems that caused production failures.

**Tool mocking and simulation** enables controlled testing without depending on production systems. Mock tools implement the same interfaces as real tools but return predetermined responses, allowing you to test agent behavior under specific conditions. You need ability to configure mock behavior per test, inject failures, introduce latency, and verify that agents call tools with expected parameters. The financial services agent eventually built comprehensive mocking that enabled testing trading logic without executing real trades, but this came only after production incidents forced testing infrastructure investment.

**Parameter fuzzing and boundary testing** systematically explores edge cases in parameter spaces. You generate parameter values at boundaries, outside valid ranges, with unusual but valid combinations, and in corner cases that might reveal bugs. Fuzzing discovers robustness problems that hand-crafted tests miss. The financial services agent's testing initially used only typical values, missing edge cases where the agent generated invalid or implausible parameters. Adding fuzzing revealed numerous parameter correctness issues.

**Adversarial tool response testing** explores how agents handle unusual or malicious tool responses. Tools might return empty results, extremely large data sets, ambiguous status codes, or responses designed to confuse agents. Robust agents handle adversarial responses gracefully. Brittle agents fail or make dangerous assumptions. You test this by deliberately crafting challenging responses and measuring agent behavior. The financial services agent proved vulnerable to responses that technically satisfied schemas but included misleading or ambiguous information.

**Workflow trajectory analysis** examines complete tool call sequences to identify patterns in failures. You log every tool invocation with parameters and responses across successful and failed tasks, then cluster trajectories to find common failure patterns. This analysis reveals systematic errors that manifest only in specific workflow contexts. The financial services agent's trajectory analysis revealed that most semantic parameter errors occurred in specific workflow patterns where certain tool combinations created parameter disambiguation challenges.

**Counterfactual parameter testing** explores whether alternative parameter values would have led to better outcomes. When an agent makes a tool call with parameters that lead to failure or suboptimal results, you test whether different plausible parameters would have succeeded. This identifies cases where the agent selected parameters within valid ranges but made poor choices given available information. The financial services agent's counterfactual analysis showed that in forty-two percent of semantic parameter errors, different plausible parameter values would have produced correct outcomes, suggesting improved parameter selection logic rather than fundamental capability gaps as the primary improvement opportunity.

## Domain-Specific Tool Use Metrics

Different application domains create specialized tool-calling challenges. Financial services, healthcare, e-commerce, and customer service applications each have domain-specific correctness criteria beyond general tool use quality. The financial services startup needed metrics around regulatory compliance, risk management, and execution quality that generic tool-calling metrics did not capture. Understanding domain-specific requirements helps you extend general tool-calling measurement appropriately.

**Regulatory compliance in tool use** matters for domains like finance and healthcare where tool calls must satisfy legal requirements. You measure whether trading tools are invoked with required disclosures, whether patient data tools respect privacy regulations, whether tool call logging satisfies audit requirements. The financial services agent needed to ensure all trades satisfied regulatory requirements around order types, execution venues, and client communication, requiring specialized compliance checking that standard tool metrics did not address.

**Risk-appropriate tool selection** ensures agents choose tools with risk profiles matching task requirements and user risk tolerance. Aggressive trading tools should not be used for conservative portfolios. High-risk diagnostic tools should not be suggested for low-risk symptoms. You measure whether tool selection reflects appropriate risk management given user profiles and task context. The financial services agent sometimes selected high-risk order types for conservative accounts, violating risk appropriateness despite technical correctness.

**Execution quality metrics** measure outcomes beyond tool call success. Trades might execute successfully but at poor prices. Orders might fill but with high market impact. You measure execution quality through domain-specific metrics like price improvement, slippage, and market impact. The financial services agent successfully executed ninety-four percent of requested trades but achieved poor execution quality in twenty-three percent of those successes, measured by deviation between requested and achieved prices.

**Cost efficiency in tool use** matters when tools have variable cost implications. Some tools are expensive to invoke, others cheap. Some achieve goals efficiently, others waste resources. You measure whether agents select cost-effective tools and whether they avoid unnecessary expensive calls when cheaper alternatives exist. The financial services agent sometimes used premium data feeds when free data would suffice and sometimes executed multiple small trades when block trades would reduce transaction costs.

Your tool-calling measurement framework must balance general quality dimensions that apply across domains with specialized metrics that capture what makes tool use effective in your specific context. The financial services startup needed both generic tool correctness metrics and financial-specific metrics around execution quality, risk management, and regulatory compliance. Building this comprehensive measurement requires deep domain understanding combined with systematic engineering of evaluation infrastructure.

Tool-calling quality ultimately determines whether agents can translate intentions into correct actions and tool results into appropriate subsequent decisions. Every agent system depends on this translation layer working reliably. The financial services startup learned that syntactic correctness provides almost no guarantee of semantic correctness, that testing tool invocation mechanics reveals little about whether tools accomplish intended purposes, and that comprehensive measurement of tool selection, parameter semantics, error handling, and workflow composition is essential to deploying capable agent systems.
