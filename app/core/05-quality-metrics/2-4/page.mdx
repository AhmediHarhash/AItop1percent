# 2.4 — Usefulness: Task Completion and User Goal Alignment

On January 19, 2026, an enterprise software company providing AI-powered customer support tools faced a crisis that had nothing to do with accuracy. Their system generated responses to customer support tickets using a RAG architecture. Every response was factually correct, grounded in official documentation, and comprehensive. The company had spent months optimizing for correctness, groundedness, and completeness. Their evaluation metrics showed 96% factual accuracy, 94% citation accuracy, and 89% completeness scores. Customer satisfaction with the AI system, however, was 61% and dropping. Human support agents were overriding AI-generated responses at a rate of 43%. When the product team conducted user interviews, they discovered the problem. One support agent said, "The AI gives me a wall of technically correct information, but I still have to figure out what to tell the customer and how to solve their problem. It is faster for me to just write the response myself." The AI was optimized for quality dimensions that did not align with the user's actual goal. Agents did not need comprehensive documentation—they needed actionable next steps. They did not need grounded citations—they needed clear resolution paths. The outputs were high quality by every metric the team measured, but they were not useful.

This is the usefulness gap. You can build a system that scores well on correctness, groundedness, and completeness and still fail because those dimensions do not add up to outputs that help users accomplish their goals. **Usefulness** is whether the user can act on the output to complete their task. It is the most product-centric quality dimension and the one most closely tied to actual user value. It is also the dimension that requires the deepest understanding of user workflows, expertise levels, and job-to-be-done.

## Why Usefulness Is Different From Other Quality Dimensions

Correctness, groundedness, and completeness are properties of the output evaluated against objective criteria. Usefulness is a property of the output evaluated against the user's goal. An output can be objectively high quality and subjectively useless, or objectively flawed and subjectively valuable.

A customer asks a chatbot, "How do I cancel my subscription?" A correct but not useful response: "Subscription cancellation is governed by Section 7.2 of the Terms of Service, which specifies that users may terminate service at any time subject to the notice period defined in Section 3.4, with refund eligibility determined by the cancellation date relative to the billing cycle as described in Section 9.1." This is accurate, grounded, and complete. It is also useless because the user cannot act on it without reading three sections of legal text.

A useful response: "You can cancel your subscription in your account settings under Billing. Click Cancel Subscription and confirm. You will have access until the end of your current billing period. If you cancel before February 5, you will receive a proactive refund for unused time." This is actionable, specific, and aligned with the user's goal of actually canceling the subscription.

The difference is not correctness. Both responses can be factually accurate. The difference is **task alignment**. The useful response is structured around the task the user is trying to complete. It provides procedural steps, not policy citations. It includes the specific deadline relevant to this user, not general refund policy. It anticipates the follow-up questions a user would have.

Usefulness is contextual. What is useful to a novice is different from what is useful to an expert. What is useful for a decision-maker is different from what is useful for someone executing a task. What is useful in an urgent situation is different from what is useful during research. You cannot evaluate usefulness without understanding who the user is and what they are trying to do.

## Task Completion Rate as the Ultimate Usefulness Metric

The most direct way to measure usefulness is to measure whether users successfully complete their tasks after using your AI system. **Task completion rate** is the percentage of users who accomplish their goal. For a customer support chatbot, it is the percentage of users who resolve their issue without escalating to a human agent. For a code generation tool, it is the percentage of generated code snippets that developers use without substantial modification. For a research assistant, it is the percentage of queries that lead to the user finding the information they needed.

Measuring task completion requires instrumenting your product to track user behavior after they receive an AI-generated output. For customer support, you track whether the user returns with the same issue, whether they request to speak to a human, and whether they rate the interaction as resolved. For code generation, you track whether the developer accepts the code, modifies it, or discards it. For research, you track whether the user clicks through to sources, reformulates their query, or exits the product.

Task completion rate surfaces failures that other metrics miss. A system might generate correct, grounded, and complete answers that users find too complex to apply, too generic to be relevant, or too verbose to extract actionable information from. Those users fail to complete their tasks, and task completion rate declines even as quality metrics remain high.

The enterprise support tool discovered this through behavioral analytics. They tracked how often agents used the AI-generated response without modification, how often they modified it before sending, and how often they discarded it entirely. The modification rate was 43%. When they analyzed the modifications, they found patterns. Agents were deleting background information and policy explanations. They were adding specific steps and concrete examples. They were restructuring from exposition to instruction. The AI was generating documentation when agents needed troubleshooting scripts.

Task completion rate also varies by task complexity and user expertise in ways that reveal whether your system is useful for your actual user base. If task completion is high for simple queries but low for complex ones, your system is useful for basic use cases but not for the cases where users most need help. If task completion is high for expert users but low for novices, your system is useful for people who already know how to use it but not for the users who need the most guidance.

## The Components of Usefulness

Usefulness is not a single property. It is a combination of several factors that together determine whether an output enables task completion.

**Actionability** is whether the output tells the user what to do. Actionable outputs include procedural steps, specific recommendations, or concrete examples. Non-actionable outputs include abstract principles, general context, or theoretical explanations. For users trying to complete a task, actionability is often more important than comprehensiveness. A short list of specific steps is more useful than a detailed explanation of the underlying concepts.

**Clarity** is whether the user can understand the output without ambiguity or cognitive load. Clear outputs use plain language, avoid jargon unless the user is an expert, structure information logically, and highlight the most important points. Unclear outputs bury key information in dense paragraphs, use technical terminology without definition, or present information in an order that does not match the user's mental model of the task.

**Relevance** is whether the output addresses the user's specific situation rather than providing generic information. Relevant outputs incorporate user context—their account status, their previous interactions, their role, their expertise level. Generic outputs provide the same answer to every user regardless of context. A billing question from an enterprise customer with a custom contract requires a different answer than the same question from a consumer on a standard plan. If your system gives both users the same generic answer, it is not useful to either.

**Conciseness** is whether the output provides the necessary information without unnecessary content. Concise outputs respect the user's time and attention. Verbose outputs bury the signal in noise. But conciseness must not come at the cost of completeness—the output needs to be as short as possible while still being complete enough to enable task completion.

**Confidence calibration** is whether the output appropriately conveys certainty or uncertainty. An output that is confidently wrong is less useful than an output that admits uncertainty. A system that tells a user "You are eligible for a refund" when eligibility is actually uncertain creates false expectations and wastes time. A system that says "Based on the information available, you appear to meet the eligibility criteria, but I recommend confirming with customer support" is more useful because it sets appropriate expectations.

A useful output maximizes all these components within the constraints of the task and user context. You cannot optimize each independently—there are trade-offs. Actionability sometimes requires sacrificing depth. Clarity sometimes requires sacrificing precision. Relevance requires access to user context. Your system design needs to navigate these trade-offs based on what your users need to accomplish their goals.

## How Usefulness Varies by User Expertise

The same output can be useful to an expert and useless to a novice, or vice versa. Expert users and novice users have different needs, different knowledge bases, and different workflows. A one-size-fits-all approach to usefulness fails both groups.

**Experts** value density, precision, and direct answers. They do not need background information or step-by-step instructions. They want the specific information they asked for, with enough detail to make decisions, and without extraneous context. An expert developer asking about an API endpoint wants the function signature, parameters, and return type. They do not need an explanation of what an API is.

**Novices** value guidance, explanation, and examples. They need background context to understand the answer. They benefit from step-by-step instructions and from understanding why the steps work, not just what the steps are. A novice developer asking the same API question might need an explanation of how the endpoint fits into the overall API structure, what the parameters mean, and an example of a complete request.

If your system provides expert-level answers to novices, you create a comprehension gap. The answer is correct but not useful because the user cannot understand it well enough to act on it. If your system provides novice-level answers to experts, you create an efficiency gap. The answer is useful in principle but wastes the expert's time with information they already know.

The enterprise support tool served agents with varying expertise. New agents needed detailed troubleshooting steps and explanations of why each step mattered. Experienced agents needed a diagnosis and a quick reference to the resolution procedure. The AI system generated the same output for everyone—comprehensive explanations suitable for new agents but too verbose for experienced agents. The experienced agents, who handled most tickets, found the system not useful and bypassed it.

One solution is explicit user profiles that specify expertise level, and you condition your outputs on that level. Another solution is adaptive outputs that start with a direct answer for experts and include expandable sections with additional context for novices. A third solution is to detect expertise implicitly from user behavior—if a user frequently uses advanced features or technical terminology, you infer they are an expert and adjust output style.

You also need to consider domain-specific expertise separately from system-specific expertise. A user might be an expert in their domain but a novice with your system. A physician using a medical AI is an expert in medicine but might be a novice in how your specific tool structures information. Your outputs need to account for both dimensions.

## Usefulness in Multi-Turn Interactions

Usefulness in a single-turn interaction is different from usefulness across a multi-turn conversation. In conversational AI systems, usefulness includes how well the system handles follow-up questions, clarifications, and iterative refinement.

A user asks a research AI, "What are the main applications of reinforcement learning?" The system provides a comprehensive answer covering robotics, game playing, recommendation systems, and autonomous vehicles. This is useful if the user wanted a broad overview. It is not useful if the user wanted to go deep on one specific application. A useful conversational system recognizes when a user asks a follow-up question like "Tell me more about robotics applications" and provides depth in that area without repeating the overview.

Usefulness in multi-turn interactions also includes **context retention**. If a user asks about contract terms and then asks a follow-up question, the system should remember which contract is being discussed. If the user has to repeat context in every message, the system is not useful for extended interactions.

**Error recovery** is another component of multi-turn usefulness. If the system misunderstands a query or provides an answer that does not match what the user needed, a useful system recognizes this from user feedback or reformulations and corrects course. A system that repeats the same unhelpful answer when the user rephrases the question is not useful.

The enterprise support tool initially operated in single-turn mode. Each ticket received one AI-generated response. When agents needed to iterate—asking for more detail or approaching the problem from a different angle—they had to craft new prompts manually. This made the tool less useful for complex troubleshooting that required multiple rounds of diagnosis. Adding multi-turn conversation capabilities improved usefulness significantly because agents could refine AI outputs iteratively.

## Measuring Usefulness Beyond Task Completion

Task completion rate is the gold standard usefulness metric, but it is not always easy to measure directly. For some applications, task completion happens outside the system where you cannot observe it. For others, task completion is delayed or ambiguous. You need proxy metrics that correlate with usefulness.

**User engagement** is one proxy. Do users return to use the system repeatedly? Do they spend time with the outputs, or do they immediately leave? High engagement suggests usefulness, though engagement alone does not guarantee it—users might engage with a system because they have no alternative, not because it is useful.

**Override rate** measures how often users modify or discard AI-generated outputs. High override rates indicate that the outputs are not useful in their default form. This metric is particularly valuable for AI systems embedded in professional workflows where human users review and edit AI outputs before using them.

**Satisfaction surveys** can ask users directly whether the output helped them accomplish their goal. You present the question immediately after the user receives an output: "Did this answer solve your problem?" or "Were you able to complete your task with this information?" Self-reported usefulness is noisy—users sometimes confuse satisfaction with usefulness—but it provides directional signal.

**Follow-up behavior** reveals whether users needed additional help after receiving an output. Do they immediately issue another query on the same topic? Do they navigate to documentation? Do they contact support? These behaviors suggest the initial output was not sufficient.

**A/B testing** between different output styles or structures can reveal which approaches are more useful. You randomize users to receive outputs with different levels of detail, different formatting, or different inclusion of examples, and you measure task completion or override rates for each variant.

Some teams use **time-to-task-completion** as a usefulness metric. If users accomplish their goals faster after you deploy an AI system or after you improve output formatting, the system is more useful. This requires baseline measurements of task completion time without AI assistance.

## The Danger of Optimizing for the Wrong Goal

The most common usefulness failure is optimizing for a goal that does not align with what users actually need. This happens when product teams make assumptions about user goals without validating them, or when they optimize for metrics that are easy to measure rather than metrics that matter.

The enterprise support tool optimized for comprehensiveness because the team assumed that more complete answers would be more useful. They did not validate this assumption with agents. When they finally did user research, they learned that agents valued speed and actionability over comprehensiveness. Agents were willing to sacrifice complete information for faster ticket resolution. The team had been optimizing for the wrong goal.

Another common misalignment is optimizing for user satisfaction scores without understanding what drives satisfaction. Users might rate an AI system highly because it is entertaining, because it validates their existing beliefs, or because it tells them what they want to hear—none of which necessarily correlate with usefulness. A system that provides blunt, accurate feedback might have lower satisfaction scores but higher usefulness.

Some systems optimize for metrics like engagement time or response length that do not correlate with usefulness. A chatbot that keeps users engaged in conversation might have high engagement metrics but low task completion if the conversation does not lead to resolution. A system that generates long, detailed responses might look impressive but might not be useful if users cannot extract actionable information.

You avoid these traps by defining usefulness in terms of user goals, not system outputs. Start by understanding what users are trying to accomplish. What does success look like from their perspective? What would enable them to complete their task faster or with higher confidence? Then design your system and your metrics around those outcomes.

## Usefulness as a Forcing Function for Product Clarity

Optimizing for usefulness forces you to make your product's value proposition concrete. If you cannot articulate what task users are trying to complete and how your AI system helps them complete it, you cannot optimize for usefulness.

This is why usefulness is the most product-centric quality dimension. Correctness can be evaluated in isolation from product context. You can check whether facts are accurate without knowing what the user will do with them. Usefulness cannot be evaluated in isolation. You must know the user's goal, expertise, context, and constraints.

Building usefulness into your evaluation framework requires cross-functional collaboration. Product managers define user goals and success criteria. Designers identify pain points in user workflows where AI outputs could provide leverage. Engineers instrument the product to measure task completion and user behavior. Researchers conduct user studies to understand what makes outputs useful in practice.

The enterprise support tool transformed their approach by starting with user goals. They conducted observational studies where they watched agents handle tickets. They identified the moments where agents needed help: diagnosing the issue, identifying the resolution procedure, drafting the response, and deciding whether to escalate. They mapped each moment to the type of information that would be most useful. For diagnosis, agents needed decision trees and symptom matching. For resolution, they needed procedural checklists. For response drafting, they needed templates. For escalation decisions, they needed severity criteria.

They restructured their AI system to provide different outputs for different moments in the workflow. Instead of one comprehensive response, they offered modular components: a diagnosis, a resolution procedure, and a draft response. Agents could use whichever components they needed. Override rates dropped from 43% to 18%. Task completion time decreased by 35%. The outputs were not necessarily more correct or more complete than before. They were more useful because they aligned with how agents actually worked.

## Usefulness and the Quality Dimension Hierarchy

Correctness, groundedness, completeness, and usefulness form a hierarchy. Correctness is foundational—an incorrect output cannot be useful. Groundedness enables verification—users need to trust outputs before they will act on them. Completeness ensures nothing critical is missing. Usefulness integrates these dimensions into outputs that enable task completion.

You cannot achieve usefulness by optimizing usefulness alone. If your outputs are useful but incorrect, users will complete tasks incorrectly and lose trust. If your outputs are useful but ungrounded, users cannot verify them, which limits usefulness in high-stakes contexts. If your outputs are useful but incomplete, users might act on partial information and encounter problems later.

But achieving correctness, groundedness, and completeness does not automatically yield usefulness. You need to intentionally design for usefulness by understanding user goals and structuring outputs to support those goals. This requires product thinking, user research, and continuous iteration based on how users actually use your system.

The quality dimensions covered so far—correctness, groundedness, completeness, and usefulness—address what your system outputs and whether those outputs help users. But AI product quality is not only about the outputs. It is also about the behavior of the system across different inputs, different users, and different contexts. The next set of dimensions addresses consistency, robustness, and reliability at the system level.
