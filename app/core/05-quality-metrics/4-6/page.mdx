# 4.6 — Extraction and Structuring Metrics

On August 7, 2025, a legal technology startup processing contracts for enterprise clients discovered that their AI extraction system had corrupted financial data in over three thousand commercial agreements processed during the previous month. The system, built on fine-tuned GPT-4.5 models, was designed to extract key terms from lease agreements, licensing contracts, and service agreements: payment amounts, dates, renewal clauses, liability caps, and termination conditions. The company's monitoring dashboard showed ninety-two percent extraction accuracy, measured by comparing system outputs to a validation set of manually labeled contracts. Their twelve-person engineering team considered this production-ready performance. But a client conducting an internal audit discovered that approximately forty percent of the contracts in their system had at least one material error in extracted financial terms. Some payment amounts were off by factors of ten due to decimal point errors. Some dates were swapped between start dates and end dates. Some liability caps were extracted from example clauses rather than binding terms. The client threatened litigation, other customers demanded audits, and the startup lost eight of their fifteen enterprise accounts within two weeks, a revenue impact of approximately four point eight million annually.

The root cause was a fundamental misunderstanding of what extraction accuracy means and how partial matches distort evaluation metrics. The team had measured accuracy as the percentage of extracted fields that exactly matched human annotations. When a contract had twenty extracted fields and eighteen matched exactly, they counted that as ninety percent accuracy for that contract. This seems reasonable until you realize that their metric treated all fields as equally important and counted all mismatches as equally wrong. A one-character typo in a company name received the same penalty as a hundred-thousand-dollar error in a liability cap. A missing middle initial in a party name counted the same as extracting the wrong renewal date. The metric also gave the model full credit for partial matches where it extracted some but not all components of a structured field, like capturing the payment amount but missing the currency or extracting the start date but not the end date. These partial matches appeared as successes in the overall accuracy number but caused downstream failures when the extracted data fed into financial systems that expected complete, correctly formatted information.

## The Exact Match Trap

When you measure extraction system performance with **exact match accuracy**—the percentage of extracted values that precisely match a gold standard—you are enforcing a brittle standard that penalizes trivial differences while potentially missing important errors. A model that extracts "John Smith" when the reference says "John A. Smith" fails exact match despite capturing the essential information. A model that extracts "ten thousand dollars" when the reference says "10000 USD" fails exact match despite conveying the correct meaning. Meanwhile, a model that extracts "ten thousand dollars" when the contract actually says "one hundred thousand dollars" also fails exact match, but this failure has dramatically different business consequences.

The legal tech startup initially used exact match as their primary metric because it was easy to compute and seemed rigorous. During development, they noticed that their scores were lower than expected due to formatting inconsistencies: their model might extract dates as "March 15, 2025" while humans annotated them as "2025-03-15", causing exact match failures despite semantic equivalence. They addressed this by adding normalization rules that standardized date formats, number formats, and text casing before comparison. This improved their metrics substantially, making the model look production-ready. But the normalization masked genuine errors where the model extracted wrong information in the right format.

The alternative to exact match is **fuzzy matching** that allows for minor differences in extracted values. Common approaches include edit distance thresholds (counting matches if strings differ by fewer than N characters), token overlap (counting matches if most words are the same), and semantic similarity using embeddings (counting matches if vector representations are close). These methods reduce brittleness but introduce ambiguity: how fuzzy is too fuzzy? If you accept matches within edit distance of three, does that mean "Smith" matches "Smyth" (edit distance two) but also "Smithe" (edit distance one)? Does "one thousand" match "one hundred" (edit distance seven in characters but only one word different)? The threshold you choose fundamentally changes what counts as correct.

## Field-Type-Specific Accuracy

A more sophisticated approach is to use **field-type-specific evaluation** that applies different matching criteria to different types of extracted data. For named entities like people and companies, you might use fuzzy string matching that tolerates minor spelling variations and formatting differences. For dates, you might require exact match on the underlying date value but allow flexible formatting. For monetary amounts, you might require exact match on the numeric value and currency but allow variations in how they are expressed. For free-text fields like termination clauses, you might use semantic similarity or human judgment of whether the extracted text preserves the essential meaning.

The legal tech company rebuilt their evaluation framework using this approach. They categorized their extracted fields into types: parties (names of contracting entities), dates (start, end, renewal, notice), financials (payment amounts, liability caps, penalties), clauses (termination conditions, assignment restrictions, confidentiality terms), and metadata (contract type, jurisdiction, execution date). Each type got custom matching logic. Party names used fuzzy string matching with edit distance threshold of two. Dates required exact match on parsed date values. Financials required exact match on normalized numeric values and currency codes. Clauses used a hybrid of keyword presence and human review for ambiguous cases.

This revealed that their overall ninety-two percent accuracy concealed huge variation across field types. Party names hit ninety-seven percent accuracy with fuzzy matching. Dates achieved ninety-four percent. But financials were only at seventy-eight percent, and clauses were at sixty-four percent. The aggregate metric had been dominated by the high-performing categories, hiding catastrophic failure in the high-stakes financial and legal terms. When they weighted categories by business impact rather than counting all fields equally, their effective accuracy dropped to sixty-one percent. This matched much better with client experience—clients cared enormously about getting payment terms and liability caps right, and cared relatively little about minor variations in how party names were formatted.

## Schema Compliance and Structural Validity

Beyond individual field accuracy, extraction systems must produce outputs that conform to expected **schemas**—the structure and format requirements that downstream systems depend on. A schema might specify that every contract extraction must include at least two parties, at least one date field, and at least one financial term. It might require that dates are valid calendar dates, that monetary amounts are positive numbers with valid currency codes, that party names are non-empty strings. When your extraction system violates these schema requirements, it produces outputs that break downstream processes even if the individual extracted values are accurate.

The legal tech startup experienced this with date fields. Their model sometimes extracted dates in ambiguous formats like "3/4/2025" without specifying whether this meant March 4 or April 3. Their validation pipeline used heuristics to disambiguate, assuming American format (month first) for US contracts and European format (day first) for others. This worked most of the time but catastrophically failed for edge cases: contracts between US and European parties, contracts executed in one jurisdiction but governing activity in another, contracts with mixed date formats where some dates followed American convention and others European. The schema required ISO 8601 formatted dates, but the model produced ambiguous strings, and the normalization layer introduced errors trying to resolve the ambiguity.

Measuring **schema compliance** requires treating structural requirements as hard constraints, not soft preferences. You count an extraction as fully compliant only if it includes all required fields, all fields have values of the correct type, all fields satisfy any specified constraints (like dates being in the future for contract start dates, or monetary amounts being positive), and all relationships between fields are valid (like end dates coming after start dates). This is much stricter than field-level accuracy because a single missing required field or constraint violation causes the entire extraction to fail.

When the legal tech company added schema compliance as a primary metric, their numbers dropped dramatically. Only sixty-eight percent of extractions were fully schema-compliant. The most common violation was missing required fields: the model would extract some but not all mandatory data points. The second most common was type errors: extracting text that could not be parsed into the required format, like "approximately 90 days" when the schema required an integer number of days. The third was constraint violations: extracting dates that were in the past when the contract specified future performance, or extracting negative amounts for payment values. Each of these violations caused downstream failures when the extracted data fed into contract management systems that expected valid, complete records.

## Partial Match Evaluation

Many extraction tasks produce complex structured outputs where individual components might be correct even if the overall extraction is incomplete or partially wrong. Consider extracting a postal address: you might correctly extract the street number and street name but miss the apartment number, or correctly extract the city and state but get the zip code wrong. How should you score these partial matches?

The standard approach is to calculate **precision and recall at the field level**. Precision measures what fraction of the fields your model extracted are correct. Recall measures what fraction of the fields that should have been extracted were actually found. If a contract has five relevant dates and your model extracts four of them correctly plus one that is wrong, you have eighty percent precision (four correct out of five extracted) and eighty percent recall (four correct out of five that exist). The F1 score combines these into a single metric.

This field-level evaluation is more nuanced than document-level accuracy (did you get the whole document right or wrong) but less granular than token-level evaluation (did you get every word right). The challenge is defining what constitutes a field. In the address example, is the address one field or six (street number, street name, apartment, city, state, zip)? If you treat it as one field, partial matches score as complete failures. If you treat it as six fields, you can get high accuracy by extracting the easy parts (city and state) while missing the hard parts (apartment number).

The legal tech company addressed this with **hierarchical evaluation** that scored extractions at multiple levels of granularity. At the highest level, they measured document-level schema compliance: did the extraction satisfy all structural requirements? At the middle level, they measured field-category accuracy: what percentage of party fields were correct, what percentage of date fields, what percentage of financial fields? At the lowest level, they measured component-level extraction for complex fields: for a payment term like "ten thousand dollars per month for twelve months", did they extract the amount, currency, frequency, and duration correctly, or just some of these components?

This revealed different failure modes at different levels. Document-level compliance failed mostly due to missing required fields—the model would extract ninety percent of what was present but miss one critical field that made the whole record invalid. Field-category accuracy was dragged down by systematic errors in specific categories, particularly complex clauses with conditional logic. Component-level accuracy showed that the model was good at extracting salient numbers and dates but bad at extracting the relationships between them, like whether a payment was per month or per year, or whether a date was a deadline or a starting point.

## Cascading Errors in Downstream Pipelines

Extraction errors do not occur in isolation—they propagate through downstream systems, often amplifying in impact as they go. A small error in extracted data can cause calculation errors, routing errors, compliance errors, and decision errors in systems that consume the extraction output. Understanding and measuring these **cascade effects** is essential for evaluating extraction quality in production contexts.

The legal tech startup discovered this when analyzing why certain contracts caused financial reconciliation failures. Their contract management system used extracted payment terms to set up automatic invoicing and revenue recognition. When the extraction system misidentified payment frequency—extracting "monthly" when the contract specified "quarterly"—the invoicing system sent bills at three times the correct frequency. Finance teams caught some of these errors during manual review, but others slipped through until customers complained about incorrect invoices. Each error consumed hours of customer support time, damaged client relationships, and created accounting corrections that rippled through financial reporting.

The cascade pattern worked like this: extraction error leads to data error in the downstream system, which leads to automated action based on that error, which leads to real-world impact (incorrect invoice, missed deadline, wrong payment), which leads to human intervention to correct the error and its consequences. The cost of the cascade often exceeded the cost of the initial extraction error by orders of magnitude. A few seconds of inference producing a wrong output could trigger hours of human work across multiple departments.

Measuring cascade effects requires **end-to-end evaluation** that traces extraction errors through downstream systems to measure their ultimate impact. This is much harder than measuring extraction accuracy in isolation because it requires instrumenting the entire pipeline and establishing causal connections between extraction errors and downstream failures. The legal tech company built this capability by adding unique identifiers to each extraction that were preserved through the pipeline, allowing them to trace downstream errors back to specific extraction failures. They then calculated metrics like error amplification ratio (how much a one-dollar extraction error costs in downstream correction effort), error detection lag (how long it takes for extraction errors to surface as visible problems), and error containment rate (what fraction of extraction errors are caught before they cause downstream impact).

This analysis revealed that their biggest problem was not low extraction accuracy per se but rather low error detectability. Many extraction errors were plausible enough that downstream systems accepted them without question, and they only surfaced when humans noticed inconsistencies or when automated checks failed far downstream. The extractions with the highest cascade costs were not the ones that were completely wrong—those usually triggered immediate validation failures—but the ones that were subtly wrong in ways that passed initial checks but caused failures later in the process.

## Record-Level vs Field-Level Metrics

Most extraction systems operate at the record level, processing an entire document or data source and producing a structured output with many fields. You can measure accuracy at the record level (what percentage of records were extracted perfectly), at the field level (what percentage of individual fields were extracted correctly), or with hybrid approaches that account for both dimensions. Each perspective reveals different aspects of system performance.

**Record-level accuracy** is the strictest metric: a record counts as correct only if every field is correct. This reflects the experience of downstream systems that often need complete, correct records to function properly. If you have a contract with twenty extracted fields and nineteen are correct but one is wrong, the record fails. This metric tends to be much lower than field-level accuracy, especially for complex records with many fields. The legal tech company's record-level accuracy was only forty-one percent even though their field-level accuracy was ninety-two percent, because most records had at least one error somewhere.

**Field-level accuracy** is more forgiving: it measures what fraction of all extracted fields across all records are correct. This gives partial credit for records that are mostly right, which better reflects the reality that some fields are more important than others and some errors are more tolerable than others. However, field-level accuracy can be misleading because it does not account for whether the errors are spread across many records (each record has a small error) or concentrated in a few records (most records are perfect but some are completely wrong). The distribution matters for downstream impact.

The solution is to measure both and understand their relationship. A large gap between field-level and record-level accuracy indicates that errors are widespread—most records have at least one error, even though most individual fields are correct. A small gap indicates that errors are concentrated—most records are perfect, but the ones that have errors have many errors. The legal tech company found that their errors were widespread: ninety-eight percent of contracts had at least one extraction error, but most contracts had only one or two errors out of twenty fields. This suggested that improving record-level accuracy required reducing the long tail of rare errors rather than fixing systematic problems.

## Extraction Confidence and Uncertainty

Like classification systems, extraction systems should ideally provide **confidence scores** indicating how certain they are about each extracted value. These confidence scores enable downstream systems to route low-confidence extractions to human review while auto-processing high-confidence ones. But the confidence scores must be calibrated—a confidence of ninety percent should mean ninety percent chance of being correct—or they do not provide useful information.

The legal tech company's initial system provided confidence scores, but they were poorly calibrated. The model reported high confidence for many incorrect extractions, particularly in cases where it extracted plausible but wrong information. For example, when extracting a liability cap from a contract that mentioned several different dollar amounts, the model would confidently extract the wrong amount because it pattern-matched on currency formatting rather than understanding which amount was the liability cap. The confidence score reflected how well the extracted value matched the expected format, not how likely it was to be correct.

They improved calibration through several mechanisms. First, they used **ensemble uncertainty**: running multiple extraction models or multiple inference passes and measuring agreement. High agreement indicated high confidence; low agreement indicated uncertainty. Second, they trained a **confidence estimator** that predicted extraction accuracy based on features of the input and output: document complexity, field type, extraction consistency across multiple mentions, and model internal confidence. Third, they used **active learning**: routing low-confidence extractions to human review, collecting human corrections, and using those corrections to improve both the extraction model and the confidence estimator.

After these improvements, their confidence scores became genuinely useful for triaging. Extractions with confidence above ninety-five percent had actual accuracy of ninety-three percent and were auto-approved. Extractions with confidence between eighty and ninety-five percent had accuracy of seventy-four percent and were flagged for spot-checking. Extractions below eighty percent confidence had accuracy of forty-one percent and were routed to full human review. This allowed them to balance automation rate against accuracy: by auto-approving only high-confidence extractions, they reduced processing volume but maintained quality guarantees.

## Annotation Quality and Gold Standard Reliability

All extraction metrics depend on comparing system outputs to **gold standard** annotations that represent ground truth. But those gold standards are usually created by human annotators, and humans make mistakes, disagree with each other, and sometimes cannot determine the correct answer even with full context and unlimited time. The quality of your evaluation is bounded by the quality of your gold standard.

The legal tech company initially used contractor-provided annotations as their gold standard. They hired freelance legal professionals through annotation platforms, provided guidelines for how to extract key terms, and used the results to measure model performance. This seemed rigorous until they discovered that different annotators produced different extractions for the same contract about thirty percent of the time. Some disagreements were minor formatting differences, but others were substantive: annotators disagreed about which clause constituted the termination provision, or whether a particular payment term was a penalty or an earnout, or whether certain dates were binding deadlines or target dates.

These disagreements meant that their gold standard was unreliable. A model that matched one annotator might not match another, not because the model was wrong but because the annotators disagreed. When the reference annotations themselves have an error rate, your model's measured accuracy is artificially depressed—it gets penalized for disagreeing with incorrect annotations. The severity of this problem depends on **inter-annotator agreement**, the rate at which different annotators produce the same annotations for the same input.

They measured inter-annotator agreement by having multiple annotators label the same contracts and calculating agreement rates. For simple fields like party names and dates, agreement was above ninety-five percent. For complex fields like clause extraction and conditional terms, agreement dropped to sixty-eight percent. This meant that even perfect model performance on complex fields could not exceed sixty-eight percent measured accuracy because thirty-two percent of the time the model would disagree with the reference annotation due to annotation ambiguity rather than model error.

The solution was to improve annotation quality through several interventions. They created more detailed annotation guidelines with examples of edge cases and difficult scenarios. They implemented consensus annotation where multiple annotators labeled each contract and disagreements were resolved through discussion or expert review. They measured and tracked annotator performance, providing feedback and retraining to improve consistency. These interventions improved inter-annotator agreement to eighty-seven percent overall, making their gold standard more reliable and their metrics more meaningful.

## Temporal Stability and Distribution Shift

Extraction system performance often degrades over time as the distribution of inputs shifts. Legal contracts evolve as new types of agreements become common, as legal language changes to address new business models, and as regulations impose new disclosure requirements. A model trained on 2024 contracts might struggle with 2026 contracts that include AI liability clauses, cryptocurrency payment terms, or EU AI Act compliance provisions that did not exist in the training data.

The legal tech company discovered this the hard way when a client uploaded a batch of recently negotiated technology licensing agreements that included AI-specific terms. Their extraction accuracy on these contracts was only fifty-three percent, compared to ninety-two percent on the validation set of older contracts. The model had never seen terms like "AI training data restrictions" or "model performance warranties" and did not know how to extract them. Even fields that should have been straightforward, like payment terms, were extracted incorrectly because they used new structures like "per API call" pricing that the model had not encountered during training.

Measuring **temporal stability** requires tracking extraction performance over time on incoming data, not just on static validation sets. They implemented continuous monitoring that calculated daily metrics on production extractions compared to a sample that received human review. This let them detect degradation quickly: when accuracy on new data dropped below acceptable thresholds, they triggered model retraining or human review for affected document types. They also tracked performance by document vintage, comparing extraction quality on recent contracts versus older ones to identify distribution shift.

The monitoring revealed that extraction accuracy degraded by approximately two to three percentage points per quarter on average, with occasional spikes when new contract types or terms became prevalent. They established a retraining cadence of quarterly model updates using recent data, which stabilized performance. They also built a mechanism to detect novel field types that did not exist in their schema and route those to human review rather than attempting extraction with an inappropriate model.

## The Path to Structured Understanding

The deeper lesson from extraction metrics is that accuracy on individual fields is necessary but not sufficient for production quality. What matters is whether the structured output your system produces enables downstream processes to function correctly. This requires measuring not just field accuracy but schema compliance, cascade effects, confidence calibration, and temporal stability. It requires understanding that different types of errors have different business impacts and weighting your metrics accordingly. And it requires recognizing that extraction is not just a technical challenge of information retrieval but a semantic challenge of understanding what information means in context.

This understanding of extraction and structuring metrics provides essential foundation as we turn to code generation systems, where the challenge shifts from extracting existing information to synthesizing new artifacts that must meet stringent functional and quality requirements.

