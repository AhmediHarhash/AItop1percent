# 4.3 â€” Agent and Multi-Step Workflow Metrics

On November 7th, 2025, a customer service automation startup discovered that their Claude 4-powered support agent was successfully resolving only forty-one percent of customer issues that required more than three steps. The company had raised twenty-seven million dollars to automate tier-one support for e-commerce platforms, deployed their agent across nineteen enterprise clients, and measured only task completion rate without examining how tasks were completed. Their headline metric showed seventy-three percent overall task success, which seemed strong enough to justify their series B valuation. What they missed was that simple single-step tasks like order status lookup succeeded at ninety-six percent while complex multi-step workflows like processing returns with restocking fees across multiple warehouses failed sixty-four percent of the time. The failure came to light when their largest client, a retailer processing eight thousand support tickets daily, analyzed resolution quality and found that nearly two-thirds of escalations to human agents came from cases where the AI agent had started the task, executed several steps incorrectly, and left customers more frustrated than if automation had never engaged. The engineering team had optimized for binary success metrics while ignoring trajectory quality, error recovery capability, and the compounding error problem where each mistake in a multi-step workflow amplified the likelihood of downstream failures.

The core challenge of agent systems is that quality emerges not from individual model calls but from sequences of decisions across extended task execution. A single bad tool call, one incorrect parameter, or a failure to recognize when a step went wrong cascades through subsequent steps until the entire task fails. The customer service startup demonstrated this when analysis revealed that seventy-nine percent of failed multi-step tasks included at least one step that succeeded technically but moved the workflow in a wrong direction, followed by additional steps that compounded the initial error. You cannot evaluate agent quality by measuring individual actions in isolation. You need metrics that capture planning quality, execution trajectory, error detection and recovery, and the complex ways that early decisions constrain or enable later success.

## Task Success Metrics Beyond Binary Completion

Traditional success metrics treat tasks as binary: either completed successfully or failed. This framing works for simple single-step operations but obscures critical information about agent quality in multi-step workflows. The customer service startup reported seventy-three percent task success, but this number aggregated trivial lookups that always succeeded with complex workflows that usually failed, hiding the pattern where agent value collapsed precisely on the tasks that most needed automation. You need success metrics that capture partial progress, distinguish between failure modes, and provide actionable information about where agent capabilities break down.

**Binary task success rate** measures the fraction of attempted tasks that complete successfully from end to end. This metric provides a headline number useful for comparing systems or tracking overall quality trends, but it tells you nothing about why tasks fail or how close failed attempts came to success. The customer service agent showed seventy-three percent binary success, but this metric provided no insight into whether failures happened in the first step or the final step, whether they resulted from capability gaps or execution errors, or whether failed tasks could have been salvaged with better error recovery. You need binary success rate as a summary metric but never as your only measure of agent quality.

**Partial credit success scoring** measures how much of a task the agent completed successfully before failure. If a task requires five steps and the agent correctly completes three before making a fatal error, partial credit scoring awards sixty percent success rather than zero. This metric reveals whether your agent fails immediately on challenging tasks or makes substantial progress before encountering obstacles. An agent that averages forty percent partial credit on failures is much more valuable than an agent that averages ten percent because the former often gets users most of the way to task completion before human intervention becomes necessary. The customer service agent averaged fifty-seven percent partial credit on failures, meaning failed tasks typically included several successful steps, suggesting that improved error recovery rather than fundamental capability gaps was the primary improvement opportunity.

**Task complexity stratification** breaks success metrics into tiers based on task difficulty. You might measure success rate separately for tasks requiring one to two steps, three to five steps, six to ten steps, and more than ten steps. This stratification reveals where agent capability cliffs exist. The customer service agent succeeded at ninety-six percent on simple tasks, seventy-one percent on moderate complexity tasks, and forty-one percent on complex tasks. This pattern immediately identified multi-step workflow handling as the critical quality gap, but the aggregate metric hid this insight until detailed analysis forced stratification.

**Error mode taxonomy** categorizes failures by root cause: capability gaps where the agent cannot perform required actions, execution errors where the agent has the capability but makes mistakes, planning failures where the agent chooses the wrong approach, and recovery failures where the agent could have salvaged the task after an error but did not. Understanding your error mode distribution tells you what improvements matter most. If eighty percent of failures stem from capability gaps, you need more tools or better models. If eighty percent stem from execution errors in existing capabilities, you need better evaluation and testing of individual actions. The customer service agent showed only eleven percent capability gap failures, with the remainder split between execution errors, planning problems, and recovery failures.

**User intervention rate** measures how often humans must step in to complete tasks the agent starts. This metric matters more than pure success rate in assisted automation scenarios where agents and humans collaborate. An agent that succeeds sixty percent of the time and requires intervention thirty percent of the time with ten percent abandonment performs differently than an agent with the same sixty percent success rate but forty percent abandonment and zero intervention. The first agent provides value even in intervention cases by handling routine steps before escalation. The second agent wastes user time on attempts that go nowhere. The customer service agent had a thirty-four percent intervention rate, meaning one-third of tasks required human completion after agent work, but the intervention cost was low because agents typically completed the easy steps before escalating.

## Planning Quality and Trajectory Metrics

Multi-step agents must plan how to accomplish tasks before executing steps. Planning quality determines trajectory efficiency, error likelihood, and whether the agent can recover from problems encountered during execution. The customer service startup never measured planning quality, only outcomes, so they could not distinguish between agents that failed due to bad plans and agents that failed due to good plans executed poorly. Planning metrics reveal whether your agent understands task structure, chooses efficient approaches, and adapts plans based on execution feedback.

**Plan optimality** measures whether the agent's initial plan represents an efficient path to task completion. You evaluate this by comparing the agent's planned sequence of actions against expert-defined optimal sequences or against the actual shortest successful path discovered through exploration. An agent that plans a seven-step sequence when an expert would use four steps shows poor planning. You calculate average plan efficiency as optimal step count divided by planned step count across test tasks. The customer service agent showed sixty-three percent average plan efficiency, meaning its plans typically included fifty-eight percent more steps than necessary. This inefficiency increased task duration and error likelihood as each unnecessary step introduced additional failure risk.

**Execution trajectory length** measures how many steps the agent actually takes to complete tasks, including error recovery steps and dead ends. While plan optimality measures initial planning, trajectory length measures real execution including adaptation. An agent might start with a suboptimal plan but still execute efficiently, or might start with a good plan that execution challenges force into extended meandering. The customer service agent averaged eight point three steps per successful task against an expert baseline of four point seven steps, indicating significant inefficiency in both planning and execution. Nearly half of this excess came from error recovery attempts, suggesting that more efficient error handling could substantially improve trajectory length.

**Plan stability under execution** measures how often agents stick to their initial plans versus replanning mid-execution. Some replanning indicates healthy adaptation to unexpected conditions. Excessive replanning suggests poor initial planning or failure to anticipate likely obstacles. You measure plan changes per task and analyze whether replanning correlates with success or failure. The customer service agent replanned in forty-seven percent of tasks, with replanning associated with slightly lower success rates. This pattern suggested that replanning often resulted from errors or unexpected conditions that challenged the agent rather than from proactive optimization.

**Backtracking frequency** measures how often agents must undo previous steps and try alternative approaches. Occasional backtracking is normal in complex problem-solving, but frequent backtracking indicates inefficient search through the solution space. You track undoing actions, explicit retry attempts, and steps that duplicate earlier work. The customer service agent backtracked in twenty-eight percent of tasks, typically when initial approaches encountered errors or when users provided clarifying information that invalidated earlier assumptions. High backtracking frequency directly contributed to long trajectory lengths and increased user frustration.

**Goal decomposition quality** measures whether agents break complex tasks into appropriate subtasks. Effective decomposition identifies natural task boundaries, sequences subtasks in dependency order, and handles subtask failures gracefully. You evaluate decomposition through comparison with expert task hierarchies and through analysis of which decomposition strategies correlate with success. The customer service agent showed weak decomposition, often treating complex tasks as flat sequences rather than hierarchical goals with clear dependencies. This led to cases where the agent attempted steps that depended on incomplete earlier subtasks, forcing errors that better decomposition would have prevented.

## Tool Use Quality Metrics

Agents accomplish tasks by invoking tools and APIs that interact with external systems. Tool use quality determines whether agents can translate intentions into correct actions and whether they handle tool responses appropriately. The customer service startup gave their agent access to nineteen different tools for order management, inventory lookup, shipping coordination, and customer communication. They measured tool use success rate but not tool selection appropriateness or parameter correctness, missing critical quality dimensions that explained their workflow failures.

**Tool selection accuracy** measures whether agents choose the correct tool for each required action. If five tools could potentially address a need but only one is appropriate given the current context, tool selection accuracy tracks how often the agent picks the right one. You evaluate this through test cases with annotated correct tool choices, measuring what fraction of selections match ground truth. The customer service agent showed eighty-seven percent tool selection accuracy, which seemed strong until stratified analysis revealed that accuracy dropped to sixty-three percent for tools requiring contextual judgment about which of several similar options best fit the situation.

**Parameter correctness rate** measures whether agents invoke tools with valid and semantically appropriate arguments. A tool call might select the correct tool but fail because parameters are malformed, use wrong data types, reference nonexistent entities, or are semantically incorrect despite syntactic validity. You measure syntactic parameter correctness through schema validation and semantic correctness through annotation of whether parameter values make sense given task context. The customer service agent achieved ninety-four percent syntactic correctness but only seventy-nine percent semantic correctness, with the gap representing cases like looking up an order from the wrong date range or checking inventory at the wrong warehouse location.

**Tool response handling** measures whether agents correctly interpret and act on tool outputs. Tools return success confirmations, error messages, data payloads, or null results, and agents must handle each appropriately. Misinterpreting tool responses causes agents to proceed with incorrect assumptions, ignore errors that should trigger recovery, or fail to use returned data in subsequent steps. You evaluate this by examining agent behavior after tool calls, checking whether the agent's next actions reflect accurate understanding of tool results. The customer service agent showed response handling failures in nineteen percent of tool invocations, particularly when tools returned partial success or warning conditions that required interpretation rather than simple success or failure.

**Error handling and retry logic** measures whether agents respond appropriately when tool calls fail. Simple retry might work for transient failures. Alternative tool selection might work when the chosen tool is unavailable. Some failures require replanning. Fatal errors require graceful task termination and user communication. You evaluate error handling by deliberately triggering tool failures and measuring agent response appropriateness. The customer service agent rarely retried transient failures, often attempted identical retry calls that failed repeatedly, and sometimes continued with the task despite clear tool failures, accounting for twenty-three percent of multi-step workflow failures.

**Tool call efficiency** measures whether agents minimize unnecessary tool invocations. An agent that calls an order lookup tool three times for the same order shows inefficiency. An agent that checks inventory after already receiving out-of-stock results wastes calls. You measure redundant tool calls, calls with predictably null results based on prior context, and cases where single calls could replace multiple calls. The customer service agent averaged one point four redundant tool calls per task, adding latency and increasing error surface area without providing value.

## The Compounding Error Problem

Multi-step workflows suffer from error amplification where early mistakes corrupt later steps, causing cascading failures. A parameter error in step two might go undetected until step five when its consequences become visible. An incorrect assumption in planning might invalidate an entire execution branch. The customer service startup's agent demonstrated classic compounding error patterns where seventy-nine percent of failed tasks included early errors that triggered downstream failures. Understanding and measuring error propagation dynamics is critical to improving multi-step agent quality.

**Error detection latency** measures how many steps elapse between an error occurring and the agent detecting it. Immediate detection allows quick recovery. Delayed detection means the agent executes additional steps based on incorrect state before recognizing the problem. You measure detection latency by annotating where errors first occur and where agents first show evidence of recognizing them. The customer service agent averaged two point seven steps between error occurrence and detection, meaning errors typically propagated through nearly three additional actions before triggering recovery attempts. This latency dramatically increased recovery difficulty because state had diverged substantially from the last-known-good configuration.

**Error propagation rate** measures what fraction of errors in step n cause failures in step n plus one or later. Some errors remain localized and do not affect subsequent steps. Others corrupt shared state or violate assumptions that later steps depend on. You measure propagation by tracking which steps fail due to problems originating earlier versus failures from independent causes. The customer service agent showed a seventy-one percent error propagation rate, meaning nearly three-quarters of errors caused downstream impacts rather than remaining isolated. This high propagation rate explained why their overall failure rate was so much higher than their per-step error rate.

**Checkpoint and rollback capability** measures whether agents can identify good intermediate states and return to them when later steps fail. Without checkpointing, agents must restart tasks from the beginning after errors. With good checkpointing, agents can roll back to the last successful state and try alternative approaches. You measure whether agents identify natural checkpoint opportunities, whether they can successfully roll back when needed, and what fraction of recoverable errors actually get recovered. The customer service agent lacked explicit checkpointing, treating every error as requiring either full retry or human escalation.

**State corruption detection** measures whether agents recognize when errors have left systems in inconsistent states requiring cleanup before continuation. Creating a partial order that later fails might require deleting the partial order before retrying. Starting a return process then encountering errors might require canceling the return. You evaluate state corruption handling by analyzing whether agents clean up after failures and whether they detect cases where proceeding without cleanup would cause further problems. The customer service agent frequently left systems in partial states after failures, contributing to the escalation pain points where human agents inherited inconsistent system state.

**Recovery success rate** measures what fraction of detected errors the agent successfully recovers from without human intervention. High recovery rates mean agents rarely need escalation despite encountering errors during execution. Low recovery rates mean even detected errors usually require abandoning the task or escalating. You stratify recovery success by error type to understand which failures your agent can handle autonomously and which require human help. The customer service agent recovered successfully from thirty-seven percent of detected errors, with recovery success strongly correlated with error detection latency. Errors detected within one step had sixty-two percent recovery success while errors detected after three or more steps had only eighteen percent recovery success.

## Measuring Agent Autonomy and Escalation Patterns

Agents exist on a spectrum from fully autonomous to heavily supervised. The appropriate autonomy level depends on task risk, capability confidence, and user preference. The customer service startup optimized for maximum autonomy, having their agent attempt every task without early escalation, which maximized automation rate but also maximized cases where agents made problems worse through failed intervention. Measuring escalation patterns helps you balance autonomy against quality and user experience.

**Appropriate escalation rate** measures whether agents escalate to humans at the right time: not too early when the agent could have succeeded, not too late after the agent has already created problems. You define appropriate escalation thresholds based on task risk and capability confidence, then measure whether actual escalation decisions align with policy. The customer service agent escalated in only fourteen percent of tasks despite having clear difficulty indicators in thirty-one percent of cases, suggesting systematic under-escalation that forced users to intervene manually after agent failures rather than being escalated proactively.

**Escalation timing quality** measures when in task execution agents escalate. Early escalation minimizes user time waste and state corruption risk. Late escalation means users inherit partially completed tasks requiring more context and cleanup. You measure escalation step distribution and analyze whether escalations occur at natural task boundaries or mid-workflow. The customer service agent's escalations averaged five point two steps into tasks, often mid-workflow rather than at clean handoff points, forcing human agents to spend significant time understanding partial state before proceeding.

**Confidence calibration in escalation decisions** measures whether agent-reported confidence correlates with actual success likelihood. Well-calibrated agents escalate when confidence is low and proceed when confidence is high, with calibration such that ninety percent confidence actually translates to ninety percent success rate. You measure calibration by binning decisions by confidence level and comparing predicted versus actual success. The customer service agent showed poor calibration, with self-reported high confidence tasks succeeding only seventy-three percent of the time and medium confidence tasks succeeding forty-nine percent, suggesting the agent could not accurately assess its own success likelihood.

**User friction at escalation points** measures the effort required when humans take over from agents. Smooth escalations provide context summaries, explain what was attempted and why, and position humans to continue efficiently. Poor escalations dump users into unclear situations requiring investigation before productive work. You measure friction through user satisfaction surveys, time-to-productivity after escalation, and error rates in human work following escalation. The customer service startup found that human agents spent an average of ninety-two additional seconds understanding context after AI escalations compared to handling the same task types from the beginning, quantifying the friction cost their escalation approach imposed.

## Multi-Agent and Collaborative Workflow Metrics

Some complex tasks involve multiple specialized agents working together or agents collaborating with humans across extended workflows. These collaborative scenarios introduce additional quality dimensions around coordination, communication, and work handoff. While the customer service startup used a single agent, similar systems employ specialized agents for different subtask types, inheriting new measurement challenges around multi-agent coordination quality.

**Work decomposition and assignment** measures whether tasks get broken down appropriately and assigned to agents with relevant capabilities. A general agent might identify that a task requires specialized knowledge and delegate to a specialized agent, or a orchestrator might decompose tasks and assign subtasks to specialist agents. You measure whether assignments respect capability boundaries and whether the decomposition creates minimal coordination overhead. Poor decomposition creates excessive handoffs and communication while inefficient decomposition creates bottlenecks where single agents handle subtasks that could parallelize.

**Inter-agent communication quality** measures whether agents exchange necessary information cleanly. Agents must communicate context, partial results, errors, and status without overwhelming each other with irrelevant detail. You evaluate communication by analyzing message clarity, information completeness, and whether receiving agents can proceed effectively based on provided context. Excessive communication creates latency and complexity. Insufficient communication forces agents to work with incomplete information or request clarifications that should have been provided initially.

**Handoff success rate** measures whether transitions between agents or between agents and humans preserve context and enable continuation. Failed handoffs force the receiving party to duplicate work or gather information already available to the sender. You measure handoff success through trajectory analysis of whether receiving agents successfully continue work versus require backtracking or clarification. You also measure through user satisfaction with collaborative interactions.

**Coordination overhead** measures the efficiency cost of multi-agent collaboration. Some task structures benefit from specialization despite coordination costs. Others suffer when coordination overhead exceeds the value of specialization. You measure overhead through comparing execution time and step count for multi-agent versus single-agent approaches, analyzing how much effort goes to coordination versus productive work. High coordination overhead suggests task decomposition boundaries that split naturally atomic operations.

## Testing and Evaluation Infrastructure for Agents

Evaluating multi-step agent quality requires infrastructure beyond what single-turn models need. You must execute complete task trajectories, handle real or simulated tool calls, track intermediate state across steps, and attribute failures to specific decisions within long execution sequences. The customer service startup initially evaluated their agent through manual testing of individual features, missing systematic quality issues that emerged only in end-to-end task execution. Building comprehensive agent evaluation infrastructure is essential to measuring and improving quality.

**Task simulation environments** provide controlled settings where agents can execute without affecting production systems. You need simulated versions of all tools and APIs your agent calls, realistic data generation that covers normal cases and edge cases, and state management that allows trajectory exploration. Quality simulators enable fast iteration, reproducible testing, and safety during development. The customer service startup eventually built a simulator that replicated their order management, inventory, and shipping systems, enabling systematic testing that revealed the multi-step workflow issues their manual testing missed.

**Trajectory logging and replay** captures complete execution sequences for analysis. You log every tool call with parameters and responses, every planning decision, every error and recovery attempt, and any internal reasoning if your agent architecture exposes it. Comprehensive logging enables post-hoc analysis of failures and supports replay where you rerun failed trajectories under modified conditions to test whether fixes work. The customer service startup initially logged only task outcomes, making failure diagnosis nearly impossible until they added comprehensive trajectory logging.

**Automated trajectory analysis** identifies patterns in logged execution sequences. You detect repeated errors, inefficient patterns, missed recovery opportunities, and common paths to failure. You cluster similar trajectories to find representative examples of each error mode. You analyze decision points where agents chose paths that led to failure and compare against alternative choices available. This analysis guides both capability improvements and evaluation focus, highlighting which scenarios deserve more test coverage.

**Counterfactual trajectory evaluation** tests whether alternative decisions would have led to better outcomes. When an agent fails, you ask what different choice at which decision point would have prevented failure. You construct modified trajectories with these alternatives and evaluate whether they succeed. This technique identifies minimal changes that would substantially improve quality, focusing improvement effort on high-leverage decision points rather than diffuse capability building.

The measurement challenges of multi-step agent systems compound as workflows become longer and more complex. Every quality dimension from single-turn models remains relevant while new dimensions around planning, trajectory, error propagation, and coordination emerge. You cannot evaluate agents by testing individual capabilities in isolation. You must measure complete task execution under realistic conditions, analyzing both outcomes and the paths agents take to reach those outcomes.

The next chapter focuses specifically on tool-calling and function-execution metrics, diving deeper into the mechanics of how agents invoke external capabilities, what can go wrong in tool use, and how to measure whether agents correctly translate intentions into tool calls and tool results into subsequent actions.
