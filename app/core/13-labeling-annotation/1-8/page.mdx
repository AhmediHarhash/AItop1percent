# 1.8 — Label Versioning: Tracking Schema Changes Over Time

What does a label mean when its definition changed twice in the last four months but nobody recorded when or why? The answer is: nothing reliable. Every label carries the context of the schema version, the guideline definitions, the annotator pool, and the sampling strategy that produced it. When you change any of those inputs without versioning the change, you create labels that cannot be interpreted consistently over time. The trend line that shows steady quality improvement might actually reflect three different definitions of "quality" spliced together as if they measured the same thing. The model comparison that shows version B outperforming version A might reflect the fact that version B was evaluated under looser criteria. Without version metadata, every historical comparison becomes suspect, and the entire labeling investment decays into uninterpretable data.

Label versioning is the practice of tracking every change to your labeling schema, guidelines, annotator pool, and tooling, and recording which version was used to produce each label. Without versioning, labels become uninterpretable over time because you do not know what they were measuring or how they were produced. With versioning, every label carries enough metadata to reconstruct the exact conditions under which it was created, which means historical labels remain meaningful even as your schema evolves.

Most teams treat their labeling schema as a living document that gets updated whenever someone notices a problem. They do not version these changes because they do not think of labels as code. This is a fundamental category error. Labels are the ground truth that your entire evaluation and monitoring system depends on. If the definition of ground truth changes and you do not track the change, every trend analysis, every A/B test, and every regression detection system becomes unreliable. Label versioning is not optional, it is the foundation of interpretable data.

## Why Label Schemas Change

Label schemas change because your product changes, your understanding of quality changes, and the edge cases you care about change. When you launch a new feature, you need new labels to evaluate it. When you discover that a label category is too ambiguous, you split it into two more precise categories. When you realize that what you called good six months ago is no longer good enough, you raise the bar and redefine the criteria. These changes are necessary and healthy, but only if you track them.

Product evolution drives schema changes. If your customer support AI starts handling refund requests in addition to general inquiries, you need a label that distinguishes refund-specific quality from general inquiry quality. If your conversational AI adds multi-turn context tracking, you need labels that evaluate context coherence across turns, not just single-turn response quality. If your content moderation system expands to cover new policy areas like synthetic media or coordinated inauthentic behavior, you need new labels for those categories. Every product change that introduces new behavior requires new labels to measure that behavior, and every new label is a schema change.

Quality bar evolution drives schema changes. In early 2024, many teams labeled responses as good if they were factually correct and polite. By late 2024, the bar had risen to include citation accuracy, refusal appropriateness, and contextual relevance because models had improved enough that correctness and politeness were table stakes. In 2026, the bar includes reasoning transparency, uncertainty acknowledgment, and cross-turn consistency because users expect more from AI systems than they did two years ago. As your quality bar rises, your schema must rise with it. A label that meant good in 2024 does not mean good in 2026, and if you do not version that change, you cannot tell whether your quality is improving or your definition of quality is drifting.

Edge case discovery drives schema changes. When you launch, you have a schema that covers the cases you anticipated. Six months later, you have discovered a dozen edge cases you did not anticipate, and your schema does not have labels for them. You add labels for refusals, for partial answers, for responses that are correct but unhelpful, for responses that are helpful but unsourced. Each addition is a schema change. If you do not version these changes, you do not know when a particular edge case started being measured, which means you cannot tell whether an increase in that edge case is a real trend or an artifact of when you started labeling for it.

## What Needs Versioning

Label versioning is not just versioning the schema. It is versioning every component of the labeling system that affects what a label means. That includes the schema itself, the annotation guidelines, the annotator pool, the labeling interface, and the sampling strategy. A label produced with schema version two, guideline version three, by annotator pool A, in interface version one, from a random sample is not comparable to a label produced with schema version two, guideline version three, by annotator pool B, in interface version two, from an uncertainty-biased sample. They were produced under different conditions, and those conditions affect the distribution of labels even if the schema itself did not change.

The schema version tracks which label categories exist and what their definitions are. If you start with categories good, acceptable, bad and later split acceptable into acceptable-correct and acceptable-incomplete, that is a schema change from version one to version two. Every label produced after the split is tagged with schema version two, and every label produced before the split is tagged with schema version one. When you analyze trends, you know that version one labels do not have acceptable-correct or acceptable-incomplete, so you cannot compare the rate of those categories across the version boundary. You can compare the combined rate of all acceptable subcategories in version two to the rate of acceptable in version one, but only if you know which labels came from which version.

The guideline version tracks how annotators are instructed to apply the schema. If your schema has a category called harmful and your guidelines define harmful as content that could cause physical injury, and you later update the guidelines to define harmful as content that could cause physical or psychological injury, that is a guideline change even though the schema category name did not change. Labels produced under the old guideline are measuring a narrower concept than labels produced under the new guideline. If you do not version the guideline, you will see an increase in the harmful rate after the change and you will not know whether it is because the system is producing more harmful content or because the definition of harmful expanded.

The annotator pool version tracks which annotators produced which labels. If you start with an internal team of five annotators and later switch to a vendor team of fifty annotators, the label distribution may shift even if the schema and guidelines stay the same, because the two pools have different training, different incentives, and different interpretations of ambiguous cases. If you do not version the annotator pool, you cannot tell whether a trend is driven by system changes or annotator changes. Versioning the annotator pool means tagging each label with an annotator ID or a pool ID, so you can analyze inter-annotator agreement within a pool and cross-pool consistency when you switch pools.

The interface version tracks what annotators saw when they labeled. If you start with a labeling interface that shows only the AI response and later update the interface to show the user query and the response together, annotators have more context and may label differently even if the schema and guidelines are identical. If your interface initially showed responses in plain text and you later added syntax highlighting or formatting rendering, annotators may judge quality differently because they are seeing the output differently. Interface changes are subtle but real, and they need to be versioned so you know when the conditions under which annotators worked changed.

The sampling strategy version tracks how tasks were selected for labeling. If you start with random sampling and switch to uncertainty-based sampling, the distribution of labels will shift toward more ambiguous and lower-quality cases even if the overall system quality has not changed. If you start sampling uniformly across all users and switch to sampling only high-value enterprise customers, the label distribution may shift because enterprise use cases have different quality profiles than consumer use cases. Sampling strategy is a versioned property of the labeling pipeline, and it must be recorded for every label so you know what population the label represents.

## Schema Migration Strategies

When you change your labeling schema, you face a choice about what to do with historical labels that were produced under the old schema. You can re-label historical data under the new schema, which is expensive but gives you a clean dataset. You can maintain parallel schemas and keep both old and new labels, which is complex but preserves historical data. You can deprecate the old schema and create a mapping table that approximates how old labels correspond to new labels, which is pragmatic but lossy. Each strategy has trade-offs, and the right choice depends on how much you need historical comparability versus how much you can afford to spend on re-labeling.

Re-labeling historical data is the gold standard. If you change your schema in March 2026 and you have 50,000 labels from January and February under the old schema, you re-label all 50,000 under the new schema so your entire dataset is consistent. This eliminates version boundaries and allows you to run trend analysis across the full timeline without worrying about schema drift. Re-labeling is expensive, especially if your historical dataset is large, but it is the only strategy that gives you full comparability. Re-labeling is worth the cost when the schema change is significant, when you need long-term trend analysis, and when the historical data is still relevant to current product decisions. Re-labeling is not worth the cost when the schema change is minor, when historical data is no longer relevant, or when the cost exceeds the value of the analysis you plan to run.

Maintaining parallel schemas allows you to keep the old schema active for historical data and the new schema active for new data, and run both in parallel for a transition period. If you change your schema in March, you continue labeling a small sample under the old schema through June so you have three months of overlap where every task is labeled under both schemas. The overlap period lets you measure how the two schemas compare and create a calibration model that predicts what the new-schema label would have been for old-schema data. After June, you stop labeling under the old schema and rely on the calibration model to translate historical labels into new-schema equivalents. Parallel schemas are complex to manage because you are running two labeling pipelines, but they give you backward compatibility without re-labeling everything.

Deprecating old schemas with mapping tables is the pragmatic middle ground. When you change the schema, you create a mapping table that defines how old labels correspond to new labels. If you split acceptable into acceptable-correct and acceptable-incomplete, your mapping table says that old acceptable labels map to new acceptable-correct or acceptable-incomplete with some estimated probability distribution, or you conservatively map all old acceptable labels to a new category called acceptable-unspecified that acknowledges the ambiguity. Mapping tables are lossy because they do not recover information that was not captured under the old schema, but they allow you to continue using historical data with known limitations. Mapping tables work best when the schema change is additive, like splitting a category, rather than redefining what a category means.

## Versioning Metadata Every Label Should Carry

Every label should carry enough metadata to reconstruct the conditions under which it was produced. That means every label is not just a category assignment, it is a record that includes the category, the schema version, the guideline version, the annotator ID, the timestamp, the interface version, the sampling strategy, and any confidence or difficulty scores the annotator provided. This metadata is as important as the label itself because it is what makes the label interpretable six months later when you have forgotten what version two of the schema meant or which annotator pool was active in April.

The schema version field records which version of the label taxonomy was used. If the label is good, the schema version tells you whether good was defined as factually correct and polite, or whether it also required citations and reasoning transparency. The schema version is a string or integer that corresponds to a schema definition stored in version control. When you analyze labels, you filter or group by schema version so you do not accidentally compare labels that were measuring different things.

The guideline version field records which version of the annotation instructions was used. If the schema version is the taxonomy, the guideline version is the interpretation manual. The same schema category can be applied differently depending on the guidelines, and guideline changes are more frequent than schema changes because guidelines evolve as you discover ambiguous cases. Guideline version is stored separately from schema version because they version independently.

The annotator ID field records which annotator produced the label. Annotator ID allows you to measure inter-annotator agreement, identify annotators whose labels are outliers, and detect drift when annotator pools change. Annotator ID is pseudonymized or anonymized depending on your data governance policies, but it must be stable over time so you can track the same annotator across labeling sessions.

The timestamp field records when the label was produced, down to the second or millisecond if possible. Timestamp is essential for correlating labels with system changes, for measuring labeling latency, and for detecting temporal drift in annotator behavior. Timestamp is stored in UTC and includes timezone information so it can be correctly interpreted across regions.

The confidence field records how confident the annotator was in the label, if your labeling interface collects that information. Confidence is useful for prioritizing which labels to review, for identifying tasks that need adjudication, and for weighting labels in aggregate metrics. Confidence can be a Likert scale, a numeric score, or a binary flag indicating whether the annotator requested review. Not all labeling systems collect confidence, but when they do, it should be versioned metadata.

## How Schema Changes Affect Trend Analysis

Schema changes create discontinuities in trend lines, and if you do not account for those discontinuities, your dashboards will show trends that do not exist or miss trends that do. A trend line that crosses a schema version boundary without acknowledging it is not a trend, it is a concatenation of two different measurements. Accounting for schema changes requires version-aware dashboards that either segment trends by schema version, apply calibration models to make versions comparable, or annotate version boundaries so viewers know where the measurement changed.

Segmented trends display each schema version as a separate line on the same chart, with a visual break or annotation at the version boundary. If you changed your schema in March, your dashboard shows one line for January to February under schema version one and a second line starting in March under schema version two. The two lines may have different y-axes if the label categories are not directly comparable, or they may share a y-axis if you have a mapping table that makes them approximately comparable. Segmented trends are honest about version boundaries, but they make it harder to see long-term trends because the viewer has to mentally stitch together multiple segments.

Calibrated trends apply a statistical model to adjust historical labels so they are approximately comparable to current labels. If you changed your schema in March and you have a parallel labeling period from March to June where tasks were labeled under both old and new schemas, you can train a model that predicts new-schema labels from old-schema labels. You then apply that model to all pre-March labels to generate synthetic new-schema labels, and you display a single trend line that combines synthetic labels for January to February and real labels for March onward. Calibrated trends are smooth and easy to interpret, but they are approximate, and the approximation error should be visualized as confidence intervals around the historical portion of the trend line.

Annotated trends display a single trend line but add visual annotations at every schema version boundary, with a tooltip or footnote explaining what changed. If you changed your schema in March and again in June, your trend line has two vertical lines or shaded regions marking those changes, and the dashboard explains that the March change split the acceptable category and the June change redefined the harmful category. Annotated trends are the minimum viable solution for version-aware dashboards, and they are appropriate when schema changes are infrequent and the label categories are mostly stable.

Version-aware dashboards are not optional in 2026. If your dashboard shows a six-month trend line and your schema changed twice during that period, and the chart does not acknowledge the changes, the chart is lying to you. It is showing a trend that assumes the measurement was constant, when in fact the measurement changed. Every team that runs trend analysis on labeled data must build version awareness into their dashboards, or they will make decisions based on artifacts of schema drift rather than real product changes.

Label versioning is the practice that makes your labeling data trustworthy over time. It acknowledges that schemas evolve, guidelines improve, and annotator pools change, and it ensures that every label carries enough metadata to remain interpretable despite those changes. Without versioning, your labels decay into noise. With versioning, your labels are a durable asset that compounds in value as your dataset grows.

The labeling fundamentals we have covered so far—quality, volume, latency, and versioning—are the foundation of a functioning labeling operation, but they are not sufficient to ensure that your labels actually improve your system, which is the subject of the next chapter on labeling design and sampling strategies.
