# 2.2 — Designing Label Taxonomies: Mutually Exclusive, Hierarchical, and Multi-Label

In mid-2025, a SaaS company building an AI-powered customer support ticket routing system hired a data labeling vendor to categorize 80,000 historical support tickets into 22 categories: billing, account access, feature request, bug report, integration question, and 17 others. The vendor delivered the labeled dataset in four months with an average inter-annotator agreement of 79 percent on a 10 percent overlap sample. The ML team trained a GPT-4o-based classifier and deployed it to production in August 2025. Within two weeks, the VP of Customer Success demanded the system be rolled back. Tickets labeled "billing" were being routed to the payments team, but 40 percent of them were actually escalations about failed payments that required immediate intervention by account managers, not routine billing inquiries. Tickets labeled "bug report" were being routed to engineering, but 30 percent of them were actually user errors caused by unclear UI, which should have gone to Product. Tickets labeled "integration question" were split between simple "how do I connect to Salesforce" questions that support could answer in five minutes and complex "we need custom API endpoints for our enterprise data warehouse" questions that required solutions engineering. The taxonomy had 22 categories but it had no structure. It mixed severity with type. It mixed user intent with technical domain. It provided no mechanism for annotators to indicate that a ticket belonged to multiple categories or that one category was more important than another. The result was a flat label space that annotators applied inconsistently, and a classifier that learned superficial patterns rather than the actual routing logic the business required. The company spent another six months redesigning the taxonomy as a three-level hierarchy with severity as an orthogonal dimension, re-labeling the entire dataset, and retraining the model. The root cause was not annotation quality or model architecture. It was taxonomy design, and it was decided in a single meeting before any labels were applied.

Label taxonomy design is the process of defining the categories in your ontology, their relationships to one another, and the rules that govern how annotators apply them. The structure of your taxonomy determines whether annotators can apply labels consistently, whether the resulting labels can answer your stakeholders' questions, and whether downstream systems can interpret the labels correctly. There are three primary taxonomy structures used in production labeling programs: mutually exclusive, hierarchical, and multi-label. Each structure makes different tradeoffs between simplicity, expressiveness, and downstream interpretability. Choosing the wrong structure for your use case does not result in slightly degraded performance. It results in labels that are systematically misleading, models that learn the wrong decision boundaries, and enforcement policies that cannot be implemented. This is not a matter of theoretical preference. This is a binding technical constraint that must be understood and respected.

## Mutually Exclusive Taxonomies and When They Work

A mutually exclusive taxonomy is one in which every item receives exactly one label from a predefined set of categories. The categories are designed such that they do not overlap: an item that belongs to category A cannot belong to category B. This is the simplest and most common taxonomy structure, and it works well when the underlying phenomenon you are labeling has natural, non-overlapping categories and when the primary use case is classification into a single bucket. Content sentiment is often mutually exclusive: a review is positive, negative, or neutral, and forcing annotators to choose one category produces labels that are consistent and interpretable. Document type is often mutually exclusive: an invoice is not also a receipt, a contract is not also a purchase order. Task priority in a support system might be mutually exclusive: every ticket is either critical, high, medium, or low, and the priority determines SLA and routing.

Mutually exclusive taxonomies are easy to implement, easy to measure, and easy to consume. Inter-annotator agreement has a single, well-defined metric: percent agreement or Cohen's kappa. Model training is straightforward: you are training a multi-class classifier with one true label per example. Dashboards and reports are simple: every item increments exactly one category count, and category distributions sum to 100 percent. Enforcement policies are unambiguous: each label maps to exactly one action. These properties make mutually exclusive taxonomies the default choice for most labeling programs, and they work well when the default choice is the correct choice.

The failure mode of mutually exclusive taxonomies is forcing exclusivity when the underlying phenomenon is inherently multi-dimensional. Consider a content moderation taxonomy with categories "hate speech," "harassment," "threats," "spam," "misinformation," and "sexual content." These categories are not mutually exclusive in the real world. A post can contain both hate speech and threats. A message can be both harassment and sexual content. A video can be both misinformation and spam. If you force annotators to choose exactly one label, they will apply different strategies: some will choose the most severe category, some will choose the first category they notice, some will choose based on which category they think is most likely to trigger enforcement. Agreement will collapse, not because annotators are poorly trained, but because the taxonomy structure is incompatible with the data. The resulting labels will be systematically biased toward whichever heuristic individual annotators happen to adopt, and the bias will be invisible in aggregate metrics.

The correct response to this failure mode is not better annotator instructions. It is redesigning the taxonomy to match the structure of the data. If the phenomenon is inherently multi-dimensional, you need either a hierarchical taxonomy with a precedence rule or a multi-label taxonomy that permits multiple labels per item. Trying to fit multi-dimensional data into a mutually exclusive structure destroys information, and that information cannot be recovered later. You cannot infer that an item had both hate speech and threats from the fact that it was labeled "threats." You cannot build a classifier that detects both if your training data only records one. Forcing exclusivity when it does not exist is not a simplification. It is data corruption.

## Hierarchical Taxonomies and the Depth vs Breadth Tradeoff

A hierarchical taxonomy is one in which categories are organized into a tree structure with broad parent categories and more specific child categories. An item is labeled at the most specific level applicable, and the label implicitly includes all ancestor categories. A support ticket labeled "Billing – Failed Payment – Credit Card Declined" is also labeled "Billing – Failed Payment" and "Billing." A product review labeled "Electronics – Laptops – Battery Life – Poor" is also labeled "Electronics – Laptops – Battery Life," "Electronics – Laptops," and "Electronics." Hierarchical taxonomies allow you to capture both coarse-grained and fine-grained distinctions in a single label, and they allow downstream consumers to query at any level of granularity. A dashboard can show aggregate metrics for all billing issues or drill down into specific failure modes. An eval pipeline can measure model performance on broad categories or narrow subcategories.

The primary tradeoff in hierarchical taxonomy design is depth versus breadth. A shallow, broad taxonomy has few levels and many categories at each level. A deep, narrow taxonomy has many levels and few categories at each level. Shallow taxonomies are faster for annotators to navigate: fewer clicks, fewer decisions, less cognitive load. Deep taxonomies are more expressive: they capture finer distinctions and support more granular analysis. The correct depth depends on your use case, your annotator expertise, and your tolerance for labeling speed versus label precision.

In practice, most production hierarchical taxonomies in 2026 use two to four levels. A two-level taxonomy has a top-level category and a subcategory: "Billing – Invoice Question," "Billing – Payment Failed," "Technical – Login Issue," "Technical – Performance Problem." This structure is fast to annotate and easy to understand, but it limits expressiveness. You cannot distinguish between different types of payment failures or different causes of performance problems without adding a third level. A three-level taxonomy adds specificity: "Billing – Payment Failed – Credit Card Declined," "Billing – Payment Failed – Insufficient Funds," "Technical – Performance Problem – Slow Page Load," "Technical – Performance Problem – Timeout Error." This structure supports finer-grained analysis but requires more annotator decisions and increases the risk of disagreement at the leaf level. A four-level taxonomy is rare outside of specialized domains like medical coding or legal document classification, because the annotation time and disagreement rate increase rapidly with depth.

Breadth is the number of categories at each level. A taxonomy with five top-level categories and ten subcategories per parent has 50 leaf categories. A taxonomy with ten top-level categories and five subcategories per parent also has 50 leaf categories, but the cognitive load is different. Annotators navigate shallow-broad taxonomies by scanning a large set of options at each level. They navigate deep-narrow taxonomies by making a sequence of smaller decisions. Shallow-broad taxonomies work well when categories are visually or semantically distinct and when annotators can recognize the correct category quickly. Deep-narrow taxonomies work well when categories require deliberation or domain knowledge and when breaking the decision into steps reduces errors. There is no universal best choice. The correct structure depends on your data and your annotators.

The most common failure mode in hierarchical taxonomy design is inconsistent depth. Some branches of the tree have three levels while others have two. Some leaf categories are extremely specific while others are very broad. This inconsistency makes annotation harder, because annotators must constantly adjust their decision strategy depending on which branch they are in. It also makes downstream analysis harder, because you cannot roll up metrics consistently. If "Billing – Invoice Question" is a leaf category but "Technical – Performance Problem" has five children, you cannot compare counts at the leaf level. You must either aggregate "Technical – Performance Problem" to the parent level, losing specificity, or split "Billing – Invoice Question" into artificial subcategories, adding noise. The fix is to design the taxonomy with consistent depth: every branch has the same number of levels, and every leaf category has approximately the same specificity. This requires more up-front design work, but it produces labels that are far easier to work with.

## Multi-Label Taxonomies and the Aggregation Problem

A multi-label taxonomy is one in which each item can receive multiple labels from a predefined set of categories. The categories are not mutually exclusive, and the label set for each item is the complete set of applicable categories. A social media post can be labeled "hate speech" and "threats" and "spam." A customer support ticket can be labeled "billing" and "urgent" and "requires manager approval." A product review can be labeled "positive" and "mentions shipping" and "mentions customer service." Multi-label taxonomies are the most expressive structure: they allow you to capture every applicable dimension without forcing annotators to choose one over the others. They are also the most complex to implement, measure, and consume.

The primary challenge with multi-label taxonomies is that agreement measurement becomes dramatically harder. In a mutually exclusive taxonomy, two annotators either agree or disagree on each item, and you can compute percent agreement or kappa. In a multi-label taxonomy, two annotators can partially agree: they might both apply "hate speech" but only one applies "threats." Standard agreement metrics do not handle partial agreement well. You need to choose between label-level agreement, which measures agreement on each label independently and ignores correlations, and set-level agreement, which measures whether annotators produced exactly the same set of labels and penalizes any difference. Label-level agreement is easier to compute but hides systematic patterns where annotators consistently miss certain labels. Set-level agreement is more stringent but is very sensitive to taxonomy size: in a taxonomy with 20 possible labels, the probability that two annotators independently produce the same set is low even if they are well-calibrated.

The second challenge is aggregation. In a mutually exclusive taxonomy, the label distribution is a simple frequency count: 40 percent of items are category A, 30 percent are category B, 30 percent are category C. In a multi-label taxonomy, label frequencies do not sum to 100 percent, and you must decide whether to report per-label prevalence, average labels per item, or label co-occurrence patterns. A report that says "35 percent of posts are hate speech, 28 percent are threats, 18 percent are spam" does not tell you whether these categories overlap or whether they are independent. You need to report co-occurrence: "12 percent of posts are both hate speech and threats, 5 percent are both hate speech and spam." This is more informative, but it is also more complex to compute, visualize, and explain to stakeholders who are accustomed to mutually exclusive distributions.

The third challenge is model training. Multi-label classification is a harder learning problem than multi-class classification. You cannot use a standard softmax output layer. You need one sigmoid output per label, and you must choose a threshold for each label independently. Label imbalance is often severe: some labels apply to 40 percent of items while others apply to 2 percent. Correlated labels create credit assignment problems: if "hate speech" and "threats" co-occur 80 percent of the time, the model may learn to predict one from the other rather than learning the independent features of each. These problems are solvable, but they require more sophisticated modeling and more labeled data than mutually exclusive classification.

Multi-label taxonomies are justified when the phenomenon you are labeling is inherently multi-dimensional and when you need to make decisions based on multiple independent dimensions. Content moderation is often multi-label because a single piece of content can violate multiple policies, and enforcement teams need to know all applicable violations. Medical diagnosis is often multi-label because a patient can have multiple conditions. Document classification in legal or compliance contexts is often multi-label because a contract can have multiple relevant clauses. If you find yourself designing a mutually exclusive taxonomy and constantly adding categories like "A and B" or "A or B or both," you need a multi-label taxonomy instead.

## The Design Process: Starting with Use Cases and Working Backward

The correct way to design a label taxonomy is to start with your use cases, enumerate every question stakeholders will ask of the labeled data, and work backward to the minimum taxonomy that can answer those questions. This is the opposite of the way most teams actually design taxonomies, which is to brainstorm categories in a meeting, pick the ones that sound reasonable, and hope they turn out to be useful. Starting with use cases forces you to design a taxonomy that is load-bearing rather than decorative.

Begin by listing every downstream consumer of labeled data and every decision they need to make. The Trust and Safety team needs to decide which content to remove and which users to suspend. The Product team needs to decide which features are causing user confusion and which workflows need redesign. The ML team needs to decide whether the model meets accuracy requirements and where it fails most often. The dashboard needs to show trends over time and breakdowns by category. For each consumer and each decision, ask: what label or combination of labels would provide the information needed to make this decision? If Trust and Safety needs to apply different enforcement actions based on violation severity, your taxonomy must include severity as a dimension. If Product needs to know whether user confusion is caused by unclear UI or missing features, your taxonomy must distinguish these cases. If the dashboard needs to show trends in hate speech versus harassment, your taxonomy must treat these as separate categories.

Once you have enumerated all required distinctions, you design the taxonomy to capture them with the minimum number of categories. Every category must support at least one use case. Every distinction must be operationalizable: annotators must be able to apply it consistently based on observable features of the data. If a distinction requires deep domain expertise that your annotators do not have, it is not operationalizable. If a distinction depends on information not present in the item being labeled, it is not operationalizable. If a distinction is subjective and different annotators will reach different conclusions even with perfect training, it is not operationalizable. You must either redesign the distinction to make it operationalizable or accept that you cannot capture it through labeling.

After designing the initial taxonomy, you pilot it with real examples and real annotators. You select 100 to 500 examples that span the range of cases you expect to encounter. You write draft definitions for each category. You ask two or more annotators to label the pilot set independently. You measure agreement and you analyze disagreement patterns. If agreement is below 70 percent on any category, the category definition is ambiguous or the category boundaries overlap with other categories. If certain examples consistently produce disagreement, they are edge cases that your definitions do not cover. If annotators frequently ask questions about which category to apply, your instructions are incomplete. You revise the taxonomy and definitions based on this empirical feedback, and you repeat the pilot until agreement is consistently above 80 percent.

This process is slow. It requires iteration. It produces no labeled data during the design phase. It delays the start of large-scale labeling by weeks. It is also the only reliable way to produce a taxonomy that will survive contact with real data. Teams that skip the pilot phase discover taxonomy flaws three months into labeling, when tens of thousands of examples have already been labeled under broken definitions. The cost of redesigning the taxonomy at that point is either re-labeling the entire dataset or accepting that the data is unusable. One week of piloting prevents three months of rework.

## Common Taxonomy Design Mistakes

The most common mistake in taxonomy design is creating too many categories. A taxonomy with 40 categories sounds comprehensive, but if your dataset has 10,000 examples, most categories will have fewer than 250 labeled examples, which is insufficient for training a reliable classifier. Annotator agreement decreases as the number of categories increases, because the probability of two annotators independently choosing the same category from a large set is lower. Dashboards become unreadable when you try to visualize 40 categories. Stakeholders cannot remember what all the categories mean. The correct number of categories depends on dataset size, but as a rule of thumb, you should have at least 500 labeled examples per leaf category, and you should have fewer than 20 leaf categories unless you have very large datasets or very specialized annotators.

The second most common mistake is overlapping definitions. Two categories overlap when some items legitimately belong to both, but the taxonomy is designed as mutually exclusive. "Spam" and "promotional content" overlap if spam is defined as unsolicited commercial messages and promotional content is defined as messages advertising products. "Bug report" and "feature request" overlap if users describe missing functionality as a bug. Overlapping definitions cause annotator disagreement and produce label distributions that depend on which annotators happened to label which examples. The fix is to redefine categories to eliminate overlap, merge overlapping categories, or switch to a multi-label taxonomy that permits items to belong to both.

The third common mistake is missing a "none of the above" or "other" category in mutually exclusive taxonomies. If your taxonomy has categories for "billing," "technical," "feature request," and "bug report," but a ticket says "I love your product, thank you," annotators have no correct label to apply. They will either leave the item unlabeled, apply an arbitrary label, or invent a new category. The correct approach is to include an "other" or "general inquiry" category that captures items that do not fit the primary categories. This category should be monitored: if it grows above 10 percent of labeled items, your taxonomy is missing important categories and should be revised.

The fourth common mistake is mixing dimensions within a single taxonomy. A taxonomy that includes "billing," "technical," "urgent," and "enterprise customer" is mixing issue type with priority with customer segment. These are orthogonal dimensions that should be captured separately, either as parallel single-label taxonomies or as a multi-label taxonomy. Mixing dimensions forces annotators to make tradeoffs: is an urgent billing issue labeled "billing" or "urgent"? Different annotators will make different choices, and agreement will collapse. The fix is to separate dimensions: one taxonomy for issue type, one for priority, one for customer segment. Each dimension can be mutually exclusive or hierarchical, and downstream consumers can combine dimensions as needed.

## Iteration Based on Disagreement Data

Taxonomy design does not end when labeling begins. It continues throughout the labeling program, informed by disagreement data from overlap samples. Every production labeling program should label a random 10 to 20 percent of examples with multiple annotators and measure agreement. When agreement on a category is consistently low, you investigate why. You review the examples where annotators disagreed, you interview annotators to understand their reasoning, and you identify whether the root cause is ambiguous definitions, overlapping categories, or edge cases not covered by instructions. You revise definitions, add clarifying examples, or restructure the taxonomy, and you propagate the changes to all annotators.

This iteration must be governed. You cannot allow individual annotators to reinterpret categories based on their own judgment. You cannot allow the annotation lead to make unilateral changes without stakeholder review. Every change to the taxonomy is a breaking change that affects downstream consumers, and every change must be versioned, documented, and communicated. In practice, this means batching taxonomy updates into monthly or quarterly releases, testing updates on pilot data before rolling them out, and maintaining parallel versions during transition periods when some examples are labeled under the old taxonomy and some under the new.

The organizations that run successful labeling programs in 2026 treat taxonomy design as an empirical discipline. They do not design the taxonomy once in a conference room and assume it is correct. They design an initial taxonomy based on use cases, pilot it with real data, measure agreement, analyze failures, revise based on evidence, and repeat. They version the taxonomy and track which examples were labeled under which version. They monitor disagreement rates throughout the program and trigger reviews when rates exceed thresholds. They invest in tooling that makes it easy to review disagreements, annotate edge cases, and propagate definition updates. This rigor is not optional for large-scale programs. It is the minimum required process to produce labels that remain interpretable and useful over time.

Designing the taxonomy is the foundation, but taxonomies are not self-documenting. The next step is writing formal label definitions that allow annotators with different backgrounds and expertise to apply the taxonomy consistently, which requires a different set of techniques.
