# 3.3 â€” Positive, Negative, and Borderline Examples

Four pages of precise definitions produced sixty-one percent agreement. Twelve concrete examples produced eighty-four percent agreement. The definitions were technically correct, written by policy experts, carefully distinguishing hate speech from offensive speech and political speech and satire. But annotators were not parsing logical definitions during labeling. They were pattern-matching against mental models built from whatever examples they had seen. Without guideline examples covering the twenty content types where disagreement concentrated, each annotator built different mental models from their early labeling experiences, and those models diverged. The experimental intervention was stark: remove two pages of abstract prose, add twelve examples showing what hate speech looks like versus what it does not look like in borderline cases. Agreement jumped twenty-three points in two weeks because human cognition is fundamentally example-based, not rule-based. Definitions provide conceptual frameworks. Examples provide operational reality.

## Why Examples Communicate More Effectively Than Definitions

Human cognition is fundamentally example-based, not rule-based. When you ask someone to identify a chair, they do not consult an abstract definition involving the number of legs and the presence of a backrest. They compare the object in front of them to their mental catalog of chairs they have seen. Annotation operates the same way. When an annotator encounters a new item to label, they are not running a logical proof against your definitional criteria. They are asking themselves: does this look like the examples I was shown for this label, or does it look like the examples for other labels. This is not laziness or lack of training. This is how pattern recognition works in the human brain, and annotation is pattern recognition applied to data labeling.

Definitions fail because they operate at the wrong level of abstraction. A definition tells you the boundaries of a concept in logical terms. An example shows you what the concept looks like when it appears in the real world. The gap between those two things is where most annotation errors occur. You can define sentiment as the emotional valence expressed in text, but that definition does not tell an annotator how to handle sarcasm, understatement, complaint disguised as praise, or praise disguised as complaint. Examples of each of those cases, with labels and brief explanations, communicate instantly what the definition cannot. The annotator sees the pattern, internalizes it, and applies it to similar cases without needing to reason through the definitional logic every time.

This is why every label in your taxonomy must be anchored to examples, not just defined in prose. The definition provides the conceptual framework. The examples provide the operational reality. Annotators refer to examples far more often than they refer to definitions, and they remember examples longer. If you give an annotator a guideline with rich definitions but no examples, they will create their own mental examples from the first few items they label. Those self-generated examples will be inconsistent across annotators, leading to drift and disagreement. If you provide the examples yourself, you control the anchor points and ensure everyone is pattern-matching against the same reference set.

## The Three Types of Examples Every Label Needs

Not all examples serve the same purpose. A well-constructed guideline provides three types of examples for every label: positive examples, negative examples, and borderline examples. Each type teaches a different aspect of the label's boundaries, and omitting any of the three leaves annotators underprepared for production data.

Positive examples are clear, unambiguous cases that belong to the label. These are the canonical instances, the textbook cases, the examples where any reasonable annotator would assign the label without hesitation. Positive examples establish the core pattern. They answer the question: what does this label look like when it is obvious. For a spam detection task, a positive example might be an email that offers a prize for clicking a link, includes no legitimate sender information, and uses urgent language to create pressure. For a medical entity extraction task, a positive example might be a sentence where a drug name appears in a standard prescription context with dosage and frequency. Positive examples are the foundation. They teach the annotator what the label is for.

Negative examples are cases that look like they might belong to the label but do not. These are the near-misses, the confusables, the examples that naive annotators often mislabel until they understand the distinction. Negative examples establish the boundaries. They answer the question: what does this label not include, even when it looks similar. For the spam detection task, a negative example might be a legitimate marketing email from a known retailer with an unsubscribe link and accurate sender information. It is promotional, but it is not spam under your taxonomy. For the medical entity extraction task, a negative example might be a drug name mentioned in a news article about pharmaceutical companies, where the context is corporate rather than clinical. Negative examples prevent overapplication. They teach the annotator what the label excludes.

Borderline examples are cases where reasonable people disagree, or where the correct label is not obvious even to experts. These are the edge cases, the ambiguous instances, the examples that require judgment rather than pattern recognition. Borderline examples establish the guidelines ruling on difficult cases. They answer the question: when the case is unclear, what does this project require. For the spam detection task, a borderline example might be a forwarded chain email with no commercial intent but clearly unwanted by most recipients. Your guideline must specify whether unsolicited non-commercial bulk email counts as spam in your taxonomy, and the borderline example shows that ruling in action. For the medical entity extraction task, a borderline example might be a supplement mentioned in a patient forum post, where the context is semi-clinical but not prescribed. Your guideline must specify whether over-the-counter supplements count as medical entities, and the borderline example demonstrates the decision with reasoning.

Most guidelines include positive examples because those are easy to generate. Far fewer include negative examples, and almost none include borderline examples. This is a mistake. Positive examples alone teach annotators what the label includes, but they do not teach what the label excludes or how to handle ambiguity. The result is high agreement on easy cases and collapse on anything difficult. You need all three types.

## Quantity Requirements: How Many Examples Per Label

The question is not whether to provide examples, but how many. The minimum is three to five examples per label, but that minimum applies only to simple, unambiguous labels in low-stakes tasks. For any label that carries ambiguity, subjectivity, or high business impact, you need more. The guideline is this: provide enough examples to cover the range of variation you expect to see in production data. If your label can manifest in ten different ways, you need examples of most of those ways, not just the most common one.

For binary labels with clear boundaries, three to five examples per class are usually sufficient. A toxicity label in a comment moderation task might need three positive examples showing different types of toxic language, three negative examples showing aggressive but non-toxic language, and two borderline examples showing edge cases with rulings. That gives annotators eight reference points, which is enough to handle most of what they encounter. For multi-class labels with overlapping categories, you need more. A sentiment label with five classes might need four examples per class, plus two borderline examples for each adjacent pair of classes. That is twenty positives plus eight borderlines, because the confusion happens at the boundaries between similar classes, not within the core of each class.

For subjective labels, you need even more examples, because the variance in human judgment is higher. A content quality label on a five-point scale might need six examples per score level, plus four borderline examples between each adjacent pair. That is thirty positives plus sixteen borderlines, because annotators need to see the full range of quality within each score level and understand where the cutoffs fall. If you provide fewer examples, annotators will anchor to their own subjective sense of quality rather than your project's calibrated standards, and agreement will suffer.

The quantity of examples also scales with the size of your annotation team. A small team of ten annotators can align around fewer examples because they can discuss ambiguous cases and converge on shared interpretations. A large team of two hundred annotators working across shifts and geographies cannot rely on informal alignment. They need the guidelines to do the work. More examples means less reliance on unstated norms and more consistent application of the taxonomy.

You also need more examples for labels that change frequently. If your taxonomy is stable, you can invest in a rich example set once and maintain it lightly. If your taxonomy evolves every quarter in response to new content types, new policies, or new model capabilities, you need a larger example set to cover the expanded scope and a process for updating examples when definitions change. Skimping on examples to save time in guideline development always costs more time in annotation correction and model retraining later.

## The Selection Process: Real Data, Not Invented Scenarios

The quality of your examples matters as much as the quantity. Examples must be drawn from real production data, not invented by the guideline author. Invented examples are too clean. They exaggerate the clarity of the label boundaries and fail to capture the messiness, ambiguity, and edge cases that appear in real content. Annotators trained on invented examples perform well on toy data and poorly on production data, because the distribution does not match.

Real data is messy. It contains typos, ambiguous phrasing, missing context, formatting inconsistencies, and all the other noise that makes annotation hard. If your examples are polished and artificial, annotators will not know how to handle the noise when they encounter it in production. They will hesitate, second-guess themselves, and apply the label inconsistently. If your examples include the noise, annotators see that the guideline accounts for it, and they apply the label with confidence.

The selection process should prioritize coverage of the difficulty range. Start by sampling from your production data or your evaluation set. Label a batch of items yourself, or have senior annotators label them. Identify items that are clear positives, clear negatives, and genuine borderlines. For each label, select examples that span the easy-to-hard spectrum. Include at least one trivial positive, at least one challenging positive, and at least one edge case that required thought. Do the same for negatives. This ensures that annotators see what the label looks like across the full range of difficulty, not just the easy or hard cases.

You should also prioritize examples that reveal common errors. If you have historical annotation data, analyze where annotators made mistakes. Find items that were frequently mislabeled and include them as negative or borderline examples with explanations of why the common label was wrong and what the correct label is. This preempts the error before it happens. Annotators see the mistake they are likely to make, see the correction, and internalize the distinction. It is far more effective than discovering the error pattern after labeling ten thousand items incorrectly.

Do not sanitize your examples. If your production data includes offensive language, slurs, graphic content, or other material that is uncomfortable to read, include it in the guidelines if it is necessary to demonstrate the label boundaries. Annotators will encounter this content in production. Shielding them from it in training means they are unprepared when it appears. You can include a content warning at the top of the guideline, but you cannot omit the examples. The one exception is when including the example would violate legal or ethical constraints, such as showing real user data that contains personally identifiable information. In those cases, anonymize the data or create a realistic synthetic example that preserves the structure and difficulty of the original while removing the sensitive details. But synthetic examples should be the exception, not the rule.

## The Maintenance Burden: Updating Examples as Schemas and Data Evolve

Examples are not static. They require maintenance, and that maintenance is one of the hidden costs of annotation operations. Every time your schema changes, your examples must change. Every time a new edge case is discovered, your examples should expand to cover it. Every time annotator feedback reveals confusion, your examples should be updated to clarify. Treating examples as a one-time effort leads to guideline drift, where the examples no longer reflect the current taxonomy or the current state of production data.

Schema changes are the most obvious trigger for example updates. If you add a new label, you need examples for it. If you split a label into two more specific labels, you need to update the examples for the original label and create examples for the two new labels, ensuring that the boundaries are clear. If you deprecate a label, you need to remove its examples or reclassify them under the new taxonomy. This is straightforward in principle but often neglected in practice. Teams update the definitions but forget to update the examples, and annotators are left with examples that reference labels that no longer exist or boundaries that have shifted.

Edge case discovery is a more subtle trigger. As annotators work through production data, they encounter cases that do not fit cleanly into the existing examples. If these cases are rare, you can handle them on a case-by-case basis. If they recur, they are not edge cases anymore. They are a gap in the guidelines. When you notice a recurring pattern of confusion or disagreement, investigate the root cause. Often it is a type of content that was not represented in the examples. Add an example of that content type to the guideline, with the correct label and a brief explanation of the reasoning. This closes the gap and prevents future disagreement on similar cases.

Annotator feedback is the third trigger. You should have a mechanism for annotators to flag confusing items or request clarification. When multiple annotators flag the same type of item, that is a signal that the guidelines are unclear. Review the flagged items, determine the correct label, and add one or more of them to the guideline as examples. This turns feedback into guideline improvement and shows annotators that their input is valued and acted upon. It also reduces the volume of future feedback, because the guideline now addresses the confusion directly.

The maintenance burden is real, but it is not optional. Guidelines that are not maintained decay. Examples stop reflecting production data, annotators stop trusting the guidelines, and agreement drops. You should plan for quarterly guideline reviews at a minimum, and more frequent reviews if your taxonomy is evolving rapidly or your production data distribution is shifting. Each review should include an analysis of annotation disagreement, a review of flagged items, and an update to examples that are no longer representative or sufficient.

## The Common Mistake: Only Providing Positive Examples

The single most common mistake in example selection is providing only positive examples. This happens because positive examples are easy to generate and feel sufficient. If the label is spam, showing three examples of spam seems like enough. But it is not. Positive examples teach what the label includes. They do not teach what the label excludes, and they do not teach how to handle ambiguity. The result is overapplication. Annotators learn the pattern for spam and then apply it too broadly, labeling things as spam that are merely promotional, unsolicited, or low-quality.

Negative examples are harder to generate because they require you to anticipate what annotators will confuse with the target label. But that anticipation is the entire point. If you cannot predict what will be confused with spam, you will discover it when annotators mislabel ten thousand legitimate emails. If you can predict it, you can provide negative examples that preempt the confusion. The effort of generating negative examples is front-loaded. The payoff is fewer errors and less rework.

Borderline examples are even harder to generate because they require you to make judgment calls and document the reasoning. This feels uncomfortable. It exposes the subjectivity in your taxonomy. But that subjectivity exists whether you document it or not. If you do not provide borderline examples with rulings, annotators will encounter borderline cases in production and make their own rulings, which will be inconsistent. If you provide borderline examples, you control the rulings and ensure consistency. The discomfort of documenting judgment calls is far smaller than the cost of inconsistent annotations and unreliable training data.

Some teams avoid borderline examples because they worry that showing disagreement will undermine annotator confidence. The logic is that if the guideline admits uncertainty, annotators will feel uncertain and label inconsistently. This is backwards. Annotators are not naive. They encounter ambiguous cases and know they are ambiguous. If the guideline pretends that every case is clear-cut, annotators lose trust in the guideline and rely on their own judgment instead. If the guideline acknowledges ambiguity and provides rulings for borderline cases, annotators trust that the guideline is realistic and follow the rulings. Transparency about difficulty increases consistency. Pretending that difficulty does not exist decreases it.

## Examples as the Primary Teaching Tool

Definitions set the conceptual boundaries. Examples make those boundaries operational. The most effective annotation guidelines are example-heavy and definition-light. This does not mean definitions are unimportant. It means definitions should be brief, focused, and supported by extensive examples. A two-paragraph definition followed by ten examples is more effective than a ten-paragraph definition followed by two examples. The definition provides the framework. The examples provide the pattern.

When annotators are uncertain, they do not re-read the definition. They scan the examples looking for a case that resembles the item they are labeling. If they find a close match, they apply the same label. If they do not find a match, they guess or flag the item for review. Your goal is to provide enough examples that most items have a close match, so guessing is rare and flagging is reserved for genuinely novel cases. This requires more examples than most teams provide, but the investment pays off in higher agreement, faster labeling, and less time spent on review and correction.

Examples also serve as calibration tools. When onboarding new annotators, you can use the guideline examples as a quiz. Show the example without the label, ask the annotator to label it, and then reveal the correct label with the explanation. This tests whether the annotator has internalized the patterns and identifies areas of confusion before they start labeling production data. Examples that most annotators get wrong are candidates for clarification or additional negative examples. Examples that everyone gets right can be replaced with harder examples that teach more nuanced distinctions.

The next subchapter addresses ambiguity budgets, which formalize the expectation that some disagreement is inevitable and should be planned for rather than treated as error.
