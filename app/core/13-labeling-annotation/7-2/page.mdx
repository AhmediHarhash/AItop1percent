# 7.2 — LLM-as-Judge: Using Models to Generate Draft Labels

**The most expensive annotation mistake is paying humans to generate labels from scratch when machines can draft them for pennies.** By 2026, continuing to treat human labelers as label generators rather than label verifiers represents a fundamental misunderstanding of both human cognition and machine capability. Humans excel at judgment, correction, and nuanced interpretation. They struggle with consistency, speed, and repetitive generation. LLMs excel at consistent draft generation at scale. They struggle with edge cases, cultural context, and tasks requiring judgment calls that lack clear patterns. The optimal architecture pairs machine generation with human verification, yet most labeling operations still ask humans to do the hardest cognitive work—creating labels from blank slates—when they should be doing what they do best: evaluating and correcting machine-generated drafts.

LLM-as-judge is not about replacing human judgment. It is about using large language models to generate draft labels that humans then verify, correct, or approve. This approach transforms the economics and quality dynamics of annotation. Instead of asking humans to label from scratch—a cognitively demanding task prone to fatigue and drift—you ask them to review and correct machine-generated drafts. The cognitive load shifts from generation to verification, which humans perform faster and more consistently. The machine provides the consistency backbone. The human provides the judgment, context, and correction. When implemented correctly, this architecture delivers higher throughput, lower cost, and better label quality than either pure human labeling or pure machine labeling. When implemented incorrectly, it amplifies machine bias, anchors human judgment to incorrect drafts, and produces data that looks consistent but is systematically wrong.

## The Draft Label Architecture

The LLM-as-judge pattern works like this. You send each unlabeled item to a language model with a carefully designed prompt that specifies the labeling task, the label schema, the output format, and any context needed for judgment. The model returns a structured label—a category, a score, a span annotation, a multi-label vector. This draft label enters a review queue where a human annotator sees the original item, the machine-generated label, and the option to approve, correct, or reject. The human decision becomes the ground truth. The machine draft becomes training data for understanding where the model succeeds and where it fails. Over time, you measure agreement rates between machine drafts and human corrections. High-agreement categories can be routed to auto-approval. Low-agreement categories remain in human review. The system learns which judgments to trust and which to verify.

This architecture inverts the traditional relationship between human effort and label volume. In pure human labeling, cost scales linearly with dataset size. Double the dataset, double the cost. In LLM-as-judge with human verification, cost scales with disagreement rate, not dataset size. If the model generates correct drafts 85% of the time, you pay for human review on 15% of items plus sampling to monitor the 85%. If you improve the model or the prompt and push agreement to 92%, cost drops further. The economic model shifts from paying per label to paying per correction. This creates the right incentive structure: invest in better prompts, better models, and better confidence calibration, and your labeling costs fall while quality rises.

But this architecture introduces new failure modes. The most dangerous is anchoring bias. When humans see a machine-generated label, they anchor to it. If the draft says "urgent," the human reviewer is more likely to confirm "urgent" even if the correct label is "routine." This is not annotator laziness. It is cognitive bias. Humans are faster and more confident when confirming a presented answer than when generating an answer from scratch. Research from 2024 and 2025 on human-AI collaboration consistently shows that annotators over-rely on AI suggestions, especially when fatigued, uncertain, or working under time pressure. If your machine drafts are biased, your human-verified labels will be biased in the same direction, just with a false sense of validation. You will have spent money on human review that rubber-stamped machine errors instead of correcting them.

## Prompt Design for Labeling Tasks

The quality of LLM-generated labels depends almost entirely on prompt design. This is not the same as prompt engineering for end-user applications. Labeling prompts require extreme precision, explicit schema definitions, and carefully constructed examples. A labeling prompt has five essential components: task definition, label schema, output format, reasoning instructions, and edge case guidance. Miss any one, and label quality degrades unpredictably.

Task definition states what you are labeling and why. Not "classify this text" but "classify this customer support message by the type of issue the customer is reporting, so that it can be routed to the correct specialist team." The why matters because it disambiguates edge cases. If the task is routing, then a message that mentions both billing and technical issues should be labeled by whichever issue is primary. If the task is issue tracking, it might warrant multi-label annotation. The model does not infer intent from context. You must state it explicitly.

Label schema defines every possible label, what it means, and how it differs from adjacent labels. Not "urgent, routine, informational" but "Urgent: requires clinical action within 24 hours, such as medication changes, new symptoms, or test results indicating risk. Routine: requires clinical follow-up within one week, such as prescription refills, appointment scheduling, or minor symptom questions. Informational: no clinical action required, such as appointment confirmations, billing questions, or general health education." Each label gets a definition and multiple examples. Definitions use bright-line rules when possible. "Within 24 hours" is a bright line. "Seems important" is not. The schema should also explicitly cover boundary cases: what if a message is both urgent and informational? Which label wins? State the precedence rule.

Output format specifies exactly how the model should return the label. Structured output formats—JSON schemas enforced by API parameters in late 2025 and 2026 models—eliminate a whole class of parsing errors. You define the schema once: label field as string constrained to your label set, confidence field as float between 0 and 1, reasoning field as string explaining the choice. The model cannot return anything else. This is vastly superior to asking the model to "respond with the label" and hoping it does not add commentary, apologize, or wrap the label in markdown. Structured outputs also enable reliable batch processing. You send 1,000 items, get back 1,000 valid JSON objects, and ingest them directly into your annotation database without parsing hacks.

Reasoning instructions tell the model how to think through the task. For most labeling tasks, you want the model to explain its reasoning before choosing a label. This is chain-of-thought reasoning adapted to annotation. The prompt says: "First, identify the key details in the message that indicate urgency or routine status. Second, compare those details to the label definitions. Third, select the label that best matches." This structure improves label quality and provides valuable signal in the reasoning field. When a human reviewer sees a draft label, they also see the model's reasoning. If the reasoning is sound but the label is wrong, that indicates a schema ambiguity. If the reasoning is nonsensical, that indicates a prompt problem or model limitation. If the reasoning is sound and the label is correct, the human can approve quickly with confidence. Reasoning makes the black box transparent.

Edge case guidance is where most labeling prompts fail. You must explicitly tell the model how to handle ambiguity, missing information, and corner cases. What if the message is unclear? What if it fits multiple labels equally well? What if critical context is missing? The prompt must specify: "If the message does not contain enough information to confidently assign a label, use the Unclear label and note what information is missing in the reasoning field. If the message fits multiple labels, choose the label that represents the highest priority action." Without this, models default to their pretraining behavior, which is to hedge, apologize, or make inconsistent guesses. You will see labels like "it could be urgent or routine depending on context" which is useless for downstream systems. Edge case guidance forces the model into the same decision-making framework you would give a human annotator.

## Model Selection and Configuration

Not all models are equally good at labeling tasks. As of early 2026, GPT-5, Claude Opus 4.5, and Gemini 2.0 are the most commonly used LLM-as-judge models for production labeling pipelines. Llama 4 and other open-weight models are used when cost or data privacy constraints prohibit API calls to external providers. The choice depends on label complexity, throughput requirements, and cost tolerance.

For simple categorical labels with clear definitions—sentiment, topic classification, spam detection—smaller models often suffice. GPT-5-mini or Claude Opus 4.5 Haiku can deliver 90% agreement with human annotators at one-tenth the cost of frontier models. For complex labels requiring domain knowledge, nuanced reasoning, or multi-step judgment—medical coding, legal clause classification, content policy violation detection—frontier models perform significantly better. A 2025 study comparing model performance on content moderation labeling found that GPT-5 achieved 89% agreement with expert human moderators while GPT-3.5 achieved only 71%. The cost difference was real but the quality gap was larger. Saving money on a cheaper model is a false economy if it means your human reviewers spend twice as long correcting errors.

Temperature settings matter more for labeling than for generation tasks. Most labeling prompts should use temperature zero or very close to it. You want deterministic, consistent labels, not creative variation. A temperature of zero means the model always picks the most likely token, producing the same output for the same input every time. This is exactly what you want for labeling. Variability is a bug, not a feature. Some practitioners worry that temperature zero makes the model "too confident" but confidence is controlled by the model's internal calibration and your schema design, not by temperature. Temperature only controls sampling randomness. For labeling, randomness is noise.

There is one exception: when you want multiple independent labels for the same item to measure uncertainty. In this case, you might run the same item through the model five times at temperature 0.3, collect five labels, and measure agreement. If all five labels agree, confidence is high. If they split three to two or scatter across labels, confidence is low. This technique, sometimes called self-consistency sampling, can provide useful confidence signals but it costs five times as much as a single label. Use it selectively, on items where confidence calibration matters most—typically items near decision boundaries or items flagged by heuristics as potentially ambiguous.

## Calibrating LLM Judges Against Human Baselines

An LLM-as-judge is only useful if its labels correlate strongly with human expert labels. Before deploying LLM-generated drafts into production, you must calibrate the model against a human-labeled baseline. This means taking a representative sample—typically 500 to 2,000 items depending on label complexity and class balance—and having both the model and human experts label them independently. You then measure agreement using metrics appropriate to your label type.

For single-label classification, use Cohen's kappa or Fleiss' kappa if you have multiple human annotators. Kappa corrects for chance agreement, which matters when class distributions are imbalanced. If 80% of items are "routine," a model that labels everything "routine" achieves 80% accuracy but near-zero kappa. Kappa tells you whether agreement is better than random. A kappa above 0.75 is considered excellent agreement. Between 0.60 and 0.75 is good. Between 0.40 and 0.60 is moderate. Below 0.40 means the model is not reliable enough to generate drafts worth reviewing. You are better off with pure human labeling.

For multi-label classification, measure precision, recall, and F1 per label and in aggregate. Some labels will have high agreement, others low. This is expected. A content moderation model might achieve 92% F1 on hate speech labels but only 68% F1 on misinformation labels because misinformation requires more contextual reasoning. The per-label breakdown tells you which drafts to trust and which to route directly to humans without a draft. It also tells you where to invest in prompt refinement. If one label has systematically low agreement, revisit its definition, add examples, or clarify edge case rules.

For span annotation—named entity recognition, highlight extraction, quote attribution—measure exact match, partial match, and boundary agreement separately. Exact match is strict: the model's span must match the human's span character-for-character. Partial match credits overlap: if the model highlights "acute respiratory distress" and the human highlights "respiratory distress," that is a partial match. Boundary agreement measures how often the model gets the boundaries approximately right even if not exact. These three metrics together tell you whether the model is finding the right spans, approximately the right spans, or completely wrong spans. For draft label purposes, partial match agreement above 80% is often good enough because humans can easily adjust boundaries when reviewing.

After measuring baseline agreement, you analyze disagreements to understand whether they are random noise or systematic bias. Random disagreements scatter across labels and items with no pattern. Systematic disagreements cluster. The model consistently over-labels certain categories, under-labels others, or makes the same error on the same item type. For example, a clinical note classifier might systematically label patient questions as "urgent" when they are actually "routine" because the model picks up on question marks and keywords like "worried" or "concerned" without weighing the clinical content. This is a fixable prompt problem. You add examples of non-urgent patient questions, adjust the definition of "urgent" to emphasize clinical criteria over emotional tone, and retest. Systematic bias is a feature of the prompt or model choice, not an inherent limitation. Random noise is harder to fix and usually means you have hit the ceiling of what the model can do given the available context.

## Failure Modes: Sycophancy, Position Bias, and Length Bias

LLMs trained with reinforcement learning from human feedback exhibit predictable biases that corrupt labeling tasks if not accounted for. The three most common are sycophancy, position bias, and length bias. Each manifests differently and requires different mitigation strategies.

Sycophancy is the tendency of models to agree with perceived authority or user preference. In a labeling context, this shows up when the prompt inadvertently signals a preferred answer. If your prompt says "this message seems urgent, please label it," the model will almost always label it urgent regardless of content because it interprets "seems urgent" as your opinion and RLHF training has taught it to align with user intent. Even subtle cues trigger this. A prompt that says "messages like this are often routine" biases the model toward "routine." The fix is to scrub all evaluative language from prompts. Never say "seems," "often," "typically," or anything that implies a prior. Present the item and schema neutrally, with no commentary.

Position bias is the tendency of models to favor options presented early in a list or late in a list depending on architecture and training. GPT-4 models as of early 2026 show slight recency bias: if you list labels as "urgent, routine, informational," the model slightly favors "informational." Claude models show slight primacy bias: they favor the first option. These biases are small, typically shifting probabilities by two to five percentage points, but they compound over thousands of labels. The fix is to randomize label order across items. If half your items list "urgent" first and half list it last, position bias cancels out in aggregate. For systems using structured output with fixed schemas, you cannot randomize the schema order, so you accept the small bias and account for it in calibration.

Length bias is the tendency of models to prefer longer or more detailed outputs when generating text, and to associate length with quality when evaluating text. For labeling tasks involving text quality, sentiment intensity, or content severity, models often conflate length with intensity. A long, detailed complaint is more likely to be labeled "high severity" than a short complaint expressing the same issue. A verbose policy violation is more likely to be flagged than a terse one. This bias exists because models are trained on data where length and quality correlate—longer answers are often more helpful, longer documents often more authoritative. But for labeling, length is noise. A two-word message can be urgent. A paragraph can be informational. The fix is to include explicit length-invariance instructions in the prompt: "Label severity based on the issue described, not the length or detail of the message. A short message can be high severity. A long message can be low severity." You also include short-and-severe and long-and-mild examples in your few-shot demonstrations.

## When LLM-as-Judge Works Well

LLM-as-judge works best for labeling tasks that require linguistic understanding, contextual reasoning, or pattern recognition but do not require specialized knowledge beyond what is encoded in the model's training data. Sentiment classification is a canonical good fit. Models trained on internet text have seen millions of examples of sentiment expression and can reliably distinguish positive, negative, and neutral tone. Topic classification works well when topics are common: technology, sports, politics, health. Spam detection works well because spam has clear linguistic patterns: urgency, scarcity, suspicious links, overpromising language. Intent classification for customer support messages works well because models have seen thousands of support conversations during pretraining.

LLM-as-judge also works well when the labeling task involves synthesizing information from a moderately long text. Summarization quality evaluation, for example, is tedious for humans but straightforward for models. Given a source document and a summary, the model checks whether the summary is factually consistent with the source, whether it captures key points, and whether it introduces unsupported claims. Human reviewers still verify a sample, but the model can draft quality scores for thousands of summaries in minutes. This is faster and often more consistent than asking humans to read full documents and summaries repeatedly.

Another strong use case is label consistency enforcement. Suppose you have an existing labeled dataset produced by multiple human annotators over time. You suspect drift or inconsistency. You can run the entire dataset through an LLM-as-judge and measure where the model disagrees with existing labels. Items with high model confidence that contradict the original label are candidates for human re-review. This does not mean the model is right and the human was wrong, but it flags potential errors cheaply. A 2025 case study from a legal tech company found that re-reviewing model-flagged inconsistencies caught 340 labeling errors in a 50,000-item contract dataset, errors that had propagated into production model training.

## When LLM-as-Judge Works Poorly

LLM-as-judge fails when the labeling task requires knowledge not present in the model's training data. Medical coding using ICD-10 or CPT codes requires knowing thousands of specific code definitions, conventions, and exclusions that change yearly. Models might hallucinate codes, apply outdated codes, or misinterpret clinical terminology. Legal contract clause classification requires understanding jurisdictional variations, precedent, and clause interactions. Models might correctly identify a "termination clause" but fail to distinguish between termination-for-cause and termination-for-convenience, a distinction that matters for downstream legal risk assessment. Domain-specific labeling often requires fine-tuning on domain data or switching to pure human labeling by credentialed experts.

LLM-as-judge also struggles with subjective labels where reasonable experts disagree. Content moderation at the boundary of acceptable speech is a classic case. Is a political cartoon offensive or satirical? Is a pointed critique hate speech or protected opinion? These questions have no single correct answer. They depend on cultural context, platform norms, and individual judgment. An LLM will give an answer, but that answer reflects the biases of its training data, not a principled adjudication. Using LLM drafts for highly subjective labels risks encoding majority-culture biases as ground truth. For these tasks, you still need diverse human reviewers, deliberation processes, and acknowledgment of irreducible disagreement.

Another failure mode is tasks requiring real-time or recent knowledge. If your labeling task is "does this news article contain information contradicted by events in the past 24 hours," the model has no access to those events unless you provide them in context. Models do not browse the web in labeling pipelines. They see only the prompt. If you need external knowledge, you must retrieve it yourself and inject it into the prompt, which adds complexity and cost. For time-sensitive labeling, pure human review with access to up-to-date information is often simpler.

Finally, LLM-as-judge fails when the cost of errors is extremely high and the task requires legally defensible audit trails. If incorrect labels lead to patient harm, financial loss, or legal liability, you cannot rely on probabilistic model outputs as the source of truth. The model can still generate drafts, but those drafts must be verified by qualified humans, and the human's judgment must be the ground truth with full traceability. In these settings, LLM-as-judge is a productivity aid, not a decision-maker. The human is always in the loop, always accountable, and always documented.

## Measuring and Monitoring Draft Quality Over Time

Once LLM-as-judge is in production, draft quality must be monitored continuously. Models do not drift the way humans do, but your data does. If the distribution of items changes—new product lines, new customer segments, new content types—the model's calibration shifts. A model calibrated on customer support messages from North American users might perform worse on messages from users in other regions with different phrasing conventions. A model calibrated on content from 2024 might misclassify slang or references that emerged in 2025.

The core monitoring metric is human override rate: what percentage of machine drafts do human reviewers change? If override rate is stable at 12%, your model is performing consistently. If it jumps to 22%, something has shifted. You investigate. Did the item distribution change? Did you deploy a new model version? Did your labeling guidelines change? Human override rate is a leading indicator. It catches drift before it corrupts your dataset.

You also monitor override rate by label, by item source, and by annotator. If one label has a suddenly high override rate, that label's definition might need clarification or the model's prompt might need adjustment. If items from a new data source have high override rates, that source has different characteristics than your calibration set. If one annotator has a much higher override rate than others, either that annotator interprets the schema differently or they are less tolerant of minor draft errors. Both are useful signals. Annotation is a sociotechnical system. The monitoring has to cover both the technical component—the model—and the social component—the annotators.

Another key metric is reasoning quality. When reviewers override a label, you can ask them to flag whether the model's reasoning was directionally correct but reached the wrong conclusion, or whether the reasoning was nonsensical. High reasoning quality with wrong labels suggests schema ambiguity or edge cases the prompt did not cover. Low reasoning quality suggests model limitations or prompt degradation. This qualitative signal guides where to invest in improvements.

You should also run periodic re-calibration. Every quarter or after significant data distribution shifts, take a fresh sample of items, have experts label them, compare to model drafts, and recalculate agreement metrics. If agreement has degraded, you retrain prompts, switch models, or add new examples. If agreement has improved—perhaps because you refined prompts based on override patterns—you can consider increasing auto-approval thresholds for high-confidence labels, further reducing human review costs.

## The Handoff to Human Reviewers

The interface between LLM-generated drafts and human reviewers is where the system succeeds or fails operationally. Reviewers need to see three things: the original item, the machine-generated label, and the machine's reasoning. Showing only the item and label invites anchoring bias without transparency. Showing reasoning allows the reviewer to judge whether the model understood the task. If the reasoning is sound, the reviewer can approve quickly. If the reasoning is flawed, the reviewer knows to scrutinize closely.

The review interface should make it equally easy to approve or correct. If approving takes one click and correcting takes five, annotators will over-approve. If correcting is easier than approving, annotators will over-correct to feel productive. The interaction design must be neutral. Show the draft label as a suggestion, not a default. Require the annotator to make an active choice: approve, correct, or escalate. Track time-to-decision. If annotators are approving in under two seconds, they are not reading. If they are taking over 30 seconds on simple items, the interface is too complex or the draft quality is too low.

You also need to surface confidence scores when available. If the model returns a label with 0.92 confidence, that is different from 0.58 confidence. High-confidence drafts can be shown with a visual indicator—green border, confidence badge—so annotators know the model is certain. Low-confidence drafts can be flagged for extra scrutiny. This is the foundation of confidence-based routing, which determines what gets auto-labeled and what requires human review, a topic we will explore in the next subchapter.
