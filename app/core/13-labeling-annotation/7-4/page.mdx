# 7.4 — Active Learning: Prioritizing the Most Informative Examples

Labeling examples in the order they arrive in your database is an expensive way to teach your model nothing new. The marginal value of the ten thousandth routine transaction label approaches zero, while the value of a single well-chosen edge case can shift decision boundaries that affect thousands of predictions. Active learning inverts this dynamic by selecting examples based on information content rather than arrival order, prioritizing the cases where the model is most uncertain, most likely to be wrong, or most likely to generalize to unseen patterns. This is not an optimization—it is a fundamental rethinking of how annotation budgets should be allocated.

The failure was a resource allocation problem. Annotation budgets are finite. Every dollar spent labeling a routine example that teaches the model nothing new is a dollar not spent on the ambiguous, boundary-defining case that would actually improve performance. Active learning solves this by flipping the selection process: instead of labeling examples in arbitrary order, you label the examples the model is most uncertain about, the examples most likely to change the decision boundary, the examples that carry the most information per annotation dollar. This subchapter explains how to implement active learning in production labeling operations, when it delivers value versus when random sampling suffices, and how to avoid the common failure modes that turn active learning into expensive theater.

## The Core Principle: Maximize Information Gain per Labeled Example

Active learning is a query strategy. The model examines a pool of unlabeled data and asks: which example, if I knew its true label, would teach me the most? You then send that example to human annotators, get the ground truth label, retrain the model with the new labeled example, and repeat. The goal is not to label a representative sample of the data distribution—that is what random sampling does. The goal is to label the examples that most efficiently collapse the model's uncertainty and refine its decision boundaries.

The healthcare company's mistake was treating all examples as equally valuable. A model trained on five thousand examples might have ninety-five percent confidence on routine medication refills. Labeling another thousand refills adds almost nothing. But the model might have fifty-five percent confidence on experimental treatment requests. Labeling two hundred of those examples could shift accuracy on that segment from sixty percent to eighty-two percent. Active learning makes this trade explicit: you measure information gain and allocate annotation effort accordingly.

This works because machine learning models do not learn uniformly. A model learns most when it encounters examples near its current decision boundary, examples where it is confused, examples that force it to distinguish between classes it currently conflates. Once a model has seen enough examples of a pattern to internalize it, additional examples of that same pattern yield diminishing returns. Active learning exploits this by concentrating labeling effort on the frontier of the model's ignorance. You do not waste time labeling what the model already knows.

In practice, this means you never label data in chronological order, alphabetical order, or random order—at least not after the initial cold-start phase. You label in **uncertainty order**: most uncertain first, most confident last. The result is faster convergence to target performance with fewer labeled examples. Studies across domains show active learning can reduce labeling costs by forty to seventy percent compared to random sampling, depending on task complexity and label noise. The healthcare company could have reached the same model performance with eighteen thousand labeled examples instead of fifty thousand, saving $200,000 and four months.

## Uncertainty Sampling: The Simplest and Most Common Strategy

Uncertainty sampling is the most widely used active learning strategy. The model scores each unlabeled example with a confidence measure. You select the examples with the lowest confidence—the ones the model is most uncertain about—and send them for annotation. This works with any model that outputs probabilities or confidence scores. For binary classification, uncertainty is highest when the probability is closest to 0.5. For multi-class classification, you measure uncertainty as one minus the maximum predicted probability, or as entropy across the class distribution.

A content moderation system classifying posts as safe, spam, harassment, or hate speech might predict probabilities of 0.42 safe, 0.31 spam, 0.18 harassment, and 0.09 hate speech for a given post. The maximum probability is 0.42, so uncertainty is 0.58. Another post might have probabilities of 0.91 safe, 0.05 spam, 0.03 harassment, and 0.01 hate speech. The maximum probability is 0.91, so uncertainty is 0.09. You annotate the first post first, because the model does not know what it is. You skip the second post, because the model is confident.

Uncertainty sampling works well when model confidence is well-calibrated—that is, when a predicted probability of 0.6 actually means the model is correct sixty percent of the time. Many deep learning models are poorly calibrated out of the box: they output probabilities near 0.9 or 0.1 even on examples they should be uncertain about. Calibration techniques like temperature scaling, Platt scaling, or isotonic regression improve this. If your model is uncalibrated, uncertainty sampling degrades into nearly random sampling, and you lose most of the efficiency gain. Calibration is not optional for active learning.

You also need to decide how to handle ties. If you have budget to label one hundred examples and five hundred examples have uncertainty above 0.55, which ones do you choose? One approach is to break ties randomly. Another is to add a secondary criterion: diversity, recency, or domain coverage. A fraud detection system might prioritize uncertain examples from underrepresented merchant categories or payment methods to avoid concentrating labels on a single segment. Tie-breaking rules matter when the uncertainty distribution is flat, which happens often in mature models.

## Query-by-Committee: Using Model Disagreement as a Signal

Query-by-committee is an alternative to uncertainty sampling. Instead of training one model and measuring its uncertainty, you train multiple models—often with different architectures, different hyperparameters, or different subsets of the training data—and measure their disagreement. You select examples where the models disagree most strongly and send those for annotation. The intuition is that disagreement indicates ambiguity: if multiple reasonable models trained on the same data cannot agree on the label, the example is informative.

A legal document classification system might train three models: a transformer-based classifier, a recurrent neural network, and a gradient-boosted decision tree. For most documents, all three models agree. For some documents, the transformer predicts contract, the RNN predicts amendment, and the tree predicts addendum. These are the documents you annotate first. The disagreement reveals that the example sits at a decision boundary none of the models has cleanly learned.

Query-by-committee works best when the models are diverse—when they make different kinds of errors. Training five transformers with slightly different random seeds does not create diversity; they will agree on almost everything. Training a transformer, an RNN, and a tree creates diversity because the models have different inductive biases. The transformer attends to long-range dependencies, the RNN to sequential structure, the tree to feature interactions. Their disagreements are meaningful.

The downside is computational cost. You must train and maintain multiple models. For large models or large datasets, this multiplies training time and infrastructure costs. Many teams start with uncertainty sampling for this reason and only move to query-by-committee if uncertainty sampling plateaus or if they already train ensembles for other reasons, like calibration or robustness.

You also need to define disagreement precisely. For binary classification, disagreement is simple: the models predict different classes. For multi-class classification, you can measure disagreement as variance in predicted probabilities across models, as entropy of the vote distribution, or as the proportion of models that disagree with the majority vote. A fraud system with five models might have three models predicting fraud and two predicting legitimate. The vote distribution has entropy. You quantify that and rank examples by disagreement.

## Diversity Sampling: Avoiding Redundant Clusters

Uncertainty sampling and query-by-committee both focus on individual example informativeness. But informativeness is not the only consideration. If you select the top one hundred most uncertain examples and they are all slight variations of the same pattern—say, a specific phrasing in customer support tickets—you waste annotation effort on redundancy. Diversity sampling addresses this by ensuring the selected examples cover different regions of the input space. You want uncertain examples that are also different from each other.

A product categorization system might find that the most uncertain examples are all electronics accessories: phone cases, charging cables, screen protectors. These examples are uncertain because the model struggles with subcategory distinctions in electronics. But labeling one hundred phone cases does not help the model learn to categorize furniture, apparel, or food. Diversity sampling would select a mix of uncertain examples across categories, ensuring the annotation batch improves the model globally rather than locally.

One common approach is clustering. You embed all unlabeled examples in a vector space using the model's intermediate representations. You cluster the embeddings. Within each cluster, you select the most uncertain examples. This ensures coverage across clusters. A customer feedback analysis system might cluster feedback by topic—billing, product quality, shipping, customer service—and then select the most uncertain examples from each topic cluster. This prevents oversampling from a single topic.

Another approach is greedy diversification. You select the most uncertain example first. You then select the next example that is both highly uncertain and maximally distant from the already-selected examples in embedding space. You repeat until the batch is full. This greedy algorithm is computationally cheap and avoids redundant selections. The distance metric can be cosine distance, Euclidean distance, or any metric appropriate to the embedding space.

Diversity sampling introduces a trade-off. Pure uncertainty sampling maximizes immediate information gain but risks redundancy. Pure diversity sampling maximizes coverage but might include low-information examples from underrepresented regions. The optimal balance depends on the data distribution and the model's current weaknesses. Many teams use a weighted combination: select examples that score high on both uncertainty and diversity, with tunable weights that shift over time.

## Batch Active Learning: Selecting Sets, Not Singles

Most production labeling operations do not annotate one example at a time. You send batches of one hundred, five hundred, or a thousand examples to annotators and wait for the batch to be completed before retraining the model. This creates a batch selection problem: you need to choose a set of examples that, taken together, maximizes information gain. Selecting the top five hundred most uncertain examples individually might create redundancy within the batch. Batch active learning methods optimize for set-level informativeness.

One approach is to combine uncertainty and diversity as described above: rank by uncertainty, then greedily add examples that are also diverse relative to the already-selected batch. Another approach is submodular optimization. You define a set function that measures the informativeness of a batch—something like coverage of the input space or reduction in model uncertainty—and you prove the function is submodular, meaning greedy selection is near-optimal. This is computationally more expensive but theoretically cleaner.

A healthcare claims processing system labels batches of five hundred claims per week. The system embeds claims in a 768-dimensional space and defines a coverage function: the batch should maximize the volume of the convex hull in embedding space, ensuring wide coverage. The system then uses a greedy algorithm to select claims that incrementally expand the hull. This ensures the batch is diverse. Within each region of the hull, the system selects the most uncertain claims. The result is a batch that is both informative and diverse.

Batch size matters. If you label ten examples at a time and retrain after each batch, you can adapt quickly but incur high retraining overhead. If you label five thousand examples at a time, retraining is infrequent but the model cannot adapt to what it learns from the first thousand before selecting the next four thousand. The optimal batch size balances retraining cost against adaptation speed. For most teams, batches of two hundred to one thousand examples hit the sweet spot.

You also need to decide when to retrain. Some teams retrain after every batch. Others retrain only when the batch is large enough to materially shift the model, which might be every third or fifth batch. Retraining too frequently burns compute. Retraining too infrequently means later examples in the queue are selected based on an outdated model. A fraud detection team might retrain weekly if fraud patterns shift rapidly, but a document classification team might retrain monthly if the distribution is stable.

## Integration with Labeling Queues and Workflow Tools

Active learning is not a research experiment; it is a production capability embedded in labeling workflow tools. This means the selection algorithm must integrate with the queue management system, the annotator assignment logic, and the model retraining pipeline. The unlabeled data pool must be refreshed as new data arrives. The labeled data must be fed back into the model training loop. The system must track which examples have been selected, which are in progress, which are complete, and which are pending review.

A common architecture is a selection service that runs on a schedule—daily or weekly—and populates a labeling queue with the highest-priority examples. Annotators pull from the queue. When an example is labeled, it is removed from the unlabeled pool and added to the training set. When the training set reaches a threshold size or a time window elapses, a retraining job is triggered. The new model is deployed, and the cycle repeats.

This requires tight integration between the model inference service, the data storage layer, and the labeling platform. The model must score the entire unlabeled pool or a large sample of it. The scores must be written to a database or cache accessible to the queue manager. The queue manager must handle concurrency: multiple annotators pulling from the queue simultaneously. The system must handle failures: if an annotator starts an example but does not complete it, the example must be released back to the pool or reassigned.

A financial services company building a transaction monitoring system runs active learning nightly. The model scores all transactions flagged by rule-based detectors but not yet reviewed by analysts. The top one thousand most uncertain transactions are pushed to the review queue. Analysts label them over the next day. Each evening, the system checks if at least five hundred new labels have been added since the last retraining. If so, it triggers a retraining job. The new model is deployed the next morning, and the cycle repeats. This creates a continuous learning loop.

You also need to handle annotator capacity constraints. If active learning selects five hundred examples but you only have capacity to label two hundred per week, you need a prioritization rule within the selected set. You might label the top two hundred by uncertainty and defer the rest. Or you might interleave active learning selections with random samples to maintain some coverage of the full distribution, preventing the model from overfitting to its own uncertainty.

## Diminishing Returns: When to Stop Active Learning

Active learning delivers the largest gains early in the labeling process, when the model is most uncertain and every label teaches it something new. As the model improves, uncertainty decreases, and the marginal value of each additional label declines. Eventually, active learning converges to random sampling: the model is confident on most examples, and the remaining uncertain examples are either noise, edge cases, or genuinely ambiguous examples that even humans struggle with.

A sentiment analysis system might start with one thousand labeled examples. Active learning reduces the number of examples needed to reach eighty-five percent accuracy from ten thousand to four thousand, a sixty percent reduction. To reach ninety percent accuracy, active learning reduces the requirement from thirty thousand to fifteen thousand, a fifty percent reduction. To reach ninety-three percent, active learning reduces the requirement from seventy thousand to fifty thousand, a twenty-eight percent reduction. The gains shrink as the model matures.

You know active learning is reaching diminishing returns when the uncertainty distribution flattens. Early in training, the uncertainty distribution is bimodal: many high-confidence examples, many high-uncertainty examples. As the model learns, the high-uncertainty peak shrinks. Eventually, almost all examples have low uncertainty, and the few remaining high-uncertainty examples are outliers. At that point, active learning is selecting from noise, and random sampling is equally effective and cheaper to implement.

Another signal is when the model's performance on a held-out validation set stops improving. If you retrain after each batch and performance plateaus for three consecutive retrains, active learning has likely exhausted its value. You are now labeling examples that do not shift the decision boundary. You can switch to random sampling or stop labeling altogether if the model meets the target performance.

Some teams use a hybrid strategy: active learning for the first ten thousand labels, random sampling for the next ten thousand. This ensures the model learns the hard cases first, then fills in coverage of the full distribution. The healthcare prior authorization team could have used active learning for the first fifteen thousand labels, focusing on rare conditions and edge cases, then switched to random sampling to ensure broad coverage of routine cases. This would have saved $180,000 and still produced a well-rounded model.

## When Active Learning Helps Versus When Random Sampling Suffices

Active learning is not always worth the complexity. It shines in specific scenarios and adds overhead in others. You should use active learning when the data distribution is highly imbalanced, when annotation budgets are tight, when the model's weaknesses are concentrated in rare slices, or when you need to reach target performance quickly. You should skip active learning when the distribution is balanced, when annotation is cheap, when the model performs uniformly across slices, or when you have unlimited data and compute.

Active learning helps most with imbalanced distributions. If ninety-five percent of examples belong to one class and five percent to another, random sampling will give you 950 labels for the majority class and 50 for the minority class in a batch of one thousand. Active learning will oversample the minority class and the decision boundary, giving you a more balanced effective training set. A fraud detection system with a fraud rate of 0.8 percent benefits enormously from active learning because random sampling would require labeling 125,000 examples to get one thousand fraud labels. Active learning can surface one thousand fraud examples from a pool of 10,000 by selecting the most uncertain cases.

Active learning helps when annotation is expensive. If each label costs $10 and you need ten thousand labels to reach target performance with random sampling, that is $100,000. If active learning reduces the requirement to four thousand labels, that is $40,000. The cost of implementing active learning—engineering time, model retraining overhead, infrastructure—might be $15,000. You save $45,000 net. But if each label costs $0.10, the savings are $600, and the implementation cost exceeds the savings. Active learning is not worth it.

Active learning helps when you need to iterate fast. If you are prototyping a new model and need to assess feasibility, active learning lets you reach reasonable performance with a small labeled set, letting you evaluate viability before committing to a full labeling campaign. A research team exploring a new document extraction task might use active learning to label five hundred examples, train a model, and check if accuracy is promising. If it is, they scale up. If not, they pivot without wasting $50,000 on labels.

Active learning does not help when the model already performs well everywhere. If your baseline model has ninety percent accuracy across all slices and you are trying to push to ninety-two percent, the remaining errors are likely scattered uniformly, and random sampling is fine. Active learning targets concentrated weaknesses, not diffuse ones. Similarly, if your task is so easy that even random sampling gives you target performance with two thousand labels, active learning is overkill.

## Common Failure Modes and How to Avoid Them

Active learning fails when the model's confidence is uncalibrated, when the unlabeled pool is not representative of the deployment distribution, when annotators introduce label noise that confuses the model, or when the selection strategy is too narrow and the model overfits to its own uncertainty.

Uncalibrated confidence is the most common failure. A model that outputs 0.95 confidence on every example, even wrong ones, will not surface uncertain examples for active learning. You must calibrate the model using a validation set before using it for active learning. Calibration techniques like temperature scaling or Platt scaling adjust the predicted probabilities to match empirical frequencies. After calibration, a predicted probability of 0.7 means the model is correct seventy percent of the time. Calibration is a prerequisite for uncertainty sampling.

Unrepresentative unlabeled pools are the second common failure. If the unlabeled pool is heavily skewed toward one domain, active learning will select examples from that domain, and the model will overfit to it. A customer support ticket classifier trained on English tickets might be deployed globally. If the unlabeled pool used for active learning contains only English tickets, the model will not learn to handle Spanish, French, or Mandarin tickets. You must ensure the unlabeled pool mirrors the deployment distribution or use stratified active learning that samples proportionally from each domain.

Label noise confuses active learning. If annotators are inconsistent or if the task is genuinely ambiguous, the most uncertain examples might also be the noisiest. The model trains on noisy labels, becomes more confused, and selects even noisier examples in the next round. This creates a vicious cycle. You mitigate this with quality control: require multiple annotators per example, measure inter-annotator agreement, and filter out examples where agreement is low. If an example has three annotators and they split one fraud, one legitimate, one unsure, that example might be inherently ambiguous and not worth including in the training set.

Narrow selection strategies cause overfitting. If you only ever select the single most uncertain example, the model will overfit to that example's neighborhood and ignore the rest of the distribution. Batch active learning with diversity constraints prevents this. You select batches of diverse uncertain examples, ensuring the model learns broadly rather than narrowly.

Finally, active learning can fail if the model architecture or feature set is fundamentally mismatched to the task. If a linear model is trying to learn a highly nonlinear boundary, no amount of active learning will help. Active learning accelerates learning; it does not fix a broken model. You must start with a model that can, in principle, learn the task given enough data. Active learning then reduces the "enough data" requirement.

## Measuring Active Learning Effectiveness

You measure active learning effectiveness by comparing learning curves: model performance as a function of number of labeled examples, with active learning versus random sampling. You run both strategies in parallel on the same data and track how quickly each reaches target performance. Active learning is effective if it reaches the target with significantly fewer labels.

A document classification team runs an experiment. They have one hundred thousand unlabeled documents and a budget to label ten thousand. They run two labeling campaigns: one labels documents randomly, the other uses uncertainty sampling. Both retrain the model after every one thousand labels. They plot accuracy on a held-out test set against number of labels. Random sampling reaches eighty-five percent accuracy at six thousand labels. Uncertainty sampling reaches eighty-five percent at 3,500 labels, a forty-two percent reduction. The team concludes active learning is worth implementing.

You also track the uncertainty distribution over time. Early in training, the histogram of predicted probabilities is broad. As the model learns, the histogram becomes more peaked near zero and one, indicating the model is more confident. If the histogram does not sharpen over time, the model is not learning, and active learning is not helping.

Another metric is the fraction of selected examples that flip the label when annotated. If the model predicts class A with sixty percent confidence and the true label is class B, that example was informative. If the model predicts class A with sixty percent confidence and the true label is also class A, the example was not informative—the model was right despite being uncertain. A high flip rate indicates the model is selecting genuinely uncertain examples. A low flip rate indicates the model is well-calibrated but already correct on most uncertain examples, meaning diminishing returns.

You also track annotation cost savings. If active learning reduces labeling requirements by fifty percent and each label costs $5, and you need ten thousand labels with random sampling, that is $50,000. With active learning, you need five thousand labels, or $25,000. The implementation cost of active learning might be $8,000 in engineering time and infrastructure. Net savings: $17,000. If implementation cost exceeds savings, active learning is not economical for your use case.

## Practical Implementation Checklist

Before implementing active learning, you need a calibrated model, a representative unlabeled pool, a labeling queue that integrates with model retraining, and a plan for handling diminishing returns. You need to choose a selection strategy—uncertainty sampling, query-by-committee, or diversity sampling—and decide on batch size and retraining frequency. You need quality control on annotations and a way to measure whether active learning is delivering cost savings.

Start with a baseline model trained on random samples. Calibrate it. Measure its performance on a held-out test set and identify the slices where it underperforms. Use active learning to oversample those slices. Track the learning curve: retrain after each batch and measure test set performance. If the curve is steep—performance improves rapidly with few labels—active learning is working. If the curve is flat, either the model is already good or it cannot learn the task.

Monitor the uncertainty distribution and the flip rate. If both are healthy, continue active learning until the model reaches target performance or until the curve flattens, indicating diminishing returns. At that point, switch to random sampling to fill in coverage, or stop labeling if the model is good enough.

Document the total cost: annotation cost with active learning, annotation cost with random sampling, implementation cost, and savings. Share the analysis with stakeholders. If active learning saved $40,000, that justifies the investment and sets a precedent for future labeling campaigns. If it saved $2,000, it might not be worth repeating.

You also need infrastructure to support continuous active learning. The model must be retrained regularly, which means you need automated training pipelines, model versioning, and deployment automation. A manual retraining process that takes three days to run and two days to review kills active learning's velocity. You need to retrain in hours, not days. This requires investment in MLOps: containerized training jobs, experiment tracking, automated validation, and canary deployments.

The cold-start problem is another consideration. Active learning requires a model to generate uncertainty scores. If you have zero labeled data, you cannot train a model. You need a cold-start strategy: label a random sample of five hundred to two thousand examples to bootstrap the first model, then switch to active learning. The size of the cold-start set depends on task complexity. A binary classification task might need only five hundred examples. A multi-class classification task with twenty classes might need two thousand. A structured extraction task might need three thousand. The goal is to train a model that is better than random guessing, not a model that is production-ready.

Some teams skip active learning during the cold-start phase and instead use heuristics or rules to create weak labels for bootstrapping. A spam detection system might label emails with certain keywords as spam and emails from known senders as not spam. These weak labels are noisy but sufficient to train a first-pass model. The model then drives active learning, and human annotators correct the most uncertain weak labels. This hybrid approach reduces cold-start labeling costs.

Active learning also requires careful handling of label noise. If annotators make mistakes, the model trains on those mistakes and becomes more confused. Noise is especially harmful in active learning because the model is learning from its hardest cases, and noisy labels on hard cases are maximally misleading. You mitigate this with multiple annotators per example, especially for examples the model is very uncertain about. If the model has fifty percent confidence on an example, get three annotators and use majority vote. If all three disagree, escalate or exclude the example. High-quality labels on uncertain examples are more valuable than low-quality labels.

Finally, you need stakeholder alignment. Active learning changes the labeling workflow. Annotators see harder examples on average, which can be frustrating or fatiguing. Throughput per annotator might decrease even though overall efficiency increases. Managers who measure annotator productivity by labels per hour might resist active learning. You need to explain the cost savings and performance gains in terms they care about: fewer total labels needed, faster time to production-ready model, lower total annotation budget. Show the learning curve comparison. Show the cost analysis. Get buy-in before you start, not after annotators complain.

## Combining Active Learning with Other Sampling Strategies

Active learning is not mutually exclusive with other sampling strategies. You can combine active learning with stratified sampling, importance sampling, or domain-specific sampling to achieve multiple goals simultaneously. A fraud detection system might use active learning to oversample uncertain cases and stratified sampling to ensure coverage across merchant categories, transaction amounts, and geographic regions. The system scores all unlabeled examples by uncertainty, then applies stratification within the top ten thousand most uncertain examples, ensuring at least fifty examples from each stratum.

Another combination is active learning with recency weighting. If your data distribution shifts over time, recent examples are more valuable than old examples because they better represent the current distribution. A content moderation system might combine uncertainty sampling with a recency bonus: multiply the uncertainty score by a recency weight, such that an example from yesterday has a higher effective score than an equally uncertain example from six months ago. This ensures the model adapts to new patterns and does not overfit to outdated data.

Some teams alternate between active learning and random sampling across retraining cycles. They use active learning for three retraining cycles to target the model's weaknesses, then switch to random sampling for one cycle to ensure broad coverage, then back to active learning. This prevents the model from becoming too specialized on edge cases and ensures it maintains strong performance on common cases.

You can also combine active learning with semi-supervised learning or transfer learning. Semi-supervised learning uses a large pool of unlabeled data to regularize the model, improving generalization. Active learning selects the most informative examples from that pool for labeling. Transfer learning starts with a model pre-trained on a related task, reducing the cold-start labeling requirement. Active learning then fine-tunes the model on task-specific examples. A medical imaging system might start with a model pre-trained on general radiology images, use active learning to select one thousand lung scans for labeling, and train a lung cancer detection model with ninety-two percent accuracy on those one thousand examples plus ten thousand unlabeled scans via semi-supervised learning.

The key is to be intentional about what you are optimizing for. Active learning optimizes for information gain per label. Stratified sampling optimizes for coverage. Recency weighting optimizes for adaptation to distribution shift. You can combine them, but you need clear priorities. If the model underperforms on rare slices and the distribution is stable, prioritize active learning. If the model performs well on average but poorly on specific domains, prioritize stratified sampling. If the distribution shifts rapidly, prioritize recency. Most teams find that active learning delivers the largest gains in the early stages of a labeling campaign, and they layer on other strategies as the model matures.

Active learning is not magic. It is a resource allocation optimization. It concentrates labeling effort where it matters most. For teams with tight budgets, imbalanced distributions, or hard tasks, it is one of the highest-leverage techniques in the labeling toolkit. The next subchapter covers human override patterns: when annotators review AI-generated labels, how do you train them to reject confidently when the AI is wrong, and how do you turn override rates into a quality signal that feeds back into the labeling loop.
