# 1.1 — Why Labeling Is the Foundation of Every Eval System

In March 2025, a healthcare technology company spent eleven months building what they called a comprehensive evaluation pipeline for their clinical documentation assistant. The system generated detailed metrics dashboards, tracked precision and recall across twelve task categories, and produced weekly performance reports that leadership reviewed religiously. The engineering team had invested heavily in the infrastructure: automated eval runs on every commit, A/B testing frameworks, statistical significance calculators, and a custom metrics aggregation system that could slice performance data dozens of ways. When they finally deployed the assistant to a pilot group of 200 physicians in July 2025, the system failed catastrophically within two weeks. Physicians reported that the assistant was generating dangerous medication suggestions, missing critical patient history, and hallucinating test results that had never been ordered. The evaluation system had shown steady improvement throughout development, with quality scores rising from 72% in January to 91% by June. How had months of rigorous evaluation missed problems so severe that clinical staff refused to use the system?

The root cause was discovered when a newly hired senior engineer asked to see the labeled evaluation dataset. The team had 50,000 labeled examples, an impressive volume by any standard. But when the engineer examined how those labels had been created, the problem became clear immediately. The labels had been generated by two junior engineers over a three-week sprint in December 2024, working from a two-page labeling guide that defined "correct" as "sounds medically plausible and matches the general topic." Neither engineer had medical training. Neither had access to actual patient records or clinical workflows. They had labeled synthetic examples created by the same model they were now evaluating, judging whether GPT-4 outputs looked reasonable when responding to GPT-4 generated patient scenarios. The evaluation system had been measuring whether the model could match the aesthetic and style of its own training data, not whether it could support real clinical decision-making. Every metric was precise, every dashboard was detailed, every trend line was carefully tracked. But all of it was built on labels that encoded no actual medical expertise, no understanding of clinical risk, no connection to how physicians work. The company ended up scrapping the entire eval system and starting over with a proper clinical labeling operation staffed by nurses and physicians, a process that took another six months and cost an additional $1.8 million.

## The Labeling Dependency That No One Wants to Acknowledge

Every number your evaluation system produces traces back to human judgment. Precision, recall, accuracy, F1 scores, Rouge scores, semantic similarity metrics, LLM-as-judge verdicts, user satisfaction ratings, task success rates—every single one depends on someone, at some point, making a judgment call about what counts as correct. You cannot evaluate quality without defining quality, and you cannot define quality without human judgment. That judgment gets encoded as labels, and those labels become the foundation everything else is built on. Your metrics are only as reliable as your labels. Your dashboards are only as meaningful as your labels. Your A/B tests are only as valid as your labels. Your claims about improvement are only as credible as your labels. Treat labeling as an afterthought, and every downstream artifact inherits that neglect.

This dependency makes most teams deeply uncomfortable because labeling is expensive, slow, and requires domain expertise that engineering teams often do not have. It is far easier to focus on evaluation infrastructure—the pipelines, the metrics, the dashboards, the statistical tests. These are engineering problems with engineering solutions, and they feel productive in a way that labeling never does. You can ship an eval pipeline in a sprint. You can automate metrics collection. You can build impressive dashboards that update in real-time. But you cannot automate away the fundamental problem: someone has to decide what good looks like, and that decision has to be correct. When teams skip or rush labeling, they are not saving time. They are building elaborate infrastructure on top of a foundation that does not exist.

The rise of LLM-as-judge in 2024 and 2025 made this problem worse by creating the illusion that labeling could be bypassed entirely. If GPT-5 or Claude Opus 4.5 can evaluate outputs automatically, why invest in expensive human labeling operations? The reality is that LLM-as-judge systems are themselves evaluation models that require calibration, validation, and ongoing monitoring. You calibrate them against human labels. You validate them by measuring agreement with human labels. You monitor them by periodically checking whether they still align with human labels. Every LLM-as-judge system is a proxy for human judgment, and like all proxies, it degrades over time as models change, tasks evolve, and contexts shift. Teams that adopted LLM-as-judge without maintaining a parallel human labeling operation discovered this the hard way in late 2025 when GPT-5.1 launched with different judgment patterns than GPT-5, invalidating months of accumulated evaluation data. The teams that weathered the transition smoothly were the ones who had never stopped doing human labeling, who treated LLM-as-judge as an acceleration layer on top of human labels rather than a replacement for them.

## Ground Truth Is Not a Dataset You Download

The term ground truth appears constantly in evaluation discussions, often used as though it is something that exists independently in the world waiting to be discovered. In reality, ground truth is something you construct through a labeling process, and the quality of that construction determines the quality of everything you build on top of it. Ground truth for a customer support classification task is not the objectively correct category that some customer inquiry belongs to. It is the consensus judgment of your labelers about which category best fits your business needs, your support workflows, and your customer expectations. Ground truth for a content moderation task is not the universal standard of what content should be allowed. It is the operationalization of your platform policies as understood and applied by your moderation team. Ground truth for a medical coding task is not the single correct diagnosis code. It is the code that aligns with your organization's documentation standards, your payer contracts, and your compliance requirements.

This constructed nature of ground truth means that labeling is not a one-time data collection exercise. It is an ongoing process of refining your understanding of what quality means in your specific context. Early in a project, your labels encode your initial assumptions about the task. As you deploy and learn from real usage, those assumptions get tested and often proven wrong. Users care about aspects of quality you had not considered. Edge cases reveal ambiguities in your definitions. Your business requirements change. Your regulatory environment evolves. If your labeling operation is a fixed dataset created at project inception, your ground truth becomes stale the moment real-world use begins. If your labeling operation is a living process that continuously incorporates new examples, new feedback, and new understanding, your ground truth evolves with your system.

The healthcare documentation company had treated ground truth as a static dataset—50,000 examples labeled once, then frozen. When physicians reported that the system was missing critical safety issues, there was no mechanism to incorporate that feedback into the evaluation system. The labels existed in one world, defined by two engineers working from synthetic examples, and the real clinical usage existed in another world, defined by physicians working with actual patients. The gap between these worlds was unbridgeable because the labeling operation had ended before deployment began. Teams that maintain continuous labeling operations can close this loop. Physician feedback becomes new labeling tasks. Reported errors become additions to the evaluation dataset. Evolving clinical guidelines become labeling guide updates. Ground truth stays grounded in truth rather than drifting into historical artifact.

## The Agreement Measurement You Cannot Do Without Labels

One of the most critical quality signals in any AI system is agreement: does your system's judgment align with human judgment? But measuring agreement requires having human judgments to compare against, which means you need labels. Not labels from six months ago, not labels on synthetic examples, not labels from people who do not understand the domain. You need recent labels, on representative examples, from qualified labelers. Without this, agreement measurement becomes impossible, and you lose your primary signal for detecting model degradation, prompt regression, or distribution shift.

Consider what happens when you deploy a model update. You want to know: is the new model better or worse than the old model? Better and worse are not intrinsic properties of model outputs. They are judgments made by comparing outputs to some standard. That standard is your labels. You run both models on your evaluation dataset, compare their outputs to your labeled ground truth, and measure which model agrees more often with human judgment. If your labels are outdated, you are measuring agreement with how your team thought about the problem months ago, which may not reflect how you think about it now. If your labels are from non-experts, you are measuring agreement with people who do not understand what good looks like. If your labels are on non-representative examples, you are measuring agreement on a distribution that does not match production. All three scenarios produce agreement metrics that are precise, calculable, and completely misleading.

The same problem appears in A/B testing. You deploy variant A to half your users and variant B to the other half, collect outputs from both, and need to determine which variant produces better results. If you rely on implicit signals like user engagement or task completion, you are measuring correlation, not quality. Users might engage more with confidently wrong answers than with hesitant correct ones. They might complete tasks more often when the system gives them what they asked for rather than what they need. To measure actual quality, you need to label a sample of outputs from each variant and compare label agreement. This requires having a labeling operation that can turn around results quickly enough to make decisions while the A/B test is still running, which requires labeling infrastructure, not labeling as an afterthought.

## The Calibration Requirement That Scales With Model Churn

In 2024, model updates were quarterly events. In 2025, they became monthly or even weekly as providers like OpenAI, Anthropic, and Google accelerated release cycles. By early 2026, teams are dealing with continuous model evolution: minor updates to existing models, new model variants, new prompting techniques, new modalities. Every change creates a calibration requirement. You need to verify that your evaluation system still works, that your metrics still mean what you think they mean, that your thresholds are still valid. This verification requires labels.

When GPT-5.1 launched in late 2025, many teams discovered that their evaluation systems produced dramatically different scores on the new model compared to GPT-5, despite the outputs looking subjectively similar. The issue was not that GPT-5.1 was worse. The issue was that GPT-5.1 had different stylistic patterns, different verbosity, different phrasing conventions. Evaluation metrics that had worked well for GPT-5 were sensitive to these stylistic differences in ways that did not reflect actual quality differences. Teams that had robust labeling operations could recalibrate: they labeled a sample of GPT-5.1 outputs, compared the labels to the metrics, and adjusted their evaluation approach to focus on aspects of quality that mattered rather than stylistic markers that did not. Teams without labeling operations had to either stick with GPT-5, adopt GPT-5.1 blindly, or spend weeks scrambling to build labeling infrastructure they should have had from the start.

This calibration requirement extends to LLM-as-judge systems. When you use Claude Opus 4.5 as an evaluator, you are relying on its judgment patterns, its interpretation of your rubric, its sensitivity to different quality dimensions. When Claude 4 launches, those judgment patterns change. Your LLM-as-judge scores shift, not because your system got better or worse, but because the judge got replaced. To detect this shift and correct for it, you need a stable reference point: human labels. You measure how well Claude Opus 4.5 agreed with human labels, then measure how well Claude 4 agrees with human labels, and use that comparison to understand whether score changes reflect real quality changes or just judge changes. Without human labels, you cannot disentangle these effects.

## The Cost Curve That Punishes Late Investment

There is a specific cost curve to labeling investment, and it punishes teams that wait. Early investment in labeling infrastructure—hiring qualified labelers, building labeling tools, developing labeling guides, establishing quality processes—has high upfront cost but low ongoing cost. You pay a lot in months one through three to build the operation, then pay relatively little to maintain and scale it because the infrastructure is in place. Late investment inverts this curve. You pay little upfront by skipping labeling, then pay exponentially more later when you discover your evaluation system is unreliable and need to retrofit labeling into a system that was designed without it.

The healthcare documentation company paid $400,000 in the first phase to build their evaluation infrastructure without proper labeling. They paid $1.8 million in the second phase to rebuild it with clinical labeling, plus the opportunity cost of eleven months of development time that produced a system they could not deploy. If they had invested in clinical labeling from the start, the initial cost would have been higher—perhaps $800,000 instead of $400,000—but the total cost would have been a fraction of what they ultimately paid. The multiplier comes from several sources. First, retrofitting labeling into an existing system requires relabeling everything you have already evaluated, duplicating work. Second, you often discover that your task decomposition was wrong because you did not understand the domain well enough, requiring rework of your entire pipeline. Third, you lose time to market, which in competitive spaces can be more expensive than any direct cost.

The cost curve is steeper for regulated domains. In healthcare, finance, and legal applications, label quality is not just an engineering concern but a compliance requirement. Your labels need to be defensible, auditable, traceable to qualified experts. Building this kind of labeling operation after you have already deployed is not just expensive but potentially impossible. Regulators want to see that you established quality standards before deployment, not that you retrofitted them after discovering problems. The EU AI Act, which came into force in 2024 and began enforcement in 2025, explicitly requires documentation of training and evaluation data for high-risk AI systems. If your labels were created by unqualified annotators working from vague guidelines, you do not have compliant documentation, regardless of how sophisticated your evaluation metrics are.

## The Specialization Problem That Engineering Teams Underestimate

Engineering teams chronically underestimate how specialized labeling expertise needs to be. The assumption is that labeling is pattern matching: given a clear rubric, anyone can label accurately. This assumption holds for trivial tasks like sentiment classification on product reviews. It fails completely for anything requiring domain knowledge, contextual judgment, or understanding of downstream consequences. Medical labeling requires medical expertise. Legal labeling requires legal expertise. Financial labeling requires financial expertise. Content moderation labeling requires understanding of platform policies, cultural context, and harm patterns. Customer support labeling requires understanding of your product, your customer base, and your support workflows.

When teams use non-specialist labelers—engineers, contractors, crowdworkers—the resulting labels encode non-specialist understanding of the task. An engineer labeling medical documentation does not know which abbreviations are standard and which are dangerous. A contractor labeling legal briefs does not know which precedents are controlling and which are distinguishable. A crowdworker labeling financial advice does not know which recommendations are compliant and which expose the company to regulatory risk. They label based on surface plausibility, on how things sound, on whether outputs match their expectations. These labels are worse than useless because they create false confidence. Your evaluation system produces numbers, your dashboards show trends, your metrics indicate improvement. None of it reflects actual quality in the domain that matters.

The solution is not to make your rubric more detailed. A 50-page labeling guide does not turn an engineer into a physician or a crowdworker into a lawyer. The solution is to hire specialist labelers or partner with specialist labeling providers who have access to domain experts. This is expensive, which is why teams avoid it, but the alternative is more expensive. The healthcare company paid two engineers for three weeks to create 50,000 bad labels, perhaps $15,000 in direct labor cost. They then paid nurses and physicians for six months to create proper labels, perhaps $300,000. The $285,000 difference looks like waste until you consider that the $15,000 investment produced a system that failed in production, endangered patients, and cost $1.8 million to fix, while the $300,000 investment produced a system that actually worked.

## The Infrastructure Investment That Pays Continuous Dividends

Labeling is not a dataset. It is infrastructure. Like all infrastructure, it requires upfront investment and ongoing maintenance, but it pays dividends for as long as you operate. A well-built labeling operation produces labeled data for initial model training, for evaluation dataset creation, for monitoring production outputs, for investigating user complaints, for validating model updates, for calibrating LLM-as-judge systems, for training new team members, for demonstrating compliance to auditors. It becomes the institutional memory of what quality means in your context, a resource that every part of your AI system draws on.

This infrastructure perspective changes how you think about labeling costs. The question is not "can we afford to spend $300,000 on labeling" but "can we afford to build an AI system without reliable quality measurement." Every team answers yes to the second question because they plan to measure quality some other way—with automated metrics, with LLM-as-judge, with user feedback. Then they discover that automated metrics miss what matters, LLM-as-judge requires calibration they do not have, and user feedback comes too late to prevent damage. They end up building labeling infrastructure anyway, but later, at higher cost, under pressure.

The teams that invest early in labeling infrastructure develop a different relationship with evaluation. Evaluation is not a checkpoint you pass before deployment. It is a continuous process of measuring, learning, and refining. Your labeling operation produces a steady stream of new ground truth examples. Your evaluation system incorporates these examples and updates your understanding of model performance. Your monitoring system flags outputs that look different from your labeled distribution. Your investigation process turns flagged outputs into new labeling tasks. The loop closes, and quality improvement becomes systematic rather than reactive. This only works if labeling is infrastructure, not a dataset you created once and forgot about.

## Why Label Quality Determines Eval Quality, Always

There is no amount of sophisticated evaluation infrastructure that can compensate for poor labels. You can build real-time dashboards, implement advanced statistical tests, deploy multi-stage evaluation pipelines, use ensemble methods that combine multiple metrics. All of it is meaningless if your labels are wrong. Garbage labels produce garbage evals, and garbage evals produce false confidence, which is worse than no confidence because it leads you to make decisions based on information you trust but should not.

This is not a theoretical concern. It is the most common failure mode in AI evaluation. Teams build impressive evaluation systems, track dozens of metrics, run rigorous experiments, make data-driven decisions about which models to use, which prompts to deploy, which features to ship. Then they deploy, and real users tell them the system does not work. The evaluation said it would work. The metrics said it was improving. The A/B test said variant B was better. But all of those signals were downstream from labels that did not reflect actual quality, so all of those decisions were based on measurements of the wrong thing.

The healthcare company measured precision at 91% because 91% of outputs matched their labels. But their labels were created by people who did not understand medicine, so matching those labels meant the system was good at sounding medical, not good at supporting clinical care. They had optimized for the wrong thing with great precision, a failure that no amount of metric sophistication could have prevented. The only prevention was better labels from the start, labels created by people who understood what clinical quality actually meant.

Understanding that labeling is the foundation of every eval system is the first step toward building evaluation infrastructure that produces reliable quality signals. But labels themselves come in different forms with different purposes, and conflating these forms leads to systems that can measure but not improve, or improve but not measure. The distinction between labeling and annotation drives architectural decisions that determine what your evaluation system can and cannot do, a distinction we examine next.
