# 1.3 â€” Label Types: Binary, Multi-Class, Scalar, and Comparative

What happens when your annotation team disagrees on forty-seven percent of labels? You discover that the label structure itself is the problem. Inter-annotator agreement below fifty percent is not a training failure or an annotator quality issue. It is a structural mismatch between what you asked humans to judge and what humans can judge reliably. Five-point scales feel precise and scientific, suggesting meaningful gradations between adjacent ratings. But when agreement collapses below chance levels, the scale is measuring annotator calibration differences rather than content characteristics. The pattern repeats across domains: teams choose label types that sound sophisticated without testing whether those types match human judgment capabilities on their specific data.

The choice of label type is one of the earliest and most consequential decisions you make when designing a labeling operation, yet most teams treat it as an afterthought. They default to five-point scales because they feel scientific, or they use binary labels because they seem simple, without analyzing whether the label structure actually matches the decision the system needs to make. Different evaluation tasks require fundamentally different label types, and choosing incorrectly wastes annotator time, produces unusable data, and leads to months of rework. The label type you choose determines what kind of signal you can extract, how reliably annotators can apply it, and what statistical methods you can use for analysis. There is no universal best label type. There is only the right label type for your specific task, your specific annotators, and your specific downstream use case.

## Binary Labels: When Two Categories Are Enough

Binary labels are the simplest label type: yes or no, pass or fail, safe or unsafe, acceptable or unacceptable. You present annotators with an example and ask them to place it into one of exactly two categories. Binary labels are appealing because they require the least cognitive load from annotators, produce the clearest inter-annotator agreement metrics, and map directly to many production decisions. If your content moderation system either shows content or removes it, binary safety labels match your production reality. If your customer service evaluation either approves a response for use in training data or rejects it, binary quality labels give you exactly the signal you need. Binary labels work best when the underlying decision is genuinely binary, when the boundary between categories is reasonably clear, and when you care more about the category than the degree.

Binary labels produce higher inter-annotator agreement than any other label type because annotators have only one decision boundary to calibrate on. In a well-designed binary labeling task for content safety, you can typically achieve agreement rates of 85-92% after proper annotator training, compared to 60-75% for five-point scalar ratings on the same content. This higher agreement translates directly into more reliable evaluation metrics and more stable training signals. When you measure the precision of your LLM system at detecting unsafe content, binary labels give you a clean confusion matrix: true positives, false positives, true negatives, false negatives. You can calculate precision, recall, F1, and set decision thresholds with confidence in the ground truth labels. Multi-class or scalar labels require additional decisions about what counts as a match, what margin of error is acceptable, and how to handle near-misses.

The critical mistake teams make with binary labels is using them when the underlying phenomenon is not binary. If content safety exists on a spectrum from completely benign to extremely harmful, forcing annotators to draw a single line creates arbitrary boundaries and hides important information. An LLM response that contains minor factual errors is not the same as an LLM response that hallucinates dangerous medical advice, but a binary accurate-or-inaccurate label treats them identically. A customer service response that is technically correct but cold in tone is not the same as a response that is warm but contains wrong information, but a binary acceptable-or-unacceptable label cannot distinguish them. When you use binary labels for non-binary phenomena, you end up with one of two failure modes: either annotators cannot agree on where to draw the line, producing low inter-annotator agreement, or they agree on an arbitrary line that discards useful signal about severity, importance, or type of failure.

The solution is not to abandon binary labels but to use them correctly. Binary labels work when you have a genuine decision boundary that matters to your production system. Content that violates your terms of service gets removed; content that does not gets shown. That is binary. Retrieval chunks that contain information relevant to the user query get passed to the LLM; chunks that do not get filtered out. That is binary. Customer service responses that meet your minimum quality bar get used for fine-tuning; responses that do not get discarded. That is binary. In each case, the label structure matches the downstream decision structure. When you find yourself wanting to add a third category like "borderline" or "uncertain" to a binary label, that is a signal that your task is not actually binary. You should either redesign the task to make the boundary clearer or switch to a label type that captures the gradations you care about.

## Multi-Class Labels: Designing Mutually Exclusive Categories

Multi-class labels ask annotators to place each example into exactly one category from a set of three or more options. Unlike binary labels where the choice is between two poles, multi-class labels categorize examples by type, topic, failure mode, or intent. A content safety task might use categories like hate speech, violence, sexual content, self-harm, and safe. A customer support intent classifier might use categories like billing question, technical support, account management, product inquiry, and complaint. A retrieval relevance task might use categories like directly relevant, tangentially relevant, off-topic, and contradictory. Multi-class labels are appropriate when you need to distinguish between qualitatively different types of examples, not just different degrees of the same property.

The fundamental challenge with multi-class labels is ensuring that categories are mutually exclusive and collectively exhaustive. Mutually exclusive means that each example should clearly belong to only one category, with no ambiguous cases that could legitimately fit into multiple categories. Collectively exhaustive means that every example you might encounter should fit into at least one category, with no orphan cases that fall outside your taxonomy. Most teams fail at one or both of these requirements. They design categories that overlap, forcing annotators to make arbitrary choices when examples exhibit characteristics of multiple categories. Or they design categories that cover only common cases, forcing annotators to shoehorn unusual examples into the closest approximate category even when it is not a good fit.

Consider a content moderation taxonomy with categories for hate speech, violence, and harassment. Where does a post that uses violent language to harass someone based on their identity belong? It exhibits characteristics of all three categories. If you tell annotators to pick the primary category, different annotators will make different choices based on their interpretation of which aspect is most salient. If you allow multiple labels per example, you have switched from multi-class to multi-label classification, which is a different task with different statistical properties. The correct solution is to redesign your taxonomy to eliminate the overlap. You might have a category specifically for identity-based violent threats, or you might organize categories hierarchically with hate speech as the top level and violent hate speech as a subcategory. The category structure should match the natural clusters in your data, not the organizational chart of your content policy team.

The other common failure mode is the "other" or "unknown" category. Teams include this as a catch-all for examples that do not fit their main categories, which seems reasonable until you analyze the data and discover that 30% of your labels are "other." This tells you that your taxonomy does not actually cover the space of examples you are labeling. The "other" category becomes a dumping ground for everything your initial category design failed to anticipate, and it provides no useful signal because it is not a coherent category. An "other" category that contains 3-5% of examples is acceptable; it captures genuine outliers and edge cases. An "other" category that contains more than 10% of examples is a sign that you need to analyze what is falling into it and either add new categories or rethink your taxonomy entirely.

Well-designed multi-class taxonomies emerge from analysis of real data, not from whiteboard brainstorming. You start by labeling several hundred examples with freeform text descriptions of what makes each one notable or problematic. You cluster these descriptions to find natural groupings. You design categories that match these clusters, ensuring that the boundaries between categories correspond to meaningful differences that annotators can reliably distinguish. You test the taxonomy on a fresh set of examples and measure inter-annotator agreement on each category. Categories with low agreement indicate boundary problems that need refinement. You iterate on category definitions, merge overlapping categories, split overly broad categories, and add categories for common "other" cases. Only after this process produces stable inter-annotator agreement above 75-80% do you scale up labeling. Most teams skip this iteration and pay for it in unusable data.

## Scalar Labels: The Calibration Problem

Scalar labels ask annotators to rate examples on a numeric scale, typically 1-5, 1-7, or 0-100. A helpfulness rating might range from 1 for completely unhelpful to 5 for extremely helpful. A factual accuracy score might range from 0 for entirely false to 100 for perfectly accurate. Scalar labels are appealing because they capture gradations that binary or multi-class labels cannot. A response can be somewhat helpful, moderately helpful, or very helpful. A retrieval chunk can be highly relevant, moderately relevant, or barely relevant. Scalar labels seem to provide richer signal than binary labels and more precision than categorical labels. This is true in theory. In practice, scalar labels are the least reliable label type for most tasks because they require annotators to make absolute judgments about subjective properties, and humans are terrible at absolute judgments.

The core problem with scalar labels is calibration. When you ask an annotator to rate response helpfulness on a 1-5 scale, what does a 3 mean? Is 3 the midpoint, meaning average helpfulness? Is 3 slightly below average? Is 3 acceptable but not great? Different annotators develop different internal scales, and even the same annotator's scale drifts over time. The content moderation company from the opening story experienced this: some labelers used 1-2 for almost everything because they calibrated their scale to the worst content they had seen, while other labelers used the full 1-5 range because they calibrated to the distribution of examples they were shown. Neither approach is wrong, but they are incompatible. When you aggregate these ratings, you get a meaningless average of differently calibrated scales.

Research on human judgment consistently shows that people are much better at relative comparisons than absolute ratings. An annotator can reliably tell you whether response A is more helpful than response B, but they cannot reliably tell you whether response A deserves a 3 or a 4 on a helpfulness scale. The difference between a 3 and a 4 is not an objective property of the response; it is a subjective judgment that depends on the annotator's reference frame, their recent experience, and their interpretation of the scale labels. This is why inter-annotator agreement on scalar labels is typically 30-50 percentage points lower than on binary labels for the same underlying task. If you compute the Pearson correlation between two annotators' 1-5 helpfulness ratings, you might get 0.4-0.6, which sounds reasonable until you realize it means annotators disagree on the rating 40-60% of the time.

Annotator drift makes scalar ratings even less reliable over time. An annotator who starts with a well-calibrated internal scale gradually shifts their standards as they see more examples. This is called **anchor effect bias**: recent examples anchor the annotator's perception of what counts as a 1 versus a 5, causing their ratings to drift relative to their initial calibration. If an annotator labels 500 examples in a row that are mostly mediocre, their perception of what counts as excellent shifts upward, and they start giving 4s and 5s to responses they would have rated 3 earlier in the session. If an annotator then encounters a batch of particularly good examples, their perception shifts again, and they retroactively wish they had saved their 5s for these better cases. You cannot fix this with instructions that say "use the whole scale" or "5 means exceptionally helpful." The drift is unconscious and automatic.

The temptation is to solve calibration problems with more detailed rubrics. You write extensive descriptions of what each rating level means: "A rating of 1 indicates a response that is completely unhelpful, provides no relevant information, and may actively mislead the user. A rating of 2 indicates a response that attempts to address the question but provides mostly irrelevant or incorrect information. A rating of 3 indicates a response that provides some relevant information but is incomplete or partially incorrect." This helps slightly, but it does not solve the fundamental problem. Annotators still have to map their subjective impression of a response onto these verbal descriptions, and different annotators will make different mappings. Longer rubrics can even make the problem worse by introducing more opportunities for interpretation disagreement.

The correct use of scalar labels is narrow and specific. Scalar labels work when you have objective anchor points that ground the scale. If you are rating transcription accuracy, a score of 0 means zero words correct, and a score of 100 means perfect transcription. Annotators can count errors and compute an objective percentage. If you are rating response latency perception, you can show annotators reference examples at specific latency points: "1 second feels instantaneous, 3 seconds feels responsive, 5 seconds feels slow, 10 seconds feels broken." Annotators can compare their example to these references. Scalar labels also work when you plan to use only the endpoints or a simple threshold, in which case you should just use binary or three-point labels instead. If you will ultimately threshold a 1-5 helpfulness scale at 4 to separate good responses from bad, you are better off asking annotators to make that binary good-or-bad judgment directly.

## Comparative Labels: Why Pairwise Comparison Works

Comparative labels ask annotators to judge the relative quality of two or more examples rather than rating each example independently. Instead of asking "Is this response helpful, on a scale from 1 to 5?" you ask "Which response is more helpful, A or B?" Instead of asking "How factually accurate is this answer, from 0 to 100?" you ask "Which answer is more factually accurate, X or Y?" Comparative labels leverage the well-established finding that humans are much more reliable at relative judgments than absolute ratings. It is hard to say whether a customer service response deserves a 3 or a 4 for professionalism, but it is easy to say whether response A is more professional than response B. Comparative labeling produces more consistent annotator judgments, higher inter-annotator agreement, and more useful training signal for many tasks.

The simplest form of comparative labeling is pairwise comparison. You show annotators two examples side by side and ask them to pick which one is better according to some criterion. Which response is more helpful? Which retrieval chunk is more relevant? Which summary is more faithful to the source? Annotators make a forced choice, sometimes with an option for "tie" or "cannot determine." Pairwise comparisons are cognitively easier than absolute ratings because annotators only need to perceive a difference, not quantify a magnitude. They do not need to calibrate an internal scale or worry about what a 3 versus 4 means. They just need to compare the two examples in front of them and pick the better one. This simplicity produces higher inter-annotator agreement. In domains where 1-5 scalar ratings achieve 40-50% exact agreement, pairwise comparisons often achieve 75-85% agreement.

The challenge with pairwise comparison is coverage. If you have 1,000 LLM responses to evaluate, comparing all possible pairs requires nearly 500,000 comparisons, which is not practical. The solution is to use sparse pairwise comparisons combined with a ranking algorithm that infers a global quality ordering from a subset of comparisons. The most common approach is Elo rating, borrowed from chess rankings, or the statistically equivalent Bradley-Terry model. You start by assigning each example an initial rating, then you sample pairs of examples and show them to annotators. When annotators judge that example A is better than example B, you update both ratings: A's rating increases and B's rating decreases, with the magnitude of the update depending on the difference in their current ratings. If a low-rated example beats a high-rated example, that is surprising and produces a large rating update. If a high-rated example beats a low-rated example, that is expected and produces a small update. After several thousand comparisons, the Elo ratings converge to a stable global ranking that reflects the collective judgment of your annotators.

Elo-based evaluation has become standard practice for LLM system evaluation in 2025-2026, driven by its use in LMSYS Chatbot Arena and similar platforms. The advantage is that you can evaluate thousands of examples with a manageable number of comparisons. If you collect five to ten comparisons per example, Elo ratings stabilize and produce a reliable ranking. This is far more efficient than collecting five to ten independent scalar ratings per example, and it produces more reliable signal because each comparison directly contrasts two examples rather than asking for an absolute judgment. The disadvantage is that Elo ratings are relative within your dataset. An Elo rating of 1200 means nothing without knowing the distribution of ratings in your pool. You cannot compare Elo ratings across different labeling projects or different time periods unless you include anchor examples that appear in both pools.

Comparative labeling is particularly valuable when you are choosing between alternative system designs or training different model variants. If you are comparing GPT-5 versus Claude 4 Sonnet for your customer service use case, you do not need absolute quality ratings. You need to know which model produces better responses for your specific queries. You sample 500 real customer queries, generate responses from both models, and show pairs of responses to annotators: "Which response is better for this query?" Annotators make 2,500 comparisons, and you aggregate the results to compute a win rate: Claude wins 58% of comparisons, GPT-5 wins 34%, and 8% are ties. This gives you a clear decision signal without needing to define what a 3 versus 4 quality rating means. The same approach works for comparing different prompts, different retrieval strategies, different reasoning chains, or any other system variant.

The failure mode with comparative labeling is trying to use it when you need absolute thresholds. If you are building a content moderation system, you need to know which content is unsafe, not just which content is more unsafe than other content. Comparative labels can tell you that example A is worse than example B, but they cannot tell you whether A crosses your moderation threshold. For threshold-based decisions, you need binary or multi-class labels that directly encode the threshold judgment. Comparative labels are a poor fit. Similarly, if you need to track absolute quality over time, comparative labels are problematic because they are relative to the pool of examples in each evaluation period. If your LLM responses improve from January to June, but you evaluate each month with different pools of examples, Elo ratings may not reflect the improvement. You need anchor examples that appear in every pool, or you need to switch to calibrated scalar ratings or binary quality thresholds.

## Choosing the Right Label Type for Your Task

The default mistake is choosing label type based on what feels sophisticated or what you have seen other teams do, rather than based on what matches your downstream use case. Five-point scales feel scientific because they appear in academic surveys, so teams use them for LLM evaluation without considering whether their task requires that granularity. Binary labels feel simplistic, so teams add unnecessary categories to appear more rigorous. Comparative labels feel novel and modern, so teams use them even when they need absolute thresholds. The correct approach is to work backwards from your production decision. What choice does your system make based on these labels? Does it show content or remove it? Does it route queries to model A or model B? Does it include a response in training data or exclude it? The label type should directly support that decision.

If your production decision is binary, use binary labels. Content moderation systems that allow or block content should use binary safe-or-unsafe labels, possibly with separate binary labels for different policy categories. Retrieval systems that include or exclude chunks based on relevance should use binary relevant-or-not labels. Quality filters that accept or reject examples for training data should use binary acceptable-or-unacceptable labels. Binary labels give you the highest inter-annotator agreement, the clearest evaluation metrics, and the most direct connection to your production threshold. The only reason to use a more complex label type is if your production decision is genuinely not binary.

If your task requires distinguishing between qualitatively different types of examples, use multi-class labels with a carefully designed taxonomy. Intent classification tasks need multi-class labels because routing a billing question to technical support is a different failure than routing it to sales. Failure mode analysis needs multi-class labels because hallucination requires different mitigation than toxicity. Content policy violation detection needs multi-class labels because hate speech policy and self-harm policy have different enforcement workflows. Invest the time to design mutually exclusive, collectively exhaustive categories, test them on real data, and iterate based on inter-annotator agreement. A well-designed six-category taxonomy with 80% agreement is far more valuable than a poorly designed ten-category taxonomy with 50% agreement.

If you are comparing alternative system designs and need to know which is better, use comparative labels with pairwise comparison and Elo ranking. Model selection, prompt optimization, retrieval strategy comparison, and A/B testing of system variants all benefit from comparative evaluation. Annotators can reliably tell you which of two responses is better even when they cannot reliably rate each response on an absolute scale. The comparative signal is cleaner, the cognitive load is lower, and the decision you need to make is directly "which one should we use?" rather than "does this meet our quality bar?" Build a pool of representative examples, collect pairwise judgments, compute Elo ratings or win rates, and make your selection based on which variant wins more comparisons.

Use scalar labels only when you have objective anchor points or when you will use the ratings for regression rather than classification. Transcription accuracy percentages, response latency measurements, and error counts are legitimate scalar labels because they have objective grounding. Sentiment intensity might be a legitimate scalar label if you have clear reference examples at each scale point and you train annotators to calibrate against those references. For most subjective properties like helpfulness, coherence, professionalism, or factual accuracy, scalar labels introduce more noise than signal unless you invest heavily in annotator training and calibration procedures. Even then, comparative labels will likely give you better data with less effort.

The final consideration is downstream model use. If you are collecting labels to train a model, the label type should match the model architecture. Binary classifiers need binary labels. Multi-class classifiers need multi-class labels. Ranking models benefit from comparative labels. Regression models need scalar labels, but only if those labels are well-calibrated and reliable. If you are collecting labels for evaluation rather than training, the label type should match your evaluation metric. If you report precision and recall, you need binary or multi-class labels that define what counts as correct. If you report ranking metrics like mean reciprocal rank or NDCG, you need comparative labels or well-calibrated scalar relevance judgments. Do not choose label type in isolation from how you will use the resulting data.

Choosing the right label type is not a one-time decision. As your system evolves, your labeling needs may change. You might start with binary labels to establish a minimum quality threshold, then switch to multi-class labels to diagnose failure modes, then switch to comparative labels to choose between mitigation strategies. You might use binary labels for high-volume production monitoring and comparative labels for lower-volume offline evaluation of new models. The teams that succeed at labeling operations treat label type as a tool to be selected based on the task, not as a fixed property of their evaluation process. They measure inter-annotator agreement for each label type they try, analyze disagreement patterns to understand what is hard for annotators, and iterate toward label structures that produce reliable signal with acceptable annotator effort.

In the next subchapter, we examine how these label types must be adapted for different AI modalities: chat systems, RAG applications, tool use, agents, and voice interfaces, where each modality introduces unique labeling challenges that generic approaches fail to handle.
