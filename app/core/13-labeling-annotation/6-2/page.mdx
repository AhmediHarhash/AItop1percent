# 6.2 â€” Measuring Agreement: Kappa, Alpha, and Beyond

In late 2024, a financial services company built a compliance monitoring system that classified internal communications as potentially problematic or benign for regulatory review. They hired eight compliance specialists to label 15,000 historical email threads, paying $180,000 for the labeling work. The reported agreement metric was 84%, which the program manager presented to executive leadership as evidence of high-quality labels. The model was trained, evaluated at 79% accuracy, and deployed to monitor communications from 2,800 employees. Three months later, the Legal team raised concerns: the AI was flagging approximately 40% of communications as potentially problematic, overwhelming the human review queue and generating complaints from business units that routine operational discussions were being unnecessarily scrutinized. A consulting firm reviewed the labeling methodology and discovered the issue immediately. The 84% agreement was raw percentage agreement: the proportion of examples where two annotators assigned the same label. The task was binary: problematic or benign. By random chance alone, two annotators choosing randomly between two options would agree 50% of the time. The 84% raw agreement meant the annotators were only 34 percentage points better than random guessing. When the consultants calculated Cohen's Kappa, which corrects for chance agreement, the score was 0.41, in the fair-to-moderate range, far below the threshold for a compliance-critical application. The labels were unreliable, the model learned unreliable patterns, and the deployment created operational chaos. The company spent $320,000 rebuilding the labeling program with proper agreement measurement, delaying the product launch by seven months.

This failure illustrates why raw agreement percentages are misleading and why professional labeling programs use chance-corrected metrics. The choice of metric matters, and understanding when to use which metric is essential for running credible labeling operations in 2026.

## Why Raw Agreement Percentage Is Insufficient

Raw agreement percentage is calculated by taking all the examples that multiple annotators labeled, counting how many times they assigned the same label, and dividing by the total number of examples. If two annotators label 100 examples and agree on 73 of them, the raw agreement is 73%. This number feels intuitive, but it is deceptive because it does not account for the agreement that would occur purely by chance.

Consider the simplest case: a binary labeling task with two labels, A and B. Two annotators label examples independently. If both annotators are guessing randomly with no pattern, they will still agree approximately 50% of the time. Half the time, both will randomly choose A. Half the time, both will randomly choose B. On a sufficiently large sample, pure random guessing produces 50% raw agreement. If your actual raw agreement is 60%, that does not mean your annotators are performing well. It means they are only 10 percentage points better than random chance. The signal in your labels is the difference between observed agreement and chance agreement, not the raw percentage.

The problem becomes worse when label distributions are skewed. Suppose you are labeling content moderation cases, and 90% of examples are benign while 10% violate policies. Two annotators who both label everything as benign will achieve 90% raw agreement even though they are providing zero useful signal. They agree on all the benign cases simply because benign cases dominate the dataset. The 10% of violating cases get lost in the noise. Raw agreement is inflated by the majority class, masking the fact that annotators cannot reliably identify the minority class that actually matters for your use case.

This is why every serious labeling program in 2026 uses chance-corrected agreement metrics. The two most widely adopted metrics are Cohen's Kappa and Krippendorff's Alpha. Both correct for chance agreement, but they differ in scope, flexibility, and mathematical assumptions. Understanding when to use each one determines whether your agreement measurement is rigorous or merely performative.

## Cohen's Kappa: The Standard Two-Annotator Metric

Cohen's Kappa measures agreement between two annotators while correcting for the agreement that would occur by chance given the observed label distributions. The formula is straightforward in concept: Kappa equals observed agreement minus expected agreement, divided by one minus expected agreement. Observed agreement is the raw percentage we already discussed. Expected agreement is the proportion of agreement you would see if both annotators were labeling randomly but maintaining their individual label distributions.

The calculation works like this. Suppose Annotator One labels 60% of examples as Category A and 40% as Category B. Annotator Two labels 70% as Category A and 30% as Category B. If they are labeling randomly with those distributions, the chance they both pick A on any given example is 0.60 times 0.70, which equals 0.42. The chance they both pick B is 0.40 times 0.30, which equals 0.12. Total expected agreement is 0.42 plus 0.12, which equals 0.54. If the observed agreement is 0.78, then Kappa equals 0.78 minus 0.54, divided by one minus 0.54, which equals 0.24 divided by 0.46, which equals approximately 0.52.

This number, 0.52, tells you that the annotators' actual agreement is 52% of the way from chance agreement to perfect agreement. It removes the inflation caused by chance overlap and gives you a measure of true signal. The interpretation scale for Kappa is widely standardized. Values below 0.20 indicate poor agreement, essentially no better than chance. Values from 0.20 to 0.40 indicate fair agreement, minimal consistency. Values from 0.40 to 0.60 indicate moderate agreement, sufficient for exploratory work but not for production systems. Values from 0.60 to 0.80 indicate substantial agreement, the minimum threshold for most production labeling tasks. Values above 0.80 indicate near-perfect agreement, appropriate for safety-critical or compliance-critical applications.

These thresholds are guidelines, not absolute rules. The acceptable Kappa for a given task depends on the task's subjectivity, business risk, and use case requirements. For labeling sentiment in customer reviews, a Kappa of 0.65 might be acceptable because sentiment is somewhat subjective and minor disagreements do not create major downstream risks. For labeling medical diagnosis codes, a Kappa of 0.75 is the minimum, and 0.85 is preferred, because errors have patient safety implications. For labeling content policy violations that determine whether users get banned, a Kappa of 0.80 is standard because the decisions have significant user impact and potential legal exposure.

Cohen's Kappa has two major limitations. First, it only works for two annotators. If you have three or more annotators labeling the same examples, you cannot directly apply Cohen's Kappa. Some teams work around this by calculating pairwise Kappa for every pair of annotators and averaging the results, but this approach loses information and becomes unwieldy as the number of annotators grows. Second, Kappa assumes your labels are nominal categories with no inherent ordering. If your labels have ordinal structure, like rating severity on a scale from one to five, Kappa treats a disagreement between one and two the same as a disagreement between one and five, even though the latter is a much larger error. For tasks with ordinal or interval scales, you need a metric that accounts for the magnitude of disagreement, not just whether annotators chose the same category.

## Krippendorff's Alpha: The Gold Standard for Complex Labeling

Krippendorff's Alpha is the most flexible and rigorous agreement metric available in 2026. It handles any number of annotators, supports missing data, and works with nominal, ordinal, interval, and ratio scales. This flexibility makes it the preferred metric for labeling programs that go beyond simple binary or multi-class classification. If you are running a professional annotation operation at scale, you should be using Alpha, not Kappa, unless you have a specific reason to prefer Kappa's simplicity.

The conceptual foundation of Alpha is the same as Kappa: it compares observed disagreement to expected disagreement and calculates how much better than chance your annotators are performing. The formula is Alpha equals one minus observed disagreement divided by expected disagreement. When Alpha equals one, there is perfect agreement. When Alpha equals zero, agreement is no better than chance. When Alpha is negative, agreement is worse than chance, which usually indicates a systematic problem like annotators misunderstanding the task or labels being applied in reverse.

The power of Alpha comes from its flexibility in defining disagreement. For nominal categories, disagreement is binary: annotators either agree or they do not. For ordinal categories, disagreement is the squared difference in rank. If one annotator assigns severity rank two and another assigns rank four, the disagreement is four, which is two squared. For interval and ratio scales, disagreement is the squared numeric difference. If one annotator rates quality at 6.2 and another rates it at 7.8, the disagreement is 2.56, which is 1.6 squared. This weighting ensures that large disagreements contribute more to the metric than small disagreements, which is appropriate when your labels have meaningful magnitude.

Alpha also handles missing data gracefully. In real labeling programs, not every annotator labels every example. You might have ten annotators but only three label any given example. Kappa cannot handle this; it requires the same two annotators to label all examples. Alpha calculates expected disagreement based on all available data, treating missing annotations as a natural part of the process. This makes Alpha practical for large-scale programs where you are sampling examples for agreement checks rather than having every annotator label every example.

The interpretation scale for Alpha is similar to Kappa but slightly more stringent. Values above 0.80 indicate high reliability, suitable for drawing definitive conclusions. Values from 0.67 to 0.80 allow tentative conclusions and are acceptable for many production use cases. Values below 0.67 indicate insufficient reliability, appropriate only for exploratory analysis or early-stage guideline development. These thresholds come from decades of research in content analysis and are widely accepted across fields including healthcare, social science, and compliance monitoring.

## When to Use Each Metric in Practice

The decision tree for choosing an agreement metric is straightforward. If you have exactly two annotators, your labels are nominal categories, and you want a simple, widely understood metric, use Cohen's Kappa. If you have more than two annotators, if your labels are ordinal or continuous, if you have missing data, or if you want a metric that will remain valid as your labeling program scales and evolves, use Krippendorff's Alpha. In 2026, the default choice for any serious labeling program is Alpha. Kappa remains useful for quick checks, small-scale pilot projects, or situations where you need to communicate with stakeholders who are already familiar with Kappa and would be confused by introducing a different metric.

There are edge cases where neither metric is ideal. If you are measuring agreement on a ranking task where annotators sort items in order, you need a rank correlation metric like Kendall's Tau. If you are measuring agreement on free-text annotations where annotators highlight spans of text, you need overlap metrics like F1 on token boundaries. If you are measuring agreement on multi-label classification where each example can have multiple labels, you need label-wise agreement metrics. These specialized cases are less common but important to recognize so you do not force-fit Kappa or Alpha where they do not apply.

The other practical consideration is tooling. As of 2026, most statistical software packages and Python libraries support both Kappa and Alpha. The scikit-learn library provides Cohen's Kappa. The Krippendorff library provides Alpha with support for all measurement levels. If your team is already using a particular analysis platform, check what it supports natively before committing to a metric. Calculating Kappa by hand is feasible for small datasets. Calculating Alpha by hand is tedious and error-prone; you should always use software.

## Interpreting Agreement Metrics in Context

A Kappa or Alpha score is not meaningful in isolation. You must interpret it in the context of the specific labeling task, the difficulty of the task, and the consequences of disagreement. A Kappa of 0.60 on a subjective task like rating the tone of customer emails might be acceptable. A Kappa of 0.60 on an objective task like categorizing support tickets into defined product areas is a signal that your guidelines are unclear or your annotators need retraining. A Kappa of 0.60 on a safety-critical task like labeling medical urgency is unacceptable and indicates you should not proceed with model training until agreement improves.

The subjectivity of the task is the first contextual factor. Tasks that require judgment calls, aesthetic assessments, or predictions about user intent will naturally have lower agreement than tasks that involve applying explicit rules to objective facts. If you ask annotators to rate whether a chatbot response is helpful, you are asking for a subjective judgment, and even well-trained annotators will legitimately disagree on borderline cases. If you ask annotators to classify whether a support ticket mentions billing, technical issues, or account management, you are asking for an objective categorization, and disagreement indicates confusion about category definitions.

The business risk is the second factor. High-risk decisions require higher agreement thresholds. If incorrect labels lead to regulatory violations, patient harm, financial loss, or user safety issues, you need Alpha above 0.80. If incorrect labels lead to suboptimal model performance that slightly degrades user experience, Alpha above 0.70 is sufficient. If incorrect labels are used only for exploratory analysis where conclusions will be validated through other means, Alpha above 0.60 is acceptable. The threshold should be set explicitly at the start of the labeling program, documented in your labeling plan, and communicated to stakeholders so everyone understands the quality bar.

The third factor is the label distribution. Agreement metrics behave differently when classes are balanced versus imbalanced. In a perfectly balanced binary task with 50% of examples in each class, chance agreement is 50%, and Kappa will be close to raw agreement minus 0.5, divided by 0.5. In a heavily imbalanced task with 95% negative and 5% positive examples, chance agreement is higher, approximately 90%, and Kappa adjusts more aggressively. This is correct behavior because in the imbalanced case, high raw agreement is easier to achieve and less meaningful. But it means you cannot directly compare Kappa scores across tasks with different distributions without considering the baseline difficulty.

## The Common Mistake of Reporting Raw Agreement

Despite the availability of Kappa and Alpha, many labeling programs in 2026 still report raw agreement percentages, either because the team does not understand the importance of chance correction or because stakeholders are unfamiliar with Kappa and Alpha and prefer a simpler number. This is a mistake. Reporting raw agreement without chance correction is like reporting model accuracy on a dataset where 90% of examples are negative: the number looks good but provides no useful information about whether the model can distinguish signal from noise.

When you encounter a labeling program that reports only raw agreement, you should immediately ask two questions. First, what is the chance agreement for this task given the label distribution? If the task is binary and labels are balanced, chance agreement is 50%, so raw agreement of 75% translates to moderate real agreement. If labels are imbalanced 80-20, chance agreement is 68%, so raw agreement of 75% translates to poor real agreement. Second, why is the team not using Kappa or Alpha? If the answer is that they did not know about chance-corrected metrics, they need training. If the answer is that stakeholders prefer raw agreement because it is easier to understand, you need to educate stakeholders about why the simple metric is misleading.

The correct approach is to report both raw agreement and chance-corrected agreement in stakeholder communications. Lead with the chance-corrected metric because it is the meaningful one, but include raw agreement as context for stakeholders who are used to seeing percentage metrics. For example: "Inter-annotator agreement measured by Krippendorff's Alpha is 0.74, indicating substantial reliability. Raw agreement is 82%, compared to 58% expected by chance given our label distribution." This framing gives stakeholders the simple percentage they find intuitive while anchoring the interpretation on the rigorous metric.

## Calculating and Reporting Agreement in Production

In a production labeling program, agreement measurement is an ongoing process, not a one-time check. You should measure agreement during annotator onboarding to verify training is effective, during guideline updates to verify changes improve clarity, and continuously during production labeling to detect drift. The operational cadence that works for most teams is weekly agreement checks on a random sample of examples. The sample size depends on your labeling volume. If you are labeling 500 examples per week, sample 50 examples and have two or three annotators label each one. If you are labeling 10,000 examples per week, sample 200 examples. The goal is to get enough data to calculate a stable agreement metric without creating excessive annotation overhead.

The mechanics of sampling are important. You want a representative sample that reflects the distribution of difficulty in your full dataset. Purely random sampling works if your examples are homogeneous. Stratified sampling is better if you have identifiable subgroups, like multiple content types, multiple domains, or multiple edge case categories. You might sample 40 examples from the easy category, 30 from the moderate category, and 30 from the hard category, ensuring you measure agreement where it matters most. Avoid the temptation to cherry-pick examples for agreement checks. Cherry-picking creates an illusion of higher agreement than actually exists in production.

Once you have agreement scores, you need to report them in a way that drives action. The report should include the overall agreement score, per-category agreement scores if you have multiple categories, per-annotator agreement scores to identify individuals who need retraining, and a trend line showing whether agreement is improving, stable, or degrading over time. If agreement drops below your threshold, the report should trigger an investigation. Common causes of declining agreement include guideline drift, annotator fatigue, introduction of new edge cases, or turnover in the annotator pool. Identifying the cause allows you to take corrective action: update guidelines, run refresher training, add examples to the calibration set, or rotate annotators to maintain focus.

## Agreement as a Leading Indicator of Label Quality

Agreement is a leading indicator, not a lagging indicator. It tells you whether your labeling process is on track before you have enough labeled data to evaluate model performance. This makes it invaluable for catching problems early. If you wait until you train a model and evaluate it to discover that your labels are unreliable, you have already invested weeks of engineering effort and thousands of dollars in annotation costs. If you measure agreement from day one and detect low agreement during the first week of labeling, you can pause, fix the guidelines, retrain annotators, and resume with confidence that you are collecting reliable data.

This front-loaded quality control is especially important for long-running labeling programs. If you are labeling 50,000 examples over three months, and you do not measure agreement until the end, you risk discovering in month three that the first two months of labeling were unreliable. You cannot recover that time or cost. If you measure agreement weekly, you catch issues in week one, fix them in week two, and the remaining eleven weeks produce reliable labels. The incremental cost of weekly agreement checks is small compared to the cost of late-stage discovery of systematic labeling failures.

Teams that treat agreement as a leading indicator also use it to optimize annotator allocation. If you have ten annotators and you discover that three of them consistently show low agreement with the rest of the pool, you can reassign those three to other tasks, provide additional training, or if necessary, remove them from the program. This is not punitive; it is quality control. Some people are better suited to annotation work than others, and some people need more training to internalize complex guidelines. Identifying low performers early and addressing the gap protects the integrity of your dataset.

## Beyond Agreement: What the Metric Does Not Tell You

Agreement measures consistency, not correctness. This limitation is important to remember. High agreement means annotators are applying a shared framework. It does not tell you whether that framework is the right one. You still need domain expert review, stakeholder validation, and outcome analysis to verify that the consistent labels are also correct. The workflow is: measure agreement to ensure consistency, then validate correctness through expert review, then scale up labeling with confidence that you are producing both consistent and correct labels.

Agreement also does not tell you whether your guidelines are complete. You can have high agreement on the examples in your dataset while having no idea how annotators would handle new edge cases that are not yet represented. This is why guideline development is iterative. You write initial guidelines, measure agreement, identify categories or edge cases with low agreement, update guidelines to clarify those cases, measure agreement again, and repeat until agreement stabilizes. Each iteration expands the coverage of your guidelines, making them more robust to future edge cases.

Finally, agreement does not tell you whether your labeling task is well-defined. If annotators consistently agree but their labels do not align with stakeholder expectations or business requirements, the problem is not annotator performance; the problem is task definition. This is why task framing and success criteria, covered in earlier sections, are prerequisites for labeling. You must define what you are labeling and why before you can measure whether annotators are labeling it consistently.

The next step in building a rigorous quality control system is understanding how to run calibration exercises that surface disagreement, diagnose its causes, and drive guideline improvements that raise agreement to acceptable levels.

