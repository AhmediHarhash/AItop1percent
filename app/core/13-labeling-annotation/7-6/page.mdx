# 7.6 — Calibrating AI Labelers Against Human Ground Truth

What happens when you deploy an AI labeling system that produces outputs faster and cheaper than human labelers but defines categories differently than humans do? You generate massive labeled datasets that look consistent internally but systematically diverge from the ground truth your human experts would produce. The labels train models that perform well on AI-labeled validation sets but fail on real-world data labeled by actual domain experts. This is not a hypothetical risk—it is the default outcome when teams replace human labeling with AI labeling without measuring alignment between the two.

Calibration is the foundation of AI-assisted labeling. You are not asking whether the AI can label, but whether its labels mean the same thing your human labelers' labels mean, at what error rate, and under what conditions that alignment holds or breaks. Without calibration, you have output, but you do not know what that output represents. With calibration, you have confidence intervals, known failure modes, and a measurement framework that tells you when the AI is safe to trust and when it requires human correction.

## Building the Calibration Set

Your calibration set is a collection of examples labeled by your highest-quality human labelers, treated as ground truth, against which you measure the AI's performance. This is not your training set, not your evaluation set, and not a random sample of unlabeled data. It is a curated, gold-standard reference that represents the full range of labeling complexity your AI will encounter in production.

You build this set by selecting examples that span the entire task distribution. If you are labeling customer support tickets into urgency tiers, your calibration set must include obvious urgent cases, obvious non-urgent cases, and the entire middle ground where urgency is ambiguous or context-dependent. If you are labeling medical imaging findings, your set must include clear positives, clear negatives, borderline cases, rare variants, and common artifacts that mimic true findings. The goal is coverage across the decision boundary, not just coverage of the common cases.

Coverage across the decision boundary means you deliberately oversample from regions where labeling is difficult. If 90 percent of your examples are easy to label and 10 percent are hard, a random sample gives you mostly easy examples. You want a calibration set where 30 to 50 percent of examples are from the hard region, because those are the cases where the AI is most likely to fail and where measurement is most important. You sample easy cases to ensure the AI is not degrading on routine work, but you oversample hard cases to ensure you have statistical power to detect problems in the regions that matter most.

The size of your calibration set depends on the number of label classes, the complexity of the task, and the error tolerance you require. For binary classification tasks with clear decision criteria, 500 to 1,000 examples may suffice. For multi-class tasks with overlapping categories, you need 2,000 to 5,000 examples to ensure sufficient representation of edge cases. For tasks with hierarchical labels or compound judgments, you may need 10,000 examples or more to cover the cross-product of decision points. The principle is that you need enough examples in each decision region to measure agreement reliably, not just enough to say you have a calibration set.

Sample size also depends on your required confidence intervals. If you need to measure agreement within plus or minus 2 percentage points with 95 percent confidence, you need larger samples than if you are comfortable with plus or minus 5 percentage points. Statistical power calculations tell you the minimum sample size required to detect a difference between your target agreement and your actual agreement with the confidence level you require.

You label this set using your best human labelers, ideally with consensus labeling where two or three labelers independently label each example and resolve disagreements through discussion. This is expensive, but it is also the only way to establish ground truth. If your human labelers disagree on 15 percent of examples, you cannot expect the AI to agree with an arbitrary single labeler at higher rates. Consensus labeling surfaces these disagreements, forces resolution, and produces labels that represent your organization's true labeling standard, not individual annotator preferences.

Consensus labeling also identifies examples where the guidelines are unclear. If your labelers disagree repeatedly on certain types of examples, the problem is not the labelers; it is the guidelines. You revise the guidelines to clarify the ambiguous cases, then relabel those examples using the updated guidelines. This iterative process produces both a high-quality calibration set and improved labeling guidelines that reduce future disagreement.

Once you have consensus labels, you freeze the set. You do not update it casually, you do not relabel it to match AI predictions, and you do not remove examples where the AI performs poorly. The calibration set is your reference ruler. If you change the ruler, you lose the ability to measure drift over time.

Freezing the set does not mean never updating it. You add new examples periodically to reflect evolving task distributions, but you do not remove or relabel existing examples except when labeling policy changes require it. When you add new examples, you append them rather than replacing old ones, so you maintain a historical record of performance on both old and new data.

## Measuring AI-Human Agreement

Agreement is measured through metrics that capture both overall alignment and alignment on specific subpopulations. The simplest metric is raw accuracy: the percentage of calibration examples where the AI's label matches the human consensus label. For the legal tech company, raw accuracy on indemnification clauses was 94 percent, which sounded high until they realized that 6 percent error on 4,200 clauses meant 252 mislabeled high-risk provisions.

Raw accuracy is necessary but insufficient. You also measure class-level precision and recall, which tell you whether the AI is systematically over-predicting or under-predicting specific labels. If your AI labels 98 percent of support tickets correctly overall but only catches 60 percent of urgent escalations, you have a recall problem on the class that matters most. If your AI flags 30 percent of content as potentially violating community guidelines but only 40 percent of those flags are correct, you have a precision problem that will overwhelm your human review queue.

Precision and recall trade off against each other in most labeling systems. You can increase recall by lowering the threshold for positive predictions, but this also increases false positives and decreases precision. You can increase precision by raising the threshold, but this increases false negatives and decreases recall. The optimal threshold depends on the relative cost of false positives versus false negatives. For fraud detection, false negatives are expensive because they allow fraud to proceed. For spam filtering, false positives are expensive because they block legitimate messages. You set thresholds based on which error is more costly in your domain.

You measure agreement separately for different slices of your data. Slice by input characteristics: does the AI perform worse on short text versus long text, on technical jargon versus plain language, on new user content versus established user content? Slice by task difficulty: does the AI agree with humans on easy cases at 98 percent but on ambiguous cases at only 72 percent? Slice by label frequency: does the AI perform well on common labels but fail on rare labels that appear in fewer than 2 percent of examples?

Slice-level metrics often reveal problems that overall metrics hide. An AI with 90 percent overall accuracy might have 98 percent accuracy on 80 percent of cases and 60 percent accuracy on 20 percent of cases. If that 20 percent represents high-stakes decisions, the overall metric is misleading. You need to know performance on every slice that matters to your stakeholders, not just performance in aggregate.

For tasks with subjective or graded labels, you measure agreement using metrics like Cohen's kappa or Fleiss' kappa, which account for the possibility of chance agreement. If you are labeling sentiment as positive, neutral, or negative, random guessing produces 33 percent agreement just by luck. Kappa adjusts for this baseline, giving you a measure of true agreement beyond chance. A kappa of 0.80 or higher is considered strong agreement, 0.60 to 0.80 is moderate, and below 0.60 suggests the AI is not reliably aligned with human judgment.

Kappa is especially important when label distributions are imbalanced. If 95 percent of examples are labeled negative and only 5 percent are positive, an AI that labels everything negative achieves 95 percent accuracy but zero kappa because it is not doing better than chance on the positive class. Kappa forces you to look at whether the AI is actually learning the task or just exploiting class imbalance.

You also track disagreement patterns. When the AI disagrees with humans, is it always wrong in the same direction? Does it consistently over-predict severity, under-predict risk, or confuse two specific classes? These patterns reveal systematic biases that can be corrected through prompt refinement, few-shot example adjustment, or post-processing rules. Random disagreement is noise; patterned disagreement is signal.

Confusion matrices visualize these patterns. You build a matrix showing which labels the AI predicted versus which labels humans assigned. Off-diagonal cells show where the AI is confused. If the AI frequently confuses medium-risk with high-risk but rarely confuses low-risk with high-risk, you know the problem is at the boundary between medium and high, not across the entire label space. You refine your prompt to better distinguish those two categories, add few-shot examples that highlight the boundary, and remeasure.

## Detecting Systematic Biases

Systematic bias occurs when the AI's errors are not random but instead cluster around specific attributes, categories, or decision boundaries. The legal tech company's AI was biased toward labeling clauses as standard when they contained familiar legal phrasing, even when the substantive terms were non-standard. This is a pattern-matching bias: the model learned to rely on surface features rather than semantic content.

You detect bias by stratifying your calibration set and measuring agreement within each stratum. If your AI labels content moderation cases at 92 percent accuracy overall but only 68 percent accuracy on content authored by users under age 18, you have an age-related bias. If your AI labels loan applications at 89 percent accuracy overall but only 74 percent accuracy on applications from non-native English speakers, you have a language bias. If your AI labels product reviews at 91 percent accuracy overall but only 79 percent accuracy on reviews longer than 500 words, you have a length bias.

Stratified analysis requires segmenting your calibration set along every dimension that might affect labeling difficulty or correctness. You segment by input length, source type, user demographic, timestamp, and any domain-specific attributes that could introduce variance. For each segment, you compute the same agreement metrics you compute overall: accuracy, precision, recall, and kappa. If any segment shows metrics more than five percentage points below the overall average, you investigate why.

Bias also appears as label imbalance in the AI's predictions. If your human-labeled calibration set has 12 percent of examples in the high-risk category, but the AI predicts high-risk only 4 percent of the time, the AI is under-predicting that class. This happens because models optimize for overall accuracy, and on imbalanced datasets, the easiest way to maximize accuracy is to predict the majority class more often. You correct this by adjusting classification thresholds, reweighting classes in the prompt, or adding more few-shot examples from the under-predicted class.

Class imbalance correction is iterative. You adjust the prompt to emphasize the under-predicted class, rerun the AI on the calibration set, and measure whether the class distribution shifts closer to the human baseline without degrading precision. If increasing recall on high-risk clauses from 68 percent to 84 percent also increases false positives from 8 percent to 22 percent, you are trading one problem for another. The goal is to match human class distribution while maintaining acceptable precision and recall on all classes, which often requires multiple rounds of tuning.

You also look for proxy biases, where the AI uses features that correlate with the true label but are not causally related. An AI labeling job candidate resumes might learn that resumes mentioning certain universities are more likely to be labeled as strong, even when the university name is not a valid criterion. An AI labeling insurance claims might learn that claims filed on Mondays are more likely to be labeled as fraudulent, even when day of the week is not a fraud signal. These proxy biases emerge because the AI optimizes for prediction accuracy on the training distribution, not for adherence to your labeling policy.

Detecting proxy bias requires domain knowledge. You cannot find it by looking at metrics alone. You find it by reviewing cases where the AI disagrees with humans, asking why the AI might have made that prediction, and checking whether the AI is relying on features you did not intend to be decision factors. This is qualitative work, done by labeling experts who understand the task deeply. You pull a random sample of 50 to 100 disagreement cases, read through them, look for patterns in what the AI got wrong, and hypothesize what surface features it might be over-relying on. Then you test that hypothesis by creating synthetic examples that have those features but different true labels, running the AI on those examples, and seeing if it makes the predicted errors.

Some proxy biases are acceptable if they improve overall accuracy and do not violate fairness constraints. If your AI labeling support tickets learns that tickets containing specific error codes are more likely to be urgent, and that correlation holds in your domain, the proxy is valid. But if your AI learns that tickets from users with certain email domain patterns are more likely to be low-priority, and that correlation reflects historical under-prioritization of certain customer segments, the proxy is a fairness problem. You decide which proxies are legitimate based on your labeling policy, your regulatory obligations, and your organizational values.

## Recalibrating After Model Updates

Every time you update the underlying model powering your AI labeler, you must recalibrate. Model updates change the prediction distribution, sometimes in ways that improve overall metrics but degrade performance on specific slices. A healthcare SaaS company learned this in late 2025 when they upgraded their clinical note labeler from GPT-5 to GPT-5.1. Overall accuracy improved from 87 percent to 91 percent, but accuracy on notes involving patients with multiple chronic conditions dropped from 84 percent to 76 percent. The new model had learned to focus on primary diagnoses and downweight secondary conditions, which was correct for most cases but wrong for the complex cases that mattered most to their clinical workflows.

Model updates are unpredictable in their effects. The new model may have different biases, different failure modes, and different performance characteristics on edge cases. What worked well in the old model may work poorly in the new model, and vice versa. You cannot assume that a newer model is better across all dimensions. You must measure.

Recalibration means rerunning the AI on your frozen calibration set and comparing the new agreement metrics to the baseline. You measure overall accuracy, class-level precision and recall, slice-level performance, and disagreement patterns. You document what changed, why it changed, and whether the change is acceptable given your task requirements. If the new model improves performance on 80 percent of slices but degrades it on 20 percent, you decide whether the trade-off is worth it based on which slices matter most to your stakeholders.

The decision to deploy a new model is not automatic. Just because a new model version is available does not mean you should upgrade. You upgrade only if the recalibration results show that the new model meets or exceeds your performance thresholds on all critical slices. If it improves some slices but degrades others, you evaluate the trade-off. If the degraded slices are low-volume or low-stakes, you may accept the trade-off. If the degraded slices are high-stakes, you do not deploy the new model, or you adjust your prompts and examples to bring performance back up before deploying.

You also recalibrate when you change your prompts, your few-shot examples, or your labeling guidelines. Each of these changes shifts the decision boundary, and you need to measure whether that shift aligns with your ground truth or moves away from it. A prompt change that increases recall on urgent support tickets from 72 percent to 88 percent is valuable. A prompt change that increases recall to 88 percent but also increases false positives from 5 percent to 18 percent may not be worth the trade-off.

Prompt changes are easier to iterate on than model changes, but they still require recalibration. You do not change a prompt in production without first testing it on the calibration set, measuring the impact, and verifying that the change improves performance without introducing new failures. Some teams maintain a staging environment where they test prompt changes on the calibration set before deploying to production. This prevents regressions.

Recalibration is not optional. It is the control mechanism that prevents silent degradation. Without it, you are flying blind every time you make a system change. Silent degradation is the worst kind because you do not discover it until downstream systems start failing, users start complaining, or audits reveal systematic errors. Recalibration catches degradation before it reaches production.

## Maintaining Calibration Over Time

Calibration drifts. The AI's performance on your calibration set may remain stable, but the real-world distribution of inputs changes, and new edge cases emerge that were not represented in your original calibration set. A content moderation AI calibrated in early 2025 might perform well on hate speech patterns common at that time but fail on new coded language, slang, or meme formats that emerge six months later. A contract labeling AI calibrated on standard commercial agreements might fail when the company expands into new industries with different contract structures.

Drift happens in three ways. First, the input distribution shifts. Your users start writing longer support tickets, uploading different types of images, or using vocabulary that was rare six months ago but is common now. Second, the task definition shifts. Your stakeholders decide that what used to be labeled medium-risk is now high-risk, or they introduce new label categories that did not exist when you built your calibration set. Third, adversarial adaptation occurs. Users learn what patterns the AI flags and adjust their behavior to evade detection, which means the AI's learned patterns no longer match current evasion techniques.

You maintain calibration by periodically adding new examples to your calibration set, drawn from recent production data. Every quarter, you sample 200 to 500 examples from the data your AI has labeled in production, have your human labelers label them with consensus, and add them to your calibration set. You then remeasure agreement on the expanded set. If agreement drops from 89 percent to 82 percent, you know the production distribution has shifted and your AI is no longer aligned with current human judgment.

The sampling strategy matters. You do not sample uniformly at random, because that gives you mostly easy cases. You oversample from regions where the AI is uncertain, where human reviewers have overridden AI predictions, or where the input characteristics differ from your historical calibration set. If your AI now processes 30 percent more video content than it did six months ago, you oversample video examples to ensure your calibration set reflects the current production mix.

You also monitor for labeling policy changes. If your organization updates its definition of what constitutes a high-risk contract clause, your calibration set must be relabeled to reflect the new policy, or you must create a new calibration set under the new standard. The AI cannot be expected to align with a policy it was not calibrated against. This is why large annotation teams version their guidelines and track which version was used for each calibration set.

Policy versioning is critical when you operate in regulated environments or when your labeling decisions have legal or compliance implications. You maintain a changelog that documents every policy update, the date it went into effect, and which calibration set version reflects that policy. When an auditor asks how you labeled data in Q2 2025, you can point to the specific guideline version and calibration set that were active at that time. This is not bureaucratic overhead; it is the audit trail that proves your labeling system was operating correctly under the standards that applied when the labels were generated.

Drift detection is automated by tracking agreement metrics over time and alerting when they cross thresholds. If your AI's accuracy on the calibration set drops below 85 percent, or if recall on high-priority classes drops below 80 percent, you trigger a recalibration review. This review includes error analysis, bias detection, and a decision about whether to retrain, adjust prompts, or revert to a previous configuration.

You set thresholds based on the tolerance of your downstream systems. If your fraud detection pipeline can tolerate a 15 percent false negative rate, you set your recall threshold at 85 percent. If your content moderation workflow requires 95 percent precision to avoid overwhelming human reviewers, you set your precision threshold accordingly. The thresholds are not arbitrary; they are derived from the operational requirements of the systems that depend on your labels.

Some organizations run continuous calibration, where a small percentage of production examples are randomly routed to human labelers instead of the AI, and those human labels are compared to what the AI would have predicted. This produces a real-time agreement metric that reflects current performance on current data, not historical performance on a frozen set. Continuous calibration is expensive because it requires ongoing human labeling, but it is the gold standard for high-stakes systems where drift must be detected within days, not months.

Continuous calibration typically samples 1 to 5 percent of production volume. If your AI labels 10,000 examples per day, you route 100 to 500 of those to human labelers, collect their labels, and compare them to the AI's predictions on the same examples. You compute daily or weekly agreement metrics and plot them over time. If you see a downward trend, you investigate before the degradation becomes severe. This early warning system is what separates reactive operations that discover problems after they cause downstream failures from proactive operations that catch problems while they are still small and fixable.

## The Calibration Feedback Loop

Calibration is not a one-way measurement where you evaluate the AI and then move on. It is a feedback loop where measurement informs tuning, tuning changes performance, and performance is measured again. You calibrate, identify biases or low-agreement slices, adjust prompts or examples to correct those issues, recalibrate to confirm the adjustment worked, and repeat.

This loop runs continuously in mature AI labeling operations. A financial services company running AI-assisted fraud labeling recalibrates weekly, adjusts prompts or thresholds based on what they find, and redeploys within 48 hours. Their calibration metrics are tracked in a dashboard visible to the fraud ops team, the engineering team, and the compliance team. When metrics degrade, the responsible team investigates immediately, not at the end of the quarter.

The feedback loop also connects calibration to your human-in-the-loop strategy. If calibration reveals that the AI performs poorly on a specific slice, you route that slice to human labelers and use their labels to expand your few-shot examples or adjust your prompt. The humans are not just fixing errors; they are generating the signal you need to improve the AI. This is why calibration and human review are interdependent: calibration tells you where you need human input, and human input improves the system so that future calibration metrics improve.

## Calibration Thresholds for Different Task Types

The acceptable agreement threshold depends on the cost of errors and the availability of downstream correction mechanisms. For customer support ticket routing, 85 percent agreement may be acceptable because misrouted tickets are corrected by human agents within hours, and the cost of a misroute is low. For medical diagnosis labeling, 95 percent agreement is the minimum because errors delay treatment and affect patient outcomes. For contract risk labeling, 90 percent agreement on high-risk clauses is non-negotiable because missing a single high-risk clause can expose the company to multi-million dollar liability.

You set thresholds by working backward from the impact of errors. If your AI mislabels 10 percent of examples, how many of those errors propagate to end users, how many are caught by downstream review, and what is the cost when an error is not caught? If 10 percent error means 1,000 mislabeled items per week, 80 percent of those are caught in review, and the remaining 200 cause an average of 50 dollars in rework cost each, your expected weekly error cost is 10,000 dollars. You decide whether that cost is acceptable or whether you need to tighten the threshold.

Different slices within the same task may require different thresholds. Your overall agreement target might be 88 percent, but your agreement target on high-stakes subsets might be 94 percent. A hiring platform using AI to label resumes might accept 85 percent agreement on screening out clearly unqualified candidates but require 95 percent agreement on screening in qualified candidates for sensitive roles. The asymmetry reflects the asymmetry in error costs: rejecting a qualified candidate is more costly than advancing an unqualified one to human review.

Thresholds are also informed by human baseline performance. If your human labelers achieve 92 percent inter-annotator agreement on a task, expecting the AI to achieve 95 percent is unrealistic. The AI ceiling is the human ceiling. You cannot calibrate the AI to perform better than the ground truth it is measured against. In practice, AI agreement with consensus human labels typically runs 3 to 7 percentage points below human inter-annotator agreement, which means if humans agree at 90 percent, you should expect AI agreement in the range of 83 to 87 percent.

## Calibration as a Governance Artifact

Your calibration set and calibration metrics are not just operational tools; they are governance artifacts. When regulators, auditors, or internal compliance teams ask how you know your AI labeling system is performing correctly, your answer is your calibration data. You show them the ground truth set, the agreement metrics, the bias analysis, the recalibration history, and the drift detection logs.

This documentation demonstrates that you have a measurement framework, that you are actively monitoring performance, and that you have processes in place to detect and correct degradation. It is the difference between saying we think our AI works well and saying we measure our AI against human expert consensus at 89 percent agreement with 0.82 kappa, recalibrated quarterly, with drift detection thresholds at 85 percent accuracy.

Governance documentation also includes your calibration methodology. You document how you built the calibration set, how you selected examples, how you achieved consensus labels, how you handle disagreements, and how you update the set over time. You document your agreement metrics, why you chose them, what thresholds you use, and how those thresholds were derived from task requirements. You document your recalibration frequency, your drift detection logic, and your escalation procedures when metrics degrade.

This level of documentation is required for AI systems subject to the EU AI Act, which mandates that high-risk AI systems have quality management processes that include validation and testing procedures. Your calibration framework is your validation procedure. It is how you demonstrate to regulators that you are not deploying an AI system on hope, but on measured evidence of performance.

Calibration also informs your human-in-the-loop strategy. If your AI agrees with humans at 95 percent on easy cases but only 70 percent on ambiguous cases, you route ambiguous cases to humans and let the AI handle easy cases. If your AI has 88 percent recall on urgent support tickets but 96 percent recall on non-urgent tickets, you use the AI for filtering and escalation but require human review on all urgent predictions. Calibration metrics tell you where automation is safe and where it is risky.

The routing logic is deterministic. You define ambiguous cases as those where the AI's confidence score is below a threshold, where the input characteristics match known difficult slices, or where the predicted label is high-stakes. Every example that meets any of these criteria goes to human review. Every example that does not meet these criteria is auto-labeled. You measure the volume split, the accuracy on each side, and the total cost, and you adjust the routing threshold to balance cost and quality.

The companies that treat calibration as a one-time validation exercise fail when their systems drift. The companies that treat calibration as an ongoing operational discipline build AI labeling systems that remain trustworthy over months and years, even as models change, policies evolve, and data distributions shift. Calibration is not a gate you pass through; it is a discipline you maintain.

Calibration is also the foundation of trust between your AI team and the stakeholders who depend on the labels. When Product asks whether the AI labeling system is production-ready, you do not answer with vague assurances. You show them the calibration metrics, explain what they mean, walk them through the error analysis, and give them the confidence intervals. When Legal asks whether the system meets compliance requirements, you show them the governance documentation, the recalibration logs, and the drift detection thresholds. When Operations asks whether they can rely on the AI to reduce human labeling costs, you show them the volume split, the accuracy on auto-labeled examples, and the cost model. Calibration is how you turn an AI experiment into an operational system that stakeholders trust enough to depend on.

## Operationalizing Calibration Measurement

Calibration is not a research activity conducted once before deployment; it is an operational process that runs continuously in production. You build infrastructure that automatically runs the AI on your calibration set, computes agreement metrics, compares them to historical baselines, and alerts when thresholds are breached. This infrastructure is part of your monitoring stack, alongside latency monitoring, error rate tracking, and cost tracking.

The calibration pipeline runs on a schedule. For high-stakes systems, you may run it daily. For lower-stakes systems, weekly or monthly may suffice. Each run produces a calibration report showing overall agreement, slice-level agreement, class-level precision and recall, disagreement patterns, and comparisons to the previous run and to the baseline established at deployment. This report is distributed to the team responsible for the labeling system, reviewed in operational meetings, and archived for audit purposes. The archive provides a historical record of system performance that regulators and auditors can inspect.

When calibration metrics degrade, the alert triggers an investigation workflow. The team reviews recent changes to prompts, models, or system configuration. They examine cases where the AI disagreed with human labels to identify patterns. They check whether the production data distribution has shifted. They decide whether the degradation is a transient anomaly or a persistent problem requiring intervention. This workflow is documented, tracked, and measured, so you know how long it takes to detect and resolve calibration issues.

Calibration metrics also feed into your release process. Before deploying a new model version or a prompt change, you run it through the calibration set in a staging environment. If calibration metrics meet or exceed thresholds, the change is approved for production. If metrics degrade, the change is rejected or sent back for refinement. This gate prevents regressions and ensures that production deployments maintain or improve quality rather than silently degrading it.

The release gate is automated when possible. Your deployment pipeline runs calibration tests as part of the continuous integration process, blocking deployment if metrics fall below thresholds. This prevents human error and ensures consistency.

You now understand how to measure and maintain alignment between AI and human labelers, but even with perfect calibration, there are categories of labeling tasks where AI agreement with humans remains persistently low, no matter how much you tune or recalibrate. The next subchapter addresses the automation ceiling, the types of labels AI cannot reliably produce, and how to identify your system's limits before you deploy.
