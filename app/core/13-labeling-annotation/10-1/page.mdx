# 10.1 â€” Labeling Ambiguous Outputs: When There Is No Right Answer

In late 2025, a healthcare technology company building an AI system to summarize patient-doctor conversations launched a labeling initiative to evaluate summary quality. They hired 12 medical professionals to label summaries as accurate or inaccurate. The labelers reviewed 5,000 summaries over six weeks. When the annotation team analyzed inter-annotator agreement, they found Cohen's kappa of 0.61, indicating moderate agreement but far below the 0.80 threshold they expected for medical data. They investigated discrepancies and discovered that in 34% of cases, the disagreement was not about careless errors or misunderstandings. It was about genuine ambiguity. A doctor's statement like "the patient is responding well but we need to monitor closely" could legitimately be summarized as "patient improving" or "patient requires continued monitoring" depending on which aspect the summarizer emphasized. Both summaries were defensible given the source material. The company had designed their labeling task as if every summary had a single correct quality label, but the reality was that many summaries existed in a legitimately ambiguous space where multiple labels were valid. They tried to force annotators to pick one label, and the result was inconsistent data that made evaluation metrics meaningless. The root cause was not annotator error. It was task design that refused to acknowledge inherent ambiguity.

Labeling ambiguous outputs requires different methods than labeling outputs with clear ground truth. You cannot simply instruct annotators to pick the best label and hope for consistency. You need to design labeling tasks that capture ambiguity explicitly, distinguish between resolvable disagreement caused by unclear guidelines and irreducible ambiguity inherent in the task, and propagate that ambiguity into downstream evaluation in a way that avoids false precision. This means using multi-label approaches when multiple labels are valid, capturing confidence distributions when labels exist on a spectrum, creating explicit legitimately-ambiguous categories when the right answer depends on subjective judgment, and designing evaluation metrics that credit systems for navigating ambiguity well rather than penalizing them for not matching a single arbitrary ground truth label.

This subchapter covers how to label outputs when there is no single right answer. You will learn how to identify when ambiguity is inherent versus when it is a labeling quality problem, how to design labeling tasks that capture ambiguity rather than suppress it, when to force annotators to choose a single label versus when to allow multiple labels or confidence scores, and how ambiguity in labeled data affects downstream evaluation and model training. This is not about tolerating low-quality labels. It is about designing labeling systems that reflect reality rather than imposing false certainty.

## Identifying Inherent Ambiguity Versus Labeling Quality Problems

Not all annotator disagreement indicates ambiguity. Some disagreement is caused by unclear guidelines, inadequate training, or careless labeling. Before you design a labeling task to capture ambiguity, you must confirm that the ambiguity is inherent in the task rather than fixable through better processes. Inherent ambiguity exists when multiple well-trained annotators, following clear guidelines, consistently disagree on the same examples because the task genuinely admits multiple valid interpretations. Labeling quality problems exist when disagreement is random, inconsistent, or resolved through clarification.

The pattern of disagreement reveals whether ambiguity is inherent. If two annotators disagree on a random 30% of examples with no consistent pattern, that indicates quality problems. If two annotators consistently disagree on the same 15% of examples and agree on the rest, that indicates inherent ambiguity in those specific examples. You identify this pattern by analyzing pairwise agreement on individual examples, not just overall agreement statistics. Calculate agreement separately for each labeled example. If an example has low agreement across multiple annotator pairs, it is a candidate for inherent ambiguity. If an example has low agreement for one pair but high agreement for other pairs, that indicates one annotator in the pair is making errors.

You also distinguish inherent ambiguity from guideline ambiguity by iterating on guidelines and measuring whether disagreement decreases. If you refine label definitions, add examples to guidelines, and retrain annotators, and disagreement on specific examples persists, that indicates inherent ambiguity. If disagreement resolves after guideline improvements, that indicates the original guidelines were unclear. For example, if annotators disagree on whether a product review is positive or neutral, and you add a guideline that says "neutral means the reviewer has no strong opinion, not that they have mixed positive and negative opinions," and disagreement resolves, the ambiguity was in the guidelines. If disagreement persists because some reviews genuinely express lukewarm positivity that could be interpreted as either positive or neutral, the ambiguity is inherent.

Another signal of inherent ambiguity is annotator confidence. When you ask annotators to label an output and also report their confidence in that label, low confidence on specific examples indicates ambiguity. If an annotator labels a summary as accurate but reports 60% confidence, they are signaling that the label is defensible but not certain. If that pattern appears across multiple annotators on the same examples, it confirms inherent ambiguity. High disagreement combined with high confidence, on the other hand, indicates annotators are confident but interpreting the task differently, which is a guideline or training problem.

You also examine the source of ambiguity by asking annotators to explain their label choices for disputed examples. When annotators provide explanations, you look for whether they identify the same ambiguity or different ambiguities. If three annotators label a summary differently but all note that the summary omits a key detail and they disagree on whether that omission is acceptable, they have identified the same ambiguity and reached different conclusions. That is inherent ambiguity. If three annotators label a summary differently for completely unrelated reasons, one focusing on factual accuracy, one on completeness, and one on readability, they are evaluating different quality dimensions. That is a task definition problem, not inherent ambiguity.

Once you confirm that ambiguity is inherent, you stop trying to eliminate it through better guidelines or training. Instead, you design the labeling task to capture it explicitly.

## Multi-Label Approaches: Capturing All Valid Interpretations

When an output legitimately supports multiple labels, multi-label annotation allows annotators to assign all applicable labels rather than forcing them to choose one. Multi-label annotation is appropriate when labels are not mutually exclusive and when the goal is to capture the full range of valid interpretations. For example, if you are labeling customer support tickets by topic, a ticket that discusses both a billing issue and a technical issue should be labeled with both billing and technical, not forced into one category.

Multi-label annotation changes the task instruction from "choose the best label" to "choose all labels that apply." The guideline specifies when multiple labels are allowed and when they are not. For example, a guideline might state that a support ticket can have multiple topic labels if it addresses multiple issues, but it can have only one urgency label because urgency is a single dimension. This specificity prevents annotators from over-labeling by applying every remotely plausible label.

Multi-label annotation also requires defining label co-occurrence patterns that are valid versus invalid. Some label combinations are logically consistent, and some are contradictory. A summary can be labeled as both accurate and incomplete, because it can correctly represent part of the source material while omitting other parts. A summary cannot be labeled as both accurate and inaccurate, because those labels are mutually exclusive. The labeling platform enforces these constraints by disallowing invalid combinations, preventing annotators from creating logically inconsistent labels.

When you use multi-label annotation, you also track label co-occurrence frequency. If you find that 80% of examples labeled as accurate are also labeled as incomplete, that indicates the labels are highly correlated and might be better represented as a single label like partially-accurate. If labels co-occur rarely, that validates that they capture independent dimensions. This co-occurrence analysis helps you refine the label schema over time.

Multi-label annotation affects evaluation metrics. Instead of measuring accuracy as the percentage of examples where the model's single predicted label matches the ground truth single label, you measure precision and recall over the label set. Precision is the percentage of labels the model predicted that are present in the ground truth multi-label set. Recall is the percentage of ground truth labels that the model predicted. For example, if the ground truth labels are billing and technical, and the model predicts billing and account, precision is 50% because one of two predicted labels is correct, and recall is 50% because one of two ground truth labels was predicted. This precision-recall framing captures partial correctness, which is appropriate when multiple labels are valid.

Multi-label annotation also enables model training that predicts label distributions rather than single labels. Instead of training a classifier to output the single most likely label, you train it to output a probability for each label, and you apply all labels with probability above a threshold. This approach reflects the reality that some outputs legitimately belong to multiple categories.

## Confidence Distributions: When Labels Exist on a Spectrum

Some labeling tasks involve judgments that exist on a continuous spectrum rather than discrete categories. Quality, tone, relevance, and appropriateness are often matters of degree, not binary distinctions. When you force these judgments into discrete labels, you lose information. Confidence distributions capture the spectrum by asking annotators to assign a confidence score to each possible label, representing how strongly they believe each label applies.

A confidence distribution labeling task presents annotators with a set of labels and asks them to distribute 100 points across those labels based on how well each label describes the output. For example, if you are labeling the tone of a customer service response as professional, friendly, or curt, an annotator might assign 60 points to professional, 30 points to friendly, and 10 points to curt, indicating the response is primarily professional with some friendly elements and a slight edge of curtness. This distribution captures ambiguity that a single label would erase.

Confidence distributions are aggregated across annotators by averaging the distributions. If three annotators assign distributions to the same output, you calculate the mean score for each label across the three annotators. The resulting aggregated distribution represents the collective judgment of the annotator pool. If the aggregated distribution is evenly spread across multiple labels, that indicates high ambiguity. If the distribution is concentrated on one label, that indicates consensus.

You use aggregated confidence distributions in evaluation by measuring the distance between the model's predicted distribution and the ground truth distribution. If the ground truth distribution is 60% professional, 30% friendly, and 10% curt, and the model predicts 70% professional, 20% friendly, and 10% curt, the model is close to the ground truth. You measure this distance using metrics like Jensen-Shannon divergence or Kullback-Leibler divergence, which quantify how different two probability distributions are. Low divergence indicates the model's predicted distribution aligns with human judgment.

Confidence distributions also allow you to identify examples where human judgment is divided. If the aggregated distribution is 33% professional, 33% friendly, and 33% curt, that indicates annotators disagree and there is no consensus. You can flag these examples as high-ambiguity and exclude them from evaluation, or you can include them but credit the model for predicting any distribution that falls within the range of human disagreement. For example, if human annotators assign anywhere from 20% to 50% to each label, you accept any model prediction within that range as valid.

Confidence distributions are particularly useful for evaluating generative outputs like summaries, where quality is multidimensional and subjective. Instead of labeling a summary as good or bad, annotators assign confidence scores to dimensions like factual accuracy, completeness, conciseness, and readability. The aggregated distribution provides a nuanced quality profile that evaluation metrics can measure against.

## The Legitimately Ambiguous Category: Explicit Uncertainty Labels

When some outputs are inherently ambiguous and cannot be meaningfully labeled even with multi-label or confidence distribution approaches, you create an explicit legitimately-ambiguous category. This category is a label that means "multiple interpretations are valid and choosing one is arbitrary." It is not a catch-all for difficult examples. It is a specific label for examples where ambiguity is the correct characterization.

The legitimately-ambiguous category is appropriate when the ambiguity is inherent in the task and cannot be resolved by more information or clearer guidelines. For example, if you are labeling whether an AI-generated email response is appropriate for a customer complaint, and the response is polite but does not directly address the complaint, annotators might disagree on whether it is appropriate. Some annotators value politeness highly and label it appropriate. Others prioritize directness and label it inappropriate. Both judgments are defensible. Rather than force annotators to pick one, you allow them to label the response as legitimately-ambiguous, signaling that appropriateness depends on subjective priorities.

The guideline for the legitimately-ambiguous category specifies when it should be used. It is used when the annotator believes that multiple labels are equally valid and choosing one would misrepresent the output. It is not used when the annotator is uncertain because they do not understand the task or lack domain knowledge. The guideline provides examples of legitimately-ambiguous cases and contrasts them with cases where uncertainty indicates a need for more training.

When you include a legitimately-ambiguous category, you track how often it is used. If 40% of examples are labeled as legitimately-ambiguous, that indicates the task is poorly defined or the label schema does not match the task. If 5% of examples are labeled as legitimately-ambiguous, that indicates the category is capturing genuinely ambiguous edge cases without becoming a crutch. High usage of the legitimately-ambiguous category triggers a review of the task design to determine whether the label schema needs to be refined or whether the task is inherently too subjective to label reliably.

You also analyze inter-annotator agreement on which examples are legitimately-ambiguous. If multiple annotators independently label the same example as legitimately-ambiguous, that confirms the ambiguity is inherent. If one annotator labels an example as legitimately-ambiguous and others assign a definitive label, that indicates the annotator who chose legitimately-ambiguous might be under-trained or risk-averse.

Legitimately-ambiguous examples are handled differently in evaluation. You exclude them from accuracy calculations, because accuracy assumes a single correct answer. You include them in separate ambiguity-aware metrics that measure how well the model navigates ambiguous cases. For example, you might measure whether the model's output on legitimately-ambiguous examples falls within the range of human responses, or whether the model abstains from high-confidence predictions on ambiguous examples.

## When to Force a Single Label Versus Preserve Uncertainty

The decision to force annotators to choose a single label versus allowing multiple labels, confidence distributions, or legitimately-ambiguous categories depends on the downstream use of the labeled data. If the labeled data will be used to train a classifier that must produce a single prediction, forcing a single label might be appropriate. If the labeled data will be used to evaluate a system that produces confidence scores or multiple candidates, preserving uncertainty is appropriate.

You force a single label when the production system must make a binary decision and ambiguity is not actionable. For example, if you are labeling content as safe or unsafe for a content moderation system that either shows the content or blocks it, there is no middle ground in production. The system must choose. In this case, you force annotators to choose safe or unsafe even for borderline cases, and you accept that some labels will be arbitrary. You mitigate the arbitrariness by using multiple annotators and taking a majority vote, but you still produce a single label per example.

You preserve uncertainty when the production system can act on ambiguity. If the content moderation system can flag borderline content for human review instead of making an automatic decision, then labeling borderline content as legitimately-ambiguous is actionable. The system learns to route ambiguous content to human review, which is the correct behavior. In this case, preserving uncertainty in the labels improves system design.

You also preserve uncertainty when labeled data will be used for evaluation rather than training. Evaluation measures how well a system performs on real-world tasks, and real-world tasks include ambiguous cases. If you force single labels during evaluation, you create false precision in metrics. A system that predicts label A for an example where labels A and B are both valid might be scored as incorrect if the ground truth was arbitrarily set to B, even though the system's prediction was defensible. Preserving uncertainty in evaluation labels allows you to credit the system for all valid predictions.

Another factor is the cost of mislabeling. In high-stakes domains like medical diagnosis or legal compliance, forcing a single label on ambiguous examples creates risk. If an annotator is forced to label a medical image as showing disease or no disease, and the image is genuinely borderline, the forced label might be wrong. A model trained on that data might learn to be overconfident in borderline cases. Preserving uncertainty by labeling the image as legitimately-ambiguous prevents this overconfidence.

The guideline is to force single labels only when the production system requires it, the cost of mislabeling is low, and you can achieve reasonable agreement through majority vote. Otherwise, preserve uncertainty through multi-label, confidence distributions, or legitimately-ambiguous categories.

## How Ambiguity in Labeled Data Affects Downstream Evaluation

Ambiguity in labeled data changes how you design and interpret evaluation metrics. Traditional metrics like accuracy assume a single correct answer for each example. When labeled data includes ambiguity, accuracy becomes misleading. A model that achieves 85% accuracy might be performing better than a model that achieves 90% accuracy if the first model is navigating ambiguous cases more carefully and the second model is overfitting to arbitrary label choices.

When labeled data includes multi-label annotations, you replace accuracy with precision, recall, and F1 score calculated over the label set. Precision measures how many of the model's predicted labels are correct. Recall measures how many of the correct labels the model predicted. F1 score is the harmonic mean of precision and recall. These metrics handle cases where multiple labels are valid and the model predicts a subset of them. A model that predicts two of three correct labels gets partial credit, which is appropriate when all three labels are valid.

When labeled data includes confidence distributions, you replace accuracy with distributional similarity metrics like Jensen-Shannon divergence or mean squared error over the distribution. These metrics measure how close the model's predicted distribution is to the ground truth distribution. A model that predicts a distribution close to the human consensus distribution is performing well even if its single highest-probability label differs from the label that happens to have the highest probability in the ground truth distribution.

When labeled data includes legitimately-ambiguous labels, you split evaluation into two metrics: performance on definitive examples and performance on ambiguous examples. For definitive examples, you use standard accuracy or F1 score. For ambiguous examples, you measure whether the model's behavior is reasonable rather than correct. Reasonable behavior might mean the model predicts a label distribution that overlaps with the range of human annotator responses, or the model abstains from making a prediction, or the model assigns lower confidence to ambiguous examples than to definitive examples. You do not expect the model to match an arbitrary ground truth label on ambiguous examples.

You also analyze whether models are learning to identify ambiguity. If you train a model on data that includes legitimately-ambiguous labels, you can evaluate whether the model learned to recognize ambiguous examples by measuring its confidence on ambiguous versus definitive examples. A well-calibrated model should have lower confidence on examples labeled as legitimately-ambiguous. If the model has uniformly high confidence regardless of ambiguity, it has not learned to navigate uncertainty, and you need to adjust the training process to penalize overconfidence on ambiguous examples.

Ambiguity also affects how you set decision thresholds for production systems. If evaluation shows that the model performs well on definitive examples but poorly on ambiguous examples, you set thresholds that route ambiguous examples to human review. You identify ambiguous examples in production by measuring model confidence. If the model's confidence is below a threshold, you treat the example as ambiguous and route it to a human. This threshold is set based on the relationship between model confidence and ground truth ambiguity observed during evaluation.

You also track the ambiguity rate in production and compare it to the ambiguity rate in labeled evaluation data. If production has a much higher ambiguity rate, that indicates the model is encountering inputs that are more ambiguous than your evaluation data represented, and you need to expand your evaluation dataset to include more ambiguous examples. If production has a much lower ambiguity rate, that indicates your evaluation data was overly cautious in labeling examples as ambiguous, and you can refine guidelines to reduce ambiguity labels.

## Designing Annotation Workflows for Ambiguous Tasks

Annotation workflows for ambiguous tasks require different structure than workflows for definitive tasks. Standard workflows assume disagreement indicates error and resolve disagreement by finding the correct label. Ambiguity-aware workflows assume some disagreement is valid and design processes that surface and preserve that disagreement rather than suppress it.

The consensus workflow for ambiguous tasks uses multiple independent annotations followed by disagreement analysis. Three to five annotators label each example independently without seeing each other's labels. After all annotations are collected, you analyze agreement. Examples with high agreement are accepted as definitive. Examples with low agreement are reviewed to determine whether the disagreement is due to ambiguity or error. If it is due to ambiguity, you preserve the disagreement by recording all labels or aggregating them into a confidence distribution. If it is due to error, you adjudicate by having a senior annotator or domain expert choose the correct label.

The confidence-first workflow asks annotators to label examples and report their confidence in each label. You aggregate confidence scores across annotators and use the aggregated distribution as the ground truth. You do not force consensus. If three annotators assign 50%, 30%, and 20% to three different labels, the aggregated ground truth is that distribution, not a single label chosen by majority vote.

The tiered annotation workflow separates examples into definitive and ambiguous based on initial annotation. All examples are annotated by a single annotator first. If the annotator reports high confidence, the example is accepted as definitive. If the annotator reports low confidence, the example is sent to two additional annotators. If those annotators agree, the example is labeled with their consensus. If they disagree, the example is labeled as legitimately-ambiguous. This tiered approach reduces annotation cost by using multiple annotators only for ambiguous cases.

The expert adjudication workflow routes ambiguous examples to domain experts after initial annotation. Non-expert annotators label all examples first. Examples with low inter-annotator agreement are sent to a domain expert who either provides a definitive label with explanation or confirms that the example is legitimately ambiguous. The expert's judgment is recorded as ground truth. This workflow is expensive but appropriate for high-stakes tasks where ambiguity must be resolved by deep expertise.

All of these workflows require annotation platforms that support capturing and aggregating multiple labels, confidence scores, or ambiguity flags. The platform must allow annotators to assign multiple labels, enter confidence scores, or select a legitimately-ambiguous option. It must aggregate annotations and calculate agreement metrics that account for ambiguity, not just binary match or mismatch.

You are labeling AI outputs in a world where not every output has a single correct evaluation. Ambiguity is real, and designing labeling tasks that acknowledge and capture that ambiguity is not a compromise. It is realism. The next subchapter addresses another challenge that emerges when outputs are complex: how do you label long-form and multi-step outputs where quality depends on structure, coherence, and reasoning chains, not just individual elements.
