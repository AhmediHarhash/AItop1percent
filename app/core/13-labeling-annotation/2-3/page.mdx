# 2.3 â€” The Other and Unknown Problem: Policies for Catch-All Categories

In mid-2024, a customer support platform serving enterprise software companies discovered that 18% of their labeled support tickets fell into a category called "Other Issue." The company had launched an AI-powered ticket routing system six months earlier, and product managers were confused about why routing accuracy had plateaued at 76% despite adding more training data every month. When the head of machine learning finally examined the label distribution, she found that "Other Issue" had grown from 3% of labels in the first month to 18% in month six. The annotators, working under time pressure with a quota of 120 tickets per day, had learned that marking tickets as "Other Issue" was faster than debating edge cases or consulting the 47-page labeling guidelines. The routing system, trained on this data, had learned to route nearly one in five tickets to a generic queue where average resolution time was three times longer than specialized queues. Customer satisfaction scores for tickets routed to the generic queue were 2.1 out of 5, compared to 4.3 for tickets routed to specialist teams. The company had spent seven months and $340,000 building a system that was systematically hiding its own failure modes inside a catch-all category. This is what happens when you treat "Other" as a legitimate long-term label instead of a signal that your taxonomy is breaking down.

Every taxonomy needs a mechanism to handle items that do not fit cleanly into existing categories. This is not a controversial statement. The controversy emerges when organizations fail to distinguish between temporary ambiguity that should trigger taxonomy improvement and permanent ambiguity that reflects genuine edge cases. "Other" and "Unknown" are the two most common catch-all categories, and they are abused in different ways for different reasons. "Other" typically means the annotator believes the item belongs to a category that exists conceptually but is not represented in the current taxonomy. "Unknown" typically means the annotator cannot determine the correct label given the available information and context. These are fundamentally different problems requiring different policies and different operational responses. Treating them as interchangeable creates a data quality disaster that compounds over time.

## The Five Percent Rule for Other

"Other" usage above 5% to 10% of total labels signals a broken taxonomy. This threshold is not arbitrary. It emerges from operational experience across dozens of labeling programs in domains ranging from content moderation to medical imaging to financial transaction classification. When "Other" remains below 5%, it typically captures genuine long-tail edge cases: the unusual transaction that does not fit standard categories, the support ticket about a beta feature not yet in the taxonomy, the content type that appears once in a thousand examples. When "Other" rises above 10%, it almost always indicates one of three failure modes: the taxonomy is missing common categories that annotators encounter frequently, the existing category definitions are unclear and annotators default to "Other" when confused, or the annotation interface makes selecting "Other" easier than navigating a complex category tree.

The customer support platform described in the opening story exhibited all three failure modes. The taxonomy had been designed two years earlier when the company's product suite included five core modules. By 2024, the product suite had grown to twelve modules, but the support ticket taxonomy still reflected the original five-module structure. Annotators encountered tickets about newer modules daily but had no appropriate categories to assign them to. The guidelines instructed annotators to "choose the closest match or select Other if no match exists," but provided no criteria for determining closeness. Annotators working under time pressure interpreted this instruction as permission to use "Other" whenever they felt uncertain. The annotation interface displayed the five original module categories as large buttons at the top of the screen and buried "Other Issue" at the bottom of a dropdown menu, but "Other Issue" had a keyboard shortcut while navigating the full category tree required multiple clicks. Annotators learned that hitting the shortcut saved fifteen seconds per ticket, and fifteen seconds multiplied by 120 tickets per day became thirty minutes of saved time. The system had inadvertently trained annotators to optimize for speed by degrading data quality.

You prevent this failure mode by treating "Other" usage as a primary metric of taxonomy health. Your labeling dashboard should display "Other" percentage prominently alongside agreement rates and throughput. You should set explicit thresholds: if "Other" usage exceeds 5% in any given week, trigger a taxonomy review. If "Other" usage exceeds 10%, halt new annotation work until the taxonomy is repaired. These thresholds force the organization to confront taxonomy problems when they are still manageable rather than after they have corrupted months of training data. The customer support platform implemented this policy in October 2024. Within two weeks, they discovered that "Other Issue" was being used for seven distinct problem types, four of which affected more than 3% of tickets each. They added four new top-level categories, split one existing category into three subcategories, and clarified the definitions of two ambiguous categories. "Other" usage dropped from 18% to 4% within one month, and routing accuracy improved from 76% to 89%.

## The Distinction Between Other and Unknown

"Other" and "Unknown" are not synonyms, and conflating them creates operational confusion. "Other" signals a gap in the taxonomy. "Unknown" signals a gap in the information available to the annotator. Consider a content moderation scenario where annotators review flagged social media posts. An "Other" label means the annotator believes the post violates community standards in a way not covered by existing violation categories: perhaps it is a new type of coordinated inauthentic behavior, or a novel form of manipulated media, or a harassment tactic that does not match existing harassment definitions. An "Unknown" label means the annotator cannot determine whether the post violates standards at all because critical context is missing: the post contains text in a language the annotator does not speak, or it references events the annotator is unfamiliar with, or it includes images that have been deleted and are no longer visible.

The operational response to "Other" is taxonomy expansion. The operational response to "Unknown" is improved tooling or escalation pathways. When a healthcare AI company building a clinical decision support system found that 12% of their labeled medical imaging cases were marked "Unknown," they initially assumed the taxonomy was incomplete and began designing new diagnostic categories. Three months of effort produced no improvement. A retrospective analysis revealed that "Unknown" was being used in two distinct ways: 8% of cases were marked "Unknown" because the image quality was too poor to support confident diagnosis, and 4% were marked "Unknown" because the radiologist annotators encountered rare conditions they were not trained to recognize. The first problem required better image acquisition protocols and quality filters. The second problem required an escalation pathway to subspecialist reviewers. Neither problem was solvable by adding taxonomy categories. The company had wasted three months solving the wrong problem because they treated "Unknown" as a synonym for "Other."

You solve this by making "Other" and "Unknown" functionally distinct in your labeling system. "Other" should require the annotator to provide a free-text description of what category the item actually belongs to. This free-text field becomes the primary input for taxonomy mining: you aggregate "Other" justifications weekly, cluster them by theme, and use the clusters to identify candidate new categories. "Unknown" should require the annotator to specify what information is missing: insufficient image quality, language barrier, missing context, ambiguous phrasing, or other enumerated reasons. This structured metadata allows you to route "Unknown" items to appropriate remediation pathways. Poor quality items get filtered out. Language barrier items get routed to annotators with appropriate language skills. Missing context items trigger requests for additional information from upstream systems. You cannot build these operational feedback loops if "Other" and "Unknown" are treated as interchangeable dumping grounds.

## Policies for When to Allow Other

Not all labeling tasks should permit "Other" as a valid label. The decision depends on whether the taxonomy is intended to be exhaustive or representative. An exhaustive taxonomy claims to cover all possible items in the domain. A representative taxonomy covers the most common items but acknowledges that long-tail cases exist. Content moderation taxonomies are typically exhaustive: every piece of content is either violating or not violating, and every violation must map to a specific policy. Transaction classification taxonomies are often representative: the taxonomy covers the 95% of transactions that fit standard patterns, and the remaining 5% are handled separately.

If your taxonomy is intended to be exhaustive, "Other" should not exist as a permanent label. Its presence indicates taxonomy incompleteness. You allow "Other" during the initial taxonomy development phase, typically the first four to eight weeks of a new labeling program, as a mechanism to discover gaps. You mine "Other" examples aggressively, add new categories to fill gaps, and aim to reduce "Other" to zero within the development period. If "Other" persists beyond the development period, you have failed to build an exhaustive taxonomy and need to revisit your category definitions. A financial services company building a transaction monitoring system for anti-money laundering used this approach in late 2024. They launched their labeling program with 23 transaction type categories and allowed annotators to use "Other" for transactions that did not fit. In the first month, 14% of transactions were labeled "Other." They mined those examples weekly, identified eight new transaction patterns, and added corresponding categories. By month three, "Other" usage was 0.7%, representing genuinely anomalous transactions that occurred less than once per thousand. They removed "Other" as an available label and required annotators to escalate truly anomalous transactions to senior analysts for individual review.

If your taxonomy is intended to be representative, "Other" serves a permanent role but must be carefully governed. You set an acceptable threshold, typically 3% to 5% of total labels, and monitor it continuously. You require free-text justification for every "Other" label to enable taxonomy mining. You establish a regular cadence, typically monthly or quarterly, for reviewing accumulated "Other" justifications and deciding which patterns warrant new categories. You communicate clearly to downstream consumers of the labeled data that "Other" represents valid data for model training: models should learn to recognize items that do not fit standard patterns and route them appropriately rather than forcing them into incorrect categories. A customer service automation platform took this approach in 2025. Their taxonomy covered the 30 most common customer inquiry types, which accounted for roughly 94% of actual inquiries. They allowed "Other" for the remaining 6%, required annotators to write a one-sentence explanation, and reviewed "Other" justifications quarterly. Over twelve months, they added four new inquiry types based on patterns that emerged from "Other" mining, while accepting that 2% to 3% of inquiries would always fall outside standard categories due to product complexity and customer creativity.

## The Operational Feedback Loop for Mining Other

"Other" is only useful if you treat it as a source of continuous taxonomy improvement. This requires an operational feedback loop with four components: collection, aggregation, analysis, and action. Collection happens at annotation time when annotators select "Other" and provide free-text justification. Aggregation happens weekly or biweekly when a taxonomy manager exports all "Other" examples and their justifications into a structured review format. Analysis happens when the taxonomy manager manually clusters justifications by theme, identifies patterns that appear in more than 1% to 2% of "Other" examples, and drafts candidate category definitions. Action happens when the taxonomy governance board reviews the proposed new categories, approves changes that meet the governance criteria covered in the previous subchapter, and schedules their addition to the taxonomy.

The customer support platform that discovered 18% "Other" usage implemented this loop in October 2024. They exported all "Other" examples from the previous three months, which totaled 6,400 tickets. The taxonomy manager spent two days reading through justifications and manually clustering them into themes. She identified seven clusters, each representing 4% to 12% of "Other" examples. The largest cluster, representing 12% of "Other" examples or roughly 2% of all tickets, involved questions about a new analytics dashboard launched in July. The taxonomy had no category for analytics-related inquiries because analytics had not been a significant product area when the taxonomy was originally designed. She drafted a new top-level category called "Analytics and Reporting" with four subcategories covering common question patterns. The taxonomy governance board reviewed the proposal, approved it with minor definition clarifications, and added it to the production taxonomy. Annotators began using the new category immediately. "Other" usage dropped by 12 percentage points in the first week after the change.

This loop only works if the aggregation and analysis steps happen frequently enough to prevent "Other" from becoming a permanent dumping ground. Weekly aggregation is appropriate for high-volume labeling programs processing thousands of items per day. Monthly aggregation is appropriate for medium-volume programs processing hundreds of items per day. If you aggregate less frequently than monthly, patterns hide in the accumulated backlog and annotators lose confidence that "Other" feedback actually influences taxonomy improvement. A legal technology company building a contract clause classification system made this mistake in early 2025. They collected "Other" justifications but only reviewed them quarterly. By the end of the first quarter, they had accumulated 1,800 "Other" examples representing 9% of all labeled clauses. The taxonomy manager spent a week trying to cluster the justifications but found the volume overwhelming and the patterns unclear because three months of justifications mixed seasonal variations, product changes, and genuine taxonomy gaps. She proposed three new categories based on her best judgment, but downstream analysis revealed that two of the three categories were rarely used after launch because they did not actually match common patterns. The company switched to weekly aggregation in the second quarter, which reduced the per-review volume from 1,800 examples to roughly 140 examples and made pattern identification straightforward.

## Tracking Other Rates as a Taxonomy Health Metric

"Other" percentage is one of the most informative single metrics of taxonomy quality. It tells you whether your taxonomy is keeping pace with the reality of the data you are labeling. A stable "Other" rate below 5% indicates a healthy taxonomy that covers the vast majority of real-world cases while acknowledging that edge cases exist. A rising "Other" rate indicates taxonomy drift: the data is changing but the taxonomy is not adapting. A falling "Other" rate after taxonomy changes indicates successful gap closure. A suddenly spiking "Other" rate indicates a discrete event: a new product feature launched, a major news event created new content patterns, or a change in data sourcing introduced a new population of items.

You should track "Other" rates at multiple levels of granularity. At the aggregate level, track the overall percentage of all labeled items marked "Other" and plot it over time as a weekly or daily time series. At the category level, if your taxonomy is hierarchical, track "Other" separately for each top-level category to identify which parts of the taxonomy are struggling. At the annotator level, track "Other" usage by individual annotator to identify annotators who overuse or underuse "Other" relative to their peers. Overuse suggests the annotator may need additional training on category definitions. Underuse suggests the annotator may be forcing ambiguous items into incorrect categories rather than marking them as "Other" appropriately.

A content moderation system for a video platform tracked all three levels in 2025. Aggregate "Other" usage hovered around 3% for the first six months of the year. In July, it spiked to 11% over a two-week period. The taxonomy manager drilled into category-level data and found that the spike was isolated to the harassment and bullying category, where "Other" usage had jumped from 2% to 34%. She drilled into annotator-level data and confirmed that the spike was consistent across all annotators, ruling out a training issue. She reviewed "Other" justifications from the spike period and found that 80% referenced a new form of harassment involving AI-generated voice clips of public figures saying offensive things. This content pattern had not existed when the taxonomy was designed and did not fit cleanly into existing harassment subcategories because it combined elements of manipulated media, impersonation, and targeted harassment. The taxonomy governance board convened an emergency review, created a new subcategory for synthetic media harassment, and published updated guidelines within five days. "Other" usage in the harassment category dropped back to 3% within one week of the taxonomy update. The ability to detect and respond to this pattern within two weeks prevented months of mislabeled data and model degradation.

## The Danger of Ignoring Other

Ignoring "Other" is professional negligence. It is not a harsh judgment. It is an accurate description of the failure mode. When "Other" usage rises above 10% and no one investigates, you are systematically discarding 10% or more of your labeling budget on data that provides almost no value to downstream systems. Models trained on this data learn nothing useful from "Other" examples because "Other" is a semantic void: it tells the model what the item is not but provides no information about what it is. Evaluation pipelines that exclude "Other" examples produce inflated accuracy metrics that do not reflect real-world performance. Evaluation pipelines that include "Other" examples treat a heterogeneous collection of unrelated edge cases as if they were a coherent category, which distorts precision and recall calculations.

The customer support platform in the opening story exemplified this negligence. For seven months, 18% of their labeling budget produced unusable data. At a labeling cost of $0.80 per ticket and a volume of 30,000 tickets per month, they spent $4,320 per month or $30,240 over seven months labeling tickets as "Other Issue" in ways that provided no training signal to their routing model. The routing model, unable to learn meaningful patterns from "Other Issue" examples, defaulted to routing them to a generic queue. The generic queue had the longest wait times and the least specialized agents, which meant that 18% of customers received the worst possible service. Customer satisfaction scores for these tickets averaged 2.1 out of 5, and 34% of customers who received "Other Issue" routing cancelled their contracts within six months compared to a baseline cancellation rate of 8%. The seven-month delay in addressing "Other" usage cost the company an estimated $2.3 million in lost revenue from cancellations, not counting the wasted labeling spend or the opportunity cost of delayed model improvement.

You prevent this by making "Other" visibility mandatory in every labeling program review. Weekly annotation quality reviews should include "Other" rates alongside agreement rates and throughput. Monthly program health reviews should include trend analysis of "Other" usage and a summary of actions taken to address gaps. Quarterly taxonomy governance reviews should include a retrospective analysis of how effectively "Other" mining fed back into taxonomy improvements. If any of these reviews cannot answer the question "what did we learn from Other examples this period and what did we do about it," the labeling program is not operating at professional standards. This level of rigor is not optional for production AI systems in 2026. The stakes are too high and the resources too expensive to tolerate systematic data quality failures hiding in catch-all categories.

The next subchapter examines schema change rules: when you can rename, merge, or split labels without breaking historical comparability and invalidating downstream systems.
