# 2.7 â€” Ontology Review Cadence: Governance Meetings and Decision Records

In June 2025, a customer support automation company discovered that their intent classification ontology had grown from 47 categories to 183 categories over eighteen months, with no one able to explain why most of the new categories existed. The labeling team's inter-annotator agreement had dropped from 0.87 to 0.61. The ML team complained that half the categories had fewer than fifty training examples. The product team was frustrated that new intent types took weeks to add while unnecessary categories cluttered the interface. When the VP of Engineering demanded an explanation, no one could provide one. There was no record of who approved the category additions, why they were needed, or what problem they solved. The ontology had evolved through hundreds of informal Slack messages, ad hoc conversations, and unilateral decisions by individual annotators who found edge cases they didn't know how to label. The company spent four months unwinding the mess, consolidating categories, relabeling thousands of examples, and retraining models. The cost exceeded four hundred thousand dollars in labor and opportunity cost. The root cause was not technical complexity. It was the absence of governance. The ontology had no owner, no review process, no decision record, and no accountability structure.

Label ontologies require ongoing governance. They don't design themselves and they don't maintain themselves. An ontology is a shared contract between product requirements, annotator capabilities, model training needs, and downstream system expectations. That contract changes as products evolve, as annotators surface ambiguities, as models reveal performance gaps, and as business priorities shift. Without explicit governance, ontologies drift into incoherence. Categories multiply without justification. Definitions blur without correction. Inconsistencies accumulate without resolution. The ontology becomes a liability instead of an asset.

## The Governance Structure: Who Owns the Ontology

Ontology ownership is a cross-functional responsibility, not a single-team burden. The typical governance structure is a standing committee that includes product management, engineering, domain experts, and labeling operations. Product management represents the business requirements and user-facing behavior. Engineering represents the model training needs and technical constraints. Domain experts represent the subject matter knowledge and definitional authority. Labeling operations represents the annotator experience and practical feasibility. This is not a ceremonial advisory board. This is the decision-making body with explicit authority to approve, reject, or modify ontology changes.

The committee needs a chair who owns the process, drives decisions, and breaks ties. In most organizations, this role sits with product management because the ontology exists to serve product requirements. In heavily regulated domains like healthcare or finance, the chair may be a domain expert who ensures compliance and accuracy. In research organizations building foundation models, the chair may be an ML research lead who prioritizes training data quality. The chair is not a dictator who makes unilateral decisions. The chair is a facilitator who ensures the committee makes informed, documented, and timely decisions.

The committee needs clear decision rights. It approves new categories, retires obsolete categories, modifies definitions, adjusts guidelines, and prioritizes schema changes. It does not design the entire ontology from scratch in every meeting. It reviews proposed changes, evaluates evidence, considers alternatives, and makes incremental improvements. The committee operates on proposals, not open-ended brainstorming. Someone brings a concrete recommendation with supporting data, and the committee approves, rejects, or requests modifications.

## The Review Cadence: Monthly for Stable Products, Weekly During Rapid Iteration

The review frequency depends on product maturity and change velocity. For stable products with well-established ontologies, monthly reviews are sufficient. The ontology has converged, annotator agreement is high, and changes are rare refinements rather than fundamental restructuring. Monthly reviews cover agreement metrics, annotator feedback, edge case accumulation, and downstream consumer requests. The meeting is typically one hour, attended by the core governance committee, with pre-circulated data and proposals.

For products in rapid iteration, weekly reviews are necessary. The ontology is still forming, new edge cases appear daily, annotators surface ambiguities constantly, and product requirements evolve with each sprint. Weekly reviews ensure the ontology keeps pace with product development without creating a backlog of unresolved questions. The meeting is typically thirty to forty-five minutes, focused on high-impact decisions that unblock labeling or training. The committee defers lower-priority refinements to a monthly deep-dive session.

For brand-new products or major ontology redesigns, daily standups may be required for the first few weeks. The initial ontology design is a hypothesis that requires rapid validation against real labeling data. Annotators encounter ambiguities immediately, categories prove too broad or too narrow, definitions conflict with edge cases, and the entire structure may need adjustment. Daily standups allow the committee to make fast corrections before thousands of examples are labeled incorrectly. Once the ontology stabilizes, the cadence steps down to weekly, then monthly.

The cadence is not arbitrary tradition. It is a forcing function that prevents ontology drift. If reviews happen quarterly or ad hoc, changes accumulate into an overwhelming backlog, decisions get made informally without documentation, and the ontology diverges from its intended design. Monthly reviews create a predictable rhythm where proposed changes are batched, evaluated systematically, and decided with appropriate rigor. Weekly reviews during high-velocity periods prevent the ontology from becoming a bottleneck that blocks progress.

## What the Committee Reviews: Agreement Metrics, Other Rates, Annotator Feedback, Schema Change Requests, Downstream Consumer Needs

Every governance meeting starts with quantitative health metrics. Inter-annotator agreement by category reveals which parts of the ontology are clear and which are ambiguous. Categories with agreement below 0.70 require immediate attention. They either have vague definitions, overlapping boundaries, or insufficient examples in the guidelines. The committee examines the confusion matrix to understand which categories are most often confused with each other, then decides whether to clarify the distinction, merge the categories, or add differentiation examples to the guidelines.

The rate of "Other" or "Unclear" labels is the second critical metric. A high "Other" rate indicates the ontology is missing important categories or forcing annotators into false choices. The committee reviews a sample of "Other" examples to identify patterns. If fifty examples all involve a specific scenario not covered by existing categories, that scenario may warrant a new category. If the "Other" examples are scattered across unrelated edge cases, the ontology is probably complete and the high "Other" rate reflects inherent noise in the data. The committee decides whether to expand the ontology, improve annotator training, or accept the noise level.

Annotator feedback is the qualitative counterpart to quantitative metrics. Annotators are the users of the ontology, and they encounter ambiguities, edge cases, and practical difficulties that metrics alone don't reveal. The labeling operations lead brings a summary of recurring annotator questions, requests for clarification, and suggested improvements. The committee considers this feedback seriously. Annotators have the most direct experience with the ontology's strengths and weaknesses. Ignoring their input is professional negligence.

Schema change requests come from multiple sources. Product management requests new categories to support new features or business priorities. ML engineering requests category consolidation to improve training data balance or model performance. Domain experts request definition refinements to align with evolving subject matter standards. Downstream consumers request schema modifications to match their integration requirements. The committee reviews each request against criteria including business value, labeling feasibility, impact on existing data, migration cost, and alignment with ontology design principles. Not every request is approved. The committee's job is to balance competing needs and maintain ontology coherence.

Downstream consumer needs are often neglected in governance reviews, which is a mistake. The ontology exists to produce labels that other systems consume. If the fraud detection model needs fine-grained fraud type classifications but the ontology only provides binary fraud or not-fraud labels, the ontology is failing its purpose. The committee solicits regular feedback from model training teams, product analytics teams, business intelligence teams, and any other consumers of labeled data. This feedback loop ensures the ontology evolves to serve its actual users, not just the theoretical requirements from the original design.

## The Decision Record Format: Rationale, Alternatives Considered, Migration Plan, Expected Impact

Every ontology change requires a decision record. This is not bureaucratic overhead. This is institutional memory that prevents repeated mistakes, enables future teams to understand past choices, and creates accountability for governance decisions. The decision record is a structured document, typically one to three pages, that captures the context and reasoning behind each approved change.

The decision record starts with the problem statement. What issue is this change addressing? Is it low inter-annotator agreement on a specific category? Is it a product requirement for new functionality? Is it a model performance gap? Is it annotator frustration with an ambiguous definition? The problem statement grounds the decision in evidence, not opinion. It prevents the committee from making changes based on someone's personal preference or an isolated edge case.

The rationale explains why the proposed solution addresses the problem. If the change is adding a new category, the rationale explains what gap it fills, what examples it will cover, and why existing categories are insufficient. If the change is merging two categories, the rationale explains why the distinction is not reliably annotatable or not valuable for downstream use. If the change is clarifying a definition, the rationale explains what ambiguity existed and how the new definition resolves it. The rationale is specific and evidence-based, referencing agreement metrics, annotator feedback, or product requirements.

The alternatives considered section documents what other solutions were evaluated and why they were rejected. This prevents future committees from revisiting the same options without understanding why they were previously dismissed. It also demonstrates that the decision was deliberate, not impulsive. If the committee considered adding a new category, splitting an existing category, and clarifying guidelines, but chose to clarify guidelines, the decision record explains why that option was superior. The alternatives section is brief but substantive, typically two to four sentences per alternative.

The migration plan specifies how the change will be implemented. For new categories, the migration plan includes when the category goes live, how existing "Other" examples will be reviewed and relabeled, and how annotators will be trained. For category mergers, the migration plan includes which category survives, how examples from the deprecated category will be remapped, and whether any data needs manual review. For definition changes, the migration plan includes whether existing examples need relabeling or can be grandfathered. The migration plan also specifies who is responsible for each step and what the timeline is.

The expected impact section quantifies the anticipated effects. If adding a new category is expected to reduce "Other" rates from twelve percent to five percent, state that. If merging two categories is expected to increase inter-annotator agreement from 0.68 to 0.82, state that. If clarifying a definition is expected to eliminate eighty percent of annotator questions about that category, state that. These are estimates, not guarantees, but they make the decision accountable. Six months later, the committee can review whether the change achieved its expected impact. If not, the committee can investigate why and adjust accordingly.

## The Anti-Pattern: Ontology by Committee Drift

The most common governance failure is ontology by committee drift, where changes happen informally through Slack messages, hallway conversations, and ad hoc decisions, leaving no record and no accountability. An annotator asks a question in Slack about how to label an edge case. A domain expert responds with an interpretation. The annotator applies that interpretation going forward. Other annotators never see the conversation and continue labeling the same edge case differently. The domain expert's interpretation was never reviewed by the full committee, never documented in the guidelines, and never validated against product requirements or model training needs. The ontology has changed, but only in the minds of the people who saw that Slack thread.

This pattern scales disastrously. Over time, the ontology exists in multiple inconsistent versions: the official guidelines document, the implicit knowledge in annotators' heads, the assumptions in the model training code, and the expectations of downstream consumers. No one has a complete or accurate picture. Disagreements arise, and there's no authoritative source to resolve them. New team members onboard to a chaotic mess of conflicting information. The ontology has no integrity.

Preventing this anti-pattern requires discipline. All ontology questions and change requests must go through the formal governance process. If an annotator encounters an ambiguity, they flag it in the labeling tool or dedicated channel, and the labeling operations lead brings it to the next governance meeting. If a domain expert wants to refine a definition, they submit a formal proposal with rationale and examples, and the committee reviews it. If a product manager wants a new category, they present the business case and the committee evaluates feasibility. No changes bypass the process, no matter how small or obvious they seem. The committee can approve trivial changes in thirty seconds, but they still go through the process and get documented.

The decision record is the enforcement mechanism. If a change is not documented, it is not official. If an annotator says they were told to interpret a category a certain way, but there's no decision record, that interpretation is not authoritative. The guidelines document is updated to reflect every approved change, and the decision record is linked from the guidelines. Anyone who wants to understand why the ontology is designed a certain way can trace back through decision records to see the reasoning.

## How Governance Scales: Single-Product Teams vs Multi-Product Enterprises with Shared Ontologies

For single-product teams, governance is straightforward. The committee is small, typically four to six people, representing the disciplines involved in that product. Meetings are focused and efficient because everyone understands the product context. Decisions can be made quickly because the scope is bounded. The decision record repository is a shared folder or wiki page, organized chronologically or by category. Overhead is minimal and value is immediate.

For multi-product enterprises with shared ontologies, governance becomes more complex. Multiple products may share a core ontology but extend it with product-specific categories. The governance committee needs representatives from each product, which can balloon to fifteen or twenty people. Meetings become unwieldy, decisions slow down, and consensus becomes difficult. The naive approach is to have a single mega-committee that reviews all changes for all products. This fails because most changes are product-specific and irrelevant to other products. The meeting becomes a waste of time for most attendees, and participation drops.

The scalable approach is federated governance. There is a central ontology governance committee responsible for the shared core ontology, and each product has a local governance committee responsible for product-specific extensions. The central committee meets monthly and includes senior representatives from each product plus cross-cutting roles like the head of labeling operations, the chief domain expert, and the ML platform lead. The local committees meet weekly or biweekly and include the product manager, engineering lead, and labeling operations lead for that specific product. Changes to the shared core ontology go through the central committee. Changes to product-specific extensions go through the local committee.

The central committee's role is to maintain coherence and prevent fragmentation. If two products both want to add a category that serves similar purposes, the central committee evaluates whether it should be added to the shared core ontology instead of duplicated in each product's extensions. If a product wants to modify a shared category definition, the central committee evaluates the impact on other products and decides whether the change is broadly beneficial or should be isolated to that product's extension. The central committee also enforces ontology design principles across products, ensuring consistency in category granularity, definition structure, and guideline format.

The local committees operate with autonomy within the boundaries set by the central committee. They can add product-specific categories, refine product-specific definitions, and adjust product-specific guidelines without central approval. They document their decisions in the same format as the central committee, maintaining a local decision record repository. The local committee chair attends central committee meetings to represent their product's needs and to stay informed of changes to the shared core ontology.

This federated structure scales to dozens of products without creating bottlenecks or wasting time. Products move at their own pace on product-specific issues while maintaining alignment on shared issues. The central committee's monthly cadence is appropriate for the slower-changing shared core, while local committees' weekly or biweekly cadence is appropriate for faster-moving product-specific needs. Decision records from both central and local committees are stored in a unified repository, searchable and cross-referenced, so anyone can trace the full history of why the ontology is structured the way it is.

## Governance as Continuous Improvement, Not Bureaucratic Gate

Effective ontology governance is a continuous improvement process, not a bureaucratic gate that slows down work. The committee exists to make the ontology better over time, not to prevent changes or defend the status quo. The default posture is openness to well-justified changes, not resistance. If a proposed change has clear rationale, supporting evidence, and a reasonable migration plan, the committee approves it. If the proposal is vague or poorly justified, the committee requests more information, not as a power play but as a quality standard.

The committee's value is visible in the ontology's stability and clarity. Inter-annotator agreement remains high or improves over time. Annotators report confidence in how to apply the ontology. Model training teams have the labeled data they need in the structure they need. Product teams can add new capabilities without redesigning the entire ontology. Downstream consumers trust the labels they receive. None of this happens by accident. It happens because the governance committee makes deliberate, evidence-based, documented decisions on a predictable cadence.

The governance meeting itself should be efficient and focused. Pre-circulated materials include the current health metrics, the list of proposed changes with supporting rationale, and any new annotator feedback or downstream consumer requests. The meeting follows a standing agenda: review metrics, discuss high-priority proposals, make decisions, assign follow-up actions. Decisions are documented in real time, either in a shared note-taking document or by a designated scribe. Within twenty-four hours of the meeting, decision records are finalized and published, and the guidelines document is updated to reflect approved changes. This cadence keeps the ontology aligned with product needs, annotator capabilities, and model requirements, ensuring the labeling operation remains a strategic asset rather than devolving into an expensive mess.

With governance structures and decision processes established, the ontology itself still needs to be communicated to annotators through detailed annotation guidelines that translate abstract category definitions into concrete labeling instructions.
