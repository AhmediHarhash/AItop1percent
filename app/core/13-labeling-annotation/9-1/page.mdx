# 9.1 â€” The True Cost of Labeling: Direct, Indirect, and Hidden

In July 2025, a healthcare technology company launched an AI-powered medical imaging analysis system with an initial labeling budget of $180,000. The product team calculated this figure carefully: 60,000 images at $3 per image for radiologist review. The CFO approved the budget, engineering began building the annotation platform, and the clinical team started recruiting radiologists. Nine months later, the project had consumed $847,000 in labeling-related expenses and still had not reached the minimum viable dataset size. The finance team demanded an audit. What they discovered shocked everyone: the $3-per-image estimate captured less than twenty-two percent of the true cost of their labeling program.

The direct annotation cost was indeed $3 per image, paid to radiologists through a medical labeling vendor. But the company had not budgeted for the annotation platform license at $4,200 per month, the two full-time clinical coordinators managing radiologist schedules and questions at $140,000 each annually, the quality assurance program that required twenty percent of labels to be double-checked by a senior radiologist at $8 per image, the three rounds of rework when the initial annotation guidelines proved insufficient, the storage costs for 60,000 high-resolution DICOM images and their annotation metadata, the engineering time spent building custom annotation tools when the vendor platform could not handle multi-region annotations, or the six-week delay that pushed the product launch back and cost the company an estimated $200,000 in lost market opportunity. The true cost per labeled image was not $3. It was $14.12. The company had fundamentally misunderstood what labeling actually costs.

This is not an isolated failure. This is the standard pattern across the industry in 2026. Teams budget for annotator wages and nothing else. They treat labeling as a simple procurement problem: find vendors, negotiate rates, multiply by volume. Then they encounter the reality that labeling is an operation, not a transaction. Operations have infrastructure costs, coordination costs, quality costs, rework costs, and opportunity costs. Every labeling program generates these costs. Most teams never see them coming.

## Direct Costs: What You Actually Budget For

Direct costs are what most teams think of as the entire labeling budget. These are the line items that appear in vendor quotes and procurement approvals. For most programs, direct costs break into three categories: annotator time, platform fees, and data handling.

Annotator time is the obvious cost. You pay people to look at data and create labels. In 2026, rates vary wildly by domain and geography. Commodity labeling for image classification runs $0.50 to $2.00 per image through offshore vendors. Domain-expert labeling runs $5 to $50 per item depending on expertise required. Medical imaging reviewed by radiologists costs $8 to $25 per image. Legal document review by attorneys costs $40 to $150 per document. Financial transaction labeling by certified fraud analysts costs $3 to $12 per transaction. Your direct annotator cost is determined by the intersection of three factors: task complexity, domain expertise required, and labor market you source from.

Task complexity drives time per label. Simple binary classification takes fifteen to forty-five seconds per item. Multi-class categorization takes one to three minutes. Bounding box annotation takes two to eight minutes per image depending on object count. Polygon segmentation takes five to twenty minutes. Document span annotation takes three to twelve minutes per page. Conversational quality rating takes four to ten minutes per dialogue. The time required determines the maximum throughput per annotator and therefore the cost. A radiologist billing at $180 per hour who spends eight minutes reviewing an image costs you $24 in direct labor. A content moderator billing at $18 per hour who spends two minutes reviewing a post costs you $0.60. Time is the direct multiplier.

Platform fees are the second direct cost. If you use a commercial annotation platform, you pay subscription fees, per-label fees, or both. Subscription models range from $500 per month for basic tools supporting small teams to $15,000 per month for enterprise platforms with advanced workflow management, quality control, and integrations. Per-label fees typically run $0.05 to $0.50 per label depending on task type and volume. Some platforms charge storage fees for media assets. Some charge API fees for programmatic access. Some charge user seat fees for your internal team members who manage the program. Your total platform cost is the sum of all these fees across the duration of your labeling program. A six-month program using a $3,000 per month platform with $0.10 per label fees on 50,000 labels costs $18,000 in subscription fees plus $5,000 in per-label fees for a total platform cost of $23,000.

Data handling costs are the third direct expense. Labeling requires moving data: from your production systems to the annotation platform, from the platform to annotators, from annotators back to the platform, from the platform to your training infrastructure. For text data, transfer and storage costs are negligible. For image data, costs become meaningful. For video data, costs become substantial. A medical imaging program labeling 60,000 DICOM images at an average size of 12 megabytes per image is handling 720 gigabytes of data. Storing this in cloud object storage costs roughly $15 per month. Transferring it costs $0.09 per gigabyte egress, so extracting the full dataset costs about $65. For video programs, multiply these costs by ten to one hundred. A content moderation program reviewing 10,000 video clips averaging 45 seconds at 1080p resolution handles approximately 80 gigabytes of data, costing $200 to $400 in transfer fees over the program lifetime if you move data multiple times for different annotation passes.

These three categories constitute your direct costs. They appear in purchase orders and invoices. They are legible to finance teams. They are what you budget for. And they represent, on average, forty to sixty percent of your true labeling costs.

## Indirect Costs: The Operational Overhead You Did Not Plan For

Indirect costs are the expenses required to operate a labeling program but not directly tied to producing individual labels. These costs appear in other budgets or are absorbed by existing teams, making them invisible during initial budgeting. But they are real, they are substantial, and they will consume resources whether you plan for them or not.

Quality assurance is the largest indirect cost. Every labeling program needs quality control. Someone must review samples of annotated data to verify correctness, identify systematic errors, and provide feedback to annotators. For low-stakes programs, you might sample five percent of labels and have a senior annotator review them at the same cost as initial annotation. For high-stakes programs, you might double-label twenty to fifty percent of data and have expert reviewers adjudicate disagreements at twice the cost of initial annotation. Your QA cost is a function of your sampling rate and your review cost. A program producing 100,000 labels at $2 per label with a ten percent QA sampling rate and $4 per label review cost spends $40,000 on quality assurance in addition to the $200,000 in direct annotation costs.

Program management is the second major indirect cost. Labeling programs require coordination. Someone must recruit and onboard annotators, answer questions about edge cases, refine guidelines based on observed errors, track progress against milestones, manage vendor relationships, and escalate blockers. For small programs labeling a few thousand items, this is a part-time responsibility for a data scientist or product manager. For large programs labeling hundreds of thousands of items, this is a full-time role or multiple full-time roles. The healthcare imaging company that budgeted $180,000 for labeling employed two full-time clinical coordinators at $140,000 each annually to manage radiologist schedules, triage questions about imaging artifacts, and coordinate with the engineering team building custom tools. These $280,000 in salaries never appeared in the labeling budget because they were hired through HR and allocated to headcount, not project expenses. But they were labeling costs.

Guideline development is the third indirect expense. Annotation guidelines do not appear fully formed. Someone must write them, test them with real data, identify ambiguities, resolve disagreements, document edge cases, and update them as annotators encounter new scenarios. For straightforward tasks, guideline development takes one person two to five days. For complex tasks, guideline development takes a small team two to six weeks. A legal document labeling program might require attorneys to spend forty hours defining what constitutes personally identifiable information across fifteen document types, writing examples for each category, and creating decision trees for ambiguous cases. At $200 per hour, that is $8,000 in guideline development costs before the first document is labeled.

Training and onboarding constitute the fourth category. New annotators need training before they produce acceptable labels. You must create training materials, conduct onboarding sessions, provide practice sets with gold-standard answers, review initial work closely, and give feedback until performance stabilizes. For simple tasks, onboarding takes two to four hours per annotator. For complex tasks, onboarding takes two to five days per annotator. Your training cost is the time spent by trainers plus the time spent by annotators during training plus the cost of discarded low-quality labels produced during the learning period. A content moderation program onboarding twenty moderators with eight hours of training each at $18 per hour costs $2,880 in annotator time during training. If a senior moderator spends four hours training each cohort of five annotators, that is sixteen additional hours at $28 per hour for $448 in trainer time. Total onboarding cost: $3,328 for a team that will produce 200,000 labels.

Tooling and infrastructure represent the fifth indirect cost. Most annotation platforms require customization. You might build custom data loaders, custom annotation interfaces for domain-specific tasks, custom quality dashboards, custom reporting tools, or custom integrations with your production systems. Engineering time spent building these tools is a labeling cost even though it appears in the engineering budget. The healthcare imaging company spent twelve weeks of engineering time building custom multi-region annotation tools because their vendor platform could not support the clinical workflow. At a loaded cost of $85 per hour, twelve weeks of one engineer's time costs approximately $40,800. This cost never appeared in the labeling budget.

Coordination and communication are the sixth category. Labeling programs generate meetings, email threads, Slack conversations, documentation, status updates, and escalations. Product managers spend time clarifying requirements. Domain experts spend time answering annotator questions. Engineering teams spend time investigating data quality issues. These fragments of time scattered across many people sum to meaningful costs. A program running six months might generate ten hours per week of coordination time across a team of eight people. That is 240 person-hours at an average loaded cost of $75 per hour for $18,000 in coordination costs.

Your total indirect costs typically range from thirty to seventy percent of your direct costs, depending on task complexity and organizational maturity. Teams budgeting only for direct annotation routinely underestimate total program cost by fifty to one hundred fifty percent.

## Hidden Costs: The Expenses That Only Appear Later

Hidden costs are the consequences of labeling decisions that manifest as expenses elsewhere in the organization or at a later time. These costs are nearly impossible to budget for because they depend on execution quality and downstream impacts. But they are often the largest costs of all.

Rework is the most common hidden cost. Your initial annotation guidelines will be incomplete. Annotators will make systematic errors. You will discover these errors during model training when performance is unacceptably poor or during quality audits when you review samples. At that point, you must re-label the affected data. Rework costs the original annotation cost plus coordination overhead plus delay. A sentiment analysis program that labeled 40,000 customer reviews at $0.80 per review spent $32,000 on initial annotation. During model training, the team discovered that annotators had inconsistently labeled sarcasm, creating a systematic bias. They re-labeled 12,000 reviews at the same cost, adding $9,600 plus two weeks of delay. The total cost of the sarcasm error was $9,600 in direct rework plus approximately $15,000 in opportunity cost from the two-week delay. Hidden rework cost: $24,600 on top of the original $32,000 budget.

Opportunity cost from delays is the second hidden expense. Labeling takes time. If labeling takes longer than planned, your product launch is delayed. Delayed launches mean delayed revenue, lost market opportunities, or competitive disadvantage. The healthcare imaging company's labeling program took nine months instead of the planned six months because of three rounds of guideline revisions and rework. The three-month delay pushed their product launch from Q4 2025 into Q1 2026, missing the critical year-end budget cycle when hospitals make purchasing decisions. The company estimated this delay cost them $200,000 in lost first-year revenue. This opportunity cost dwarfed the $847,000 in direct and indirect labeling expenses.

Technical debt from inadequate labels is the third category. Poor label quality forces engineering teams to build workarounds: more complex model architectures, additional training data filtering, ensemble methods to compensate for noisy labels, or extensive post-processing. These workarounds increase model complexity, slow inference, raise compute costs, and make the system harder to maintain. A fraud detection system trained on inconsistently labeled data required an ensemble of five models instead of one to achieve acceptable precision. The ensemble increased inference latency from 120 milliseconds to 580 milliseconds and quintupled compute costs. Over two years of operation, the additional compute cost exceeded $400,000. This technical debt resulted directly from inadequate initial labeling but appeared in infrastructure budgets, not labeling budgets.

Data waste is the fourth hidden cost. Teams over-label to be safe, then discover they did not need all the data they labeled. A document classification program labeled 100,000 documents at $1.50 per document for a total cost of $150,000. During model development, the team discovered that performance plateaued at 35,000 labels. The remaining 65,000 labels contributed nothing to model quality. The company spent $97,500 on labels they did not need because they did not run pilot studies to determine the optimal dataset size before committing to full-scale labeling.

Vendor lock-in represents the fifth hidden cost. If you build your entire labeling workflow around a specific vendor platform, switching vendors becomes expensive. You must retrain annotators on new tools, rebuild integrations, migrate data, and potentially re-format annotations. A financial services company used a proprietary annotation format from their initial vendor. When they switched vendors two years later to reduce costs, they spent $60,000 in engineering time building conversion tools to migrate 400,000 annotations to the new format. This lock-in cost was invisible during the initial vendor selection but became painfully real when circumstances changed.

Compliance and audit costs are the sixth category. Regulated industries must maintain audit trails for labeling decisions. Who labeled each item, when, under what guidelines, with what quality scores, reviewed by whom. If you do not build these audit capabilities into your initial program, adding them retroactively is expensive or impossible. A healthcare AI company faced an FDA audit and could not produce complete provenance records for 40 percent of their training labels because their initial annotation platform did not track reviewer identities and timestamps. They had to re-label 25,000 images at $12 per image to create compliant labels with full audit trails. Hidden compliance cost: $300,000.

Hidden costs are unpredictable in magnitude but inevitable in occurrence. Every labeling program generates some subset of these costs. Teams that budget only for direct annotation costs are budgeting for failure.

## The Complete Cost Model

Understanding the true cost of labeling requires accounting for all three categories: direct, indirect, and hidden. A realistic cost model multiplies your direct annotation costs by a factor of 1.8 to 3.5 depending on task complexity, organizational maturity, and domain requirements.

For straightforward tasks with mature guidelines and experienced vendors, expect total costs to be 1.8 to 2.2 times your direct annotation costs. Direct costs might be sixty percent of total, indirect costs thirty percent, and hidden costs ten percent. A commodity image classification program spending $50,000 on direct annotation will likely spend $90,000 to $110,000 total.

For moderately complex tasks requiring domain expertise and guideline iteration, expect total costs to be 2.2 to 2.8 times your direct annotation costs. Direct costs might be fifty percent of total, indirect costs thirty-five percent, and hidden costs fifteen percent. A medical record labeling program spending $200,000 on direct annotation will likely spend $440,000 to $560,000 total.

For highly complex tasks in regulated domains requiring extensive quality assurance and audit trails, expect total costs to be 2.8 to 3.5 times your direct annotation costs. Direct costs might be forty percent of total, indirect costs forty percent, and hidden costs twenty percent. A clinical trial document labeling program spending $500,000 on direct annotation will likely spend $1,400,000 to $1,750,000 total.

These multipliers are not contingency padding. They are the structural reality of labeling operations. Teams that budget only for direct costs will run out of money, cut corners on quality, or both.

The implication for your labeling program is straightforward: when your vendor quotes $3 per label and you need 50,000 labels, your budget should not be $150,000. Your budget should be $270,000 to $525,000 depending on task complexity. When finance asks why the budget is so much higher than the vendor quote, you explain that the vendor quote covers annotation, not operation. You are budgeting for operation.

Your next challenge is turning this cost understanding into an actual budget that accounts for volume uncertainty, timeline risk, and the economic trade-offs inherent in labeling program design.
