# 9.2 — Budgeting for Labeling Programs: Forecasting Volume and Spend

Most labeling budgets fail not because the numbers are wrong but because the assumptions beneath the numbers are fiction. Teams budget for the labeling program they imagine, not the labeling program that reality imposes. They estimate volume based on wishful thinking, price based on oversimplified task definitions, and timeline based on the assumption that nothing will change. By the time they discover the gap between projection and reality, they have already committed resources, set stakeholder expectations, and locked themselves into delivery timelines they cannot meet. Accurate labeling budgets require forecasting models, not guesswork—models that account for task complexity variation, annotator availability constraints, schema evolution, quality rework cycles, and the inevitable discovery that your initial scope was incomplete.

The team discovered that 25,000 contracts was insufficient; they needed 60,000 to handle the diversity of agreement types their customers used. The $8 per contract estimate assumed simple yes-no labeling; the actual task required attorneys to extract and categorize dozens of clauses per contract at $18 per contract. The timeline assumed steady labeling throughput; the reality was that attorney availability fluctuated wildly, creating weeks-long gaps. The budget assumed one labeling pass; the team needed three passes as they refined their annotation schema. The original budget captured none of these realities because it was built on guesses instead of forecasts, assumptions instead of models, and optimism instead of contingency planning.

This is the standard failure mode of labeling budgets in 2026. Teams anchor on a number that feels reasonable, multiply by a unit cost from a vendor quote, and call it a budget. Then reality teaches them that budgeting for labeling is forecasting under uncertainty. You are predicting how much data you will need, how much that data will cost to label, how long the program will run, and what will go wrong along the way. Getting this right requires volume forecasting, cost modeling, timeline estimation, and contingency planning. Getting it wrong means running out of money halfway through your program.

## Volume Forecasting: Predicting How Much Data You Need

The first challenge in budgeting is estimating how many labels you need. This is not a number you know in advance. It is a number you must forecast based on task complexity, model architecture, performance requirements, and data diversity.

Task complexity determines the baseline data requirement. Simple binary classification tasks often reach acceptable performance with 5,000 to 20,000 labels. Multi-class classification tasks require 15,000 to 100,000 labels depending on class count and class imbalance. Named entity recognition tasks require 20,000 to 150,000 labeled spans depending on entity type diversity. Document-level tasks require 10,000 to 80,000 labeled documents. Generative tasks requiring output quality assessment require 30,000 to 200,000 labeled examples depending on output diversity. These ranges are not guarantees. They are starting points based on 2026 industry norms for transformer-based models.

Data diversity expands the requirement. If your task operates on homogeneous data, you can train effective models with smaller datasets. If your task operates on heterogeneous data with many subcategories, rare events, or edge cases, you need larger datasets to ensure adequate representation. A content moderation system handling only English text comments on a single platform might train well on 30,000 labels. A content moderation system handling forty languages across text, images, and video on twelve platform types might need 500,000 labels to cover the distribution adequately. Your volume forecast must account for the diversity of your production data.

Performance requirements determine the upper bound. Higher performance demands more data. If you need seventy percent precision, you might achieve it with 10,000 labels. If you need ninety-five percent precision, you might need 80,000 labels. If you need ninety-nine percent precision, you might need 300,000 labels or discover that labeling alone cannot get you there. Your forecast must connect data volume to your success criteria. This connection requires empirical measurement, not guessing.

The only reliable way to forecast volume is to run pilot studies. Label a small initial dataset, train a model, measure performance, plot learning curves, and extrapolate. A standard pilot approach labels three dataset sizes: 2,000 examples, 5,000 examples, and 10,000 examples. You train models on each dataset size and measure performance. If performance improves significantly from 5,000 to 10,000 examples, you have not saturated; you need more data. If performance plateaus from 5,000 to 10,000 examples, you are near saturation; labeling more data will yield diminishing returns. If you plot accuracy against dataset size and fit a power law curve, you can extrapolate how much data you need to hit your target performance. A pilot study showing that accuracy increases from 0.76 at 2,000 examples to 0.84 at 5,000 examples to 0.88 at 10,000 examples suggests you might need 35,000 to 50,000 examples to reach 0.92 accuracy if the trend continues.

Pilot-based forecasting costs money and time up front but saves both in the long run. The legal technology startup that budgeted 25,000 contracts would have discovered through a 2,000-contract pilot that their task required far more data. A $36,000 pilot would have prevented a $340,000 budget overrun. The return on investment for pilot studies is typically five to twenty times the pilot cost in avoided waste.

When pilots are not feasible due to timeline constraints, use industry benchmarks with generous safety margins. If similar tasks in your domain required 40,000 labels, budget for 60,000. If your task is more complex than the benchmark, budget for 80,000. Overestimating volume is safer than underestimating because unused labels cost nothing; insufficient labels cost launch delays.

Your volume forecast should produce three numbers: a minimum viable dataset size below which the model will not meet success criteria, a target dataset size where you expect to meet criteria, and a maximum dataset size beyond which additional data yields negligible improvement. Budget for the target size with contingency to reach the maximum size if needed.

## Unit Cost Estimation: Predicting the Cost per Label

Once you have a volume forecast, you need a unit cost estimate. This is not simply the vendor quote. It is the fully-loaded cost per label including direct annotation, quality assurance, rework, and operational overhead.

Start with the direct annotation cost from vendor quotes or internal labor costs. Request quotes from at least three vendors with representative sample tasks. Vendor quotes will vary by two to five times for the same task. A document labeling task might get quotes of $4, $7, and $11 per document from three vendors. The variation reflects differences in annotator expertise, quality processes, geographic labor markets, and vendor margin. Choose the vendor that balances cost and quality for your requirements, but do not anchor on the lowest quote. The lowest quote often comes from the lowest quality vendor.

Add quality assurance costs. If you will review ten percent of labels at the same cost as initial annotation, your QA cost is ten percent of annotation cost. If you will double-label twenty percent of data and have an expert adjudicate disagreements at twice the cost, your QA cost is forty percent of annotation cost for that subset, or eight percent overall. A $5 per label annotation cost with twenty percent double-labeling at $5 per label and expert adjudication at $10 per label adds $1 per label in QA costs on the double-labeled subset, or $0.20 per label overall.

Add expected rework costs. First-pass annotation guidelines are always incomplete. You will discover errors and re-label some portion of data. Industry norms suggest that ten to thirty percent of data gets re-labeled at least once. Budget for fifteen percent rework as a baseline. If your direct annotation cost is $5 per label, budget an additional $0.75 per label for rework.

Add operational overhead. Divide your estimated indirect costs—program management, tooling, coordination, guideline development—by your total label volume to get a per-label overhead cost. A program labeling 50,000 items with $80,000 in indirect costs has a per-label overhead of $1.60. Add this to your direct cost.

Your fully-loaded unit cost is the sum of direct annotation, quality assurance, rework, and overhead. A program with $5 direct cost, $0.20 QA cost, $0.75 rework cost, and $1.60 overhead cost has a true unit cost of $7.55 per label. This is the number you use for budgeting, not the $5 vendor quote.

Validate your unit cost estimate with a small pilot. Label 500 to 2,000 items, measure actual time spent on annotation and QA, track overhead hours, and calculate realized unit cost. If your pilot unit cost is significantly different from your estimate, update your forecast before committing to full-scale labeling.

## Timeline Estimation: Predicting How Long Labeling Will Take

Labeling programs take longer than expected. Teams underestimate the time required for annotator recruitment, onboarding, ramp-up, coordination, quality issues, and iteration. Accurate timeline estimation requires modeling throughput, dependencies, and buffers.

Annotator throughput depends on task complexity and expertise. A simple classification task might allow an annotator to process sixty to two hundred items per hour. A document annotation task might allow ten to thirty documents per hour. A medical imaging task might allow three to eight images per hour. Request throughput estimates from vendors or measure throughput during pilots. A task with measured throughput of forty items per hour means one annotator working full-time produces 1,600 labels per week.

Your total timeline is your label volume divided by your effective weekly throughput. If you need 50,000 labels and can sustain 5,000 labels per week with a team of annotators, your timeline is ten weeks of active annotation. But active annotation is not total timeline.

Add time for onboarding and ramp-up. New annotators take one to three weeks to reach full productivity. If you onboard a team of ten annotators, assume two weeks before they hit target throughput. Add time for guideline iteration. Your first annotation guidelines will be incomplete. Annotators will surface ambiguities and edge cases. You will revise guidelines, which requires re-training annotators and possibly re-labeling data. Guideline iteration adds two to six weeks to most programs. Add time for quality issues. You will discover systematic errors that require investigation, root cause analysis, guideline updates, and rework. Quality issues add one to four weeks depending on severity and frequency.

Add time for coordination delays. Vendors have capacity constraints. Domain experts have limited availability. Engineering teams have competing priorities. These constraints create gaps in your timeline. A program that should take ten weeks of active annotation often takes sixteen to twenty-four weeks of calendar time when you account for delays.

Your timeline estimate should include these components: onboarding, guideline development, active annotation, quality review, iteration, and buffer for unexpected delays. A realistic timeline for a 50,000-label program might be two weeks onboarding, two weeks initial guideline testing, ten weeks active annotation, two weeks quality review and iteration, two weeks rework, and four weeks buffer for delays. Total: twenty-two weeks. If your vendor quotes ten weeks based on throughput alone, your budget should plan for twenty-two weeks.

Timeline matters for budgeting because it determines the duration of recurring costs. If your program takes twenty-two weeks instead of ten weeks, you pay platform subscription fees for twenty-two weeks instead of ten, you pay program manager salaries for twenty-two weeks instead of ten, and you incur opportunity costs for the additional twelve weeks of delay.

## Budget Structure: Fixed, Variable, and Contingency

A well-structured labeling budget separates fixed costs, variable costs, and contingency. This separation makes it possible to track spending, identify variances, and adjust plans as the program evolves.

Fixed costs are expenses that do not vary with label volume: platform subscription fees, program manager salaries, guideline development, tooling and infrastructure, onboarding and training. These costs are incurred regardless of whether you label 10,000 items or 100,000 items. A program with a six-month timeline might have fixed costs of $18,000 in platform fees, $70,000 in program management salary, $12,000 in guideline development, $25,000 in custom tooling, and $8,000 in onboarding and training for a total of $133,000 in fixed costs.

Variable costs are expenses that scale with label volume: direct annotation costs, quality assurance costs, data handling costs. These costs increase linearly with the number of labels produced. A program labeling 50,000 items at $5 per label direct cost, $0.50 per label QA cost, and $0.10 per label data handling cost has variable costs of $5.60 per label for a total of $280,000 in variable costs.

Contingency is the budget reserve for rework, scope expansion, and unforeseen issues. Industry best practice allocates twenty to forty percent of the sum of fixed and variable costs as contingency. A program with $133,000 in fixed costs and $280,000 in variable costs has a base budget of $413,000. A thirty percent contingency adds $124,000 for a total budget of $537,000. This contingency is not padding. It is insurance against the inevitable surprises that every labeling program encounters.

Structuring your budget this way provides flexibility. If your volume forecast changes, you can adjust variable costs without re-calculating the entire budget. If your timeline extends, you can adjust fixed costs. If you encounter rework, you draw from contingency. Finance teams understand this structure because it mirrors standard project budgeting practices.

## Budget Approval: Selling the True Cost

The hardest part of labeling budgets is often getting them approved. Finance teams and executives see vendor quotes and assume that is the cost. When you present a budget that is three times the vendor quote, you will face skepticism. Selling the true cost requires education, justification, and risk framing.

Start by showing the cost breakdown. Do not present a single number. Present fixed costs, variable costs, and contingency separately with line-item detail. Show that the vendor quote covers only the variable annotation cost, which is sixty-seven percent of your variable costs and thirty-four percent of your total budget. Explain each cost category and why it is necessary.

Provide comparisons to industry benchmarks. Show that your cost multiplier is consistent with industry norms for similar programs. Reference similar projects your company has executed if available. A medical device company budgeting $800,000 for a labeling program can point to a prior program that budgeted $300,000 based on vendor quotes and actually spent $680,000 when all costs were included.

Frame the budget in terms of risk mitigation. Explain that inadequate budgets lead to three outcomes: cutting scope and launching with insufficient data, cutting quality and launching with unreliable models, or running out of money and canceling the project. All three outcomes are more expensive than funding the program adequately up front. A product launch delayed by six months because of labeling budget shortfalls costs far more than the incremental budget required to fund the program properly.

Offer a phased approach if the full budget is not immediately available. Propose funding a pilot phase with fifteen to twenty-five percent of the full budget to validate volume forecasts, unit costs, and timelines. Use pilot results to refine the budget and request full funding. This approach reduces perceived risk and builds credibility.

Tie the budget to business outcomes. Show the revenue impact, cost savings, or strategic value of the AI capability enabled by the labeling program. If the capability is expected to generate $5 million in annual revenue, a $500,000 labeling budget is a ten percent cost of revenue in year one and negligible in subsequent years. Frame labeling as an investment in a revenue-generating asset, not an expense.

Most budget rejections stem from inadequate justification, not excessive costs. If you present a detailed budget, explain each component, provide benchmarks, and connect the investment to business value, you will get approval.

## Common Budgeting Mistakes and How to Avoid Them

Five mistakes account for most labeling budget failures. Avoid them and your budget will survive contact with reality.

The first mistake is anchoring on vendor quotes. Vendor quotes are not budgets. They are input costs. Your budget must account for the total cost of operating a labeling program, not just paying annotators. The fix is to build a complete cost model that includes direct, indirect, and hidden costs.

The second mistake is underestimating volume. Teams budget for the minimum viable dataset size and hope it will be sufficient. When it is not, they have no budget for additional labeling. The fix is to run pilots, validate volume forecasts empirically, and budget for target volume plus contingency, not minimum volume.

The third mistake is ignoring timeline risk. Teams budget for optimistic timelines based on vendor throughput estimates. When the program takes twice as long due to coordination delays and iteration, recurring costs double. The fix is to build realistic timelines that account for onboarding, iteration, quality issues, and coordination delays, then budget for the realistic timeline.

The fourth mistake is zero contingency. Teams budget to the dollar for expected costs with no reserve for rework, scope changes, or unforeseen issues. When anything goes wrong, they have no budget to address it. The fix is to allocate twenty to forty percent contingency and defend it as risk mitigation, not padding.

The fifth mistake is failing to track actuals. Teams create budgets, then never compare actual spending to budgeted spending. They discover overruns only when finance flags them. The fix is to track spending weekly, compare actuals to budget, investigate variances, and adjust plans proactively when trends indicate you will exceed budget.

Budgeting for labeling programs is not guessing. It is forecasting based on empirical data, modeling based on complete cost structures, and planning for uncertainty with contingency. The teams that do this well launch on time and on budget. The teams that do not run out of money, cut corners, or both.

Your next challenge is reducing these costs without sacrificing quality, which requires understanding the economic trade-offs and optimization strategies available in labeling program design.
