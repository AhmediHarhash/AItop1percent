# 7.3 — Versioning Your Standards

Here's a question that should terrify you: someone asks, "What was our ground truth on March 15th?" Can you answer?

Most teams can't. They've updated their ground truth over time — that's good. But they've overwritten the old versions — that's bad. They know what their current standard is, but they have no record of what it used to be.

Why does this matter? Let me tell you about a fintech company that learned this lesson the hard way.

They built a loan approval AI and spent months evaluating it against carefully crafted ground truth. By the time they launched in January, they had 94 percent accuracy against their ground truth. Great numbers.

In March, regulations changed. They updated their ground truth to reflect the new compliance requirements. Sensible move.

In July, they got audited. The auditor asked to see their evaluation data from February. They provided it: 94 percent accuracy, everything looks great.

Then the auditor asked: "What ground truth was this evaluated against?" They said, "Our current ground truth." The auditor said, "Show me the ground truth as it existed in February."

They couldn't. They'd overwritten it with the March updates. So they had no way to prove that the 94 percent accuracy they showed was against standards that were actually compliant at the time. The auditor couldn't verify that they weren't optimizing for non-compliant behavior in February and then covering it up by updating the ground truth.

It wasn't malicious. It was just sloppy version control. But the consequences were real: extended audit, regulatory scrutiny, and a requirement to implement proper ground truth versioning before the next review.

## Why Ground Truth Needs Version Control

Your code has version control. Your data has versioning. Your documentation has change tracking. Your ground truth needs the same treatment, for the same reasons.

First, auditability. You need to be able to show what standards you were using at any point in time. This isn't just for regulatory compliance, though that's critical. It's for understanding your own history.

When you look at evaluation data from six months ago, you need to know what it was evaluated against. Otherwise, you can't interpret the results. Was that 90 percent score against rigorous standards or lenient ones? Against standards that matched user needs or outdated ones?

Second, change analysis. When you update your ground truth, you need to understand the impact. How much did the change affect your scores? What behavior changes did it drive? You can only answer these questions if you can compare versions.

Third, rollback capability. Sometimes you make a ground truth change that seemed right but turns out to be wrong. Maybe you made standards too strict and now nothing passes. Maybe you updated based on a misunderstanding that got corrected. You need to be able to roll back cleanly.

Fourth, communication. When you update ground truth, stakeholders need to know what changed. Not just "we updated the rubric" but specifically what's different. Version control makes this visible.

## What to Version

Let me walk you through everything that needs versioning in your ground truth system.

### Rubrics and Evaluation Criteria

Every time you update how you define "good," that's a new version. This includes:

Adding a new criterion to your rubric.

Removing an outdated criterion.

Changing the definition of an existing criterion.

Reordering criteria to change priority.

All of these change what you're measuring. All need versioning.

### Example Sets and Reference Answers

Your curated examples of good and bad outputs are part of your ground truth. When you:

Add new examples.

Remove outdated examples.

Update the rating or annotation on an example.

Change the reference answer for an example.

That's a version change.

I've seen teams update individual examples without versioning. Six months later, they can't figure out why their evaluation scores changed. Turns out they'd gradually replaced their examples with harder ones, effectively raising the bar without documenting it.

### Behavior Specifications

The detailed specifications of how your system should behave in different scenarios — these are ground truth too.

When you specify that the chatbot should escalate to a human in certain situations, that's a standard. When you update that specification to add or remove escalation triggers, version it.

### Threshold Values

Many ground truth systems include numerical thresholds: "Responses should be under 200 words," "Confidence should be above 0.8 to proceed," "Latency should be under 2 seconds."

When you change these thresholds, version it. A system that meets "under 200 words" might not meet "under 150 words," and you need to be able to distinguish which threshold historical data was measured against.

### Annotation Guidelines

If you use human annotators, they work from guidelines that tell them how to apply your ground truth. These guidelines are an interpretation layer on top of your ground truth, and they need versioning too.

When you clarify an ambiguous guideline, that's a change that affects annotations going forward. Version it.

## Semantic Versioning for Ground Truth

Software uses semantic versioning — major.minor.patch, like 2.3.1. This same pattern works beautifully for ground truth.

### Major Version: Breaking Changes

Increment the major version when you make a change that fundamentally breaks comparability with previous versions.

Examples:

Completely restructuring your rubric.

Changing the scale (moving from 1-5 ratings to 1-10).

Replacing your entire example set with a new one.

Fundamentally redefining what you're measuring (shifting from accuracy focus to safety focus).

If you were at version 2.4.3 and you make any of these changes, you go to 3.0.0.

Why does this matter? Because you cannot meaningfully compare evaluations done under version 2.x to evaluations done under version 3.x. They're measuring different things.

When you increment major version, you're saying: "Historical comparisons need to account for this discontinuity."

### Minor Version: Additions

Increment the minor version when you add something without changing existing standards.

Examples:

Adding a new criterion to your rubric while keeping all existing criteria unchanged.

Adding new examples to your example set while keeping existing examples.

Adding specifications for a new feature while keeping existing specifications.

Adding a new annotation guideline for a scenario you didn't previously cover.

If you were at version 2.4.3 and you add a criterion, you go to 2.5.0.

Evaluations under 2.4.x and 2.5.x aren't perfectly comparable — 2.5.x might be stricter because it checks more things. But they're compatible enough that you can often normalize or compare them with appropriate caveats.

### Patch Version: Clarifications

Increment the patch version when you clarify existing standards without changing their meaning.

Examples:

Fixing a typo in your rubric.

Adding an example that illustrates an existing criterion better but doesn't change the criterion.

Rewording a guideline for clarity without changing the intent.

Correcting an error in an example's annotation.

If you were at version 2.4.3 and you fix typos, you go to 2.4.4.

Evaluations under different patch versions of the same minor version should be fully comparable. You're not changing what you measure, just making it clearer.

## The Version Decision Tree

When you're about to update your ground truth, ask yourself:

"Does this change what I'm fundamentally measuring?" If yes, major version bump.

"Does this add new things I'm measuring while keeping everything else the same?" If yes, minor version bump.

"Does this just clarify what I already measure without changing it?" If yes, patch version bump.

This decision tree makes versioning consistent across your team.

## Version Control Implementation

Now let's talk about how to actually implement this in practice.

### Git for Ground Truth

If your ground truth is in text files — YAML configs, markdown documentation, JSON data files — use Git. The same version control that manages your code can manage your ground truth.

Each ground truth update becomes a commit. Each version number becomes a tag. You get all the benefits of Git: history, diffs, branches, rollback.

When you update from version 2.4.3 to 2.5.0, you commit the changes and tag the commit as v2.5.0. Now you can always check out exactly what ground truth version 2.5.0 looked like.

### Database Versioning

If your ground truth is in a database — common for large example sets or dynamically managed rubrics — implement versioning at the schema level.

Every ground truth record should have a version field. When you update a criterion, you don't overwrite the old version — you insert a new record with an incremented version and mark the old one as superseded.

Queries for "current ground truth" filter for the active version. Queries for "ground truth as of date X" filter for the version that was active on that date.

### Hybrid Approaches

Many systems need both. Configuration files in Git for rubrics and specs. Database records for large example sets. The key is linking them with version numbers.

Your Git tag and your database version field should agree. If your rubric is at version 2.5.0 in Git, your example set should be at version 2.5.0 in the database.

## The Changelog: Making Changes Visible

Version numbers tell you that something changed. Changelogs tell you what changed.

Every time you increment a version, document what changed in a changelog. This doesn't need to be elaborate. It needs to be clear:

Version 2.5.0 (2026-01-15):
- Added criterion for source citation in rubric (minor)
- Added 47 examples demonstrating proper citation (minor)
- Clarified wording of "factual accuracy" criterion (patch)

Changelogs make version history useful. Someone looking at evaluation data from January 15, 2026 can see exactly what ground truth version 2.5.0 measured.

### What to Include in Changelogs

Document the change: what specifically was updated.

Document the reason: why the change was made.

Document the impact: what effect this is expected to have on evaluations.

Document the author: who made the change (for questions later).

This creates an audit trail that's human-readable and valuable.

## Migration Paths: Re-evaluating Under New Standards

Here's a critical question: when you update your ground truth, what happens to existing evaluation data?

You have three options, and you need to decide which applies to each version update.

### Option 1: Leave Historical Data As-Is

Most of the time, this is the right choice. Historical evaluations were valid under the ground truth version they used. You don't need to redo them.

Just make sure you always record which version of ground truth was used for each evaluation. Then you can interpret historical data correctly.

When you look at February data evaluated under version 2.4.0 and March data evaluated under version 2.5.0, you know they're not perfectly comparable, and you can account for that in your analysis.

### Option 2: Re-evaluate Recent Data

Sometimes you want to understand the impact of a ground truth change by re-evaluating recent data under both the old and new versions.

This is useful when:

You made a significant change and want to quantify the impact.

You need to continue a trend line across a version boundary.

You're trying to decide if a change was good or bad based on data.

Pick a sample of recent production examples. Evaluate them under the old version, then under the new version. Compare the results. This tells you concretely how the version change affects your metrics.

### Option 3: Full Re-evaluation

Rarely, you need to re-evaluate all historical data under new ground truth. This is expensive and time-consuming, so only do it when necessary.

When is it necessary?

When you discovered your ground truth was fundamentally wrong and you need to correct the historical record.

When regulatory requirements demand re-evaluation under new standards.

When you're making a major change and need to establish a new baseline.

If you do this, keep both versions of the evaluation data. Don't overwrite the original evaluations — add the new evaluations as a separate dataset. You want to be able to show before and after.

## Communicating Version Changes

When you release a new version of your ground truth, communicate it clearly to everyone who uses it.

### For Engineers

They need to know if automated evaluations need to be updated, if test cases need to change, if expected metrics will shift.

"We released ground truth version 2.5.0. New evaluations will use this version automatically. Expect accuracy scores to drop 2-3 percentage points due to the added citation criterion."

### For Product and Business Stakeholders

They need to know how this affects what you're measuring and why.

"We updated our quality standards to require source citations because users are increasingly asking for evidence. This makes our bar higher but aligns with user expectations."

### For Compliance and Legal

They need to know what changed and why, especially if the change relates to regulations or policies.

"We updated our ground truth to reflect new EU AI Act transparency requirements effective February 2026. All evaluations after February 1 use compliant standards."

## The "Which Version?" Audit Question

In every evaluation result you record, store the ground truth version used. This is non-negotiable.

When you store evaluation data, include:

The example or input that was evaluated.

The output or response that was evaluated.

The evaluation result (scores, ratings, pass/fail).

The ground truth version used for the evaluation.

The timestamp of the evaluation.

With this data, you can always answer the question: "This evaluation says 92 percent — 92 percent against what standard, from when?"

Without the version number, evaluation data is almost meaningless.

## Rollback Procedures

Sometimes you need to roll back a ground truth change. The procedure should be straightforward:

First, identify the version you want to roll back to. Check out that version from Git or mark it as active in your database.

Second, update the version number. If you were at 2.5.0 and you're rolling back to 2.4.3, you don't go back to 2.4.3 — you go to 2.5.1 and make 2.5.1 identical to 2.4.3. This preserves the history that 2.5.0 happened and was then reverted.

Third, document the rollback in your changelog. "Version 2.5.1: Reverted changes from 2.5.0 due to X issue."

Fourth, communicate the rollback to stakeholders, just like you would communicate a new version.

## Version Alignment Across Systems

Many organizations have multiple AI systems, each with their own ground truth. Those ground truths often share common standards — tone, safety, compliance.

When you version shared standards, keep versions aligned. If your company-wide safety standards are at version 3.2.0, all systems using those standards should reference version 3.2.0, even if their system-specific ground truth is at different versions.

This might mean:

Shared ground truth components with their own version numbers.

System-specific ground truth components with their own version numbers.

A composite version that includes both.

For example: "System A ground truth version 2.5.0, incorporating shared safety standards version 3.2.0."

## Version Control Anti-Patterns

Let me warn you about mistakes I see teams make with ground truth versioning.

### The Overwrite

Updating ground truth by overwriting the old version without any record of what changed. This makes version control useless.

### The Version Explosion

Creating a new version for every tiny change, including things that don't actually affect evaluations. You end up with version 2.47.193 and nobody can track what changed or when.

Use patch versions for clarifications, but don't version every single edit. Group small clarifications and release them together.

### The Undocumented Version

Incrementing the version number without updating the changelog or communicating what changed. Now you have version history, but no one knows what it means.

### The Mixed Version

Using different ground truth versions in the same evaluation run. Some examples evaluated against 2.4.0, some against 2.5.0, all recorded as a single result.

This corrupts your data. An evaluation run should use a single ground truth version throughout.

## Making Versioning Automatic

The more you can automate versioning, the more consistently it will happen.

Automate version increments: when someone merges a ground truth update, CI/CD automatically increments the version based on labels (breaking/feature/fix).

Automate changelog generation: commits follow a format that allows automatic changelog creation.

Automate version stamping: every evaluation automatically records the current ground truth version without manual intervention.

Automate compatibility checking: systems alert you if you're trying to compare evaluations across incompatible ground truth versions.

The less manual work versioning requires, the less likely it is to be skipped.

## Version History as a Learning Tool

Over time, your version history becomes a valuable asset. It tells the story of how your understanding of quality has evolved.

Looking back, you can see:

When you added safety criteria (probably after an incident).

When you made standards stricter (probably as the model improved).

When you loosened standards (probably when you realized they were unrealistic).

When you pivoted focus (probably when product strategy changed).

This history helps new team members understand context. It helps you avoid repeating mistakes. It shows auditors that you have mature quality processes.

Treat your version history as documentation of your learning, not just a compliance checkbox.

In the next section, we'll tackle an awkward but increasingly common problem: what to do when your AI starts producing outputs that are better than your ground truth says they should be.
