# 4.6 — Synthetic Ground Truth (When and How)

Here's a conversation that happens in every AI team eventually.

You're building your ground truth dataset. You have fifty hand-crafted examples. You need five hundred. Creating each example manually takes twenty minutes of expert time. That's a hundred and fifty hours of work. Your experts are expensive and busy. Someone on the team says:

"Wait, why don't we just use GPT to generate the examples? It can create five hundred examples in like an hour. Then we'll have humans spot-check them. Way faster."

This is a tempting and dangerous idea.

Tempting because it's true — AI can generate examples incredibly fast. What takes a human twenty minutes takes an AI twenty seconds. You can scale your dataset from hundreds to thousands in an afternoon.

Dangerous because synthetic data has subtle failure modes that won't be obvious until you've already built your entire evaluation system on quicksand. You'll feel productive while actively making things worse.

Let me walk you through when synthetic ground truth works, when it catastrophically fails, and how to use it without destroying the integrity of your evaluation system.

## The Seductive Promise of Synthetic

First, let's acknowledge why synthetic ground truth is so appealing.

Speed: Generate thousands of examples overnight instead of spending months on manual creation.

Coverage: Systematically cover edge cases and combinations that might not appear in real data for years.

Cost: Pay for API calls instead of expert time. Orders of magnitude cheaper.

Consistency: No human fatigue, no annotation drift, no inter-rater disagreement. Same quality bar every time.

Scale: Go from dozens to thousands of examples trivially. No recruitment, no training, no management overhead.

These benefits are real. Teams at Anthropic, OpenAI, Google, and leading startups use synthetic data extensively. But they use it in specific ways for specific purposes. The problem is teams cargo-cult the "we use synthetic data" part without understanding the "for this specific purpose with these specific safeguards" part.

So they generate synthetic ground truth, train on it, evaluate on it, launch, and discover in production that their AI fails in ways their tests never caught. Because synthetic data has systematic blind spots.

## The Model Collapse Problem

Here's the core issue: when you use AI-generated data to train or evaluate AI, you risk model collapse.

Model collapse is what happens when AI systems train on their own outputs or outputs from similar systems. The result is a gradual loss of diversity, a narrowing of the distribution, and a reinforcement of existing biases and limitations. It's like making a copy of a copy of a copy — each generation loses something.

In ground truth terms, this manifests as: you use GPT to generate test cases for evaluating GPT. GPT thinks certain responses are good, so it generates test cases where those responses are labeled as good. You evaluate your system against those test cases. Your system learns to produce GPT-like outputs. You've created a closed loop that optimizes for GPT's preferences, not actual quality.

The insidious part is this feels like progress. Your eval scores go up. You're "improving." But you're improving at mimicking GPT, which is not the same as improving at helping users.

Real-world example: A team building a medical chatbot used GPT-4 to generate ground truth for medical questions. The synthetic examples were articulate, confident, and well-structured. They evaluated their medical bot against these examples and scored well. Then they tested with real doctors and discovered the bot was confidently giving outdated treatment recommendations — because GPT-4's training data included outdated medical information, and the synthetic ground truth inherited that staleness. The eval system was optimizing for sounding like GPT, not for medical accuracy.

This is model collapse in action. The synthetic ground truth encoded GPT's limitations, then evaluated the system based on those same limitations. No alarm bells went off because the whole evaluation loop was contaminated.

## When Synthetic Ground Truth Actually Works

Okay, so synthetic data is dangerous. Does that mean never use it? No. It means use it strategically for specific purposes where its strengths matter and its weaknesses don't.

Synthetic data works well for:

Volume augmentation: You have a hundred real, human-validated examples. You need a thousand to cover combinations. Use AI to generate nine hundred more variations on your validated examples. Then human-review a sample to ensure quality. You're using synthetic to scale human judgment, not replace it.

Edge case generation: You've identified a gap — you don't have examples of users asking about feature X in language Y with context Z. That combination is rare in real data. Use AI to generate plausible examples of that specific combination. Then validate them with experts. You're using synthetic to systematically cover the long tail.

Adversarial examples: You want to test whether your system handles tricky inputs. Use AI to generate challenging questions, ambiguous phrasings, or subtle attacks. These don't need to be perfect representations of reality — they need to stress-test your system. Synthetic is great for adversarial.

Privacy-compliant substitutes: Your real data contains PII you can't use in test sets. Use AI to generate structurally similar but fictional examples. "Patient presents with symptoms X, Y, Z" becomes synthetic examples with the same medical logic but fake patient details. You're preserving structure while removing privacy issues.

Bootstrapping sparse domains: You're building an AI for a brand new domain with near-zero existing examples. Synthetic gives you something to start with while you collect real data. It's a temporary scaffold, not a permanent foundation.

Notice the pattern: synthetic works when it's AUGMENTING human judgment or GENERATING COVERAGE where real examples are impossible/impractical, not REPLACING human judgment about what's good.

## When Synthetic Catastrophically Fails

Synthetic ground truth fails catastrophically in these scenarios:

Defining quality standards: Never use AI to define what "good" means in your domain. "Good" must come from humans — experts, users, stakeholders. Synthetic can generate examples once you've defined good, but it can't define good for you.

Novel or evolving domains: Synthetic data is based on training data from the past. If your domain is cutting-edge or fast-changing, synthetic examples will be outdated or wrong. A synthetic dataset about 2026 regulations generated by a model trained on 2024 data will miss everything that changed.

High-stakes accuracy: Medical, legal, financial, safety-critical domains. Synthetic data will confidently generate plausible-but-wrong information. This is fine for low-stakes experimentation, deadly for high-stakes ground truth.

Nuanced human judgment: Empathy, cultural sensitivity, ethical reasoning, creative quality. These require human judgment grounded in human experience. Synthetic data will give you a model's simulation of human judgment, which misses crucial nuance.

Brand voice and style: Your specific brand voice is not in any AI's training data. Synthetic examples will sound generic or like they're mimicking some other brand. Brand voice ground truth must come from your actual brand guidelines and human examples.

Rare expert knowledge: Deep domain expertise that's not well-represented in training data. Synthetic will generate common knowledge or plausible-sounding nonsense, not true expert insight.

If you use synthetic data for these purposes, you're building evaluation on a foundation of AI hallucinations. Your tests will pass while your real-world performance fails.

## The Human-in-the-Loop Requirement

Here's the rule: synthetic ground truth requires human validation. Always. No exceptions.

The workflow is:

1. AI generates candidate examples
2. Humans review and filter them
3. Only validated examples become ground truth
4. Track which examples are synthetic vs real

Step two is not optional. You cannot skip human review and maintain integrity.

How much human review? Depends on trust and stakes.

Low-stakes, trusted model: Review twenty percent of synthetic examples. If rejection rate is under five percent, you can trust the rest provisionally.

Medium-stakes: Review fifty percent. Look for systematic errors in the unreviewed half.

High-stakes: Review one hundred percent. Synthetic is just helping you draft, not making final decisions.

Track your rejection rate. If you're rejecting more than twenty percent of synthetic examples, the generation process is not working. You're spending more time reviewing and rejecting than you would spend creating examples from scratch. Fix your prompts or switch approaches.

## Prompting for Ground Truth Generation

If you're going to use synthetic, do it well. That means careful prompting.

Bad prompt: "Generate 100 customer support questions and good responses."

This will give you generic, obvious examples that sound like what an AI thinks customer support looks like. Useless.

Good prompt: "You are generating test cases for a customer support AI in the B2B SaaS subscription management domain. Generate questions that:

- Reflect realistic confusion points from users who are not technical experts
- Include specific context about their subscription tier, renewal date, or payment method
- Test edge cases like partial refunds, plan migrations, and billing errors
- Vary in emotional tone from neutral to frustrated
- Range from simple factual questions to complex multi-part requests

For each question, provide:
- The customer's question with realistic context
- The correct response based on [attached policy documentation]
- The key quality criteria this example tests (accuracy, empathy, policy adherence, etc.)
- Any edge case factors that make this example non-trivial

Make the language natural, not formal. Include typos and unclear phrasings occasionally — real users don't write perfectly."

See the difference? The second prompt:

- Specifies domain and context
- Defines what makes examples useful (edge cases, variation)
- Requires structured output with metadata
- Demands realism over perfection
- Grounds generation in actual policies

You'll get much better synthetic data. Still requires human review, but the accept rate will be higher.

## Detecting Synthetic Contamination

Here's a scary scenario: your team has been building ground truth for months. Multiple people contributing. Someone used synthetic examples without telling anyone or without adequate review. Now your ground truth is contaminated and you don't know which parts.

You need to detect this before it poisons your entire evaluation system.

Red flags for synthetic contamination:

Unnatural uniformity: Real examples are messy, varied, idiosyncratic. Synthetic examples tend to be suspiciously well-structured and consistent. If all your examples follow the same template, suspect synthetic.

Impossible knowledge: Examples that reference information that didn't exist when they were supposedly created. "Customer asking about our new pricing in a dataset from six months ago, but the new pricing launched last week."

Generic phrasing: Real customer questions have personality. Synthetic questions sound like "Dear customer service, I am experiencing an issue with..." Nobody talks like that.

Perfect coverage: Real data has gaps and clusters. Synthetic data has suspiciously even coverage across categories. If every edge case appears exactly once, suspect synthetic.

Metadata inconsistencies: Examples marked as from "real customer interactions" but the customer IDs don't exist in your database. Creation timestamps that don't match claimed collection dates.

To audit your dataset:

- Sample one hundred examples randomly
- Have domain experts review them for realism
- Check metadata against source systems
- Look for linguistic patterns that suggest AI generation (there are tools for this now)
- Track down the provenance of suspicious examples

If you find contamination, quarantine it. Mark contaminated examples clearly. Don't delete them (you might learn from understanding the contamination), but exclude them from evaluation until they're validated.

## Synthetic Data for Diversity and Fairness

One legitimate use of synthetic data: generating examples that test for bias and fairness across demographics, languages, and contexts that might be underrepresented in real data.

Real example: A hiring AI trained mostly on US data. Real examples skew toward US names, US credentials, US career paths. Synthetic data can systematically generate examples with international names, non-US universities, non-traditional career paths. This helps you test whether your system is biased against underrepresented groups.

But careful — synthetic data about demographic groups you're not part of can itself be biased. If you're generating synthetic examples of neurodiverse users, non-native speakers, or cultural contexts you don't belong to, you MUST have validation from people in those groups. Your synthetic data might encode stereotypes instead of reality.

Best practice: Use synthetic to generate diversity, then validate with real humans from those diverse groups. Synthetic gives you scale, humans give you authenticity.

## The Argilla and Scale AI Approaches

In 2026, tools like Argilla and Scale AI have developed workflows specifically for safely using synthetic data in ground truth pipelines.

Argilla's approach: Generate synthetic, immediately flag it as synthetic, require human annotation before it becomes ground truth. The tool tracks synthetic vs human-labeled examples separately and lets you control the mix.

Scale AI's approach: Use synthetic to draft annotations, then have multiple human reviewers vote on whether to accept, modify, or reject each one. Only examples with human consensus become ground truth.

Both approaches treat synthetic as a productivity tool for humans, not a replacement for humans. This is the right model.

If you're using synthetic at scale, invest in tooling that enforces human validation. Don't rely on discipline — discipline fails under time pressure. Build the validation into your workflow so it's impossible to skip.

## Synthetic for Test Input Generation vs Test Label Generation

Important distinction: using synthetic to generate TEST INPUTS is safer than using it to generate TEST LABELS.

Test inputs: "Generate realistic customer questions" — relatively safe. The worst case is the questions are too easy or too similar. That makes your tests weak, but it doesn't create false confidence.

Test labels: "Generate the correct answer to this question" — dangerous. If the AI generates a wrong answer and you trust it as ground truth, you've now defined wrong as right. Your system will be penalized for being correct and rewarded for matching the AI's mistake.

Prefer this workflow: Use synthetic to generate diverse inputs. Use humans (or highly trusted documentation) to generate the labels. You get the scale benefits of synthetic without the accuracy risks.

## Version Control and Provenance

Every synthetic example in your ground truth should be tagged with:

- Generation date
- Model used to generate it (GPT-4, Claude Opus, etc.)
- Prompt used
- Whether it was human-validated and by whom
- Original vs modified (if a human edited the synthetic output)

This provenance tracking is critical for two reasons:

One, when models improve, you can regenerate synthetic examples with better models and compare. Maybe your 2024 synthetic examples from GPT-4 have quality issues, but 2026 examples from GPT-6 are much better. Provenance lets you upgrade.

Two, if you discover a systematic issue with synthetic examples from a particular source, you can identify and review all affected examples. "All examples generated with prompt version 2.1 have a problem — let's audit them."

Treat synthetic ground truth as code: version controlled, reviewed, documented, with clear ownership.

## The Hybrid Approach That Works

Here's the approach elite teams use:

Start with real, human-created ground truth. Get to fifty or a hundred high-quality examples from experts, documentation, and real user interactions. This is your seed set.

Use the seed set to guide synthetic generation. "Generate more examples like these, with similar structure and quality criteria, covering edge cases we haven't seen yet."

Generate synthetic examples in batches of fifty. For each batch:

- Have humans review twenty-five
- If pass rate is above eighty percent, provisionally accept the batch with light spot-checking
- If pass rate is below eighty percent, review all fifty and fix your generation process

Track synthetic vs real in your dataset metadata. Aim for at least twenty-five percent real examples even as you scale. Real examples keep you grounded.

Periodically validate your synthetic examples against real-world performance. Are the examples your system fails in evals the same ones that fail in production? If synthetic test cases don't correlate with real failures, they're not useful.

Refresh synthetic examples annually. Models improve, your domain changes, your understanding of quality evolves. Don't let synthetic ground truth fossilize.

This hybrid approach gives you the scale of synthetic with the groundedness of real data.

## The Philosophical Question

Let me end with a deeper question: should we be using AI to evaluate AI at all?

The argument for: AI can scale evaluation beyond what human reviewers can handle. As systems get more complex, human evaluation becomes a bottleneck. We need AI assistance.

The argument against: AI evaluation inherits the biases and limitations of AI judgment. We're creating a closed loop where AI optimizes for AI preferences, potentially diverging from human values and needs.

Both arguments have merit. The resolution is: AI can assist in evaluation, but humans must remain in the loop for defining quality, validating edge cases, and ensuring alignment with real human needs.

Synthetic ground truth is a tool, not a replacement for human judgment. Used carefully, it accelerates ground truth development without compromising integrity. Used carelessly, it creates the illusion of progress while building on a foundation of model-generated assumptions.

In the next section, we'll look at how to structure your first fifty examples — the ones that become the foundation everything else builds on. These examples are so important they deserve their own deep dive. Let me show you exactly how to craft them for maximum teaching power.
