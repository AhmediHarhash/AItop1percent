# 7.2 — The Review Cadence That Works

I once worked with a team that reviewed their ground truth every single day. They'd start each morning with a standup about what examples needed updating, what rubrics needed clarification, what edge cases they'd discovered. It sounds proactive, but after three weeks, they were exhausted and making changes just to feel productive. Their ground truth was in constant flux, and nobody could keep up with what the current standard actually was.

Another team I advised went the opposite direction. They reviewed their ground truth once a year, in Q4, as part of annual planning. For eleven months, their standards slowly drifted. By the time review season came around, they had so many changes to make that it took six weeks and involved re-evaluating thousands of examples. It was painful enough that they started dreading it, which made them even less likely to do it more often.

Both teams were doing it wrong. Ground truth maintenance isn't about constant tweaking or annual overhauls. It's about having the right cadence — different types of reviews at different intervals, each serving a specific purpose.

Let me walk you through the review rhythm that actually works, the one I've seen elite teams use to keep their ground truth fresh without burning out.

## The Three-Tier Review System

Think of ground truth maintenance like maintaining a garden. You water daily, weed weekly, prune monthly, and do major landscaping seasonally. Each activity has its rhythm, and together they keep the garden healthy without consuming your entire life.

Ground truth works the same way. You need quick checks frequently, deeper reviews regularly, and strategic assessments periodically. The key is knowing what to do at each interval and not trying to do everything at once.

### Weekly Spot Checks: The Pulse

Every week, someone on your team should spend 30 to 60 minutes doing a ground truth spot check. This isn't a comprehensive review. It's a pulse check to catch obvious drift before it compounds.

Here's the process that works: pull ten random examples from production over the past week. These should be real queries or inputs your system handled, not synthetic test cases. Diversity matters more than volume here — you want different types of interactions, not ten variations of the same question.

Now evaluate each example against your current ground truth. Not with automated tools, with human judgment. Ask: "According to our ground truth, how should we handle this? And is that actually the right answer today?"

You're looking for mismatches. Cases where the ground truth says one thing but current reality demands another. Cases where the ground truth is silent on something that's now common. Cases where the ground truth is overly specific about something that now needs flexibility.

If you find zero mismatches week after week, your spot check might not be diverse enough. Real systems in real environments encounter drift. If you're not seeing it, you're not looking in the right places.

If you find mismatches in more than two or three examples out of ten, that's a signal to schedule a deeper review sooner than planned. Your ground truth might be drifting faster than expected.

#### Who Does the Spot Check?

This should rotate among people who understand both the domain and the ground truth. Don't always give it to the same person — different people catch different issues.

A support agent will notice when the tone is off. An engineer will notice when the technical accuracy is wrong. A compliance person will notice when a disclosure is missing. Rotate the responsibility so you get different perspectives.

#### What to Produce

The spot check should produce a simple artifact: a list of the ten examples, the ground truth assessment, the current reality assessment, and any mismatches noted.

This becomes your early warning system. Over time, you'll see patterns in the mismatches that tell you where drift is happening fastest.

### Monthly Deep Reviews: The Audit

Once a month, block two to three hours for a deep review of your ground truth. This isn't looking at production examples. This is auditing the ground truth itself.

Pull up your entire rubric, your behavior specifications, your threshold values, your example sets. Read through them with fresh eyes. For each section, ask: "Is this still true? Is this still complete? Is this still clear?"

You're looking for four types of problems:

First, staleness — statements that were true when written but aren't anymore. "Users prefer concise responses under 50 words" might have been true in 2023, but if users are now asking for detailed explanations, that's stale.

Second, gaps — things that should be in the ground truth but aren't. Maybe a new product feature launched and there's no guidance on how to handle questions about it. Maybe a new regulation requires certain disclosures that your rubric doesn't mention.

Third, ambiguity — statements that made sense to the person who wrote them but are confusing to anyone else. "Respond professionally" means different things to different people. If your rubric doesn't define it, different evaluators will interpret it differently.

Fourth, conflicts — cases where different parts of your ground truth contradict each other. Maybe one section says "be concise" and another says "provide thorough explanations." Those might both be correct in different contexts, but if the ground truth doesn't clarify when to apply which, you've got a conflict.

#### The Stakeholder Circle

Monthly deep reviews should involve stakeholders beyond the core ground truth team. Invite people from:

Product — they know what features changed, what's coming next, what user feedback has been saying.

Compliance — they know what regulations changed, what auditors are asking about, what risk areas need attention.

Customer-facing teams — support, sales, success — they know what users are actually saying, what pain points are emerging, what expectations are shifting.

Domain experts — whoever knows the subject matter your AI deals with, whether that's medical knowledge, financial regulations, technical documentation, or creative writing.

You don't need everyone at every review, but you need regular input from each group. A quarterly rotation works well — product and support one month, compliance and domain experts the next, rotating through so everyone has input over time.

#### The Review Artifact

The monthly review should produce a change list: what you're updating, why you're updating it, and what version this becomes. Even if you find nothing to change, document that you reviewed and found everything current.

This creates an audit trail that shows you're actively maintaining ground truth, not letting it drift. In regulated environments, this documentation matters.

### Quarterly Strategic Reviews: The Realignment

Every quarter, step back from the details and ask the big questions. Is your ground truth still aligned with your product strategy? Do your standards still reflect what your business actually needs? Are you measuring the right things?

This is the review where you might make major changes. Not tweaks to examples or clarifications to rubrics, but fundamental shifts in what "good" means for your system.

#### The Alignment Questions

Start with product alignment. Your product has a roadmap. Your ground truth should too. If the product is evolving toward more conversational interactions but your ground truth still rewards formal, structured responses, there's misalignment.

Sit with the product team and ask: "Where is this product going in the next six months? What will users expect? What will competitors be doing? What does success look like at the end of the year?"

Then ask: "Does our current ground truth support that vision, or is it anchored to where we were?"

Next, business alignment. Your company has goals — revenue, retention, efficiency, compliance, market share. Is your ground truth optimizing for those goals or for some abstract notion of quality?

If the business needs faster responses to handle more volume, but your ground truth rewards thoughtful, time-intensive answers, you're misaligned.

If the business needs to serve enterprise customers with strict compliance needs, but your ground truth was designed for consumer use cases, you're misaligned.

Finally, capability alignment. What can your AI actually do today, and what will it be able to do soon? Is your ground truth holding it back by optimizing for limitations that no longer exist? Or is it expecting capabilities you don't have yet?

#### The Strategic Changes

Quarterly reviews might result in major updates: new sections added to your rubric, old sections retired, threshold values significantly changed, entire categories of examples replaced.

These changes require more than just documentation. They require communication. Everyone who uses the ground truth needs to understand what changed and why.

They also require re-evaluation. If you make a strategic change to what "good" means, you need to re-assess recent evaluations under the new standard to understand the impact.

#### Who's in the Room?

Strategic reviews need leadership presence. Not just the team maintaining ground truth, but the people who set product direction, business priorities, and technical strategy.

This is where ground truth maintenance becomes visible to leadership as a strategic activity, not just operational overhead. The decisions made here affect what the AI optimizes for, which affects what users experience, which affects business outcomes.

## Triggers for Emergency Reviews

Beyond the regular cadence, some events should trigger immediate ground truth reviews, regardless of when the last scheduled review happened.

### Regulatory Changes

When a regulation that affects your system changes, review your ground truth within days, not weeks. You need to know if you're now measuring against non-compliant standards.

The EU AI Act required specific transparency and explainability measures for high-risk systems. When those requirements went into effect, any affected system needed an immediate ground truth review to ensure evaluations reflected the new requirements.

### Major Product Launches

When you ship a significant new feature or make a major change to how your system works, review the ground truth before launch, not after.

I've seen teams launch features and only then realize their ground truth doesn't cover the new functionality. They're flying blind on whether the new feature is working well because they have no standard to measure against.

### Compliance Incidents

If you have a compliance failure, incident, or near-miss, review your ground truth immediately. Often these incidents reveal that your standards weren't comprehensive enough, weren't clear enough, or weren't being followed.

The review should ask: "Would our ground truth have prevented this?" If not, what needs to change?

### Model Upgrades

When you upgrade to a new model or make significant changes to your prompts, architecture, or data pipeline, review your ground truth. The new system might be capable of things your ground truth doesn't measure, or it might struggle with things your ground truth doesn't check for.

### User Feedback Spikes

If you see a sudden increase in negative feedback, complaints, or escalations — especially if they're about similar issues — do an emergency spot check. Users might be telling you that your ground truth is out of sync with their needs.

## Making Reviews Sustainable

Here's the trap teams fall into: they commit to a review cadence, it feels like a lot of work, they start skipping reviews, and then they're back to unmanaged drift.

The key to sustainability is making reviews efficient, not avoiding them.

### Build Review Into Existing Workflows

Don't create separate meetings for ground truth reviews. Fold them into meetings you're already having.

Weekly spot checks can happen during regular ops reviews. Someone presents the ten examples and any findings in five minutes.

Monthly deep reviews can happen during sprint planning or monthly retrospectives. You're already gathering stakeholders to talk about what's working and what isn't.

Quarterly strategic reviews can align with quarterly business reviews or OKR planning sessions.

When ground truth review is an agenda item in existing meetings, it's harder to skip and easier to maintain.

### Use Tools to Reduce Manual Work

You don't need to manually track when reviews are due, manually select random examples, or manually assemble stakeholders. Build or buy tools that:

Automatically surface examples for spot checks based on diversity criteria.

Track the last review date for each section of your ground truth and alert when reviews are due.

Generate review reports showing what's changed since the last review.

Version control your ground truth so you can see diffs between reviews.

The less manual overhead, the more sustainable the practice.

### Rotate Responsibility

Don't make one person the "ground truth maintainer" who owns all reviews. Distribute the work.

Different team members do weekly spot checks on a rotating schedule. Different stakeholders lead different monthly reviews. This builds shared ownership and prevents burnout.

### Measure the Value

Track what reviews catch. When a spot check finds drift that gets fixed, note it. When a deep review prevents a compliance issue, note it. When a strategic review realigns ground truth with product direction and user satisfaction improves, note it.

Make the value of reviews visible so the work feels meaningful, not bureaucratic.

## What to Track Over Time

As you build a review cadence, start tracking trends that tell you how your ground truth is evolving.

### Change Velocity

How often are you updating ground truth? If it's constant, your ground truth might be too reactive or too vague. If it's never, you're not keeping up with reality.

Most healthy systems update ground truth every few weeks — frequently enough to stay current, infrequently enough that each change is meaningful.

### Drift Patterns

Where does drift happen most often? Certain sections of your ground truth might drift faster than others, which tells you where to focus attention.

If policy-related ground truth drifts every month but technical accuracy ground truth is stable for quarters, you might need different review cadences for different sections.

### Review Findings

What do reviews typically catch? Staleness? Gaps? Ambiguity? Conflicts? The pattern tells you what to watch for and potentially what to improve in how you create ground truth initially.

### Stakeholder Engagement

Who participates in reviews consistently, and who's hard to engage? This tells you where you might have communication or prioritization gaps.

If compliance never attends reviews and then you fail an audit, the problem wasn't the review cadence — it was stakeholder engagement.

## The Rhythm Becomes Habit

When you first implement a review cadence, it feels like extra work. You're adding meetings, creating new artifacts, involving more people. But after a few months, the rhythm becomes habit.

Teams start anticipating reviews. They flag ground truth issues as they notice them, knowing there's a forum to address them. They prepare for monthly reviews by gathering feedback. They use quarterly reviews to advocate for strategic changes.

And most importantly, they stop having the crisis of discovering major drift six months too late. They catch it early, fix it incrementally, and keep their ground truth aligned with reality.

The cadence isn't about perfection. It's about continuous course correction. Small adjustments at regular intervals keep you on track far better than occasional major overhauls.

In the next section, we'll talk about how to version your ground truth properly, so you can track these changes over time, understand what changed when, and maintain historical comparability even as your standards evolve.
