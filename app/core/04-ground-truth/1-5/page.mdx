# 1.5 — Task-Specific Truth (No Universal Standard)

Here's a mistake I see constantly: teams define ground truth once and try to apply it everywhere.

They'll say something like: "Our ground truth is that responses must be accurate, concise, and professional." Then they use that standard to evaluate everything—customer support, creative content, technical documentation, internal chatbots, and user-facing assistants.

It doesn't work. What's "correct" for a creative brainstorming session is completely wrong for a regulatory compliance check. What's "professional" for a legal memo is stiff and weird for a casual chatbot helping someone pick a restaurant.

There is no universal ground truth. Ground truth is always task-specific. Understanding this changes everything about how you build evaluation systems.

## The Creative vs Compliance Example

Let me show you why universal standards fail with two products from the same company.

Product A is a creative writing assistant. It helps novelists brainstorm plot ideas, develop characters, and explore narrative possibilities. Users want surprising suggestions, unconventional connections, and ideas that push beyond the obvious.

Product B is a regulatory compliance assistant. It helps lawyers ensure contract language meets legal requirements. Users want conservative accuracy, established precedents, and zero creative interpretation.

If you define ground truth as "factually accurate," Product A becomes boring and useless. Creative writing isn't about facts—it's about possibilities.

If you define ground truth as "creative and surprising," Product B becomes dangerous. Compliance work isn't about creativity—it's about precise adherence to rules.

These products need opposite ground truth definitions even though they're both "AI writing assistants." The task determines the truth standard.

## Eight Product Types, Eight Truth Standards

Let me walk you through eight common AI product types and show how ground truth differs radically for each.

Type 1: Factual Question Answering

Example: A customer asking "What's your return policy?"

Ground truth priority: Accuracy above all. The answer must match documented policy exactly. Creativity is harmful. Conciseness is good but not at the expense of completeness.

Correctness means: Matches official policy documents word-for-word when necessary.
Safety means: Doesn't make promises the company can't fulfill.
Usefulness means: Directly answers the specific question without requiring follow-up.

Type 2: Creative Assistance

Example: A user asking "Help me come up with a name for my bakery."

Ground truth priority: Usefulness and variety. The suggestions should be interesting, diverse, and spark ideas. Factual accuracy barely applies—these are invented names.

Correctness means: Names are linguistically valid and pronounceable.
Safety means: Names aren't offensive or trademarked.
Usefulness means: Names fit the style the user wants and inspire further thinking.

Type 3: Diagnostic Troubleshooting

Example: A user reporting "My internet isn't working."

Ground truth priority: Systematic problem-solving. Responses should follow a diagnostic tree, rule out causes, and lead to resolution.

Correctness means: Technical steps are valid for the described symptoms.
Safety means: Steps don't risk breaking things further.
Usefulness means: Gets the user to a solution in minimum steps.

Type 4: Educational Explanation

Example: A student asking "How does photosynthesis work?"

Ground truth priority: Pedagogical effectiveness. The explanation should be accurate but also match the learner's level and build understanding.

Correctness means: Scientific facts are accurate according to current knowledge.
Safety means: Doesn't discourage learning or create misconceptions.
Usefulness means: Pitched at the right level and builds on what the student likely knows.

Type 5: Persuasive Content

Example: A marketer requesting "Write a product description for our new headphones."

Ground truth priority: Conversion effectiveness. The content should be compelling, accurate enough to avoid false advertising, and drive action.

Correctness means: Product features and specs are accurately described.
Safety means: Claims are defensible and comply with advertising standards.
Usefulness means: Actually drives purchase consideration and clicks.

Type 6: Analytical Summary

Example: An analyst asking "Summarize this 50-page earnings report."

Ground truth priority: Faithful compression. The summary should capture key information without distortion or editorial interpretation.

Correctness means: All statements can be traced to specific parts of the source.
Safety means: Doesn't omit material information that changes conclusions.
Usefulness means: Highlights the most decision-relevant information.

Type 7: Conversational Companion

Example: A user chatting casually about their day.

Ground truth priority: Engagement and empathy. The conversation should feel natural, show understanding, and keep the user engaged.

Correctness means: Factual claims, when made, are accurate.
Safety means: Doesn't encourage unhealthy behaviors or create inappropriate attachment.
Usefulness means: User feels heard and enjoys the interaction.

Type 8: Specialized Professional Tool

Example: A radiologist using AI to analyze medical images.

Ground truth priority: Clinical accuracy with appropriate confidence calibration. The tool should match or exceed specialist-level diagnosis and clearly indicate confidence.

Correctness means: Diagnoses match expert consensus on ground truth labeled data.
Safety means: False negatives for serious conditions are minimized, and uncertainty is flagged.
Usefulness means: Saves the radiologist time while maintaining or improving diagnostic accuracy.

Notice how different these are. What matters for creative assistance actively hurts compliance checking. What matters for conversational engagement is irrelevant for analytical summary.

## The Multi-Task Product Problem

Most products aren't just one task. They're multiple tasks under one interface.

A customer service chatbot might handle:
- Factual questions (account balance)
- Troubleshooting (login not working)
- Policy explanations (how does billing work)
- Complaint handling (I'm angry about this charge)
- Transactional requests (change my password)

Each task needs different ground truth criteria. "Concise and factual" works for account balance. It fails miserably for complaint handling, where empathy and acknowledgment matter more than brevity.

Elite teams don't define one ground truth for the whole product. They define ground truth per task type and classify each interaction.

Here's what that looks like:

First, categorize your product's tasks. What are the different types of things users might ask for?

Second, define ground truth for each task category separately. What does "correct" mean for this specific task?

Third, build classifiers or rules to identify which task type an interaction is. Tag your eval data with task types.

Fourth, measure performance separately per task type. Don't average across tasks—report breakdowns.

This approach reveals problems that universal metrics hide. You might be at 90% overall but only 60% on complaint handling. That's actionable. A single 90% score hides the problem.

## The Tone Spectrum

One dimension where task-specific truth becomes obvious is tone. What's "correct" tone varies wildly:

For a funeral home chatbot: somber, respectful, gentle.
For a kids' homework helper: encouraging, simple, playful.
For a financial fraud alert: urgent, clear, serious.
For a fitness motivation app: energetic, challenging, supportive.
For a legal contract assistant: formal, precise, unemotional.

If you define ground truth as "professional tone" across all these, you'll fail everywhere. Professional means completely different things in different contexts.

Your ground truth definition must specify tone appropriate to the task, not tone in the abstract.

## The Precision-Recall Trade-off by Task

Different tasks have different optimal precision-recall balances, which affects ground truth:

For medical diagnostics (screening for serious conditions): prioritize recall. False negatives are dangerous. You'd rather have false positives than miss a real condition.

For spam filtering: prioritize precision. False positives are very costly (missing important email). False negatives are tolerable (a bit of spam gets through).

For content moderation: balance depends on your values. High precision means you only remove things you're very sure violate policy (some bad content stays). High recall means you catch more violations (but wrongly remove some okay content).

Your ground truth definition must specify the acceptable precision-recall trade-off for each task.

## The Completeness-Conciseness Trade-off by Task

Different tasks need different completeness levels:

For an emergency instruction bot ("what do I do if someone is choking"): extreme conciseness. Every second counts. Give the essential steps only.

For a tax filing assistant: high completeness. Missing a requirement could cause legal problems. Better to be thorough than brief.

For a casual recipe suggestion: moderate conciseness. Enough detail to be useful, but don't overwhelm someone who just wants a quick idea.

For a contract review tool: maximum completeness. Every clause matters. Nothing can be omitted.

If your ground truth just says "responses should be concise," you'll fail on tasks that need completeness.

## The Authority-Hedging Spectrum by Task

How much should your AI hedge its statements? It depends on the task:

For documented facts (product specifications): state with confidence. No hedging needed. "This model has 256GB storage."

For probabilistic predictions (weather forecast): calibrated confidence. "There's a 70% chance of rain."

For medical advice: strong hedging and disclaimers. "This could indicate X, but you should consult a doctor."

For creative suggestions: no hedging needed. "You could try naming it Sunrise Bakery."

For legal interpretation: heavy hedging. "This clause might be interpreted as X, but consult your attorney."

Universal ground truth can't capture this. You need task-specific standards for when to hedge and when to state confidently.

## The Novelty-Reliability Spectrum by Task

Some tasks reward novelty. Others punish it.

For brainstorming product features: prioritize novelty. Users want unexpected ideas, not the obvious ones they already thought of.

For generating SQL queries: prioritize reliability. Users want the query that definitely works, not a creative interpretation.

For writing marketing copy: moderate novelty. Familiar enough to be clear, novel enough to be interesting.

For filling out tax forms: zero novelty. Exactly by-the-book, nothing creative.

Your ground truth must specify where on this spectrum each task belongs.

## Industry-Specific Task Differences

Even the same task type differs across industries:

"Answer a customer question" means:

In healthcare: prioritize safety, err toward professional consultation, use careful language.
In e-commerce: prioritize speed, get to the answer fast, use casual friendly language.
In banking: prioritize accuracy, include required disclosures, use formal language.
In gaming: prioritize engagement, add personality, use playful language.

The task is "answer a question," but ground truth is completely different because the industry context changes what users need and expect.

## Geographic and Cultural Task Differences

The same task in different regions needs different ground truth:

A customer service bot in Japan needs extreme politeness and formal language that would feel stiff and weird in Australia.

A healthcare bot in the US needs different privacy disclaimers than in the EU (HIPAA vs GDPR).

A financial advice bot in markets with different regulations needs region-specific ground truth for what advice is legal to give.

If you're building for multiple markets, you need market-specific ground truth definitions, not one global standard.

## The User Expertise Spectrum

Ground truth differs based on who the user is:

For an expert user asking a technical question: assume knowledge, use precise terminology, skip background explanation.

For a novice user asking the same question: define terms, provide context, explain step-by-step.

For a mixed audience: either adapt to detected expertise level or target a middle ground.

Some products know user expertise level (internal tools for specific roles). Others don't (public-facing chatbots). Your ground truth must account for this.

## Time-Sensitive vs Timeless Tasks

Some tasks have tight time constraints that affect ground truth:

For real-time crisis support: speed matters enormously. An okay answer in 2 seconds beats a perfect answer in 30 seconds.

For research synthesis: speed barely matters. A perfect answer in a day beats a hasty answer in an hour.

For breaking news updates: recency matters more than depth. Surface the latest information quickly.

For historical analysis: depth matters more than recency. Provide comprehensive context.

Your ground truth must account for time sensitivity of the task.

## The Personalization Dimension

Some tasks require personalization. Others don't:

Product recommendations: must personalize. What's right for one user is wrong for another.

Factual information: usually doesn't personalize. The return policy is the same for everyone.

Fitness advice: must personalize. Different people need different programs.

Weather forecast: personalizes by location but not by person.

If your ground truth says "accurate and helpful" but doesn't specify when to personalize and when not to, you'll get it wrong.

## Building Task-Specific Ground Truth

Here's how to operationalize this:

Step 1: Enumerate your tasks. List every meaningfully different type of task your product handles.

Step 2: For each task, define the three-layer stack (correct, safe, useful) with task-specific criteria.

Step 3: Specify the trade-offs for each task: precision vs recall, completeness vs conciseness, authority vs hedging, novelty vs reliability.

Step 4: Document task-specific tone, formality, and style requirements.

Step 5: Build a task classification system so you can route inputs to the right ground truth definition.

Step 6: Evaluate performance per task type separately and identify where each task type succeeds or fails.

This is more work than defining one universal standard. It's also the only way to build something that actually works across all your use cases.

## The Common Mistake

The most common mistake is defining ground truth at the product level instead of the task level.

"Our product's ground truth is accurate, helpful, and professional."

That's meaningless. Accurate how? Helpful for what goal? Professional in what context?

Elite teams define ground truth at the task level:

"For factual support questions, ground truth is: matches documentation exactly, addresses the specific question, includes no unnecessary information."

"For complaint handling, ground truth is: acknowledges the customer's frustration, explains what went wrong, offers a specific remedy, follows escalation policy when appropriate."

Notice the difference. Task-specific ground truth is testable and actionable. Product-level ground truth is vague and unusable.

## What This Means For Your Eval Set

Your evaluation dataset should be stratified by task type with enough examples of each to measure performance independently.

If you have 1,000 eval examples but 10 task types, you need at least 100 examples per type (more for important or difficult types).

If you have one task that's 5% of traffic but 50% of complaints, you need over-sampling for that task in your eval set.

You should report metrics per task type, not just overall. "We're at 92% overall" is less useful than "We're at 98% on factual questions, 87% on troubleshooting, and 73% on complaint handling."

The per-task breakdown tells you where to focus improvement.

## The Bottom Line

There is no universal ground truth because there is no universal task. What's correct for creative work is wrong for compliance work. What's helpful for experts is confusing for novices. What's appropriate in one culture is offensive in another.

If you're trying to define one ground truth for everything your product does, you're setting yourself up for mediocrity across the board. You'll be pretty good at nothing instead of excellent at each specific thing.

Define ground truth per task type. Measure per task type. Improve per task type. That's how you build products that actually work for the full diversity of what users need.

In the next section, we'll address another universal truth about ground truth: it's not static. Products evolve, regulations change, user expectations shift. Understanding that ground truth is a living document, not a set-and-forget artifact, changes how you build systems that stay relevant over time.
