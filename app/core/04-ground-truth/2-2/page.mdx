# 2.2 — Tier 0: Zero Tolerance (Irreversible Actions)

Picture this: you're sitting in a hospital room watching a nurse prepare to administer medication to your child. The AI-assisted dosage system has calculated the amount. The nurse is about to inject it. You have one question: "Are you absolutely certain this is correct?"

"Pretty sure" isn't an acceptable answer. "Probably right" isn't acceptable. "We have a 95% accuracy rate" isn't acceptable. You want zero ambiguity. The system must be right, or it must refuse to act.

This is Tier 0 — the highest stakes category where mistakes cannot be undone and consequences can be catastrophic. We're not talking about inconvenience or annoyance. We're talking about financial ruin, data loss, legal liability, physical harm, or death.

## What Makes Something Irreversible

Not all mistakes are created equal. Some you can fix with an apology. Some require a support ticket. Some cannot be fixed at all.

Tier 0 is defined by irreversibility. Once the action happens, you cannot undo it or the cost of undoing it is catastrophic.

**Medical examples:**
- Administering a drug dosage
- Executing a surgical plan
- Deactivating life support
- Dispensing controlled substances

**Financial examples:**
- Executing wire transfers
- Trading securities
- Processing loan approvals
- Disbursing insurance claims

**Legal examples:**
- Filing court documents
- Executing contracts
- Submitting regulatory reports
- Issuing official statements

**Data examples:**
- Permanent data deletion
- Releasing encryption keys
- Publishing confidential information
- Revoking access credentials in critical systems

**Physical safety examples:**
- Operating autonomous vehicles
- Controlling industrial equipment
- Managing building safety systems
- Dispatching emergency services

Notice what these have in common: once executed, the consequences are in motion. You can't un-send a wire transfer. You can't un-administer medication. You can't un-delete data after the backup window has passed. You can't un-file a legal document with the court.

This is fundamentally different from actions that can be reversed, corrected, or compensated for. Tier 0 actions are one-way doors.

## The Zero Tolerance Principle

In Tier 0, ground truth isn't about achieving high accuracy. It's about achieving certainty or refusal.

The principle: the system must be absolutely correct, or it must not act at all. There is no middle ground. "Probably right" is not acceptable. "Right most of the time" is not acceptable. Each individual decision must be certain.

This is counterintuitive for people coming from traditional machine learning where we celebrate 95% accuracy. In Tier 0, 95% accuracy means one in twenty actions is wrong — one in twenty patients gets the wrong dosage, one in twenty wire transfers goes to the wrong account, one in twenty data deletion commands destroys the wrong database.

That's not a quality metric — it's a disaster metric.

## Better to Refuse Than to Guess

The core strategy for Tier 0 ground truth is the refusal protocol: when the system cannot determine the correct action with certainty, it refuses to act and escalates to a human.

This sounds like it would make AI useless in Tier 0 scenarios. Won't it just refuse everything?

No — because refusal is itself a correct response. A well-designed Tier 0 system operates in three modes:

1. **High confidence, clearly correct:** Execute the action
2. **High confidence, clearly incorrect:** Refuse and explain why
3. **Low confidence or ambiguous:** Refuse and escalate

Mode 3 is not a failure state — it's a safety state. The system is doing exactly what it should: recognizing its own uncertainty and deferring to human judgment.

Let me give you a medical example. An AI system helps pharmacists verify prescription dosages. For a standard prescription — common medication, typical dosage, no interaction warnings — the system confirms: "This is correct, proceed." For a clearly wrong prescription — dosage exceeds safety limits, known drug interaction — the system flags it: "Do not dispense, this prescription has a critical safety issue." For an ambiguous case — unusual but potentially valid dosage, unclear patient history — the system escalates: "I cannot verify this prescription, pharmacist review required."

All three responses are correct. The third isn't a limitation — it's appropriate caution.

## What Zero Tolerance Ground Truth Looks Like

In Tier 0, your ground truth dataset cannot have ambiguous examples. Every example must have one of three labels:

1. **Definitely correct:** The action should be executed
2. **Definitely incorrect:** The action should be refused
3. **Uncertain:** The system should escalate to human review

Notice that label 3 is not a failure to label properly — it's a valid label. Some scenarios are inherently uncertain, and ground truth should reflect that uncertainty.

Here's what this means in practice for ground truth creation:

**Single annotators are not acceptable.** Every Tier 0 example must be reviewed by multiple expert annotators. If experts disagree, that's data — the example is uncertain and should be labeled as such.

**Annotators must be domain experts.** You cannot have a general contractor annotate medical dosages or legal documents. The stakes are too high for learning on the job.

**Edge cases need exhaustive coverage.** In lower tiers, you can sample edge cases. In Tier 0, you need to identify and label every edge case you can find. The goal is not representative sampling — it's comprehensive coverage of failure modes.

**Ambiguity must be explicit.** If your annotators aren't sure, that uncertainty must be captured in the label. "I think this is probably right" becomes "Uncertain — escalate."

**Validation must include consequences.** Annotators should consider not just whether the action is correct in isolation, but whether any reasonable interpretation could lead to harm.

## The Mandatory Human-in-the-Loop Pattern

Tier 0 systems should never operate fully autonomously. There must always be a human who can intervene, verify, or override.

This doesn't mean humans do all the work — that would eliminate the value of AI. It means humans are positioned to catch errors before irreversible consequences occur.

**Pre-action confirmation:** Before executing a Tier 0 action, the system requires human confirmation. A wire transfer system shows the details and requires explicit approval. A medical system displays the dosage calculation and requires a clinician to verify.

**Active verification, not passive approval:** The human must actively verify the action, not just click through a prompt. Good confirmation UX requires the human to engage with the details — read the transfer amount, confirm the recipient, verify the dosage matches the prescription.

**Audit trail creation:** Every Tier 0 action must log who approved it, when, what information they reviewed, and what decision they made. This isn't just for debugging — it's for accountability and legal compliance.

**Override capability:** Humans must be able to override the AI's recommendation, in both directions. If the AI says "execute" and the human spots a problem, they can refuse. If the AI says "refuse" and the human has additional context that makes it safe, they can proceed.

The pattern is: AI proposes, human disposes. The AI does the heavy lifting — calculation, data lookup, pattern matching — but the human makes the final call.

## Confirmation Protocols That Actually Work

"Are you sure?" prompts don't work. People click through them without reading. Effective confirmation protocols require cognitive engagement.

**Display consequences explicitly:** Don't just show the action, show the outcome. For a wire transfer, show: "This will send $50,000 from your business account to John Smith's account at Chase Bank. This action cannot be undone."

**Require detail verification:** Make the human confirm specific details, not just general approval. "Please verify the transfer amount: $___" where they must type the amount, not just click yes.

**Slow down high-risk actions:** Introduce friction proportional to risk. A $100 transfer might require one click. A $100,000 transfer might require typing the amount, confirming the recipient, and waiting 30 seconds for a cooldown period.

**Make uncertainty visible:** If the AI is uncertain, show why. "I cannot verify this prescription because the dosage is higher than typical for this patient's weight. Pharmacist review required."

**Separate decision from execution:** The person who approves the action should not be the same system component that executes it. Approval creates an authorization token, which a separate execution system validates and acts on. This separation prevents accidental execution.

## Audit Trails for Tier 0

Every Tier 0 action needs a complete audit trail. Not "nice to have" — absolutely required. Here's what that trail must capture:

**What action was taken:** The exact action, with all parameters. Not "transferred money" but "transferred $50,000 from account 1234-5678 to account 8765-4321 at 2026-01-30 14:32:15 UTC."

**What information informed the decision:** What data did the AI consider? What did the human see when they approved? If there's a discrepancy later, you need to know what was known at decision time.

**Who approved it:** The human who confirmed the action, with authentication. Not just a user ID — a verified identity tied to an authenticated session.

**What the AI recommended:** Did the AI say "execute," "refuse," or "escalate"? If the human overrode the AI, that's critical information.

**Any uncertainty flags:** If the AI indicated uncertainty or if the human added a note, that context is part of the record.

**Outcome verification:** After the action, what happened? For a wire transfer, confirmation from the bank that funds moved. For a medical dosage, confirmation of administration. The loop must close.

This audit trail serves three purposes: debugging (when something goes wrong, you can reconstruct what happened), compliance (regulators want to see decision-making processes), and improvement (patterns in human overrides tell you where the AI needs work).

## Real Example: Financial Transaction Processing

Let me walk you through what Tier 0 ground truth looks like in a real financial system I worked with — an AI that helps process wire transfer requests.

**The task:** Users submit wire transfer requests through a web form. The AI validates the request, checks for fraud indicators, verifies account balances, and either approves for processing or flags for review.

**Why this is Tier 0:** Wire transfers are irreversible. Once sent, funds cannot be recalled without recipient cooperation. Mistakes mean money goes to the wrong account, potentially forever.

**Ground truth structure:** Each example in the dataset is a transfer request with complete details — amount, sender account, recipient account, routing information, transfer reason. The label is one of three:

1. **Approve:** All details valid, no fraud indicators, sufficient funds, standard transfer pattern
2. **Reject:** Invalid account, fraud indicators present, insufficient funds, regulatory violation
3. **Escalate:** Unusual but potentially legitimate (new recipient, large amount, international transfer to high-risk country)

**Annotator requirements:** Annotators must be certified anti-fraud specialists with banking experience. They review each request against bank policies, fraud databases, and regulatory requirements.

**Multi-annotator protocol:** Every example is labeled by three annotators independently. Agreement is required for "Approve" labels — if any annotator says "Escalate" or "Reject," that's the label. This is asymmetric labeling: we err on the side of caution.

**Edge case focus:** The dataset over-samples unusual scenarios — first-time large transfers, international wires, business account transfers to personal accounts, round-number amounts (fraud indicator), transfers to recently added recipients. Common, straightforward transfers are under-sampled because the AI handles them easily. The ground truth focuses on the hard cases.

**Uncertainty documentation:** For every "Escalate" label, annotators document why. "Large amount for this account's typical pattern" or "Recipient in country with elevated fraud risk" or "Business account sending to personal account with unclear purpose." This gives the AI training signal about what uncertainty looks like.

**Validation protocol:** Before the AI recommendation is shown to the human operator, the system displays: the AI's recommendation, the confidence level, any uncertainty factors, and the full request details. The operator must review and either confirm or override.

**Audit trail:** Every decision logs: the request details, AI recommendation, operator decision, timestamp, operator ID, and any override reason. This record is retained for seven years per financial regulations.

**The result:** The system approves about 70% of requests automatically (high-confidence, clearly legitimate), rejects about 5% automatically (clear fraud indicators), and escalates about 25% for human review. The human review rate is high, but that's appropriate — these are irreversible financial transactions. The 70% automation still saves enormous time while maintaining safety on the remaining 30%.

## Healthcare Example: Medication Dosage Verification

Another Tier 0 scenario: an AI that helps verify medication dosages in hospital settings.

**The task:** A physician writes a prescription. The system checks the dosage against the patient's weight, age, kidney function, liver function, other medications, allergies, and clinical guidelines. It either confirms the prescription is safe or flags it for review.

**Why this is Tier 0:** Medication errors can be fatal. An overdose can cause organ damage or death. An underdose can fail to treat the condition, allowing disease progression.

**Ground truth structure:** Each example is a prescription in context — patient demographics, medical history, current medications, lab results, the proposed medication and dosage. Labels:

1. **Safe:** Dosage is within guidelines for this patient
2. **Unsafe:** Dosage exceeds safety limits or has dangerous interactions
3. **Uncertain:** Dosage is unusual but might be justified by patient-specific factors

**Annotator requirements:** Clinical pharmacists with hospital experience. They must understand both the pharmacology and the clinical context.

**Multi-expert review:** Every "Safe" label requires agreement from two pharmacists. Every "Unsafe" label requires agreement from two pharmacists plus documentation of the specific safety issue. "Uncertain" labels require three pharmacists, and if they disagree, it's automatically "Uncertain."

**Edge cases:** The dataset includes rare medications, pediatric cases (where dosing is complex), geriatric cases (altered metabolism), renal failure patients (altered clearance), drug interactions, off-label uses, and maximum-dose scenarios. These are over-sampled because they're where errors happen.

**Outcome tracking:** The system tracks whether prescriptions labeled "Unsafe" were indeed dangerous (confirmed by adverse event reports) and whether "Safe" prescriptions had no issues (confirmed by absence of adverse events). This creates a feedback loop for ground truth refinement.

**Human verification protocol:** When the AI flags a prescription as potentially unsafe, it shows the pharmacist: the specific safety concern, the relevant clinical guideline, the patient factors that triggered the alert, and alternative dosing suggestions. The pharmacist must review and either agree (prescription is modified) or override (with documentation of why the unusual dosage is justified).

**The result:** The system confirms about 85% of prescriptions as safe, flags about 5% as clearly unsafe, and marks about 10% as uncertain. The uncertain cases get pharmacist review. Of those reviews, about half are modified (the AI was right to be cautious) and half are approved as-is (the unusual dosage was justified by patient factors). The system has prevented dozens of medication errors while reducing pharmacist workload on routine verifications.

## Legal Document Filing Example

A third Tier 0 scenario: an AI that helps prepare and file legal documents with courts.

**The task:** Generate court filings based on case information, ensuring compliance with court rules, filing deadlines, procedural requirements, and legal standards.

**Why this is Tier 0:** Court filings create legal obligations and deadlines. A missed filing deadline can forfeit a case. An incorrect filing can waive rights or create liability. Once filed, documents become part of the official record and can only be amended through formal legal processes.

**Ground truth structure:** Each example is a case scenario with full context — case type, jurisdiction, procedural stage, client goals, relevant facts. The document draft is labeled:

1. **Correct:** Meets all legal requirements, ready to file
2. **Incorrect:** Contains legal errors, procedural violations, or factual inaccuracies that would harm the case
3. **Requires attorney review:** Legally complex, strategic implications, novel legal issues, or uncertain procedural requirements

**Annotator requirements:** Licensed attorneys in the relevant jurisdiction with litigation experience. Paralegals can assist with procedural checks, but legal substance must be reviewed by attorneys.

**Multi-attorney review:** Every example is reviewed by two attorneys independently. Agreement is required for "Correct" labels. Any "Incorrect" label requires documentation of the specific legal error.

**Edge case coverage:** The dataset includes complex procedural scenarios, multi-party cases, jurisdictional edge cases, novel legal arguments, procedural deadline conflicts, and recent changes in legal standards. These represent the situations where errors are most likely.

**Attorney verification:** Before filing, the system requires an attorney to review: the complete document, the legal authorities cited, the procedural requirements verified, and any uncertainty flags. The attorney must sign off electronically, creating a record of professional responsibility.

**The result:** The system drafts about 60% of routine filings correctly without modification, flags about 10% as having clear errors, and marks about 30% as requiring attorney review for strategic or complex issues. The attorney review rate is high because legal work is inherently complex and stakes are high. But that 60% automation on routine filings saves enormous time.

## The "Refuse and Explain" Pattern

When a Tier 0 system cannot act with certainty, refusal isn't enough — it must explain why.

A system that just says "I cannot process this request" creates frustration and doesn't help the human understand the problem. A system that says "I cannot process this wire transfer request because the recipient account number appears invalid" gives actionable information.

Good refusal explanations include:

**What the system cannot verify:** "I cannot verify this prescription because the dosage is higher than standard for this patient's weight"

**Why uncertainty exists:** "The recipient account is new and hasn't been used before, which is a common fraud indicator"

**What action the human should take:** "Please verify the account number and confirm this is a legitimate new recipient"

**What information would resolve uncertainty:** "If you can provide documentation of the patient's previous tolerance for high dosages, that would help verify this prescription"

The pattern is: refuse, explain, suggest. This turns a dead end into a path forward.

## When Tier 0 Systems Should Not Be Built

Not every high-stakes task should have an AI system, even with zero tolerance standards.

If the task is so critical that any system uncertainty is unacceptable and human review of every case is required anyway, AI adds complexity without value. You're better off with pure human decision-making.

If the ground truth is so ambiguous that even experts cannot agree on correct answers, you cannot build reliable Tier 0 AI. The foundation isn't solid enough.

If the consequences of an error are so catastrophic that no amount of review and confirmation would be acceptable risk, the task shouldn't be automated. Some decisions are too important for AI assistance.

Tier 0 is for high-stakes tasks where AI can provide value through calculation, pattern matching, data synthesis, or procedural verification — but where ultimate authority and accountability remain with humans.

## The Liability Question

Who is liable when a Tier 0 AI system makes a mistake that a human approved?

This is an evolving legal landscape, but the current general principle: the human who approved the action retains responsibility. The AI is a tool, the human is the decision-maker.

This is why Tier 0 systems must have clear human-in-the-loop protocols and audit trails. If a mistake happens, the question becomes: did the human have adequate information to make an informed decision? Did the AI properly disclose uncertainty? Was the human qualified to verify the action?

If the AI hid uncertainty, misrepresented confidence, or failed to flag a known issue, there may be AI system liability. If the AI properly disclosed all relevant information and the human approved anyway, the liability is human.

This legal reality reinforces why Tier 0 ground truth must include uncertainty labels and why systems must disclose confidence. It's not just good engineering — it's legal protection.

## Regulatory Requirements for Tier 0

In 2026, Tier 0 systems are generally classified as high-risk under the EU AI Act and require:

**Conformity assessment:** Third-party evaluation that the system meets safety and quality requirements

**Data governance:** Documented data quality standards, including ground truth provenance, annotator qualifications, and validation procedures

**Human oversight:** Mandatory human-in-the-loop with qualified operators

**Accuracy requirements:** Not specific percentage targets, but documented standards for what constitutes acceptable performance

**Risk management:** Documented processes for identifying, evaluating, and mitigating risks

Your Tier 0 ground truth dataset is a key part of demonstrating compliance. Regulators will ask: How did you ensure ground truth quality? Who labeled the data? How did you handle ambiguous cases? Can you show that your system was trained on data appropriate for high-risk use?

Having clear answers — expert annotators, multi-reviewer protocols, uncertainty labeling, edge case coverage — is not just good practice, it's regulatory evidence.

## If You Skip This, Here's What Happens

Teams that treat Tier 0 tasks with lower-tier standards face catastrophic risks:

**People get hurt.** Medication errors, financial losses, legal liabilities, data breaches — the stakes are real, not theoretical.

**Regulatory enforcement.** High-risk AI systems that don't meet quality standards face fines, injunctions, and bans.

**Reputation destruction.** When your Tier 0 system makes a mistake that harms someone, the news story isn't "AI made an error" — it's "Company deployed unsafe AI system." Recovery is difficult.

**Legal liability.** If you deployed a system with inadequate ground truth standards and it caused harm, that's evidence of negligence.

**Loss of trust.** Users won't give you a second chance with Tier 0 tasks. One serious error and they'll demand pure human processes.

The path forward is clear: Tier 0 tasks require zero tolerance ground truth with expert annotation, multi-reviewer validation, uncertainty labeling, human-in-the-loop protocols, and comprehensive audit trails. It's expensive and slow, but it's appropriate for stakes this high.

Next, we'll look at Tier 1, where mistakes are costly but reversible — and the ground truth standards shift accordingly.
