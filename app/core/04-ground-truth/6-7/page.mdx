# 6.7 — Code Generation

Let me tell you about the most dangerous line of code I ever saw an AI generate. A developer asked an AI coding assistant to "write a function to execute user queries on the database." The AI produced clean, working code that executed SQL queries. It was syntactically perfect. It ran without errors. It was also a massive SQL injection vulnerability that would have let attackers delete the entire database.

The developer almost shipped it. They tested that it worked (it did) and moved on. They didn't test that it was secure.

This is the deceptive challenge of code generation ground truth. Unlike other AI tasks where "works" equals "correct," code can work perfectly and still be catastrophically wrong. It can be functional but insecure. Correct but inefficient. Working but unmaintainable. Successful but violating every coding standard in your organization.

Your ground truth needs to evaluate not just "does it run?" but a multi-layered definition of "good code."

## The Layers of Code Correctness

Let's break down what makes code actually correct, because your ground truth needs to check all of these layers:

**Layer 1: Syntactic Correctness**

Does the code compile or parse? This is the bare minimum. Python code should be valid Python. JavaScript should parse without syntax errors.

This is easy to check automatically. Run it through the compiler or interpreter. If it fails, it's wrong.

But passing this layer means almost nothing. Syntactically correct code can do literally anything, including nothing.

**Layer 2: Functional Correctness**

Does the code produce the right output for the given input? If you asked for a function that sorts a list, does it actually sort the list?

This requires test cases. Your ground truth needs input-output pairs to verify behavior.

Example:
Request: "Write a function to check if a string is a palindrome"

Ground truth test cases:
- is_palindrome("racecar") → True
- is_palindrome("hello") → False
- is_palindrome("A man a plan a canal Panama") → True (case-insensitive)
- is_palindrome("") → True (empty string is palindrome)
- is_palindrome("a") → True (single character)

If the generated code passes all test cases, it's functionally correct for these inputs.

But here's the catch: you can't test all possible inputs. Maybe it works for these cases but fails on others. Ground truth testing is always a sample, not exhaustive.

**Layer 3: Edge Case Handling**

Does the code handle edge cases correctly? Empty inputs, null values, boundary conditions, invalid inputs?

This requires adversarial test cases designed to break the code:

- Very large inputs (what if the list has a million items?)
- Null or undefined inputs
- Invalid types (what if a number is passed where a string is expected?)
- Boundary values (max int, min int, zero, negative numbers)
- Special characters (unicode, emojis, control characters in strings)

Your ground truth should include edge cases and verify the code handles them gracefully (either processes correctly or fails with appropriate error handling).

**Layer 4: Security**

Does the code have vulnerabilities? SQL injection, XSS, buffer overflows, insecure randomness, hardcoded credentials, etc.?

This is hard to check automatically but critical. Your ground truth should flag common security issues:

**Input Validation**: Does the code validate user input before using it?

**SQL Injection**: Does it use parameterized queries or does it concatenate user input into SQL?

**XSS Prevention**: Does it escape output when rendering user content?

**Authentication**: Does it check permissions before sensitive operations?

**Secrets Management**: Does it avoid hardcoding passwords, API keys, etc.?

**Cryptography**: Does it use secure algorithms and proper implementations?

For ground truth, you can run static analysis tools (linters with security rules) and manually review for common vulnerabilities.

**Layer 5: Efficiency**

Is the code efficient, or does it use wasteful algorithms?

A function that sorts a list by checking every possible permutation is correct but absurdly inefficient. Your ground truth should prefer efficient solutions.

Efficiency metrics:
- Time complexity (Big O notation)
- Space complexity (memory usage)
- Appropriate data structures

This is tricky for ground truth because sometimes "good enough" efficiency is fine, and premature optimization is bad. But egregiously inefficient code (O(n!) when O(n log n) is possible) should be flagged.

**Layer 6: Readability and Maintainability**

Is the code clear, well-structured, and easy to understand?

This is subjective but important. Code is read far more often than it's written. Clever, obfuscated code is bad even if it works.

Ground truth for readability:
- Are variables named clearly?
- Is the code organized logically?
- Are functions appropriately sized (not too long)?
- Is there appropriate documentation?
- Does it follow language idioms?

Use human reviewers for this. Readability is a human judgment.

**Layer 7: Idiomatic and Style Compliance**

Does the code follow the conventions and style of the language and project?

Python code should look like Python (PEP 8), not like Java written in Python. React code should follow React patterns. Your internal codebase might have specific conventions.

Ground truth checks:
- Does it pass your linter?
- Does it follow your team's style guide?
- Does it use language features appropriately (not avoiding useful features or overusing clever ones)?

This is automatically checkable with linters and formatters.

## Building Test-Based Ground Truth

The most reliable ground truth for code generation is test-based: you write tests that the generated code must pass.

**Unit Tests**: Test individual functions in isolation

Example:
Request: "Write a function to calculate factorial"

Ground truth unit tests:
```
assert factorial(0) == 1
assert factorial(1) == 1
assert factorial(5) == 120
assert factorial(10) == 3628800
```

The generated code must pass all tests to be considered correct.

**Integration Tests**: Test how code interacts with other components

If generating code that calls an API, your ground truth includes tests that verify the API calls are correct (using mocks or test APIs).

**Property-Based Tests**: Instead of specific input-output pairs, test properties that should always hold

Example: "For any list, the sorted output should have the same length and elements as the input"

This catches bugs that specific test cases might miss.

Your ground truth should include comprehensive test suites. The more tests, the more confident you can be that the code is correct.

## The Test Coverage Challenge

Here's a hard truth: you can't test everything. So what's "enough" testing for ground truth?

**Critical Path Coverage**: At minimum, test the main use case. If code should sort a list, test that it sorts a standard list.

**Edge Case Coverage**: Test empty inputs, single-element inputs, very large inputs, invalid inputs.

**Error Path Coverage**: Test that errors are handled correctly. If the function should raise an exception for invalid input, test that it does.

Aim for test cases that cover:
- Happy path (normal use)
- Edge cases (boundary conditions)
- Error cases (invalid inputs)
- Common real-world scenarios

You can't cover everything, but you can cover enough to catch most bugs.

## Security Ground Truth: The Critical Dimension

Security is so important it deserves special attention. Your ground truth must explicitly test for common vulnerabilities.

**SQL Injection Test**

If the code generates SQL queries, test with malicious input:

Request: "Write code to fetch user by username from database"

Security test:
```
username = "admin' OR '1'='1"
result = fetch_user(username)
# Should NOT return all users or cause an error exposing SQL structure
# Should safely handle the malicious input
```

If the generated code is vulnerable, it fails ground truth.

**XSS Test**

If the code renders user content in HTML:

Request: "Display user comment on a webpage"

Security test:
```
comment = "<script>alert('XSS')</script>"
html = render_comment(comment)
# html should NOT contain executable script tags
# Should escape or sanitize the input
```

**Path Traversal Test**

If the code handles file paths:

Request: "Read a file from the uploads directory"

Security test:
```
filename = "../../../etc/passwd"
result = read_file(filename)
# Should NOT read files outside uploads directory
# Should validate and sanitize the path
```

Include security tests in your ground truth for any code that handles user input, accesses data, or performs sensitive operations.

## Style and Lint Compliance

Code style is automatically checkable, so include it in ground truth.

Your ground truth pipeline:
1. Generate code
2. Run it through your linter (flake8, eslint, etc.)
3. Run it through your formatter (black, prettier, etc.)
4. Fail if linter reports errors or formatter makes changes

This ensures generated code fits your project's standards without manual review.

Configuration:
- Use your project's existing linter config
- Enforce the same rules as for human-written code
- Don't lower standards for AI-generated code

AI code should be indistinguishable from human code in style and quality.

## The "Right Answer, Wrong Reasoning" Problem

Code can produce correct output through incorrect logic. Your ground truth needs to catch this.

Request: "Write a function to check if a number is even"

Bad code:
```
def is_even(n):
    # Happens to work for positive integers
    return n % 2 == 0 if n >= 0 else n % 2 != 0
```

This gives correct answers for positive integers but has wrong logic for negatives.

Better test coverage:
```
assert is_even(2) == True
assert is_even(3) == False
assert is_even(-2) == True  # This catches the bug
assert is_even(-3) == False
assert is_even(0) == True
```

The broader your test coverage, the more likely you catch "right answer, wrong reasoning" issues.

## Evaluating Multiple Correct Solutions

Here's a challenge: many programming tasks have multiple valid solutions. Your ground truth needs to accept all correct approaches, not just one specific implementation.

Request: "Write a function to find the maximum value in a list"

Correct solutions:
```
# Solution 1: Using max()
def find_max(lst):
    return max(lst)

# Solution 2: Using a loop
def find_max(lst):
    max_val = lst[0]
    for val in lst[1:]:
        if val > max_val:
            max_val = val
    return max_val

# Solution 3: Using reduce
from functools import reduce
def find_max(lst):
    return reduce(lambda a, b: a if a > b else b, lst)
```

All three are correct. Your ground truth should accept all of them.

This means ground truth is test-based, not implementation-based. You don't check if the code matches a specific implementation; you check if it passes tests.

## Handling Incomplete or Partial Code

Sometimes the AI generates code that's partially correct: the main logic works but error handling is missing, or it handles some cases but not others.

Your ground truth should distinguish:

**Complete Correct**: Fully functional, handles all cases

**Partial Correct**: Works for main cases, missing some edge cases or error handling

**Complete Wrong**: Fundamentally incorrect

Use granular test suites and report which tests pass and which fail. This gives you partial credit scoring instead of binary pass/fail.

Example:
- Main functionality tests: 10/10 passed
- Edge case tests: 6/10 passed
- Security tests: 8/10 passed
- Overall: 24/30 (80% correct, but with known gaps)

This is more informative than "failed" and helps you understand what the model does well and what it struggles with.

## The Documentation and Comment Challenge

Should generated code include comments? Docstrings? Documentation?

Your ground truth should specify:

**Function Docstrings**: Required for public functions (describe purpose, parameters, return value)

**Inline Comments**: Required for complex logic (explain why, not what)

**Type Hints**: Required or optional (depends on language and project standards)

Some ground truth approaches:

**Require Documentation**: Code fails ground truth if it lacks proper docstrings

**Optional Documentation**: Code passes without docs, but gets higher score with them

**No Documentation Requirement**: Focus on code correctness only

Choose based on your use case. For internal tools, correctness might matter most. For library code that others will use, documentation is critical.

## Evaluating Code Modifications and Refactoring

Sometimes you're not generating new code from scratch—you're modifying existing code. The ground truth challenge changes.

Request: "Add error handling to this function"

Your ground truth must verify:

**Preservation**: Did it preserve the original functionality?

**Addition**: Did it add the requested feature?

**No Regression**: Did it avoid breaking anything else?

This requires:
- Tests for original functionality (should still pass)
- Tests for new functionality (should now pass)
- Tests for potential regressions (common bugs introduced by changes)

Modification ground truth is harder because you're checking both what changed and what didn't.

## Language-Specific Ground Truth

Different programming languages have different correctness criteria.

**Python**:
- Should use appropriate data structures (lists, dicts, sets)
- Should follow PEP 8 style
- Should use exceptions for error handling
- Should include type hints (increasingly expected in 2026)

**JavaScript/TypeScript**:
- Should handle async operations correctly (promises, async/await)
- Should avoid callback hell
- Should use modern ES6+ features appropriately
- TypeScript: should have correct type annotations

**Go**:
- Should follow Go conventions (error returns, not exceptions)
- Should use goroutines and channels appropriately for concurrency
- Should pass go fmt (formatting) and go vet (static analysis)

**Rust**:
- Should compile without warnings (Rust compiler is very strict)
- Should handle ownership and borrowing correctly
- Should use Result and Option types for error handling
- Should avoid unsafe code unless absolutely necessary

Tailor your ground truth to language-specific best practices.

## The "Works on My Machine" Problem

Code can pass tests in one environment and fail in another. Your ground truth needs to test in realistic environments.

**Dependency Management**: Does the code specify required libraries/versions?

**Environment Variables**: Does it rely on environment setup that might not be present?

**Platform Differences**: Does it work on Linux, Mac, and Windows (if cross-platform support is needed)?

**Version Compatibility**: Does it work with the specified language version?

Test generated code in the actual environment where it will run, not just a developer's local setup.

## Evaluating Generated Tests

Sometimes you're asking the AI to generate tests, not production code. The ground truth for this is meta: you're evaluating code that evaluates code.

Request: "Write unit tests for this function"

Your ground truth must check:

**Coverage**: Do the tests cover main functionality, edge cases, and error cases?

**Correctness**: Do the tests actually test what they claim to test?

**Reliability**: Do the tests pass when the function is correct and fail when it's buggy?

The last one is tricky. You need to verify tests by running them against both correct and intentionally buggy implementations. Good tests should pass for correct code and fail for buggy code.

## The Warning: What Happens If You Skip This

If you evaluate generated code only on "does it run?" without checking security, efficiency, style, and edge cases, here's what happens:

Your AI will generate working but vulnerable code. It will ship SQL injection bugs. It will create inefficient algorithms that work fine on small data but crash on realistic data. It will produce code that works in tests but fails in production. It will create a mess that's technically functional but unmaintainable.

Developers will lose trust in the AI because they'll catch bugs in code review that should have been prevented. Users will experience failures that should have been caught in testing. Security audits will find vulnerabilities that are trivially detectable.

I've seen teams deploy AI-generated code that looked great and worked in testing, only to discover critical security vulnerabilities in production. The reputational damage was severe.

Don't evaluate code like it's text. Evaluate it like it's code: run it, test it, scan it for security issues, check it against style guides, and verify it handles edge cases.

Code that "works" is not the same as code that's correct. Your ground truth must know the difference.

## Bridge to Creative Tasks

We've been talking about code, where there's a clear notion of correctness: it either works or it doesn't, it's either secure or it's not. But what happens when your AI is creating something where there is no single correct answer? Marketing copy, blog posts, creative writing, design suggestions—these tasks are fundamentally different because success is subjective. There's no test suite for whether a poem is good. Let's walk through how to build ground truth when creativity and human judgment are the only measures that matter.
