# 1.1 — The Question Every AI Team Gets Wrong

Let me tell you about a team that thought they had it all figured out.

They were six months into building a customer support chatbot for a mid-sized insurance company. The demos looked amazing. Executives loved it. The bot answered questions smoothly, pulled from the knowledge base perfectly, and even had a friendly tone that felt human. Everyone in the room nodded approvingly during presentations.

They shipped it to 10,000 customers on a Tuesday morning.

By Thursday afternoon, the bot was pulled offline. What happened? The bot had been giving answers that were technically accurate according to the documentation but completely wrong for customers with policies issued before 2023. It recommended claim processes that didn't apply to certain states. It told a customer with a bronze-tier plan that they had coverage they absolutely did not have. Nobody caught these issues because nobody had ever written down what "correct" actually meant.

The team had built an entire evaluation system. They had metrics. They had test cases. They had labelers checking responses. But they'd skipped the first and most important question: what does "correct" even mean for our specific product?

## The Question Nobody Asks

Here's the question that separates great AI teams from everyone else: "What is a correct response?"

Not "how good is the model?" Not "what's our accuracy score?" Not "does this pass our eval?" Those questions come later. The foundational question is defining correctness itself.

Most teams assume everyone shares the same definition of "correct." They don't. Ask three people on your team what makes a chatbot response correct, and you'll get three different answers:

- Your engineer might say: "It accurately retrieves information from the knowledge base."
- Your product manager might say: "It solves the customer's problem in under two minutes."
- Your legal counsel might say: "It doesn't make claims we can't back up contractually."

They're all right. They're also all incomplete. And when you don't explicitly define what "correct" means, you end up with a system that satisfies nobody.

## What Happens When You Assume Ground Truth

Let me walk you through the cascade of problems that happen when you assume ground truth instead of defining it.

First, your labeling process becomes a nightmare. You hire annotators to label responses as "good" or "bad." Without clear ground truth, each annotator invents their own standard. One focuses on brevity. Another prioritizes completeness. A third cares most about tone. Your labels become meaningless noise because you're measuring different things.

I watched a team spend three months labeling 50,000 examples, only to discover their inter-annotator agreement was 42%. That means annotators disagreed on more than half of all labels. Why? Because "label this response as good or bad" isn't a real instruction when "good" hasn't been defined.

Second, your metrics become fiction. You track accuracy, precision, recall, F1 scores. But accuracy against what? If your ground truth is undefined, your metrics are measuring compliance with an invisible, inconsistent standard. You might report "94% accuracy" to leadership, but that number is meaningless if the underlying definition of correct keeps shifting.

Third, you can't detect regressions. You update your prompt. You swap models. You modify your retrieval system. Did quality improve or degrade? You can't know because you never established what quality means in the first place. Teams end up shipping changes that feel better in demos but perform worse in production, and they don't discover it until users complain.

Fourth, you can't debug failures systematically. A user reports a bad response. Your team investigates. Was it a retrieval problem? A generation problem? A prompt problem? Without ground truth, you can't categorize the failure. You end up treating symptoms instead of diagnosing root causes.

Fifth, and most dangerously, you ship bugs you can't see. The insurance chatbot team shipped a system that worked perfectly according to their vague, assumed standard of "correct." It failed catastrophically according to the actual standard that mattered: "legally accurate for every policy type across all jurisdictions."

## The Ambiguity Cascade

Here's how undefined ground truth cascades through your entire system:

You start building without defining correct. Your engineering team makes reasonable assumptions about what users want. They build retrieval systems, prompts, and ranking algorithms based on these assumptions. Nobody writes them down.

Then you hire labelers. You tell them to "rate response quality." They make their own assumptions, which don't match engineering's assumptions. The labels are inconsistent, but you don't realize it because you never defined what consistency would look like.

Then you train or fine-tune models on this inconsistent labeled data. The model learns to predict labels that reflect multiple conflicting standards. It becomes mediocre at everything because it's trying to satisfy everyone.

Then you define metrics. You pick accuracy because that's what everyone uses. But accuracy against inconsistent labels is just measuring noise. Your metrics tell you nothing useful about actual product quality.

Then you ship. Users interact with your product. They have their own definition of "correct" based on their actual needs. This definition doesn't match your assumptions, your labelers' assumptions, or your model's learned behavior. Users report that the product "feels off" but can't articulate why. Your team can't fix it because you don't know what "right" would look like.

This cascade is why the first question must be: what is ground truth for our specific product?

## The Production Reality Check

Let me show you what undefined ground truth looks like in production with a real scenario.

A fintech company built an AI system to answer questions about tax implications of different investment strategies. They tested it thoroughly on common questions. It worked great. They shipped it.

Then tax season hit. Users asked edge-case questions about niche scenarios: "What if I have a backdoor Roth IRA and I'm also subject to the Alternative Minimum Tax?" The system gave answers. Users trusted those answers. Some of those answers were dangerously wrong.

The company pulled the feature and did a post-mortem. What went wrong? They'd never defined ground truth for edge cases. Their eval set had 500 questions, all common scenarios. They'd labeled responses based on "sounds reasonable to a financial services professional." But "sounds reasonable" isn't the same as "legally accurate across all tax situations."

Their actual ground truth should have been: "Matches guidance from IRS publications, or explicitly states when professional consultation is required." But they'd never written that down. They'd assumed it. And assumptions don't scale.

## The Cost of Getting It Wrong First

Some teams think: "We'll figure out ground truth as we go. We'll start broad and refine based on user feedback."

This approach is expensive. Let me show you why.

When you define ground truth after building, you discover that 40% of your labeled data is wrong. You have to relabel. That's not just time—it's actual money. If you spent $50,000 on labeling, you're throwing away $20,000.

When you define ground truth after building, you realize your prompt is optimized for the wrong objective. You have to rewrite it. You have to re-run all your evals. You might have to re-architect parts of your system.

When you define ground truth after building, your team has already formed opinions about what quality looks like. Those opinions are now biases. People resist changing the definition because it means admitting their previous work was mis-targeted.

When you define ground truth after shipping, you've trained users to expect something you can't consistently deliver. Changing the behavior feels like a downgrade, even if it's technically an improvement.

The teams that win define ground truth first. Before the first prompt. Before the first label. Before the first eval. They ask: "What does correct mean for our users, in our domain, for our specific tasks?"

## What Definition Actually Looks Like

You might be thinking: "Okay, I get it, I need to define ground truth. But what does that actually mean?"

It means writing down explicit criteria that anyone on your team could use to judge whether a response is correct. Not vague principles. Specific, testable criteria.

For the insurance chatbot, ground truth might be:

"A response is correct if it: (1) only references coverage that exists in the customer's specific policy document, (2) cites the exact policy section number, (3) accounts for state-specific regulations in the customer's jurisdiction, (4) includes required legal disclaimers for claim-related questions, and (5) directs customers to human agents for questions involving coverage disputes or policy changes."

Notice what that definition does. It removes ambiguity. An engineer can read it and build systems that satisfy it. A labeler can read it and consistently judge responses. A product manager can read it and understand what success means. Legal can read it and approve it.

That's what defined ground truth looks like. It's specific enough to be testable and comprehensive enough to cover what actually matters.

## The Question Behind The Question

When I tell teams they need to define ground truth, they often jump straight to: "Okay, we'll write down our quality criteria."

But there's a deeper question first: who are we building for, and what do they actually need?

The insurance chatbot team thought they were building for "customers who have questions about their coverage." That's not specific enough. Different customers need different things:

- A customer trying to understand their coverage before making a claim needs education and clarity
- A customer in the middle of a crisis needs fast, actionable guidance
- A customer comparing plans needs detailed feature breakdowns
- A customer with a dispute needs careful, legally precise language

One ground truth definition doesn't serve all these needs. You might need different ground truth for different user contexts. And that's okay—as long as you know it up front.

The question behind the question is: what is the job this AI is being hired to do? Not in general. For specific users in specific situations.

When you answer that question precisely, ground truth becomes much easier to define. You're not defining "correct for a chatbot." You're defining "correct for helping a stressed customer understand if their car accident is covered while they're standing on the side of the road."

That specificity changes everything.

## The Difference Between Demos and Production

Here's why teams get destroyed by the ground truth question: demos and production require different standards.

In a demo, you control the questions. You prepare the scenarios. You show the happy path. "It looks good" is a sufficient standard because you've pre-selected cases where it will look good.

In production, users control the questions. They ask things you didn't prepare for. They find edge cases. They combine concepts in unexpected ways. They make typos. They ask follow-up questions that completely change context.

"It looks good" in a demo means nothing about production performance. You need a definition of "correct" that holds up when you're not in the room, when the questions are messy, when the user is confused, when the context is ambiguous.

That definition is your ground truth. And if you don't have it before you ship, you're about to learn what it should have been the hard way—through production incidents, user complaints, and emergency rollbacks.

## Why This Question Comes First

Every other question in AI evaluation depends on this one.

How do you measure quality? Against ground truth.
How do you know if you've regressed? Compare to ground truth.
How do you label training data? According to ground truth.
How do you choose between models? Test against ground truth.
How do you debug failures? Identify where behavior diverges from ground truth.

Ground truth is the foundation. Everything else is built on top. If the foundation is assumed instead of defined, everything built on top is unstable.

The question every AI team gets wrong is thinking evaluation starts with picking metrics or building test sets. It doesn't. It starts with defining what "correct" means in your specific context.

That's not a data problem. It's not a modeling problem. It's a product definition problem. And it's the most important question you'll answer.

## The Teams That Get It Right

Let me show you what it looks like when teams get this right.

There's a company building an AI coding assistant. Before they wrote a single line of code, they spent two weeks defining ground truth. They broke down "helpful code suggestions" into specific criteria:

- Syntactically valid in the target language
- Follows the codebase's existing style conventions
- Doesn't introduce security vulnerabilities from a defined list
- Runs without errors in the current environment
- Solves the specific task implied by the cursor position and surrounding code
- Doesn't duplicate functionality that already exists in the codebase

They documented this. They got engineering, product, and security to sign off. They used it to guide everything that came after: their labeling instructions, their eval design, their model selection, their prompt engineering.

When they shipped, they had 1/10th the edge-case failures of comparable products. Why? Because they'd defined edge cases as part of ground truth before building. They knew what correct meant in weird situations, not just common ones.

That's the difference. Elite teams define ground truth first. Everyone else assumes it and pays later.

## What You Need To Do Next

If you're building an AI product right now and you haven't explicitly defined ground truth, stop. Don't write more prompts. Don't label more data. Don't run more evals.

Write down your ground truth definition. Make it specific. Make it testable. Get stakeholders to agree on it. Version it like code.

Then proceed. Everything you build from that point forward will be more focused, more measurable, and more likely to work in production.

The question every AI team gets wrong is: "What does correct mean?" The teams that answer it first are the ones that ship products that actually work.

In the next section, we'll dig into a confusion that derails almost every conversation about ground truth: the difference between ground truth, training data, and benchmarks. These three concepts get mixed up constantly, and untangling them changes how you think about evaluation entirely.
