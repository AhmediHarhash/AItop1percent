# 1.9 — What Elite Teams Do Differently

I've worked with teams across the spectrum—from startups scrambling to ship their first AI feature to top tech companies with mature AI practices. The difference between the top 1% and everyone else isn't resources, model access, or technical sophistication.

It's discipline around ground truth.

Elite teams treat ground truth as a first-class product artifact. They define it before writing prompts. They version it like code. They review it systematically. They tie it to business metrics. They have clear ownership. And they test their ground truth itself for consistency.

Let me walk you through exactly what elite teams do that everyone else skips.

## Practice 1: Ground Truth Before Prompts

Most teams start by writing prompts and seeing what happens. Elite teams start by defining what "good output" means, then write prompts to achieve it.

Here's what the timeline looks like for an elite team building a customer support chatbot:

Week 1: Stakeholder interviews and user research. What do users actually need? What do business stakeholders require? What does legal mandate?

Week 2: Draft ground truth criteria for each task type (factual questions, troubleshooting, complaints, etc.). Get cross-functional review and agreement.

Week 3: Create evaluation dataset with labeled examples using the ground truth criteria. Measure inter-annotator agreement to validate that criteria are clear.

Week 4: Begin prompt engineering, testing every iteration against the ground truth eval set.

Notice that prompts don't appear until Week 4. By that point, the target is crystal clear. Everyone agrees on what they're building toward.

Compare this to typical teams:

Week 1: Start writing prompts. See what the model does.

Week 2: Iterate on prompts based on vibes and spot-checking.

Week 3: Realize nobody agrees on what "good" means. Argue about it.

Week 4: Scramble to define criteria retroactively while also trying to launch.

The elite team reaches Week 4 with clarity and momentum. The typical team reaches Week 4 in chaos.

The practice: Define ground truth before writing a single prompt. Make the target clear before you start building.

## Practice 2: Version Control and Change Tracking

Elite teams version their ground truth documentation with the same rigor they version code.

Every ground truth definition lives in version control (usually in the same repo as the product). Changes go through pull requests with reviews. There's a changelog. There are tags for major versions.

Why does this matter? Because three months from now, when you're debugging why performance changed, you need to know if ground truth changed too.

I watched an elite team investigate a "regression" where their metrics dropped 8%. They pulled up the ground truth version history and discovered the metrics didn't drop—ground truth had gotten stricter. They'd added new safety criteria. The system hadn't regressed; the standard had elevated.

Without version control, they would have spent days debugging a non-existent problem.

The practice:
- Ground truth lives in version control
- Changes require review and approval
- Every version is tagged and dated
- Changelog documents what changed and why
- Eval data is tagged with the ground truth version used to create it

This makes ground truth auditable, maintainable, and debuggable.

## Practice 3: Quarterly Review Rituals

Elite teams don't wait for ground truth to break. They review it proactively on a fixed schedule.

Every quarter, there's a ground truth review meeting with key stakeholders:
- Product review: Has the product changed in ways that affect what "correct" means?
- Policy review: Have business or legal policies changed?
- User feedback review: What are top complaint themes? Do they reveal ground truth gaps?
- Competitive review: Have user expectations shifted based on what competitors offer?
- Performance review: Are there criteria we consistently fail on that might be unrealistic?

The output is a list of ground truth updates to implement before the next quarter.

I sat in on one of these meetings. The team discovered:
- They'd added a feature two months ago but never updated ground truth to include criteria for it
- Legal had updated their privacy policy and ground truth still referenced the old version
- Users were complaining about tone, but tone wasn't in their ground truth definition at all

They left with concrete action items and an updated ground truth ready for the next quarter.

The practice: Quarterly reviews are calendared recurring meetings, not ad-hoc. They're treated as essential, not skippable. Output is documented updates with assigned owners.

## Practice 4: Tie Ground Truth to Business Metrics

Elite teams don't define ground truth in a vacuum. They explicitly connect it to business outcomes.

For a customer support bot, their ground truth might map to metrics like:

- Correctness (factually accurate responses) → drives CSAT and reduces escalations
- Safety (no liability-creating statements) → reduces legal risk and compliance violations
- Usefulness (solves user problems) → drives deflection rate and reduces support costs

This mapping serves two purposes:

First, it validates that ground truth actually matters. If a ground truth criterion doesn't connect to a business metric, why is it in there?

Second, it helps prioritize when ground truth criteria conflict. If you have to trade off between two criteria, you can ask: which has bigger business impact?

I worked with a team that had "responses should be concise" as a ground truth criterion. When they mapped to business metrics, they realized conciseness didn't actually matter to users—completeness did. Users preferred longer, thorough answers. They updated ground truth based on what drove actual CSAT.

The practice: Document the business rationale for each ground truth criterion. If you can't articulate why something matters to users or the business, reconsider whether it belongs in ground truth.

## Practice 5: Clear Ownership with Real Authority

Elite teams assign explicit ownership of ground truth to a specific person or team, and they give that owner real authority.

The owner can:
- Convene stakeholders for ground truth decisions
- Approve or reject proposed changes
- Allocate labeling budget to test ground truth
- Block launches if quality doesn't meet ground truth standards

This is different from typical teams where ground truth is "owned" by someone who can document it but has no power to enforce it.

I saw an elite team where the AI Product Manager owned ground truth. When engineering wanted to ship an update that performed well on speed but poorly on a ground truth safety criterion, the PM blocked the launch. Not because they outranked engineering, but because they had explicit authority over quality standards. Engineering could escalate to leadership, but the default was that ground truth owner's call stood.

The practice: Write down who owns ground truth. Give them authority commensurate with responsibility. Make it clear they can say "no" when ground truth isn't met.

## Practice 6: Test Ground Truth Itself

Here's something elite teams do that almost nobody else does: they test whether their ground truth is actually good.

How do you test ground truth? By measuring:

Inter-annotator agreement: Do different people applying your ground truth criteria reach the same conclusions? If agreement is below 80%, your criteria aren't clear enough.

Correlation with user outcomes: Do responses that score high on ground truth also get high user satisfaction? If not, your ground truth might be measuring the wrong things.

Achievability: Can your system realistically meet your ground truth standards? If you consistently achieve only 40% on a criterion, maybe it's unrealistic for the current technology.

Completeness: Do real production failures get caught by ground truth? If users complain about things that pass your ground truth, you have gaps.

An elite team I worked with did this brilliantly. They had labelers evaluate 500 responses using their ground truth criteria. Then they showed the same 500 responses to actual users and got satisfaction ratings.

They found that responses meeting all ground truth criteria had 91% user satisfaction. Responses failing any criterion had 64% user satisfaction. That validated that their ground truth was measuring things that mattered to users.

They also found one criterion ("responses should include empathy phrases") had no correlation with user satisfaction. They removed it from ground truth as non-predictive noise.

The practice: Periodically test your ground truth against real user outcomes. Measure agreement between evaluators. Remove criteria that don't predict user satisfaction. Add criteria for failure modes you're seeing in production.

## Practice 7: Stratified Evaluation

Elite teams don't just measure overall quality. They measure quality stratified by:

- Task type (factual questions vs troubleshooting vs complaints)
- User segment (new users vs power users)
- Input characteristics (simple queries vs complex multi-part queries)
- Edge cases vs common cases
- Different languages or regions

This reveals where the system is strong and where it's weak.

I watched an elite team discover their chatbot was 96% on simple factual questions but only 72% on multi-part questions that required synthesizing information. Overall they were at 89%, which looked decent. The stratified view revealed an actionable gap.

They focused improvement on multi-part questions specifically and got that segment up to 85%, which brought overall quality to 92%.

Without stratification, they would have been optimizing blindly.

The practice: Build evaluation datasets with explicit stratification. Report metrics broken down by segment. Set different quality bars for different segments if appropriate (maybe edge cases can be 80% while common cases must be 95%).

## Practice 8: Ground Truth Informs Everything

For elite teams, ground truth isn't just an eval thing. It informs:

Prompt engineering: Prompts explicitly instruct the model to meet ground truth criteria.

System architecture: If ground truth requires citing sources, the architecture includes citation tracking. If ground truth requires personalization, the architecture includes user context.

Labeling instructions: Labelers are trained on ground truth criteria directly.

Error analysis: When debugging failures, the team asks "which ground truth criterion was violated and why?"

Product roadmaps: Features are prioritized partly based on which ground truth criteria they improve.

Hiring: When hiring domain experts or labelers, candidates are evaluated on their ability to apply ground truth criteria consistently.

One elite team had their ground truth criteria literally printed on the wall in the team space. Every standup referenced it. Every design review checked against it. It was the shared language of quality.

The practice: Make ground truth central to how the team thinks and communicates. Use it as the framework for decisions across engineering, product, and data work.

## Practice 9: Separate Eval Datasets for Different Purposes

Elite teams maintain multiple eval datasets:

Development set: Used during active prompt iteration. Can be small (100-500 examples). Updated frequently.

Validation set: Used for gating decisions like "is this ready for user testing?" Larger (500-2000 examples). Updated quarterly.

Holdout set: Never touched during development. Used only for final pre-launch evaluation. Prevents overfitting to the eval set.

Regression set: Specific examples that previously failed. Every fixed bug becomes a regression test case.

Production sample set: Regular sampling of actual production traffic, evaluated against ground truth to detect drift.

Different datasets for different purposes prevents overfitting and gives you confidence that improvements generalize.

I saw one elite team discover through their production sample set that performance on real traffic was 6% lower than on their validation set. The validation set had gotten stale and no longer represented production diversity. They refreshed it and got a more realistic view of quality.

The practice: Don't use one eval set for everything. Build multiple sets for development, validation, regression testing, and production monitoring.

## Practice 10: Transparent Reporting

Elite teams report ground truth metrics transparently, including failures.

Their dashboards show:
- Overall ground truth compliance rate
- Breakdown by individual criterion
- Trends over time
- Comparison to user satisfaction metrics
- Known gaps and limitations

They don't hide failures. If they're at 73% on a particular criterion, that's visible to leadership and the team works to improve it.

This transparency builds trust. Stakeholders know the team is measuring honestly and working on real problems.

I watched a team present to their VP: "We're at 88% overall on ground truth. We're strong on factual accuracy (96%) and safety (94%). We're weak on conversational flow (78%) and personalization (74%). Here's our plan to improve those two areas."

The VP appreciated the clarity and approved resources to tackle the weak areas. If the team had just said "we're at 88%, looks good," the VP would have had questions but no clear picture of what to do.

The practice: Make ground truth metrics visible. Report honestly on strengths and weaknesses. Use ground truth breakdowns to guide improvement priorities.

## The Cumulative Effect

These practices compound. Defining ground truth before prompts makes everything that follows more efficient. Versioning ground truth makes debugging faster. Quarterly reviews prevent drift. Tying to business metrics ensures you're measuring what matters. Clear ownership drives accountability. Testing ground truth ensures it's actually good. Stratified evaluation reveals specific gaps. Ground truth informing everything creates alignment. Multiple eval sets prevent overfitting. Transparent reporting builds trust.

Elite teams don't do just one of these. They do all of them. That's why they're elite.

## The Anti-Patterns to Avoid

In contrast, here's what struggling teams do:

- Define ground truth after building most of the product (reactive instead of proactive)
- Keep ground truth in someone's head or a stale doc (no version control)
- Never review ground truth until something obviously breaks (no proactive maintenance)
- Define ground truth based on what seems nice instead of what drives outcomes (no business tie)
- Have vague ownership where everyone and no one is responsible (no accountability)
- Never validate that ground truth predicts user satisfaction (assume it's right)
- Only look at aggregate metrics (no stratification)
- Treat ground truth as an eval concern separate from building (no integration)
- Use the same eval set for everything and overfit to it (no separation)
- Hide quality problems or report only aggregate scores (no transparency)

If you recognize your team in that list, you're not alone. Most teams operate this way. But it doesn't have to be that way.

## The Transition Path

If you want to move from typical practices to elite practices, here's a realistic path:

Month 1: Define ground truth explicitly and get stakeholder agreement. Put it in version control.

Month 2: Assign clear ownership. Build an initial eval set covering your main task types.

Month 3: Start reporting ground truth metrics transparently. Identify your weakest areas.

Month 4: Implement quarterly review cadence. Do your first review.

Month 5-6: Build out stratified eval sets and separate datasets for different purposes.

Ongoing: Test ground truth against user outcomes. Refine based on learnings. Make ground truth central to team process.

This is a six-month transition, not a one-week sprint. But each month you get incrementally better, and by Month 6 you're operating at a different level.

## What Success Looks Like

You know you're an elite team when:

- You can articulate exactly what "correct" means for every task type your system handles
- Your ground truth hasn't been updated in three months only because you did a quarterly review three months ago and another is coming up
- Every team member can point to the ground truth doc and explain how it guides their work
- When production issues arise, your first question is "which ground truth criterion did we violate?"
- Your labeling quality is high because instructions are crystal clear
- Your metrics predict production performance because they're grounded in real user needs
- Leadership trusts your quality assessments because you've demonstrated systematic evaluation
- You catch regressions before shipping because your eval framework is robust
- You can show the ROI of quality investments by tying ground truth to business metrics

That's what elite looks like. It's not magic. It's discipline.

## The Bottom Line

The practices that separate elite teams from everyone else aren't about having better models or more data. They're about rigor in defining and maintaining quality standards.

Elite teams define ground truth first, version it properly, review it systematically, tie it to business outcomes, assign clear ownership, test it for validity, stratify their analysis, integrate it into all their work, maintain separate eval sets, and report transparently.

These practices are all learnable and achievable. You don't need special resources. You need commitment to doing the work properly.

That commitment is what makes teams elite.

In the next section, we'll tackle a source of endless confusion and wasted arguments: the five types of ground truth that teams constantly mix up. Understanding that factual truth, policy truth, preference truth, business truth, and brand voice truth are different things will clarify 90% of team disagreements about quality.
