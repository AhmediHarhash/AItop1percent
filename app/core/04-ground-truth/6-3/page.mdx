# 6.3 — Agents & Tool-Using Systems

Let me tell you about the most expensive agent failure I've witnessed. A travel booking agent was supposed to book a flight from New York to London. The user said "book me the cheapest flight next Tuesday." The agent searched flights, found options, and then called the booking tool. But it passed the wrong date parameter—it booked Tuesday of next month, not next week. The user didn't notice until three days before their intended trip. Rebooking cost four times the original price.

Here's the kicker: the agent's text output looked perfect. It said "I've booked your flight for next Tuesday, confirmation number ABC123." The conversation was polite, clear, and helpful. By traditional chatbot ground truth standards, this was a good response.

But the action was wrong. The agent chose the right tool, but passed the wrong parameters. This is why agent ground truth is fundamentally different from chatbot ground truth. You're not just evaluating what the AI said—you're evaluating what it did, and whether the world changed in the way the user intended.

## What Makes Agent Ground Truth Different

Chatbots generate text. Agents take actions. This shifts everything about ground truth.

Think of it this way: imagine evaluating a human assistant. If they write you a beautiful email explaining how they organized your files but they actually didn't organize anything, that's a failure. The text output is meaningless if the action didn't happen correctly.

Agent ground truth needs to verify three fundamental things:

1. **Intent Understanding**: Did the agent understand what action the user wanted?
2. **Execution Correctness**: Did the agent perform the correct action with correct parameters?
3. **State Verification**: Did the world actually change in the intended way?

Most teams only evaluate the first one (intent understanding) because it looks like traditional NLU. They're missing the harder, more important parts.

## The Anatomy of an Agent Action

Let's break down what happens when an agent takes action, because your ground truth needs to cover each piece.

**Step 1: Tool Selection**

The agent decides which tool to use. If the user says "send an email to my team," the agent needs to choose the send_email tool, not send_slack_message or create_calendar_event.

Ground truth for tool selection: given the user's intent, did the agent choose the correct tool?

This sounds simple until you have dozens of tools with overlapping functionality. Should "notify my team" use email, Slack, or Teams? It might depend on context (urgency, team preferences, time of day). Your ground truth needs to encode these contextual rules.

**Step 2: Parameter Extraction**

The agent extracts the parameters needed for the tool. For send_email, it needs: recipient, subject, body, maybe cc/bcc, attachments, etc.

Ground truth for parameters: did the agent extract all required parameters correctly from the user's request?

This is where things get gnarly. User says "email my team about the project update." The agent needs to:
- Map "my team" to specific email addresses
- Generate an appropriate subject line (not specified)
- Compose a body based on available project update information
- Decide on any defaults (mark as high priority? include project dashboard link?)

Your ground truth needs to specify what the correct parameter extraction looks like for ambiguous requests.

**Step 3: Parameter Validation**

Before calling the tool, the agent should validate parameters. Email addresses should be valid format. Dates should be in the future (usually). File paths should exist.

Ground truth for validation: did the agent catch invalid parameters before executing, and did it handle them appropriately?

The sophisticated version: the agent asks clarifying questions when parameters are ambiguous rather than making assumptions. "You said 'next Tuesday'—did you mean January 28th or February 4th?" Ground truth should reward this clarification behavior.

**Step 4: Action Execution**

The agent calls the tool with the parameters. This is where the actual work happens.

Ground truth for execution: did the tool call succeed? If it failed, did the agent handle the error appropriately?

Here's the critical part: execution can partially succeed. Maybe 3 out of 5 emails sent successfully, but 2 had invalid addresses. Your ground truth needs to handle partial success.

**Step 5: Result Verification**

The agent checks that the action achieved its goal.

Ground truth for verification: did the agent confirm the action succeeded, and did it communicate the result accurately to the user?

Many agents skip this step. They call the tool, assume it worked, and tell the user "Done!" Your ground truth should penalize this. The agent should verify success and report any issues.

## Tool Selection Ground Truth: The Multi-Tool Challenge

When your agent has access to many tools, tool selection becomes complex. Let me show you why.

Imagine a workplace assistant with these tools:
- send_email
- send_slack_message
- create_calendar_event
- set_reminder
- create_task
- update_spreadsheet

User says: "Make sure I don't forget to review the Q1 report tomorrow."

Which tool should the agent use? Multiple could work:
- set_reminder (most direct)
- create_calendar_event (puts it on calendar)
- create_task (adds to task list)
- send_slack_message (message yourself)

Your ground truth needs to specify which is correct, or whether multiple are acceptable. This requires understanding user preferences and context.

Maybe for this user, calendar events are for meetings with others, and reminders are for personal tasks. Or maybe they ignore reminders but always check their calendar. Context matters.

Your ground truth should include:

**Primary Tool**: The best tool for this intent
**Acceptable Alternatives**: Other tools that would work
**Unacceptable Tools**: Tools that would not accomplish the intent

And ideally, the reasoning: "set_reminder is preferred because the user wants a notification, not a calendar block that looks like a meeting to others."

This reasoning helps you evaluate edge cases and maintain consistency as you add new tools.

## Parameter Correctness Ground Truth

Parameter extraction is where most agent failures happen. The user gives partial information in natural language, and the agent needs to fill in the gaps.

Let's walk through a complex example:

User: "Book a conference room for the product team standup next Monday at 10"

The agent needs to extract:
- room_id: ? (not specified)
- start_time: next Monday at 10:00 AM (assuming AM, not PM)
- end_time: ? (not specified)
- attendees: product team members (needs to resolve this to specific people)
- meeting_title: "Product Team Standup" (inferred)
- recurrence: ? (standups are usually recurring, but not specified)

Your ground truth for this example needs to specify:

**Required Parameters**: What must the agent extract correctly?
- start_time: Monday, January 27, 2026 at 10:00 AM

**Inferred Parameters**: What should the agent reasonably infer?
- end_time: 10:30 AM (standard standup length)
- meeting_title: "Product Team Standup"
- attendees: [list of product team members]
- recurrence: weekly (if this is a new standup) OR none (if one-time)

**Ambiguous Parameters**: What should the agent ask about?
- room_id: Should ask "Which conference room?" or use smart default (largest available room that fits the team)
- recurrence: Should ask "Is this a one-time meeting or should I set it up weekly?"

See the judgment calls? Your ground truth needs to make these calls explicitly. Otherwise, labelers will be inconsistent.

A practical pattern: for each tool, maintain a parameter specification that defines:
- Required vs optional parameters
- Default values for optional parameters
- Validation rules
- When to ask for clarification vs use defaults

## Action Sequencing Ground Truth

Many agent tasks require multiple actions in sequence. This is where ground truth gets really complex.

Example: "Find a restaurant nearby and book a table for 4 at 7 PM."

The agent needs to:
1. get_current_location
2. search_restaurants(location, cuisine_preferences, availability)
3. Present options to user (or auto-select based on criteria)
4. book_reservation(restaurant_id, time, party_size)
5. add_to_calendar(event_time, location, attendees)

Your ground truth needs to verify:

**Correct Sequence**: Are actions in the right order? (Can't book before searching)

**Completeness**: Are all necessary actions included? (Many agents forget the calendar event)

**Efficiency**: Are there unnecessary actions? (Don't search 5 times if once is enough)

**Dependency Handling**: Does each action correctly use results from previous actions?

That last one is critical. The book_reservation call needs to use the restaurant_id from the search results. Your ground truth should verify that parameters flow correctly through the sequence.

Here's a failure mode I see constantly: the agent does the right actions in the right order, but doesn't thread the context properly. It searches for restaurants, then books a reservation at a completely different restaurant (maybe from its training data). The action sequence looked right, but the execution was wrong.

## Error Recovery Ground Truth

Actions fail. APIs go down. Permissions are missing. Users change their minds. A great agent handles this gracefully. A mediocre agent crashes.

Your ground truth needs to cover error scenarios:

**Error Detection**: Does the agent recognize when an action failed?

**Error Communication**: Does it explain the failure clearly to the user?

**Recovery Strategy**: Does it try an appropriate recovery action?

**Graceful Degradation**: If the task can't be completed, does it accomplish what it can?

Let me show you a concrete example:

User: "Send the Q1 report to the executive team and post it in the #general Slack channel."

The agent tries to:
1. send_email(recipients=exec_team, attachment=Q1_report.pdf) - SUCCESS
2. send_slack_message(channel=general, attachment=Q1_report.pdf) - FAILURE (file too large for Slack)

What should the agent do? Your ground truth should specify:

**Good Response**: "I've emailed the Q1 report to the executive team. I couldn't post the full PDF to Slack because it's too large, but I've posted a link to the document instead."
- Detected the failure
- Communicated it clearly
- Attempted a reasonable alternative (link instead of file)

**Acceptable Response**: "I've emailed the Q1 report to the executive team. I couldn't post it to Slack because the file is too large. Would you like me to upload it to Google Drive and share the link?"
- Detected the failure
- Communicated it clearly
- Asked how to proceed

**Bad Response**: "Done! I've sent the Q1 report to the executive team and posted it in #general."
- Didn't detect the failure
- Gave false confirmation

**Terrible Response**: *Crashes with error stack trace*

Your ground truth should include error scenarios and grade the agent's recovery behavior.

## Knowing When to Stop: The Termination Challenge

A subtle but critical ground truth dimension: does the agent know when it's done?

Some tasks have clear endpoints: "book a flight" ends when the flight is booked. But many are more ambiguous: "help me plan my vacation" could go on forever.

Your ground truth should verify:

**Goal Achievement Detection**: Does the agent recognize when the primary goal is complete?

**Premature Termination**: Does the agent stop before fully completing the task?

**Excessive Continuation**: Does the agent keep acting after the task is done?

I've seen agents that get stuck in loops, repeatedly checking and re-checking the same thing. Or agents that do 80% of a task and then just stop, leaving the user confused.

A good pattern: after completing what it thinks is the final action, the agent should confirm with the user: "I've booked your flight to London departing January 28th and returning February 4th. Is there anything else you need for your trip?"

This gives the user a chance to add more (hotel, car rental) or confirm they're satisfied. Ground truth should reward this confirmation behavior.

## The Difference Between Correct Plan and Correct Execution

Here's a crucial distinction that many ground truth frameworks miss: an agent can have the right plan but execute it wrong, or the wrong plan that accidentally works.

**Right Plan, Right Execution**: Best case. Agent chose the right actions and performed them correctly.

**Right Plan, Wrong Execution**: Agent knew what to do but made errors in execution (wrong parameters, incorrect sequence, etc.). This tells you the planning is good but execution needs work.

**Wrong Plan, Right Execution**: Agent had the wrong approach but executed it flawlessly. This is actually more concerning because it means the agent is confidently wrong.

**Wrong Plan, Wrong Execution**: Total failure.

Your ground truth should distinguish these cases because they have different implications for how you improve the system.

Example: User asks to "cancel all my meetings tomorrow."

**Right Plan**:
1. get_calendar_events(date=tomorrow)
2. For each event: cancel_event(event_id)
3. Notify user of cancellations

**Wrong Plan That Might Work**:
1. cancel_event(event_id=tomorrow) - if the agent misunderstands the API

**Right Plan, Wrong Execution**:
1. get_calendar_events(date=today) - wrong date
2. For each event: cancel_event(event_id) - correct action, wrong data

See how these failures give you different information about what to fix?

## Partial Success Handling

Real-world agent tasks often partially succeed. Your ground truth needs to handle this nuance.

User: "Email the design mockups to Alice, Bob, and Charlie."

The agent executes, but:
- Alice's email goes through
- Bob's email address is invalid
- Charlie's email is caught by spam filter (agent doesn't know)

What's the ground truth label for this? It's not a complete success, but it's not a total failure either.

Your ground truth should include:

**Success Rate**: How many sub-tasks succeeded? (1 of 3 confirmed, 1 of 3 failed, 1 of 3 uncertain)

**Critical Failure**: Did any failure prevent the overall goal? (If the user said "email Alice or anyone on her team," maybe partial success is fine)

**Communication Accuracy**: Did the agent accurately report what succeeded and what failed?

A sophisticated ground truth framework includes partial success scenarios and evaluates how well the agent handles them.

## Tool Misuse Patterns

Some failure modes are common enough that you should explicitly include them in ground truth.

**Wrong Tool for Task**: Agent uses a tool that can't accomplish the goal. (Using calculator for email)

**Right Tool, Catastrophically Wrong Parameters**: Agent uses the right tool but with dangerous parameters. (delete_all instead of delete_one)

**Tool Hallucination**: Agent attempts to call a tool that doesn't exist. (This happens when models are trained on tool use and they invent plausible-sounding tools)

**Privilege Escalation**: Agent attempts actions it doesn't have permission for.

**Irreversible Actions Without Confirmation**: Agent deletes, cancels, or sends without asking for confirmation when it should.

Your ground truth should include examples of each type of misuse and verify that your agent avoids them.

## Context-Dependent Tool Selection

Here's a pattern that requires sophisticated ground truth: the right tool depends on context that's not explicit in the request.

User says: "Remind me about the meeting."

In isolation, this seems simple: use set_reminder tool. But context matters:

- If the meeting is in 5 minutes, maybe a reminder is useless—the agent should just tell them now.
- If the meeting is next week, a reminder makes sense.
- If there are multiple meetings, the agent needs to clarify which one.
- If the user's calendar shows they're already in the meeting, maybe the request is about a different meeting.

Your ground truth needs to include this context and specify the correct tool choice for each scenario.

A practical approach: include rich context in your ground truth examples (current time, user's calendar, user's location, previous conversation) and specify how that context should influence tool selection.

## Multi-Turn Agent Ground Truth

Agents often work across multiple conversation turns, especially when they need clarification or encounter errors.

Turn 1:
User: "Book a flight to London"
Agent: "When would you like to travel?"

Turn 2:
User: "Next week"
Agent: "I see several options next week. What's your preferred departure day?"

Turn 3:
User: "Wednesday"
Agent: "I found 3 flights on Wednesday, January 29th. [shows options]"

Turn 4:
User: "The morning one"
Agent: "Booking the 8:00 AM departure..." [executes booking]

Your ground truth needs to evaluate:

**Information Gathering**: Did the agent ask the right questions to get necessary parameters?

**Question Efficiency**: Did it ask too many or too few questions?

**Context Maintenance**: Did it remember information from earlier turns?

**Graceful Interaction**: Did the conversational flow feel natural?

This is much more complex than single-turn ground truth because you're evaluating a strategy, not just a single action.

## The Idempotency Requirement

Here's a technical ground truth requirement that's often overlooked: agents should be idempotent where appropriate.

Idempotency means performing the same action twice has the same effect as performing it once. If a user says "create a task to review the report" and the agent does it, then the user accidentally says it again (or the agent misunderstands and thinks they did), the agent shouldn't create a duplicate task.

Your ground truth should test:

**Duplicate Detection**: Does the agent recognize when it's about to perform an action it already performed?

**Confirmation Before Duplication**: If unsure, does it ask "I already created this task. Do you want me to create another one?"

**State Awareness**: Does the agent check current state before acting?

This is especially critical for actions like:
- Creating calendar events
- Sending emails or messages
- Making purchases or bookings
- Creating records in databases

Your ground truth should include scenarios where the user request would create duplicates and verify the agent handles them correctly.

## Ground Truth for Agent "Hallucinations"

LLMs can hallucinate in text generation, but agents can hallucinate in action space. They might:

- Call tools that don't exist
- Use parameters that aren't valid for a tool
- Report actions they didn't actually perform
- Confabulate results from failed API calls

Your ground truth needs to catch these. Some patterns:

**Tool Existence Verification**: After the agent says it called a tool, verify that tool exists in the agent's toolkit.

**Parameter Schema Validation**: Verify all parameters passed to tools match the tool's schema.

**Action-Report Consistency**: Verify the agent's description of what it did matches what actually happened.

The last one is critical and often missed. The agent might call send_email and get an error, but then tell the user "I've sent the email." Your ground truth should verify the agent's communication matches reality.

## Putting It Together: A Practical Framework

Here's a ground truth framework for agent evaluation:

**Task-Level Ground Truth**
- Goal: What is the user trying to accomplish?
- Success Criteria: What does successful completion look like?
- Context: What relevant information is available?

**Plan-Level Ground Truth**
- Required Actions: What actions must the agent perform?
- Action Sequence: What order must they be in?
- Acceptable Alternatives: Are there multiple valid approaches?

**Execution-Level Ground Truth**
- Tool Selection: Did it choose the right tool for each action?
- Parameter Correctness: Did it extract/generate correct parameters?
- Error Handling: Did it handle failures appropriately?

**Outcome-Level Ground Truth**
- State Change: Did the world change as intended? (This is the next chapter)
- Communication Accuracy: Did the agent accurately report what happened?
- User Satisfaction: Would the user consider this task successfully completed?

## The Warning: What Happens If You Skip This

If you evaluate your agent only on conversation quality (like a chatbot) without verifying tool use and action correctness, here's what happens:

Your agent will say confident, helpful things that are completely wrong. It will tell users "I've booked your flight" when it actually booked the wrong date. It will say "I've sent the email" when it crashed. It will choose wrong tools, pass wrong parameters, and create chaos while sounding perfectly professional.

Users will initially trust the agent because the conversation quality is good. Then they'll discover the actions were wrong. Trust will evaporate faster than it built up. You'll get a reputation for an unreliable system that sounds good but doesn't work.

I've seen teams launch agent systems with beautiful conversational UI and terrible action reliability. Within weeks, users stop using them because they can't trust the agent to actually do what it says.

Don't evaluate the talk. Evaluate the walk.

## Bridge to Agent State Truth

We've covered how to verify that agents choose the right actions with the right parameters, but there's a deeper question: how do you verify that those actions actually changed the world correctly? An agent can call book_flight with perfect parameters, but did a booking actually appear in the reservation system? This is the state verification problem, and it's where agent evaluation gets really hard. Let's walk through how to verify not just what the agent said it did, but what actually happened in the systems it touched.
