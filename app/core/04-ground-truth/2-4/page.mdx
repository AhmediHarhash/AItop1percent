# 2.4 — Tier 2: Medium Tolerance (Advisory)

You're planning a trip to Japan and ask an AI travel assistant: "What should I see in Tokyo?" The AI suggests visiting Senso-ji Temple, the teamLab Borderless museum, and the Tsukiji Outer Market. You follow the advice. Later, you realize you missed the Tokyo Skytree and the Imperial Palace gardens, which you would have loved.

Did the AI fail? Not really. Its suggestions were genuinely good — those are excellent places to visit. It just didn't suggest everything you might have enjoyed. The AI gave helpful advice, even if it wasn't perfectly tailored to your specific interests.

This is Tier 2 — advisory tasks where the AI is helping, not deciding. The system recommends, explains, summarizes, or suggests, but the user maintains control over the actual decision. Mistakes here don't break things — they just make the experience less helpful.

## What Makes Something Advisory

Tier 2 tasks have a fundamental characteristic: no direct action is taken. The AI informs or suggests, the user decides.

**Tier 0 and 1 create commitments.** Execute a transaction, modify a record, book a reservation. These actions change state in the world.

**Tier 2 provides information.** Recommend an option, explain a concept, summarize a document, suggest next steps. These actions inform user decisions but don't execute them.

The distinction matters because it changes the failure mode. When a Tier 0 or 1 task fails, something wrong happens in the world. When a Tier 2 task fails, the user gets suboptimal information but can still make their own call.

**Common Tier 2 examples:**

**Recommendations:**
- Product recommendations in e-commerce
- Content suggestions (articles, videos, music)
- Restaurant or hotel recommendations
- Learning path suggestions
- Feature discovery hints

**Explanations:**
- Help documentation
- Concept explanations
- Troubleshooting guidance
- Policy clarifications
- Feature tutorials

**Summaries:**
- Meeting notes summarization
- Document summaries
- Email thread summaries
- Project status summaries
- Research synthesis

**Analysis and insights:**
- Data trend explanations
- Performance insights
- Customer behavior analysis
- Report generation
- Comparative analysis

**Planning and ideation:**
- Trip itinerary suggestions
- Project planning assistance
- Content outlines
- Study plans
- Strategy brainstorming

The pattern: the AI contributes information or ideas, the user decides what to do with them.

## The Tier 2 Quality Standard: Helpfulness Over Precision

Tier 0 requires certainty. Tier 1 requires high accuracy. Tier 2 requires helpfulness — a squishier but equally important quality.

What makes advice helpful?

**Relevant:** The information relates to what the user actually needs, not tangentially related topics.

**Actionable:** The user can actually do something with the advice. "Visit these five specific places" is more actionable than "Tokyo has many interesting locations."

**Comprehensible:** The user can understand the advice without specialized knowledge. Explanations should be clear, not jargon-filled.

**Comprehensive enough:** The advice covers the key points without overwhelming detail. For trip planning, mentioning the top 5-10 attractions is helpful. Mentioning 100 is overwhelming. Mentioning 2 is sparse.

**Appropriately confident:** When the AI is certain, it should sound confident. When it's uncertain, it should acknowledge that. "The best restaurant in Tokyo is..." sounds confident. "Here are some highly-rated restaurants you might enjoy..." acknowledges subjectivity.

Notice what's not in this list: "correct" or "accurate." For advisory tasks, there often isn't one right answer. Recommending the Tokyo Skytree or teamLab Borderless are both valid suggestions. The question is whether the advice helps the user make a good decision for themselves.

## Embracing Uncertainty in Tier 2

One of the biggest mental shifts from Tier 1 to Tier 2 is that uncertainty is acceptable — even valuable — when disclosed appropriately.

**Tier 0:** Uncertainty means refusal. "I cannot verify this prescription, pharmacist review required."

**Tier 1:** Uncertainty means asking for clarification. "You have three bookings — which one do you want to change?"

**Tier 2:** Uncertainty means acknowledging multiple valid options. "Here are several approaches you could take, depending on your priorities."

In advisory contexts, showing the user multiple perspectives or options is often more helpful than pretending there's one right answer.

Let me show you what this looks like:

**False certainty (pretending there's one answer):**
"The best programming language to learn first is Python."

This sounds confident but ignores context. Best for what? Web development? Data science? Systems programming? Different users have different goals.

**Appropriate uncertainty acknowledgment:**
"Python is a great first language if you're interested in data science or scripting. If you're more interested in web development, JavaScript might be more immediately useful. If you want to understand computer fundamentals deeply, starting with C could be valuable. What are you hoping to build?"

This acknowledges that "best" depends on goals and helps the user make an informed decision.

## What Tier 2 Ground Truth Looks Like

Tier 2 ground truth is trickier than Tier 0 or 1 because you're not labeling "correct" vs "incorrect" — you're labeling degrees of helpfulness.

**Ground truth structure:**

Each example includes:
- User query or context
- AI response (recommendation, explanation, summary)
- Helpfulness rating (scale: not helpful, somewhat helpful, helpful, very helpful)
- Specific quality dimensions:
  - Relevance: Does this address what the user needs?
  - Completeness: Are key points covered?
  - Clarity: Can the user understand and act on this?
  - Accuracy: Are factual claims correct? (Even if multiple answers are valid, facts should be right)
  - Tone appropriateness: Does the confidence level match the certainty?

**Rating scales vs binary labels:**

Tier 0 and 1 use binary or ternary labels: correct/incorrect/uncertain, approve/reject/escalate.

Tier 2 uses rating scales because helpfulness exists on a spectrum. A product recommendation might be:
- 1/5: Completely irrelevant (recommending winter coats when user is shopping for summer dresses)
- 2/5: Loosely related but not helpful (recommending formal dresses when user wants casual)
- 3/5: Relevant but generic (recommending popular summer dresses without considering user's style)
- 4/5: Well-targeted (recommending summer dresses in styles the user has liked before)
- 5/5: Exceptionally helpful (recommending dresses that match the user's style, size, and a specific event they mentioned)

This granularity helps the AI learn what makes recommendations go from "okay" to "great."

**Annotator requirements:**

Tier 2 annotators don't need to be domain experts, but they should represent your user base. For travel recommendations, you want people who travel. For learning content recommendations, you want learners. For data analysis insights, you want people who work with data.

The goal is to label helpfulness from the user's perspective, not from an expert's. An expert might find a technical explanation perfectly clear, but if your users are beginners, that expert clarity doesn't matter — user clarity does.

**Single annotator is often sufficient:**

Unlike Tier 0 and 1, Tier 2 can often work with single-annotator review for most examples. Since you're rating helpfulness on a scale rather than making binary correctness judgments, individual annotator variance is acceptable.

You do want multiple annotators for:
- Calibration examples (to align annotators on what the scale means)
- Factual accuracy checks (if the advice contains verifiable facts)
- Edge cases where helpfulness is genuinely unclear

But for mainstream examples, one qualified annotator rating helpfulness is often enough.

## The "Good Enough" Concept

Tier 2 introduces the idea that approximate answers are acceptable when they're helpful enough.

A trip planning AI suggests 8 things to do in Tokyo. An expert could probably suggest 12 even better things. Is the AI's response wrong? No — it's good enough to help the user plan a trip.

A document summarizer captures the main points but misses some nuance. Is the summary incorrect? Not necessarily — it might be a perfectly serviceable summary that gives the reader the gist.

A data analysis AI explains a sales trend as "likely due to seasonal patterns and a recent marketing campaign." There might be other contributing factors. Is this explanation wrong? No — it highlights the major drivers even if it's not exhaustive.

"Good enough" doesn't mean low quality. It means fit for purpose. The purpose of advisory AI is to help users make decisions or understand information, not to achieve academic perfection.

Your ground truth should reflect this. Annotators shouldn't ask "Is this the absolute best possible response?" They should ask "Does this response help the user enough to be valuable?"

## When Approximation is Acceptable

Tier 2 allows approximation in several ways:

**Simplified explanations:** Explaining complex concepts in simple terms often requires leaving out nuance. That's acceptable if the simplification helps understanding without being misleading.

Example: "Machine learning is like teaching a computer to recognize patterns by showing it many examples."

This is simplified — it leaves out training algorithms, optimization, validation, etc. But for someone trying to understand what ML is, this approximation is helpful.

**Representative examples rather than comprehensive coverage:** When suggesting options, the AI doesn't need to list every possible option. A representative set that covers the main categories is helpful.

Example: "Here are some popular JavaScript frameworks: React for user interfaces, Express for backend servers, and Jest for testing."

This omits Vue, Angular, Next.js, Nest.js, Mocha, and dozens of others. But it gives a beginner starting points in each category.

**Probability-weighted recommendations:** The AI can recommend things that are likely to be helpful even if they're not certain to be the perfect fit.

Example: "Based on your reading history, you might enjoy 'Project Hail Mary' by Andy Weir. It's science fiction with a similar problem-solving focus as 'The Martian,' which you loved."

The user might not like it. That's okay — the recommendation is based on reasonable inference.

**Qualified uncertainty:** The AI can express uncertainty while still being helpful.

Example: "I'm not sure if this solution will work for your specific case, but here's what typically helps with this error: ..."

Providing a probable solution with acknowledged uncertainty is more helpful than refusing to suggest anything.

## Factual Accuracy Still Matters

Even though Tier 2 allows approximation and subjectivity, factual claims must still be accurate.

If the AI says "Senso-ji Temple is in Tokyo," that must be true. (It is.)
If the AI says "The Tokyo Skytree is 500 meters tall," that must be accurate. (It's actually 634 meters — error.)
If the AI explains "React was created by Meta," that's a fact that must be correct. (It is.)

Ground truth annotation for Tier 2 should flag factual errors even in otherwise helpful responses. An explanation might be clear and relevant, but if it contains wrong information, that's a quality issue.

The distinction:
- Subjective judgments can vary (which restaurant is "best")
- Factual claims cannot (how tall the building is)

Your annotators should verify factual accuracy separately from overall helpfulness.

## Real Example: E-Commerce Product Recommendations

Let me walk you through a real Tier 2 system: an AI that recommends products on an e-commerce platform.

**The task:** User is browsing or searching. The AI suggests products they might like based on browsing history, purchase history, similar users' behavior, and current context.

**Why this is Tier 2:** The AI isn't adding items to cart or making purchases. It's suggesting. The user decides whether to click, explore, or ignore. A "wrong" recommendation is just a missed opportunity, not a broken transaction.

**Ground truth structure:**

Each example includes:
- User context (browsing history, purchases, demographics, current page)
- Recommended products (up to 10)
- Helpfulness rating for each recommendation:
  - 5: Excellent fit, user would likely be very interested
  - 4: Good fit, user might be interested
  - 3: Relevant but generic, user could be interested
  - 2: Loosely relevant, probably not interesting
  - 1: Irrelevant or inappropriate

**Annotator protocol:**

Annotators with e-commerce experience review recommendations from the user's perspective. They consider:
- Does this match the user's demonstrated interests?
- Is the product type relevant to what they're browsing?
- Is the price point appropriate given their purchase history?
- Is this a duplicate or near-duplicate of something they've already seen/bought?
- Does the recommendation add discovery value (showing them something new) or convenience value (showing them something they're clearly looking for)?

**Multi-dimensional evaluation:**

Beyond the overall helpfulness rating, annotators evaluate:
- **Relevance:** 1-5 scale, how well does this match user intent?
- **Diversity:** Are recommendations varied, or all very similar?
- **Novelty:** Does this show the user something new, or just what they've already seen?
- **Appropriateness:** Is the product suitable for the user (age, gender if relevant, location)?

**Example annotation:**

User is browsing hiking boots, has previously purchased outdoor gear.

Recommended products:
1. Merrell hiking boots - Rating: 5 (Excellent fit, directly relevant)
2. Columbia waterproof hiking boots - Rating: 5 (Excellent fit, adds waterproof angle)
3. Wool hiking socks - Rating: 4 (Good complementary product)
4. Trekking poles - Rating: 4 (Related activity, good discovery)
5. Trail running shoes - Rating: 3 (Related but different use case)
6. Camping tent - Rating: 3 (Same outdoor category, but different immediate need)
7. Casual sneakers - Rating: 2 (Footwear, but not relevant to hiking context)
8. Winter jacket - Rating: 2 (Outdoor gear, but not relevant to current browsing)
9. Sunglasses - Rating: 1 (Very generic, no clear relevance)
10. Kitchen knife set - Rating: 1 (Completely irrelevant)

A set with items 1-6 would be rated as "helpful" overall. A set with items 1-2 and then 7-10 would be rated "somewhat helpful" — it has some good suggestions but also irrelevant filler.

**The result:**

The recommendation system achieves an average helpfulness rating of 3.8/5.0 across all recommendations. Users click on recommended products 12% of the time and purchase them 3% of the time. These metrics suggest the recommendations are helpful enough to influence behavior but not so aggressive that they feel spammy.

## Content Summarization Example

Another Tier 2 scenario: an AI that summarizes long documents, articles, or email threads.

**The task:** User provides a lengthy document. The AI generates a summary that captures the key points in a fraction of the length.

**Why this is Tier 2:** The summary doesn't take action — it informs. If the summary misses some nuance, the user can read the full document. The worst case is that the user wastes time on an unhelpful summary, not that something breaks.

**Ground truth structure:**

Each example includes:
- Original document
- AI-generated summary
- Quality ratings:
  - **Completeness:** Are all key points included? (1-5 scale)
  - **Accuracy:** Are the points correctly representing the original? (1-5 scale)
  - **Conciseness:** Is the summary appropriately brief? (1-5 scale)
  - **Clarity:** Is the summary understandable without reading the original? (1-5 scale)
  - **Overall helpfulness:** Would this summary help a user understand the document? (1-5 scale)

**Annotator protocol:**

Annotators read the original document and the summary, then rate each dimension. They also identify:
- Major points that were missed
- Minor points that were included but aren't essential
- Any factual errors or misrepresentations
- Confusing phrasing or unclear references

**What "good enough" looks like:**

A perfect summary would capture every important point with perfect accuracy, be maximally concise, and be crystal clear. But perfection isn't the bar.

A helpful summary:
- Captures the 3-5 most important points (even if 6-8 total points exist)
- Represents those points accurately (no factual errors)
- Is 10-20% the length of the original
- Can be understood without reading the original

If the summary does this, it's helpful even if it's not perfect.

**Example:**

Original document: 3,000-word article about remote work trends in 2026

AI summary (300 words):
"Remote work has stabilized at about 30% of knowledge workers working fully remote, with another 40% in hybrid arrangements. This represents a plateau after the rapid increase during 2020-2022.

Key trends include:

1. Companies are establishing permanent remote policies rather than temporary arrangements. 65% of companies now have formal remote work policies.

2. Salary adjustments based on location are becoming less common. Most companies now pay based on role, not employee location.

3. Async communication tools are evolving. Document-based collaboration is overtaking video calls for many teams.

4. Remote work is expanding beyond tech. Healthcare, education, and financial services now have significant remote workforces.

5. Employee preference for remote work remains strong, with 78% of workers wanting at least some remote option.

Challenges include maintaining company culture, managing performance, and ensuring equity between remote and in-office workers. Companies that succeed treat remote work as a design problem, not a temporary accommodation."

Annotation:
- Completeness: 4/5 (Captures main trends and challenges, but could mention the geographic distribution shift)
- Accuracy: 5/5 (All statistics and claims are correct)
- Conciseness: 5/5 (10% of original length, no fluff)
- Clarity: 5/5 (Can understand the landscape without reading the full article)
- Overall: 4.5/5 (Very helpful summary)

This isn't perfect — a human expert might craft a better summary. But it's helpful enough to give a reader the key takeaways quickly.

**The result:**

The summarization system achieves an average helpfulness rating of 4.2/5.0. Users report that summaries save them time 85% of the time. The 15% "not helpful" cases are usually highly technical documents where domain expertise is needed to identify what's important — a known limitation.

## Troubleshooting Guidance Example

A third Tier 2 scenario: an AI that helps users troubleshoot technical problems.

**The task:** User describes a problem with a product or service. The AI suggests diagnostic steps and potential solutions.

**Why this is Tier 2:** The AI is guiding, not fixing. It suggests "try restarting your router" but doesn't restart the router. The user tries the suggestions and sees if they work. Bad advice wastes the user's time but doesn't break anything.

**Ground truth structure:**

Each example includes:
- Problem description
- System context (product version, configuration, error messages)
- AI-generated troubleshooting guidance
- Quality ratings:
  - **Relevance:** Does this address the described problem? (1-5)
  - **Completeness:** Are all likely solutions covered? (1-5)
  - **Prioritization:** Are solutions ordered from most to least likely? (1-5)
  - **Clarity:** Can a non-technical user follow these steps? (1-5)
  - **Safety:** Does the guidance avoid suggesting dangerous actions? (1-5 or binary pass/fail)

**Annotator protocol:**

Annotators with technical support experience evaluate whether the guidance would help a user resolve the issue. They consider:
- Would these steps likely solve the problem?
- Are the steps in a logical diagnostic order?
- Are the instructions clear enough for the target user?
- Does the guidance acknowledge uncertainty ("if this doesn't work, try...")?
- Are there any suggestions that could make things worse?

**Probabilistic guidance is acceptable:**

Troubleshooting is inherently uncertain — you don't know the root cause until you diagnose. So AI guidance that says "This is most often caused by X, but could also be Y or Z" is appropriate and helpful.

**Example:**

Problem: "My WiFi is connected but internet isn't working"

AI guidance:
"Let's troubleshoot your internet connection step by step:

1. First, verify the problem: Try opening a website on another device connected to the same WiFi. If that device works, the problem is with your original device, not your internet. If it also doesn't work, continue to step 2.

2. Restart your router: Unplug it, wait 30 seconds, plug it back in. Wait 2-3 minutes for it to fully restart. This solves the issue about 60% of the time.

3. Check for outages: Visit your internet provider's website on mobile data or call them to confirm there's no service outage in your area.

4. Check router lights: Your router should have a steady light for internet/WAN. If it's red, amber, or blinking, that indicates a connection problem with your provider.

5. Check cables: Ensure the cable from the wall to your router is securely connected at both ends.

If none of these work, the issue likely requires contacting your internet provider's support. Let me know if you want their contact information."

Annotation:
- Relevance: 5/5 (Directly addresses the problem)
- Completeness: 4/5 (Covers common causes, could mention DNS issues for thoroughness)
- Prioritization: 5/5 (Steps ordered from most to least likely cause)
- Clarity: 5/5 (Non-technical user can follow these steps)
- Safety: Pass (No risky suggestions)
- Overall: 4.5/5 (Very helpful guidance)

**The result:**

The troubleshooting system achieves a 4.0/5.0 average helpfulness rating. User self-resolution rate (solving the problem without contacting human support) is 55%, up from 30% before the AI was deployed. This suggests the guidance is helpful enough to resolve many issues, while appropriately escalating complex problems to human support.

## The Difference Between "Wrong" and "Unhelpful"

This is a crucial distinction for Tier 2 ground truth.

**Wrong** means factually incorrect or misleading. The AI said Tokyo Tower is 1,000 meters tall (it's 333 meters). This is wrong.

**Unhelpful** means not useful to the user, even if not factually wrong. The AI suggested visiting Tokyo Tower when the user asked about modern architecture (Tokyo Tower is a 1958 structure inspired by the Eiffel Tower — not really modern architecture). This isn't wrong, but it's not helpful given the user's interest.

Tier 2 ground truth should capture both:
- Factual accuracy: Are verifiable claims correct?
- Practical helpfulness: Does this information help the user?

An answer can be factually correct but unhelpful (technically accurate but not what the user needs). An answer can be approximately correct and helpful (simplified explanation that aids understanding). An answer can be wrong and harmful (incorrect facts that mislead).

Your annotators need to evaluate both dimensions, not collapse them into a single quality score.

## Framing Uncertainty Appropriately

When the AI is uncertain in Tier 2 contexts, it should communicate that uncertainty in ways that are still helpful:

**Acknowledge multiple valid perspectives:**
"Different developers have different opinions on this. Some prefer approach A because of X. Others prefer approach B because of Y."

**Provide probabilistic guidance:**
"This issue is most commonly caused by network settings, but it could also be a firewall configuration or VPN interference."

**Explain limitations:**
"I don't have information about your specific model's capabilities, but generally speaking, devices in this category can..."

**Invite refinement:**
"I'm not sure exactly what you're looking for. Are you interested in family-friendly restaurants, fine dining, or casual local spots?"

These phrasings acknowledge uncertainty while still providing value. The user learns that there isn't one right answer, or that more context would help, or that the AI's knowledge has limits. That's useful information.

Ground truth annotation should reward this appropriate uncertainty framing. An answer that says "I'm not sure, but here are some options..." can score higher on helpfulness than an answer that confidently suggests one wrong thing.

## If You Skip This, Here's What Happens

Teams that treat Tier 2 tasks with Tier 0/1 rigor (demanding perfect accuracy) face:

**Annotation paralysis:** Annotators spend hours debating which product recommendation is "correct" when there are multiple valid recommendations.

**Delayed shipping:** Trying to achieve perfect explanations or recommendations means slow iteration and late product delivery.

**Missed opportunities:** Pursuing perfection on advisory features means you can't ship enough volume to be helpful.

**Poor user experience:** Over-cautious AI that refuses to make recommendations unless certain, or only recommends the single "safest" option, is less helpful than AI that offers multiple good suggestions.

Teams that treat Tier 2 tasks too casually (no quality standards at all) face:

**User frustration:** Recommendations are irrelevant, explanations are confusing, summaries miss the point. Users stop trusting the AI.

**Wasted user time:** Bad advice sends users down wrong paths, unhelpful summaries force them to read the full document anyway.

**Support burden:** Users contact support when the AI's guidance doesn't work, asking "Why did you suggest this?"

**Competitive disadvantage:** Users switch to products with more helpful AI.

The sweet spot is Tier 2 standards: helpfulness ratings on scales, single-annotator review (with multi-annotator for calibration), evaluation of multiple quality dimensions, factual accuracy verification, and acceptance of "good enough" when it's genuinely helpful.

Next, we'll explore Tier 3, where even the concept of "correctness" breaks down entirely, and ground truth shifts to evaluating creativity and inspiration.

