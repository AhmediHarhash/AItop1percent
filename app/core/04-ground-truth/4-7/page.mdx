# 4.7 — The First 50 Examples: Your Foundation

Let me tell you about the most important afternoon you'll spend in your entire AI project.

Not the day you choose your model. Not the day you launch. Not the day you hit your performance targets. The afternoon you sit down and carefully craft your first fifty ground truth examples.

Those fifty examples will shape every decision that follows. They'll define what "good" means for your team. They'll be the calibration set for new annotators. They'll be the test set you return to again and again to measure progress. They'll be the examples you show executives to explain your quality standards. They'll be the foundation every other example is compared against.

Most teams treat their first examples as rough drafts — quick sketches they'll refine later. This is a catastrophic mistake. Your first fifty examples are not a draft. They're your constitution. Everything else is case law.

Let me walk you through how to make them count.

## Why Fifty Is The Magic Number

We talked about this briefly in section 4.1, but it's worth unpacking further. Why fifty? Why not ten or a hundred or a thousand?

Ten is too few. You can't see patterns with ten examples. Every example feels unique. You don't have enough to cover major categories with multiple instances. Ten examples is a sketch, not a foundation.

A hundred is too many for your first set. You'll exhaust your focus before you finish. The quality will drift. The last twenty will be noticeably worse than the first twenty because you're tired and pattern-matching instead of thinking carefully. You haven't yet internalized what matters, so you'll waste effort on examples that don't teach anything new.

Fifty is the sweet spot. Large enough to see patterns, small enough to maintain quality, structured enough to cover key categories, flexible enough to include edge cases.

More specifically, fifty lets you do this structure:

- Ten canonical "great" examples (your gold standard)
- Ten canonical "terrible" examples (your red lines)
- Ten edge cases (the tricky stuff)
- Ten "good enough" examples (where the bar is)
- Ten controversial or boundary cases (where reasonable people disagree)

This structure ensures coverage across the quality spectrum and the complexity spectrum. You're not just showing what good looks like — you're mapping the entire space from excellent to awful, from obvious to ambiguous.

## The Ten Canonical Great Examples

Start here. These are your aspirational standard — the responses you'd be proud to show customers, executives, and competitors. If someone asks "what does success look like?" you point to these ten.

But here's the key: they must be DIVERSE. Not ten variations on the same theme. Ten genuinely different archetypes of excellence.

If you're building a customer support bot, your ten great examples might include:

1. A clear, accurate answer to a simple factual question ("What's your return policy?") — shows you can do the basics well

2. A warm, empathetic response to an upset customer — shows emotional intelligence

3. A nuanced answer to a multi-part question — shows you can handle complexity

4. A response that proactively anticipates the customer's next question — shows helpfulness beyond the minimum

5. An answer that requires looking up account-specific information — shows you can handle context

6. A response that politely declines an unreasonable request — shows boundary-setting

7. An answer that includes appropriate citations or links — shows grounding

8. A response to a question about a complex technical topic explained simply — shows clarity

9. An answer that adapts tone to match a formal business inquiry — shows flexibility

10. A response that successfully disambiguates an unclear question — shows comprehension

Notice each example tests something different. Together, they define the full range of capabilities you aspire to.

For each great example, write:

- The input (customer question or scenario)
- The perfect response
- A detailed explanation of WHY this is great
- The key quality dimensions it exemplifies
- What would make it less great (so you're defining not just the target but the boundaries)

That last part is crucial. It's not enough to say "this is great." You need to articulate what specifically makes it great, so you can recognize greatness in new examples. This is how you convert examples into transferable principles.

## The Ten Canonical Terrible Examples

Now for the opposites. These are responses so bad they're unacceptable under any circumstances. Your red lines.

Terrible examples teach boundaries. They show what failure looks like. They help you build intuition for what to avoid. They're especially valuable for new team members who don't yet have a sense of what's obviously wrong.

Your terrible examples should also be diverse in their failure modes:

1. Factually wrong answer — fails accuracy
2. Rude or dismissive response — fails empathy
3. Correct but completely incomprehensible jargon — fails clarity
4. Answer to a different question than what was asked — fails comprehension
5. Response that creates legal liability — fails safety
6. Answer that exposes private information — fails privacy
7. Response that's correct but wildly off-brand — fails brand alignment
8. Answer that's so verbose it buries the point — fails efficiency
9. Response that makes promises the company can't keep — fails reliability
10. Answer that's technically accurate but completely unhelpful — fails utility

Each terrible example should make you wince. If you're not actively uncomfortable putting them in your ground truth set, they're not terrible enough.

For each terrible example, document:

- The input
- The bad response
- A detailed explanation of WHY this is unacceptable
- The specific harm it could cause
- What a minimally acceptable response would look like (so you're showing the path from terrible to tolerable)

Terrible examples are not just about avoiding mistakes. They're about defining your non-negotiables. These are the failure modes that justify stopping a launch or rolling back a deployment.

## The Ten Edge Cases

Edge cases are where the real learning happens. These are scenarios that don't fit clean patterns, where the right answer isn't obvious, where multiple considerations pull in different directions.

Edge cases force you to articulate principles instead of relying on pattern matching. They reveal the nuance in your domain.

Examples of edge cases for a customer support bot:

1. Customer asks a question that's half about a technical issue and half about wanting emotional support — tests prioritization

2. Question that's ambiguous and could be interpreted two valid ways — tests disambiguation strategy

3. Customer asks for something that's against policy but policy has exceptions and this might be one — tests judgment

4. Question that requires information from multiple systems that might contradict each other — tests conflict resolution

5. Customer uses hostile language but asks a legitimate question — tests tone management

6. Question that's technically outside your scope but easy to answer — tests helpfulness vs boundary-setting

7. Customer asks the same question they asked yesterday and got a correct answer to — tests repetition handling

8. Question where the honest answer is "our product doesn't do that but our competitor's does" — tests integrity vs sales

9. Customer asks a question that's factually answerable but reveals they're confused about something deeper — tests surface vs root cause

10. Question that has different right answers depending on context you don't have — tests information gathering

For each edge case, document:

- The scenario
- What makes it tricky
- Multiple reasonable approaches and trade-offs
- Your recommended approach and why
- What you'd need to know to be more confident

Edge cases are often the best candidates for team discussion. These are the ones where you want multiple people weighing in to refine your thinking.

## The Ten Good Enough Examples

This category is underrated and often skipped. These are responses that pass, that meet the bar, but aren't excellent. They're the B+ answers.

Why include these? Because most of your AI's outputs will be in this category. Perfect is rare. Terrible is rare. Good enough is common. You need to define where the "good enough" bar is so you're not constantly trying to achieve unattainable perfection.

Good enough examples might have minor flaws:

- Slightly more verbose than ideal but not excessively so
- Tone is a bit off-brand but not offensive
- Covers the question but doesn't anticipate follow-ups
- Technically correct but could be explained more clearly
- Has the right information but formatting is suboptimal
- Appropriate but not particularly warm or engaging
- Answers the question but misses an opportunity to add value
- Correct but cites a less-than-ideal source
- Good answer but took a circuitous path to get there
- Right information but presented in the wrong order

For each good enough example, document:

- The input
- The response
- Why it's acceptable despite flaws
- What would elevate it to great
- Where the line is between good enough and not good enough

This calibration prevents two failure modes: perfectionism (rejecting acceptable outputs because they're not excellent) and complacency (accepting mediocre outputs because they're not terrible). Good enough examples define the middle ground.

## The Ten Controversial Cases

These are scenarios where reasonable people disagree about what the right answer is. Maybe experts differ. Maybe different stakeholders have different priorities. Maybe the "right" answer depends on values or trade-offs.

Controversial cases are valuable because they force you to make explicit choices about values and priorities. They prevent you from pretending hard problems have easy answers.

Examples:

1. Should you prioritize user privacy or user convenience when they're in tension?
2. Should you admit uncertainty and risk looking incompetent, or give the best guess and risk being wrong?
3. Should you follow official policy that's outdated or provide information you know is actually correct?
4. Should you be warm and casual (accessible) or professional and formal (credible)?
5. Should you give a complete answer that's hard to understand or a simplified answer that's slightly inaccurate?
6. Should you promote your product when it's relevant or stay neutral to maintain trust?
7. Should you correct a customer's misconception bluntly or gently even if it takes longer?
8. Should you follow a script for consistency or adapt for this specific situation?
9. Should you protect company interests or advocate for the customer when they conflict?
10. Should you optimize for immediate satisfaction or long-term customer education?

For each controversial case, document:

- The scenario
- The competing considerations
- What different stakeholders might prioritize
- Your current stance and the reasoning
- That this is subject to revision as you learn more

Controversial cases should be revisited quarterly. As you gather data on what actually works, some controversies resolve. Others stay genuinely ambiguous, and that's okay — you document the ambiguity instead of pretending it doesn't exist.

## Crafting Each Example With Intention

Now let's talk about how to write individual examples within this structure. Each example should be:

Realistic: Based on real inputs you'll encounter, not hypotheticals. If you're guessing what questions users will ask, you're probably wrong. Pull from support tickets, user research, beta testing, or adjacent domains.

Specific: Not "user asks about pricing" but "user with a free trial expiring in three days asks whether the team plan includes the advanced analytics feature they've been using in trial."

Complete: Include all context needed to evaluate the response. What tier is the customer on? What's their history? What time zone? What device? Real evaluation requires real context.

Unambiguous in label: For your canonical examples (great and terrible), there should be zero debate about the label. If you have to argue about whether an example is great, it's not great — it's an edge case or controversial case.

Teachable: Each example should teach something. If an example doesn't illuminate a principle or test a capability that's different from your other examples, cut it. Fifty is too few to waste on redundancy.

Documented: The example itself is not enough. You need the metadata explaining what it teaches and why it matters.

## The Collaborative Review Process

Don't create your first fifty alone. This is a team exercise.

Process:

Week one: Primary owner drafts thirty examples, getting diverse input from team members for different categories.

Week two: Full team reviews the thirty. Debate. Refine. Identify gaps.

Week three: Primary owner drafts twenty more to fill gaps. Team reviews again.

Week four: Test the fifty. Give them to someone new to your project and ask them to evaluate ten responses using only these fifty examples as guidance. Do they understand what good means? If not, your examples aren't clear enough.

The review process is not about approval. It's about calibration. You're building shared understanding of quality across the team. The debates that happen during review are valuable — they surface disagreements about priorities that would otherwise stay hidden until they cause problems later.

## Versioning Your First Fifty

Your first fifty will evolve. That's expected. But you need version control.

When you revise an example:

- Document what changed and why
- Keep the old version with a clear deprecation note
- Update any test cases that referenced the old version
- Communicate the change to anyone who uses these examples

Major revisions (changing what "good" means) warrant a version bump and team discussion. Minor revisions (fixing typos, updating product details) can happen more fluidly.

Aim for stability. Your first fifty should be refined in the first three months, then largely stable for the next year. If you're constantly revising, you don't have clarity on your quality bar yet.

## Using Your First Fifty

Once you have them, your first fifty examples serve multiple purposes:

Calibration set: New annotators study these before labeling anything else. They're learning what your quality standards are.

Regression test: Every time you update your AI, run it on these fifty. If performance on this set drops, something broke.

Communication tool: Stakeholders understand these fifty examples better than they understand rubrics or metrics.

Boundary finder: When you encounter a new scenario and aren't sure how to evaluate it, compare it to your fifty. Which examples is it most similar to? That guides your judgment.

Training data: For some systems, your first fifty become few-shot examples in prompts or training data for fine-tuning.

Argumentation baseline: When someone says "the AI should do X," you can point to relevant examples from your fifty and say "here's how we've defined good for this type of scenario."

## Anti-Patterns To Avoid

Let me warn you about common mistakes:

All easy examples: If you can evaluate all fifty examples in thirty seconds each, they're too simple. Include examples that require thought.

All hard examples: If every example is an edge case, you haven't defined the baseline. Include obvious goods and bads.

Trendy over foundational: Don't chase edge cases that are interesting but rare. Cover the fundamentals first.

Examples without context: "User asks about pricing" is not enough detail to evaluate a response. Include realistic context.

Examples that age poorly: "User asks about our new feature launched last week" will be confusing in six months. Make examples that stay relevant.

Too much similarity: Ten variations of the same question doesn't teach as much as ten different question types.

No clear teaching point: Each example should illuminate something specific. If you can't articulate what an example teaches, cut it.

## The Fifty-Example Test

Here's how you know your first fifty are good:

Test one: Give them to a new team member. After reading all fifty and their documentation, can they correctly evaluate ten new examples with seventy-five percent agreement with experts? If yes, your fifty are teaching effectively.

Test two: Look at each example. Can you immediately articulate what quality dimension or principle it teaches? If not, it's not pulling its weight.

Test three: Look across all fifty. Do they cover the major categories of inputs you'll see, the major failure modes you need to avoid, and the major quality dimensions you care about? If there's a glaring gap, fill it.

Test four: Ask yourself honestly: would you be comfortable showing these fifty examples to customers, executives, or regulators as your definition of quality? If not, they're not ambitious enough or clear enough.

Test five: Run your current AI on these fifty. Does performance on this set correlate with your intuition about overall quality? If the AI scores well on your fifty but feels wrong in practice, your fifty aren't representative.

## Your First Fifty Are Never Finished

One final point: your first fifty are your foundation, but foundations need maintenance.

Set quarterly reviews:

- Are all fifty still relevant to the current product?
- Have any become obsolete?
- Do you need to add new categories?
- Has your understanding of quality evolved?
- Do the examples still reflect real user needs?

Treat your first fifty examples with the respect they deserve — not as a one-time exercise but as a living document that captures your organization's evolving understanding of what "good" means.

In the next section, we'll talk about converting your examples into rubrics. Examples show WHAT good looks like. Rubrics explain WHY it's good and how to recognize goodness in new situations. Let me show you how to make that translation without losing the nuance that makes your examples valuable.
