# 6.6 — Classification & Extraction

Let me tell you about the content moderation system that accidentally censored 30,000 legitimate posts in one day. The company had built an AI classifier to detect hate speech. Their ground truth definition seemed clear: label examples as "hate speech" or "not hate speech." They trained the model, reached 94% accuracy on their test set, and deployed it.

Within hours, the system was flagging posts discussing the history of racism (educational content), LGBTQ+ advocacy (using reclaimed language), and even song lyrics. The model had learned patterns but not context. It could identify words but not understand intent. Their ground truth had been too simplistic for the complexity of the real task.

This is the trap of classification ground truth. The task looks simple—just put things in categories—but defining those categories correctly is incredibly difficult. What seems like an objective labeling task ("is this hate speech?") is actually deeply contextual and requires nuanced judgment.

## Why Classification Looks Easy But Isn't

Classification feels straightforward: you have a set of categories, and you put each item into the right category. Email goes in spam or not spam. Sentiment is positive, negative, or neutral. Simple, right?

Wrong. Here's what makes classification ground truth hard:

**Boundary Cases**: Most examples are clearly in one category or another. But the examples that matter—the ones where the model makes mistakes—are right on the boundary. Your ground truth needs to handle these edge cases consistently.

**Overlapping Categories**: Sometimes an item legitimately belongs to multiple categories. Is "This product is terrible but shipping was fast" positive or negative sentiment? Both? Your ground truth needs rules for overlaps.

**Context Dependency**: The same text can belong to different categories depending on context. "This is sick!" is positive if it's about a skateboarding video, negative if it's about food. Your ground truth needs to encode context.

**Subjective Judgment**: Many classification tasks require human judgment, and humans disagree. Your ground truth needs to handle disagreement systematically.

Let's walk through how to build ground truth that actually works for classification tasks.

## Category Definition: The Foundation

Before you label anything, you need crystal-clear category definitions. This is where most teams mess up.

Bad category definition: "Positive sentiment: The text expresses positive feelings."

This is circular (positive = positive) and vague (what counts as feelings?). Labelers will interpret this differently.

Good category definition: "Positive sentiment: The author expresses satisfaction, approval, or enthusiasm about the subject. Examples: praising a product, expressing happiness, recommending something. Non-examples: neutral descriptions, mixed feelings where negative outweighs positive."

This is specific, includes examples and non-examples, and gives labelers something concrete to apply.

For every category in your classification scheme, write:

**Definition**: What does this category mean?

**Inclusion Criteria**: What characteristics must an item have to belong here?

**Exclusion Criteria**: What disqualifies an item from this category?

**Positive Examples**: Clear cases that definitely belong

**Negative Examples**: Clear cases that definitely don't belong

**Boundary Examples**: Tricky cases and how to handle them

This documentation becomes your ground truth specification. Every labeler uses it to make consistent judgments.

## Mutually Exclusive vs Overlapping Categories

One of the first decisions in classification ground truth: can items belong to multiple categories, or just one?

**Mutually Exclusive Categories**

Each item belongs to exactly one category. Example: email classification (spam, promotions, social, updates, personal).

Ground truth requirement: Categories must cover all possible items (exhaustive) and must not overlap (exclusive). Every email must fit somewhere, and it can't be both spam AND personal.

This requires careful category design. You need an "other" or "unclassified" category for things that don't fit elsewhere. And you need clear rules for edge cases.

Example edge case: "Your friend Bob wants to sell you supplements!" Is this personal (from a friend) or spam (selling something)? Your ground truth must specify. Maybe the rule is: "If the primary intent is commercial, classify as spam even if the sender is a friend."

**Multi-Label Categories**

Items can belong to multiple categories. Example: article tagging (an article can be about both "technology" and "business").

Ground truth requirement: Each category is evaluated independently. For each item, ask "Does this belong to category X?" for every category.

This is more flexible but harder to label. You need to consider each category separately and avoid bias (not assuming that because something is labeled "technology" it can't also be "business").

Your ground truth specification must be clear about which approach you're using and provide rules for ambiguous cases in either scenario.

## Handling "Other" and "Unknown"

Every classification scheme needs a way to handle items that don't fit any category. But how you do this matters.

**The "Other" Category**

Items that are legitimate but don't fit existing categories. Example: email classification where you have spam, personal, work, and "other" for everything else (newsletters, automated notifications, etc.).

Ground truth for "other" requires negative definition: "Everything that doesn't fit the other categories." This is broad and often becomes a catch-all that's hard to learn from.

Better approach: if "other" gets large, analyze it and create new specific categories. If 20% of your emails are in "other" and half of those are newsletters, make "newsletters" a category.

**The "Unknown" Category**

Items where you genuinely can't make a judgment. Example: sentiment analysis where the text is too ambiguous or lacks context.

Ground truth for "unknown" should be rare. It means "even an expert can't classify this with confidence." If you're using "unknown" for more than 1-2% of cases, your categories are poorly defined or you need more context in your data.

**Confidence Levels**

Instead of (or in addition to) an "unknown" category, you can label confidence:
- High confidence: Clearly belongs to this category
- Medium confidence: Probably belongs here, but some ambiguity
- Low confidence: Best guess, but uncertain

This gives your model more nuanced ground truth to learn from and helps you identify where human judgment is unreliable.

## Label Hierarchy: Coarse vs Fine-Grained

Many classification tasks have natural hierarchies. Example: product categorization (Electronics > Computers > Laptops > Gaming Laptops).

Your ground truth can be:

**Coarse-grained**: Only the top level (Electronics)

**Fine-grained**: The most specific level (Gaming Laptops)

**Hierarchical**: All levels (Electronics, Computers, Laptops, Gaming Laptops)

Which you choose affects both labeling cost and model performance.

**Coarse-grained Ground Truth**

Pros: Easier to label consistently, higher inter-labeler agreement, cheaper
Cons: Less useful for users who want specific categories

Use when: The specific subcategory doesn't matter much, or you have limited labeling resources

**Fine-grained Ground Truth**

Pros: More useful, enables specific filtering and search
Cons: Harder to label consistently, more labeling errors, more categories to distinguish

Use when: Specific categories matter for your use case

**Hierarchical Ground Truth**

Pros: Flexible (users can drill down or stay high-level), enables fallback (if model unsure about Gaming Laptop, at least it knows it's a Laptop)
Cons: Most expensive to label, requires complex model architecture

Use when: You have diverse user needs (some want broad categories, some want specific)

Your ground truth specification should define which level(s) you're labeling and how to handle items that are ambiguous at fine-grained levels but clear at coarse levels.

## Extraction Ground Truth: Structured Data from Unstructured Text

Extraction is related to classification but with a key difference: instead of putting the whole item in a category, you're pulling out specific pieces of information.

Example: Extract name, email, and phone number from a contact form submission.

Ground truth for extraction needs to specify:

**Schema**: What fields are you extracting?

**Data Types**: What format should each field be in? (phone as "555-1234" or "(555) 123-4567"?)

**Missing Data**: How do you handle fields that aren't present? (null, empty string, or "N/A"?)

**Ambiguous Data**: How do you handle uncertainty? (if two names appear, extract both or pick the most likely?)

Let me show you how this gets complex fast.

## Extraction Schema Design

Consider extracting information from job postings:

Bad schema:
- Job Title (text)
- Description (text)

This is too simple. You'll get inconsistent extractions.

Good schema:
- Job Title (text, required)
- Company Name (text, required)
- Location (text, may be "Remote")
- Salary Range (text or null, format: "XX,000 - YY,000" or "Competitive")
- Employment Type (enum: Full-Time, Part-Time, Contract, or null)
- Required Skills (list of text, may be empty)
- Experience Level (enum: Entry, Mid, Senior, or null)

This is specific. Each field has a type, format, and rule for missing data.

Your ground truth labels should follow this schema exactly. If one labeler writes "Senior" and another writes "senior level," your ground truth is inconsistent.

Provide extraction guidelines:

**Format Rules**: How to format each field (capitalization, punctuation, abbreviations)

**Normalization Rules**: How to standardize variations ("New York" vs "NY" vs "New York City")

**Ambiguity Rules**: How to handle multiple valid extractions (if posting mentions multiple locations, extract all or just primary?)

**Missing Data Rules**: When to leave a field null vs fill with default value

## Handling Missing Fields

Extraction often involves missing fields. The job posting doesn't mention salary. The receipt doesn't show a tip amount. The email doesn't include a phone number.

Your ground truth must distinguish:

**Explicitly Not Present**: The information genuinely isn't in the source. (Extract as null)

**Implicitly Present**: The information is implied but not stated. (Maybe extract, with a guideline on when to infer)

**Ambiguously Present**: The information might be there, hard to tell. (Mark as uncertain or null)

Example: Job posting says "competitive salary."

- Explicitly present: There's a salary (competitive)
- Not present: There's no specific range
- Should you extract "Competitive" or leave it null?

Your ground truth specification must decide and be consistent.

## Entity Resolution in Extraction

Sometimes you extract entities that need to be normalized or resolved.

Extract company names from news articles:
- "Microsoft" (easy)
- "MS" (abbreviation, should normalize to Microsoft)
- "the Redmond-based tech giant" (reference, should resolve to Microsoft)

Your ground truth should specify:

**Canonical Form**: What's the standard name? (Microsoft Corporation, Microsoft, or MSFT?)

**Resolution Rules**: How to handle abbreviations, references, and variations?

**Ambiguity Handling**: When the same name could refer to multiple entities (Apple the company vs apple the fruit)

This gets complex fast. Many teams create an entity database: a list of canonical entities with their known variations. Ground truth extraction always uses canonical forms.

## Multi-Field Extraction and Relationships

Some extraction tasks involve multiple related fields, and the relationships matter.

Extract flight information:
- Departure City: "New York"
- Arrival City: "London"
- Departure Time: "8:00 AM"
- Arrival Time: "8:00 PM"

But wait—8:00 AM in what timezone? 8:00 PM in what timezone? Are these local times or UTC?

Your ground truth needs to capture:

**Field Dependencies**: Some fields provide context for others (timezone depends on city)

**Relationship Constraints**: Some combinations are valid, others aren't (can't arrive before departing)

**Contextual Information**: Some fields need additional context to be meaningful

A better extraction schema includes:
- Departure City: "New York, NY, USA"
- Departure Time: "8:00 AM EST"
- Arrival City: "London, England, UK"
- Arrival Time: "8:00 PM GMT"

Now the times are unambiguous. Your ground truth should specify the level of detail required for each field.

## Nested and Hierarchical Extraction

Some extraction tasks involve nested structures.

Extract order information from an email:
- Order ID: "12345"
- Order Date: "2026-01-25"
- Items:
  - Item 1: "Laptop"
    - Quantity: 1
    - Price: "$999"
  - Item 2: "Mouse"
    - Quantity: 2
    - Price: "$25 each"

This is hierarchical. Your ground truth needs to capture the structure, not just the values.

**Flat Approach**: Extract each piece independently, lose structure

**Structured Approach**: Extract nested objects, preserve relationships

The structured approach is better but harder to label. You need labeling tools that support hierarchical data, not just flat key-value pairs.

Your ground truth should specify:
- The schema structure
- How to handle variable numbers of items (some orders have 1 item, some have 10)
- How to handle nested optionality (some items have discount codes, some don't)

## Confidence and Uncertainty in Extraction

Sometimes the extraction is uncertain. The text is ambiguous, the handwriting is unclear, the format is non-standard.

Your ground truth should include confidence levels:

**Certain**: This extraction is definitely correct

**Probable**: This is most likely correct, but some ambiguity

**Uncertain**: This is a guess, low confidence

**Unable**: Cannot extract this field with any confidence

This helps in two ways:

1. Training: Models can learn to express uncertainty
2. Evaluation: You can separate model errors (wrong answer) from ambiguous ground truth (humans unsure)

When labelers disagree on an extraction, that's a signal of genuine ambiguity. Don't force consensus—record the disagreement and mark the item as uncertain.

## The Special Case: Extraction from Noisy Sources

Extraction from receipts, handwritten forms, low-quality scans, or damaged documents is especially hard.

Your ground truth needs to handle:

**OCR Errors**: The text extraction itself has errors. "Invoice #12345" becomes "Inv0ice #1Z345"

**Partial Information**: Parts of the document are illegible or cut off

**Format Variations**: Every receipt from every vendor has a different layout

Your ground truth should:

1. Include the raw OCR text (with errors)
2. Include the correct extraction (despite errors)
3. Note which fields are affected by OCR errors
4. Mark illegible fields as "unable to extract"

This lets you evaluate whether your model can extract correctly despite noisy input.

## Inter-Labeler Agreement: The Quality Check

For classification and extraction, inter-labeler agreement is critical. This measures how often labelers agree on the same item.

Calculate agreement rate: (number of items where labelers agreed) / (total items)

For binary classification (spam / not spam), reasonable agreement is above 90%. Below that, your category definitions are unclear.

For fine-grained classification (50 categories), agreement above 80% is good. It's harder to agree when there are more options.

For extraction, agreement is trickier because there are more ways to disagree. Maybe labelers extract the same information but format it differently. Your ground truth specification needs to be very specific to get high agreement.

When agreement is low:

1. Review your category/schema definitions—are they clear enough?
2. Review examples where labelers disagree—are these genuinely ambiguous cases?
3. Add more guidelines for the disagreement patterns you see
4. Retrain labelers on the updated guidelines

Don't just accept low agreement. It means your ground truth is unreliable.

## Handling Subjectivity

Some classification tasks are genuinely subjective. Sentiment analysis, content quality, appropriateness for age groups—these involve judgment.

Your ground truth options:

**Majority Vote**: Use the label that most labelers chose. Works when most labelers agree.

**Consensus**: Only include items where all labelers agree. Smaller dataset, but higher quality.

**Weighted Vote**: Weight labelers by expertise or historical accuracy. Expert opinions count more.

**Multiple Labels**: Keep all labels and train the model to predict the distribution. If 3 labelers say positive and 2 say neutral, the ground truth is 60% positive, 40% neutral.

The last approach is most honest about subjectivity but requires different model training techniques.

## The Annotation Tool Matters

Your labeling tool affects ground truth quality. A good tool:

**Enforces Schema**: Doesn't let labelers enter invalid formats (phone numbers must match a pattern, dates must be real dates)

**Provides Context**: Shows labelers relevant information (previous sentences for context, related fields in extraction)

**Enables Efficiency**: Keyboard shortcuts, auto-suggestions, bulk operations

**Tracks Disagreement**: Flags items where labelers disagree for review

**Supports Hierarchy**: For hierarchical labels or nested extraction

I've seen teams lose months of labeling work because their tool didn't enforce the schema and labelers used inconsistent formats.

Invest in a good annotation tool or build guidelines that compensate for tool limitations.

## Active Learning: Smarter Ground Truth Collection

You don't need to label everything randomly. Active learning focuses labeling effort on items the model is uncertain about.

The process:

1. Label a small initial set (maybe 1000 examples)
2. Train a model
3. Run the model on unlabeled data
4. Label the examples where the model is most uncertain (close to decision boundary)
5. Retrain with new labels
6. Repeat

This finds ground truth for the cases that matter most, improving your model faster with less labeling.

Your ground truth collection strategy should include active learning for efficiency.

## Domain-Specific Classification Challenges

Different domains have unique ground truth challenges.

**Medical Text Classification**: High stakes, requires expert labelers (doctors, nurses), disagreement common due to genuine diagnostic uncertainty

**Legal Document Classification**: Requires legal expertise, precedent matters, small details can change categories

**Content Moderation**: Cultural context matters enormously, policies evolve, edge cases are the norm

**Financial Document Extraction**: Regulatory requirements for accuracy, specific formats, high cost of errors

Tailor your ground truth approach to your domain's specific needs.

## The Warning: What Happens If You Skip This

If you define classification categories vaguely or don't specify extraction schemas clearly, here's what happens:

Your labelers will be inconsistent. The same example will be labeled differently by different people or even by the same person at different times. Your ground truth will be unreliable, and your model will learn noise instead of signal.

You'll achieve 85% accuracy on your test set and think you're doing well, but in production you'll get constant edge cases that don't match your categories. Users will be frustrated by inconsistent predictions.

I've seen teams spend $50,000 on labeling only to discover their guidelines were so vague that the labels were mostly useless. They had to relabel everything.

Define your categories crisply. Specify your schemas precisely. Test your guidelines on a small batch before scaling up. Measure inter-labeler agreement. Fix inconsistencies early.

Classification and extraction look simple. The ground truth is anything but.

## Bridge to Code Generation

We've been talking about classifying and extracting from existing content—sorting things into buckets and pulling out structured information. But what about when your AI creates entirely new content that has to actually execute and work? Code generation takes us into a different realm: the output isn't just text to read, it's instructions that a machine will follow. And "correct" code isn't just code that compiles—it's code that's functional, secure, efficient, readable, and maintainable. Let's walk through what ground truth looks like when your AI is writing code that has to actually run.
