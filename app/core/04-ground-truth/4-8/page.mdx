# 4.8 — From Examples to Rubrics

Let me show you the moment when your ground truth work either clicks into place or falls apart.

You've built your first fifty examples. They're beautiful. They show exactly what good looks like, what terrible looks like, what edge cases look like. You're proud of them.

You hand them to a new annotator and say "use these to evaluate our AI."

They look at the examples. They look at a new response they need to evaluate. They look back at the examples. They say: "Okay, but... this new response doesn't look exactly like any of these examples. How do I know if it's good?"

You realize the problem. Examples show specific instances. But your annotator needs to evaluate infinite variations. They need principles, not just samples. They need to understand WHY your great examples are great, so they can recognize greatness in novel situations.

That's what rubrics do. They extract the principles from your examples and make them portable. They're the bridge from "here are ten great responses" to "here's how to evaluate any response."

Most teams skip this step or do it badly. They create rubrics that are either so vague they're useless ("response should be helpful") or so rigid they're brittle ("response must be between 50 and 75 words"). Neither works.

Let me walk you through how to create rubrics that actually help people make consistent, nuanced judgments.

## What Rubrics Actually Are

First, let's be clear about what a rubric is and isn't.

A rubric is not a checklist. Checklists are binary: "Does the response include a greeting? Yes/No." Checklists work for mechanical tasks. They fail for tasks requiring judgment.

A rubric is not a formula. Formulas combine metrics mathematically: "Quality score = 0.4 × accuracy + 0.3 × empathy + 0.2 × brevity + 0.1 × brand alignment." Formulas pretend that quality is arithmetic. It's not.

A rubric is a structured framework for judgment. It identifies the dimensions that matter, defines what excellence looks like on each dimension, provides anchors showing different levels of performance, and guides evaluators through consistent decision-making without eliminating nuance.

Good rubrics capture expertise. When an expert evaluates a response, they're unconsciously weighing multiple factors and using pattern recognition built from thousands of examples. A rubric makes that unconscious process conscious and teachable.

## Identifying Your Quality Dimensions

Start by looking across your first fifty examples. Ask: what dimensions separate good from bad?

For a customer support bot, you might identify:

Accuracy: Is the information correct?
Completeness: Does it answer the full question?
Clarity: Is it easy to understand?
Empathy: Does it acknowledge the customer's feelings?
Tone: Does it match brand voice?
Actionability: Does it tell the customer what to do next?
Safety: Does it avoid creating liability or harm?

These dimensions come from your examples. Look at why your great examples are great and your terrible examples are terrible. The reasons cluster into dimensions.

A common mistake is having too many dimensions. If you have fifteen quality dimensions, evaluators will be overwhelmed and inconsistent. Aim for five to eight dimensions that matter most.

Another mistake is having overlapping dimensions. "Helpfulness" and "utility" are probably the same thing. "Professional tone" and "brand alignment" might be redundant. Combine overlapping dimensions to avoid confusion.

A third mistake is having dimensions you can't actually evaluate. "Customer satisfaction" is not a dimension of a response — it's an outcome you can only measure later. Stick to properties observable in the response itself.

## Defining Levels Within Dimensions

For each dimension, define levels. Most teams use either:

Pass/fail: Only two levels. Either it meets the bar or it doesn't.

Three levels: Poor, acceptable, excellent. Simple enough to be consistent.

Five levels: Poor, below average, acceptable, good, excellent. More granular.

Numerical scale: 1-10 or 1-100. Maximum granularity.

Each approach has trade-offs. Pass/fail is easy to apply consistently but loses nuance. Numerical scales capture nuance but invite spurious precision — is this really a 7.2 or a 7.3? Most teams find three or five levels to be the sweet spot.

Whatever scale you choose, define what each level means for each dimension.

For example, accuracy dimension with five levels:

Level 1 (Poor): Response contains factual errors that would mislead or harm the customer. Information is wrong and could not be defended.

Level 2 (Below average): Response is partially correct but contains minor errors or outdated information. Not egregiously wrong but not reliable.

Level 3 (Acceptable): Response is factually correct for the question asked. Information can be verified against documentation. No meaningful errors.

Level 4 (Good): Response is correct and includes relevant context or caveats that make the information more useful. Goes beyond minimum accuracy.

Level 5 (Excellent): Response is correct, contextualized, cited with sources, and demonstrates deep domain knowledge. Sets the gold standard for accuracy.

Notice what this does. Each level has a clear definition grounded in observable characteristics. An evaluator can read a response and match it to a level based on what they see, not gut feeling.

## Using Anchor Examples

Definitions help, but humans learn better from examples. For each level of each dimension, provide anchor examples from your first fifty.

For accuracy dimension:

Level 5 anchor: [Point to example #3 from your first fifty] — "See how this response cites the specific policy section and includes the effective date? That's level 5 accuracy."

Level 3 anchor: [Point to example #27] — "This response is factually correct but doesn't include the nuance about exceptions. That's level 3 — acceptable but not excellent."

Level 1 anchor: [Point to example #42] — "This response states the return window is 60 days when it's actually 30 days. Clear factual error, level 1."

Anchors convert your rubric from abstract to concrete. Instead of asking "is this level 3 or level 4 accuracy?" evaluators ask "is this more like the level 3 anchor or the level 4 anchor?" Much easier to answer.

As you gather more examples, you'll add more anchors. The richness of your anchor set determines how useful your rubric is. Aim for at least two anchors per level per dimension, preferably showing different types of examples that achieve that level.

## Dimensional Independence vs Interaction

Here's a subtle but important question: are your quality dimensions independent or do they interact?

Independent dimensions: Accuracy and empathy don't affect each other. A response can be accurate without being empathetic, or empathetic without being accurate. You can evaluate them separately.

Interacting dimensions: Brevity and completeness often trade off. Making a response more complete might make it less brief. You can't maximize both independently.

Most rubrics pretend dimensions are independent when they're not. This creates confusion. An evaluator thinks "this response is maximally complete (level 5) but quite verbose (level 2 on brevity)." How do they resolve the tension?

Your rubric needs to address this. Two approaches:

Approach one: Define dimensions such that they truly are independent. Maybe instead of "brevity" you have "efficiency" — which means answering completely without wasting words. Now completeness and efficiency can both be high.

Approach two: Explicitly document the trade-offs. "Completeness and brevity often trade off. When in doubt, prioritize completeness for complex questions and brevity for simple questions." Now evaluators know how to handle the tension.

Don't pretend trade-offs don't exist. Build them into your rubric.

## Weighting Dimensions

Are all quality dimensions equally important? Almost certainly not.

For a customer support bot, accuracy might be critical — a wrong answer is disqualifying regardless of how empathetic it is. Empathy might be important but not critical — lack of empathy makes a response worse but doesn't make it unacceptable if it's otherwise correct.

Your rubric should reflect these priorities. Three ways to do this:

Explicit weights: Accuracy is 40% of quality, completeness is 25%, empathy is 20%, tone is 15%. You're making the weights numerical.

Tiered importance: Accuracy and safety are tier-one (must-pass). Completeness and clarity are tier-two (important). Empathy and tone are tier-three (nice-to-have). You're making the hierarchy categorical.

Decision rules: "If accuracy is below level 3, the response fails regardless of other dimensions. If accuracy is level 3 or higher, evaluate on the other dimensions." You're making certain dimensions gatekeepers.

I prefer the tiered or decision-rule approach over explicit numerical weights. Numerical weights feel precise but are usually arbitrary. Did you really think through why accuracy should be 40% and not 35%? Probably not.

Tiered importance is honest: some things matter more, some things are nice-to-have, and there's a qualitative difference between levels of importance that numbers obscure.

## Making Rubrics Usable

You can have a perfectly designed rubric that nobody uses because it's too complex or time-consuming. Usability matters.

Length: If your rubric is ten pages, nobody will read it. If it's one sentence, it's too vague. Aim for one page of core content plus examples and anchors. Evaluators should be able to internalize it.

Structure: Don't make evaluators hunt for information. Put the dimensions up front. Put levels and definitions in a clean table. Put anchors right next to the levels they illustrate.

Language: Write for your evaluators. If they're domain experts, you can use technical language. If they're general annotators, use plain English. Don't make them decode jargon.

Testing: Before you finalize a rubric, test it. Have three people independently use it to evaluate the same ten responses. Do they mostly agree? If not, your rubric isn't clear enough.

Iteration: Your first rubric will have gaps and ambiguities. That's normal. Use it for a week, collect feedback from evaluators, revise it. Do this three or four times until it stabilizes.

## The Disagreement Test

Here's how you know if your rubric is working: inter-rater reliability.

Have two evaluators independently score the same fifty responses using your rubric. Calculate agreement rate. If they agree on scores eighty percent of the time or more, your rubric is working. If agreement is below seventy percent, your rubric needs work.

When evaluators disagree, investigate:

Is it random disagreement (they're inconsistent) or systematic (they're interpreting the rubric differently)?

Is it on certain dimensions more than others? Maybe your accuracy rubric is clear but your empathy rubric is vague.

Is it on certain types of examples? Maybe your rubric works for simple cases but breaks on edge cases.

Use disagreements to improve your rubric. Maybe you need clearer level definitions. Maybe you need more anchor examples. Maybe a dimension is too subjective and needs to be split into measurable sub-dimensions.

## Rubrics for Different Use Cases

You might need different rubrics for different use cases, even within the same product.

For simple factual questions, you might care about accuracy, brevity, and clarity. Empathy is less critical.

For emotionally charged complaints, empathy and tone become critical. Brevity is less important — customers want to feel heard.

For complex multi-part questions, completeness and structure matter more. You're okay with longer responses if they're well-organized.

Instead of one universal rubric, you might have:

A core rubric with dimensions that always apply (accuracy, safety)
Specialized rubrics for different question types (factual, emotional, complex, troubleshooting)
Guidance on which rubric to use when

This complexity has costs — evaluators need to learn multiple rubrics. But it has benefits — your evaluations are more appropriate to context.

Most teams start with one universal rubric and add specialization as they discover gaps. That's fine. Don't over-engineer early.

## Automating Rubric Application

Once you have a rubric, you can use it manually (humans score each response) or automate it (AI scores responses based on the rubric).

Automated rubric scoring is increasingly common in 2026. You give an LLM your rubric and anchor examples, show it a response to evaluate, and ask it to score based on your rubric. This scales much better than human evaluation.

But automated scoring has risks:

The LLM might not apply your rubric correctly, especially for subtle dimensions like empathy or tone.

The LLM's scores might be consistent but systematically biased (always scoring too high or too low on certain dimensions).

The LLM might achieve high correlation with human scores on easy examples but diverge on hard ones.

If you automate rubric scoring:

Validate it against human scoring on a large sample first (at least 500 examples).
Measure not just correlation but bias (is the LLM systematically too harsh or lenient?).
Audit edge cases — automated scoring often fails exactly where it matters most.
Have humans spot-check a random sample of automated scores regularly.
Use automated scoring for high-volume low-stakes evaluation, human scoring for high-stakes decisions.

Think of automated rubric scoring as a junior evaluator: fast and cheap but requiring supervision.

## Common Rubric Failure Modes

Let me warn you about patterns that make rubrics fail:

Too vague: "Response should be helpful." Okay, but what does helpful mean? How do I recognize it? Vague rubrics produce inconsistent scoring.

Too rigid: "Response must be 2-3 sentences, include one citation, and end with a question." This is a template, not a rubric. It can't handle variation. Responses that meet the checklist can still be bad. Responses that break the rules can still be good.

Unmeasurable criteria: "Response should make the customer feel valued." You can't observe feelings from a response. You can observe tone, word choice, acknowledgment — observable proxies for making someone feel valued. Make your criteria observable.

Hidden subjectivity: "Response should have appropriate tone." Appropriate according to whom? Your rubric shouldn't hide judgment calls behind words like "appropriate," "reasonable," or "professional" without defining what those mean.

Too many dimensions: If you're scoring responses on fifteen dimensions, evaluators will cut corners or burn out. Prioritize ruthlessly.

Dimensions that don't discriminate: If every response scores level 3 on a dimension, that dimension isn't useful. Either the bar is too easy or the dimension doesn't matter.

No guidance on trade-offs: When dimensions conflict, evaluators make inconsistent choices. Your rubric needs to guide those choices.

## Evolving Your Rubrics

Your rubric is not static. As you learn more about what drives quality, your rubric should evolve.

Set quarterly rubric reviews:

Are all dimensions still relevant? Drop dimensions that don't discriminate or don't matter.

Are definitions still clear? Tighten language based on evaluator confusion.

Do you need new dimensions? Maybe you've discovered that citing sources matters more than you thought.

Are level definitions well-calibrated? Maybe what you called "excellent" is actually table-stakes now.

Do anchor examples need refreshing? Maybe your product changed and old anchors are obsolete.

When you update a rubric, version it clearly. "Rubric v2.0 — updated accuracy dimension to require citations for factual claims." This creates an audit trail and prevents confusion.

Communicate changes to all evaluators. Ideally, retrain them on the updated rubric. Don't let different people use different rubric versions unknowingly.

## From Rubrics to Metrics

Rubrics give you structured judgments. Metrics turn those judgments into numbers you can track over time.

If your rubric scores responses on five dimensions using a 1-5 scale, you can calculate:

Average score per dimension (e.g., average accuracy score is 3.8)
Overall average score across dimensions (weighted or unweighted)
Pass rate (percentage of responses above a threshold on all critical dimensions)
Distribution of scores (how many 1s, 2s, 3s, 4s, 5s you're getting)

These metrics let you track whether quality is improving, identify which dimensions are weakest, and compare performance across different types of questions or time periods.

But don't let metrics replace rubrics. Metrics are summaries. Rubrics are the detailed evaluations that metrics summarize. You need both.

## The Rubric as Teaching Tool

Finally, remember that rubrics aren't just for evaluation. They're teaching tools.

Show your rubric to your prompt engineers. It tells them what to optimize for.

Show it to your fine-tuning team. It tells them what the training data should demonstrate.

Show it to your product team. It tells them what quality dimensions users care about.

Show it to new team members. It teaches them your quality standards faster than months of osmosis.

A good rubric is your organization's shared definition of quality, made concrete and actionable. Invest in making it excellent.

In the next section, we'll talk about validating your ground truth before you build on it. Because even with carefully crafted examples and rubrics, you can still encode biases, miss blind spots, or define quality in ways that don't match reality. Let me show you how to audit and validate your ground truth to catch problems before they become expensive.
