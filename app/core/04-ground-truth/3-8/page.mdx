# 3.8 — Writing Behavior Specs That Teams Actually Follow

There's a company I know that has a 147-page behavior specification document for their customer service AI. It's comprehensive. It's detailed. It covers every scenario the team could think of. And absolutely nobody reads it.

When a new engineer joins, they're told "check the behavior spec for guidance." They open it, see a wall of text, and instead just ask another engineer. When product wants to update a behavior, they're supposed to "update the spec." Instead, they send a Slack message and hope everyone sees it.

The spec exists, but it's not actually being used. Which means it might as well not exist.

Let me walk you through how to write behavior specs that are clear enough to implement, specific enough to test, and readable enough that people actually read them. The goal isn't documentation for documentation's sake. It's a living tool that actually guides decisions.

## The Behavior Spec Paradox

Here's the tension: behavior specs need to be comprehensive (covering all the cases) and concise (readable in reasonable time). These goals pull in opposite directions.

Make it comprehensive: 147 pages that nobody reads.
Make it concise: 3 pages that miss critical edge cases.

The solution isn't finding the perfect middle length. It's structuring the spec so people can find what they need quickly without wading through everything.

Think of it like code documentation: you don't read every docstring every time. You read the overview to understand the system, then drill into specific sections when you need detail.

## The Three-Layer Structure

Effective behavior specs have three layers, each serving a different purpose.

**Layer 1: The One-Pager (Executive Summary)**

This is the 10-minute read that gives anyone the gist of how your system behaves.

Contents:
- Product purpose (what does this system do?)
- Core principles (what values guide behavior?)
- Required behaviors (must always do)
- Forbidden behaviors (must never do)
- Escalation triggers (when to hand off to humans)

This layer is what new team members read first, what leadership reviews, and what everyone should be able to recall.

Example:

---
**Customer Support Assistant - Behavior Overview**

**Purpose:** Help customers resolve common issues, answer questions about products and policies, and escalate complex issues to human agents.

**Core Principles:**
- Solve problems, don't just provide information
- Be clear and concise — respect user time
- Never guess — if uncertain, ask or escalate
- Protect user privacy above all else

**Must Always Do:**
- Verify user identity for account-specific requests
- Cite policy sources when stating policies
- Escalate when user requests human assistance
- Log all conversations for quality assurance

**Must Never Do:**
- Share information across user accounts
- Make refunds or account changes without confirmation
- Provide specific medical, legal, or financial advice
- Fabricate information when data is unavailable

**Escalate When:**
- User explicitly requests human help
- Issue requires access to systems AI can't reach
- User expresses strong dissatisfaction (3+ negative signals)
- Problem is outside documented procedures
---

This fits on one page. Anyone can read it in 10 minutes and understand the basics.

**Layer 2: The Playbook (Operational Guide)**

This is the detailed guide for specific scenarios. Each scenario gets its own section, structured identically so they're scannable.

Contents:
- Scenario name
- When this applies (trigger conditions)
- Required behaviors (must-dos)
- Forbidden behaviors (must-not-dos)
- Discretionary behaviors (judgment calls)
- Examples (good and bad responses)
- Edge cases (known exceptions)

This layer is what engineers reference when implementing features, what QA uses to write tests, and what product reviews when making decisions.

Example:

---
**Scenario: Password Reset Request**

**Applies When:** User requests password reset, password change, or reports inability to log in.

**Required Behaviors:**
- Verify user identity via email or SMS code
- Send reset link within 60 seconds
- Expire reset link after 24 hours
- Log reset request for security monitoring
- Confirm completion after password is changed

**Forbidden Behaviors:**
- Never send reset link without identity verification
- Never disclose current password (even if hashed)
- Never reset password for different account than verified
- Never allow reuse of last 5 passwords

**Discretionary Behaviors:**
- Tone: Adapt based on user frustration level (calm = efficient, frustrated = reassuring)
- Detail: Provide more explanation if user seems confused, less if they seem experienced
- Proactivity: Offer additional security tips if reset was due to suspected compromise

**Examples:**

Good response (verified user, calm tone):
"I've sent a password reset link to your email at j***@example.com. It'll arrive within a minute and is valid for 24 hours. Check your spam folder if you don't see it."

Bad response (no verification):
"What's your new password? I'll update it for you."
(Violates security requirements)

Bad response (fabricated info):
"I've reset your password to 'tempPass123'. You can log in with that."
(Fabricates action the system didn't actually take)

**Edge Cases:**
- Email address no longer accessible → Escalate to account recovery team
- Multiple reset requests in short time → Flag for security review, still process request
- User's account locked due to too many attempts → Inform user, explain unlock process
---

This is more detailed but still scannable. If you need to know about password reset behavior, you can find this section and get everything you need.

**Layer 3: The Deep Dive (Technical Specification)**

This is the detailed technical specification for implementation: API endpoints, exact prompt templates, error codes, validation rules, logging specifications.

This layer is what engineers reference during implementation, what gets turned into code, and what forms the basis of automated tests.

Most people never read this layer cover-to-cover. They reference specific sections as needed.

Example:

---
**Password Reset: Technical Implementation**

**Endpoint:** POST /auth/password-reset-request

**Input Validation:**
- Email or phone must match account records
- Rate limit: 3 requests per account per hour
- Block requests from IP addresses with >50 requests/hour

**Identity Verification:**
- Generate 6-digit code
- Send via email or SMS (user preference)
- Code valid for 15 minutes
- Maximum 3 verification attempts

**Reset Link Generation:**
- Generate cryptographically secure random token (32 bytes)
- Store token hash in database with expiration timestamp
- Link format: https://app.example.com/reset?token=[token]
- Token valid for 24 hours

**Logging Requirements:**
- Log reset request with timestamp, user ID, and IP address
- Log verification attempts (success and failure)
- Log password change with timestamp
- Never log tokens, codes, or passwords in any form

**Error Responses:**
- 429: Too many requests
- 404: Account not found (return same message as success for security)
- 403: Account locked (provide unlock instructions)

**Prompt Template:**
```
The user has requested a password reset. You must:
1. Verify their identity by sending a code to their email or phone
2. Once verified, send reset link
3. Confirm link was sent
4. Do not proceed without verification
5. Never ask for or accept the new password directly

If identity verification fails 3 times, escalate to account recovery team.
```
---

This is dense and technical. Only people implementing or debugging this feature need to read it.

## Format for Scannability

Even well-structured specs can be hard to use if they're not formatted for quick scanning.

**Use consistent headers:**
Every scenario section uses the same header structure. This lets people scan quickly.

**Use bullet lists:**
Paragraphs are for reading. Bullets are for scanning. Behavior specs should be heavy on bullets.

Bad:
"When a user requests a password reset, the system should verify their identity by sending a verification code to their registered email address or phone number, which they must enter correctly within 15 minutes to receive the reset link."

Good:
**Identity Verification:**
- Send verification code to email or phone
- Code valid for 15 minutes
- Maximum 3 attempts
- After verification, send reset link

**Use tables for comparisons:**

| Situation | Required Response | Forbidden Response |
|-----------|------------------|-------------------|
| User verified | Send reset link | Send password directly |
| User not verified | Request verification | Send link without verification |
| Too many attempts | Explain lockout, offer alternative | Continue allowing attempts |

Tables let people see patterns and differences quickly.

**Use examples liberally:**
Every abstract rule should have a concrete example. People understand examples much faster than principles.

**Use visual hierarchy:**
Make important information stand out:
- **Bold** for critical points
- *Italic* for emphasis
- `Code formatting` for exact values
- CAPS for severity (WARNING, CRITICAL)
- Color coding if your tool supports it (red for forbidden, green for required)

## The Formula for Clear Behavior Statements

Vague behavior statements are useless. "Be helpful" tells you nothing. "Respect privacy" could mean anything.

Here's the formula for clear behavior statements:

**[MUST/SHOULD/MUST NOT] + [ACTION] + [CONDITION] + [EXCEPTION]**

Example:
"MUST verify user identity WHEN handling account changes UNLESS change is non-sensitive (e.g., email preferences)"

Breaking it down:
- MUST (requirement level)
- verify user identity (action)
- WHEN handling account changes (condition)
- UNLESS change is non-sensitive (exception)

This is specific enough to implement and test.

Compare to vague version:
"Systems should respect user privacy."

What does that mean? Verify identity? Encrypt data? Not share data? It's not actionable.

## Testability: Every Behavior Must Be Verifiable

If you can't test whether a behavior is being followed, it's not a real spec — it's an aspiration.

For every behavior, ask: "How would I write a test that proves this is working?"

**Testable:**
"MUST respond within 5 seconds for queries under 100 tokens"

Test: Send 100 queries under 100 tokens, measure latency, verify all under 5 seconds.

**Not testable:**
"SHOULD respond quickly"

How fast is quickly? How do you measure this?

**Testable:**
"MUST cite source documents for all factual claims"

Test: Take random sample of responses with factual claims, verify each has citation, verify citations are accurate.

**Not testable:**
"SHOULD be accurate"

How accurate? What's the threshold?

When writing a behavior spec, write the test criteria alongside the behavior. If you can't write test criteria, the behavior isn't well-defined enough.

## The Update Process: Keeping Specs Current

Behavior specs get out of date fast. Features change, edge cases are discovered, policies evolve. If the spec doesn't reflect current reality, people stop trusting it.

**Make updates easy:**
If updating the spec is onerous (long approval process, complex tool, unclear ownership), people won't do it.

**Version control:**
Behavior specs should be in version control (Git) just like code. This gives you:
- History of what changed and when
- Ability to review changes before merging
- Clear ownership and approval process
- Rollback if change causes problems

**Change log:**
Maintain a changelog at the top of the spec:

---
**Behavior Spec Changelog**

**v2.3 - 2026-01-15**
- Added password reset rate limiting (security requirement)
- Clarified escalation criteria for billing disputes
- Updated tone guidance for frustrated users

**v2.2 - 2025-12-10**
- Added support for multi-factor authentication flows
- Updated forbidden behaviors to include new PII categories
- Added edge case handling for suspended accounts
---

This lets people quickly see what's changed without reading the whole spec.

**Notification process:**
When the spec changes, notify the team:
- What changed
- Why it changed
- When it takes effect
- Who to contact with questions

Don't assume people will notice changes. Tell them explicitly.

## The Review Cadence

Behavior specs need regular review, even if nothing's obviously broken.

**Weekly review:**
Product and engineering review recent edge cases and user feedback. Do any require spec updates?

**Monthly review:**
Full team reviews one section of the spec. Is it still accurate? Clear? Complete?

**Quarterly review:**
Leadership reviews the full spec. Does it still align with product strategy? Are priorities clear?

**Incident-triggered review:**
After any major incident, review relevant sections. What needs to be added or clarified to prevent recurrence?

This keeps the spec living and relevant.

## Ownership: Who Maintains the Spec?

Unclear ownership means nobody maintains the spec.

**Designate a spec owner:**
One person (often product manager or tech lead) is ultimately responsible for the spec's accuracy and currency.

They don't write everything, but they:
- Ensure updates happen
- Approve changes
- Triage conflicts
- Maintain format consistency
- Drive review cadence

**But empower the team:**
Anyone should be able to propose changes. The owner approves, but the team contributes.

Use pull requests for changes. This creates natural review process and preserves history.

## Making It Discoverable

The best spec in the world is useless if people can't find it.

**Central location:**
One authoritative source. Not scattered across wikis, docs, Slack, and people's heads.

**Pinned in team channels:**
Link should be pinned in primary communication channels (Slack, Teams, etc.)

**Linked from code:**
In your system prompt or configuration files, include a comment with the spec link:

```
# Behavior spec: https://docs.company.com/ai-behavior-spec
# Last updated: 2026-01-15
# Owner: @product-team
```

This reminds people the spec exists and where to find it.

**Searchable:**
Use clear, consistent terminology throughout the spec so people can search for topics.

**Indexed by scenario:**
Include a table of contents or index that lets people jump to relevant sections.

---
**Index**

- Account Management
  - Password reset
  - Account deletion
  - Profile updates
  - Email preferences
- Billing
  - Refund requests
  - Subscription cancellation
  - Payment method updates
- Escalations
  - Technical issues
  - Billing disputes
  - Policy exceptions
---

## Common Mistakes in Behavior Specs

**Mistake 1: Writing for robots, not humans**

Specs that read like legal documents or technical manuals don't get used.

Write in plain language. Use short sentences. Use examples.

**Mistake 2: Too abstract**

"Respect user preferences" is too vague.

"Store user's preferred name and use it in responses" is concrete.

**Mistake 3: No prioritization**

Not all behaviors are equally important. Make priority clear.

Use labels:
- CRITICAL (system is broken if this fails)
- HIGH (major user impact if this fails)
- MEDIUM (noticeable issue if this fails)
- LOW (polish, nice-to-have)

**Mistake 4: No examples**

Every rule needs at least one example of correct and incorrect behavior.

**Mistake 5: Describing current bugs**

If the spec says "SHOULD respond in 5 seconds" but the system actually takes 30 seconds, the spec is describing the problem, not the solution.

Specs describe the target state, not current state.

**Mistake 6: No connection to tests**

If behaviors in the spec aren't tested, they're aspirational, not actual.

Link each behavior to its test coverage:

"MUST verify user identity before account changes [Test: test_identity_verification_required]"

This creates accountability.

## The Spec Review Checklist

Before publishing or updating a behavior spec, review:

**Clarity:**
- [ ] Can a new team member understand this in 10 minutes?
- [ ] Are technical terms defined?
- [ ] Are examples provided?
- [ ] Is visual hierarchy clear?

**Completeness:**
- [ ] Are required behaviors documented?
- [ ] Are forbidden behaviors documented?
- [ ] Are edge cases covered?
- [ ] Are escalation rules clear?

**Testability:**
- [ ] Can every behavior be verified with a test?
- [ ] Are test criteria specified?
- [ ] Are thresholds quantified?

**Maintainability:**
- [ ] Is ownership clear?
- [ ] Is the update process defined?
- [ ] Is version history maintained?
- [ ] Is the changelog current?

**Discoverability:**
- [ ] Is location documented?
- [ ] Is it linked from relevant places?
- [ ] Is it searchable?
- [ ] Is there an index or TOC?

## Living Documentation

The best behavior specs aren't static documents. They're living artifacts that evolve with your product.

Think of your behavior spec like your codebase:
- It's version controlled
- It's reviewed before changes merge
- It's tested (behaviors link to tests)
- It's refactored when it gets messy
- It's documented (with examples and explanations)
- It's the source of truth

When your spec is treated like code, it stays relevant and useful.

## Templates and Tools

Consider providing templates for common scenarios to make updates consistent:

---
**Scenario Template**

**Scenario Name:** [Name]

**Applies When:** [Trigger conditions]

**Required Behaviors:**
- [Must-do 1]
- [Must-do 2]

**Forbidden Behaviors:**
- [Must-not-do 1]
- [Must-not-do 2]

**Discretionary Behaviors:**
- [Judgment call 1]
- [Judgment call 2]

**Examples:**

Good:
[Example of correct behavior]

Bad:
[Example of incorrect behavior]
[Why it's wrong]

**Edge Cases:**
- [Edge case 1] → [How to handle]

**Tests:**
- [Test name or link]

**Owner:** [Team/person]
**Last updated:** [Date]
---

This structure ensures consistency and completeness.

## From Spec to Implementation

The behavior spec is the bridge between product intent and engineering implementation.

When a behavior is specified clearly:
- Engineers know what to build
- QA knows what to test
- Product knows what to expect
- Users get consistent experience

When it's vague or missing:
- Engineers guess
- QA misses edge cases
- Product gets surprised
- Users get inconsistent experience

A good behavior spec isn't overhead. It's the foundation that makes everything else possible.

In the next subchapter, we'll explore evidence policy: when your system is allowed to make claims without explicit proof, and when it must cite its sources. This is critical for RAG systems but applies broadly to any AI that presents factual information. The line between "general knowledge" and "requires citation" is subtler than it looks.
