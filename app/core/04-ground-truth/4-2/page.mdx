# 4.2 — Expert Elicitation: Getting Knowledge Out of Heads

Here's a scene that plays out in every organization trying to build AI.

You sit down with Sarah, your top customer support agent. Sarah has been doing this job for eight years. She can read a customer email and instantly know if they're genuinely confused, subtly angry, or fishing for a discount. She crafts responses that feel personal but stay on-brand. She knows exactly when to escalate and when to close. Customers love her. Management loves her. Her knowledge is exactly what you need to build ground truth.

You ask her: "Sarah, can you describe what makes a good customer support response?"

She thinks for a moment. "Well, you just... you know... you have to understand what they're really asking. And then you respond in a way that helps them but doesn't promise things we can't deliver. You match their energy, but professionally. It's not that complicated once you've done it for a while."

You try again. "Okay, but specifically, what do you look for? What are the rules?"

She struggles. "I don't know if there are rules exactly. It's more like... you read the email and you feel what the right response is. Sometimes you need to be warmer, sometimes you need to be more direct. It depends."

This is the expert elicitation problem. The knowledge you need to build ground truth lives inside people like Sarah. But it lives there as intuition, pattern recognition, and implicit knowledge developed through thousands of repetitions. It's not stored as explicit rules they can simply recite.

Experts are experts precisely because they've internalized their domain so deeply that they no longer consciously process what they know. They just know. A radiologist sees a tumor in an X-ray in milliseconds without conscious rule-following. A chef tastes a sauce and knows it needs acid without calculating anything. A programmer spots a bug from a stack trace without explicit reasoning.

Your job is to extract that implicit knowledge and turn it into explicit ground truth. This is not easy, but it's absolutely learnable. Let me walk you through how it actually works.

## Why Experts Can't Just Tell You

Before we talk about techniques, you need to understand why the "just ask them" approach fails. It's not that experts are hiding information or being difficult. It's that human expertise doesn't work the way we imagine.

When Sarah reads a customer email, her brain is doing something remarkable. She's processing linguistic cues, emotional tone, subtext, business context, customer history, company policy, and tactical communication strategies simultaneously. Most of this processing happens below conscious awareness. It's the same way you can catch a ball without calculating trajectories or recognize a friend's face without measuring feature distances.

If you ask her to describe this process, she has to reconstruct it consciously. And conscious reconstruction is unreliable. She'll tell you about the parts that are easy to articulate — "check if they're asking about billing" — and miss the subtle parts that actually drive quality — like how she mirrors the customer's formality level without thinking about it.

Psychologists call this the "expert blind spot." The better someone is at a skill, the less aware they are of what makes them good. They've compressed complex decision trees into instant pattern matching. Asking them to expand those patterns back into rules is like asking you to describe how you balance while walking.

This is why interviewing experts often produces thin, obvious insights. "Make sure the response is relevant." Thanks, Sarah. Very helpful.

## Think-Aloud Protocol: Watching Experts Work

The first real technique is think-aloud protocol. Instead of asking experts to describe what they do in the abstract, you watch them do it and ask them to narrate their thought process in real-time.

Here's how it works. You sit with Sarah and give her a real customer email. Not a hypothetical one — a real email from your support queue. You ask her to respond to it exactly as she normally would, but to speak aloud everything she's thinking as she does it.

She reads the email and starts talking: "Okay, customer is asking about a refund for a pro subscription. They say it's not working for them. First thing I notice is the tone — they're disappointed but not angry. They said 'unfortunately' not 'your terrible product.' So I want to be empathetic but I don't need to be super apologetic."

She keeps going: "They've been a customer for six months, that's in their account. So they gave us a fair shot. I'm not going to push back on the refund. But I want to understand what wasn't working before I just process it. Maybe it's something we can fix. Let me ask an open question about what they were hoping to achieve."

She drafts a response: "First sentence, acknowledge their disappointment. Don't go straight to business. Second sentence, confirm I can process the refund — I want them to know they'll get their money back so they're not defensive. Third sentence, ask what wasn't working. Make it optional — 'if you're willing to share' — so they don't feel interrogated."

See what just happened? You didn't get abstract principles. You got concrete decision-making in context. You learned that Sarah:

- Assesses emotional tone before content
- Checks customer tenure to calibrate her response
- Prioritizes acknowledgment before solutions
- Uses specific phrases to reduce defensiveness
- Balances business goals with customer experience

None of this would have come out if you just asked "what makes a good response?" But when she shows her work, the expertise becomes visible.

The key to think-aloud is consistency. Don't do it once. Do it ten times with different examples. You'll start seeing patterns in how Sarah processes different situations. The things she checks first. The mental frameworks she uses. The trade-offs she makes.

Record these sessions. Transcript them. Pull out the recurring principles. Those principles become your ground truth rubrics.

## Example Sorting: Revealing Implicit Criteria

The second technique is example sorting. This one is powerful because it bypasses verbal explanation entirely and gets straight to judgment.

Here's the setup. You collect thirty customer support responses — ten great, ten mediocre, ten poor. You don't tell Sarah which is which. You just hand her the pile and ask her to sort them into three categories: "I'd be proud to send this," "this is acceptable but not great," and "this needs work."

Then you watch what she does.

She picks up the first response. Reads it. Puts it in the "proud" pile. You ask: "What made that one great?" She might say "it just feels right," but now you have the example in front of you to analyze together. You can point to specific sentences. "Was it this part where they explained the policy? Or this part where they offered alternatives?" She can react to concrete text instead of trying to articulate abstract principles.

As she works through the pile, patterns emerge. Maybe she consistently puts responses in the "needs work" pile when they use corporate jargon. Or when they answer the wrong question. Or when they're technically correct but emotionally tone-deaf. You're seeing her implicit criteria in action.

The beautiful thing about example sorting is it works even when experts disagree. If you have three experts sort the same pile and they mostly agree, you've found shared intuition — strong signal for ground truth. When they disagree, you've found genuine ambiguity. Those disagreements tell you where you need clearer guidelines or where you need to allow for judgment calls.

Here's a pro move: include some examples that you've secretly modified to test specific hypotheses. Take a great response and make one change — maybe make it wordier, or more formal, or add a policy citation. If Sarah downgrades it, you've learned that brevity or informality or terseness matters to quality. You're experimentally testing her implicit criteria.

## Boundary Probing: Finding the Edges

The third technique is boundary probing. This is where you systematically test the edge cases and gray areas to figure out where "good" ends and "bad" begins.

Start with an example Sarah rated as great. Now modify it slightly and ask: "Is this still great?" If she says yes, modify it again. Keep going until she says no. You've just found a boundary.

Let's say the original response was warm and empathetic. You make it slightly more formal. Still great? Make it more formal again. Keep going until Sarah says "okay, now it's too stiff." You've discovered the upper bound of acceptable formality for this context.

Do the same thing in the other direction. Make the response warmer. Then warmer again. Eventually Sarah says "this is too casual for our brand." Now you've found both edges of the acceptable formality range.

This works for any dimension. Response length. Technical detail. Policy strictness. Proactiveness. Whatever matters in your domain.

Boundary probing reveals the things experts know intuitively but can't state as rules. Sarah might not be able to define the right formality level, but she can definitely tell you when you've crossed the line. By systematically crossing lines, you map out the acceptable space.

A warning: boundaries are often context-dependent. A response that's too casual for a billing question might be perfect for a feature request. Don't expect universal rules. Instead, build a map of which boundaries apply in which situations. That context-dependency becomes part of your ground truth.

## Contrastive Pairs: Teaching Through Comparison

The fourth technique is contrastive pairs. You show Sarah two responses to the same customer email that differ in exactly one dimension. Then you ask which is better and why.

For example, Response A is warm and detailed. Response B is identical except it's more concise. Which does Sarah prefer? Her answer tells you whether she prioritizes warmth-and-completeness or efficiency-and-clarity for this type of question.

Make the differences small and focused. If the responses differ in three ways, you don't know which difference drove her preference. If they differ in one way, you're isolating that variable.

Build a matrix of contrastive pairs testing different dimensions:

- Formal vs casual tone
- Brief vs detailed explanation
- Policy-focused vs empathy-focused
- Proactive vs reactive
- Educational vs transactional

For each pair, Sarah's preference teaches you about her implicit value hierarchy. And when she says "both are fine" or "it depends," you've found a dimension where rigid rules don't apply — useful information for building flexible rubrics.

The best contrastive pairs test trade-offs. When Sarah can't have both completeness and brevity, which does she sacrifice? When warmth and professionalism pull in different directions, where does she land? These forced choices reveal priorities that never surface in open-ended discussion.

## Dealing With Expert Disagreement

Here's the moment that makes teams panic: you do example sorting with Sarah and Mark, another top support agent. They disagree on fifteen of the thirty examples.

Don't panic. Disagreement is information.

First, examine the disagreements. Are they random or patterned? If Sarah consistently prefers brief responses and Mark prefers detailed ones, you've discovered a legitimate dimension of variation. Some customers probably do prefer brief. Others want detail. Both experts are right for different contexts.

If the disagreement is random — Sarah liked response twelve on Monday but not on Wednesday — that's different. It might mean the evaluation criteria are genuinely ambiguous. Or it might mean the example is edge-casey and reasonable people can disagree.

Here's what you do with each type of disagreement:

For patterned disagreement, embrace it. Build rubrics that allow for different valid approaches. "Responses can be either concise-informative or detailed-thorough. Both are acceptable as long as they're accurate and on-brand." You've discovered legitimate stylistic variation.

For random disagreement, bring the experts together. Show them the example they disagreed on and facilitate a conversation. Often they'll realize they were focusing on different aspects. Sarah was judging tone, Mark was judging completeness. They're not actually disagreeing — they're using different criteria. Your job is to make those criteria explicit.

Sometimes they'll discuss and converge on a shared judgment. Great — you've refined everyone's understanding. Sometimes they'll discuss and still disagree. Also great — you've found a genuinely ambiguous case. Mark it as such in your ground truth. Not every example needs a confident label.

The goal is not to eliminate disagreement. The goal is to understand where and why it happens. That understanding becomes part of your ground truth documentation.

## Avoiding Anchoring Bias

Here's a trap that ruins expert elicitation: anchoring. You show Sarah an example and accidentally reveal what you think the answer should be. Maybe through your tone of voice, or the way you frame the question, or the order you present examples in. Sarah picks up on your expectation — consciously or not — and aligns with it. You think you're extracting her expertise, but you're actually getting your own opinions reflected back.

Anchoring is powerful and subtle. If you say "this response seems a bit harsh, right?" you've anchored Sarah toward agreeing. Even if you say it as a genuine question, the phrasing suggests your view. Sarah might have thought the response was appropriately direct, but now she's second-guessing herself.

Here's how to avoid anchoring:

Present examples neutrally. "Here's a response. What do you think?" Not "Here's a response that I thought was pretty good — agree?"

Randomize order. Don't show all the good examples first and then the bad ones. Mix them up. Otherwise Sarah might pattern-match based on position rather than content.

Don't explain your rating system until after Sarah has made her judgments. If you tell her "we're rating on a scale of one to five where five means excellent" before she's seen any examples, you've anchored her expectations of what five means.

Use multiple elicitation sessions. In session one, Sarah makes judgments clean. In session two, you can share early findings and test whether they resonate. This two-stage approach lets you gather unanchored data first and then validate it collaboratively.

Have different team members conduct elicitation sessions with different experts. Compare notes. If everyone is getting similar patterns, you're probably capturing real expertise. If patterns only show up when certain team members run the sessions, you're probably seeing anchoring.

## Recording Rationale, Not Just Judgments

Most teams make this mistake: they extract expert judgments and stop there. Sarah says response twelve is great. You mark it as ground truth and move on.

But the judgment alone is not enough. You need the rationale.

When Sarah says response twelve is great, ask "what specifically makes it great?" When she says response seven needs work, ask "what would you change?" The rationale is where the real knowledge lives.

Here's why this matters. Judgments tell you the answer. Rationales tell you how to generalize. If Sarah says "this response is great because it addresses the customer's underlying concern, not just their stated question," you've learned a principle that applies beyond this one example. That principle helps you evaluate new responses you haven't seen yet.

Rationales also help you catch mistakes. Sometimes Sarah will say a response is great and then, when pressed to explain why, realize she was wrong. "Actually, now that I'm thinking about it, this response doesn't mention the return policy. It's missing something important." The act of articulating rationale forces deeper reflection.

Create a standard template for capturing expert elicitation:

- Example ID
- Expert name and date
- Overall judgment (great/acceptable/needs work/unclear)
- Specific strengths identified
- Specific weaknesses identified
- Suggested improvements
- Principles or rules the expert articulated
- Context factors that influenced the judgment
- Any disagreements or uncertainties

This template ensures you're capturing the rich qualitative data, not just binary labels.

## When Experts Don't Know That They Don't Know

Here's the uncomfortable truth: sometimes experts are confidently wrong.

Sarah might be certain that customers prefer formal language. But maybe your data shows customers actually respond better to casual tone. Sarah's expertise is based on thousands of interactions, but those interactions were shaped by the approach she was trained to use. She never experimented with alternatives. Her confidence is based on a limited sample of the possibility space.

Or maybe Sarah is great at her job but not great at your specific use case. You're building an AI for a new product line that differs from what she usually supports. Her expertise doesn't fully transfer, but she doesn't realize it.

This is why expert elicitation can't be your only ground truth source. Experts give you hypotheses, not gospel. You still need to:

- Validate expert judgments against objective outcomes (did the customer issue get resolved? Did they churn?)
- Test expert principles with A/B experiments (does formal language really perform better?)
- Bring in multiple experts and look for consensus (is this Sarah's personal style or shared professional wisdom?)
- Ground expertise in real data (show Sarah her own response-resolution patterns)

Think of expert elicitation as high-quality guesses that need empirical validation. Experts fast-track you to probably-correct principles. But "probably-correct" still needs testing.

## Practical Session Structure

Let me give you a concrete structure for a ninety-minute expert elicitation session.

Minutes 0-10: Explain what you're doing and why. "We're trying to understand what makes a great customer support response. You're one of our best, so we want to learn from how you think. There are no wrong answers. We're not testing you — we're learning from you."

Minutes 10-40: Example sorting. Give them thirty responses to sort while thinking aloud. This is the meat of the session. Record it.

Minutes 40-60: Contrastive pairs. Show them ten pairs and ask preferences. Dig into trade-offs.

Minutes 60-75: Boundary probing. Take two examples they rated highly and modify them incrementally to find edges.

Minutes 75-85: Open discussion. "What are we missing? What questions should we have asked? What makes this harder than it looks?"

Minutes 85-90: Thank them sincerely and explain next steps.

One session per expert is not enough. Plan for three sessions minimum, spaced out over weeks. Early sessions extract initial intuitions. Later sessions test hypotheses you've developed and resolve ambiguities.

## From Expert Knowledge to Ground Truth Records

The final step is converting expert elicitation notes into actual ground truth records. This is where messy qualitative insights become structured data you can use for evaluation.

Go through your elicitation transcripts and identify every principle that experts articulated. "Responses should address underlying concerns, not just stated questions." That's a principle.

Turn each principle into testable criteria. "Does this response address the customer's underlying concern, or only their surface question?" Now you can evaluate any response against that criterion.

Group related criteria into dimensions. Maybe you have five principles that all relate to empathy. Those become the "empathy" dimension of your rubric. Three principles about accuracy. That's your "accuracy" dimension.

For each dimension, define levels based on expert examples. What does "excellent empathy" look like? Point to specific responses experts rated highly and extract the pattern. What about "poor empathy"? Use the negative examples.

The result is a rubric grounded in expert judgment but expressed as explicit, testable criteria. You've transformed Sarah's intuitive expertise into a tool that other people — or automated systems — can use to evaluate quality.

## The Hard Truth About Expert Elicitation

Let me be honest about what this process requires: time, patience, and humility.

Expert elicitation is slow. One expert, three sessions, ninety minutes each — that's four-and-a-half hours plus preparation and analysis time. If you need three experts for consensus, you're looking at fifteen hours of expert time plus thirty hours of your time. It's not an afternoon workshop.

It's also uncomfortable. You'll have experts disagree. You'll have experts give you principles that don't match your data. You'll have experts who are confident but wrong. Navigating this requires diplomatic skill and intellectual honesty.

But here's what you get in return: ground truth that's grounded in real expertise, not academic theory. Rubrics that capture nuance and context. Evaluation criteria that experienced professionals recognize as legitimate. And a shared understanding across your team of what quality actually means.

The alternative is building ground truth from your own assumptions and hoping they're right. Which is a great way to build an AI system that passes your tests but fails in production.

In the next section, we'll look at another underutilized source of ground truth: the documentation that already exists in your organization. Policy manuals, help docs, training materials, SOPs — all contain implicit ground truth waiting to be mined. Let me show you how to extract it systematically.
