# 4.1 — Starting With Zero (The Cold Start Problem)

Let me tell you about the moment that stops most AI projects dead in their tracks.

You have a brilliant idea. Maybe it's a customer support bot that should sound empathetic but professional. Maybe it's a document analyzer that should flag compliance risks. Maybe it's a code assistant that should suggest clean, maintainable solutions. You've got the vision crystal clear in your head.

Then someone asks: "Okay, show me ten examples of what good looks like."

And you freeze.

Because you don't have ten examples. You don't have any examples. You have a concept, a feeling, an intuition about what "good" means for your use case. But nothing concrete. No labeled data. No test cases. No rubrics. Just a blank page and mounting pressure to ship something.

This is the cold start problem, and it's completely normal. Every AI system that ever worked had to start here. The question isn't whether you'll face this moment — it's how you'll navigate it without wasting months building the wrong foundation.

## Why Zero Feels Paralyzing

Here's what makes the cold start problem so painful. Traditional software development starts with requirements. You write down what the system should do, you implement it, you test that it does what you specified. The path is clear.

AI development inverts this. You need examples before you can define good requirements. You need to see patterns before you can write rubrics. You need data before you know what dimensions even matter. It's like being asked to write a restaurant review before you've tasted any food.

Most teams respond to this discomfort in one of three dysfunctional ways.

The first group panics and tries to collect thousands of examples before doing anything. They spend six months building elaborate data collection pipelines, hiring annotation teams, creating complex tooling. By the time they have "enough" data, the market has moved, the requirements have changed, or the budget is exhausted. They never ship.

The second group does the opposite — they ship immediately with zero ground truth. They build based on intuition, deploy to production, and hope user feedback will tell them what good means. Sometimes they get lucky. Usually they build the wrong thing and have to rebuild from scratch when reality hits.

The third group — and this one hurts because they're trying to do the right thing — gets stuck in analysis paralysis. They debate definitions endlessly. They form committees to argue about edge cases they haven't seen yet. They write hundred-page specification documents that nobody will read. They mistake thinking about ground truth for actually creating it.

All three approaches waste time and money. Let me show you the fourth way, the one that actually works.

## The First-50 Philosophy

Here's the insight that unlocks the cold start problem: you don't need thousands of examples to start. You don't even need hundreds. You need the RIGHT first fifty.

Think about how you learned to recognize good writing. You didn't read ten thousand essays and compute statistical patterns. You read a handful of excellent examples, a handful of terrible ones, and someone helped you understand why one was better than the other. After maybe fifty examples with good feedback, you had enough pattern recognition to evaluate new writing on your own.

Ground truth works the same way. Fifty carefully chosen examples will teach you more than a thousand random ones. The trick is choosing them strategically.

Here's the cold start playbook that's worked for teams at Anthropic, OpenAI, Google, and hundreds of startups. It's not sexy, but it's battle-tested.

## Week One: The First Ten (Hand-Crafted Excellence)

Start by creating ten examples yourself. Not collecting them. Not generating them with AI. Creating them by hand, with care and intention.

These ten examples have one job: establish the aspirational standard. What would a perfect response look like if you had unlimited time and expertise? Don't worry about whether your AI can achieve this yet. You're defining north, not planning the route.

Let me walk you through what this looks like in practice. Say you're building that customer support bot. Your first ten examples might include:

A customer who's angry about a late delivery. The perfect response acknowledges their frustration specifically, explains what happened without making excuses, offers a concrete solution, and includes a gesture of goodwill that matches the severity of the issue. It's warm but not fake-friendly. It's apologetic but not groveling.

A technical question about a complex feature. The perfect response confirms they understand the question, explains the answer in simple terms, provides a specific example, and anticipates the likely follow-up question. It doesn't oversimplify or talk down. It doesn't drown them in jargon.

A billing inquiry where the customer is confused but not upset. The perfect response explains their bill clearly, breaks down charges line by line, points out where they can view this information themselves next time, and asks if anything else is unclear. It's educational without being condescending.

Notice what we're doing. We're not covering every possible customer question. We're identifying archetypes — the fundamental patterns your system will encounter. Each example teaches something different about what "good" means.

Spend real time on these ten. Write them, rewrite them, have teammates review them. Argue about word choices. Debate whether a response is too formal or too casual. This is not wasted time. You're calibrating your intuition and building shared understanding of quality.

## Week Two: The Next Forty (Real-World Messiness)

Now that you have your aspirational standard, you need to test it against reality. This is where you expand to fifty examples, but with a specific structure.

The next forty examples should come from real scenarios, not your imagination. If you have any existing system — even a human-powered one — pull real inputs. Customer emails. User queries. Support tickets. Bug reports. Sales conversations. Whatever inputs your AI will eventually see.

If you have literally nothing, borrow from adjacent domains. Building a medical AI? Read patient forums and doctor Q&A sites. Building a legal assistant? Study real legal questions from public forums. Building a code helper? Look at GitHub issues and Stack Overflow. The inputs don't need to be from your exact system, they need to be realistic.

Here's the structure for those forty:

Ten more "great" examples. But these ones should be harder than your first ten. Edge cases that test your intuition. A customer who's angry AND wrong. A technical question about a feature you're deprecating. A billing inquiry that involves a system error. These force you to articulate principles, not just templates.

Ten "terrible" examples. This is crucial and often skipped. Show what bad looks like. Responses that are factually wrong. Ones that are correct but insultingly rude. Ones that completely miss the question. Ones that create legal liability. You need negative examples to define boundaries.

Ten "good enough" examples. This is the reality zone. Not perfect, not terrible, but acceptable. Maybe the tone is a bit off but the information is correct. Maybe it's slightly verbose but covers everything. These teach you about trade-offs and where to draw the pass/fail line.

Ten "I'm not sure" examples. These are the hard judgment calls. The ones where you and your teammates disagree. The edge cases that don't fit clean categories. Don't resolve these yet — mark them as controversial. They'll teach you where you need clearer rubrics later.

By the end of week two, you have fifty examples with a deliberate structure. Ten aspirational, twenty grounded-in-reality, ten boundary-setting negative examples, ten trade-off examples, and ten "we need to think harder about this" examples.

## The Magic of Fifty

Why does fifty matter? Two reasons: coverage and calibration.

Fifty examples is enough to cover the major archetypes in most domains without drowning in detail. Customer support usually has five to eight core question types. Most chatbots have ten to fifteen intent categories. RAG systems typically need to handle a dozen retrieval patterns. Fifty examples lets you put multiple examples in each bucket with room for edge cases.

But the deeper reason is calibration. Fifty is the point where patterns become visible. With ten examples, everything looks unique. With a thousand, patterns hide in noise. At fifty, you start seeing the shape of the problem. You notice that tone matters more than you thought. Or that completeness varies wildly. Or that certain edge cases keep appearing.

Fifty is also where team disagreement surfaces. You'll hit example thirty-two and someone will say "wait, I thought we were optimizing for brevity" and someone else will say "no, we're optimizing for completeness." That disagreement was always there, but it stayed hidden until you had enough examples to reveal it. Better to discover that at fifty examples than at five thousand.

## What You Can Do With Fifty

Here's what becomes possible once you have your first fifty:

You can build a prototype rubric. Look across your examples and ask: what dimensions separate good from bad? For customer support, it might be accuracy, empathy, actionability, tone, and completeness. For code generation, it might be correctness, readability, efficiency, and maintainability. Your fifty examples will scream at you what matters.

You can start testing. Pick five examples from your fifty as a test set. Build or prompt your AI. See what it does. You now have a concrete way to measure whether you're getting closer to "good."

You can onboard new team members. Hand them your fifty examples and say "this is what we mean by quality." They'll understand your standards in thirty minutes instead of three months.

You can identify gaps. You'll notice patterns you haven't covered. "We have lots of technical questions but no examples of people who just want reassurance." That tells you what to collect next.

You can get stakeholder alignment. Executives who would never read a requirements doc will look at ten examples and say "yes, that's what we need" or "no, that's too formal." Concrete examples cut through abstraction.

## From Fifty to Five Hundred

Once you have fifty solid examples, the path to more becomes clear. You're not randomly collecting data anymore. You're strategically filling gaps.

Look at your fifty and identify the missing archetypes. You probably have good coverage of common cases but thin coverage of edge cases. You probably have lots of examples where things go right but few where things go wrong. You probably have examples from your power users but not from confused beginners.

Make a list of ten gaps. For each gap, create five examples. That's your next fifty, bringing you to a hundred.

Then do it again. And again. Each round of fifty should target specific weaknesses in your coverage.

Most teams find that two hundred to three hundred examples is the sweet spot for initial ground truth. Enough to cover the domain, not so many that maintenance becomes crushing. You'll add more over time, but two hundred lets you start building and evaluating with confidence.

## The Psychological Shift

Here's what really changes when you embrace the first-fifty philosophy. You stop feeling paralyzed by the cold start.

Before, you couldn't start because you didn't have enough. Now you know "enough" to start is fifty, and you can create fifty in two weeks. Before, you weren't sure what to create. Now you have a structure: aspirational examples, real-world examples, negative examples, edge cases, controversial cases. Before, fifty felt arbitrary. Now you understand it's the minimum for pattern recognition and team calibration.

The cold start problem never fully goes away. Every new domain, every new use case, every new quality bar means starting again from zero. But once you've done it right once, you have a playbook. You know that progress comes from strategic examples, not random collection. You know that small, thoughtful datasets beat large, messy ones in the early days.

## Common Cold Start Mistakes

Let me warn you about the traps teams fall into even when they embrace the first-fifty approach.

Mistake one: using synthetic data too early. It's tempting to use an AI to generate your first fifty examples. "GPT can write customer support responses faster than humans!" Yes, but GPT doesn't know YOUR quality bar. If you let it define "good" for you, you've outsourced your judgment to a system that doesn't understand your brand, your users, or your constraints. Synthetic data comes later, after you've established your own standard. We'll cover this in depth in section 4.6.

Mistake two: optimizing for speed over quality in the first ten. Those first ten examples are your foundation. If they're sloppy, everything built on them will be sloppy. Spending eight hours on ten examples is not waste — it's investment. Spending thirty minutes on fifty examples is waste pretending to be efficiency.

Mistake three: collecting examples without labeling them. "We'll just grab fifty real customer support conversations and that's our ground truth." No. Examples without labels are just data. Ground truth requires judgment. Each example needs a label: is this good or bad? Why? What would make it better? The collection is easy. The judgment is the work.

Mistake four: keeping your fifty examples static. Your first fifty are a living document. As you learn more, some examples will become obsolete. Others will need refined labels. New edge cases will emerge. Plan to revisit and update your core examples every month for the first six months.

Mistake five: trying to make your fifty examples cover everything. They won't. They can't. Fifty examples will never capture every nuance of a complex domain. That's okay. The goal is not comprehensive coverage. The goal is establishing clear patterns and principles that generalize. Trust that good examples of core archetypes teach more than mediocre examples of every edge case.

## The Reality Check

Let me be honest about what fifty examples won't give you.

They won't give you production-ready evaluation. You'll need hundreds more for that.

They won't eliminate all disagreement. Some hard cases will stay hard.

They won't prevent all mistakes. You'll still build things that don't work.

They won't impress people who think "real" data science requires gigabytes of data.

But here's what fifty examples will give you: a foundation. A shared vocabulary. A way to answer "is this good?" with examples instead of arguments. A path forward when you're staring at a blank page.

Every expert started somewhere. Every ground truth dataset began with the first example. The cold start problem is not about having zero. It's about going from zero to one, and then from one to fifty, with intention and structure.

## Your Two-Week Challenge

If you're facing a cold start right now, here's your assignment. Set a timer for two weeks.

Week one: Create ten aspirational examples. Perfect outputs for representative inputs. Refine them with your team until everyone agrees they represent your quality bar.

Week two: Add forty more. Ten hard-good examples, ten terrible examples, ten good-enough examples, ten controversial examples. Pull real inputs if you have them. Borrow adjacent examples if you don't.

At the end of two weeks, you'll have fifty examples. You'll also have something more valuable: clarity. You'll know what "good" means in concrete terms. You'll see patterns you couldn't see before. You'll have gaps you can name and fill.

That's how you beat the cold start problem. Not by avoiding it or overthinking it, but by moving through it with a plan.

In the next section, we'll tackle the hardest part of building ground truth: extracting the knowledge that lives inside expert heads. Because your first fifty examples are usually limited by what you personally know. To scale beyond that, you need techniques for getting knowledge out of the people who have it — domain experts who know what good looks like but struggle to articulate it. Let me walk you through how expert elicitation actually works.
