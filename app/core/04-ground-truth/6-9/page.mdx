# 6.9 — Multi-Modal Systems

Let me tell you about the accessibility disaster that happened because nobody checked multi-modal consistency. A news website used AI to generate image captions for screen readers. The system would analyze photos and create text descriptions for visually impaired users. It seemed to work great in testing.

Then they published an article about a protest march. The image showed peaceful protesters with signs. The AI-generated caption said: "A crowd of people in a chaotic scene with police presence." Technically accurate—there were people, it was somewhat chaotic, police were present. But the connotation was completely wrong. It made a peaceful protest sound like a riot.

Visually impaired users who relied on the captions got a very different understanding of the event than sighted users who saw the photo. The AI had failed at cross-modal consistency: the text didn't convey the same meaning as the image.

This is the multi-modal ground truth challenge: you're not just evaluating text or just evaluating images—you're evaluating whether multiple modalities work together correctly, consistently, and appropriately.

## What Makes Multi-Modal Different

When you evaluate a text-only system, you check if the text is good. When you evaluate an image-only system, you check if the image is good. But multi-modal systems require checking three things:

1. Is each modality good on its own? (Is the text clear? Is the image high-quality?)
2. Do the modalities match? (Does the text describe the image accurately?)
3. Do they work together effectively? (Does combining them create more value than either alone?)

Most teams only check the first. Elite teams check all three.

## The Types of Multi-Modal Tasks

Let's map out the landscape of multi-modal AI, because ground truth differs for each type.

**Text-to-Image Generation**

Input: Text description ("a sunset over mountains")
Output: Image matching the description

Ground truth challenge: Does the image accurately represent the text prompt?

**Image-to-Text (Captioning)**

Input: Image
Output: Text description of the image

Ground truth challenge: Does the text accurately describe the image?

**Visual Question Answering**

Input: Image + Question ("What color is the car?")
Output: Text answer ("Red")

Ground truth challenge: Is the answer correct based on the image?

**Document Understanding**

Input: Document image (receipt, form, invoice)
Output: Structured data extracted from the document

Ground truth challenge: Is the extraction accurate given the document's layout and content?

**Text + Image Retrieval**

Input: Text query ("images of golden retrievers playing in snow")
Output: Relevant images

Ground truth challenge: Are the retrieved images relevant to the query?

**Multi-Modal Generation**

Input: Text prompt
Output: Text + matching image (like a social media post with image)

Ground truth challenge: Are both outputs good AND do they work well together?

Each of these requires different ground truth approaches. Let's walk through them.

## Text-to-Image Ground Truth

When AI generates an image from a text prompt, your ground truth needs to verify multiple things.

**Prompt Adherence**

Does the image actually show what the prompt requested?

Prompt: "A red sports car parked in front of a modern glass building"

Your ground truth should check:
- Is there a car? (Yes/No)
- Is it red? (Yes/No)
- Is it a sports car? (Yes/No/Subjective)
- Is it parked? (Yes/No)
- Is there a building? (Yes/No)
- Is it modern? (Yes/No/Subjective)
- Is it glass? (Yes/No/Partial)

Each element of the prompt should be checked independently. An image might get the car right but miss the building entirely.

**Composition Quality**

Is the image well-composed, or is the layout awkward?

Evaluate:
- Visual balance (is the composition pleasing?)
- Subject placement (is the main subject clear and well-positioned?)
- Lighting (is it realistic and appropriate?)
- Perspective (does it make sense?)

This is subjective, so use multiple labelers and average their ratings.

**Aesthetic Quality**

Is it visually appealing?

This is even more subjective, but it matters. Two images might both accurately show "a red sports car" but one looks professional and one looks amateurish.

Rate on dimensions:
- Color harmony
- Detail quality
- Realism (if that's the goal)
- Artistic appeal

**Technical Quality**

Are there artifacts, distortions, or errors?

Common issues in AI-generated images:
- Weird hands (classic problem)
- Distorted faces
- Impossible physics (shadows going wrong direction, reflections not matching)
- Text gibberish (AI-generated text in images is often nonsense)
- Blurriness or artifacts

Your ground truth should flag these technical errors.

**Safety and Appropriateness**

Does the image avoid inappropriate content?

Check for:
- NSFW content (when not requested)
- Violence or gore
- Copyrighted material (clear reproductions of known artworks, logos)
- Stereotypical or biased representations

This is critical for production systems. A single inappropriate image can cause serious problems.

## Image-to-Text Ground Truth

When AI generates a caption for an image, your ground truth verifies the caption is accurate and useful.

**Factual Accuracy**

Does the caption correctly describe what's in the image?

Show labelers the image and the caption. Ask: "Is this description accurate?"

Check for:
- Hallucination (describing things not in the image)
- Omission (missing important visible elements)
- Misidentification (calling a dog a cat, calling a sedan a truck)

**Completeness**

Does it describe all the important elements, or just some?

A photo shows three people at a beach with a sunset in the background. Captions:
- "A person at the beach" (Incomplete—only mentions one person, misses sunset)
- "Three people at the beach" (Better but still incomplete)
- "Three people standing on a beach with a colorful sunset in the background" (Complete)

Your ground truth should rate completeness separately from accuracy.

**Specificity vs Generality**

Is the level of detail appropriate?

Sometimes you want specific captions ("a golden retriever puppy"), sometimes general ("a dog").

For accessibility (screen readers), more detail is usually better. For image search, the right level depends on the use case.

Your ground truth should specify the desired specificity level for your application.

**Tone and Style**

Is the caption written appropriately for the context?

For social media: Casual, engaging, maybe creative
For news: Objective, factual, professional
For accessibility: Clear, detailed, neutral

The same image might need different captions for different contexts. Your ground truth should include context and verify style match.

## Cross-Modal Consistency

Here's the hardest part of multi-modal ground truth: checking that different modalities tell the same story.

Image shows: Happy children playing with a dog in a yard
Caption says: "Children enjoying outdoor activities"

Technically accurate, but the caption misses the emotional warmth of the scene and the presence of the dog (a key element). A sighted user gets a different experience than someone relying on the caption.

Your ground truth should check:

**Semantic Consistency**: Do the modalities convey the same meaning?

**Emotional Consistency**: Do they evoke the same feeling?

**Emphasis Consistency**: Do they prioritize the same elements?

This requires labelers to evaluate both modalities together and judge whether they align.

## Visual Question Answering Ground Truth

VQA combines image understanding with question answering. Ground truth is straightforward in principle but tricky in practice.

Image: [A photo of a kitchen]
Question: "What color is the refrigerator?"
Answer: "White"

Your ground truth verifies:

**Answer Correctness**: Is "white" the right answer based on the image?

**Answerable Question Detection**: If the refrigerator isn't visible in the image, the correct answer is "I can't see a refrigerator in this image," not a guess.

**Ambiguity Handling**: If the refrigerator is partially visible and might be white or off-white, the answer should reflect uncertainty.

The challenge: labelers need to look at both the image and the question, which takes more time than evaluating text alone.

## Document Understanding Ground Truth

Document AI (extracting data from receipts, invoices, forms, IDs) is a special case of multi-modal AI because it combines vision, OCR, and layout understanding.

Your ground truth needs to verify:

**OCR Accuracy**: Did it read the text correctly?

Compare extracted text to the actual text in the document. This is straightforward if you have digital originals, harder if you only have scans.

**Layout Understanding**: Did it understand the document structure?

A receipt has: merchant name at top, line items in the middle, total at bottom. Did the AI correctly identify which text is which?

**Field Extraction**: Did it extract the right data into the right fields?

From a receipt, extract:
- Merchant name: "Acme Coffee Shop"
- Date: "2026-01-25"
- Total: "$14.50"
- Items: ["Latte - $5.00", "Muffin - $4.00", "Tip - $2.00", "Tax - $3.50"]

Your ground truth includes the document image and the correct structured extraction. Then you verify if the AI's extraction matches.

**Handling Poor Quality Inputs**

Documents are often:
- Crumpled or torn
- Poorly lit
- Photographed at an angle
- Handwritten
- Partially obscured

Your ground truth should include challenging examples and verify the AI handles them appropriately (either extracts correctly despite issues, or indicates low confidence).

## Multi-Modal Retrieval Ground Truth

When users search for images using text (or search for text using images), your ground truth evaluates relevance.

Query: "golden retriever puppies playing in snow"

Retrieved images should show:
- Golden retrievers (not other dog breeds)
- Puppies (not adult dogs)
- Multiple dogs (playing implies interaction)
- Snow

Your ground truth should rate each retrieved image:

**Highly Relevant**: Perfect match (golden retriever puppies playing in snow)

**Somewhat Relevant**: Partial match (golden retrievers in snow, but adults not puppies)

**Marginally Relevant**: Weak match (golden retrievers, but no snow)

**Not Relevant**: No match (different dog breed, no snow)

Then evaluate the retrieval set as a whole:
- Precision: What percentage of retrieved images are relevant?
- Recall: What percentage of all relevant images were retrieved?
- Ranking: Are the most relevant images ranked first?

## Multi-Modal Generation Ground Truth

When AI generates both text and image together (like creating a social media post), you evaluate:

**Individual Quality**: Is the text good? Is the image good?

**Consistency**: Do they tell the same story?

**Complementarity**: Do they add value to each other, or is one redundant?

Best case: The text and image complement each other. The image shows something the text explains, or the text adds context the image doesn't convey.

Worst case: The text and image contradict each other or are completely unrelated.

Your ground truth should rate both individual quality and combined effectiveness.

## Bias and Fairness in Multi-Modal Systems

Multi-modal systems can exhibit bias in ways that text-only or image-only systems can't.

**Image Generation Bias**

Prompt: "A doctor"
Generated image: Always shows men, rarely women

Prompt: "A nurse"
Generated image: Always shows women, rarely men

Your ground truth should test for stereotypical associations and penalize them.

**Caption Bias**

Image: Professional woman in business attire
Biased caption: "A woman in professional dress"
Non-biased caption: "A professional in business attire"

The first unnecessarily emphasizes gender when it's not relevant.

Your ground truth should check for:
- Unnecessary demographic mentions
- Stereotypical associations
- Biased language choices

**Representation Bias**

Do generated images show diversity across race, gender, age, ability, body type?

If prompts don't specify, do generated images default to showing diverse people or only certain demographics?

Your ground truth should include diversity testing across demographic dimensions.

## Cultural Appropriateness in Multi-Modal Content

Images can be offensive in ways that vary by culture. Your ground truth needs cultural awareness.

Example: Hand gestures that are innocuous in one culture might be offensive in another.

Or: Clothing that's normal in one cultural context might be inappropriate in another.

Your ground truth labeling team should include diverse cultural perspectives to catch these issues.

## Safety: The Multi-Modal Wildcard

Multi-modal systems have unique safety challenges.

**Misleading Image-Text Pairs**

Image: Stock photo of a healthy person
Text: "This person cured their disease with this one weird trick"

Each modality might be individually fine, but combined they're misleading.

**Deepfakes and Misinformation**

AI-generated images paired with false text can spread misinformation very effectively.

Your ground truth should flag:
- Misleading combinations (even if each part is individually acceptable)
- Potential for misinformation (claims paired with synthetic "evidence")
- Deepfake or manipulated images

## Accessibility as Ground Truth

For many multi-modal systems, accessibility is a core requirement, not an afterthought.

If you generate images, you should generate alt text. Your ground truth should verify:

**Alt Text Quality**: Is it descriptive enough for screen readers?

**Redundancy Avoidance**: If text already describes the image, the alt text can be brief. If the image contains information not in the text, the alt text must include it.

If you generate audio with visual elements, you should provide transcripts or captions. Your ground truth verifies:

**Caption Accuracy**: Do captions match the audio?

**Speaker Identification**: Are different speakers identified?

**Non-Speech Audio Description**: Are important sounds described? (e.g., "[laughter]", "[door slams]")

## The Format and Technical Quality Dimension

Multi-modal outputs have technical requirements beyond content quality.

**Image Technical Requirements**:
- Resolution (high enough for intended use?)
- Format (PNG, JPEG, WebP?)
- File size (small enough to load quickly?)
- Color space (RGB, CMYK for print?)

**Video Technical Requirements**:
- Frame rate (smooth playback?)
- Audio sync (does audio match video?)
- Length (appropriate for platform?)
- Aspect ratio (vertical for TikTok, horizontal for YouTube?)

Your ground truth should verify technical requirements are met, not just content quality.

## Labeling Tools for Multi-Modal Ground Truth

Multi-modal ground truth requires specialized labeling tools that can:

**Display Multiple Modalities**: Show image and text together, or video with audio

**Support Rich Annotations**: Draw bounding boxes on images, timestamp video segments, link text to image regions

**Enable Cross-Modal Linking**: Mark which part of the text describes which part of the image

Standard text labeling tools don't work well for multi-modal tasks. Invest in tools designed for this.

## The Warning: What Happens If You Skip This

If you evaluate multi-modal systems by checking each modality independently without verifying they work together, here's what happens:

Your AI will generate images that don't match the text prompts. It will write captions that misrepresent images. It will create text-image pairs that contradict each other. Users will be confused, misled, or frustrated.

For accessibility, the impact is worse: you'll provide a different, possibly wrong, experience to users who can't access all modalities.

I've seen a company launch an image generation feature that produced beautiful images that often didn't match the prompts. Users would request "a sunset over the ocean" and get a forest. The images were high-quality, but they were wrong. The feature was quickly deprecated.

Don't evaluate the modalities in isolation. Evaluate the system as a whole, including how the modalities interact.

## Bridge to Internal Automation

We've been discussing AI systems that interact with end users: chatbots, content generators, multi-modal assistants. But a huge portion of AI deployment is internal: automating workflows, processing documents, routing tickets, generating reports. These systems don't have users who see the output—they have downstream processes that depend on the output being correct. Ground truth for internal automation is less about "is this engaging?" and more about "is this accurate and efficient enough to replace the manual process?" Let's walk through how ground truth changes when your AI is automating internal work instead of serving external users.
