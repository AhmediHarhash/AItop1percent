# 7.9 — From Ground Truth to Everything Else

Let me tell you about the moment I really understood how ground truth connects to everything.

I was helping a team debug why their automated evaluations kept disagreeing with their human evaluations. The automated evals said 92 percent pass rate. The human reviewers said more like 78 percent. We spent days checking the code, reviewing the automated eval logic, looking for bugs.

Then someone asked: "What ground truth are the automated evals using?" We checked. They'd copied some example-based tests from an old prototype. The examples were outdated, incomplete, and based on different product requirements than what the human reviewers were working from.

We weren't measuring the same thing.

Once we aligned the automated evals to the same ground truth the humans were using, the scores converged. The problem wasn't the automation. It was that we had two different definitions of "good" running in parallel.

This is the insight that changes how you think about ground truth: it's not just one component of your evaluation system. It's the foundation that everything else is built on. If your ground truth is wrong, misaligned, or unclear, every downstream system inherits that wrongness.

Let me walk you through how ground truth connects to every other piece of your AI evaluation practice.

## Ground Truth to Datasets (Section 3)

Your ground truth determines what data you need to collect.

### The Connection

When you define ground truth, you're saying: "Here's what good looks like." That immediately raises the question: "Do we have enough examples to represent all the scenarios where we need to deliver good?"

Ground truth tells you what diversity you need in your dataset. If your ground truth says you must handle questions in multiple languages, your dataset needs examples in multiple languages. If your ground truth says you must adapt tone based on user context, your dataset needs examples with various user contexts.

Ground truth also tells you what volume you need. If your ground truth has detailed criteria for twenty different quality dimensions, you need enough data to evaluate each dimension reliably. A tiny dataset won't give you sufficient signal.

### The Failure Mode

Teams often collect data before defining ground truth. They gather thousands of examples, then create ground truth, then realize their dataset doesn't cover half the scenarios their ground truth requires.

Now they're in a bind: either go collect more data (expensive and slow) or weaken their ground truth to match their data (compromising quality standards).

### The Right Sequence

Define ground truth first, at least in draft form. Even rough ground truth tells you what to collect.

Then design your dataset to cover the scenarios and dimensions your ground truth measures.

Then refine your ground truth as you learn from the data collection process.

It's iterative, but starting with ground truth ensures your dataset is purposeful, not random.

## Ground Truth to Labeling and Annotation (Section 4)

Your ground truth becomes the instruction manual for your annotation team.

### The Connection

When you hire people to label or annotate data, what do you give them? Ideally, your ground truth.

The rubrics you created define what annotators should check for. The examples you curated show them what different quality levels look like. The criteria you documented tell them how to make judgment calls.

Ground truth isn't separate from annotation guidelines — it is annotation guidelines, possibly with additional context or training for the annotation team.

### The Failure Mode

Teams create ground truth for evaluating their AI, then create separate annotation guidelines for labeling training data. The two sets of standards diverge.

Annotators label data according to one standard. The AI is then evaluated against a different standard. Even if the AI perfectly learns from the labeled data, it fails evaluations because the standards were misaligned.

### The Right Approach

Your ground truth should be the source of truth (hence the name) for annotation guidelines.

Annotators should be trained on the same rubrics, examples, and criteria that you use for evaluation. They should achieve good calibration on the same calibration sets.

When you update ground truth, update annotation guidelines. When annotation guidelines need clarification, consider if ground truth needs the same clarification.

Keep them synchronized, or accept that you're optimizing for one thing and evaluating for another.

## Ground Truth to Quality Metrics (Section 5)

Your ground truth defines what you measure.

### The Connection

Every metric you track should connect back to some aspect of your ground truth.

If your ground truth says factual accuracy is critical, you better have an accuracy metric.

If your ground truth says responses should be concise, you should measure verbosity.

If your ground truth says the system should escalate uncertain cases, you should track escalation rates.

Metrics that don't connect to ground truth are vanity metrics — numbers you track but that don't actually tell you if you're delivering quality as you've defined it.

### The Failure Mode

Teams track whatever's easy to measure rather than what their ground truth says matters.

Ground truth says "empathetic and helpful." Metrics track response time and word count. These aren't unrelated, but they're not the same thing.

Result: you optimize for fast, concise responses that meet your metrics but fail your ground truth's actual requirements.

### The Right Approach

For each criterion in your ground truth, define at least one metric that measures it.

Some criteria are easy to measure automatically (length, response time, format compliance).

Some require human evaluation (empathy, nuance, appropriate tone).

Some can be approximated with proxy metrics (user satisfaction as a proxy for helpfulness).

Accept that not everything in ground truth is directly measurable, but ensure that your measurable metrics collectively represent your ground truth priorities.

## Ground Truth to Human Evaluations (Section 6)

Your ground truth calibrates human evaluators.

### The Connection

When you bring in human evaluators to assess your AI's outputs, they need to know what "good" means. That's your ground truth.

You train evaluators on your rubrics. You calibrate them on your example sets. You measure their reliability by checking if they rate ground truth examples consistently.

Ground truth is both the training material and the calibration tool for human evals.

### The Failure Mode

Teams hire expert evaluators and let them use their own judgment, assuming expertise equals consistency.

But experts have different areas of expertise, different priorities, different standards. Without shared ground truth, expert evaluations are just expensive subjective opinions that don't agree with each other.

### The Right Approach

Even when working with experts, provide ground truth as the baseline. Say: "Here's what we consider excellent, good, acceptable, and poor. Your expertise should inform ratings, but within this framework."

Measure inter-rater reliability among human evaluators. When they disagree, trace back to ground truth: is ground truth ambiguous? Do evaluators interpret it differently? Update ground truth to resolve the ambiguity.

Use ground truth to onboard new evaluators and to refresh existing evaluators' calibration over time.

## Ground Truth to Automated Evaluations (Section 7)

Your ground truth becomes test cases for automation.

### The Connection

Automated evaluations are attempts to encode your ground truth as runnable tests.

Example-based tests use your ground truth examples as test cases: "The system should respond to this input in this way."

Criteria-based tests encode your ground truth rubrics as checks: "The response must include a disclaimer when discussing medical information."

Model-based evaluations (LLM-as-judge) use your ground truth to calibrate the judge model.

In every case, automated evals are trying to mechanize ground truth application.

### The Failure Mode

Teams build automated evals without grounding them in documented ground truth. Each engineer writes tests based on their own understanding of quality.

Result: a test suite that reflects whoever wrote it, not a shared quality standard. When tests fail, debates ensue about whether the test or the system is wrong.

### The Right Approach

Automated evals should be derived from ground truth, not invented independently.

When you add a criterion to ground truth, ask: "How do we test for this automatically?" If you can't test it, accept that you'll need human evaluation for that criterion.

When an automated eval fails, check both the system and the ground truth. Maybe the system is wrong, or maybe the ground truth has drifted and the test is outdated.

Periodically validate that automated evals still match ground truth by running both automated and human evals on the same examples and checking correlation.

## Ground Truth to Agent Evaluations (Section 8)

For agentic systems, ground truth defines both action quality and decision quality.

### The Connection

Agents don't just produce outputs — they take actions and make decisions. Ground truth for agents specifies:

What actions are acceptable in what contexts.

What decision-making processes are correct.

What goals agents should optimize for.

What constraints they must respect.

This is harder than output-based ground truth because you're judging process and intent, not just final results.

### The Failure Mode

Teams evaluate agents only on outcome: "Did it complete the task?" But the same outcome can result from good decisions or lucky flukes, from safe processes or risky shortcuts.

Without ground truth that specifies how agents should behave, you can't distinguish between an agent that got lucky and one that's reliably good.

### The Right Approach

Develop ground truth that covers both outcomes and processes. Specify not just what agents should achieve but how they should achieve it.

Include edge cases in ground truth: when should agents ask for clarification? When should they refuse a task? When should they escalate to humans?

This process-oriented ground truth becomes the foundation for agent evaluation.

## Ground Truth to RAG Evaluations (Section 9)

For retrieval-augmented generation, ground truth specifies both retrieval quality and generation quality.

### The Connection

RAG systems have two components to evaluate: did they retrieve the right information? Did they use it correctly in generation?

Ground truth for RAG includes:

What documents or passages should be retrieved for specific queries.

How retrieved information should be incorporated in responses.

When to cite sources and how.

What to do when retrieved information conflicts or is insufficient.

### The Failure Mode

Teams evaluate only the final output, ignoring whether retrieval was correct. An RAG system might give the right answer for the wrong reasons (retrieved irrelevant docs but generated a plausible answer anyway).

This creates brittle systems that seem to work but fail when inputs vary slightly.

### The Right Approach

Ground truth should cover the full pipeline. Example sets should show:

Query → Expected retrieved documents → Expected response using those documents.

This lets you evaluate retrieval and generation separately and together, diagnosing where failures occur.

## Ground Truth to Production Monitoring (Section 11)

Your ground truth defines what to monitor in production.

### The Connection

When your AI is live, you need to know if it's maintaining quality. But you can't manually review every output. You need automated monitoring.

What should you monitor? Whatever your ground truth says matters.

If ground truth says factual accuracy is critical, monitor for potential hallucinations or contradictions.

If ground truth says safe handling of sensitive topics, monitor for safety issues.

If ground truth says appropriate escalation, monitor escalation rates and appropriateness.

Ground truth translates directly into monitoring alerts and dashboards.

### The Failure Mode

Teams monitor generic metrics (latency, error rates, traffic) but not quality metrics aligned with ground truth.

Production system degrades in quality, but monitoring doesn't catch it because you're not measuring what your ground truth says matters.

### The Right Approach

Design monitoring to detect violations of ground truth in production.

Use automated evals based on ground truth as continuous production monitoring.

Alert when production behavior diverges from ground truth expectations.

Use production monitoring data to feed back into ground truth reviews — when you see new failure modes in production, add them to ground truth.

## Ground Truth to Regression Testing (Section 12)

Ground truth prevents regressions.

### The Connection

Regression testing asks: "Did this change make anything worse?" But worse according to what standard? Your ground truth.

Ground truth examples become regression tests: "This example used to pass; it should still pass after this change."

Ground truth criteria become regression checks: "We used to meet this standard; we should still meet it."

### The Failure Mode

Teams do regression testing by comparing new and old model outputs and eyeballing differences. Different people have different opinions on whether changes are regressions.

Without ground truth, "regression" is subjective.

### The Right Approach

Evaluate both old and new systems against the same ground truth. If the new system scores lower on ground truth criteria, that's a regression.

Maintain a regression test suite that's grounded in your ground truth examples and criteria.

When you update ground truth, re-evaluate your regression tests. Some former "regressions" might not be regressions under updated standards.

## Ground Truth to Release Gates (Section 13)

Ground truth defines your quality bar for shipping.

### The Connection

Release gates are criteria you must meet before shipping. Those criteria should come from ground truth.

"Must score above 90 percent on ground truth evaluation" is a release gate grounded in ground truth.

"Must pass all safety criteria from ground truth rubric" is another.

Ground truth makes release gates objective. Not "it seems good enough" but "it meets our documented quality standard."

### The Failure Mode

Teams set arbitrary release gates (must be 95 percent accurate) without grounding them in actual ground truth about what quality means.

Result: you might meet the gate but still ship poor quality because the gate measured the wrong thing.

### The Right Approach

Release gates should be derived from ground truth. Ask: "What level of ground truth compliance is acceptable for launch?"

This makes gates meaningful and defensible. When someone asks "Why can't we ship yet?" the answer is "We don't meet our quality standards" with specific ground truth criteria that aren't met.

## Ground Truth to Red Teaming (Section 14)

Ground truth guides adversarial testing.

### The Connection

Red teaming tries to break your system by finding inputs that cause undesired behavior. But what counts as undesired? Your ground truth.

Ground truth specifies what the system must never do (safety criteria), what it should always do (reliability criteria), and what boundaries it must respect (policy criteria).

Red team efforts target these ground truth requirements: can we make it violate safety criteria? Can we make it fail reliability requirements?

### The Failure Mode

Red teaming without clear ground truth becomes a fishing expedition. Testers find things that seem bad, but there's debate about whether they're actually violations or just unexpected behavior within acceptable bounds.

### The Right Approach

Give red teamers your ground truth and ask them to find violations. This focuses effort and makes findings actionable.

When red teaming discovers new failure modes, update ground truth to cover them. This closes the loop: adversarial testing strengthens ground truth.

## The Ground Truth Dependency Graph

Visualize it like this:

Ground truth sits at the foundation. Everything else builds on it:

Datasets are collected to cover ground truth scenarios.

Annotations use ground truth as guidelines.

Metrics measure ground truth criteria.

Human evals apply ground truth rubrics.

Automated evals encode ground truth as tests.

Production monitoring watches for ground truth violations.

Regression tests prevent ground truth degradation.

Release gates enforce ground truth compliance.

Red teaming targets ground truth boundaries.

If ground truth is wrong or unclear, everything downstream inherits that problem. If ground truth is solid, everything downstream has a reliable foundation.

## The Coherence Check

Here's a powerful exercise for your team: take any two pieces of your evaluation infrastructure and check if they're grounded in the same ground truth.

Do your human evaluations and automated evaluations agree on what "good" means?

Do your production metrics track the same things your ground truth rubric measures?

Do your release gates check the criteria your ground truth says are most important?

Where you find misalignment, you've found a problem. Fix it by aligning to ground truth, or update ground truth if the misalignment reveals that your ground truth is wrong.

## Ground Truth as Integration Point

In mature organizations, ground truth becomes the integration point for the entire evaluation practice.

When product teams define new features, they update ground truth to specify what good behavior looks like for those features.

When dataset teams collect new data, they check ground truth to ensure coverage.

When annotation teams label data, they use ground truth guidelines.

When eval teams build new tests, they derive them from ground truth.

When monitoring teams set up alerts, they base them on ground truth criteria.

When compliance teams audit, they check that ground truth incorporates regulatory requirements.

Everyone speaks the same language: the language of ground truth.

## The Cascade of Quality

Think of ground truth as a quality cascade:

Poor ground truth leads to poor datasets (wrong coverage), poor annotations (wrong standards), poor metrics (wrong measurements), poor evals (wrong tests), poor monitoring (wrong alerts), and ultimately poor AI systems (optimized for the wrong thing).

Strong ground truth leads to purposeful datasets, aligned annotations, meaningful metrics, reliable evals, effective monitoring, and ultimately high-quality AI systems.

The leverage is enormous. Investing in ground truth quality pays dividends across every downstream system.

## The "So What" of Ground Truth

This is the "so what" moment: ground truth isn't bureaucracy or documentation for its own sake. It's the foundation of your entire quality system.

When someone asks "Why should we invest time in ground truth?" the answer is: because everything else depends on it. Skimp on ground truth, and you undermine every evaluation practice you build.

Invest in ground truth, and you enable everything else to work well.

In the next section, we'll talk about a practical consideration that affects every type of ground truth: freshness rules — understanding when ground truth expires and how to manage its shelf life.
