# 1.4 — The Definition Stack: Correct, Safe, Useful

Most teams define ground truth as "factually accurate." Then they ship and discover that factually accurate responses can still destroy their product.

Let me show you what I mean with a real scenario.

A health tech company built a symptom checker. Users typed in symptoms, and the AI suggested possible causes. They defined ground truth as: "medically accurate according to peer-reviewed clinical guidelines."

They tested rigorously. They had medical professionals review responses. They checked every claim against UpToDate and medical textbooks. Their accuracy was 94%. They shipped with confidence.

Within three weeks, they had to add a crisis intervention layer because the bot was giving medically accurate but psychologically harmful responses to users with health anxiety. It would list rare but serious conditions (technically accurate) without appropriate context, triggering panic attacks. It would provide detailed medical information (factually correct) without emphasizing when professional care was needed.

Every response was medically accurate. Many responses were still terrible. What went wrong?

They'd only defined one layer of ground truth: correctness. They'd missed two others: safety and usefulness.

## The Three-Layer Stack

Good ground truth has three layers. Think of them as a stack—each one builds on the previous:

Layer 1 is Correct: factually accurate, logically valid, based on reality.

Layer 2 is Safe: doesn't cause harm, respects boundaries, follows regulations.

Layer 3 is Useful: actually helps the user, fits their context, achieves their goal.

A response needs all three layers to be truly good. Most teams only define Layer 1 and wonder why their product still fails.

Let me walk you through each layer and show you why they're all essential.

## Layer 1: Correct

Correctness is foundational. If a response is factually wrong, nothing else matters.

For a customer support bot, correct means: the information matches the actual product features, pricing, and policies.

For a coding assistant, correct means: the code is syntactically valid and logically sound.

For a research assistant, correct means: the claims can be verified against cited sources.

This is what most teams focus on. It's important. But it's not sufficient.

Here's an example of a response that's correct but fails the other layers:

User asks a banking chatbot: "What's my account balance?"

Bot responds: "Your checking account balance is $47.23. Based on your transaction history, you're spending significantly more than you earn. You've overdrafted three times in the past six months. Your financial behavior suggests poor money management."

Every fact in that response is accurate. The account balance is right. The transaction history is right. The overdraft count is right. It's 100% correct.

It's also terrible. Why? Because it violates safety (judgmental tone could harm users with financial stress) and usefulness (unsolicited financial criticism doesn't help someone who just wanted their balance).

Correctness alone isn't enough.

## Layer 2: Safety

Safety means the response doesn't cause harm—to users, to the company, to society.

Safety is where most teams have blind spots because it requires thinking beyond accuracy. You have to ask: "Even if this is true, could it cause harm?"

Let me break down the types of safety that matter:

Physical safety: doesn't give advice that could injure someone. A fitness AI shouldn't recommend unsafe exercises. A nutrition AI shouldn't suggest dangerous diets.

Psychological safety: doesn't trigger trauma, anxiety, or distress. A mental health bot shouldn't use language that could worsen someone in crisis. A medical AI shouldn't catastrophize symptoms.

Legal safety: doesn't expose the company to liability. A customer service bot shouldn't make promises the company can't keep. A financial advisor bot shouldn't give advice that requires a license to give.

Privacy safety: doesn't leak sensitive information. A support bot shouldn't reveal one customer's data to another customer. An internal tool shouldn't expose confidential strategy.

Brand safety: doesn't damage the company's reputation. A public-facing bot shouldn't make controversial political statements. A customer service bot shouldn't use sarcasm that could be perceived as mean.

Societal safety: doesn't perpetuate harm at scale. A hiring assistant shouldn't encode discriminatory biases. A content recommendation system shouldn't radicalize users.

Here's an example: A legal AI is asked, "Can I fire an employee for being pregnant?"

A correct-only response: "In the United States, the Pregnancy Discrimination Act of 1978 prohibits employment discrimination based on pregnancy. Firing an employee because she is pregnant is illegal under federal law."

A correct-and-safe response: "In the United States, firing an employee because of pregnancy is illegal under the Pregnancy Discrimination Act. I'm an AI assistant and can't provide legal advice. For questions about employee termination, you should consult with an employment attorney to ensure you're complying with federal, state, and local laws."

Both responses are factually accurate. The second one adds safety by disclaiming legal advice (legal safety) and directing the user to appropriate professional help.

## Layer 3: Useful

A response can be correct and safe and still be useless. Usefulness means it actually helps the user accomplish their goal.

Let me show you what useless-but-correct-and-safe looks like:

User asks a recipe AI: "How do I make chocolate chip cookies?"

Bot responds: "Chocolate chip cookies are a baked dessert originating in the United States in the 1930s. They typically contain flour, sugar, eggs, butter, and chocolate chips. Baking involves mixing ingredients and applying heat in an oven. For food safety, ensure ingredients are fresh and stored properly."

Everything in that response is correct. It's safe—no dangerous food advice. It's also completely useless. The user wanted a recipe with measurements and steps. They got a Wikipedia summary.

Usefulness requires understanding:

- What is the user trying to accomplish?
- What information do they need to accomplish it?
- What's their context and skill level?
- What format helps them best?

For the cookie question, a useful response is:

"Here's a simple chocolate chip cookie recipe:

Ingredients: 2 1/4 cups flour, 1 tsp baking soda, 1 tsp salt, 1 cup butter (softened), 3/4 cup sugar, 3/4 cup brown sugar, 2 eggs, 2 tsp vanilla, 2 cups chocolate chips.

Steps: Preheat oven to 375F. Mix dry ingredients. Beat butter and sugars until fluffy. Add eggs and vanilla. Gradually blend in dry ingredients. Stir in chocolate chips. Drop spoonfuls onto baking sheets. Bake 9-11 minutes until golden brown.

Makes about 5 dozen cookies."

That's useful. It gives actionable information in a format the user can follow.

## Why Teams Miss Safety and Usefulness

Let me walk you through why teams focus on correctness and miss the other layers.

First, correctness is measurable. You can fact-check a claim. You can verify code executes. You can check math. Safety and usefulness are more nuanced. They require judgment and context.

Second, correctness is what academic ML research focuses on. Papers measure accuracy, F1 scores, precision, and recall. They rarely measure "did this avoid psychological harm" or "was this actually helpful." Teams coming from research backgrounds default to what they know.

Third, correctness is what breaks loudly. If your bot gives a factually wrong answer, users complain immediately. If it gives a correct but harmful answer, the damage is often silent or delayed. Teams prioritize what breaks loudly.

Fourth, correctness feels objective while safety and usefulness feel subjective. "Is this fact true?" has a clear answer. "Could this cause harm?" and "Is this helpful?" feel fuzzy. Teams avoid fuzziness.

But in production, safety and usefulness matter just as much as correctness. Often more.

## The Cost of Missing Layers

Let me show you what happens when you only define correctness:

A customer service bot gives accurate information about a policy change that makes the customer's service worse. Correct information, delivered with no empathy, no acknowledgment of the customer's frustration, no offer of alternatives. The customer churns.

A coding assistant suggests code that works perfectly but violates your company's security standards. Correct code, unsafe practice. A vulnerability ships to production.

A writing assistant generates an email that's grammatically perfect and factually accurate but uses a tone completely wrong for the recipient relationship. Correct information, useless for the actual goal of building that relationship.

In each case, correctness was necessary but not sufficient. The missing layers caused the failure.

## Building The Full Stack

Here's how to define ground truth with all three layers:

Start with correctness. What facts must be accurate? What logic must be valid? What sources must be cited? Define this explicitly.

Add safety. For each type of response, ask: what could go wrong? What harms could this cause? What boundaries must we respect? What disclaimers do we need?

Then add usefulness. What is the user trying to accomplish? What information helps them succeed? What format works best? What context matters?

Let me show you this with a concrete example: a financial planning AI.

Correctness criteria:
- All numerical calculations are mathematically accurate
- All financial product descriptions match official documentation
- All tax information reflects current tax law for the user's jurisdiction
- All historical market data is cited from verified sources

Safety criteria:
- All advice includes disclaimers that this is educational, not professional financial advice
- Recommendations account for the user's stated risk tolerance
- No guarantees about future investment performance
- Red flags high-risk products with appropriate warnings
- Suggests professional consultation for complex tax situations

Usefulness criteria:
- Recommendations match the user's stated goals (retirement, home purchase, etc.)
- Explanations are appropriate for the user's financial literacy level
- Action steps are specific and achievable
- Alternatives are provided for different scenarios
- Time horizons match the user's situation

That's a complete ground truth definition. It covers all three layers.

## Layer Conflicts

Sometimes layers conflict. A response can't be both completely safe and maximally useful. You have to make trade-offs.

Example: A medical AI is asked about a symptom that could indicate either a common, harmless condition or a rare, serious condition.

Maximum usefulness would be: "This is most likely [common condition]. Try [simple remedy]."

Maximum safety would be: "This could indicate [serious condition]. See a doctor immediately."

The first response is more helpful for the 99% of cases where it's the common condition. The second response is safer for the 1% where it's serious.

How do you resolve this? You make an explicit choice based on your product values and risk tolerance. Maybe you err toward safety for medical products. Maybe you provide both possibilities with appropriate context.

The key is making this choice consciously and documenting it in your ground truth definition, not leaving it to chance.

## Measuring Each Layer

Each layer needs its own measurement approach:

For correctness, you can often measure objectively. Fact-check against sources. Test code execution. Verify calculations.

For safety, you need expert review plus edge case testing. Have domain experts review for potential harms. Specifically test adversarial inputs, vulnerable populations, and misuse scenarios.

For usefulness, you need user feedback and outcome measurement. Did the user accomplish their goal? Did they have to ask follow-up questions? Did they abandon the interaction?

Don't collapse these into a single score. Track them separately. A response can score 95% on correctness, 60% on safety, and 40% on usefulness. That breakdown tells you where to improve.

## Industry-Specific Priorities

Different industries weight these layers differently:

Healthcare and finance prioritize safety. Correctness is necessary, usefulness matters, but safety is paramount. A medical app that's useful but occasionally unsafe is unacceptable.

Creative tools prioritize usefulness. Correctness is less rigid (there's no "correct" poem), safety matters for not producing harmful content, but usefulness—did it help the user create something they like—is the main measure.

Education balances all three equally. Content must be factually correct, pedagogically safe (doesn't confuse or discourage learners), and useful (actually helps students learn).

Customer service prioritizes usefulness with strong safety constraints. The response must solve the customer's problem (useful) without making unauthorized promises or violating privacy (safe).

Your ground truth priorities should match your industry requirements and user needs.

## The 2026 Reality: EU AI Act

As of 2026, the EU AI Act makes the safety layer legally mandatory for high-risk AI systems. You can't just define correctness anymore. You must document safety criteria, test for potential harms, and maintain evidence of safety compliance.

For systems deployed in the EU, your ground truth must explicitly include:

- Risk identification for your use case
- Mitigation measures for identified risks
- Testing procedures for safety violations
- Documentation of safety-correctness trade-offs

This isn't optional. It's a legal requirement. Teams that only defined correctness are now scrambling to retrofit safety criteria.

Elite teams defined all three layers from the start and are ready.

## What Elite Teams Do

Let me show you what the top 1% does differently:

They define all three layers before building anything. Correctness criteria, safety boundaries, and usefulness measures are written down and agreed upon.

They measure each layer separately. Their dashboards show correctness rate, safety violation rate, and user goal achievement rate as distinct metrics.

They have different review processes for each layer. Engineers review correctness. Domain experts review safety. User researchers assess usefulness.

They make layer trade-offs explicit. When safety and usefulness conflict, there's a documented decision about which takes priority in that context.

They update all three layers together. When ground truth evolves, they don't just update correctness criteria—they revisit safety and usefulness too.

## The Priority Order

When layers conflict and you can't satisfy all three perfectly, here's the default priority order for most products:

First priority: Safety. Don't cause harm.
Second priority: Correctness. Be factually accurate.
Third priority: Usefulness. Help the user achieve their goal.

This order means: a safe but less useful response is better than a harmful but useful response. An accurate but less helpful response is better than a helpful but wrong response.

But this order isn't universal. For different products, you might prioritize differently. A creative writing tool might put usefulness above strict correctness. A crisis counseling bot might weight safety so heavily that it sacrifices some usefulness.

The key is making your priority order explicit.

## Building Your Stack

Here's how to build your three-layer ground truth definition:

First, list the types of responses your system produces. What are the different tasks, questions, or scenarios?

Second, for each type, define correctness. What facts must be accurate? What logic must be valid?

Third, for each type, define safety. What could go wrong? What harms must be prevented? What disclaimers are needed?

Fourth, for each type, define usefulness. What is the user trying to accomplish? What makes a response helpful for that goal?

Fifth, identify conflicts. Where do these layers pull in different directions? Make explicit trade-off decisions.

Sixth, assign measurement approaches. How will you evaluate each layer?

That's your ground truth stack. Three layers, fully defined, with clear measurement.

In the next section, we'll tackle one of the most common ground truth mistakes: trying to apply a universal standard across different tasks. The reality is that ground truth is always task-specific, and understanding why changes how you design your entire evaluation system.
