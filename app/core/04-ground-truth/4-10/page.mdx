# 4.10 — Ground Truth Granularity: Claim-Level vs Response-Level vs Outcome-Level

Let me show you a debate that shapes everything about how you build evaluation.

You're testing a customer support AI. A user asks: "Can I return my order? I ordered it last week and it arrived damaged."

Your AI responds: "I'm sorry to hear your order arrived damaged. Yes, you can return it within 30 days of delivery. Since you ordered it last week, you're well within the return window. To start the return process, go to your account, click Orders, select the damaged item, and click Return. We'll send you a prepaid shipping label within 24 hours. We also offer replacements instead of refunds if you'd prefer."

How do you evaluate this response? Three approaches:

Approach one (response-level): Rate the overall response on a 1-5 scale. This response is helpful, accurate, and empathetic. Score: 5.

Approach two (claim-level): Evaluate each factual claim separately.
- "30 days return window" — TRUE
- "Well within the window" — TRUE
- "Return process: account > orders > select item > return" — TRUE
- "Prepaid label within 24 hours" — FALSE (it's actually 48 hours)
- "Can choose replacement instead" — TRUE

Four claims true, one false. Overall claim accuracy: eighty percent.

Approach three (outcome-level): Did the customer successfully complete their return? Did they understand the process? Did they need follow-up help? Outcome: customer completed return without escalation. Success: TRUE.

All three approaches are evaluating the same response. But they measure different things and will give you different quality signals. Choosing the wrong granularity for your use case means building an evaluation system that looks rigorous but misses what matters.

Let me walk you through when each level is right and how to implement it.

## Response-Level Ground Truth

Response-level is the simplest approach. You evaluate the entire output as a unit. "Is this response good or bad overall?"

When response-level works:

Short responses where holistic judgment makes sense: A chatbot answering "What are your hours?" Either the response is helpful or it isn't. Decomposing it into claims is overkill.

Subjective quality dimensions: Tone, empathy, brand voice. These are emergent properties of the whole response, not reducible to individual claims.

Creative outputs: Writing, art, music. Quality is holistic, not a sum of parts.

Cases where individual errors don't matter if the overall message is right: A response might have a minor typo but still accomplish its goal.

When response-level fails:

Long complex responses: A response might be ninety percent excellent but have one critical error buried in paragraph three. Response-level evaluation might score it well overall and miss the error.

High-stakes factual domains: Medical, legal, financial. One wrong claim in an otherwise good response can cause serious harm. You need claim-level precision.

RAG and cited responses: If your AI is citing sources and making specific claims, you need to verify each claim, not just the overall gestalt.

Debugging and improvement: Response-level scores tell you "this response is bad" but not why or how to fix it. Claim-level scores pinpoint the specific failure.

How to implement response-level:

Use rubrics with holistic dimensions (accuracy, completeness, tone, etc.). Evaluators score each dimension for the whole response, then combine scores.

Provide anchor examples showing what different overall quality levels look like.

Accept that you're trading precision for simplicity. Response-level is faster but coarser.

## Claim-Level Ground Truth

Claim-level means decomposing responses into individual factual assertions and evaluating each one separately.

The customer support response above contains five claims. Claim-level evaluation checks each claim independently against ground truth.

When claim-level is essential:

RAG systems: You're retrieving facts from documents and including them in responses. Each retrieved fact needs to be verified against the source. One hallucinated fact contaminates the whole response.

High-stakes domains: Healthcare, legal, finance, safety-critical. You cannot afford to miss a single factual error even if the rest is perfect.

Long-form content: Research summaries, reports, analysis. These contain dozens or hundreds of claims. Response-level evaluation can't provide enough resolution.

Debugging: When a response fails, you need to know exactly which claim was wrong so you can fix the retrieval, the knowledge base, or the generation logic.

Compliance and audits: Regulators want to know not just that your AI is "generally accurate" but that specific claims are verifiable and traceable.

When claim-level is overkill:

Subjective or creative content: "This writing has good flow" is not a verifiable claim. It's a subjective judgment better handled at response-level.

Very short responses: If the entire response is one sentence with one claim, claim-level and response-level are the same thing.

Real-time evaluation at scale: Claim-level evaluation is expensive. Decomposing responses into claims, checking each one, and aggregating takes time and compute. If you're evaluating millions of responses, claim-level might be prohibitive.

How to implement claim-level:

Step one: Claim extraction. Given a response, identify all factual claims. This can be done manually for small datasets or with an LLM for scale. "List all factual claims in this response as separate statements."

Step two: Claim verification. For each claim, check it against your knowledge base, documentation, or source documents. Label each claim as TRUE, FALSE, or UNVERIFIABLE.

Step three: Aggregation. Combine claim-level results into a response-level metric. Maybe "a response is acceptable if at least ninety-five percent of claims are true and zero claims are critically false."

Tools for claim-level evaluation in 2026:

Patronus AI and Context.AI have built claim-level factuality evaluation specifically for RAG systems. They automatically extract claims, verify them against sources, and identify hallucinations.

Many teams build custom pipelines: use an LLM to extract claims, use a verification function to check each claim, aggregate results.

## Outcome-Level Ground Truth

Outcome-level zooms out further. Instead of evaluating what the AI said, you evaluate what happened as a result.

Did the user accomplish their goal? Did they need follow-up help? Did they escalate? Did they churn? These are outcomes.

When outcome-level is the right measure:

Agents and multi-step systems: An agent might take ten actions to complete a task. Individual responses might be imperfect, but what matters is whether the task completed successfully.

Chatbots and assistants: The goal is not to give good answers, it's to solve user problems. A response could be concise and accurate but still fail to help the user.

Product-level success: You care about business metrics (retention, satisfaction, revenue) more than technical metrics (accuracy, latency). Outcome-level ties AI quality to business impact.

Long-term effects: Sometimes the "right" response in the moment has bad long-term effects, or vice versa. Outcome-level captures downstream impact.

Example: Customer asks "How do I cancel?" A response-level evaluation might score two responses equally:

Response A: "Click here to cancel [link]"
Response B: "I can help you cancel. First, can I ask what's prompting you to cancel? Sometimes there's a solution."

Both are accurate and helpful. But outcome-level evaluation reveals that Response A has higher churn while Response B sometimes saves customers by solving their underlying issue. Outcome-level tells you B is actually better even though both score well on response-level.

When outcome-level is hard or misleading:

Delayed outcomes: If it takes weeks or months to measure the outcome, you can't use it for rapid iteration.

Noisy outcomes: Many factors besides your AI affect outcomes. A customer might churn because of a product issue, not AI quality. Outcome-level conflates AI quality with other factors.

Unobservable outcomes: You can't always see whether the user succeeded. They might have gotten your AI's answer and then completed the task offline. You don't know if the answer helped.

Ethical concerns: Optimizing for outcomes can lead to manipulative behavior. Maximizing "user engagement" might mean making responses addictive rather than helpful.

How to implement outcome-level:

Step one: Define success outcomes. For customer support: ticket resolved without escalation. For sales: user completed purchase. For education: user demonstrated understanding.

Step two: Track outcomes per interaction. Link each AI response to the outcome that followed. This requires instrumentation.

Step three: Analyze correlations. Which types of responses correlate with successful outcomes? Which correlate with failure?

Step four: Use outcomes to validate other metrics. If response-level scores say quality is high but outcome metrics show users aren't succeeding, your response-level metrics are measuring the wrong thing.

Outcome-level is often the ultimate ground truth but not the only ground truth. Use it to validate and calibrate claim-level and response-level evaluations.

## Choosing Your Granularity

So which level should you use? The answer is often: all three, for different purposes.

For RAG systems, the typical stack is:

Claim-level for hallucination detection: Every factual claim must be verified against sources. This catches hallucinations and errors.

Response-level for quality assessment: Overall, is the response clear, well-structured, appropriately detailed? This catches issues that aren't about individual claim accuracy.

Outcome-level for validation: Do users who get high-scoring responses actually succeed at their tasks more often? This keeps you honest.

For chatbots:

Response-level for most evaluation: Is each response helpful, on-brand, appropriate?

Outcome-level for critical paths: For key flows (onboarding, purchase, cancellation), track whether users complete successfully.

Claim-level for specific high-risk responses: If the bot makes a promise or states a policy, verify that claim.

For agents:

Outcome-level as primary: Did the task complete? That's what matters.

Response-level for debugging: When a task fails, examine individual responses to understand why.

Claim-level rarely: Unless the agent makes specific factual claims that need verification.

The pattern: use the finest granularity that's practical for high-risk, high-value scenarios. Use coarser granularity for scale and speed.

## Granularity and Cost

Finer granularity costs more:

Claim-level requires claim extraction (expensive), per-claim verification (expensive), and aggregation. For a response with ten claims, you do ten verification operations instead of one.

Response-level requires one evaluation per response. Cheaper than claim-level.

Outcome-level requires tracking and attribution infrastructure but can be measured automatically once set up.

If you're evaluating millions of responses, claim-level might be prohibitively expensive. You might do:

Claim-level on a sampled subset for deep quality assurance.
Response-level on the majority for continuous monitoring.
Outcome-level on everything because it's often observable from logs.

This layered approach balances depth with scale.

## Ground Truth Records at Different Granularities

What a ground truth record looks like depends on granularity.

Response-level record:

- Input: [user question]
- Response: [AI response]
- Overall accuracy: 4/5
- Overall helpfulness: 5/5
- Overall tone: 4/5
- Pass/fail: PASS

Claim-level record:

- Input: [user question]
- Response: [AI response]
- Claim 1: "Return window is 30 days" → TRUE (source: policy doc section 4.2)
- Claim 2: "Process: account > orders > return" → TRUE (source: help article)
- Claim 3: "Label sent within 24 hours" → FALSE (actual: 48 hours per policy 4.5)
- Claim 4: "Replacement available" → TRUE (source: policy doc section 4.3)
- Claim accuracy: 75% (3/4 correct)
- Pass/fail: FAIL (critical claim false)

Outcome-level record:

- Input: [user question]
- Response: [AI response]
- Outcome: User completed return process without escalation
- Time to resolution: 8 minutes
- Follow-up questions: 1
- User satisfaction rating: 4/5
- Pass/fail: PASS

Notice they capture different information and support different analyses.

## Mixing Granularities in One Dataset

You don't need to pick one granularity for your entire ground truth dataset. You can mix them strategically.

Example structure:

Tier 1 examples (100 examples): Full claim-level annotation plus response-level rubrics plus outcome tracking. These are your most important test cases. Deep evaluation.

Tier 2 examples (500 examples): Response-level rubrics plus outcome tracking. Good balance of depth and coverage.

Tier 3 examples (2000 examples): Outcome-level only. Broad coverage, lightweight evaluation.

This tiered approach lets you invest deeply where it matters while still getting broad coverage.

## Granularity for Different Quality Dimensions

Even within one response, different quality dimensions might need different granularities.

Accuracy: Claim-level. Each fact must be verified.

Completeness: Response-level. "Did this response address all parts of the question?" is a holistic judgment.

Tone: Response-level. Tone emerges from the whole response, not individual sentences.

Actionability: Response-level or outcome-level. Did the user know what to do next? Did they do it?

Safety: Claim-level for factual safety (no harmful medical advice), response-level for social safety (no offensive language).

This means your ground truth structure might be hybrid:

- Claim-level labels for factual accuracy
- Response-level scores for tone, completeness, clarity
- Outcome-level metrics for success rate

All for the same set of examples.

## Special Case: Claim-Level for RAG

RAG systems deserve special attention because claim-level evaluation is critical.

For RAG, your ground truth must include:

Input question
Retrieved context (what documents were pulled)
Generated response
Extracted claims from the response
For each claim:
  - Is it supported by the retrieved context? (grounding)
  - Is it factually correct? (accuracy)
  - Is it relevant to the question? (relevance)

This structure lets you diagnose RAG failures:

Retrieval failure: Retrieved context doesn't contain the answer.
Grounding failure: Retrieved context has the answer but the response doesn't use it.
Hallucination: Response makes claims not supported by retrieved context.
Irrelevance: Response makes true claims that don't answer the question.

Each failure mode requires different fixes. Claim-level ground truth lets you pinpoint which failure occurred.

In 2026, RAG evaluation tools expect claim-level ground truth. The standard is: every factual claim must have a citation to a source chunk, and that source chunk must actually support the claim.

## Automating Claim Extraction

Claim-level evaluation requires extracting claims from responses. For small datasets, humans can do this. For large datasets, you need automation.

LLM-based claim extraction:

Prompt: "Extract all factual claims from this response as a list of independent statements."

Input: [response]

Output: List of claims

Then validate the extraction. Have humans review a sample to ensure the LLM is extracting claims correctly and completely.

Common extraction errors:

Splitting one claim into multiple redundant claims
Combining multiple claims into one complex statement
Missing claims
Extracting subjective statements as factual claims

Refine your extraction prompt based on these errors. Once extraction quality is high (above ninety percent correct), you can scale it.

## Granularity and Improvement Cycles

Finer granularity enables faster improvement:

Claim-level: "Claim 7 is consistently false because our knowledge base is outdated." → Update knowledge base, immediately improve.

Response-level: "Responses about returns score poorly." → Investigate to find the specific issue, then improve. Slower.

Outcome-level: "Users asking about returns have low success rates." → Investigate to find why, then figure out what's wrong with responses, then improve. Slowest.

But finer granularity is more expensive to gather and maintain.

Balance: Use claim-level for debugging and deep dives. Use response-level for regular monitoring. Use outcome-level for validation and strategic direction.

## When to Shift Granularity

Your needs change over time:

Early stage: Response-level is enough. You're iterating quickly, you need fast feedback, you don't have infrastructure for finer granularity.

Growth stage: Add claim-level for high-stakes domains. You're scaling, quality matters more, you can invest in infrastructure.

Mature stage: Outcome-level becomes primary. You have enough data to measure outcomes reliably. You optimize for business impact, not technical metrics.

Don't lock yourself into one granularity forever. Revisit as your system and needs evolve.

In the next section, we'll look at the practical structure of a ground truth record — what fields and metadata you need to make ground truth usable, auditable, and maintainable at scale. This is where the rubber meets the road for production ground truth systems.
