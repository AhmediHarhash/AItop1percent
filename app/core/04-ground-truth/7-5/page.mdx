# 7.5 — Incorporating Production Feedback

I worked with a team that spent six months building what they thought was the perfect ground truth for their code documentation AI. They brought in senior engineers, studied documentation best practices, created detailed rubrics, validated everything with experts. Their ground truth was beautiful.

They launched. Within two weeks, they knew something was wrong.

Users were giving thumbs down to responses that scored 95 percent on their ground truth. They were giving thumbs up to responses that scored 78 percent. The correlation between ground truth ratings and user satisfaction was weak at best, inverse at worst.

They went back to the drawing board. They analyzed the user feedback. And they discovered something humbling: real users cared about different things than their expert panel had predicted.

The experts valued comprehensive, technically precise documentation. The users — mostly junior developers under deadline pressure — valued quick, practical answers that got them unblocked. The ground truth optimized for the first. The product needed to deliver the second.

The feedback from production users was telling them clearly that their ground truth was misaligned. But they'd nearly ignored it because they were so confident in their expert-driven process.

This is the hard lesson about ground truth: you can craft it as carefully as you want in the lab, but reality happens in production. And production feedback is the ultimate validator of whether your ground truth actually captures what "good" means.

## The Production Feedback Loop

Your ground truth is a hypothesis about what constitutes quality. Production is the experiment that tests that hypothesis.

Every user interaction with your AI generates feedback signals. Some are explicit and obvious. Some are implicit and subtle. All of them contain information about whether your ground truth is right.

The teams that maintain the most accurate ground truth aren't the ones who spend the most time crafting it upfront. They're the ones who have the tightest feedback loops between production and ground truth updates.

Let me walk you through how to build those loops.

## Explicit Feedback Signals

These are the signals where users directly tell you whether the AI's output was good or bad.

### Thumbs Up / Thumbs Down

The simplest signal. After the AI responds, you give users a way to say "this was helpful" or "this was not helpful."

This signal is noisy — some users thumbs down because the answer was correct but not what they wanted to hear. Some thumbs up out of politeness even when the answer was mediocre. But in aggregate, it's valuable.

Here's how to use it for ground truth updates:

Sample responses that scored highly on your ground truth but received thumbs down from users. These are potential false positives in your ground truth — things you think are good but users don't.

Sample responses that scored poorly on your ground truth but received thumbs up from users. These are potential false negatives — things users value that your ground truth doesn't capture.

Look for patterns. If 30 percent of your "excellent" responses by ground truth get thumbs down, and most of those are verbose technical explanations, maybe your ground truth over-values comprehensiveness and under-values brevity.

### User Corrections

Some systems let users correct or edit the AI's output. A user might get a response, then modify it before using it. That modification is gold — it shows you exactly what the AI got wrong and what the user actually needed.

Aggregate these corrections. If users consistently add disclaimers that your AI doesn't include, maybe your ground truth should require disclaimers. If users consistently remove hedging language that your AI includes, maybe your ground truth is too cautious.

Corrections are explicit ground truth from real users. They're showing you their rubric through their edits.

### User Complaints and Support Tickets

When users are frustrated enough to complain or contact support, they're giving you strong signal that something is wrong.

Analyze complaint patterns. What are users saying is bad that your ground truth says is good? What requirements are they expressing that your ground truth doesn't capture?

I've seen teams discover entire missing categories in their ground truth by reading support tickets. Users were asking for a feature the AI could provide but didn't, because the ground truth never specified it should.

### Explicit Quality Ratings

Some systems ask users to rate quality on a scale: "How helpful was this response? 1-5 stars."

This is more nuanced than thumbs up/down but potentially noisier because users interpret scales differently.

The value is in the extremes. Responses that consistently get 5 stars but score medium on your ground truth are telling you something. Responses that consistently get 1 star but score well on your ground truth are telling you something else.

## Implicit Feedback Signals

Users don't always tell you directly whether the AI was good. But their behavior reveals it.

### Abandonment

The user asks a question, gets a response, and immediately leaves without engaging further. This might mean the response was perfect and fully satisfied their need. Or it might mean the response was so bad they gave up.

Context helps distinguish these. If the user's original query suggested a complex, multi-step task and they abandon after one response, that's likely negative signal. They got stuck and bounced.

If users consistently abandon after certain types of responses that your ground truth rates highly, investigate. Your ground truth might be rewarding responses that fail to help users progress.

### Retry and Rephrasing

The user asks a question, gets a response, and immediately asks again in different words. This is strong negative signal — they didn't get what they needed.

Look at what your ground truth said about that first response. If it rated it highly, you've found a gap. The response satisfied your ground truth but not the user's actual need.

### Escalation to Human

If your system offers escalation to human support, track when users take it. Escalation after an AI response is implicit feedback that the AI failed.

Pattern match: what kinds of responses trigger escalation? If your ground truth rates them well but users escalate anyway, your ground truth is measuring the wrong things.

### Copy/Paste Behavior

In some systems, you can track whether users copy the AI's response to use elsewhere. High copy rate suggests the response was valuable. Low copy rate suggests it wasn't.

Correlate copy rate with ground truth scores. If responses your ground truth calls "excellent" rarely get copied, but responses it calls "acceptable" frequently get copied, your ground truth might be optimizing for the wrong quality dimensions.

### Time Spent Reading

How long does the user spend reading the response before taking their next action? Very brief time might mean they immediately saw it was wrong. Very long time might mean they're carefully studying a complex, valuable answer — or struggling to understand a confusing one.

Combined with other signals (did they copy it? did they rephrase? did they abandon?), time-spent helps interpret quality.

## Behavioral Feedback Signals

These are higher-level patterns that emerge from how users interact with your system over time.

### Task Completion Rate

If your AI is meant to help users accomplish tasks, do they complete those tasks? And how does task completion correlate with your ground truth scores?

If users who receive "excellent" responses by your ground truth don't complete their tasks more often than users who receive "acceptable" responses, your ground truth isn't measuring what drives success.

### Repeat Usage

Users who had a good experience come back. Users who had a bad experience don't.

Track user retention and correlate it with the quality of responses they received, as measured by your ground truth. If retention doesn't correlate with ground truth scores, your ground truth might not be capturing the factors that drive user satisfaction.

### Feature Adoption

If you add a new capability to your AI, how quickly do users adopt it? If your ground truth says responses using the new feature are better, but users don't engage with them more, your ground truth might be wrong about their value.

### Session Depth

How many turns does a conversation typically go? If your ground truth rates one-turn conversations as successful but users who get "excellent" first responses still need five more turns to accomplish their goal, maybe your ground truth is measuring answer quality but not problem resolution.

## Converting Signals to Ground Truth Updates

Collecting feedback signals is the easy part. The hard part is deciding which signals warrant ground truth updates and how to make those updates.

### Signal Reliability Assessment

Not all signals are equally reliable for ground truth updates.

Explicit corrections from expert users in your domain? Very reliable. They're showing you what good looks like according to people who know.

Thumbs down from a user who turned out to be looking for something your AI explicitly doesn't do? Not reliable for ground truth. That's user expectation mismatch, not quality failure.

Abandonment in a workflow where abandonment could mean success or failure? Low reliability until combined with other signals.

Before you update ground truth based on a signal, assess: is this signal actually telling me my ground truth is wrong, or is it telling me something else (user expectations, product-market fit, UI/UX issues)?

### Volume and Consistency Thresholds

One user complaining doesn't mean your ground truth is wrong. One hundred users complaining about the same thing probably does.

Set thresholds. You might decide: if more than X percent of "excellent" responses receive negative feedback, or if a pattern appears in more than Y production examples, we investigate for ground truth updates.

This prevents over-reacting to outliers while ensuring you respond to real patterns.

### The User Sample Bias Problem

The users who give feedback aren't a random sample. They're more engaged, more opinionated, more likely to have extreme experiences (very good or very bad).

When you use their feedback to update ground truth, you risk optimizing for the vocal minority instead of the silent majority.

Balance explicit feedback signals (which suffer from sample bias) with implicit behavioral signals (which represent all users but are noisier).

### The Feedback-Ground Truth Mapping

When you identify a feedback pattern that suggests a ground truth issue, you need to translate user feedback into ground truth language.

Users say: "The responses are too long." That's feedback, not ground truth.

Ground truth update: Add a criterion for conciseness. Define a word count target. Add examples of appropriately concise responses.

Users say: "This doesn't answer my question." That's feedback.

Ground truth update: Strengthen criteria for relevance. Add requirement to directly address the user's query in the first sentence. Update examples to demonstrate direct answering.

The translation requires judgment. You're inferring what quality dimension users are actually responding to and how to encode that in your ground truth.

## The Lag Problem

There's always a delay between when something goes wrong in production and when you can update ground truth to prevent it.

The user has a bad experience today. You analyze the feedback tomorrow. You identify a ground truth gap next week. You update ground truth two weeks from now. You improve the model based on new ground truth a month from now.

For a month, users are getting the bad experience that prompted the feedback.

Minimize this lag by:

Automating signal collection and aggregation, so patterns surface quickly.

Having rapid response processes for severe issues (safety, compliance, major UX failures).

Doing small, frequent ground truth updates instead of large, infrequent ones.

The faster your feedback loop, the less time users spend with degraded experiences.

## Feedback Priority Triage

You'll collect more feedback than you can act on. You need to prioritize which feedback drives ground truth updates.

### High Priority: Safety and Compliance

Any feedback indicating safety issues (harmful outputs, privacy violations, security risks) or compliance problems (regulatory violations, policy breaches) gets immediate ground truth review and update.

These aren't quality issues — they're risk issues. Act fast.

### High Priority: High-Volume Patterns

Feedback patterns that affect many users or many interactions. If 20 percent of responses get negative feedback for the same reason, that's high priority.

These represent systematic ground truth misalignment, not edge cases.

### Medium Priority: User Delight Opportunities

Feedback that suggests ways to exceed user expectations, not just meet them. Users expressing surprise and delight at certain behaviors, or disappointment that certain helpful behaviors don't happen.

These are opportunities to raise your ground truth bar.

### Low Priority: Edge Cases and Outliers

Feedback about rare scenarios, idiosyncratic preferences, or cases outside your AI's intended scope.

Document them, but don't let them drive immediate ground truth updates unless they cluster into patterns.

## The Experimentation Mindset

When production feedback suggests a ground truth update, treat it as a hypothesis to test, not a fact to implement.

Run an experiment: update your ground truth for a subset of evaluations. See if the updated ground truth better predicts user satisfaction than the original.

If it does, roll out the update broadly. If it doesn't, you might have misinterpreted the feedback.

This prevents you from ping-ponging your ground truth based on every piece of feedback that comes in.

## Feedback Loops for Different Stakeholders

Different stakeholders care about different feedback signals.

Engineering teams care about thumbs down rates, error patterns, edge case discoveries.

Product teams care about task completion, feature adoption, user retention.

Compliance teams care about policy violation reports, safety incidents, audit findings.

Make sure feedback flows to the right stakeholders and that each stakeholder's concerns can trigger ground truth reviews when warranted.

## The Positive Feedback Trap

It's tempting to focus only on negative feedback — what's broken, what needs fixing. But positive feedback is just as important for ground truth.

When users love something, you want to make sure your ground truth captures why. Otherwise, you might accidentally optimize it away.

I've seen teams "fix" things users loved because their ground truth said those things were flaws. Users gave explicit positive feedback, but the team ignored it because it contradicted their expert-driven ground truth.

Trust the users. If they consistently love something your ground truth rates poorly, your ground truth is wrong.

## Closing the Loop: Showing Users Their Impact

When you update ground truth based on user feedback, close the loop. Let users know their feedback mattered.

This doesn't mean emailing everyone who gave feedback. It means communicating in release notes, in-app messaging, or community forums: "We heard your feedback about responses being too technical. We've updated our quality standards to emphasize clarity and accessibility. You should see more understandable responses going forward."

This builds trust. It shows users their feedback drives real change, which encourages more feedback, which gives you better signals for ground truth updates.

It's a virtuous cycle.

## The Production-Ground Truth Divergence Alert

Build monitoring that alerts you when production feedback and ground truth evaluations diverge significantly.

If your ground truth says 90 percent of responses are excellent but only 60 percent of users give positive feedback, you have a divergence that needs investigation.

If your ground truth says responses are getting worse over time but user satisfaction is increasing, you have a divergence.

These divergences are your early warning system that ground truth has drifted from reality.

## Balancing Expert Judgment and User Feedback

You'll often face tension between what experts say is good and what users demonstrate they want.

Experts say responses should be comprehensive and technically precise. Users want quick, practical answers.

Experts say responses should be conservative and carefully hedged. Users want confident, actionable guidance.

Who's right? Often both, for different contexts.

Your ground truth needs to capture this. It might specify: "For users identified as domain experts, optimize for technical precision. For users identified as beginners, optimize for practical clarity."

Use user feedback to understand what different user segments value. Use expert judgment to ensure you're delivering value safely and correctly.

The best ground truth is informed by both.

## The Continuous Calibration Model

Think of production feedback as continuous ground truth calibration. Your initial ground truth is your best hypothesis. Production feedback is the data that helps you refine that hypothesis.

Every week, every month, you're incorporating new signals, adjusting your ground truth to better reflect what quality actually means for your users in your context.

This is how ground truth stays alive and relevant instead of becoming a static artifact that drifts from reality.

In the next section, we'll zoom out and talk about how to scale ground truth across multiple teams, each with their own domains and needs but all operating within a shared quality framework.
