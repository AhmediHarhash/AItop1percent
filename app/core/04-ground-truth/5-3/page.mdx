# 5.3 — The Alignment Workshop (Getting Everyone in a Room)

I'm going to tell you about a meeting that saved a company six months of wasted effort and probably prevented them from shipping a product that would have failed.

The company built an AI legal research assistant for small law firms. After eight months of development, they were ready to launch a beta. The week before the announcement, the founder invited me to observe a demo with their pilot customers.

The demo was a disaster. Not because the AI didn't work — technically, it was impressive. But because what the engineers built and what lawyers actually needed were completely different things.

Engineers had optimized for comprehensiveness. The AI returned every potentially relevant case, statute, and regulation. Lawyers needed prioritization — they wanted the three most relevant cases, not thirty.

Engineers had optimized for accuracy of citations. The AI always provided perfect legal citations. Lawyers needed context — why is this case relevant to my specific situation?

Engineers had optimized for speed. The AI returned results in under two seconds. Lawyers needed confidence — they wanted to understand the reasoning, even if it took ten seconds.

Nobody was wrong. The engineers built what they thought lawyers needed. The lawyers expected what they actually needed. Nobody had checked if those two things were the same.

After that painful demo, we ran an alignment workshop. Three hours with engineers, product managers, the founder, and five lawyers from the pilot program. We looked at twenty example queries and AI responses together.

It was uncomfortable. Engineers had to hear that months of optimization work had been focused on the wrong things. Lawyers had to articulate needs they'd assumed were obvious. The founder had to acknowledge that internal assumptions about users were way off.

But by the end of those three hours, everyone understood what "good" actually meant for this product. They threw out the beta, redesigned the core experience, and launched three months later to rave reviews.

That's the power of the alignment workshop. It's the single most valuable meeting you'll ever run.

## Why Workshops, Why Not Just Documentation?

You might be thinking: "Why do we need a meeting? Why can't we just write down the quality standards and share them?"

Because written standards without shared context are interpreted differently by everyone who reads them.

Let me show you what I mean. Here's a quality standard you might write: "Responses should be concise but complete."

What does that mean? Concise to a software engineer might mean "no unnecessary words, get to the technical answer." Concise to a designer might mean "short enough that users can scan it quickly." Concise to a subject matter expert might mean "no extraneous details, but all the important caveats."

Complete to an engineer might mean "includes all the facts." Complete to a product manager might mean "answers the underlying question, not just the literal question." Complete to legal might mean "includes all necessary disclaimers."

One sentence, wildly different interpretations.

The alignment workshop solves this by creating shared understanding through shared experience. Instead of telling people what concise and complete means, you show them examples. You rate them together. You discuss why someone rated an example as too long while someone else rated it as appropriately thorough.

That conversation — the specific, concrete discussion of actual examples — creates understanding that documentation alone cannot.

## The Workshop Structure (Three Hours That Will Change Everything)

Here's the format that works. I've run this workshop dozens of times across different companies, different domains, different product types. The structure adapts but the core is the same.

Duration: three hours. Two is too short to get through the necessary examples and discussion. Four is too long — people lose focus.

Attendees: you need diversity and you need decision-makers.

Diversity means: engineering, product, design, legal/compliance, domain experts, customer-facing teams (support, sales, success), and if possible, actual users or user representatives.

Decision-makers means: people who have authority to commit to the standards you're setting. If your head of legal can't attend but sends a junior associate, that associate will have to "check with my boss" on hard decisions, and you'll lose momentum.

Aim for eight to twelve people. Fewer and you miss important perspectives. More and the discussion becomes unwieldy.

Location: in-person is ideal. Remote can work but is harder — you need really good facilitation and you need to make sure everyone has their cameras on and is actively engaged.

Structure: five phases over three hours.

## Phase One: Framing (Fifteen Minutes)

Start by explaining why you're here and what you're trying to accomplish.

"We're building an AI product that needs to meet multiple quality standards — technical accuracy, user helpfulness, safety, compliance, business goals. Right now, those standards are mostly implicit and we're discovering misalignment when it's expensive to fix. This workshop is about making those standards explicit and shared."

Then explain the ground rules:

One: there are no wrong perspectives. If you think a response is good and someone else thinks it's bad, that's not a conflict to resolve immediately — it's data about implicit standards we need to surface.

Two: focus on examples, not abstractions. We're not debating what "quality" means philosophically. We're looking at specific AI outputs and discussing specific judgments.

Three: document the reasoning, not just the conclusion. It's not enough to agree that an example is bad — we need to capture why it's bad and what would make it better.

Four: parking lot for scope expansion. You'll inevitably discover issues that are important but outside this workshop's scope. Capture them, commit to addressing them, but don't derail the workshop.

This framing sets expectations and gives people permission to disagree productively.

## Phase Two: Independent Rating (Forty-Five Minutes)

Before the workshop, you've prepared twenty to thirty examples. These are real or realistic AI outputs from your product. Not cherry-picked successes or obvious failures — a representative mix including edge cases and borderline calls.

For each example, participants independently rate it on key dimensions. The dimensions depend on your product, but typically include:

- Accuracy: is the information factually correct?
- Helpfulness: does it actually help the user accomplish their goal?
- Safety: does it avoid potential harm or risk?
- Completeness: does it include everything necessary?
- Clarity: is it easy to understand?
- Tone: is the communication style appropriate?
- Compliance: does it meet legal and regulatory requirements?

Use a simple scale. Pass/fail works. A three-point scale (good, borderline, poor) works. Don't overthink the scale — you're looking for divergence in judgment, not precise calibration.

Critically: this phase is independent and silent. No discussion yet. Everyone rates the examples using their own judgment.

Why independent? Because you want to see the unfiltered differences in how people evaluate quality. If people discuss before rating, you get groupthink and you miss the divergences.

Use a tool to collect ratings in real-time so you can see the results immediately. A shared spreadsheet works. Dedicated workshop tools work. Even paper forms that you quickly tally work.

Forty-five minutes feels long, but people need time to really look at each example and think through their ratings.

## Phase Three: Divergence Discussion (Ninety Minutes)

This is the heart of the workshop. You're going to discuss the examples where people disagreed, because disagreements reveal implicit standards.

Don't discuss the examples where everyone agreed. If everyone rated something as good or everyone rated it as bad, you have alignment — move on.

Focus on the divergent examples. Sort by disagreement — which examples had the most split ratings? Start with those.

For each divergent example, follow this pattern:

One: show the ratings. "Five people rated this as good, six people rated it as poor. Let's understand why."

Two: ask someone who rated it positively to explain their reasoning. "Sarah, you rated this as good — walk us through your thinking."

Three: ask someone who rated it negatively to explain their reasoning. "James, you rated this as poor — what made you rate it that way?"

Four: probe for the underlying standard. "It sounds like Sarah is prioritizing directness and James is prioritizing thoroughness. Is that fair?"

Five: look for common ground. "Is there a version of this response that would satisfy both standards? What would that look like?"

Six: document the decision and the reasoning. Don't just capture "this example is bad" — capture "this example is bad because it's too direct and misses important context. Good responses in this category should balance directness with necessary caveats."

This process is where magic happens. You'll see patterns emerge:

"Oh, I keep rating things as too long and you keep rating them as appropriately thorough. I think we have different assumptions about user expertise."

"I keep marking things as failing because they don't cite sources, but nobody else seems to care about citations. Should that be a requirement?"

"I'm comfortable with this level of uncertainty in the response but legal is not. We need to define where the line is."

These realizations are gold. They're the invisible standards becoming visible.

As the facilitator, your job is to keep the conversation productive. Watch for:

People talking past each other. If two people are arguing but seem to be discussing different things, stop and clarify what each person is actually concerned about.

Scope creep. "This response is bad" turning into "we should redesign the entire feature." Acknowledge the broader point, put it in the parking lot, keep the workshop focused.

Dominance. If one person is doing seventy percent of the talking, actively invite quieter participants to share their perspective.

Abstraction. If people start debating philosophical principles instead of discussing the specific example in front of them, redirect: "That's an interesting principle, but let's ground it. Looking at this specific response, does that principle lead you to rate it as good or bad?"

Ninety minutes seems like a lot for this phase, but you're usually discussing ten to fifteen examples in depth. That's about six minutes per example, which is the right pace for thoughtful discussion.

## Phase Four: Standard Documentation (Twenty Minutes)

By this point, you've discussed a lot of examples and surfaced a lot of implicit standards. Now you need to capture them in a usable format.

You're not writing a comprehensive quality manual in this twenty minutes. You're capturing the key themes and decisions that emerged.

Use a simple framework:

Dimensions: What are the key quality dimensions for this product? (accuracy, helpfulness, safety, tone, completeness, etc.)

For each dimension, document:

- What good looks like (with example)
- What bad looks like (with example)
- Borderline cases and how to handle them
- Who has decision authority when there's ambiguity

Hierarchies: When dimensions conflict (e.g., helpfulness vs. safety), which wins?

Escalation paths: What happens when someone is unsure? Who do they ask?

Open questions: What did you discover you don't have answers for yet?

This documentation will be refined after the workshop, but capturing the bones of it while everyone's in the room ensures you don't lose the context.

## Phase Five: Next Steps (Ten Minutes)

End the workshop with concrete commitments:

Who will refine the documentation from this workshop? By when?

Who will create the example library showing good, bad, and borderline cases? By when?

How will this documented standard be integrated into actual workflows — eval creation, model testing, product decisions?

When will you revisit these standards? (Quarterly is usually right.)

What open questions need follow-up? Who's responsible for each?

Get verbal commitments in the room. Don't leave this as "we should probably..." — get specific owners and dates.

## The Example Selection (This Makes or Breaks the Workshop)

The quality of your workshop depends enormously on the quality of your examples. Here's how to choose them well.

Represent real diversity. Don't just show twenty variations of the same type of query. Show different use cases, different user intents, different edge cases.

Include borderline cases. The obvious successes and obvious failures aren't interesting. The borderline cases — where reasonable people might disagree — are where you learn.

Include known pain points. If you've had production incidents or user complaints, include examples similar to those issues.

Include the conflicts you expect. If you know engineering and product have different views on response length, include examples of varying lengths.

Mix easy and hard. Start with a few easy examples to build calibration, then move into the hard ones. If everything is hard, people get fatigued.

Use real data when possible. If you have actual user queries and AI responses, use those. They're more realistic and they often surface issues you wouldn't think to create.

Anonymize when necessary. If you're showing real user data, remove any personally identifiable information.

For a three-hour workshop, twenty to thirty examples is the right range. Fewer and you don't see enough patterns. More and you rush through without adequate discussion.

## Common Pitfalls (And How to Avoid Them)

Pitfall one: inviting too many people. More than twelve and the discussion becomes unmanageable. If you have more stakeholders, consider running multiple workshops or having clear speaking roles.

Pitfall two: inviting people without decision authority. If attendees have to "check with my boss" on decisions, you're wasting everyone's time. Get the decision-makers in the room.

Pitfall three: poor facilitation. If the facilitator doesn't actively manage the conversation, you'll get dominated by loud voices, tangents, and unproductive arguments. Invest in a good facilitator — this is not the time for whoever happens to be available.

Pitfall four: trying to resolve everything. You won't reach perfect consensus on everything in three hours. That's okay. The goal is shared understanding and a framework, not perfect agreement on every edge case.

Pitfall five: not documenting in the moment. If you don't capture decisions and reasoning during the workshop, you'll forget the nuance later. Have a dedicated note-taker or scribe.

Pitfall six: no follow-through. The workshop is the beginning, not the end. If you don't refine the documentation, create the example library, and integrate the standards into workflows, the workshop was just an expensive meeting.

## What Success Looks Like

You'll know the workshop was successful if:

One: people leave with shared vocabulary. They're using the same terms to mean the same things.

Two: implicit standards are now explicit. Things that were "obvious" to one team are now documented and understood by all teams.

Three: you have a decision framework. When stakeholders disagree in the future, they have a process for resolving it that's based on the workshop outcomes.

Four: you've identified gaps. You know what you don't know, and you have plans to address it.

Five: people feel heard. Everyone's perspective was represented and considered, even if their perspective didn't always win.

## The Follow-Up Work

The workshop is day one. Here's what happens next.

Within one week: the facilitator refines the documentation, creates a clean summary of decisions and reasoning, and shares it with all participants for feedback.

Within two weeks: the example library is created. This is a structured collection of examples showing good, borderline, and bad responses for each quality dimension, with annotations explaining the judgment.

Within one month: the standards are integrated into actual workflows. Eval creators use them when designing test sets. Engineers use them when making model decisions. Product uses them when defining requirements.

Quarterly: you revisit the standards. Are they still right? Have you learned things that should update them? Do you have new edge cases to discuss?

Annually: you run the workshop again, especially if you've had significant team changes or product evolution.

## Adapting for Remote Teams

If your team is distributed and you can't get everyone in a room, the workshop can still work but requires adjustment.

Use a video conferencing tool where everyone can see each other. Cameras must be on — you need to read body language and maintain engagement.

Use collaborative tools for the rating phase. A shared spreadsheet or specialized workshop tool where everyone can input ratings simultaneously.

Be more deliberate about participation. In person, you can make eye contact and invite someone to speak. Remotely, you need to explicitly call on people to prevent the loud voices from dominating.

Take more breaks. Remote attention spans are shorter. Break the three hours into two ninety-minute sessions with a break in between.

Share the examples in advance. Give people fifteen minutes before the workshop starts to review the examples, so you're not spending time in the workshop just reading.

## The Emotional Labor

Let me be honest about something: alignment workshops are emotionally demanding.

You're asking people to surface disagreements. You're revealing that assumptions people thought were universal are actually individual. You're sometimes discovering that months of work were optimized for the wrong things.

This can be uncomfortable, frustrating, even upsetting. Engineers might feel defensive when their work is critiqued. Product might feel embarrassed that they didn't catch misalignments earlier. Domain experts might feel frustrated that their expertise wasn't consulted sooner.

As a facilitator, you need to hold space for that discomfort while keeping the conversation productive.

Acknowledge the feelings: "I know it's frustrating to realize we've been working from different assumptions. That's exactly why this workshop is valuable — we're catching this before we ship."

Emphasize learning over blame: "We're not here to figure out who was wrong. We're here to align going forward."

Celebrate the insights: "This is a really important realization. I'm glad we surfaced it now."

The emotional labor is real, but the payoff is enormous. Teams that go through this process together come out stronger and more aligned.

## When to Run This Workshop

Ideal timing for the alignment workshop:

Before you start building. If you're at the planning stage, run this workshop to align before you write code. It's way cheaper to align upfront than to realign after building the wrong thing.

When you're seeing repeated conflicts. If your team is having the same arguments over and over, that's a signal that implicit standards need to become explicit.

After a major product pivot. If your product direction changes significantly, old alignment might not apply anymore.

When you're adding new stakeholders. If legal just joined the team, or you just hired domain experts, run the workshop to get everyone on the same page.

Before a major launch. If you're about to ship something big, make sure everyone agrees on what "ready to ship" means.

Don't wait for perfect timing. If you need alignment, run the workshop. The cost of misalignment is almost always higher than the cost of the workshop.

## What's Next

The alignment workshop gets everyone on the same page. But alignment at a point in time isn't enough — you need to navigate ongoing conflicts as they arise. Product will want shorter responses, support will want more detail. Legal will want disclaimers, design will want clean UX.

In the next subchapter, we're diving into conflict resolution frameworks: how to handle the inevitable tensions between stakeholders, who gets to make which decisions, and how to document trade-offs so you're not relitigating them every week.

The workshop creates the foundation. Ongoing conflict resolution builds the structure on top of it. Let's talk about how to make stakeholder disagreements productive instead of destructive.
