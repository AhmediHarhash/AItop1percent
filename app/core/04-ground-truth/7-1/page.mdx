# 7.1 — Ground Truth Drift (Why Standards Change)

Let me tell you about a team I worked with at a financial services company. In January, they launched a customer support bot with what they called "perfect" ground truth. They'd spent three months with their best support agents, documented every scenario, created beautiful rubrics, and validated everything against real customer conversations. Their evaluation scores were stellar. The bot went live, and for the first month, everything was great.

By March, something felt off. Customer satisfaction scores were slipping, but the bot's evaluation metrics looked fine. By June, they had a real problem. The bot was giving technically correct answers according to the ground truth, but users were frustrated. What happened?

The product team had shipped a new investment feature in February. The compliance team had updated their disclosure policies in March. A competitor launched a chatbot that set new expectations for response speed in April. The bot itself — the same model, same prompts — was now "wrong" even though it was doing exactly what the ground truth said to do.

This is ground truth drift. Your standards don't change because someone updates a document. They change because the world changes around them. And if you're not actively maintaining your ground truth, it slowly becomes a historical artifact instead of a living standard.

## The Silent Killer of AI Systems

Ground truth drift is insidious because it doesn't announce itself. Your code doesn't break. Your tests don't fail. Your dashboards stay green. But your AI system is slowly becoming irrelevant, one outdated example at a time.

Here's what makes it dangerous: everyone involved thinks everything is fine. The engineering team sees passing evaluations. The product team sees the features they requested. The compliance team sees the policies they documented. But nobody sees the gap between what the ground truth says is "good" and what users actually need today.

I've seen companies discover drift six months too late, after they've lost customers or failed an audit. The conversation always goes the same way: "But our evals are great! How did we not know?" The answer is always: your evals are measuring against outdated standards.

If you skip maintaining ground truth, here's what happens: you build an optimization engine pointed at yesterday's goals. Your AI gets better and better at something that matters less and less. You spend months improving from 92 percent to 95 percent accuracy, only to realize you've been measuring the wrong thing the whole time.

## The Five Causes of Drift

Let me walk you through the five ways ground truth drifts, so you can watch for them in your own systems.

### Product Evolution

Your product doesn't stand still, and neither should your ground truth. Every new feature, every interface change, every workflow update potentially invalidates some piece of your ground truth.

Think about that financial services bot. When they added the investment feature, dozens of their ground truth examples became incomplete. An example that was marked "excellent response" for explaining account types now needed to mention investment options too. But nobody updated the ground truth, so the bot kept getting rewarded for incomplete answers.

Here's the pattern I see repeatedly: product teams move fast, ground truth teams move slow. By the time ground truth catches up, the product has moved again. You end up in a perpetual state of lag.

The fix isn't to slow down product development. It's to build ground truth updates into the product development process. When someone designs a new feature, part of the work is updating the ground truth to reflect it. Not as an afterthought, but as a core deliverable.

### Policy Changes

Company policies change for all sorts of reasons: legal requirements, competitive pressure, leadership decisions, lessons learned from incidents. Every policy change should trigger a ground truth review, but usually doesn't.

The compliance team updates their documentation. The legal team updates their templates. But the ground truth that the AI is evaluated against? It sits unchanged in a repository somewhere, silently teaching the AI to violate the new policies.

I worked with a healthcare company that updated their privacy policy to be more restrictive about what information could be shared over chat. Great decision for patient safety. But for three weeks, their chatbot kept getting high marks for responses that violated the new policy, because the ground truth still reflected the old rules.

The scary part? Their automated evaluations kept showing everything was fine. It was only when a compliance officer happened to review some live conversations that they caught it.

### User Expectation Shifts

What users consider "good" isn't static. It's shaped by every other AI system they interact with, every app they use, every experience they have.

When ChatGPT launched, it reset expectations for how conversational AI should sound. Systems that sounded professional and helpful in 2022 suddenly felt stiff and robotic in 2023, even though nothing about them had changed. The ground truth that said "formal, structured responses are best" was suddenly outdated.

Or think about response time. Users who would have happily waited five seconds for an answer in 2020 are frustrated by a two-second delay in 2026. Your ground truth might say nothing about response time — it's all about content quality. But users are judging you on both.

This kind of drift is the hardest to detect because it's gradual and subjective. There's no policy document that changed, no feature that shipped. The goalposts just slowly moved, and your ground truth is still aimed at where they used to be.

### Regulatory Updates

If you operate in a regulated industry — and honestly, most AI systems do now — regulatory changes are non-negotiable ground truth updates.

The EU AI Act, which came into full force in 2025, requires specific documentation and behavior for high-risk AI systems. If your ground truth was created before these requirements, it's automatically outdated. An answer that was "compliant" in 2023 might be non-compliant now if it doesn't include required disclosures or explanations.

I've seen teams scramble when a regulation changes. They update their system behavior, they update their documentation, they might even retrain their models. But they forget to update the ground truth, so their evaluation system keeps telling them the old behavior was better.

Then audit time comes, and they discover that their historical evaluation data — which they're required to keep — shows they were optimizing for non-compliant behavior. That's not a good conversation to have with regulators.

### Model Capability Changes

Here's an interesting one: sometimes your ground truth drifts because AI itself gets better, not because the world changes.

Let's say your ground truth defines a "good summary" as hitting the key points in under 100 words, because that was the limit of what models could reliably do when you created it. Now you're using a newer model that can produce nuanced, contextual summaries in 150 words that users clearly prefer. But your ground truth still says 100 words is the target.

You're leaving capability on the table because your ground truth reflects what was possible, not what's desirable.

Or consider a more subtle case: your ground truth for a code generation tool includes examples where the AI asks clarifying questions because early models couldn't handle ambiguity. But newer models can infer context more reliably. Should you still reward asking questions, or should you update your ground truth to expect smart inference?

There's no universal answer. But there is a universal mistake: not asking the question at all.

## How Drift Manifests in Production

Let me show you what drift looks like when it hits production, so you can recognize the early warning signs.

### The Eval-Reality Gap

Your evaluations say 95 percent accuracy. Your users say the system is wrong half the time. Who's right?

Usually, both. The system is accurate according to outdated ground truth, and wrong according to current reality. This gap is the signature of drift.

When you see your evaluation metrics and your user satisfaction metrics diverging, drift should be your first hypothesis. Don't assume the users are wrong or that the metrics are broken. Assume your ground truth might be stale.

### The Regression That Isn't

You ship an improvement. Your users complain it got worse. You check your evals — they're better than ever. What happened?

Often, you improved according to outdated standards and regressed according to current needs. Your ground truth said make responses more formal, but users actually wanted more conversational. Your evals went up; satisfaction went down.

I call this a "false improvement" — you optimized for the wrong thing because your ground truth was drifted.

### The Edge Case Explosion

Early in a system's life, edge cases are rare. You handle the common scenarios well, and the weird stuff is actually weird.

But over time, yesterday's edge cases become today's mainstream. Your ground truth still treats them as exotic exceptions, but your users encounter them daily.

A travel booking bot in 2019 might have considered "book a flight with a 30-hour layover to work remotely from the airport lounge" an edge case. In 2024, it's a normal request. If your ground truth still treats it as weird, you're under-serving a real use case.

### The Compliance Near-Miss

This is the scariest one. You're asked to demonstrate compliance with a new regulation. You pull up your ground truth and evaluation data. And you realize that while your current system might be compliant, your ground truth isn't — and you've been evaluating against non-compliant standards.

Even if you've since fixed the system, you now have a paper trail of evaluations that suggest you were optimizing for the wrong behaviors. In a regulated environment, that's evidence of inadequate controls.

## Detecting Drift Before It Causes Failures

You can't prevent drift — the world changes, and your ground truth should change with it. But you can detect drift early, before it causes production problems.

### The Sample Mismatch Test

Once a week, take ten random production examples. Evaluate them against your current ground truth. Now have a human evaluate them against current reality — what would actually be considered good today?

If those two evaluations agree, your ground truth is probably still accurate. If they disagree more than ten percent of the time, you've got drift.

This is a leading indicator. It catches drift before it shows up in user metrics, before it causes compliance issues, before it damages trust.

### The Stakeholder Spot Check

Once a month, sit down with the people who define "good" for your domain. Product managers, compliance officers, customer support leads, domain experts. Show them examples that your ground truth rates highly.

Ask: "Is this still what good looks like?" Don't explain, don't justify. Just show and ask.

If they hesitate, if they say "well, technically..." if they start suggesting improvements to things your ground truth calls excellent, you've got drift.

### The Policy Delta Review

Every time any policy, regulation, or requirement changes, someone should review your ground truth for impact. This sounds obvious, but it usually doesn't happen because nobody owns the connection.

The policy team updates policies. The ground truth team updates ground truth. Neither knows the other exists. The delta falls through the gap.

Create an explicit trigger: policy change equals ground truth review. Build it into the workflow so it can't be skipped.

### The Competitive Benchmark

At least quarterly, evaluate your ground truth against what competitors are doing and what users are seeing in adjacent domains.

If every other chatbot in your space now cites sources and yours doesn't, and your ground truth doesn't mention citations, that's drift. The standard changed in the market even if it didn't change in your documentation.

### The Model Upgrade Stress Test

Before you upgrade to a new model, evaluate your current ground truth examples with the new model. Look for cases where the new model gives different — especially better — answers than what your ground truth calls correct.

If you find many, you need to update your ground truth before the upgrade, or you'll evaluate the new model against standards it's already exceeded.

## The Drift Tax

Every day your ground truth is drifted, you pay a tax. Your evaluations mislead you. Your improvements go in the wrong direction. Your compliance posture weakens. Your users get a worse experience than you could provide.

The teams that maintain ground truth actively pay a maintenance cost. The teams that let it drift pay a much larger drift tax. You're paying one or the other — you just get to choose which.

## Creating Drift Awareness

The biggest barrier to managing drift isn't technical — it's cultural. Teams treat ground truth as a launch deliverable, not an ongoing asset.

Here's how to shift that mindset. First, make drift visible. Put the age of your ground truth on your dashboards. Highlight examples that haven't been reviewed in six months. Create alerts when ground truth goes stale.

Second, make maintenance a shared responsibility. Product changes equal ground truth updates. Policy changes equal ground truth reviews. User feedback includes ground truth implications.

Third, celebrate maintenance. Teams celebrate launches, but they should celebrate "we updated our ground truth to reflect current reality" just as much. It's equally important work.

## The Drift Response Plan

When you detect drift, how you respond matters as much as detecting it.

Start with impact assessment. How much of your ground truth is affected? Is this a localized issue or a systemic drift? Can you patch it or do you need a full refresh?

Then triage. What needs updating immediately for compliance or safety? What can wait for the next regular review? What needs stakeholder input before you can update?

Update the ground truth, but version it — don't just overwrite. You need to know what changed and when, both for auditing and for understanding the evolution of your standards.

Finally, re-evaluate recent production data under the new ground truth. This tells you how much impact the drift had and validates that your update actually fixed the problem.

## Living With Change

Ground truth drift isn't a failure. It's a natural consequence of building AI systems for a changing world. The failure is in not expecting it and not having a plan to handle it.

The best teams I've worked with treat ground truth as living documentation. It grows with the product, adapts to new requirements, incorporates user feedback, and improves as models improve.

They have regular rhythms for review, clear triggers for updates, version control to track changes, and processes to handle drift when it's detected.

They don't aim for perfect ground truth — they aim for ground truth that's true today and has a plan to stay true tomorrow.

In the next section, we'll walk through the specific review cadence that makes this possible — how often to review, who should be involved, and what to look for at each interval.
