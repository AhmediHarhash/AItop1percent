# 11.3 — The First Responder Toolkit: What On-Call Engineers Need

The alert fired at 11:34 PM: "Eval precision dropped below 85 percent." The on-call engineer opened their laptop, navigated to the monitoring dashboard — or tried to. The dashboard URL was saved in a Slack message from three months ago. After five minutes of searching, they found it. The dashboard showed precision at 78 percent, down from 91 percent baseline. What now? The runbook said "check recent model deployments." Where were deployment logs? Another five-minute search. The logs showed no recent deployments. The runbook said "spot-check outputs for quality issues." Where was the tool to sample outputs? The engineer did not have access. They escalated to the team lead, who sent them a link to an internal admin tool they did not know existed. By the time the engineer had the right tools, 40 minutes had passed. The root cause turned out to be a retrieval pipeline cache failure — a 30-second fix if the engineer had known where to look.

On-call engineers need a specific toolkit to respond effectively. Not "figure out where things are during the incident" — a pre-assembled, documented, accessible set of tools, dashboards, scripts, and runbooks that let them diagnose and mitigate incidents without hunting for links or asking for permissions. The first responder toolkit for AI incidents is different from traditional software incident toolkits. It must cover model behavior, eval metrics, retrieval pipelines, provider status, and content policy — not just infrastructure and deployment.

## Essential Access and Permissions

The most common reason incident response takes too long is that the on-call engineer does not have the access they need. Fix this before the first incident, not during it.

**Production read access.** On-call engineers need read access to production logs, production databases, and production configuration. They need to see what the system is doing, what users are experiencing, and what changed recently. Read access should be default for anyone in the on-call rotation.

**Production write access, scoped.** On-call engineers need limited write access — ability to toggle feature flags, adjust rate limits, restart services, trigger rollbacks. They should not have unrestricted production write access — that creates security risk — but they should have the specific permissions needed to execute common mitigations. Define the scope carefully: "can toggle feature flags in the AI service, can trigger model rollback, can adjust provider API rate limits." Document what they can and cannot do.

**Monitoring platform access.** On-call engineers need access to all monitoring dashboards — infrastructure metrics, model behavior metrics, eval metrics, cost dashboards, content policy dashboards. They should not have to request access to a dashboard during an incident. Set up a monitoring landing page that links to all dashboards and give the entire on-call rotation access.

**Incident management platform access.** On-call engineers need admin-level access to your incident management platform — PagerDuty, Opsgenie, VictorOps, or similar. They need to acknowledge alerts, create incidents, escalate, and update incident timelines. This access should be configured before they join the rotation.

**Provider console access.** On-call engineers need access to your model provider consoles — OpenAI dashboard, Anthropic console, Google Vertex AI, AWS Bedrock. They need to see provider status, check API quotas, review usage metrics, and open support tickets. Some providers require separate account setup. Handle this during onboarding, not during an incident.

**Communication platform access.** On-call engineers need access to incident communication channels — a dedicated Slack or Teams channel where incidents are coordinated. They need to be able to mention key stakeholders — engineering leads, product leads, legal, finance — and escalate quickly. Create the incident channel in advance and document who should be in it.

**Admin tool access.** On-call engineers need access to internal admin tools — tools for sampling outputs, inspecting retrieval results, reviewing user queries, checking eval results. These tools are often built internally and access is often restricted. Grant access to the entire on-call rotation. If there are privacy or security concerns, build role-based access controls rather than blocking access entirely.

**Credential management.** On-call engineers need access to shared credentials — API keys, service account credentials, provider support escalation emails. Store these in a password manager or secrets management system and grant access to the on-call rotation. Do not make engineers hunt for credentials during an incident.

## Monitoring and Observability Dashboards

Your on-call engineer needs a set of dashboards that show the health of your AI system at a glance. These are not just infrastructure dashboards — they are AI-specific dashboards.

**System health overview.** A single dashboard that shows high-level system health — model availability, request volume, average latency, error rate, cost per hour, eval metric trends. This is the first dashboard the on-call engineer opens when an alert fires. It should answer "is the system healthy?" in five seconds.

**Model behavior dashboard.** A dashboard that shows model-specific metrics — precision, recall, F1, content policy violation rate, hallucination rate, refusal rate, average output length. These metrics should be tracked over time so the on-call engineer can see trends and detect drift.

**Retrieval pipeline dashboard.** A dashboard that shows retrieval-specific metrics — retrieval latency, cache hit rate, number of documents retrieved per query, retrieval precision at k. Retrieval failures are a common cause of AI incidents. This dashboard makes them visible.

**Provider status dashboard.** A dashboard that shows provider API health — API availability, API latency, rate limit usage, quota usage. If your provider is having an outage, this dashboard should show it immediately. Some providers offer status pages — embed those into your dashboard.

**Cost dashboard.** A dashboard that shows real-time cost metrics — cost per hour, cost per request, cost per day, cost trend over the last week. Cost spikes are often the first signal of a deeper issue — a runaway retry loop, a bad deployment, an attack. This dashboard makes cost visible so the on-call engineer can catch spikes early.

**Content policy dashboard.** A dashboard that shows content policy metrics — policy violation rate by category, user reports of policy violations, automated content filter triggers. If your system generates user-facing content, this dashboard is critical. Policy violations create legal and reputational risk. Detect them early.

**User impact dashboard.** A dashboard that shows user-facing metrics — active users, queries per minute, errors reported by users, support ticket volume. This dashboard answers "are users affected?" Alerts often fire before users notice issues. This dashboard tells you whether the issue is internal or external.

## Runbooks and Playbooks

Runbooks and playbooks are the most important tools in the first responder toolkit. They turn incident response from improvisation into procedure.

**Runbook structure.** Each runbook covers one incident type. The runbook starts with detection — what alert fired, what metrics are abnormal. Then triage — is this a provider issue, a deployment issue, a model issue, a data issue? Then diagnostic steps — specific commands to run, specific dashboards to check, specific logs to inspect. Then mitigation options — ordered by priority, with rollback as the last resort. Then verification — how to confirm the fix worked.

**Common runbooks for AI incidents.** Your on-call engineers need runbooks for model degradation, retrieval failures, provider outages, cost spikes, content policy violations, latency increases, and availability drops. Build these runbooks based on past incidents and anticipated failure modes. Do not wait for the incident to happen before writing the runbook.

**Playbooks for complex scenarios.** Playbooks are decision trees for complex incidents that require judgment. "If precision drops below 85 percent, check retrieval cache hit rate. If cache hit rate is normal, check recent model deployments. If no recent deployments, escalate to the model team. If there was a recent deployment, consider rollback." Playbooks guide the on-call engineer through the decision-making process.

**Runbook maintenance.** Runbooks go stale. After every incident, update the runbook. Did the runbook work? Were there missing steps? Were there incorrect assumptions? Were there better mitigation options? Update the runbook immediately while the incident is fresh. Schedule quarterly runbook reviews to catch anything that was missed.

**Runbook location.** Runbooks should live in your incident management platform or in a dedicated wiki. Do not store runbooks in Google Docs, personal notes, or Slack threads. The on-call engineer should be able to find the right runbook in ten seconds.

## Communication Tools

Incident response is not just technical — it is coordination. On-call engineers need tools to communicate with stakeholders, coordinate with other teams, and keep everyone informed.

**Incident channel.** Create a dedicated Slack or Teams channel for incident coordination — something like "incidents-ai" or "ai-on-call." This channel should have clear membership — the on-call rotation, engineering leads, product leads, and escalation contacts. During an incident, this channel becomes the war room. All incident-related communication happens here, not in DMs or side channels.

**Incident status updates.** On-call engineers need a way to broadcast status updates. Some teams use Slack. Some teams use their incident management platform. Some teams use both. The key is that stakeholders can get updates without interrupting the on-call engineer. Set a cadence — updates every 30 minutes for Severity 1, every hour for Severity 2 — and stick to it.

**Escalation contacts.** On-call engineers need a clear list of escalation contacts — who to page for engineering help, who to notify for product decisions, who to escalate to for legal issues, who to contact for provider support. This list should be documented in the incident channel topic or pinned message. Do not make the on-call engineer hunt for contact information during an incident.

**Customer support coordination.** On-call engineers need a way to coordinate with customer support. If users are reporting issues, support needs to know what is happening and what to tell users. If the on-call engineer needs more information about user impact, support can provide it. Set up a process for support to notify engineering of user-reported issues and for engineering to notify support of ongoing incidents.

**External communication tools.** For Severity 1 incidents that affect users, on-call engineers may need to help draft external communication — status page updates, user notifications, public statements. They do not write this communication alone — PR and legal are involved — but they provide the technical facts. Have templates ready for common incident types so drafting is faster.

## Diagnostic Tools for AI Systems

Traditional diagnostic tools — log viewers, metric dashboards, deployment histories — are not enough for AI systems. On-call engineers need AI-specific diagnostic tools.

**Output sampling tool.** A tool that lets the on-call engineer sample recent model outputs — pull 10 or 50 or 100 recent queries and their outputs, inspect them for quality issues, and compare them to baseline examples. This tool is critical for diagnosing model behavior drift. It should filter by time, by user, by query type, and by eval metric score.

**Eval runner tool.** A tool that lets the on-call engineer run spot-check evals during an incident. Not a full eval suite — that takes too long — but a small, fast eval that covers the most important quality dimensions. This tool should show results within a few minutes and should highlight outputs that fail the eval.

**Retrieval inspector tool.** A tool that lets the on-call engineer inspect retrieval results for a given query. What documents were retrieved? What were their relevance scores? Was the cache hit? Was the query rewritten? This tool makes retrieval failures visible and debuggable.

**Prompt inspector tool.** A tool that lets the on-call engineer see the full prompt sent to the model for a given user query. What system instructions were included? What retrieval context was included? What few-shot examples were included? Prompt issues often cause model behavior problems. This tool makes the prompt visible.

**Provider API tester.** A tool that lets the on-call engineer send test queries directly to the provider API, bypassing your application layer. This helps isolate whether the issue is in the provider, in your application code, or in your retrieval pipeline. The tool should show the raw request, the raw response, and the latency.

**Cost breakdown tool.** A tool that shows cost by component — model inference cost, retrieval cost, embedding cost. If cost spikes, this tool helps identify where the spike came from. Is it more queries? Longer outputs? More expensive model? Retrieval re-indexing?

## Rollback Capabilities

Rollback is often the fastest way to mitigate an AI incident. On-call engineers need rollback capabilities that actually work.

**Model rollback.** The ability to rollback to a previous model version with a single command or button click. Not "re-deploy the old model from scratch" — that takes too long. Rollback should be instant or near-instant. Your deployment system should keep the previous few model versions available for immediate rollback.

**Prompt rollback.** The ability to rollback to a previous prompt version. If a prompt change caused the issue, rolling back the prompt should fix it. Prompts should be versioned and rollback should be fast.

**Feature flag toggle.** The ability to disable a feature that is causing issues — turn off a retrieval enhancement, turn off a fine-tuned model routing rule, turn off a new eval metric that is causing false positives. Feature flags let you rollback parts of the system without rolling back the entire deployment.

**Configuration rollback.** The ability to rollback configuration changes — rollback a change to temperature, top-p, max tokens, retrieval threshold. Configuration changes can cause behavior drift. Rollback should be as easy as reverting a config file.

**Traffic rollback.** The ability to route traffic away from a problematic component — route traffic away from a fine-tuned model back to the base model, route traffic away from a new retrieval strategy back to the old strategy. This is canary rollback. It lets you isolate the issue without affecting all users.

## Provider Support Contacts

Your on-call engineer needs escalation paths to your model providers. Not generic support emails — direct escalation contacts for production issues.

**Negotiated support contracts.** When you sign a contract with a model provider — OpenAI, Anthropic, Google, AWS, Azure — negotiate direct escalation contacts for production outages. Most enterprise contracts include this. Get the names, email addresses, phone numbers, and Slack or Teams channels of your provider support contacts.

**Provider status pages.** Know where your provider status pages are. OpenAI, Anthropic, Google, and AWS all publish real-time status pages. Embed these into your monitoring dashboard so the on-call engineer can check them without hunting for URLs.

**Support ticket workflow.** Know how to open a support ticket with your provider and what information they need. Most providers need your account ID, your API key, example queries, timestamps, and error messages. Pre-populate a support ticket template so the on-call engineer can fill it in quickly.

**Escalation timelines.** Know how long provider escalation typically takes. If you page your provider contact, do they respond in 15 minutes or four hours? Set expectations with your team so they know whether to wait for provider response or to implement workarounds.

## Incident Management Platform

Your incident management platform — PagerDuty, Opsgenie, VictorOps, or similar — is the central hub for on-call response. Configure it correctly so it actually helps rather than adding noise.

**Alert routing.** Configure alerts to route to the right on-call person based on rotation schedule. Do not route alerts to a shared email alias. Do not rely on someone manually forwarding alerts. Alerts should page the on-call engineer directly via phone, SMS, and push notification.

**Escalation policies.** Configure escalation policies so that if the primary on-call engineer does not acknowledge within five minutes, the alert escalates to the secondary. If the secondary does not acknowledge within five minutes, escalate to the team lead. If the team lead does not acknowledge, escalate to the VP of Engineering. Do not let alerts go unacknowledged.

**Incident timeline.** Use the incident management platform to track incident timelines — when the incident started, when it was acknowledged, when mitigation started, when it was resolved, who was involved, what actions were taken. This timeline becomes the foundation for post-incident review.

**Runbook integration.** Link runbooks directly in the incident management platform. When an alert fires, the on-call engineer should see a link to the relevant runbook in the alert itself. Do not make them hunt for the runbook.

**Status page integration.** If you have a public status page, integrate it with your incident management platform so that status updates are synchronized. When the on-call engineer updates the incident status, the status page updates automatically.

Your first responder toolkit determines whether incidents are resolved in minutes or hours, whether on-call engineers feel empowered or helpless, whether your system recovers quickly or slowly. Build the toolkit before the first incident. Maintain it after every incident. Treat it as critical infrastructure.

Next, we cover cross-functional incident teams — who needs to be involved in AI incidents beyond engineering.
