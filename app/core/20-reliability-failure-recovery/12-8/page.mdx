# 12.8 — Training Data Contamination from User Interactions

User interactions are the richest training signal and the most dangerous contamination source. Every interaction carries signal and noise. The signal improves the model. The noise degrades it. Without separation, you train on both. A customer support AI company learned this in late 2025. They trained a new Claude Sonnet 4.5 model on six months of real user conversations. The model was faster, more conversational, and completely wrong 11 percent of the time. The previous model, trained on curated data, was correct 97 percent of the time. What happened? They trained on everything users said without filtering. That included angry customers testing the limits, confused customers asking nonsense questions, and adversarial users deliberately feeding bad information. The model learned from all of it. Six months of contamination baked into the weights. They rolled back to the previous model and spent four months building filters. The contaminated model cost them 340,000 dollars in wasted Mistral Large 3 training compute and three enterprise customer escalations. The lesson: user data is gold, but unfiltered user data is poison.

## The Contamination Problem

User interactions contain bad examples by design. Users make mistakes. Users test boundaries. Users enter garbage when frustrated. Users intentionally try to break the system. Users misunderstand what the system does. Users copy-paste malformed input. Users ask questions the system cannot answer. Users provide feedback that contradicts ground truth. Every single interaction might be contamination.

The problem is scale. A system handling 100,000 interactions per day generates 3 million interactions per month. If 5 percent are contaminated, that is 150,000 bad examples per month. Train on that data and the model learns 150,000 incorrect patterns. The contamination accumulates. The model drifts toward the noise.

Contamination is not random. It clusters. Certain user types generate more contamination than others. Certain time periods generate more contamination. Certain features attract contamination. Certain edge cases produce contamination. The contamination is non-uniform. It concentrates in specific areas of the input space. The model overfits to those areas. It becomes exceptionally good at learning the wrong thing.

A translation service running Gemini 3 Deep Think discovered this in December 2025. They analyzed their training data and found that 8 percent of interactions came from a single user who was testing the system by submitting deliberately absurd translations. The user was a red-teamer hired by Security, but Engineering didn't know. The model trained on 240,000 adversarial examples that taught it to translate phrases like "the sky is falling" as "the ceiling requires inspection." Quality dropped 6 percentage points over three months. They filtered out the red-team user's data, retrained, and recovered. But the contamination had already spread — other users had seen the bad translations, assumed they were correct, and provided positive feedback on similar patterns. Cleaning the obvious contamination wasn't enough. They had to trace the second-order effects.

## Types of Contamination

**Noise contamination** comes from user errors and misunderstandings. A user asks "what is the capital of France" and your system answers "Paris" but the user clicks thumbs-down because they wanted the population, not the capital. The negative feedback is contamination. A user enters "123 Main St" into a name field. The system stores it. Later, that interaction trains a model to expect addresses in name fields. The pattern spreads.

**Adversarial contamination** comes from users testing limits or deliberately breaking the system. Red-teamers probe for weaknesses. Malicious users inject prompt injections. Curious users try absurd inputs to see what happens. The system logs every attempt. If you train on logs without filtering adversarial interactions, the model learns adversarial behavior as normal. It starts treating boundary cases as central examples. A content generation system trained on unfiltered logs learned to respond to prompt injections because those interactions appeared 40,000 times in training data. The model thought prompt injection syntax was part of normal user language.

**Edge case contamination** comes from rare, unusual, or malformed interactions. A user enters a 10,000-character message because they pasted a document by accident. A user asks a question in a language your system does not support. A user triggers a UI bug that sends malformed data to the backend. These interactions are logged. They are rare enough that the model does not generalize from them — instead, it memorizes them. Edge cases become memorized responses that the model retrieves incorrectly in unrelated contexts. A chatbot memorized a user's 8,000-word product complaint and started quoting fragments of it in unrelated conversations. The complaint had been logged as training data. The model treated it as a template.

**Feedback contamination** comes from incorrect or inconsistent user feedback. A user rates an excellent response as poor because they were frustrated about something unrelated. A user thumbs-up a response that is factually wrong but politely worded. A user provides written feedback that contradicts their rating. The feedback signal is noisy. Train on it directly and the model learns to optimize for user mood, not correctness. A financial advice chatbot trained on user ratings learned to give optimistic projections because users rated optimistic answers higher, even when pessimistic answers were more accurate. The model learned that users prefer to hear good news, not true news.

## Implicit vs Explicit Contamination

Explicit contamination is obvious. A user submits a SQL injection attempt. A user pastes binary data into a text field. A user sends 500 requests per second to test rate limiting. These are attacks or errors, clearly labeled by their structure. You can filter them with pattern matching and automated rules.

Implicit contamination is subtle. A user asks a legitimate question but phrases it unclearly. A user provides feedback that reflects personal preference, not correctness. A user accepts an output that is subtly wrong because they didn't read it carefully. These interactions look normal. They pass standard validation. But they corrupt the training signal. Detecting implicit contamination requires understanding semantics, intent, and downstream consequences — much harder than pattern matching.

A hiring assistant powered by GPT-5.1 trained on recruiter feedback. Recruiters rated candidate summaries. Positive ratings were training signal. But recruiters were rating for culture fit, not for accuracy. The model learned to emphasize personality traits over qualifications. Summaries became less informative and more subjective. The contamination was implicit — the feedback was genuine recruiter opinion, but it didn't reflect the system's actual objective, which was factual candidate representation. Fixing it required redefining the feedback mechanism to separate factual accuracy from subjective assessment.

## Data Quality Gates at Ingestion

Contamination prevention starts with gates. Every interaction passes through filters before it becomes training data. The filters catch contamination. The clean data flows through. The contaminated data is flagged, reviewed, or discarded.

The first gate is format validation. Interactions that violate schema constraints are contaminated by definition. Null fields where values are required. String fields containing binary data. Numeric fields containing text. Timestamps in the future. These are errors, not training signal. Filter them immediately. Do not log them as valid interactions. A scheduling assistant running Llama 4 Scout filtered out 30,000 interactions per month that had malformed date fields. Those interactions would have taught the model to accept invalid dates.

The second gate is length filtering. Interactions that are too short or too long are likely contamination. A user message of three characters is probably accidental. A user message of 50,000 characters is probably a paste error or an attack. Define reasonable bounds. Filter outliers. The bounds vary by domain — a chatbot allows longer messages than a form field — but every domain has bounds. Enforce them. One system set bounds at 10 to 5,000 characters. This removed 4 percent of interactions, nearly all of which were malformed.

The third gate is language detection. If your system operates in English and receives input in Mandarin, that interaction is likely contamination. The user is lost, testing, or deliberately probing. Do not train on it. If your system is multilingual, filter interactions in unsupported languages. If you plan to support those languages later, flag them for review instead of training. A global customer service system filtered 8 percent of interactions that were in languages outside their supported set. This prevented the model from learning incorrect patterns from languages it didn't understand.

The fourth gate is adversarial detection. Flag interactions that contain known attack patterns. Prompt injection attempts. SQL injection attempts. Script tags. Encoded payloads. Repeated special characters. Interactions that trigger rate limits. Interactions from banned users. These are not training data. These are attacks. Log them for security review. Never train on them. A code assistant using DeepSeek R1 filtered out 12,000 prompt injection attempts per month. Training on those would have taught the model that injection syntax was legitimate code.

The fifth gate is feedback consistency checking. If a user rates a response 1 star but their written feedback says "perfect, exactly what I needed," the feedback is inconsistent. Flag it. If a user thumbs-down a factually correct response, flag it. If a user thumbs-up a response your system later determined was incorrect, flag it. Inconsistent feedback is contamination. It trains the model to optimize for the wrong signal. A medical information system flagged 3 percent of feedback as inconsistent and sent it to manual review. Half of flagged examples were legitimate edge cases where users had unusual preferences. Half were genuine contamination. Manual review separated them.

## Detecting Contamination in Training Sets

Even with gates, contamination slips through. Detection finds it after the fact. You sample training data and inspect it for patterns that indicate contamination.

Sample randomly across time periods. Contamination clusters. If you only sample recent data, you miss historical contamination. If you only sample specific features, you miss contamination in other features. Sample across the entire dataset. Look for anomalies. One team sampled 1,000 interactions per month across a 12-month dataset. They found that month seven had a contamination spike — a UI bug had logged malformed data for two weeks. The bug was long fixed, but the contaminated data was still in the training set.

Look for duplicate or near-duplicate interactions. If the same malformed input appears 500 times, it is contamination, not signal. Users do not independently make the same unusual mistake 500 times. Someone is testing, or a bug is logging the same error repeatedly. Deduplicate aggressively. One instance of an edge case might be signal. Five hundred instances are contamination. A search engine found 80,000 duplicate queries in their training set — all from a single bot. Removing them improved relevance by 2 percentage points.

Look for interactions with extreme characteristics. Messages that are all capital letters. Messages with no spaces. Messages that are single repeated characters. Messages that contain only punctuation. These are not natural language. These are errors, tests, or attacks. Flag them. Review them. Do not train on them. A chatbot flagged 14,000 messages that consisted only of repeated characters like "aaaaaaa" or "!!!!!!!" — all contamination.

Look for feedback outliers. If 98 percent of users rate a response positively but 2 percent rate it extremely negatively with no explanation, investigate. Either the 2 percent encountered a bug, or they are adversarial, or they misunderstood the task. Either way, their feedback is not representative signal. One system found that negative outlier feedback came disproportionately from users with less than three total interactions — new users who didn't understand the system yet. Filtering feedback from users with fewer than five interactions removed most outliers.

Look for temporal spikes. If contamination volume suddenly increases on a specific date, something changed. A new feature launched. A UI bug appeared. A viral post sent unusual traffic. Investigate the spike. Filter the contaminated period if necessary. A news recommendation system saw a contamination spike on February 14, 2026 — Valentine's Day traffic included users sending romantic messages to the AI instead of requesting news. The messages were off-topic and contaminated training data. They filtered the 48-hour window.

## Recovery from Contaminated Training

You discover contamination after training. The model is already deployed. The weights are already learned. Recovery requires retraining, but smart recovery minimizes cost.

First, quantify the contamination. What percentage of training data is affected? If it is less than 1 percent, the model might be fine. If it is greater than 10 percent, the model is probably degraded. Between 1 and 10 percent is the gray zone — test the model on clean validation data to measure actual impact. A travel planning assistant found 7 percent contamination in training data. They evaluated the model on a held-out set. Performance had dropped 4 percentage points. The contamination had measurable impact. Retraining was necessary.

Second, filter the contaminated data. Remove it from the training set. If contamination is clustered by time, remove the entire time window. If contamination is clustered by feature, remove interactions from that feature. If contamination is clustered by user, remove interactions from those users. Be aggressive. It is better to lose 5 percent of training data than to train on 5 percent contamination. One system filtered an entire month of data after discovering a logging bug had corrupted 40 percent of that month's interactions.

Third, retrain from the last clean checkpoint. Do not retrain from scratch. If you have model checkpoints before contamination was introduced, start from the last clean checkpoint and continue training on clean data only. This is faster than full retraining and preserves the learning that occurred before contamination. A customer support system reverted to a checkpoint from two months prior, filtered contaminated data from the intervening period, and resumed training. Recovery took three weeks instead of three months.

Fourth, validate on holdout data that predates contamination. If the retrained model performs as well as the original model on clean data, recovery succeeded. If performance is worse, contamination spread further than you detected. Investigate deeper. One team found that even after filtering obvious contamination, performance was still 3 percentage points below baseline. Deeper investigation revealed second-order contamination — users had seen bad outputs from the contaminated model and provided feedback that reflected lowered expectations. That feedback had contaminated the training set even after the bad outputs were removed.

Fifth, deploy with monitoring. The retrained model might still have subtle contamination effects. Monitor drift metrics, quality metrics, and user feedback carefully for two weeks post-deployment. If issues appear, roll back and investigate further. One system deployed their recovered model with a 10 percent canary — 10 percent of traffic went to the new model, 90 percent to the old. After one week with no quality regressions, they scaled to 100 percent.

## Prevention Strategies

Prevention is cheaper than recovery. Build contamination prevention into data pipelines from the start.

Implement automated filtering at ingestion. Every interaction passes through validation before it is stored as potential training data. This is not optional. This is infrastructure. If you log first and filter later, contamination accumulates faster than you can clean it. A financial services chatbot built a three-stage filter: format validation, adversarial detection, and length bounds. The filter ran in 12 milliseconds per interaction. It blocked 6 percent of incoming data. Cost: negligible. Value: contamination prevented before it entered the pipeline.

Implement human review for edge cases. Not every interaction — that does not scale. But sample 0.1 percent of interactions daily. Review them for contamination patterns. When you find a new contamination type, add a filter for it. The filters improve over time. One team reviewed 100 interactions per day. Over six months, they identified and filtered 23 distinct contamination patterns they hadn't anticipated. The manual review loop was 30 minutes per day. The contamination it prevented was worth thousands of hours of retraining.

Implement feedback validation. Do not trust user feedback blindly. Cross-check feedback against ground truth when possible. If a user says a factually correct answer is wrong, flag it. If a user says a factually incorrect answer is correct, flag it. Use the flags to identify users who provide consistently unreliable feedback. Downweight or exclude their feedback from training. A medical Q and A system cross-checked user ratings against expert evaluations. When ratings disagreed with expert judgment by more than two points on a five-point scale, the system flagged the rating. Flagged ratings were reviewed manually. 60 percent were legitimate disagreements based on user context. 40 percent were genuine feedback errors. The 40 percent were excluded.

Implement temporal separation. Do not train on data from the last 30 days. The most recent data has the highest contamination rate because it has not been reviewed yet. Wait 30 days. Review it. Filter it. Then train on it. The delay prevents fresh contamination from entering training sets before it is caught. One system maintained a 45-day delay between data collection and training. During that window, they ran automated contamination detection and sampled 0.2 percent for manual review. By the time data entered training, contamination rate was under 0.5 percent.

Implement adversarial simulation. Generate synthetic adversarial inputs. Test your filters against them. If the filters fail, adversarial users will find the same gaps. Close them before deployment. A content moderation system generated 10,000 synthetic adversarial prompts covering known attack patterns. Their filters caught 92 percent. The remaining 8 percent revealed filter gaps. They updated the filters to cover those cases. When real adversarial traffic arrived, filter effectiveness was 99 percent.

## The Contamination-Quality Tradeoff

Aggressive filtering removes contamination but also removes signal. If you filter too aggressively, you lose valuable training data. If you filter too leniently, you train on contamination. The tradeoff is unavoidable.

Measure false positive rate. What percentage of filtered interactions were actually clean? Sample filtered data weekly. Review it. If more than 5 percent of filtered interactions are clean, your filters are too aggressive. Relax them slightly. If less than 1 percent are clean, your filters are well-calibrated. One team found their length filter was blocking 9 percent of legitimate long-form responses. They raised the upper bound from 2,000 to 4,000 characters. False positive rate dropped from 9 percent to 2 percent without increasing false negatives.

Measure false negative rate. What percentage of training data is still contaminated after filtering? Sample training data weekly. Review it. If more than 1 percent is contaminated, your filters are too lenient. Tighten them. One team found 3 percent of training data contained subtle adversarial patterns their filters missed. They added a semantic similarity check that flagged inputs similar to known adversarial examples. False negative rate dropped to 0.7 percent.

The optimal point depends on model sensitivity. Models fine-tuned on small datasets are more sensitive to contamination. Models trained on massive datasets tolerate more noise. Calibrate your filters to your training scale. A small fine-tuning run with 5,000 examples needs near-zero contamination. A large pretraining run with 50 million examples can tolerate 2 percent contamination without significant degradation. Adjust filter aggressiveness accordingly.

User interactions are the richest signal for improving AI systems. They are also the most dangerous contamination source. The difference between signal and noise is filtering. Without filters, you're training on everything users give you — the good, the bad, and the adversarial. With filters, you extract the signal and discard the poison.

---

Next: **12.9 — Policy Misalignment Drift** — when behavior drifts from intended policy even when data is clean.
