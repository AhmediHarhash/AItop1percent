# 10.9 — Token Burn and Cost Explosion During Incidents

The primary provider went down at 2:14pm on a Thursday. The fallback logic kicked in automatically, routing all traffic to the secondary provider. By 2:47pm, the infrastructure lead was staring at a cost dashboard that made no sense. The system was processing the same query volume as normal. But token consumption had increased by 340%. The bill for that single afternoon came to $47,000 — more than the entire previous week. The post-mortem revealed what happened: the fallback provider used a different model that generated longer responses, the retry logic wasn't throttled properly, and the error logging was capturing full model outputs for every failed request. The incident lasted three hours. The cost explosion lasted exactly as long.

AI incidents don't just degrade quality and user experience. They multiply costs in ways that traditional software incidents do not. Token consumption explodes during retries, cascading fallbacks amplify requests, verbose error logging burns through inference budgets, and runaway loops consume credits faster than monitoring can detect them. A reliability failure that costs $5,000 in normal operation can generate $150,000 in costs during a three-hour incident. Understanding where costs explode and implementing cost circuit breakers is as critical as implementing quality circuit breakers.

## Why Incidents Cause Cost Spikes

Traditional software incidents increase costs through overtime, lost revenue, and customer churn. AI incidents add a direct operational cost spike that happens in real time as the incident unfolds. Every retry consumes tokens. Every fallback attempt consumes tokens. Every diagnostic log that captures model output consumes tokens. Every user refreshing the page because the system is slow triggers another request. The infrastructure scales to handle load — exactly as designed — but the per-request cost is higher because the system is operating in degraded mode.

The cost spike comes from four sources: retry amplification, fallback cost multipliers, diagnostic verbosity, and user behavior changes. Retry amplification happens when requests fail and the system automatically retries. Each retry consumes tokens. A request that normally costs 2,000 tokens and succeeds on the first try now costs 10,000 tokens across five retries before timing out. The same user request generates five times the cost.

Fallback cost multipliers happen when the system routes to a more expensive provider, uses a larger model to compensate for lower quality, or increases context window size to maintain functionality. A request that normally costs $0.002 on the primary provider now costs $0.009 on the fallback provider using a different model. The infrastructure continues processing the same request volume. The bill increases by 4.5x.

Diagnostic verbosity happens when error logging captures full prompts, full responses, and full context for every failed request. This is essential for debugging. It is also expensive. A single failed request might log 50,000 tokens worth of diagnostic information. At scale, diagnostic logging during an incident can consume more tokens than the actual user-facing inference workload.

User behavior changes happen when users perceive the system as slow or unresponsive. They refresh pages. They retry requests. They open multiple sessions. They escalate to support, who trigger additional diagnostic queries. A user who normally generates three requests now generates twelve. The infrastructure sees a 4x increase in request volume during the incident. The cost scales accordingly.

## The Retry Storm Problem

Retry logic is essential for reliability. It is also the most common source of cost explosions. A retry storm happens when a large percentage of requests fail, each triggers retries, and the retries also fail. The system enters a state where it is processing mostly retries, all of which fail, generating more retries. Token consumption explodes while useful work drops to near zero.

The mechanics are simple but devastating. A provider experiences a partial outage where 60% of requests fail. Your retry logic attempts each failed request three times with exponential backoff. The system is now processing 1.6 times the normal request volume — the original 40% that succeeded, plus three retry attempts for the 60% that failed. But the provider is still failing 60% of requests. The retries also fail. Now the system has attempted 1.6x requests, all consuming tokens, with only 40% of requests succeeding. The token consumption is 4x normal for 40% of the intended output.

The cost impact is asymmetric. Successful requests generate value and consume tokens. Failed retries consume tokens and generate no value. During a retry storm, the ratio of cost to value delivered increases dramatically. A system normally spending $10,000 per day at a cost-per-successful-request of $0.05 now spends $40,000 per day at a cost-per-successful-request of $0.20. The user experience is degraded. The bill is quadrupled.

The fix is retry throttling based on failure rate. When failure rate exceeds a threshold — typically 30% to 50% — reduce retry attempts. If 60% of requests are failing, attempting three retries per request amplifies costs without improving success rates. Better to attempt one retry, fail fast, and route to a fallback. The system preserves budget for requests more likely to succeed.

Cost-aware retry logic tracks token consumption per request and stops retrying when token spend exceeds a threshold. A request that normally costs 2,000 tokens should not consume 50,000 tokens across twenty-five retry attempts. After three failures and 6,000 tokens consumed, the system stops retrying and returns an error or routes to fallback. This caps cost per request regardless of failure modes.

## Fallback Cost Multipliers

Multi-provider fallback strategies protect against single-provider failures. They also create cost multipliers during incidents. The fallback provider typically has different pricing, different models, different context handling, or different rate limits. When traffic shifts from primary to fallback, cost-per-request changes — almost always upward.

A fintech company in November 2025 operated with a primary provider costing $0.0015 per request and a fallback provider costing $0.008 per request. The pricing difference came from model capabilities and SLA tiers. Under normal operation, 98% of traffic used the primary provider. During a six-hour primary provider outage, 100% of traffic used the fallback provider. The cost for those six hours exceeded the entire previous week. The system operated exactly as designed. The bill multiplied by 5.3x.

The cost multiplier is not a bug. The fallback provider is more expensive because it offers higher reliability, different capabilities, or premium SLA guarantees. But teams often fail to budget for extended fallback operation. They design fallback capacity for short-term provider issues — minutes to hours. When an incident extends beyond expected duration, the cost accumulates beyond budgeted thresholds.

The solution is cost-aware fallback policies. The system tracks cumulative spend on fallback providers during an incident. When cumulative spend exceeds a threshold — perhaps 150% of daily budget — the system implements cost protection measures. It reduces request volume by degrading non-critical features, increases caching to reduce inference calls, or temporarily switches to a cheaper model with acceptance of quality degradation.

Some teams implement tiered fallback strategies where the first fallback is expensive but high-quality, the second fallback is cheaper but lower-quality, and the third fallback is a static response or cached output. This preserves system availability while controlling cost. The system degrades gracefully through cost tiers rather than spending unlimited budget to maintain full functionality.

## Token Waste During Degradation

Systems operating in degraded mode often consume tokens without producing useful output. Requests time out after consuming tokens. Partial responses are generated but discarded due to quality checks. Retry attempts consume tokens on every attempt. Error handling generates verbose diagnostic outputs. All of this consumes budget without delivering value to users.

A healthcare platform in January 2026 experienced a RAG pipeline failure where the retrieval component was returning low-quality documents. The generation component attempted to synthesize responses from poor source material, failed quality checks, logged full prompts and responses for diagnostics, and retried. Each attempt consumed 8,000 to 12,000 tokens. The quality checks correctly prevented bad output from reaching users. But the system burned through 60% of its monthly token budget in a single day processing requests that generated no user value.

The token waste happens because safety mechanisms — retries, quality checks, verbose logging — all operate independently. Each layer does its job correctly. Together they create a cascade where the system spends enormous resources producing output it then discards. This is correct behavior from a safety perspective. It is devastating from a cost perspective.

Cost-aware degradation reduces token consumption when operating in degraded mode. The system detects that quality check failure rates are elevated. It reduces max tokens per response to limit spend on outputs likely to be discarded. It reduces retrieval depth to save tokens if retrieval quality is low. It disables expensive quality checks that are failing most requests anyway. The system spends less per failed request, preserving budget for when the underlying issue is resolved.

Diagnostic verbosity controls prevent logging from consuming more budget than inference. During normal operation, full logging is acceptable — failures are rare. During incidents, failures are common. Logging every failed request with full context burns budget. Cost-aware logging samples failures rather than capturing every one. It logs full context for the first 100 failures, then switches to minimal logging. Diagnostics remain available. Cost stays bounded.

## Cost Monitoring During Incidents

Traditional incident response focuses on availability, latency, and error rates. AI incident response must also focus on cost burn rate. A cost dashboard during incidents tracks cumulative spend, spend rate compared to normal, cost per request, cost per successful request, and projected daily spend if current rate continues. This visibility turns cost from a post-incident surprise into an active incident metric.

A cost burn rate that is 5x normal is a signal as important as error rate or latency. It indicates the system is in a cost-inefficient state. It triggers investigation: are retries amplifying costs, is fallback multiplying costs, is diagnostic logging burning budget, or have users changed behavior? The incident response includes cost containment as a parallel workstream to quality restoration.

Cost monitoring enables cost circuit breakers. The system defines maximum acceptable cost thresholds for different incident severities. A Sev-2 incident might accept up to 3x normal cost burn rate for up to four hours. A Sev-3 incident might accept only 1.5x normal cost for up to two hours. When thresholds are exceeded, cost circuit breakers activate: reduce retry attempts, disable non-critical features, switch to cheaper models, increase caching, or implement request throttling.

This is not about prioritizing cost over reliability. It is about recognizing that unbounded cost burn during an incident can create a second crisis. A three-hour incident that generates $200,000 in costs might force budget cuts that degrade the system for the rest of the quarter. Cost circuit breakers prevent reliability incidents from becoming financial crises.

## Cost Circuit Breakers

A cost circuit breaker is a mechanism that limits spending during incidents without requiring manual intervention. It monitors cost metrics in real time and implements automated cost-reduction measures when thresholds are exceeded. The goal is to preserve system availability while bounding financial impact.

The simplest cost circuit breaker is a spend rate limit. The system tracks tokens consumed per minute. When the rate exceeds a threshold — perhaps 200% of the normal hourly average — the circuit breaker activates. It reduces request acceptance rate, increases caching, or switches to a cheaper model. It does not shut down the system. It degrades functionality to match budget constraints.

More sophisticated cost circuit breakers implement tiered responses. At 150% of normal spend rate, the system disables optional features and increases caching. At 200% of normal spend rate, it switches to cheaper models for non-critical requests. At 300% of normal spend rate, it implements request throttling and rate limiting. At 500% of normal spend rate, it enters emergency mode where only critical functionality remains active. Each tier preserves progressively more budget while degrading progressively more functionality.

Cost circuit breakers require careful tuning. Trip too early and you degrade functionality during transient cost spikes that would have resolved naturally. Trip too late and you accumulate significant unexpected costs before activation. The threshold should be set based on normal cost variance — typically two to three standard deviations above the mean spend rate. This allows for normal traffic spikes while catching genuine cost anomalies.

Post-incident, teams review cost circuit breaker activations. Did they trigger appropriately? Did they prevent significant cost overruns? Did they degrade functionality more than necessary? The circuit breaker thresholds are adjusted based on this review. Over time, the system learns the appropriate balance between cost protection and functionality preservation.

## Post-Incident Cost Analysis

Every incident review includes cost impact analysis. How much did the incident cost in direct operational spending? How much of that cost was useful — processing legitimate user requests — versus wasted on retries, failed requests, and diagnostics? What was the cost multiplier compared to normal operation? What specific mechanisms caused the cost spike?

A post-incident cost analysis for a three-hour outage might show: total incident cost was $73,000, normal cost for that time period would have been $8,000, cost multiplier was 9.1x, breakdown was 40% fallback provider premium, 35% retry amplification, 15% diagnostic logging, and 10% user retry behavior. This breakdown identifies where cost controls would have the most impact.

The analysis drives concrete improvements. If fallback provider premium dominated costs, the team negotiates better pricing, builds cheaper fallback options, or implements faster primary provider recovery. If retry amplification dominated costs, the team implements smarter retry throttling and cost-aware retry limits. If diagnostic logging dominated costs, the team implements sampling-based logging during incidents.

Cost analysis also informs budget planning. If incidents are rare but expensive, the team maintains reserves for incident costs. If incidents are frequent, the team builds more aggressive cost controls into normal operation to create headroom for incidents. The monthly budget reflects both normal operation and expected incident overhead.

## Budgeting for Incident Cost Spikes

AI infrastructure budgets must include reserves for incident cost spikes. A budget based on normal operation will be exceeded during incidents. A team that spends $50,000 per month during normal operation might spend $75,000 during a month with a single significant incident. The budget must accommodate this variance or force uncomfortable mid-incident cost conversations.

The typical approach is to budget 120% to 150% of expected normal costs, with the excess representing incident reserves. A team expecting $50,000 in normal monthly costs budgets $65,000, with $15,000 reserved for incidents. If no incidents occur, the reserve rolls over. If incidents occur, the reserve prevents budget overruns.

More sophisticated approaches implement cost pools where incident costs are tracked separately from operational costs. This creates visibility into incident cost trends without penalizing normal operations. It also prevents the perverse incentive where teams under-respond to incidents to avoid budget impact. The incident cost pool is sized based on historical incident frequency and severity.

Cost visibility during incidents changes incident response behavior. When cost is invisible, teams implement expensive mitigation strategies without considering alternatives. When cost is visible in real time, teams consider whether aggressive retry logic is worth 4x cost multiplier, whether maintaining full functionality on expensive fallback providers is worth the premium, and whether diagnostic verbosity should be reduced after initial debugging.

In February 2026, the best teams treat cost as a first-class incident metric alongside availability, latency, and error rate. They implement cost circuit breakers that activate automatically during cost anomalies. They track cost multipliers in post-incident reviews. They budget for incident cost spikes rather than treating them as surprises. Cost explosions during incidents are not inevitable. They are a failure mode that can be engineered away with proper observability, circuit breakers, and cost-aware incident response.

The next subchapter examines reliability investment prioritization — how to decide where to spend on reliability improvements when you cannot afford to fix everything.

