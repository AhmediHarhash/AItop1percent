# 2.9 — The Compound Incident: Detecting Multi-Component Failures

A customer service AI stopped answering questions. The logs showed retrieval working, generation working, guardrails working. Every component passed its health check. But the system did not work. Forty-five minutes into the incident, an engineer noticed that retrieval was returning documents in Spanish while generation expected English. Both components functioned correctly in isolation. Together they produced garbage. This was a compound incident — multiple components interacting incorrectly even though each component individually succeeded.

Compound incidents are more common in AI systems than single-component failures. Your system is not one model. It is retrieval, generation, tools, guardrails, context management, and integration logic. Each component can succeed on its own while the system fails. Retrieval returns documents that do not match the query intent. Generation produces output that violates guardrails. Tools return data in formats the model cannot parse. None of these are component failures. All of them are system failures.

Detection built for single-component failures misses compound incidents. You monitor retrieval accuracy, generation quality, guardrail precision — all independently. Each metric shows green. The system is red. By the time you realize multiple components are involved, users have experienced degraded service for thirty minutes and your investigation has checked each component individually twice.

## Why Compound Failures Dominate AI Systems

AI systems are loosely coupled pipelines with implicit contracts between components. The retrieval system promises to return relevant documents. The generation system promises to use those documents faithfully. The guardrail system promises to catch policy violations. But these contracts are not enforced. If retrieval returns irrelevant documents, generation does not reject them. If generation produces policy-violating output, guardrails might miss it. If guardrails fire unnecessarily, generation does not adapt.

This loose coupling enables flexibility and modularity. You can swap retrievers without changing generators. You can update guardrails without changing prompts. But it also creates failure modes where components interact incorrectly. A change in one component breaks an implicit assumption in another component. The system degrades even though no component failed its unit tests.

The problem worsens with agent systems that orchestrate multiple tool calls. An agent might call a search tool, a database tool, and a calculation tool in sequence. If the search tool returns results in a format the database tool does not expect, the agent fails to synthesize information correctly. If the calculation tool has precision errors, the agent makes decisions on bad data. Each tool works in isolation. The composition fails. And the failure is not detectable by monitoring each tool separately.

The fundamental issue is that component-level monitoring does not capture system-level properties. You monitor whether each step succeeds. You do not monitor whether the steps compose correctly. You monitor whether each component meets its specification. You do not monitor whether those specifications are compatible. When components interact incorrectly, you have no alert, no diagnostic information, and no clear owner.

## The Multi-Component Architecture Problem

Most AI systems follow a standard architecture: input validation, retrieval or tool calls, context assembly, prompt construction, generation, post-processing, guardrails, output formatting. A production system might have eight to twelve distinct components, each owned by different teams, each with its own health metrics, each tested independently.

When the system fails, the failure might originate in any component or in the interaction between any two components. That is twelve components plus sixty-six pairwise interactions to consider. Your investigation process cannot brute-force check all seventy-eight possibilities. You need detection that narrows the search space immediately.

The first step is dependency mapping. Document the flow of data through your system. Input validation passes data to retrieval. Retrieval passes documents to context assembly. Context assembly passes formatted context to prompt construction. Prompt construction passes the prompt to generation. Generation passes output to guardrails. Guardrails pass filtered output to formatting. This is your dependency graph. Every compound incident involves a failure in this graph.

The second step is contract monitoring. For each edge in the dependency graph, define the contract. Retrieval promises to return documents with specific fields: title, content, metadata. Context assembly promises to check that those fields exist before using them. If retrieval returns documents missing required fields, that is a retrieval failure. If context assembly crashes because fields are missing, that is a context assembly failure. If context assembly does not validate but passes corrupt data to prompt construction, that is a contract violation.

Contract monitoring catches compound incidents early. Instead of waiting for the system to fail, you detect when one component sends data that violates another component's expectations. You alert before the downstream failure occurs. This changes incident response from "the system is broken, find out why" to "component A is sending bad data to component B, fix component A."

## Detection Challenges: Which Component Failed First

During a compound incident, multiple components show degraded metrics. Retrieval relevance is down. Generation quality is down. Task completion is down. Every dashboard is red. The question is: which component failed first? Did retrieval degrade and cause generation to fail, or did generation degrade and cause the overall task to fail despite good retrieval?

The answer determines mitigation. If retrieval failed first, you fix retrieval. If generation failed first, you fix generation. If both failed independently, you have two incidents running in parallel and need to fix both. But during the incident, with limited time and limited information, you must choose where to focus.

The practical solution is timestamp correlation. Log the timestamp of every component's input and output. When the system fails, trace backward through the logs. Find the first component whose output quality degraded. That component is the root cause or the first domino. Then trace forward from that component. Which downstream components received the degraded output? Which of those handled it correctly, and which propagated the failure?

This requires structured logging at component boundaries. Not just "retrieval succeeded" but "retrieval returned 8 documents with average relevance score 0.74." Not just "generation completed" but "generation produced 240 tokens with toxicity score 0.03 and refusal rate 0." The logs must contain quality metrics, not just success indicators. When compound failures occur, you need to reconstruct the quality of data flowing between components.

Timestamp correlation also reveals cascading failures. Component A degrades at 14:32. Component B, which depends on A, degrades at 14:34. Component C, which depends on B, degrades at 14:36. The cascade is visible in the logs if you log quality metrics with timestamps. Without this, all three components appear to fail simultaneously, and you waste time investigating all three instead of fixing the root cause.

## The Cascade Pattern: One Failure Triggering Others

Cascading failures are compound incidents where one component's failure causes downstream components to fail even though those components are functioning correctly. The retrieval system returns empty results because the database is overloaded. The generation system receives no context and produces generic, unhelpful responses. The guardrail system sees low-quality generation and fires more frequently. Task completion drops. User satisfaction drops. Every metric degrades.

The root cause is database overload. But the symptoms appear in retrieval, generation, guardrails, and user experience. If you investigate each symptom independently, you find four problems. If you trace the cascade, you find one problem with four consequences. The mitigation is fix the database, not fix four components.

Cascade detection requires monitoring causal paths. When component A degrades, check whether components that depend on A degrade shortly after. If they do, the failures are likely related. If they do not, the failures are independent. This is not perfect causality inference, but it is good enough for incident response. During an incident, you need a hypothesis in minutes, not hours. Causal path monitoring gives you that.

The most dangerous cascades involve feedback loops. Retrieval degrades, so generation produces poor output. Users retry, increasing load. Increased load further degrades retrieval. This is a positive feedback loop where the failure amplifies itself. Detection must identify feedback loops before they spiral. If retry rate increases simultaneously with retrieval degradation, you have a feedback loop. The mitigation is rate limiting or circuit breakers, not just fixing retrieval.

## Alerting Strategies for Compound Incidents

Traditional alerting fires when a metric crosses a threshold. Compound incident alerting fires when multiple metrics cross thresholds in a pattern. If retrieval relevance drops below 0.6 and generation quality drops below 0.7 and task completion drops below 0.5 within a five-minute window, that is a compound incident. The alert should fire on the pattern, not on each metric individually.

Pattern-based alerting reduces noise. Single-metric alerts fire frequently because individual components have transient issues. Multi-metric pattern alerts fire only when the system is genuinely broken. This improves alert precision and reduces false positives. The tradeoff is complexity. You must define which patterns indicate failures and which patterns are benign.

The most reliable patterns are upstream-downstream pairs. If an upstream component degrades and a downstream component degrades within two minutes, that is a pattern. If task completion drops and no upstream component degrades, that is a different pattern indicating a failure in the final integration layer. If multiple unrelated components degrade simultaneously, that is a pattern indicating shared infrastructure failure — database, network, or provider outage.

Pattern-based alerting requires a monitoring platform that supports multi-metric queries. Your alerting logic cannot live in isolated metric dashboards. It must correlate metrics across components and fire on complex conditions. Most teams build this on top of their observability platform using custom query languages or alerting rules. The investment is worth it. Compound incidents are the majority of high-severity incidents in mature AI systems.

## Root Cause Isolation Techniques

During a compound incident, your first goal is stabilization. Your second goal is root cause isolation. Stabilization means stop the bleeding — circuit breaker, rate limiting, fallback to simpler behavior. Root cause isolation means find the first failure so you can fix it and prevent recurrence.

The fastest isolation technique is binary search through the component graph. Start in the middle of your pipeline. Is the failure upstream or downstream of this point? If upstream, focus on retrieval and context assembly. If downstream, focus on generation and post-processing. Then repeat. Within three or four queries, you have narrowed the root cause to one or two components.

This requires instrumentation at each component boundary. You must be able to query the input and output quality of each component independently. If you can only measure end-to-end system quality, binary search does not work. You need per-component metrics that update in real-time and are queryable during incidents.

Another technique is differential diagnosis. Compare behavior on synthetic test cases to behavior on live traffic. If the system works on synthetic cases but fails on live traffic, the failure is input-dependent — something about real user queries triggers the failure. If the system fails on both, the failure is systemic — a component is broken regardless of input. Synthetic test cases give you ground truth about component health independent of traffic variation.

Differential diagnosis requires maintaining a suite of test cases that exercise each component and each component interaction. Not just unit tests that run in CI. Synthetic monitoring that runs continuously in production, sending known-good inputs through the full pipeline and verifying known-good outputs. When the system fails, you immediately check whether synthetic monitoring also failed. If it did not, the failure is specific to organic traffic patterns.

## Organizational Ownership of Compound Incidents

Compound incidents are harder organizationally than single-component incidents. Single-component incidents have clear owners. The team that owns the broken component owns the incident. Compound incidents involve multiple teams. Who owns an incident where retrieval and generation both degrade, but neither team's component technically failed its specification?

The answer is that system-level reliability needs system-level ownership. This is usually a platform team, an SRE team, or an AI infrastructure team. They do not own individual components, but they own the integration between components and the health of the overall system. When a compound incident occurs, this team coordinates the response even if the root cause is in another team's component.

This requires authority and process. The system reliability team must have authority to pull in component teams, request diagnostic information, and drive the investigation. They must have runbooks for compound incidents that specify how to isolate root cause across components. They must have postmortem processes that identify gaps in component contracts and monitoring.

Without this ownership, compound incidents devolve into cross-team finger-pointing. Each team insists their component is fine. No one investigates the interaction between components. The incident drags on for hours because no one has end-to-end visibility or authority. System-level ownership prevents this. One team is always responsible for the system working, even when the failure is not in any single component.

## The Instrumentation Investment

Detecting compound incidents requires more instrumentation than detecting single-component failures. You need metrics at every component boundary. You need structured logs with quality signals, not just success indicators. You need dependency graphs that map data flow. You need alerting rules that fire on multi-metric patterns. You need dashboards that show end-to-end traces, not just isolated components.

This is a significant investment. But compound incidents are expensive. A single compound incident that takes ninety minutes to diagnose and forty minutes to fix costs more in lost user trust, lost revenue, and lost engineering time than building comprehensive instrumentation. The instrumentation pays for itself after two or three incidents.

The best time to build this instrumentation is before you need it. After the incident, when you are writing the postmortem and wondering why you could not see which component failed first, it is too late. Build component boundary monitoring as you build the components. Build pattern-based alerting as you scale from prototype to production. Build root cause isolation tools as you hire your second on-call engineer.

Your system is not one model. It is a pipeline. Pipelines fail at joints, not just at individual components. Detect the joints. Monitor the joints. Alert on joint failures. That is how you catch compound incidents before they cost you hours of downtime and thousands of confused users.

---

Next, we examine detection SLOs — defining and measuring how fast you must detect failures, and treating detection latency as a first-class reliability metric.
