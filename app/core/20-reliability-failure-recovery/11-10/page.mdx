# 11.10 — The Reliability Culture: Making Reliability Everyone's Job

Reliability cannot be the responsibility of a single team. The moment you assign reliability to one group — SRE, DevOps, the on-call rotation, the infrastructure team — everyone else stops thinking about it. Developers write code that assumes perfect uptime. Product managers prioritize features that increase system complexity without considering failure modes. Data scientists deploy models without understanding how they degrade under distribution shift. The reliability team spends all their time firefighting incidents caused by decisions made by people who were not thinking about reliability when they made those decisions.

A mature AI organization does not have a reliability team. It has reliability practices embedded in every team. Developers write code that degrades gracefully. Product managers evaluate features based on reliability risk. Data scientists design models with failure modes in mind. Reliability is not someone else's job — it is everyone's job. This is reliability culture.

Building reliability culture is harder than building reliable systems. You can fix a system with better architecture, better monitoring, and better runbooks. You cannot fix culture with architecture. Culture requires leadership commitment, organizational design, incentive alignment, and sustained reinforcement. Most organizations never build it. They settle for a reliability team that cleans up everyone else's messes until the team burns out and the best people leave.

## What Reliability Culture Actually Means

Reliability culture is not about caring more or trying harder. It is about making reliability a first-class constraint in every decision. When an engineer proposes a new feature, the default question is not "how fast can we ship this?" The default question is "what happens when this fails?" When a data scientist trains a new model, the default question is not "what is the accuracy?" The default question is "how does this model degrade when input distribution shifts?" When a product manager prioritizes the roadmap, the default question is not "what creates the most user value?" The default question is "what creates the most user value without increasing our failure surface?"

These questions do not slow teams down. They prevent teams from building things that will fail expensively in production. A feature that takes three weeks to build and causes six incidents in the first month is slower than a feature that takes four weeks to build and causes zero incidents. A model that achieves 94 percent accuracy but requires constant manual intervention is less valuable than a model that achieves 91 percent accuracy and runs reliably for six months without intervention. Reliability culture does not trade speed for safety — it recognizes that unreliable systems are slow systems once you account for the cost of incidents, rollbacks, and firefighting.

Reliability culture also means that reliability work is valued the same way feature work is valued. An engineer who spends a quarter improving monitoring, refactoring brittle code, and automating incident response should be rewarded the same way an engineer who ships three new features is rewarded. A team that spends a sprint hardening their system against cascading failures should be celebrated the same way a team that launches a new product is celebrated. Most organizations say they value reliability. Reliability culture means proving it with promotions, recognition, and resource allocation.

Finally, reliability culture means treating incidents as learning opportunities, not blame opportunities. When a model fails in production, the question is not "who caused this?" The question is "what systemic conditions allowed this to happen, and how do we change those conditions?" A culture that punishes individuals for incidents creates fear. Fear creates hiding. Engineers stop reporting near-misses, stop escalating ambiguous situations, and stop experimenting with new approaches because every failure could become a career-limiting event. A culture that treats incidents as system failures creates transparency. Transparency creates learning. Learning creates improvement.

Reliability culture is what happens when an organization stops treating reliability as a specialized function and starts treating it as a shared responsibility that shapes every decision.

## Leadership's Role in Reliability Culture

Reliability culture starts at the top. If leadership does not prioritize reliability, no amount of grassroots effort will change the organization. Developers will continue to ship fast and break things because that is what gets rewarded. Product managers will continue to stack features on fragile foundations because that is what executives celebrate. The reliability team will continue to fight losing battles because they have no organizational leverage.

Leadership sets culture through three mechanisms: what they measure, what they reward, and what they say when things break.

**What leadership measures** determines what teams optimize for. If the exec dashboard shows only feature velocity, revenue, and user growth, teams will optimize for those metrics and ignore reliability. If the exec dashboard also shows incident frequency, mean time to recovery, and system availability, teams will optimize for those metrics too. The act of measuring reliability at the executive level signals that reliability is a first-class concern. It creates accountability. It makes reliability visible.

Effective reliability metrics at the exec level are simple and outcome-focused. How many critical incidents occurred this quarter? What was the total user-facing downtime? What percentage of deploys caused incidents? How many incidents were caused by known issues that were not prioritized for remediation? These metrics do not require deep technical knowledge to interpret. They show whether the organization is getting more reliable or less reliable over time. If the trend is deteriorating, leadership can ask why and reallocate resources accordingly.

**What leadership rewards** determines what behavior scales. If the only engineers who get promoted are the ones who ship features, reliability work will be deprioritized. If engineers who improve system resilience, reduce incident frequency, and mentor others on reliability practices also get promoted, reliability work becomes attractive. Promotion criteria should explicitly include reliability contributions. An engineer who refactored a critical service to eliminate a class of cascading failures is doing work that is as valuable — often more valuable — than an engineer who built a new feature. If the promotion committee does not recognize this, the organization does not have reliability culture.

Recognition matters beyond promotion. When an engineer stays up all night to mitigate an incident, leadership should acknowledge it publicly. When a team completes a quarter-long project to improve observability, leadership should celebrate it the same way they celebrate a product launch. What gets celebrated gets repeated. If only feature launches get celebrated, only features will be built.

**What leadership says when things break** determines whether the organization learns from incidents or hides from them. If the CEO's first question after an incident is "who is responsible?" the organization learns that incidents are career-threatening. Engineers will minimize incident severity in reports, delay escalation to avoid visibility, and avoid taking ownership of risky projects. If the CEO's first question is "what did we learn and how do we prevent this category of failure?" the organization learns that incidents are opportunities for improvement. Engineers will report incidents transparently, escalate early, and take ownership of complex projects because failure is treated as a system problem, not a personal problem.

Leadership does not need to understand the technical details of every incident. Leadership needs to model the right response to incidents: curiosity instead of blame, systemic thinking instead of individual accountability, and investment in prevention instead of punishment for failure.

## Developer Ownership of Reliability

The developers who build the system are the people best positioned to make it reliable. They understand the code. They understand the failure modes. They understand the trade-offs. But in most organizations, developers build features and then hand them off to SRE or operations to keep them running. This handoff is where reliability dies.

Developer ownership means the team that builds a system is also responsible for running it. They carry the pager. They respond to incidents. They see the consequences of their architectural decisions at 3am when the service falls over because they did not handle a rare edge case. This creates a feedback loop. Developers who experience the pain of operating unreliable systems become motivated to build more reliable systems. Developers who never experience that pain have no incentive to change their behavior.

This is the "you build it, you run it" principle, and it is foundational to reliability culture. It does not mean that every developer is on-call 24/7. It means that the team responsible for a service is also responsible for its operational health. They own the monitoring, the runbooks, the incident response, and the long-term reliability improvements. They can call on centralized SRE teams for tooling, best practices, and escalation support — but they cannot delegate ownership.

Developer ownership also means developers are given time to do reliability work. If your sprint planning allocates 100 percent of capacity to feature development, reliability work will never happen. A sustainable ratio is 70 to 80 percent feature work, 20 to 30 percent reliability work. Reliability work includes improving monitoring, writing runbooks, refactoring brittle code, automating incident response, and addressing tech debt that creates operational burden. This ratio is not optional. It is the cost of maintaining a system that runs reliably over years.

Teams that try to run at 100 percent feature velocity accumulate reliability debt. The system becomes more fragile with every deploy. Incident frequency rises. Mean time to recovery increases. Eventually the team spends so much time firefighting that feature velocity drops to zero. The team that invests 20 percent of their capacity in reliability work every quarter ships faster over a multi-year horizon because they are not constantly interrupted by incidents.

Developer ownership requires training. Most developers have never carried a pager, never written a runbook, never coordinated a cross-functional incident. They need to be taught how to do these things. SRE teams can provide that training through shadowing programs, incident response workshops, and embedded coaching. The goal is not to turn every developer into an SRE. The goal is to give every developer the basic skills they need to operate the systems they build.

## Reliability in Hiring and Onboarding

Culture is transmitted through hiring and onboarding. If you hire people who do not value reliability, and you onboard them into an organization that does not teach reliability, your culture will erode no matter what your leadership says.

Hiring for reliability culture means evaluating candidates on their attitude toward failure. In interviews, ask candidates to describe a system they built that failed in production. How did they respond? Did they blame external factors, or did they take ownership? Did they fix the immediate issue, or did they also address the systemic conditions that allowed the failure? Did they document what they learned, or did they move on quickly? The answers reveal whether the candidate sees reliability as someone else's job or as their own responsibility.

Also evaluate candidates on their approach to trade-offs. Give them a scenario: you have to choose between shipping a feature on time or delaying to add better error handling and monitoring. What do they choose, and why? There is no single right answer — the right answer depends on context — but the reasoning reveals their mental model. A candidate who says "ship fast, fix later" without considering the cost of incidents is not a good fit for a reliability-focused culture. A candidate who says "it depends on the failure impact and the cost of delay" is thinking about trade-offs correctly.

Onboarding reinforces culture. Every new hire should go through incident response training within their first month. They should shadow an on-call shift. They should read postmortems from recent incidents and discuss them with the team. They should learn the organization's monitoring tools, runbooks, and escalation paths before they deploy their first change. This front-loads reliability knowledge. It signals that reliability is not something you learn eventually — it is something you learn immediately because it is core to how the organization operates.

New hires should also participate in blameless postmortems as observers before they participate as contributors. Watching how the organization responds to incidents — whether the focus is on systemic improvement or individual fault — teaches them the real culture, not the stated culture. If postmortems are blameless, learning-focused, and result in concrete action items, new hires will adopt that approach. If postmortems are blame-focused and result in vague commitments to "be more careful," new hires will learn that incidents are career risks to be minimized rather than learning opportunities.

## Celebrating Reliability Wins

Feature launches are easy to celebrate. You ship the feature, users see it, leadership recognizes the team. Reliability wins are harder to celebrate because the best reliability work is invisible. The system does not go down. The incident does not happen. The cascading failure is prevented before it starts. Absence of failure is not emotionally satisfying the way presence of a new feature is. But if you only celebrate feature launches, you only incentivize feature work.

Reliability wins worth celebrating include: completing a quarter without a critical incident, reducing mean time to recovery by 40 percent, eliminating a class of recurring failures, improving monitoring coverage from 60 percent to 95 percent, refactoring a brittle service that had caused six incidents in the previous year, or running a chaos engineering experiment that identified and fixed three latent failures before they hit production.

Celebrate these wins the same way you celebrate feature launches. Send a company-wide announcement. Recognize the team in an all-hands meeting. Give them the same visibility that a product launch gets. This signals that reliability work is valued, not just tolerated.

Also celebrate near-misses. If an engineer identifies a latent failure during a code review and prevents it from reaching production, recognize that. If a team runs a pre-deployment test that catches a regression before it impacts users, recognize that. Near-misses are reliability wins — they are failures that were caught before they caused damage. Celebrating near-misses encourages proactive reliability work. It teaches the organization that finding problems before they become incidents is valuable.

Public recognition does not need to be elaborate. A message in the engineering-wide Slack channel works. A shout-out in a team meeting works. The key is visibility. Reliability work that happens in silence is work that no one learns from and no one is motivated to repeat.

## Learning from Failures Publicly

Blameless postmortems are the foundation of learning culture, but blameless is not enough. Postmortems need to be public. Not just within the engineering team — across the entire organization. Product, Legal, Customer Support, Sales, and executive leadership should see postmortems for major incidents. They should understand what failed, why it failed, and what is being done to prevent recurrence.

Public postmortems create accountability without blame. The team responsible for the incident is accountable for fixing the systemic issues that caused it. Leadership is accountable for providing the resources and prioritization needed to implement the fixes. The rest of the organization is accountable for understanding the constraints under which engineering operates and adjusting their expectations accordingly.

Public postmortems also prevent the same failure from happening in multiple teams. If Team A experiences a failure caused by insufficient monitoring, and Team B never hears about it, Team B will make the same mistake six months later. Public postmortems share lessons across the organization. They turn every incident into a teaching moment for everyone.

Some incidents are too sensitive to share company-wide — incidents involving customer data breaches, security vulnerabilities, or regulatory violations may need to remain within a smaller group until remediation is complete. But the default should be public. The more widely you share incident learnings, the more reliably your organization operates.

Public postmortems also build trust with stakeholders. When Product sees that Engineering is transparently documenting failures and taking concrete action to prevent recurrence, they trust that Engineering is managing risk appropriately. When executives see postmortems that include not just what went wrong but what is being done to fix the underlying system, they are more willing to invest in reliability work. Transparency creates trust. Trust creates the organizational support needed to prioritize reliability.

## Reliability Metrics in Performance Reviews

Performance review criteria shape behavior. If reliability contributions are not explicitly part of performance reviews, reliability work will be deprioritized. Engineers will focus on the work that gets them promoted, and if that work is exclusively feature development, reliability will suffer.

Effective performance review criteria for reliability include: participation in on-call rotation, incident response quality, contributions to postmortems, improvements to monitoring or observability, reduction in technical debt that causes operational burden, mentorship of other engineers on reliability practices, and proactive identification of reliability risks before they cause incidents.

These criteria apply to all engineers, not just SRE. A frontend engineer who improves error handling in the UI so that transient failures do not break the user experience is doing reliability work. A data scientist who designs a model with a confidence threshold so that low-confidence predictions fall back to a safer baseline is doing reliability work. A backend engineer who refactors a service to eliminate a class of race conditions is doing reliability work. All of these contributions should be recognized in performance reviews.

Reliability contributions should also be weighted appropriately. An engineer who spends a quarter shipping three features and causing two incidents should not be rated higher than an engineer who spends a quarter shipping two features and causing zero incidents. Feature velocity is valuable, but reliable feature velocity is more valuable. Performance reviews should reward the engineer who balances both.

For senior and staff engineers, reliability contributions should be weighted even more heavily. Senior engineers are expected to improve the systems they work in, not just add features to them. A senior engineer who does not contribute to reliability — who does not mentor others on operational best practices, who does not improve monitoring, who does not reduce technical debt — is not operating at the expected level. Seniority means taking responsibility for the long-term health of the system, not just the short-term delivery of features.

## Sustaining Reliability Culture Over Time

Culture is not a one-time initiative. It is a sustained practice. Organizations that build reliability culture and then stop reinforcing it will watch that culture erode within two years. New hires will not internalize the values. Leadership changes will shift priorities. Feature pressure will crowd out reliability work. The organization will regress to the default state: a small reliability team firefighting incidents caused by everyone else.

Sustaining reliability culture requires continuous reinforcement through rituals, storytelling, and structural incentives.

**Rituals** keep reliability top-of-mind. Monthly incident review meetings where the entire engineering organization reviews the most impactful incidents from the past month. Quarterly reliability retrospectives where teams reflect on their incident trends and identify systemic improvements. Annual reliability awards that recognize the teams and individuals who made the biggest contributions to system health. These rituals create recurring opportunities to discuss reliability, celebrate progress, and surface problems.

**Storytelling** transmits culture to new members. Senior engineers tell stories about past incidents, what went wrong, and how the organization responded. These stories teach new hires how to think about reliability in context. They show that incidents happen, that learning from them is normal, and that the organization values transparency and improvement over blame. The stories that get told repeatedly become part of the organization's identity. If the stories are about heroic individuals who saved the system through brute force, the culture will reward heroism. If the stories are about teams that systematically eliminated entire classes of failures, the culture will reward systemic thinking.

**Structural incentives** ensure that reliability work continues to happen even when feature pressure is high. Dedicate 20 to 30 percent of every team's sprint capacity to reliability work. Make reliability contributions a requirement for promotion. Include reliability metrics in executive dashboards. Compensate on-call appropriately. Give teams ownership of the systems they build. These structures do not rely on individuals caring more — they make reliability the path of least resistance.

Culture is the accumulation of thousands of small decisions made by individuals across the organization. If those decisions consistently prioritize reliability — because leadership measures it, because performance reviews reward it, because teams own the systems they build, because incidents are treated as learning opportunities — reliability becomes embedded in how the organization operates. If those decisions consistently deprioritize reliability — because feature velocity is the only thing that gets rewarded, because incidents are blamed on individuals, because reliability is someone else's job — reliability remains fragile and dependent on the heroic efforts of a small team.

Reliability culture is not about caring more. It is about designing systems, incentives, and rituals that make reliable behavior the default.

---

Chapter 12 examines drift, decay, and system aging — the long-term processes that degrade AI systems even when no single incident occurs, and how to detect and reverse these trends before they compound into catastrophic failures.
