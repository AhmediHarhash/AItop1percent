# 11.2 — Escalation Paths: When and How to Escalate AI Incidents

The on-call engineer had been fighting the incident for 35 minutes. The model was generating outputs that violated content policy — not egregiously, but consistently enough that the violation rate had jumped from 0.03 percent to 0.24 percent. The engineer followed the runbook: checked the provider status, reviewed recent deployments, spot-checked outputs, ran evals. Everything pointed to model behavior drift. The runbook said "consider rollback if violations exceed 0.15 percent for more than 30 minutes." But rollback was not just an engineering decision. The model was the core product feature. Rolling back meant reverting to an older version with lower accuracy. The engineer escalated to the engineering team lead, who escalated to the VP of Engineering, who escalated to the VP of Product. By the time all stakeholders were aligned, 90 minutes had passed. The decision was made to rollback — but the delay meant another 4,000 users had seen policy-violating content.

Escalation is not failure. Escalation is appropriate routing. Not every incident can or should be resolved by the first responder. Some incidents require specialized expertise. Some require cross-functional decisions. Some require executive authority. The goal of escalation paths is to get the right people involved at the right time — not too early, not too late, and with clear decision-making authority.

## Severity-Based Escalation Criteria

Incidents are not all equal. Your escalation path should reflect severity. Define clear severity levels and escalation criteria for each.

**Severity 1: System down or critical safety issue.** The model is completely unavailable, or the model is generating content that creates immediate safety, legal, or reputational risk. Examples: provider outage with no fallback, model generating dangerous medical advice, model leaking PII at scale, model violating legal compliance requirements.

Escalation: Immediate. Page the on-call engineer, the engineering team lead, the VP of Engineering, and the relevant product and legal stakeholders. Assemble a war room within 15 minutes. Executive involvement is expected. All hands focus on mitigation first, root cause later.

**Severity 2: Significant degradation or policy violation.** The model is available but significantly degraded in quality, cost, or compliance. Examples: precision drops from 92 percent to 73 percent, cost spikes from 15,000 dollars per day to 120,000 dollars per day, content policy violation rate increases by an order of magnitude.

Escalation: Within 30 minutes. The on-call engineer attempts initial mitigation, then escalates to the engineering team lead and relevant product or finance stakeholders. Executive involvement depends on the specific issue. War room assembled if mitigation does not work within the first hour.

**Severity 3: Minor degradation or isolated issue.** The model is available and mostly functioning, but some metrics are off or some users are experiencing issues. Examples: precision drops from 92 percent to 87 percent, latency increases from 800 milliseconds to 1,400 milliseconds, retrieval fails for a specific query type.

Escalation: The on-call engineer handles the incident and documents it. Escalation to the team lead occurs if the issue persists beyond two hours or if the engineer needs specialized expertise. No executive involvement unless the issue escalates to Severity 2.

**Severity 4: Monitoring alert or potential issue.** An alert fired but there is no user-visible impact yet. Examples: eval metrics trending downward but still within acceptable range, cost trending upward but not yet over budget, retrieval cache hit rate declining.

Escalation: The on-call engineer investigates and documents findings. No immediate escalation. If investigation reveals a deeper issue, escalate appropriately. These incidents often become the subject of proactive maintenance work.

Define these severity levels clearly and train your on-call engineers to classify incidents correctly. Misclassification in either direction wastes time. Over-classification creates alert fatigue and executive distrust. Under-classification delays critical response.

## Role-Based Escalation

Different incidents require different expertise. Define escalation paths based on the type of issue, not just severity.

**Engineering escalation.** Most incidents escalate within engineering first. The on-call engineer escalates to the engineering team lead. The team lead escalates to the VP of Engineering if needed. Engineering escalation is appropriate for deployment issues, infrastructure failures, provider outages, retrieval pipeline failures, and model performance regressions that do not cross into policy or legal territory.

**Product escalation.** Escalate to product when the incident requires product decisions. Examples: whether to rollback a model that has lower accuracy but better policy compliance, whether to disable a feature that is causing cost overruns, whether to change content filtering thresholds. Product escalation should happen in parallel with engineering escalation, not sequentially. The engineering lead and product lead coordinate the response together.

**Legal escalation.** Escalate to legal immediately for any incident involving PII leakage, compliance violations, or content that creates legal liability. Do not wait for legal to be available. Page them. Examples: the model generates outputs that violate GDPR, the model leaks patient health information, the model generates content that could be interpreted as financial advice without appropriate disclaimers. Legal decides whether to notify regulators, users, or the public. Engineering executes the technical mitigation.

**Finance escalation.** Escalate to finance when cost incidents exceed predefined thresholds. Most teams set finance escalation thresholds at 2x to 3x normal daily spend. If your normal spend is 12,000 dollars per day and you hit 30,000 dollars in a single day, finance needs to know. They may need to adjust budgets, notify stakeholders, or authorize additional spend to resolve the incident.

**PR and communications escalation.** Escalate to PR and communications for any incident that might become public or affect customer trust. Examples: widespread model unavailability, content policy violations that users are likely to screenshot and share, data leakage incidents. PR decides whether to issue public statements, how to communicate with affected users, and how to manage reputational risk.

**Executive escalation.** Escalate to executives — VP of Engineering, CTO, CPO, CEO — for Severity 1 incidents and for any incident with significant business, legal, or reputational risk. Executives make final decisions on high-stakes trade-offs and authorize emergency resources. Do not surprise executives. If an incident might become a crisis, escalate early.

## Time-Based Escalation Triggers

Some incidents do not resolve quickly. Define time-based escalation triggers so incidents do not linger indefinitely without higher-level involvement.

**First escalation: 30 minutes.** If the on-call engineer cannot mitigate the incident within 30 minutes, escalate to the engineering team lead. This does not mean the incident must be fully resolved — it means initial mitigation should be in progress and the path forward should be clear. If neither is true, the team lead needs to help.

**Second escalation: 90 minutes.** If the incident is not mitigated within 90 minutes, escalate to the VP of Engineering and relevant cross-functional stakeholders. At this point, the incident is either genuinely complex or the team is missing something. Bring in more eyes.

**Third escalation: 3 hours.** If the incident is not mitigated within three hours, escalate to executive leadership and consider assembling a full war room with representatives from all relevant functions. Incidents that last this long often have cascading effects or require non-obvious solutions.

These time triggers are defaults. Adjust them based on your system's criticality and your team's capacity. A consumer-facing feature with millions of users might escalate faster. An internal tool might escalate slower. The key is that escalation happens on a clock, not just on subjective judgment.

## Escalation Mechanics

Escalation is not just sending a Slack message. Escalation requires clear communication, clear handoff, and clear decision-making authority.

**Escalation notification.** When escalating, notify the escalation target through multiple channels. Send a Slack message in the incident channel, send a direct message, and if the issue is Severity 1, page them through your incident management platform. Do not assume they saw your Slack message. Confirm receipt.

**Escalation context.** When escalating, provide clear context. What happened? When did it start? What have you tried? What worked and what did not? What is the current user impact? What are the mitigation options? Do not make the escalation target start from scratch. Write this context in the incident channel so everyone can see it.

**Escalation decision-making.** When escalating, clarify who makes the final decision. The on-call engineer provides options and recommendations. The escalation target makes the decision. If the decision requires multiple stakeholders — engineering and product, for example — clarify who has final authority before you need it. Do not discover during an incident that nobody knows who gets to make the call.

**Escalation handoff.** When escalating, clarify whether you are handing off incident command or just seeking help. In most cases, the on-call engineer remains the incident commander even after escalation. The escalation target provides expertise or makes decisions, but the on-call engineer coordinates execution. In Severity 1 incidents, incident command often shifts to a more senior engineer or engineering leader. Make this explicit.

## Over-Escalation vs Under-Escalation

Both over-escalation and under-escalation waste time and erode trust. Find the right balance.

**Over-escalation** happens when the on-call engineer escalates too early or too broadly. Every minor alert pages the VP of Engineering. Every latency spike assembles a war room. Over-escalation creates alert fatigue. Executives stop responding because they have been paged for too many non-issues. When a real crisis happens, nobody takes it seriously.

Prevent over-escalation by training on-call engineers to classify severity correctly, by empowering them to handle Severity 3 and Severity 4 incidents independently, and by building runbooks that give them clear mitigation steps before escalation.

**Under-escalation** happens when the on-call engineer tries to handle an incident alone for too long. They spend two hours troubleshooting a complex issue that the team lead could have resolved in 20 minutes. They do not escalate a policy violation to legal until users start complaining publicly. Under-escalation delays mitigation and increases impact.

Prevent under-escalation by setting time-based escalation triggers, by creating clear escalation criteria for cross-functional issues, and by building a culture where escalation is expected, not penalized. Engineers should never feel that escalating is admitting failure.

## Documenting Escalation Decisions

Every escalation decision should be documented in the incident timeline. Who was escalated to? When? Why? What decision did they make? This documentation serves multiple purposes.

**Post-incident review.** After the incident, you will review the timeline to understand what worked and what did not. Did escalation happen at the right time? Did the right people get involved? Were decisions made quickly or slowly? Documentation makes this review possible.

**Escalation pattern analysis.** Over time, escalation patterns reveal process gaps. If every model degradation incident escalates to the VP of Engineering, maybe your on-call engineers need more training or authority. If every cost incident escalates to finance, maybe you need automated cost alerts and thresholds. Track escalation patterns and fix the underlying issues.

**Accountability and learning.** Documentation creates accountability. If an escalation was delayed or mishandled, the timeline shows it. If an escalation was handled well, the timeline shows that too. Use this data to improve training and to recognize good incident response.

**Legal and compliance.** For incidents involving data leakage, compliance violations, or legal risk, escalation documentation may be required for regulatory reporting or legal proceedings. Document who was notified, when, and what actions were taken.

## Escalation Path Maintenance

Escalation paths are not static. People change roles. New risks emerge. Old risks evolve. Review and update your escalation paths regularly.

**Quarterly review.** Every quarter, review your escalation paths with engineering, product, legal, finance, and PR. Are the severity definitions still accurate? Are the escalation contacts still correct? Are there new risks that require new escalation paths? Update your runbooks accordingly.

**Post-incident updates.** After every significant incident, review whether the escalation path worked. Did the right people get involved at the right time? Were there delays? Were there gaps in decision-making authority? Update the escalation path immediately if gaps are identified.

**New hire onboarding.** When new people join the on-call rotation or join key escalation roles — new team lead, new VP of Engineering, new legal counsel — update them on escalation paths and incident response expectations. Do not assume they will figure it out during their first incident.

**Contact information.** Keep escalation contact information up to date. Phone numbers, Slack handles, PagerDuty schedules. If your escalation path says "page the VP of Engineering" but the VP's phone number in PagerDuty is outdated, the escalation will fail.

## Executive Escalation

Executive escalation is rare but critical. Executives need to know about incidents that create business, legal, or reputational risk — but they do not need to be involved in every alert.

**When to escalate to executives.** Escalate to executives for Severity 1 incidents, for incidents with significant financial impact — cost overruns above 100,000 dollars, revenue loss, large refunds — for incidents with legal or compliance implications, and for incidents that might become public or require external communication. If you are unsure, escalate. Executives would rather hear about a non-issue than miss a crisis.

**How to escalate to executives.** Executives do not need technical details. They need business impact, user impact, mitigation status, and timeline. "The model is unavailable due to a provider outage. Approximately 15,000 users are affected. We have enabled fallback to an older model with slightly lower accuracy. Provider ETA for resolution is 90 minutes. No data loss, no security risk." That is what they need to hear.

**Executive decision-making.** Executives make decisions on high-stakes trade-offs. Should we pay for emergency provider support? Should we notify affected users immediately or wait until mitigation is complete? Should we issue a public statement? Should we authorize overtime or contractor support? Engineering provides options and recommendations. Executives make the call.

**Executive communication cadence.** During a prolonged incident, provide regular updates to executives even if there is no new information. Every 30 minutes for Severity 1, every hour for Severity 2. "No change in status, still working on mitigation, next update in 30 minutes." This prevents executives from interrupting the response team to ask for updates.

Your escalation paths determine whether incidents are resolved efficiently or chaotically, whether the right people get involved at the right time, whether decisions are made quickly or slowly. Build clear escalation paths, train your team to use them, and maintain them as your organization and risks evolve.

Next, we cover the first responder toolkit — what on-call engineers actually need to respond to AI incidents.
