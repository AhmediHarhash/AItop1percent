# 9.11 — Chaos-Driven Circuit Breaker Calibration

Circuit breakers protect your system by stopping requests to failing dependencies before those failures cascade. But every circuit breaker has threshold parameters — how many failures trigger it, over what time window, with what recovery behavior. Set thresholds too sensitive and the circuit opens during normal transient errors, causing unnecessary degradation. Set them too loose and the circuit stays closed during real failures, allowing cascading damage. Most teams pick thresholds by guessing. Teams that use chaos engineering calibrate thresholds with data.

A payment processing system initially configured circuit breakers to open after 5 failures in 10 seconds. This felt reasonable. During normal operation, the circuit never opened. During their first chaos experiment — injecting 30% error rate into the account balance service — the circuit opened after 2 seconds, blocking all payment requests for 60 seconds while the circuit was open. Payment completion rate dropped from 98% to 14%. The circuit breaker protected the account balance service but destroyed the payment service. Post-chaos analysis revealed that 70% of users had sufficient cached balance data to complete payments even when the account service failed. The circuit breaker was too sensitive. It triggered before fallback mechanisms could handle the degradation. The team recalibrated: circuit opens after 20 failures in 10 seconds, allowing fallbacks to absorb transient issues while still protecting against sustained failures. Re-running chaos validated the new thresholds: payment completion rate stayed at 91% even during account service degradation.

Chaos engineering is the only reliable way to calibrate circuit breakers. You cannot predict correct thresholds from theory. You must observe actual system behavior under actual failure conditions, measure the impact of different threshold values, and iteratively tune until you find the configuration that maximizes resilience. This requires structured experimentation, careful measurement, and willingness to run multiple chaos experiments to converge on optimal settings.

## Why Circuit Breakers Need Calibration

Circuit breakers are not install-and-forget components. They have parameters that determine their behavior, and those parameters interact with your system's failure modes, traffic patterns, and fallback strategies in complex ways. Default settings from framework documentation are rarely optimal for your specific system.

The core circuit breaker parameters are failure threshold, time window, and recovery behavior. Failure threshold: how many failures or what percentage of requests must fail before the circuit opens. Time window: over what period are failures counted. Recovery behavior: how long the circuit stays open, and whether it transitions through a half-open state for testing recovery. Each parameter affects when the circuit opens, how long it stays open, and whether it reopens appropriately after the dependency recovers.

These parameters interact with system characteristics. If your traffic rate is 1000 requests per second, a threshold of "5 failures in 10 seconds" might be appropriate — it represents sustained failure, not transient errors. But if your traffic rate is 10 requests per second, that same threshold triggers on the first error burst. A threshold that works at high traffic becomes hypersensitive at low traffic. Circuit breaker tuning must account for actual traffic patterns.

Parameters also interact with failure modes. Some dependencies fail fast — they return errors within milliseconds. Others fail slow — they hang for 5 seconds before timing out. A circuit breaker threshold based on request count might work well for fast failures but respond too slowly to slow failures. By the time the threshold is reached, 50 slow-failing requests have tied up threads and degraded the caller. You might need time-based thresholds or latency-based thresholds instead of count-based thresholds.

Circuit breakers also interact with fallback mechanisms. If your system has high-quality fallbacks, you can tolerate more failures before opening the circuit — let the fallback handle initial degradation, only open the circuit if degradation becomes severe. If your fallbacks are low-quality or absent, you need the circuit to open quickly to prevent cascading failures. Tuning circuit breakers in isolation from fallbacks produces suboptimal results. You must tune them as a system.

## The Too-Sensitive Problem

A circuit breaker that opens too easily causes unnecessary degradation. Every time the circuit opens, requests are blocked or routed to fallback mechanisms. If those fallback mechanisms are lower quality than the primary dependency, opening the circuit reduces system quality. If fallback mechanisms are absent, opening the circuit causes user-facing errors. Too-sensitive circuit breakers turn transient errors into sustained outages.

The symptoms of too-sensitive circuit breakers appear during chaos experiments. You inject a 10% error rate — representing realistic transient failures that any production system experiences — and the circuit opens within seconds. Completion rate drops by 30%. User-facing error rate spikes. The system is less resilient with the circuit breaker than without it. This is the clearest signal that thresholds are too tight.

A document retrieval system had circuit breakers configured to open after 3 failures in 5 seconds. During chaos testing with 5% error rate on the vector database — a realistic failure scenario that happened monthly in production — the circuit opened and stayed open for 30 seconds. During that window, document retrieval returned empty results, causing downstream summarization and question-answering features to fail. Post-chaos analysis showed that the retrieval cache had a 60% hit rate, meaning 60% of requests could succeed even when the vector database was degraded. The too-sensitive circuit blocked those successful requests unnecessarily.

Too-sensitive circuit breakers also interact poorly with retry logic. If your client retries failed requests, and your circuit breaker counts retries as separate failures, the threshold is reached faster than the actual failure rate would suggest. A real 5% error rate becomes a measured 15% error rate after retries, causing the circuit to open during conditions it should tolerate. Calibration must account for retry behavior.

Fix too-sensitive circuit breakers by increasing the failure threshold, lengthening the time window, or both. A threshold of "10 failures in 20 seconds" is more tolerant than "3 failures in 5 seconds." Test different values through chaos experiments. Inject realistic error rates — 5%, 10%, 15% — and measure system behavior. Find the threshold where the circuit stays closed during transient errors but opens during sustained failures. The goal is not to prevent the circuit from ever opening. The goal is to prevent it from opening during conditions the system can handle.

## The Too-Insensitive Problem

The opposite failure mode is circuit breakers that never open or open too late. These provide no protection. Failures cascade before the circuit intervenes. By the time the threshold is reached, the caller has already exhausted connection pools, degraded its own performance, and potentially failed over entirely.

Symptoms of too-insensitive circuit breakers appear as slow degradation during chaos. You inject a 50% error rate into a dependency. Failures start immediately. But the circuit does not open for 30 seconds, during which time thousands of requests have failed, latency has spiked to 10 seconds, and thread pools are exhausted. When the circuit finally opens, it is too late — the damage is done. The system eventually recovers, but slowly and with extensive collateral damage.

A medical appointment scheduling system had circuit breakers configured to open after 50 failures in 60 seconds. During chaos testing with the patient records database fully unavailable, the circuit did not open for 45 seconds. During that time, 1200 appointment booking requests queued up, each waiting 10 seconds for the database timeout. The system's thread pool was exhausted. New requests were rejected. The scheduling service became completely unavailable. When the circuit finally opened, it took 3 minutes for the thread pool to drain and the system to stabilize. A faster circuit breaker would have limited the damage to the first few seconds.

Too-insensitive circuit breakers are often the result of copy-pasting configuration from other services or using framework defaults without validation. A threshold that works for a high-traffic service handling 10K requests per minute might be far too loose for a low-traffic service handling 100 requests per minute. On the low-traffic service, 50 failures in 60 seconds represents 50% error rate. The circuit should have opened much sooner.

Fix too-insensitive circuit breakers by decreasing the failure threshold, shortening the time window, or adding latency-based triggers. Instead of waiting for 50 failures, open after 10. Instead of measuring failures over 60 seconds, measure over 10 seconds. Add a latency trigger: open the circuit if p95 latency exceeds 2 seconds, even if error rate is low. Test through chaos experiments. The circuit should open fast enough that the caller experiences minimal degradation before protection kicks in.

## Using Chaos to Find the Sweet Spot

The optimal circuit breaker threshold balances two competing goals: tolerate transient errors without opening, and protect against sustained failures by opening quickly. Finding this balance requires experimentation across a range of failure intensities.

Design a chaos experiment series with escalating error rates. Start with low error rates: 5%, 10%, 15%. These represent transient failures. Your circuit breaker should stay closed. System quality should degrade slightly but remain acceptable, with fallback mechanisms absorbing the errors. Then test moderate error rates: 30%, 50%. These represent serious but recoverable failures. Your circuit breaker should open, preventing cascade, but the system should maintain some level of service through fallbacks. Finally, test high error rates: 80%, 100% unavailability. The circuit should open immediately. Fallback mechanisms should activate. Recovery should begin as soon as the dependency returns.

For each error rate, measure circuit open time, system completion rate, system quality, latency, and recovery time. Plot these metrics against error rate. The sweet spot is the threshold where the circuit stays closed for error rates your system can handle and opens quickly for error rates it cannot. This threshold is specific to your system. It depends on your traffic patterns, your fallback quality, your quality requirements, and your dependency characteristics.

A fraud detection system ran this experiment series with their transaction history database. At 5% error rate, fraud detection accuracy was 94% with the circuit closed. At 10%, accuracy was 92%. At 20%, accuracy was 88% — below their 90% threshold. The circuit breaker was tuned to open at 15% error rate sustained for 10 seconds. This allowed the system to tolerate transient errors without degradation, but protected quality when errors became sustained. Re-running chaos at various error rates validated that the circuit now opened at the right time: closed at 10%, open at 20%.

Test recovery behavior explicitly. After the circuit opens, how long should it stay open before testing whether the dependency has recovered? If it stays open too long, you miss the opportunity to resume normal operation. If it tests recovery too soon, you trigger premature reopening that causes flapping. Test by running chaos experiments where the dependency fails for a fixed period then recovers. Observe whether the circuit reopens at the right time and whether reopening is stable or causes flapping.

## Calibrating Error-Based Thresholds

Error-based circuit breakers open when the number or percentage of errors exceeds a threshold. These are the most common type and the most straightforward to calibrate through chaos. Inject errors, measure impact, tune threshold, repeat.

Count-based thresholds — "open after 10 failures in 20 seconds" — work well for systems with consistent traffic. The threshold represents an absolute rate of failure. Calibrate by testing whether your system can handle 10 failures in 20 seconds without meaningful degradation. If it can, increase the threshold. If it cannot, decrease it. The right threshold is the one that matches your tolerance for degradation.

Percentage-based thresholds — "open after 20% error rate over 30 seconds" — work better for systems with variable traffic. During high-traffic periods, 20% error rate might represent 200 failures. During low-traffic periods, it might represent 2 failures. The threshold adapts to traffic patterns. Calibrate by testing at different traffic levels. Inject 20% error rate during peak traffic and off-peak traffic. Ensure the circuit behavior is appropriate in both scenarios.

Hybrid thresholds — "open after 10 failures OR 20% error rate, whichever comes first" — combine the advantages of both. They protect against absolute error counts during high traffic and relative error rates during low traffic. Calibrate each component independently, then test the interaction. Ensure that during high traffic, the percentage threshold dominates, and during low traffic, the count threshold dominates.

A recommendation system calibrated hybrid thresholds through chaos. During peak traffic at 500 requests per second, they tested error rates from 5% to 30%. The circuit opened at 15%, which corresponded to 75 failures in 10 seconds. During off-peak at 20 requests per second, the same 15% error rate corresponded to 3 failures in 10 seconds — too few to be meaningful. They added a minimum count threshold: circuit opens at 15% error rate AND at least 10 failures. This prevented the circuit from opening on small traffic spikes during off-peak while maintaining protection during peak traffic.

## Calibrating Quality-Based Thresholds

Error-based circuit breakers react to technical failures — timeouts, 500 errors, exceptions. But AI systems can fail without technical errors. A model can return a response with 200 OK and 0% error rate while producing completely incorrect outputs. Quality-based circuit breakers open when output quality drops below acceptable thresholds, even if technical success rate remains high.

Quality-based circuit breakers require instrumentation to measure quality in real-time. This might be LLM-as-judge scores, rule-based quality checks, user feedback signals, or downstream success metrics. The circuit monitors these quality signals. If quality drops below threshold, the circuit opens and routes to a different model, a fallback strategy, or human review.

Calibrating quality-based circuit breakers is harder than error-based calibration because quality degradation is often gradual rather than binary. A dependency does not suddenly become 50% lower quality. It degrades slowly as load increases, as models are replaced, or as data drifts. Your threshold must detect meaningful degradation without triggering on normal quality variance.

A customer support classification system used quality-based circuit breakers. Their primary classifier had 93% accuracy under normal conditions. During chaos, they simulated model serving degradation by routing 50% of traffic to an undertrained version of the model with 82% accuracy. The quality-based circuit breaker was configured to open when accuracy dropped below 88% over a 5-minute window. This gave the system time to detect the degradation without opening on short-term variance. Once opened, the circuit routed to a fallback rules-based classifier with 85% accuracy — lower than the primary, but higher than the degraded primary.

Test quality-based thresholds by simulating quality degradation, not just availability degradation. Inject responses from a lower-quality model. Inject responses with known errors. Inject responses that are technically correct but semantically wrong. Measure how long it takes for the quality signal to detect the degradation and whether the circuit opens at the right time. Quality-based circuits need longer time windows than error-based circuits because quality metrics are noisier and require more samples to be statistically meaningful.

## Calibrating Recovery Timing

Circuit breaker recovery behavior determines how the system transitions from "circuit open" back to "circuit closed" after the dependency recovers. Poor recovery timing causes flapping — the circuit reopens prematurely, fails again, closes again, creating instability.

Most circuit breakers use a three-state model: closed, open, half-open. When the circuit is closed, requests flow normally. When failures exceed threshold, the circuit opens, blocking requests. After a timeout, the circuit enters half-open state, allowing a small number of test requests. If test requests succeed, the circuit closes. If they fail, the circuit reopens. The timeout duration and the half-open behavior need calibration.

Timeout duration depends on how long your dependency typically takes to recover. If your database recovers from overload in 10 seconds, a 60-second circuit timeout is too long — you are blocking requests for 50 seconds after recovery. If recovery takes 2 minutes, a 10-second timeout is too short — you are testing recovery before it completes, causing repeated failures. Calibrate by measuring actual dependency recovery time during chaos experiments.

A real-time translation service had circuit breakers with 30-second timeout. During chaos, they simulated translation API outages of varying duration: 10 seconds, 30 seconds, 60 seconds, 120 seconds. For the 10-second outage, the circuit stayed open for 30 seconds — 20 seconds longer than necessary. For the 120-second outage, the circuit tested recovery at 30 seconds, failed, reopened, tested again at 60 seconds, failed, reopened, tested again at 90 seconds, succeeded. The repeated testing created flapping. They implemented adaptive timeout: after each failed half-open test, double the timeout. First test at 30 seconds, second test at 60 seconds, third test at 120 seconds. Chaos validation showed this eliminated flapping.

Half-open state behavior also needs tuning. How many test requests should be allowed through? If you allow only one request, a transient error causes the circuit to reopen even if the dependency has mostly recovered. If you allow 100 requests, you are essentially reopening the circuit without validation. Common practice is 5-10 test requests. Calibrate by simulating partial recovery during chaos: the dependency comes back online but is still unstable, succeeding 70% of the time. Observe whether the half-open logic correctly detects partial recovery and reopens the circuit, or whether it closes prematurely and causes a second failure spike.

## Continuous Calibration Through Chaos

Circuit breaker calibration is not a one-time event. Systems evolve. Traffic patterns change. Dependencies change. Failure modes change. Circuit breakers that were optimally tuned six months ago might be too sensitive or too loose today. Continuous calibration through periodic chaos experiments ensures your circuit breakers remain effective.

Schedule regular circuit breaker validation experiments. Once per quarter, re-run the chaos experiments that originally calibrated your circuit breakers. Inject the same error rates and failure conditions. Compare metrics to the original baseline. If behavior has changed — circuits opening at different times, different recovery patterns, different system impact — recalibrate. Update thresholds to match current system characteristics.

Trigger recalibration after significant system changes. If you change traffic patterns — launching a new feature that doubles request volume — recalibrate circuit breakers for dependencies that feature uses. If you deploy a new version of a dependency with different performance characteristics, recalibrate. If you add or improve fallback mechanisms, recalibrate — better fallbacks might allow more tolerant circuit breaker thresholds.

Automate chaos-based circuit breaker testing where possible. Some teams run lightweight circuit breaker validation experiments weekly in staging environments. These experiments inject small amounts of errors — 5% for 30 seconds — and validate that circuits remain closed. This creates a continuous regression test for circuit breaker configuration. If a configuration change accidentally makes a circuit too sensitive, the automated chaos catches it before production deployment.

A media streaming service ran quarterly circuit breaker calibration chaos for 18 months. In Q1, they calibrated thresholds for their content metadata service. In Q2, traffic increased by 40% and chaos testing revealed that circuits were now too sensitive — they opened during traffic spikes that were normal at the new scale. Recalibration increased thresholds by 50%. In Q3, they deployed a new caching layer that improved fallback quality. Chaos testing showed they could now tolerate higher error rates before opening circuits. Thresholds were relaxed by 20%. In Q4, metadata service response times degraded due to database growth. Chaos revealed that latency-based triggers were needed in addition to error-based triggers. Continuous calibration kept circuit breakers aligned with evolving system behavior.

Circuit breakers are one of your most important resilience mechanisms. They prevent cascading failures and protect system stability. But poorly calibrated circuit breakers can cause more harm than good. Chaos engineering transforms circuit breakers from guessed configurations into data-driven, validated components that behave correctly under real failure conditions. Next, we extend this principle: instead of running chaos experiments manually a few times per year, how do you build continuous, automated chaos testing into your operations so that resilience is constantly validated?
